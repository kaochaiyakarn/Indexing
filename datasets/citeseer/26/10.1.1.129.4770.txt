learning structure dynamic probabilistic networks nir friedman kevin murphy stuart russell computer science division california berkeley ca nir russell cs berkeley edu dynamic probabilistic networks compact representation complex stochastic processes 
examine learn structure dpn data 
extend structure scoring rules standard probabilistic networks dynamic case show search structure variables hidden 
examine applications technology useful predicting classifying dynamic behaviors learning causal orderings biological processes 
provide empirical results demonstrate applicability methods domains 
probabilistic networks pns known bayesian networks belief networks established representations domains involving uncertain relations random variables 
somewhat wellestablished equal importance dynamic probabilistic networks dpns model stochastic evolution set random variables time 
dpns significant advantages competing representations kalman filters handle unimodal posterior distributions linear models hidden markov models hmms parameterization grows exponentially number state variables 
example show dpns outperform hmms standard speech recognition tasks 
pns dpns defined graphical structure set parameters specify joint distribution random variables 
algorithms learning parameters pns dpns widely :10.1.1.16.2929
algorithms typically gradient methods em handle hidden variables missing values 
algorithms learning graphical structure hand restricted networks complete data values variables specified training case :10.1.1.156.9918
friedman developed structural em sem algorithm learning pn structure data hidden variables missing values :10.1.1.24.1555:10.1.1.29.9821
sem combines structural parametric modification single em process appears substantially effective previous approaches parametric em operating outer loop structural search 
sem shown find local optima defined scoring function combines likelihood data structural penalty discourages overly complex networks 
property holds bayesian information criterion bic score variant minimum description length mdl scoring bde score bayesian metric uses explicit prior networks :10.1.1.156.9918
extend bic bde scores handle problem learning dpn structure complete data 
importantly extend sem algorithm learn dpns incomplete data scores 
partial observations set random variables time algorithm constructs dpn possibly including additional hidden variables fits observations possible 
addition hidden variables dpns particularly important processes human decision making speech generation disease processes example partially observable 
formal definitions pns dpns 
section discusses complete data case developing scores dpns existing algorithms learning pn structures applied directly 
section handle case incomplete data show extend sem dpns 
computation necessary sufficient statistics highlighted principal bottleneck 
section describes applications learning simple models human driving behavior videotapes learning models biological processes sparse observations 
preliminaries start short review probabilistic networks dynamic probabilistic networks 
concerned distributions sets discrete random variables variable may take values finite set val denoted denote val size capital letters variable names lowercase letters denote specific values taken variables 
sets variables denoted boldface capital letters sets values denoted boldface lowercase letters network denote variables network denote joint probability distribution variables prior network transition network defining dpn attributes 
corresponding unrolled network 
probabilistic network pn annotated directed acyclic graph encodes joint probability distribution formally pn pair component directed acyclic graph vertices correspond random variables encodes set conditional independence assumptions variable independent parents pa second component represents set parameters quantifies network 
simplest case contains parameter pr pa possible value possible set values pa conditional probability distribution represented table called cpt conditional probability table 
representations require fewer parameters noisy ors trees possible experimental results section simplicity notation shall stick cpt case 
pn defines unique joint probability distribution pa pn describes probability distribution fixed set variables 
dpns extend representation model temporal processes 
simplicity assume changes occur discrete time points indexed non negative integers 
assume set attributes process changes 
random variable denotes value attribute time set random variables represent beliefs possible trajectories process need probability distribution random variables course distribution extremely complex 
assume process markovian assume process stationary transition probability independent assumptions dpn represents joint distribution possible trajectories process consists parts prior network specifies distribution initial states transition network variables taken specify transition probability shows simple example 
transition network prior network variables parents 
transition probability implied network pa dpn defined pair corresponds semi infinite network variables practice reason finite interval unroll dpn structure pn slice parents specified prior network slice parents nodes slices corresponding parents copy conditional distributions variables similar manner 
shows result unrolling network time slices 
dpn model joint distribution obtained obvious way transition model 
learning complete data section develop theory learning dpns complete data 
brief review learns standard pns 
derive details bic bde score dpns discuss optimized search structures 
upshot section learning dpns complete data uses techniques learning pns complete data 
learning dpns quite applying pn methods unrolled network due constraint repeated structure repeated parameters 
learning pns problem learning probabilistic network stated follows 
training set instances find network best matches notion best match defined scoring function 
different scoring functions proposed literature 
frequently bayesian information criterion bde score :10.1.1.156.9918
authors define dpns just transition network assuming slices including slice structure 
general prior distribution quite different independence structure slices may represent way process initialized dependencies induced variables unobserved portion process 
example represents arbitrary starting point taken infinite process reasonable dependency structure reflects stationary distribution markov chain defined transition network 
combine prior transition networks single slice network learning processes prior transition models distinct chosen represent separately 
scores combine likelihood data network pr log penalty relating complexity network 
learning structure pns complexity penalty essential maximum likelihood network usually completely connected network 
bic bde scores derived posterior probability network structure 
random variable range possible network structures fact obtain real world 
bayes rule posterior distribution pr pr pr likelihood data network structure computed associated network parameters pr pr pr obviously specifying parameter priors evaluating integral difficult 
approach avoiding full computation integral examine asymptotic behavior term 
large number data points posterior probability insensitive choice prior assuming prior give probability zero event 
schwarz derives asymptotic estimate behaved priors pr log pr log log parameter settings maximize likelihood data number training instances dimension case complete data just number parameters constant term independent bic score uses equation rank candidate network structures 
notice obviates need parameter priors prior structures reduced counting parameters 
alternative approach evaluate closed form restricted family priors :10.1.1.52.1068:10.1.1.156.9918
roughly speaking prior parameters structure assumed factor independent priors pa rameters conditional pa distribution data complete implies posterior factors way 
conditional distribution updated scored separately 
score computed closed form assume addition prior conditional distribution conjugate family dirichlet distribution 
details appear 
examine large number possible network structures avoid having assign prior distribution parameters possible structure 
provide set assumptions allow parameter prior structures specified single network dirichlet priors single virtual data count describes confidence prior :10.1.1.156.9918:10.1.1.156.9918
similar formula arises minimum description length mdl principle 
approach desirable property scores networks equivalent describe set independence assumptions 
compute full bde score simple description length penalty corresponding pr added 
bic score dpns describe bic score dpns 
unsurprisingly results mirror results pns 
section assume training set consisting seq complete observation sequences 
th sequence length specifies values variables dataset give seq instances initial slices train instances transitions train start introducing notation 
define pr pa similarly pr pa need notation sufficient statistics family pa pa indicator function takes value event occurs sequence 
rearranging terms find likelihood function decomposes structure dpn just pns pr log likelihood log log decomposition facilitates computation bic bde scores ways 
note likelihood expressed sum terms term depends conditional probability variable particular assignment parents 
want find maximum likelihood parameters maximize family independently 
second decomposition implies learn independently learn exactly manner learning pn set samples transitions 
arguments precise 
standard maximum multinomial distributions immediately get expression similarly transition case 
case cpts number parameters network similarly transition case 
substituting find bic score pa bic bic bic bic log seq log bic log log note penalty families original time slice function number sequences seq observe examples part model penalty transition model function total number transitions observed 
bde score dpns recall compute bde score need evaluate integral 
pr pr pr term inside integral decomposes equation 
assume prior conditional probability independent rest prior term decomposes pr inserting preceding equation see entire expression consists integral product independent terms 
pr written product integrals pr pr similarly transition case 
order obtain closed form solution assume dirichlet priors 
dirichlet prior multinomial distribution variable specified set hyperparameters val follows dirichlet pr pr intuitively hyperparameters thought pseudo counts play similar role actual counts derive data 
dirichlet prior probability observing sequence values counts pr val gamma function satisfies properties returning bde score assume structure chosen hyperparameters pr rewrite product terms similarly transition case 
requires supply dirichlet hyperparameters candidate dpn structure 
number possible dpn structures large prior estimates hard asses practice 
assign prior dpn equivalent sample sizes components assign dirichlet weights follows pa pa note choice parents differ structure intuitively consider belief equivalent having previously experienced sequences transitions :10.1.1.156.9918
easy verify choosing priors manner preserves main property require dpn structures imply identical independence assumptions receive score 
claim definition natural extension bde score case dynamic probabilistic networks 
learning practice scores considered far important properties 
score dpn written sum terms bde case look logarithm term determines score pr particular choice parents particular variable 
local change family arc addition removal affects terms 
second term eval parents function appropriate counts family 
caching counts efficiently evaluate families 
properties exploited hill climbing search procedures gradually improve candidate structure applying best arc addition deletion reversal :10.1.1.52.1068
case dpns opposed static pns additional constraint network structure repeat time 
reduces number options point search 
search best structure independently search best structure learning incomplete data examine learn dpns incomplete data 
incomplete data crucial real life applications complete observability process want model 
means process stationary markov process partial observations markovian 
example suppose tracking car moving highway observing car speed lane relative distance cars 
example clearly markovian observations 
hand markovian model reasonable provided include part state information driver goals get left lane exit ramp learning hidden variables capture state information process 
allows model remember additional information past better predictions 
main difficulty learning partial observations longer score decomposition properties 
means optimal parameter choice part network depends parameter choices parts network 
problem understood better notice partial observability longer talk counts training data 
events interest counts defined know exact value variables questions 
commonly method alleviate problem expectation maximization em algorithm 
step em uses currently estimated parameters complete data computing expected counts 
step re estimates maximumlikelihood parameter values expected counts true observed counts 
central theorem behavior em cycle guaranteed improve data model reaches local maximum 
em traditionally viewed method adjusting parameters fixed model structure 
underlying theorem generalized apply structural parametric modifications 
friedman structural em sem algorithm step em completing data computing expected counts current structure parameters :10.1.1.29.9821
addition re estimating parameters step sem expected counts current structure evaluate candidate structure essentially performing complete data structural search inner loop 
friedman shows large family rules including bic score bde score resulting network higher score original 
true expected counts evaluating new structure computed old structure 
extend friedman results dpn case way 
bic expected bic score dpn possible completions data assignment values hidden variables 
expectation taken pr respect probability assigned completion old dpn 
theorem prove theorem bic bic bic bic choose new dpn higher expected score expected score old dpn true score new dpn higher true score old dpn :10.1.1.29.9821
difference expected scores lower bound improvement terms actual score trying optimize 
crucial property expected bic score decomposes sum local terms follows 
linearity expectation push operator summation signs implicit get equation involves terms form pa pr pa similarly called expected sufficient statistics 
key requirement apply sem algorithm incomplete data case ability compute probabilities pr families pa networks wish evaluate current network neighboring networks search space 
efficiently compute probabilities families convert dpn join tree pass dynamic programming algorithm similar algorithm hmms :10.1.1.150.82
efficiently compute probability set nodes contained join tree nodes need sophisticated techniques 
simpler approach currently connect variables single timeslice single node convert dpn markov chain compute expected number transitions pair consecutive states chain marginalize counts get counts family 
maintain cache expected counts computed far current completion model cache avoid recomputing expected counts 
technique computing exact expected sufficient statistics models interest remain computationally challenging 
discussion section mention approximation techniques may allow learn larger models 
summary sem procedure follows 
procedure dpn sem possibly randomly 
choose loop convergence improve parameters em search dpn structures expected counts computed best scoring dpn seen return data size bic bde seq logloss bits slice test data networks learned driving domain bic bde scores 
columns labeled number additional binary variables introduced 
dist camera dist camera lane lane xdot xdot xdot xdot lane dist camera lane dist camera xdot xdot dist camera lane transition models learned driving domain 
shaded nodes correspond hidden variables 
discussion application sem bic score 
friedman shows extend sem procedure learn bde score 
details involved bde score linear 
friedman shows reasonable approximation bde score expected counts inside sem loop 
applications section describe preliminary investigations attempt evaluate usefulness dpn technology develop real life applications 
inferring driver behavior tracking domains learn predictive model behavior object tracked 
accurate model robust tracking system useful predictions decision making 
objects hidden state hope dpns learned correctly reflect unobserved process governing behavior 
learned model may provide insight behavior generated 
section describe experiments carried simulated driving domain 
data idealization cameras mounted side road collect 
particular time step simulation get report cars dist camera lane camera range 
report car attributes position velocity relative camera frame relative speed distance car front car immediately left right 
data want learn models typical classes driving behavior 
models useful tasks 
prime example arises autonomous car project 
autonomous controller attempts predict behavior neighboring car 
example useful know just driven lanes attempting leave freeway consequently cut front 
tracking information real cameras readily available reasonable hope realistic models human drivers obtained 
addition autonomous vehicles models paramount importance called microscopic traffic models freeway design construction planning safety studies 
experiments generate variety simulated traffic patterns consisting populations vehicles mixture different driving tendencies trucks sports cars sunday drivers 
generated cars tracked behavior sequences roughly time steps 
observed data discretized fixed sized bins 
trained networks dataset consisting sequences tested networks sequences 
simple pathway model vertices 
vertex represents site genome arc possible triggering pathway 
dpn model equivalent pathway model shown 
learned networks hidden variables decision tree cpts bic bde score 
initial network structure case hidden variable dependent value previous time slice observable variable dependent hidden variables time slice 
put persistence arcs initial model 
structure connects hidden variable variables allowing variable carry forward information previous time slices 
table summarizes logloss time slice test data different networks learned 
roughly interpret negative logloss number bits needed encode time slice network 
numbers indicate addition hidden variables improves predictive ability models 
indicate benefit training sequences important events lane changes quite rare typical freeway traffic patterns 
learned networks shown figures 
networks include essential relationships expect see 
example single hidden node parent relative distance children lane position longitudinal velocity lateral velocity 
suggests encodes need take avoiding action learning causal pathways biological processes dpns powerful representation language describing causal models stochastic processes 
models useful areas science including molecular biology interested inferring structure regulatory pathways 
mcadams shapiro model part genetic circuit lambda terms sequential logic circuit attempts automatically infer form boolean circuits data 
known abstraction binary valued signals deterministic switches breaks model comparison utility compresses observations approximately bits time slice 
continuous nature inherent noise underlying system get accurate predictions 
able deal noisy incomplete observations system 
believe dpns provide tool modeling noisy causal systems furthermore sem provides way learn models automatically noisy incomplete data 
describe initial experiments dpns learn small artificial examples typical causal processes involved genetic regulation 
generate data models known structure learn dpn models data variety settings compare original models 
main purpose experiments understand dpns represent processes observations required sorts observations useful 
refrain describing particular biological process sufficient real data processes studying learn scientifically useful model 
simple genetic systems commonly described pathway model graph vertices represent genes larger chromosomal regions arcs represent causal pathways 
vertex normal state abnormal state 
system starts state vertices spontaneously turn due unmodelled external causes probability unit time 
vertex turned stays may trigger neighboring vertices turn certain probability unit time 
arcs graph usually annotated half life parameter triggering process 
note pathway models pns contain directed cycles 
important biological processes structure parameters graph completely unknown discovery constitute major advance scientific understanding 
pathway models natural representation dpns vertex state variable triggering arcs represented links dpn 
tendency vertex stay triggered represented persistence links dpn 
shows dpn representation vertex pathway model 
nature problem suggests noisy ors noisy ands provide parsimonious representation conditional density function node 
specify noisy node parents parameters child node state th par ent state 
need parameters full cpt 
vertex dpn model experiments reported parameters persistence arcs value 
strict persistence model vertices stay triggered parameters persistence arcs fixed 
learn noisy distributions follow technique suggested entails introducing new hidden node arc network noisy version parent replacing noisy gate deterministic gate 
tried gradient descent encountered difficulties convergence cases optimal parameter values close boundaries hamming distance hamming distance number training sequences number training sequences relative logloss relative logloss number training sequences number training sequences hamming distance left learned models compared generating model vertex process relative log loss right compared generating model independent sample sequences 
tests regimes slices hidden random 
top tabulated cpts bottom noisy network 

experiments enforced presence persistence arcs network structure 
alternative initial topologies persistence arcs system learn add arcs fully interconnected system learn delete arcs 
performance cases similar 
experimented observation regimes correspond realistic settings complete state system observed time step 
entire time slices hidden uniformly random probability corresponding intermittent observation 
observations pro cess begins unknown time obs process initiated external spontaneous event 
case disease processes dna diseased cell observed elapsed time disease process began known 
initial observation dna healthy cell individual 
case obtains realistic situations raises new challenge machine learning 
resolve follows supply network diseased observation time slice high probability larger actual elapsed time obs process began 
augment dpn model hidden switch variable initially come spontaneously 
switch system evolves normal transition model parameters set true network actual system state high probability length training testing 
determined data 
switch turns state system frozen conditional probability distribution fixed probability 
persistence parameter determines probability distribution obs fixing parameter priori obs high probability effectively fix scale time arbitrary 
learned network imply constrained distribution obs pair observations 
consider measures performance 
number different edges learned network compared generating network hamming distance adjacency matrices 
second difference logloss learned model compared generating model measured independent test set 
results vertex model shown 
see noisy ors perform better tabular cpts amount missing data high 
missing slices exact structure learned examples noisy network 
slices hidden system learn effectively reasonable amount data results shown 
case know time second observation modeled switching variable harder learn results shown 
obviously application prior knowledge important reducing data requirements 
discussion addressed question learning structure parameters dpns complete incomplete data 
best knowledge examine problem 
experiments show learn non trivial structures synthetic data realistic data nontrivial simulator 
main bottleneck application procedures inference necessary compute expected sufficient statistics 
case pns sparse dpn structure necessarily ensure fast inference minimum size posterior distribution slice generally exponential number variables parents previous slice 
various approximations speed inference method proposed boyen koller approximates posterior probabilities dpn factored form particularly appropriate biological models investigating 
stochastic simulation example er sof algorithm kanazawa :10.1.1.48.7248
variational approximations jaakkola ghahramani jordan :10.1.1.16.2929
methods multilevel abstraction hierarchy detect variables related 
currently extending sem learn structure linear gaussian dpns hope prove competitive traditional techniques system identification 
advantage gaussian case discrete case marginalizing posterior slices efficient operation 
ultimately wish tackle case hybrid dpns discrete continuous variables 
advantage hybrid dpns switching state space models state variables represented factored form 
example driving domain separate variables continuous observations speed position discrete hidden states want change lane want overtake 
question know add hidden variables interesting currently investigating 
acknowledgments jeff forbes help getting training data driving domain 
supported part aro muri program integrated approach number daah 
binder koller russell kanazawa 
adaptive probabilistic networks hidden variables 
mach 
learning 
boyen koller 
tractable inference complex stochastic processes 
uai 
buntine 
theory refinement bayesian networks 
uai pp 


cooper herskovits 
bayesian method induction probabilistic networks data 
mach 
learning 
dean kanazawa 
probabilistic temporal reasoning 
aaai pp 

degroot 
optimal statistical decisions 

dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal stat 
soc 
forbes huang kanazawa russell 
bayesian automated taxi 
ijcai 
forbes parr russell 
feasibility study fully automated traffic decision theoretic control 
cal path research report ucb prr inst 
transportation studies berkeley 
friedman :10.1.1.29.9821
learning belief networks presence missing values hidden variables 
icml 
friedman 
bayesian structural em algorithm 
uai 
friedman goldszmidt 
learning bayesian networks local structure 
jordan ed learning graphical models 
preliminary version appeared uai 
geiger heckerman 
learning gaussian networks 
uai 
ghahramani jordan :10.1.1.16.2929
factorial hidden markov models 
mach 
learning 
heckerman geiger chickering :10.1.1.156.9918
learning bayesian networks combination knowledge statistical data 
mach 
learning 
ghahramani hinton 
switching state space models 
submitted publication 
jaakkola jordan 
recursive algorithms approximating probabilities graphical models 
nips pp 

kanazawa koller russell :10.1.1.48.7248
stochastic simulation algorithms dynamic probabilistic networks 
uai pp 

kjaerulff 
computational scheme reasoning dynamic probabilistic networks 
uai 
lam bacchus 
learning bayesian belief networks approach mdl principle 
comp 
intel 
lauritzen 
em algorithm graphical association models missing data 
comp 
stat 
data anal 
liang fuhrman somogyi 
reveal general reverse engineering algorithm inference genetic network architectures 
pacific symp 
biocomputing vol 
pp 

ljung 
system identification theory user 
prentice hall 
malik russell 
traffic surveillance detection technology development new sensor technology final report 
research report ucb prr cal path program 
mcadams arkin 
stochastic mechanisms gene expression 
proc 
nat 
acad 
sci 
mcadams shapiro 
circuit simulation genetic networks 
science 
meek heckerman 
structure parameter learning causal independence causal interaction models 
uai pp 

schwarz 
estimating dimension model 
ann 
stat 
smyth heckerman jordan :10.1.1.150.82
probabilistic independence networks hidden markov probability models 
neural computation 
xu 
computing marginals arbitrary subsets marginal representations markov trees 
art 
intel 
zweig russell 
speech recognition dynamic bayesian networks 
aaai 
