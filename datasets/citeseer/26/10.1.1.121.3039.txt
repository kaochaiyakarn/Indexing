journal arti cial intelligence research submitted published connectionist theory re nement genetically searching space network topologies david opitz opitz cs umt edu department computer science university montana mt usa jude shavlik shavlik cs wisc edu computer sciences department university wisconsin dayton st madison wi usa algorithm learns set examples ideally able exploit available resources abundant computing power domain speci knowledge improve ability generalize 
connectionist theory re nement systems background knowledge select neural network topology initial weights proven ective exploiting domain speci knowledge exploit available computing power 
weakness occurs lack ability re ne topology neural networks produce limiting generalization especially impoverished domain theories 
algorithm uses domain speci knowledge help create initial population knowledge neural networks genetic operators crossover mutation speci cally designed knowledge networks continually search better network topologies 
experiments real world domains indicate new algorithm able signi cantly increase generalization compared standard connectionist theory re nement system previous algorithm growing knowledge networks 

scienti industrial problems better understood learning samples task 
reason machine learning statistics communities devote considerable research ort inductive learning algorithms 
learning algorithms fail capitalize number potentially available resources knowledge computing power improve ability generalize 
domain speci knowledge desirable inductive learners start approximately correct theory achieve improved generalization accuracy examples seen training signi cantly fewer training examples ginsberg ourston mooney pazzani kibler towell shavlik 
making ective available computing power desirable applications important obtain concepts generalize induce concepts quickly 
article algorithm called regent re ning genetic evolution network topologies utilizes available computer time extensively search neural network ai access foundation morgan kaufmann publishers 
rights reserved 
opitz shavlik topology best explains training data minimizing changes domain speci theory 
inductive learning systems utilize set approximately correct domain speci inference rules called domain theory describe currently known domain called theory re nement systems 
making knowledge shown important rules may contain insight easily obtainable current set training examples ourston mooney pazzani kibler towell shavlik 
domains expert created theory willing wait weeks months learning system produce improved theory 
rapid growth computing power believe important learning techniques able trade expense large numbers computing cycles gains predictive accuracy 
analogous anytime planning techniques dean boddy believe machine learning researchers create better anytime learning algorithms 
learning algorithms produce concept quickly continue search concept space reporting new best concept 
concentrate connectionist theory re nement systems shown frequently generalize better inductive learning theory re nement systems fu towell 
kbann towell shavlik example connectionist system translates provided domain theory neural network determining network topology re nes reformulated rules backpropagation rumelhart hinton williams 
kbann connectionist theory re nement systems alter network topologies su er impoverished domain theories ones missing rules needed adequately learn true concept opitz shavlik towell shavlik 
topgen opitz shavlik improvement systems heuristically searches space possible network topologies adding hidden nodes neural representation domain theory 
topgen showed statistically signi kbann real world domains opitz shavlik empirically show su ers considers simple expansions kbann network 
address limitation broaden types topologies topgen considers genetic algorithms gas 
choose gas reasons 
gas shown ective optimization techniques cient global information goldberg holland mitchell 
second gas inherent quality suitable anytime learning 
line application mode dejong gas simulate alternatives output best alternative seen far 
new algorithm regent proceeds rst trying generate domain theory diversi ed initial population 
produces new candidate networks genetic operators crossover mutation networks trained backpropagation 
regent crossover operator tries maintain rule structure network mutation operator adds nodes network topgen algorithm 
genetic operators specialized connectionist theory re nement 
experiments reported show regent better able search network topologies topgen 
connectionist theory refinement rest organized follows 
section brie argue importance ectively exploiting data theory available computer time learning process 
review kbann topgen algorithms 
details regent algorithm section 
followed empirical results human genome project domains 
section discuss results 
review related concluding 

data prior knowledge available cpu cycles system learns set labeled examples called inductive learner alternately supervised empirical similarity learner 
output example provided teacher set labeled examples learner called training set 
task inductive learning generate training set concept description correctly predicts output examples just training set 
inductive learning algorithms previously studied michalski quinlan rumelhart 
algorithms di er concept representation language method bias constructing concept language 
di erences important determine concepts classi er induce 
alternative inductive learning paradigm build concept description set examples querying experts eld directly assembling set rules describe concept build expert system waterman 
problem building expert systems theories derived interviewing experts tend approximately correct 
expert provided domain theory usually rst approximation concept learned inaccuracies frequently exposed empirical testing 
theory re nement systems ginsberg ourston mooney pazzani kibler towell shavlik systems revise theory basis collection examples 
systems try improve theory making minimal repairs theory consistent training data 
changes initial domain theory kept minimum theory presumably contains useful information completely correct 
hybrid learning systems designed learn theory data empirical tests shown achieve high generalization signi cantly fewer examples purely inductive learning techniques ourston mooney pazzani kibler towell shavlik 
ideal inductive learning system able incorporate background knowledge available form domain theory improve ability generalize 
indicated earlier available computer time important resource computing power rapidly increasing problems expert willing wait lengthy period improved concept 
reasons develop anytime learning algorithms continually improve quality answer time 
dean boddy de ned criteria anytime algorithm algorithm suspended resumed minimal overhead algorithm stopped time return answer algorithm return answers improve output opitz shavlik input classical regression example smooth function solid curve noisy data points probably better predictor high degree polynomial dashed curve 
time 
criteria created planning scheduling algorithms apply inductive learning algorithms 
standard inductive learners rumelhart id quinlan unable continually improve answers receive additional training examples 
fact run long algorithms tend training set holder 
tting occurs learning algorithm produces concept captures information training examples general characteristics domain 
concepts great job classifying training instances poor job generalizing new examples ultimate measure success 
help illustrate point consider typical regression case shown 
tting noisy data high degree polynomial lead poor generalization 
general framework encouraging algorithm improve answer time quite simple 
spend computer time considering di erent possible concept descriptions scoring possibility keeping description scores best 
framework anytime respect scoring function 
scoring function approximate measure generalization obviously prone problems tting guarantee generalization monotonically decrease time 
assuming accurate scoring function long considering wide range possibilities quality best concept improve longer period time 

term anytime learning di ers grefenstette ramsey mean continuous learning changing environment 

review kbann topgen connectionist theory refinement goal research exploit prior knowledge available computing cycles search neural network generalize best 
proceed choosing initial guess network de ned kbann algorithm 
continually re ne topology nd best network concept 
presenting new algorithm regent kbann algorithm initial approach re ning kbann created network topology topgen 
kbann algorithm kbann towell shavlik works translating domain theory consisting set propositional rules directly neural network see 
shows rule set de nes membership category represents hierarchical structure rules solid lines representing necessary dependencies dotted lines representing dependencies 
represents network kbann creates translation 
sets biases nodes representing disjuncts output near high weighted antecedents satis ed nodes representing conjuncts high weighted antecedents satis ed near positive links near negative links 
activations near 
kbann creates nodes handle rules disjunctively de ning thin lines represent low weighted links kbann adds allow rules add new antecedents backpropagation training 
network initialization kbann uses available training instances re ne network links 
refer towell towell shavlik details 
kbann successfully applied real world problems control chemical plant scott shavlik ray protein folding maclin shavlik kbann translation knowledge base neural network 
panel shows sample propositional rule set prolog clocksin mellish notation panel illustrates rule set corresponding dependency tree panel shows resulting network created kbann translation 
opitz shavlik nding genes sequence dna opitz shavlik towell shavlik ecg patient monitoring watrous towell glassman 
case kbann shown produce improvements generalization standard neural networks small numbers training examples 
fact towell favorably compared kbann wide variety algorithms including purely symbolic theory re nement systems version promoter splice junction tasks include testbeds section 
training kbann created network alters antecedents existing rules capability inducing new rules add additional hidden nodes training 
instance kbann unable add third rule inferring example 
help illustrate point consider example 
assume target concept consists domain theory plus rule trained kbann network shown possible examples target concept unable completely learn conditions true 
topology kbann network modi ed order learn new rule 
studies show opitz shavlik towell kbann ective removing extraneous rules antecedents expert provided domain theory generalization ability su ers impoverished domain theories theories missing rules antecedents needed adequately learn true concept 
ideal connectionist theory re nement algorithm able dynamically expand topology network training 
topgen algorithm topgen opitz shavlik addresses kbann limitation heuristically searching space possible expansions knowledge neural network topology determined direct mapping dependencies domain theory kbann network 
topgen proceeds rst training kbann network placing search queue 
cycle topgen takes best network search queue estimates errors occur network adds new nodes response estimates trains new networks places back queue 
topgen judges errors occur network training examples increment counters node false negatives false positives 
illustrates possible ways topgen add nodes networks 
symbolic rule base uses negation failure decrease false negatives dropping antecedents existing rules adding new rules rule base 
kbann ective removing antecedents existing rules unable add new rules topgen adds nodes intended decreasing false negatives fashion analogous adding new rule rule base 
existing node node topgen adds new node child see fully connects new node input nodes 
existing node node topgen creates new node parent original node new node topgen fully connects inputs see topgen moves outgoing links original node outgoing links new node 
existing node 
node 
node connectionist theory refinement decrease false negatives decrease false positives new node 

new node new node 

new node new node new node possible ways add new nodes knowledge neural network arcs indicate nodes 
decrease false negatives wish broaden applicability node 
conversely decrease false positives wish constrain node 
symbolic rule base decrease false positives adding antecedents existing rules removing rules rule base 
kbann ectively remove rules ective adding antecedents rules unable invent constructively induce michalski new terms antecedents 
topgen adds new nodes intended decrease false positives fashion analogous adding new constructively induced antecedents network 
figures illustrates thisis done analogous figures explained 
refer opitz shavlik details 
topgen showed statistically signi cant improvements kbann real world domains comparative experiments simpler approach adding nodes veri ed new nodes added intelligent manner opitz shavlik 
article increase number networks topgen considers search show increase generalization primarily limited rst networks considered 
topgen anytime algorithm rst step 
due fact topgen considers larger networks contain original kbann network subgraphs increases number networks considered increase variety considered opitz shavlik search 
broadening range networks considered search topology space major focus 

regent algorithm new algorithm regent tries broaden types networks topgen considers gas 
view regent having phases genetically searching topology space training network backpropagation gradient descent method 
regent uses domain theory aid phases 
uses theory help guide search topology space give starting point space 
table summarizes regent algorithm 
regent rst sets aside validation set part training instances scoring di erent networks 
perturbs kbann produced network create initial set candidate networks 
regent trains networks backpropagation places population 
cycle regent creates new networks crossing mutating networks current population randomly picked proportional tness validation set correctness 
trains new networks places population 
searches regent keeps network lowest validation set error best concept seen far breaking ties choosing smaller network application occam razor 
parallel version regent trains candidate networks time condor system litzkow livny mutka runs jobs idle workstations 
initial population broaden types networks regent considers search domain theory may provide useful information training set desirable theory generating initial population 
regent creates diversity domain theory randomly perturbing kbann network various nodes 
regent perturbs node deleting adding new nodes manner analogous topgen methods adding goal search best network topology describing domain theory data 

set aside validation set training instances 

perturb kbann produced network multiple ways create initial networks train networks backpropagation place population 

loop forever create new networks crossover mutation operators 
train networks backpropagation score validation set place population 
new network network lowest validation set error seen far breaking ties preferring smallest network report current best concept 
table regent algorithm 
crossover networks connectionist theory refinement goal crossover networks generate new network topologies 

divide network hidden nodes sets 

set forms network set forms 
new network created follows inherits weight ji parent nodes inherited input output nodes 
link unconnected nodes levels near zero weights 
adjust node biases keep original function node see text explanation 
goal divide hidden nodes sets probabilistically maintaining network rule structure 
hidden node assigned set collect unassigned hidden nodes output linked nodes output nodes 
ii set set empty node collected part randomly assign set set probabilistically add nodes collected part set set equation shows probability assigned set probability assigned set minus value 
table regent method crossing networks 
nodes 
happen multiple theories domain seed population 
regent crossover operator regent crosses networks rst dividing nodes parent network sets combining nodes set form new networks nodes sets form network nodes sets form 
table summarizes regent method crossover illustrates example 
regent divides nodes level time starting level nearest output nodes 
considering level set set empty cycles randomly assigns set 
set empty nodes probabilistically placed set 
equation calculates probability 
de ne level di erent ways de ne node level longest path output node 
original networks crossed resulting networks output input output input opitz shavlik output input output input regent method crossing networks 
hidden nodes original network divided sets nodes sets form new network nodes sets form 
grey lines represent low weighted links added fully connect neighboring levels 
agiven node assigned set prob node means node member set wji weight value node node probability belonging set minus probability 
probabilities regent tends assign set nodes heavily linked 
helps minimize destruction rule structure crossed networks nodes belonging syntactic rule connected heavily linked weights 
regent crossover operator produces new networks crossing rules simply crossing nodes 
regent decide connect nodes newly created networks 
new network inherits weight values parents links connect nodes inherited new network connect inherited hidden node input output node directly connect input node output node 
adds randomly set low weighted links unconnected nodes consecutive levels 
adjusts bias nodes help maintain original function 
instance regent removes positively weighted incoming link node decrements node bias subtracting product link magnitude connectionist theory refinement average activation set training examples entering link 
bias node needs slightly sum positive weights incoming links see towell shavlik details 
regent increments bias node analogous amount negatively weighted incoming links bias node slightly greater sum negative weights incoming links node inactive incoming negatively weighted linked nodes active positively weighted linked nodes inactive 
regent mutation operator regent mutates networks applying variant topgen 
regent uses topgen method incrementing false negatives false positives counters node 
regent adds nodes values counters way topgen 
neural learning ective removing unwanted antecedents rules see section regent considers adding nodes deleting mutation 
mutation operator adds diversity population maintaining directed heuristic search technique choosing add nodes directedness necessary currently unable evaluate possible networks day 
additional details regent adds newly trained networks population validation set correctness better equal existing member population 
regent replaces member replaces member having lowest correctness ties broken choosing oldest member 
techniques goldberg replacing member nearest new candidate network promote diverse populations want promote diversity expense decreased generalization 
research topic plan investigate incorporating diversity promoting techniques able consider tens thousands networks 
regent considered lamarckian genetic hillclimbing algorithm ackley performs local optimizations individuals passes successful optimizations spring 
ability individuals learn smooth tness landscape facilitate subsequent learning 
lamarckian learning lead large increase learning speed solution quality ackley littman farmer belin 

experimental results section test regent real world human genome project problems aid locating genes dna sequences recognizing promoters splice junctions ribosome binding sites 
domains input short segment dna nucleotides elements long task learn predict dna subsequence contains biologically important site 
domain accompanied domain theory generated dna expert noordewier 

lamarckian evolution theory inheritance characteristics acquired lifetime 
opitz shavlik promoter domain contains positive examples negative examples rules 
splice junction domain contains examples distributed equally classes rules 
ribosome binding sites rbs domain contains positive examples negative examples rules 
note promoter data set domain theory version appears towell 
domains available university wisconsin machine learning uw ml site world wide web ftp ftp cs wisc edu machine learning shavlik group datasets anonymous ftp ftp cs wisc edu machine learning shavlik group datasets 
rst directly compare regent topgen kbann 
perform lesion study regent 
particular investigate value adding randomly created networks regent initial population examine utility regent genetic operators 
experimental methodology results article fold cross validation runs 
fold cross validation data set rst partitioned equal sized sets set turn test set classi er trains sets 
fold regent run population size 
network trained backpropagation 
parameter settings neural networks include learning rate momentum term number training epochs rst standard settings epochs may typically neural network literature set help avoid tting 
set aside validation set consisting training examples regent scoring function 
generalization ability regent section experiments compare test set accuracy generalization regent topgen 
shows test set error kbann topgen regent search space network topologies 
horizontal line graph results kbann algorithm 
drew horizontal line sake visual comparison recall kbann considers single network 
rst point graph network considered nearly algorithms start kbann network topgen regent di er slightly kbann set aside part training set score candidate networks 
notice topgen stops improving considering networks generalization ability better topgen point 
reason occasional upward movements due fact validation set scoring function inexact estimate true generalization error results fold cross validation 
presents test set error topgen regent consider candidate topologies 
standard neural network results fully connected single layer feed forward neural network fold trained networks containing hidden nodes validation set choose best network 
results 
lesion study components algorithm individually disabled ascertain contribution full algorithm performance kibler langley 
test set error connectionist theory refinement ribosome binding sites kbann splice junctions promoters topgen regent networks considered error rates human genome problems 
opitz shavlik show kbann generalizes better best standard networks con rming kbann ectiveness generating network topologies 
topgen able improve network regent able signi cantly decrease error rate kbann topgen 
benchmark purposes regent error rate fold cross validation full splice junction dataset examples commonly machine learning researchers 
table contains number hidden nodes nal networks produced kbann topgen regent 
results demonstrate regent produces networks larger kbann topgen networks topgen adds nodes search 
regent networks larger necessarily mean complex 
inspected sample networks large portions network weights insigni cantly small functional duplications groups hidden nodes 
prune weights nodes regent search pruning prematurely reduce variety structures available recombination crossover koza 
real life organisms instance super uous dna believed enhance rate evolution watson hopkins roberts weiner 
pruning network size genetic search may unwise prune regent nal network say hassibi stork optimal brain surgeon algorithm 
post pruning process may increase classi cation speed network increase comprehensibility possibly accuracy 
lesion study regent section describe lesion study performed regent 
single run regent takes cpu days consider networks single fold cross test set error splice junctions promoters key standard nn kbann topgen regent test set error rates topgen regent consider networks 
pairwise tailed tests indicate regent di ers standard nn kbann topgen con dence level problems 
connectionist theory refinement domain kbann topgen regent rbs splice junction promoters table number hidden nodes networks produced kbann topgen regent 
columns show mean number hidden nodes networks 
standard deviations contained parentheses report standard deviations kbann uses network 
validation takes minimum cpu days 
inherent similarity investigating various aspects regent multiple datasets feasible run experiments section con dence level reached cases assuming level exists 
results convey important information various components regent shown previous section complete regent algorithm generate statistically signi cant improvements existing algorithms 
including non regent population correct theory may quite di erent initial domain theory 
section investigate include initial population networks variety networks obtained directly domain theory 
currently regent creates initial population perturbing kbann network 
include networks obtained domain theory rst randomly pick number hidden nodes include network randomly create hidden nodes network 
adding new nodes randomly selected output hidden node topgen methods adding new nodes refer 
adding nodes manner creates random networks node structure analogous dependencies symbolic rule bases creating networks suitable regent crossover mutation operators 
table shows test set error regent various percentages knowledge neural networks initial population 
rst row contains results initializing regent purely random initial population population contains 
second row lists results regent creates half population domain theory half randomly 
row contains results seeding entire population domain theory 
results suggest including initial population networks created domain theory increases regent test set error domains 
occurs randomly generated networks correct opitz shavlik rbs splice junction promoters knn knn knn table test set error considering networks 
row gives initial population 
pairwise tailed tests indicate initializing regent di ers con dence level domains di erence runs signi cant 
spring original knn quickly replace random networks 
diversity population su ers compared methods start population 
assuming domain theory malicious better seed entire population kbann network 
domain theory malicious contain information promotes spurious correlations data reasonable randomly create population 
running regent domain theory allows investigate utility theory 
results interesting ga point view 
forrest mitchell showed gas perform poorly complex problems basic building blocks non trivial nd get split crossover 
seeding initial population domain theory regent help de ne basic building blocks problems 
value regent mutation typically gas mutation secondary operation sparingly goldberg regent mutation directed approach heuristically adds nodes ective manner uses topgen 
reasonable hypothesize apply mutation operator frequently traditionally done gas 
results section test hypothesis 
presents test set error regent varying percentages mutation versus crossover creating new networks step table 
graph plots curves mutation regent uses crossover mutation mutation mutation 
performing mutations tests value solely crossover mutation tests cacy mutation operator 
note mutation just topgen di erent search strategy keeping open list heuristic search population generated members population improved proportional tness 
curves mutation test synergy operators 
performing mutation test set error connectionist theory refinement ribosome binding sites mutation mutation mutation mutation splice junctions promoters networks considered error rates regent di erent fractions mutation versus crossover considering networks 
arguably due inherent similarity algorithms limited number runs due computational complexity results signi cant atthe con dence level 
opitz shavlik closer traditional ga viewpoint mutation secondary operation mutation means operations equally valuable 
previous experiments section mutation crossover 
di erences statistically signi cant results suggest synergy exists operations 
middle portion promoter domain results show qualitatively operations time better operation 
fact equally mixing mutation crossover operator better curves domains regent considered networks 
result particularly pronounced splice junction domain 
value regent crossover regent tries cross rules networks just blindly crossing nodes 
probabilistically dividing nodes network sets nodes belonging rule tend belong set 
section test cacy regent crossover comparing variant randomly assigns nodes sets table 
table contains results test networks considered 
rst row regent random crossover regent randomly breaks hidden nodes sets second row regent assigns nodes sets table 
cases regent creates half networks mutation operator half crossover operator 
di erences statistically signi cant results suggest keeping rule structure networks intact crossover important basic building blocks networks rules get split crossover studies shown importance keeping intact basic building blocks crossover forrest mitchell goldberg 
promoters splice junction rbs regent random crossover regent table test set error runs regent randomly crossing nodes networks crossing rules network de ned equation 
runs considered networks half crossover half mutation 
results signi cant con dence level slight di erence learning algorithms long run times limited runs fold cross validation 

discussion towell showed kbann generalized better machine learning algorithms promoter splice junction domains rbs dataset exist 
connectionist theory refinement despite success regent able ectively available computer cycles signi cantly improve generalization kbann previous improvement topgen algorithm 
regent reduces kbann test set error rbs domain splice junction domain promoter domain reduces topgen test set error rbs domain splice junction domain promoter domain 
regent ability available computing time aided inherently parallel train networks simultaneously 
results show regent genetic operators complement 
crossover operator considers large variety topologies probabilistically combining rules contained successful 
mutation hand smaller directed improvements members population time adding diversity population adding new rules population 
equal operators allows wide variety topologies considered allowing incremental improvements members population 
regent searches candidate networks important able recognize networks generalize best 
mind rst planned extension regent develop test di erent network evaluation functions 
currently validation set validation sets drawbacks 
keeping aside validation set decreases number training instances available network 
second performance validation set noisy approximator true error mackay weigend huberman rumelhart :10.1.1.27.5965
increase number networks searched regent may start selecting networks validation set 
fact explains occasional upward trend test set error topgen regent 
avoid problem tting data common regression trick function includes smoothness term error term 
best function smoothest function ts data 
neural networks add estimated error smoothness component measure complexity network 
complexity network simply estimated counting number possible parameters tends signi cant duplication function weight network especially early training process weigend 
techniques try take account ective size network generalized prediction error moody bayesian methods mackay :10.1.1.27.5965
quinlan cameron jones propose adding additional term accuracy smoothness term takes account length time spent searching 
coin term oversearching describe phenomenon extensive searching causes lower predictive accuracy 
claim oversearching orthogonal tting complexity methods prevent oversearching 
increase number networks consider search may start oversearching plan investigate adding oversearching penalty 
indicated earlier regent lamarckian passes local optimizations individuals trained weights network spring 
viable alternative called baldwin ect ackley littman baldwin belew mitchell hinton nowlan local search change tness individual backpropagation learning case pass changes spring form opitz shavlik evolution darwinian nature 
learned explicitly coded genetic material individuals best able learn spring learning impacts evolution 
fact form evolution outperform forms lamarckian evolution employ local search strategy whitley gordon mathias 
investigate utility ofthe baldwin ect regent 
case cross trained networks cross initial weight settings backpropagation learning took place 
times multiple con icting theories domain 
investigate ways domain theories seed initial population 
results section show including randomly generated networks degrades generalization performance seeding population multiple approximately correct theories degrade generalization assuming networks initial correctness 
regent able naturally combine parts multiple theories 
domain theory di erent equivalent ways represent theory set propositional rules 
representation leads di erent network topology network starts theory topologies may conducive neural re nement 

related regent mainly di ers previous anytime theory re nement system continually searches non hillclimbing manner improvements domain theory 
summary unique provides connectionist approach attempts ectively utilize available background knowledge available computer cycles generate best concept possible 
broken rest section parts connectionist theory re nement algorithms purely symbolic theory re nement algorithms algorithms nd appropriate domain speci neural network topology optimization algorithms wrapped induction algorithms 
connectionist theory re nement techniques discussion connectionist theory re nement systems 
systems developed re ne types rule bases 
instance number systems proposed revising certainty factor rule bases fu mahoney mooney nite state automata maclin shavlik giles push automata das giles sun fuzzy logic rules watanabe kawamura mathematical equations hofmann tresp scott 
systems kbann rst translating domain knowledge neural network modifying weights resulting network 
attempts describe dynamically adjust resulting network topology training regent 
topgen regent fletcher approach adds nodes kbann network 
system constructs single layer nodes fully connected input output nodes side kbann network 
generate new hidden nodes variant baum lang constructive connectionist theory refinement algorithm 
baum lang algorithm rst divides feature space hyperplanes 
nd hyperplane randomly selecting points di erent classes localizing suitable split points 
baum lang repeat process generate xed number hyperplanes 
fletcher map baum lang hyperplanes new hidden node de ning weights input layer hidden node 
fletcher algorithm change weights kbann portion network modi cations initial rule base solely left constructed hidden nodes 
system take advantage kbann strength removing unwanted antecedents rules original rule base 
fact topgen compared favorably similar technique added nodes side kbann opitz shavlik regent outperformed topgen article experiments 
mahoney mooney designed domain theories containing probabilistic rules 
connectionist theory re nement systems rst translates domain theory neural network re nes weights network modi ed backpropagation algorithm 
regent able dynamically re ne topology network 
algorithm frean add new nodes network 
aside designed probabilistic rules di ers regent adds nodes intention completely learning training set generalizing 
training set learned regent continually searches topology space looking network minimizes scoring function error 
initially creates links speci ed domain theory explicitly adds links id quinlan information gain metric 
regent hand fully connect consecutive layers networks allowing rule possibility adding antecedents training 
algorithm towell shavlik extension kbann uses domain theory help train kbann network 
kbann ective dropping antecedents adding tries nd potentially useful inputs features mentioned domain theory 
backing errors lowest level domain theory computing correlations features 
increases weight links potentially useful input features correlations 
mainly di ers regent re ne topology kbann network 
addresses kbann limitation ectively adding antecedents unable introduce new rules constructively induce new antecedents 
su er impoverished domain theories 
notice improvement training regent train network considers search done 
opitz shavlik variant regent learning algorithm generating neural network ensemble 
neural network ensemble successful technique outputs set separately trained neural networks combined form uni ed prediction drucker cortes jackel lecun vapnik hansen salamon perrone 
regent considers networks select subset nal population networks ensemble minimal extra cost 
previous shown ideal accurate errors di erent parts input space hansen salamon opitz shavlik krogh vedelsby 
result opitz shavlik changed scoring function regent network accurate disagreed members population possible 
addition algorithm actively tries generate candidates emphasizing current population erroneous examples backpropagation training 
result alterations able create diversity population networks able ectively exploit knowledge domain theory 
opitz shavlik show able generate signi cantly better ensemble domain theory running bene theory simply combining regent nal population networks 
actively searching highly diverse population aid searching single best network 
fact single best network produced signi cantly worse regent single best network domains 
purely symbolic theory re nement techniques additional related regent includes purely symbolic theory re nement systems modify domain theory directly initial form 
systems pazzani kibler forte richards mooney rst order theory re nement systems revise predicate logic theories 
drawback systems currently generalize connectionist approaches real world problems dna promoter task cohen 
genetic rst order logic multimodal concept learners greene smith 
giordana saitta showed integrate system giordana saitta neri saitta deductive engine ml smart bergadano giordana help re ne incomplete inconsistent domain theory 
version works rst automated theorem prover recognize unresolved literals proof uses ga induce corrections literals 
regent hand genetic algorithms neural learning re ne domain theory time 
dogma proposed ga learner background knowledge learn description language 
current restrictions force representation language domain theory propositional rules 
dogma converts set background rules handle intermediate individual bitstrings building blocks higher level concept 
dogma focus theory re nement builds completely new theory substructures background knowledge 
term approach theory suggested theory guided 
systems including proposed re ning propositional rule bases 
early approaches handle improvements overly speci theories specializations overly general theories flann dietterich 
systems ginsberg ourston mooney ptr feldman donoho rendell able handle types re nements 
discuss system representative propositional systems 
connectionist theory refinement theory revision operators removing antecedents rule adding antecedents rule removing rules rule base inventing new rules 
uses operators revisions domain theory correctly classify previously misclassi ed training examples unde ning correctly classi ed examples 
uses inductive learning algorithms invent new rules currently uses id quinlan induction component 
regent mutation operator add nodes manner analogous symbolic system adds antecedents rules underlying learning algorithm connectionist 
towell showed kbann outperformed promoter task regent outperformed kbann article 
kbann power domain largely attributed ability ne grain re nements domain theory towell 
di culty domain ba es mooney extension called able learn rules rules true antecedents true 
improvement generated concept closely matches kbann generalization performance 
want minimize changes theory want expense accuracy donoho rendell demonstrate existing theory re nement systems su er able small local changes domain theory 
accurate theory signi cantly far structure initial theory systems forced trapped local maximum similar initial theory forced drop entire rules replace new rules inductively created purely scratch 
regent su er translates theory restricting representation neural networks donoho rendell 
regent able recon gure structure domain genetic algorithms 
authors reported results varying subsets splice junction domain donoho rendell mahoney neri saitta towell shavlik 
authors di erent training set sizes worthwhile qualitatively discuss 
towell shavlik compared kbann numerous machine learning algorithms learning algorithm training set examples kbann generalization ability compared favorably algorithms splice domain regent turn compared favorably kbann article 
donoho rendell showed purely symbolic approach converged performance kbann examples 
mahoney showed training set sizes examples algorithm generalized better kbann domain results look similar regent 
neri saitta showed generalization ability ga compares favorably purely symbolic non ga techniques slightly di erent training set sizes article regent compares results reported 
finding appropriate network topologies third area related covers techniques attempt nd topology dynamically re ning network topology training 
opitz shavlik studies shown generalization ability neural network depends topology network baum haussler tishby levin solla 
trying nd appropriate topology approach construct modify topology incremental fashion 
network shrinking algorithms start parameters remove nodes weights training hassibi stork le cun denker solla mozer smolensky 
network growing algorithms hand start parameters add nodes weights training fahlman lebiere frean 
obvious di erence regent algorithms regent uses domain knowledge symbolic rule re nement techniques help determine network topology 
algorithms restructure network solely training set error regent minimized validation set error 
incrementally nding appropriate topology mount richer search hillclimbing space topologies 
common approach combine genetic algorithms neural networks regent 
genetic algorithms applied neural networks di erent ways optimize connection weights xed topology optimize topology network 
techniques solely genetic algorithms optimize weights montana davis whitley hanson performed competitively gradient training algorithms problem genetic algorithms ine ciency ne tuned local search scalability methods question yao 
kitano presents method combines genetic algorithms backpropagation 
genetic algorithm determine starting weights network re ned backpropagation 
regent di ers kitano method domain theory help determine network starting weights genetically search appropriate network topologies 
methods genetic algorithms optimize network topology similar regent backpropagation train network weights 
methods directly encode link network miller todd hegde furst mann joost werner 
methods relatively straightforward implement ne tuning small networks miller scale require large matrices represent links large networks yao 
techniques dodd harp samad guha kitano encode important features network number hidden layers number hidden nodes layer indirect encoding schemes evolve di erent sets parameters network topology shown scalability yao 
techniques koza rice evolve architecture connection weights time combination greatly increases search space 
regent mainly di ers genetic algorithm training methods designed knowledge neural networks 
regent uses domain speci knowledge symbolic rule re nement techniques aid determining network topology initial weight setting 
regent di ers explicitly encode networks spirit lamarkian evolution passes trained network weights connectionist theory refinement spring 
nal di erence algorithms restructure network solely training set error regent minimizes validation set error 
wrapping optimization learning related discussion brief overview methods combine global local optimization strategies 
local search algorithms iteratively improve estimate minimum searching local neighborhood current solution local minima guaranteed global minima 
inductive learning methods equated local optimization techniques rumelhart 
global optimization methods gas hand perform sophisticated search multiple local minima nding regions search space nearoptimal solutions usually re ning solution close near optimal solution local optimization strategies hart 
research shown desirable global local search strategy hart 
hybrid gas combine local search traditional ga focus hybrid ga algorithms section tiered search strategy employed researchers kohavi john provost buchanan scha er 
gas combined local search methods bala huang vafaie dejong wechsler belew hinton nowlan turney 
neural networks common choice local search strategy hybrid ga systems discussed ga neural network hybrids section 
common forms hybrid gas lamarckian evolution darwinian evolution baldwin ect 
lamarckian evolution encodes local improvements directly genetic material darwinian evolution leaves genetic material unchanged learning 
discussed section authors lamarckian local search techniques shown numerous cases lamarckian evolution outperforms non lamarckian local search belew mcinerney schraudolph hart hu gutierrez 

ideal inductive learning algorithm able exploit available resources extensive computing power domain speci knowledge improve ability generalize 
kbann towell shavlik shown ective translating domain theory neural network kbann su ers alter topology 
topgen opitz shavlik improved kbann algorithm available computer power search ective places add nodes kbann network show empirically topgen su ers restricting search expansions kbann network unable improve performance searching topologies 
topgen unable exploit available computing power increase correctness induced concept 
new algorithm regent uses specialized genetic algorithm broaden types topologies considered topgen search 
experiments indicate regent able signi cantly increase generalization topgen new opitz shavlik algorithm successful overcoming topgen limitation searching small portion space possible network topologies 
doing regent able generate solution quickly kbann improve solution searches concept space 
regent takes step true anytime theory re nement system able ective problem speci knowledge available computing cycles 
supported ce naval research national science foundation iri 
richard maclin richard sutton anonymous reviewers helpful comments 
extended version published machine learning proceedings eleventh international conference pp 
new brunswick nj morgan kaufmann 
david opitz completed portion graduate student university wisconsin professor university minnesota 
ackley 

connectionist machine genetic hillclimbing 
kluwer norwell ma 
ackley littman 

interactions learning evolution 
langton taylor farmer rasmussen 
eds arti cial life ii pp 
redwood city ca 
addison wesley 
ackley littman 

case lamarckian evolution 
langton 
ed arti cial life iii pp 
redwood city ca 
addison wesley 
ba es mooney 

symbolic revision theories rules 
proceedings thirteenth international joint conference onarti cial intelligence pp 
chambery france 
morgan kaufmann 
bala huang vafaie dejong wechsler 

hybrid learning genetic algorithms decision trees pattern classi cation 
proceedings fourteenth international joint conference onarti cial intelligence pp 
montreal canada 
morgan kaufmann 
baldwin 

physical social 
american naturalist 
baum haussler 

size net gives valid generalization 
neural computation 
baum lang 

constructing hidden units examples queries 
lippmann moody touretzky 
eds advances neural information processing systems vol 
pp 
san mateo ca 
morgan kaufmann 
connectionist theory refinement belew 

evolution learning culture computational metaphors adaptive search 
complex systems 
belew mcinerney schraudolph 

evolving networks genetic algorithm connectionist learning 
langton taylor farmer rasmussen 
eds arti cial life ii pp 
redwood city ca 
addison wesley 
belew mitchell 

adaptive individuals evolving populations models algorithms 
addison wesley massachusetts 


re nement approximate reasoning controllers reinforcement learning 
proceedings eighth international machine learning workshop pp 
evanston il 
morgan kaufmann 
bergadano giordana 

deduction top inductive learning 
proceedings sixth international workshop machine learning pp 
ithaca ny 
morgan kaufmann 


learning radial basis function networks line 
proceedings thirteenth international conference machine learning pp 
bari italy 
morgan kaufmann 
clocksin mellish 

programming prolog 
springer verlag new york 
cohen 

compiling prior knowledge explicit bias 
proceedings ninth international conference machine learning pp 
aberdeen scotland 
morgan kaufmann 


finding new rules incomplete theories explicit biases induction contextual information 
proceedings sixth international workshop machine learning pp 
ithaca ny 
morgan kaufmann 
das giles sun 

prior knowledge learn context free languages 
hanson cowan giles 
eds advances neural information processing systems vol 
pp 
san mateo ca 
morgan kaufmann 
dean boddy 

analysis time dependent planning 
proceedings seventh national conference arti cial intelligence pp 
st paul mn 
morgan kaufmann 
dejong 

analysis behavior class genetic adaptive systems 
ph thesis university michigan ann arbor mi 
dodd 

optimization network structure genetic techniques 
proceedings ieee international joint conference networks vol 
iii pp 
paris 
ieee press 
opitz shavlik donoho rendell 

restructuring domain theories constructive induction approach 
journal arti cial intelligence research 
drucker cortes jackel lecun vapnik 

boosting machine learning algorithms 
proceedings eleventh international conference machine learning pp 
new brunswick nj 
morgan kaufmann 
fahlman lebiere 

cascade correlation learning architecture 
touretzky 
ed advances neural information processing systems vol 
pp 
san mateo ca 
morgan kaufmann 
farmer belin 

arti cial life coming evolution 
langton taylor farmer rasmussen 
eds arti cial life ii pp 
redwood city ca 
addison wesley 
flann dietterich 

study explanation methods inductive learning 
machine learning 
fletcher 

combining prior symbolic knowledge constructive neural network learning 
connection science 
forrest mitchell 

problem hard genetic algorithm 
anomalous results explanation 
machine learning 
frean 

algorithm method constructing training feedforward neural networks 
neural computation 
fu 

integration neural heuristics knowledge inference 
connection science 
ginsberg 

theory reduction theory revision 
proceedings eighth national conference arti cial intelligence pp 
boston ma 
aaai mit press 
giordana saitta 

integrated system relations genetic algorithms 
proceedings second international workshop multistrategy learning pp 
ferry wv 
giordana saitta 

learning disjunctive concepts means genetic algorithms 
proceedings eleventh international conference machine learning pp 
new brunswick nj 
morgan kaufmann 
goldberg 

genetic algorithms search optimization machine learning 
addison wesley reading ma 
greene smith 

competition induction decision models examples 
machine learning 
connectionist theory refinement grefenstette ramsey 

approach anytime learning 
proceedings ninth international conference machine learning pp 
aberdeen scotland 
morgan kaufmann 
hansen salamon 

neural network ensembles 
ieee transactions pattern analysis machine intelligence 
harp samad guha 

designing application speci neural networks genetic algorithm 
touretzky 
ed advances neural information processing systems vol 
pp 
san mateo ca 
morgan kaufmann 
hart 

adaptive global optimization local search 
ph thesis university california san diego 
hassibi stork 

second order derivatives network pruning optimal brain surgeon 
hanson cowan giles 
eds advances neural information processing systems vol 
pp 
san mateo ca 
morgan kaufmann 


background knowledge ga concept learning 
proceedings thirteenth international conference machine learning pp 
bari italy 
morgan kaufmann 
hinton nowlan 

learning guide evolution 
complex systems 
holder 

maintaining utility learned knowledge model control 
ph thesis computer science department university illinois urbana champaign 
holland 

adaptation natural arti cial systems 
university michigan press ann arbor mi 


knowledge intensive ga supervised learning 
machine learning 
hu gutierrez 

intelligent con guration search techniques outperform random search large molecules 
international journal quantum chemistry 
kibler langley 

machine learning experimental science 
proceedings third european working session learning pp 
edinburgh uk 
kitano 

designing neural networks genetic algorithms graph generation system 
complex systems 
kitano 

empirical studies speed convergence neural network training genetic algorithms 
proceedings eighth national conference arti cial intelligence pp 
boston ma 
aaai mit press 
opitz shavlik kohavi john 

wrappers feature subset selection 
arti cial intelligence 
feldman 

bias driven revision logical domain theories 
journal arti cial intelligence research 
koza 

genetic programming 
mit press cambridge ma 
koza rice 

genetic generation weights architectures neural network 
international joint conference neural networks vol 
pp 
seattle wa 
ieee press 
krogh vedelsby 

neural network ensembles cross validation active learning 
tesauro touretzky leen 
eds advances neural information processing systems vol 
pp 
cambridge ma 
mit press 


back propagation learning expert networks 
ieee transactions neural networks 
le cun denker solla 

optimal brain damage 
touretzky 
ed advances neural information processing systems vol 
pp 
san mateo ca 
morgan kaufmann 
litzkow livny mutka 

condor hunter idle workstations 
proceedings eighth international conference distributed computing systems pp 
san jose ca 
computer society press 
mackay 

practical bayesian framework backpropagation networks 
neural computation 
maclin shavlik 

knowledge neural networks improve algorithms re ning chou algorithm protein folding 
machine learning 
mahoney 

combining symbolic connectionist learning methods re ne certainty factor rule bases 
ph thesis university austin tx 
mahoney mooney 

combining connectionist symbolic learning re ne certainty factor rule bases 
connection science 
mahoney mooney 

comparing methods re ning certainty factor 
proceedings eleventh international conference pp 
new brunswick nj 
morgan kaufmann 
watanabe kawamura 

system fuzzy inference structured neural network 
proceedings international conference fuzzy logic neural networks pp 
japan 
michalski 

theory methodology inductive learning 
arti cial intelligence 
connectionist theory refinement miller todd hegde 

designing neural networks genetic algorithms 
proceedings third international conference genetic algorithms pp 
arlington va morgan kaufmann 
mitchell 

genetic algorithms 
mit press cambridge ma 
mitchell 

generalization search 
arti cial intelligence 
montana davis 

training feedforward networks genetic algorithms 
proceedings eleventh international joint conference arti cial intelligence pp 
detroit mi 
morgan kaufmann 
moody 

ective number parameters analysis generalization regularization nonlinear learning systems 
moody hanson lippmann 
eds advances neural information processing systems vol 
pp 
san mateo ca 
morgan kaufmann 
mozer smolensky 

relevance reduce network size automatically 
connection science 
neri saitta 

exploring power genetic search learning symbolic classi ers 
ieee transactions pattern machine intelligence 
furst 

distributed genetic algorithm neural network design training 
complex systems 
giles 

training second order recurrent neural networks hints 
proceedings ninth international conference machine learning pp 
aberdeen scotland 
morgan kaufmann 
opitz shavlik 

heuristically expanding knowledge neural networks 
proceedings thirteenth international joint conference arti cial intelligence pp 
chambery france 
morgan kaufmann 
opitz shavlik 

dynamically adding symbolically meaningful nodes knowledge neural networks 
knowledge systems 
opitz shavlik 

actively searching ective neural network ensemble 
connection science 
ourston mooney 
theory re nement combining analytical empirical methods 
arti cial intelligence 
pazzani kibler 

utility inductive learning 
machine learning 
perrone 

improving regression estimation averaging methods variance reduction extension general convex measure optimization 
ph thesis brown university providence ri 
opitz shavlik provost buchanan 

inductive policy pragmatics bias selection 
machine learning 
quinlan 

induction decision trees 
machine learning 
quinlan cameron jones 

lookahead pathology decision tree induction 
proceedings fourteenth international joint conference onarti cial intelligence pp 
montreal canada 
morgan kaufmann 
richards mooney 

automated re nement rst order horn clause domain theories 
machine learning 
hofmann tresp 

neural control rolling mills incorporating domain theories overcome data de ciency 
moody hanson lippmann 
eds advances neural information processing systems vol 
pp 
san mateo ca 
morgan kaufmann 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart mcclelland 
eds parallel distributed processing explorations microstructure cognition 
volume foundations pp 

mit press cambridge ma 
scha er 

selecting classi cation method cross validation 
machine learning 
mann joost werner 

synthesis performance analysis multilayer neural network architectures 
tech 
rep university koblenz institute physics 
scott shavlik ray 

re ning pid controllers neural networks 
neural computation 
tishby levin solla 

consistent inference probabilities layered networks predictions generalization 
international joint conference neural networks pp 
washington ieee press 
towell 

symbolic knowledge neural networks insertion re nement extraction 
ph thesis computer sciences department university wisconsin madison wi 
towell shavlik 

symbolic learning neural networks 
proceedings tenth national conference arti cial intelligence pp 
san jose ca 
aaai mit press 
towell shavlik 

knowledge arti cial neural networks 
arti cial intelligence 
turney 

cost sensitive classi cation empirical evaluation hybrid genetic decision tree induction algorithm 
journal arti cial intelligence research 
connectionist theory refinement waterman 

guide expert systems 
addison wesley reading ma 
watrous towell glassman 

synthesize optimize analyze repeat soar application neural network tools ecg patient monitoring 
proceedings symposium nonlinear theory applications pp 
honolulu hawaii 
watson hopkins roberts weiner 

molecular biology gene fourth edition 
benjamin cummings menlo park ca 
weigend 

tting ective number hidden units 
proceedings connectionist models summer school pp 
boulder lawrence erlbaum associates 
weigend huberman rumelhart 

predicting connectionist approach 
international journal neural systems 
whitley gordon mathias 

lamarckian evolution baldwin ect function optimization 
davidor schwefel manner 
eds parallel problem solving nature ppsn iii pp 

springer verlag 
whitley hanson 

optimizing neural networks faster accurate genetic search 
proceedings third international conference genetic algorithms pp 
arlington va morgan kaufmann 
yao 

evolutionary arti cial neural networks 
international journal neural systems 

