max margin markov networks ben taskar carlos guestrin daphne koller guestrin koller cs stanford edu stanford university typical classification tasks seek function assigns label single object 
kernel approaches support vector machines svms maximize margin confidence classifier method choice tasks 
popularity stems ability high dimensional feature spaces strong theoretical guarantees 
real world tasks involve sequential spatial structured data multiple labels assigned 
existing kernel methods ignore structure problem assigning labels independently object losing useful information 
conversely probabilistic graphical models markov networks represent correlations labels exploiting problem structure handle high dimensional feature spaces lack strong theoretical generalization guarantees 
new framework combines advantages approaches maximum margin markov networks incorporate kernels efficiently deal high dimensional features ability capture correlations structured data 
efficient algorithm learning networks compact quadratic program formulation 
provide new theoretical bound generalization structured domains 
experiments task handwritten character recognition collective hypertext classification demonstrate significant gains previous approaches 
supervised classification goal classify instances set discrete categories 
support vector machines svms demonstrated impressive successes broad range tasks including document categorization character recognition image classification 
svms owe great part success ability kernels allowing classifier exploit high dimensional possibly infinite dimensional feature space 
addition empirical success svms appealing due existence strong generalization guarantees derived margin maximizing properties learning algorithm 
supervised learning tasks exhibit richer structure simple categorization instances small number classes 
cases need label set inter related instances 
example optical character recognition ocr part speech tagging involve labeling entire sequence elements number classes image segmentation involves labeling pixels image collective webpage classification involves labeling entire set interlinked webpages 
cases want label instance news article multiple non exclusive labels 
cases need assign multiple labels simultaneously leading classification problem exponentially large set joint labels 
common solution treat problems set independent classification tasks dealing instance isolation 
known approach fails exploit significant amounts correlation information :10.1.1.120.9821
alternative approach offered probabilistic framework specifically probabilistic graphical models 
case define learn joint probabilistic model set label variables 
example learn hidden markov model conditional random field crf labels features sequence probabilistic inference algorithm viterbi algorithm classify instances collectively finding joint assignment labels simultaneously :10.1.1.120.9821
approach advantage exploiting correlations different labels resulting significant improvements accuracy approaches classify instances independently 
graphical models allows problem structure exploited effectively 
unfortunately probabilistic graphical models trained discriminatively usually achieve level generalization accuracy svms especially kernel features 
associated generalization bounds comparable margin classifiers 
clearly frameworks kernel probabilistic classifiers offer complementary strengths weaknesses 
maximum margin markov networks unify frameworks combine advantages 
approach defines log linear markov network set label variables labels letters ocr problem network allows represent correlations label variables 
define margin optimization problem parameters model 
markov networks triangulated tractably resulting quadratic program qp equivalent polynomial size formulation linear sequences allows effective solution 
contrast previous margin formulations sequence labeling require exponential number constraints 
non triangulated networks provide approximate reformulation relaxation belief propagation algorithms 
importantly resulting qp supports kernel trick svms allowing probabilistic graphical models inherit important benefits kernels 
show generalization bound margin classifiers 
previous results bound grows logarithmically linearly number label variables 
experimental results character recognition hypertext classification demonstrate dramatic improvements accuracy kernel instance instance classification probabilistic models 
structure classification problems supervised classification task learn function set instances drawn fixed distribution dx classification function typically selected parametric family common choice linear family real valued basis functions fj ir hypothesis hw defined set coefficients wj hw arg max arg max features basis functions 
common classification setting single label classification takes 
yk 
consider general setting multi label classification 
yl yi 
yk 
ocr task example yi character full word 
webpage collective classification task yi webpage label joint label entire website 
cases number possible assignments exponential number labels representing basis functions fj computing maximization arg maxy infeasible 
alternative approach framework probabilistic graphical models 
case model defines directly indirectly conditional distribution 
select label arg maxy 
advantage probabilistic framework exploit sparseness correlations labels yi 
example ocr task markov model yi conditionally independent rest labels yi yi 
encode structure markov network 
purely simplicity presentation focus case pairwise interactions labels 
emphasize results extend easily general case 
pairwise markov network defined graph edge associated potential function ij yi yj 
network encodes joint conditional probability distribution ij yi yj 
networks exploit interaction structure parameterize classifier compactly 
cases tree structured networks effective dynamic programming algorithms viterbi algorithm find highest probability label approximate inference algorithms exploit structure 
markov network distribution simply log linear model pairwise potential ij yi yj representing log space sum basis functions yi yj 
parameterize model set pairwise basis functions yi yj assume simplicity notation edges graph denote type interaction define set features fk fk yi yj 
network potentials ij yi yj exp yi yj exp yi yj parameters log linear model trained fit data typically maximizing likelihood conditional likelihood 
presents algorithm selecting maximize margin gaining advantages svms 
margin structured classification single label binary classification problem support vector machines svms provide effective method learning maximum margin decision boundary 
multi class classification crammer singer provide natural extension framework maximizing margin subject constraints maximize fx fx 
constraints formulation ensure arg maxy 
maximizing magnifies difference value true label best runner increasing confidence classification 
structured problems predicting multiple labels loss function usually simple loss arg maxy fx label loss proportion incorrect labels predicted 
order extend margin framework multi label setting generalize notion margin take account number labels misclassified 
particular margin scale linearly number wrong labels tx maximize fx tx tx tx yi tx yi yi 
standard transformation eliminate get quadratic program qp minimize fx tx 
unfortunately data separable hyperplane defined space set features 
cases need introduce slack variables allow constraints violated 
complete form optimization problem equivalent dual problem primal formulation min dual formulation max fx tx tx fx 
note add extra dual variable effect solution 
exploiting structure networks unfortunately number constraints primal qp number variables dual qp exponential number labels section equivalent polynomially sized formulation 
main insight variables dual formulation interpreted density function conditional 
dual objective function expectations tx fx respect 
tx tx yi fx fx yi yj sums functions nodes edges need node edge marginals measure compute expectations 
define marginal dual variables follows yi yj yi yj yi yj yi yi yi yi yj denotes full assignment consistent partial assignment yi yj 
reformulate entire qp terms dual variables 
consider example term objective function tx tx yi tx yi yi tx yi 
decomposition second term objective uses edge marginals yi yj 
order produce equivalent qp ensure dual variables yi yj yi marginals resulting legal density belong marginal polytope 
particular enforce consistency pairwise singleton marginals overlapping pairwise marginals yi yj yj yj 
yi markov network basis functions forest singly connected constraints equivalent requirement variables arise density 
factored dual qp equivalent original dual qp max yi tx yi yi yj yr ys fx yi yj yr ys yi yj yr ys yi yj yj yi yi yj 
similarly original primal factored follows min fx yi yj ij mx yj mx yi ij mx yi tx yi ij 
solution factored dual gives yi yj yi yj fx yi yj 
theorem edges form forest set weights optimal qp optimal factored qp 
underlying markov net forest constraints sufficient enforce fact marginal polytope 
address problem triangulating graph introducing new lp variables span larger subsets yi example graph cycle triangulate graph adding arc introducing variables joint instantiations cliques 
new variables linear equalities constrain original variables consistent density 
variables appear constraints add new basis functions change objective function 
number constraints introduced exponential number variables new cliques 
classification problems sequences graphs low tree width extended qp solved efficiently 
unfortunately triangulation feasible highly connected problems 
solve qp defined graph loops 
procedure enforces local consistency marginals optimizes objective relaxation marginal polytope 
way approximation analogous approximate belief propagation bp algorithm inference graphical models 
fact bp additional approximation relaxed marginal polytope approximate objective bethe free energy 
approximate qp offer theoretical guarantee theorem solutions accurate practice demonstrate 
svms factored dual formulation uses dot products basis functions 
allows kernel define large infinite set features 
particular define basis functions fx yi yj yi yj ij product selector function yi yj possibly infinite feature vector ij 
example ocr task yi yj indicator function class adjacent characters ij rbf kernel images characters 
operation fx yi yj yr ys objective function factored dual qp yi yj yr ys ij rs kernel function feature 
complex functions dot product required compute executed efficiently 
smo learning networks number variables constraints factored dual polynomial size data number coefficients quadratic term kernel matrix objective quadratic number examples edges network 
unfortunately matrix large standard qp solvers 
coordinate descent method analogous sequential minimal optimization smo svms 
considering original dual problem 
smo approach solves qp analytically optimizing variable subproblems 
recall take variables move weight keeping values variables fixed 
precisely optimize 
clearly perform optimization terms original dual exponentially large 
fortunately perform precisely optimization terms marginal dual variables 

consider dual variable yi yj 
easy see change effect yi yj yi yj yi yj yi yj yi yj 
solve variable quadratic subproblem analytically update appropriate variables 
inference network test optimality current solution kkt conditions violations optimality heuristic select pair omit details lack space 
generalization bound section show generalization bound task multi label classification allows relate error rate training set generalization error 
shall see bound significantly stronger previous bounds problem 
goal multi label classification maximize number correctly classified labels 
appropriate error function average label loss tx arg maxy fx 
generalization bounds margin classifiers relate generalization error margin classifier 
sec 
define notion label margin grows number mistakes correct assignment best runner 
define margin label loss sup fx tx tx arg maxy 
loss function measures worst label loss classifier perturbed fx margin label 
prove generalization accuracy classifier bounded expected margin label loss training data plus term grows inversely margin intuitively term corresponds bias margin decreases complexity hypothesis class considering label margin ball fx selecting worst classifier ball 
shrinks hypothesis class complex term smaller cost increasing second term intuitively corresponds variance 
result provides bound generalization error trades effective complexity hypothesis space training error 
theorem edge features bounded norm max yi yj fx yi yj family hyperplanes parameterized exists constant label margin samples label loss bounded exl esl ln ln ln ln ln probability maxi maximum edge degree network number classes label number labels 
proof see appendix 
proof uses covering number argument analogous previous results svms 
propose novel method covering structured problems constructing cover loss function cover individual edge basis function differences fx yi yj 
new type cover polynomial number edges yielding significant improvements bound 
specifically bound logarithmic dependence number labels ln depends norm basis functions 
significant gain previous result collins linear dependence number labels depends joint norm features sequence normalized separately ineffective practice 
note example ocr number instances constant times length word bound independent number labels result open problem margin sequence classification 
experiments evaluate approach different tasks sequence model handwriting recognition arbitrary topology markov network hypertext classification 
test error average character linear quadratic cubic log reg crf cor tex wis ave example words ocr data set ocr average character test error logistic regression crfs multiclass svms ns linear quadratic cubic kernels hypertext test error multiclass svms ns school average 
test error pages school rmn handwriting recognition 
selected subset handwritten words average length characters human subjects data set collected kassel 
word divided characters character rasterized image binary pixels 
see fig 

framework image word corresponds label individual character yi labeling complete word label yi takes values classes 

data set divided folds training testing examples 
accuracy results summarized fig 
averages folds 
implemented selection state art classification algorithms independent label approaches consider correlation neighboring characters logistic regression multi class svms described svms performance slightly lower multi class svms sequence approaches crfs proposed networks 
logistic regression crfs trained maximizing conditional likelihood labels features zero mean diagonal gaussian prior parameters standard deviation 
methods trained margin maximization 
features label yi corresponding image ith character 
sequence approaches crfs indicator basis function represent correlation yi yi 
margin methods svms able kernels quadratic cubic evaluated increase dimensionality feature space 
high dimensional feature spaces crfs feasible enormous number parameters 
fig 
shows types gains accuracy kernels margin methods achieve significant gain respective likelihood maximizing methods 
second sequences obtain significant gain accuracy 
interestingly error rate method linear features lower crfs multi class svms cubic kernels 
cubic kernels error rate lower crfs lower best previous approach 
comparison previously published results different setup larger training set comparable multiclass svms 
hypertext 
tested approach collective hypertext classification data set contains web pages different computer science departments 
page labeled course faculty student project 
experiments learn model schools test remaining school 
text content web page anchor text incoming links represented set binary attributes indicate presence different words 
baseline model simple linear multi class svm uses words predict category page 
second model relational markov network rmn taskar addition word label dependence edge potential labels pages hyper linked 
model defines markov network web site trained maximize conditional probability labels words links 
third model net features trained maximizing margin relaxed dual formulation loopy bp inference 
fig 
shows gain accuracy svms correlations labels linked web pages significant additional gain maximum margin training 
error rate ns lower lower multi class svms 
discussion discriminative framework labeling segmentation structured data sequences images approach seamlessly integrates state art kernel methods developed classification independent instances rich language graphical models exploit structure complex data 
experiments ocr task example sequence model significantly outperforms approaches incorporating high dimensional decision boundaries polynomial kernels character images capturing correlations consecutive characters 
construct models solving convex quadratic program maximizes label margin 
number variables constraints qp formulation polynomial example size sequence length address quadratic growth effective optimization procedure inspired smo 
provide theoretical guarantees average label generalization error models terms training set margin 
generalization bound significantly previous results collins suggests possibilities analyzing label generalization properties graphical models 
brevity simplified presentation graphical models pairwise markov networks 
formulation generalization bound easily extend interaction patterns involving labels higher order markov models 
believe networks significantly applicability high accuracy margin methods real world structured data 
acknowledgments 
supported onr contract darpa program 
altun tsochantaridis hofmann 
hidden markov support vector machines 
proc 
icml 
bertsekas 
nonlinear programming 
athena scientific belmont ma 
collins 
parameter estimation statistical parsing models theory practice distribution free methods 
iwpt 
cowell dawid lauritzen spiegelhalter 
probabilistic networks expert systems 
springer new york 
crammer singer 
algorithmic implementation multiclass vector machines 
journal machine learning research 
kassel 
comparison approaches line handwritten character recognition 
phd thesis mit spoken language systems group 
lafferty mccallum pereira :10.1.1.120.9821
conditional random fields probabilistic models segmenting labeling sequence data 
proc 
icml 
pearl 
probabilistic reasoning intelligent systems 
morgan kaufmann 
platt 
sparseness analytic qp speed training support vector machines 
nips 
taskar koller 
discriminative probabilistic models relational data 
proc 
uai edmonton canada 
vapnik 
nature statistical learning theory 
springer verlag new york 
yedidia freeman weiss 
generalized belief propagation 
nips 
zhang 
covering number bounds certain regularized linear function classes 
journal machine learning research 
proof theorem proof theorem uses covering number bounds zhang data dependent structural risk minimization framework 
zhang provides generalization guarantees linear binary classifiers form hw sgn 
analysis upper bounds covering number class linear functions fl norms vectors bounded 
reproduce relevant definitions theorems highlight necessary extensions structured classification 
covering number key quantity measuring function complexity 
intuitively covering number infinite class functions parameterized set weights number vectors necessary approximate values function class sample 
margin analysis generalization error uses margin achieved classifier training set approximate original function class classifier finite covering precision depends margin 
define norm covering number 
binary classification binary classification sample ir fold 
definition covering number 
ir covering function class precision metric exists data sample 
covering number sample size smallest covering inf covering 
define covering number sample size sup 
norms bounded upper bound covering number linear functions linear metric 
theorem theorem log fl log ab order classifier margin bound expected loss bounds stricter margin loss training sample measures worst loss achieved approximate covering margin 
ir loss function 
binary classification step function loss sgn fl 
theorem bounds expected loss terms margin loss sup training sample 
loss linear metric corresponding margin loss 
theorem corollary ir loss function sup margin loss metric 

decreasing sequence parameters pi sequence positive numbers pi probability data esf ln ln pi fixed denote smallest index 
shawe taylor bartlett williamson anthony 
structural risk minimization data dependent hierarchies 
ieee trans 
information theory 
structured classification extend framework bound average label loss structured classification defining appropriate loss function class metric computes average label loss provides suitable margin loss 
bound corresponding covering number building bound theorem 
longer simply fold vector 
order loss function compute average label loss convenient function class vector valued scalar valued 
define new function class fm vector minimum values fx error level tx described 
definition dth error level function dth error level function md 
md min tx fx 
definition multi error level function class multi error level function class fm fm 
md 
ml compute average label loss fm defining appropriate loss function definition average label loss average label loss ir arg min zd zd case zd define arg mind zd zd 
definitions average label loss fm arg min md md tx arg min fx desired 
note case md corresponds classifier making mistakes arg maxy fx 
need define appropriate metric turn defines margin loss structured classification 
margin grows number mistakes metric looser number mistakes room error 
definition multi error level metric multi error level metric ir ir ir vector ir max zd define corresponding margin loss new metric definition margin average label loss margin average label loss ir combining definitions get sup 
fm sup arg min zd md zd zd 
define corresponding covering number vector valued function class definition multi error level covering number 
irl covering fm 

precision metric exists data sample fm 
covering number sample size smallest covering fm inf covering fm 
fm fm 
define provide bound covering number new function class terms covering number linear function class 
lemma bound multi error level covering number fm fl maxi maximum edge degree sample number classes label 
proof show fm fl sample size construct sample size order cover edge potentials described 
note sufficient fl fl definition fm sup mn fm fl 
construction inspired proof technique collins key difference construction linear number labels edge degree lq exponential number labels 
reduction size comes covering approximates values edge potentials fx yr ys edge edge assignment yr ys opposed values entire assignments fx 
sample create lq samples fx yr ys edge assignment yr ys 
construct set vectors 
ir component corresponding sample assignment yr ys labels edge denoted yr ys 
convenience define correct label assignments 
norm covering fl require exists sample yr ys yr ys yr ys 
definition number vectors fl 
construct covering 


irl multi error level function fm 
yr ys md miny tx 
md 
ml 
note yr ys zero edges assignment yr ys correct 
assignment mistakes dq yr ys non zero label appear edges 
combining fact eq 
obtain fx dq 
conclude proof showing covering fm pick corresponding satisfies condition eq 

bound fm miny tx max min tx arg min arg min 
consider case yv fx yw reverse case analogous prove fx fx dq step follows definition yv yv yw 
step direct consequence eq 

fm 
lemma numeric bound multi error level covering number log fm edge log max ys fx yr ys proof substitute theorem lemma 
theorem multi label analog theorem defined 

decreasing sequence parameters pi sequence positive numbers pi probability data fm esf fm ln fm ln pi fixed denote smallest index 
proof similar proof zhang theorem corollary step derandomization substitute vector valued fm metric theorem follows theorem pi argument identical proof theorem zhang 
