practical prefetching techniques parallel file systems david kotz carla ellis dept math computer science dept computer science dartmouth college duke university hanover nh durham nc david kotz dartmouth edu carla cs duke edu improvements processing speed multiprocessors improvements speed disk hardware 
parallel disk subsystems proposed way close gap processor disk speeds 
previous showed prefetching caching potential deliver performance bene ts parallel le systems parallel applications 
describe experiments practical prefetching policies show prefetching implemented ciently complex parallel le access patterns 
test ability policies range architectural parameters 
computers grow powerful increasingly di cult provide su cient bandwidth keep running full speed large problems may consume immense amounts data 
disk slower processing speed trends shown improvements speed disk hardware keeping increasing raw speed processors 
widening access time gap known crisis 
problem compounded typical parallel architectures multiply processing memory capacity balancing capabilities 
promising solution crisis extend parallelism subsystem 
approach connect disks computer parallel spreading individual les disks 
parallel disks provide signi cant boost performance possibly equal degree parallelism signi cant bottlenecks subsystem requests generated applications mapped lower level operations drive available parallelism 
rst challenge designers multiprocessor le system con gure parallel disk hardware avoid bottlenecks shared busses avoid bottlenecks system software 
ective le research supported part nsf ccr ccr darpa nasa subcontract ncc 
system multiprocessor fully parallel scale additional processors disks 
second challenge extensive disk hardware bandwidth easily available application programs 
meet challenges propose highly parallel le system implementation incorporates caching prefetching means delivering bene ts parallel architecture user programs 
expect le cache useful multiprocessor le systems reason uniprocessor le systems locality le behavior 
expect multiprocessor le access patterns increased opportunities locality 
interprocess locality arise processes multi process program read le coordinated fashion reading di erent small records block 
le access pattern sequential le system read blocks cache requested making quickly available requested 
extension caching known prefetching 
prefetching access patterns course bene cial common sequential patterns 
showed prefetching signi cant potential improve read performance multiprocessor le systems 
measured potential idealistic prefetching policy provided complete le access pattern advance 
practice course prefetching policy access le access pattern advance base prefetching decisions real time view access pattern 
leads questions know prefetching potential possible design implement practical prefetching policies 
practical policy ective choosing correct blocks prefetch cient having low overhead 
question primary focus 
practical policies achieve full potential determined unrealizable full knowledge policy 
design general policies practical di erent types access patterns 
copyright ieee 
appeared conf 
parallel distributed information systems pages 
available url ftp ftp cs dartmouth edu pub cs papers kotz kotz practical ps prefetching policies implementation scale processors disks wider gap processor speed disk access speed 
answer questions testbed developed 
testbed implemented prefetching caching policies real multiprocessor simulated parallel disk evaluated prefetching policies wide variety workloads architectural parameters 
section provide background information 
section describe testbed workload experimental methods 
section de nes practical prefetching policies 
section experiments performance measures results 
section concludes 
background previous hardware parallelism involved disk striping 
technique le interleaved numerous disks accessed parallel simultaneously obtain blocks le positioning overhead block 
schemes rely single controller manage disks 
multiprocessors form parallel disk architecture notion parallel independent disks multiple conventional disk devices addressed independently attached separate processors 
les may disks multiple controllers independent access disks technique di erent disk striping 
examples architecture include concurrent file system intel ipsc multiprocessor bridge le system bbn butter multiprocessor 
caching commonly disk blocks signi cantly improve le system performance technique modern le systems 
prefetching successful uniprocessor le systems 
central idea prefetching overlap time computation issuing disk operations requested 
parallel disk hardware expect prefetching overlap obtaining larger bene ts 
file access patterns studied parallel computers studied extensively uniprocessors 
floyd studied le access patterns unix system les opened reading completely read usually sequentially 
les opened opened read write 
classic unix le system study les processed sequentially le accesses seek 
parallel le access discussed 
study actual workload related le access patterns possible storage techniques 
basic le access patterns re ected workload model 
concentrate scienti workloads characterized sequential access large les 
de spite lack ofany parallel le access study sequential access parallel le access patterns scienti applications prefetching policies assume sequential access successful 
models methods methodology experimental mix implementation simulation 
implemented le system testbed called rapid transit read ahead parallel independent disks actual multiprocessor 
multiprocessor parallel disks simulated 
unfortunately parallel programs parallel access real workload 
forced synthetic workload 
synthetic workload captures nuances real workloads sequentiality regularity process interactions 
consists real parallel programs generate le requests may incur synchronization delays 
testbed executes synthetic application measuring elapsed real time signi cant statistics 
implementation policies real parallel processor combined real time execution measurement allows directly include ects memory contention synchronization overhead inter process dependencies overhead caused workload various management policies 
method allows evaluate practical prefetching policies implemented 
models assumptions architecture architecture research orts multiple instruction stream multiple data stream mimd shared memory multiprocessor 
subset problems proposed solutions implementation may apply message passing architectures 
represent disk subsystem parallel independent disks 
mapping les disks blocks le allocated roundrobin disks system 
le system handles mapping transparently managing disks requests le system manager running processor 
spreads overhead processors allows processors computation reserving set processors exclusively workload parallel le systems applications su ciently mature know access patterns 
parallel applications may patterns complex versions application 
le access patterns disk access patterns 
examine pattern access logical blocks le physical blocks disk 
le access pattern best place look sequentiality disk access patterns complicated layout logical blocks disk activities multiple les 
assumptions disk layout 
note application accessing records le translated accesses logical le blocks interface le system 
le system internals responsible caching prefetching see block access pattern 
research investigate read write le access patterns les opened reading writing les updated 
expect especially true large les scienti applications 
covers readonly patterns write patterns covered 
sequential patterns consist sequence accesses sequential portions 
portion number contiguous blocks le 
note le may considered large portion 
accesses portion may sequential viewed local perspective single process accesses successive blocks portion 
call locally sequential access patterns just local access patterns 
traditional notion sequential access uniprocessor le systems 
alternatively pattern accesses sequential global perspective processes share access portion reading disjoint blocks portion 
call globally sequential access patterns just global access patterns 
view process may accessing blocks portion random regular increasing order 
strings processes merged respect time accesses follow roughly sequential pattern 
pattern strictly sequential due slight variations global ordering accesses variation global patterns di cult detect 
addition length portions blocks may regular le system predict portion prefetch past 
di erence block portion rst may regular regular skip allowing system prefetch rst blocks portion 
representative parallel le access patterns 
local patterns global patterns random 
lw local le process reads entire le 
special case local sequential pattern single portion 
lfp local fixed length portions process reads sequential portions 
sequential portions regular length skip di erent places le process 
lrp local random portions lfp portions irregular random length skip 
portions may overlap coincidence 
seg segmented le divided set nonoverlapping contiguous segments process 
process sequential portion 
gw global le entire le read 
processors read distinct records le self scheduled order globally entire le read exactly 
gfp global fixed length portions analogous lfp processors cooperate read appears globally sequential portions xed length skip 
grp global random portions analogous lrp processors cooperate globally read sequential portions random length skip 
rnd random records accessed random 
represents patterns complex represented sequential way 
note patterns necessarily representative distribution access patterns applications 
covers range patterns scienti applications 
methods rapid transit testbed parallel program implemented bbn gp butter parallel processor 
testbed heavily parameterized incorporates synthetic workload le system set simulated disks 
le system allocates manages bu er cache hold disk blocks 
see details 
prefetching attempted processor idle 
assuming commonly processor allocation strategy processor user process processor idle assigned process idle usually waiting disk activity synchronization complete 
decide block prefetch prefetching module calls predictor encapsulates particular policy pattern prediction heuristic 
predictor predictions observed history application 
base evaluations prefetching policies simple policy prefetching 
line predictor called exact entire access pattern advance 
approach 
advance knowledge perfect predictor mistakes requires little overhead 
realistic real predictor know entire access pattern advance 
sense exact gives rough upper bound potential prefetching 
exact limitations lrp grp patterns prefetch past portion demand fetch established location sequential portion rnd pattern exact prefetching reasonably possible 
simple predictors evaluate line predictors described 
practical predictors strategy coarse comparison predictors patterns relatively limited set parameters 
evaluate generally practical predictors wide range parameters examining scalability predictors architectural situations 
predictors local patterns consider global patterns 
local pattern predictors predictors designed predicting local access patterns 
fourth hybrid rst simpler predictors 
predictors monitor individual process patterns looking sequential access 
process patterns independent predictors totally concurrent 
obl block look ahead algorithm predicts block block referenced 
ibl nite block look ahead ibl predicts follow recommends prefetched order 
prefetched depends currently available resources 
ibl logical extension obl designed lw seg patterns 
port portion recognition algorithm attempts recognize sequential portions 
essentially port tries handle lfp access pattern family 
watches regular portion length regular portion skip 
ibl tries predict pattern ahead order prefetch blocks 
ibl limits number blocks predicts limit mistakes may jump portion skips portions regular 
random patterns short portions irregular skip port predicts 
ibl obl port predictor attempting combine best 
begins ibl treat lw seg patterns ciently switches obl rst non sequential 
conservative obl appropriate pattern unexpected non sequential accesses 
regular portions detected port 
global pattern predictors recognize predict globally sequential patterns runtime di cult 
predictor collect examine global history merging local histories 
recognize sequential access blocks pattern may referenced roughly sequential order due variations process speed 
addition cient concurrent implementations di cult due need global decision making 
determine importance tradeo accuracy ciency compare highly accurate ine cient predictor accurate cient predictor 
predictors concurrent processors may active simultaneously synchronization controlling access shared state information 
rst called gaps works hard detect sequentiality global access pattern doing prefetching 
second called assumes pattern sequential appears random 
detecting random access simpler concurrent accurate detecting sequential access 
decide prefetch predictors track accesses prefetches suggest blocks prefetching fetched 
mode capable recognizing sequential portions port unexpected non sequential accesses requiring re evaluation pattern 
see predictors 
experiments details experiments measures give results experiments compare practical predictors exact 
scalability general predictors 
experimental parameters experiments parameters vary time 
parameters described base variations 
combination parameters represents test case 
processes running processors 
generated set access patterns predictors including exact 
patterns contained exactly record accesses record size block 
block size kbyte 
local patterns divided process 
note patterns translates blocks read disk lw distinct blocks read processes read set blocks 
cache contained block bu ers 
record read delay added tests simulate computation delay exponentially distributed mean msec 
tests delay read simulating intensive process 
le interleaved disks granularity single block 
disk requests appropriate disk queue 
disk service time simulated constant arti cial delay msec reasonable approximation average access time current technology small inexpensive disk drives kind replicated large numbers 
measures rapid transit testbed records statistics intended measure interpret performance prefetching 
primary performance metric measuring performance application total execution time 
time measures testbed real time including forms overhead 
record average time read block total synchronization time cache hit ratio prefetch overhead 
measures hit rate average block read time improved prefetching indicators performance 
total execution time incorporates measures ects delays best measure performance 
note data data point represents average trials 
coe cient variation cv standard deviation divided mean average 
experiments cv usually meaning standard deviation trials mean 
places give maximum cv data set 
normalized performance due limited data space experimental data see 
summarizing measure 
exact represents potential prefetching performance evaluate line predictors terms relative performance exact 
measure normalized performance ability line predictor improve compared exact ability improve 
te execution time exact tn time time predictor normalized performance predictor np tn te tn te normal case te normalized performance predictor question exact zero negative slower 
ex act line predictor slower normalized performance may greater 
best normalized performance near 
case te considered anomaly line predictor run faster exact happen subtle reasons 
assign cases normalized performance certainly reached full potential exact 
normalized performance unde ned rnd pattern te tn 
ideal execution time compare experimental execution time simple model ideal execution time 
total execution time combination computation time time overhead 
ideal situation overhead overlapped computation computation overlapped ideal execution time simply maximum time computation time 
assumes workload evenly divided disks processors disks perfectly utilized 
real execution program faster ideal execution time 
base parameter values computation times seconds ideal execution time seconds 
ideal time lw shorter seconds reads blocks disk 
results local pattern predictors measured performance local pattern predictors synthetic workload experimental parameters de ned section varying pattern predictor synchronization style computation computation computation variation forming di erent test case 
primary measure total execution time summarized normalized performance metric 
plots distribution normalized performance predictor achieved set test cases form cumulative distribution function cdf 
recall desired normalized performance indicating line predictor performed exact 
ibl extreme negative positive values indicate slower exact cases 
obl relatively values near 
best minimum value negative points exact performance half test cases 
rnd pattern included port execution time exact test cases 
recognized random pattern irregular set block portions prefetching 
obl ibl prefetched blindly running times slower 
general purpose local predictor excellent performance time mediocre performance time terrible performance 
experiments block record size 
non integral record sizes block size blocks 
predictors handle ignoring performance vary record size experimented record sizes varying quarter block blocks 
small records block overhead slow execution percent cases ected slowing case 
results global pattern predictors set tests similar local predictors global patterns measured performance gaps synthetic workload 
plot cdfs distributions normalized performance 
negative cases grp pattern gaps slower 
general half gaps cases reached normalized performance performance improvement exact half cases reached normalized performance 
rnd pattern isnot included gaps exact time essentially di erence 
handled random patterns ciently 
experiments block record size 
longer records multiple blocks cdf local predictors ibl obl port full potential ibl normalized performance rr normalized performance local predictors patterns rnd 
normalized performance indicates predictor matched exceeded exact performance negative large positive number indicates slower 
ibl range 
total execution time cv 
di cult detect sequentiality inthe block access pattern 
gaps fact failed records larger blocks ran times slower prefetching failed orts recognize sequentiality 
little varying record size closely ex act performance 
generally successful predictor gaps 
scalability knew reasonably general successful predictors various access patterns workload practicality wide range architectural variations 
particular varied number processors number disks ratio processor speed disk speed 
give sample results key see presentation 
number processors varied number processors test scalability le system software including predictors 
holding number disks constant allowed study ects having fewer processors disks preceding experiments processors disks 
essentially holding number processors varying number disks 
total amount ofwork blocks read com cdf global predictors gaps full potential gaps normalized performance ccc normalized performance gaps patterns rnd 
total execution time cv 
putation time held constant 
ideal execution time max seconds total computation time seconds number processors 
seconds 
shows results lfp pattern computation various numbers processors 
ideal execution time decreased processors limited leveled seconds processors 
exact followed curve closely nearly matched exact normalized performance 
slower particularly processors 
disks processors unable full parallel disk bandwidth overlap computation graph shows prefetching successfully overlapped computation scaled processors 
results patterns computation similar global patterns 
shows results bound gfp pattern 
ideal execution time constant seconds 
disks processors full parallel disk bandwidth 
prefetching able disk bandwidth processors 
results gw lfp similar 
prefetching di culty lrp patterns faster prefetching processors 
lw pattern limited disk time regardless number processors prefetching disks 
total time sec lfp computation exact ideal re re re re number processors processors variation 
cv processors disks slightly faster predictors 
point parallelism keep disks occupied prefetching required overhead task mistakes 
expect multiprocessors processors disks somewhat negative result 
small slowdown caused prefetching processors disks small price pay cases prefetching signi cant bene ts small record sizes fewer processors disks lw pattern unbalanced disk loads 
predictors practical variation number processors evidence extrapolate scalability past processors 
particularly performance fewer processors disks slightly negative performance cases processors disks 
application bottleneck limit performance higher performance number processors number disks increased exact ratio depending expected access patterns computational loads 
disk access time expected processor speed disk speed increase time increase processor speed increases disk speed making disks appear slower processors today 
able change processor speed single type machine disks simulated easily change disk access time 
test behavior prefetching access time gap changed 
total time sec bound gfp exact ideal re re re re re re re number processors processors variation 
cv example plots total execution time gfp function disk access time 
ideal execution time linear disk access time pattern contains computation 
ex act followed ideal curve matched slope fastest disks indicating constant overhead 
faster disks relative processor speed occurrence architectural trends broke slower 
bene ts prefetching reduced decreased disk access time costs prefetching function processor speed unchanged 
slower disks success prefetching scaled directly disk access time 
access time gap widens prefetching continue bene cial 
similar reached patterns 
practical predictor general purpose local pattern workloads practical predictor general purpose global pattern workloads 
predictors non prefetching time cases 
cases prefetching bene cial resulting performance loss minor 
remarkably successful reaching potential prefetching determined exact predictor originally reported 
addition predictors robust variations architectural parameters disks number processors disk access time 
important considerations expect see increasing gap processor speed disk access time expect see machines processors disks 
total time sec disk access time variation gfp exact ideal disk access time msec disk access time variation 
cv bbn advanced computers 
butter products overview 
thomas 
file concepts parallel proceedings supercomputing pages 
peter michael scott carla ellis 
bridge high performance le system parallel processors 
proceedings eighth international conference distributed computer systems pages june 
peter 
parallel interleaved file system 
phd thesis university march 

short term le patterns unix environment 
technical report dept computer science univ rochester march 
james french terrence pratt das 
performance measurement input output system intel ipsc hypercube 
proceedings acm sigmetrics conference modeling computer systems pages 
michelle kim 
synchronized disk interleaving 
ieee transactions computers november 
david kotz 
prefetching caching techniques file systems mimd multiprocessors 
phd thesis duke university april 
available technical report cs 
david kotz carla ellis 
prefetching le systems mimd multiprocessors 
ieee transactions parallel distributed systems april 
david kotz carla ellis 
caching writeback policies parallel le systems 
ieee symposium parallel distributed processing december 
ethan miller 
input output behavior supercomputing applications 
technical report ucb csd university california berkeley 
submitted supercomputing 
john ousterhout da costa david harrison john kunze mike kupfer james thompson 
trace driven analysis unix bsd le system 
proceedings tenth acm symposium operating systems principles pages december 
john ousterhout fred douglis 
beating bottleneck case log structured le systems 
acm operating systems review january 
david patterson garth gibson randy katz 
case redundant arrays inexpensive disks raid 
acm sigmod conference pages june 
paul pierce 
concurrent le system highly parallel mass storage system 
fourth conference concurrent computers applications pages 
kenneth salem hector garcia molina 
disk striping 
ieee conference data engineering pages 
alan jay smith 
sequential program prefetching memory 
ieee computer pages december 
alan jay smith 
sequentiality prefetching database systems 
acm transactions database systems september 
alan jay smith 
cache memories 
computing surveys september 
alan jay smith 
disk cache ratio analysis design considerations 
acm transactions computer systems august 
andrew tucker anoop gupta 
process control scheduling issues multiprogrammed shared memory multiprocessors 
proceedings twelfth acm symposium operating systems principles pages december 
