lexical attraction models language ko university istanbul turkey ku edu tr 
presents going lexical attraction models language explicitly represented linguistic knowledge likelihood pairwise relations words 
contrast models represent linguistic knowledge terms lexicon assigns categories word grammar expresses possible combinations terms categories 
word nature simplicity lexical attraction models candidates experiments language learning 
introduce parser line unsupervised learning algorithm expectation maximization 
sentence natural language underlying structure consists relations words john ate cake restaurant simple representation indicates related words links connecting john subject cake object verb ate restaurant eating took place 
sentence john ate cake box word box linked cake presumably john box eating 
effortlessly inferences know usually come boxes people usually eat restaurants 
word associations eat restaurant cake box useful resolving syntactic ambiguity may play important role early child language acquisition 
common assumption language acquisition theories children understand word meanings syntax acquisition begins 
word meanings coupled general world knowledge usually sufficient identify word pairs sentence related 
pre syntax child hears john ate cake conclude john eater cake thing eaten regardless positions words 
turn fuels acquisition syntactic patterns employed particular language express functions subject object 
computer acquire word associations large corpus 
information infer relations words sentence 
nature accuracy relations interest represent important portion information available pre syntax child 
lexical attraction defined likelihood words syntactically related sentence 
deciding words related sentence types information 
grammatical constraints determiner modify noun transitive verbs take objects 
second selectional restrictions verb eat cake object book 
grammatical constraints typically restrictive uniquely identify correct relations 
lexical attraction models quantify selectional restrictions demonstrate sentence understand help syntax 
linguistic models pairwise relations words called dependency models long history 
sleator developed large scale implementations dependency grammar english :10.1.1.12.1238
supervised learning probabilistic dependency models introduced 
unsupervised learning dependency models introduced developed 
unsupervised learning dependency constituency explored 
section gives basic results dependency structures 
section formalizes lexical attraction model language information theory 
section describes parsing algorithm 
section presents unsupervised learning algorithm experiments 
dependency structures linguistic formalism takes syntactic relations words basic primitives known dependency formalism 
mel uk discusses important properties syntactic relations book dependency syntax 
sleator temperley large scale implementation english syntax similar formalism call link grammars :10.1.1.12.1238
section presents basic properties linguistic dependency structures 
assume dependency structure acyclic 
generally case syntactic relations sentence form acyclic graph tree 
linguistically word sentence unique head root word governs sentence sentences natural languages property syntactic relation links drawn words cross 
property called planarity projectivity adjacency various researchers 
examples illustrate planarity english 
sentence easily seen woman red dress meeting afternoon 
second sentence interpretation possible 
fact plausible john red dress 
gaifman gave formal analysis dependency structures satisfy planarity condition 
gives natural correspondence see discussion 
lexical attraction models language john met woman red dress afternoon john met woman afternoon red dress fig 

example illustrating planarity dependency systems phrase structure systems shows dependency model characterized planarity context free 
sleator temperley show planar model context free allows cycles :10.1.1.12.1238
number possible dependency structures word sentence indicates binomial coefficient 
values 
shows possible planar dependency structures words 
fig 

possible planar dependency structures words 
lexical attraction model lexical attraction best described framework information theory 
shannon defines entropy discrete random variable pi log pi ranges possible values random variable pi probability value 
consider sequence tokens drawn independently discrete distribution 
order construct shortest description sequence token encoded log pi bits average 
log pi defined information content token entropy interpreted average information token 
english sentence information content word assuming words independently selected 
word probabilities estimated large corpus news material 
note information content lower frequently occurring words 
ira fighting british rule northern ireland mutual information language models achieve lower entropy account relations words sentence 
consider phrase northern ireland 
independent probability northern seen ireland time 
way saying northern carries bits information adds log bits new information ireland 
dependency northern ireland encoded bits bits 
bit gain correlation words called mutual information 
measure lexical attraction mutual information 
basic assumption words high lexical attraction syntactically related 
diagram gives information content words example bigram model 
information content word computed conditional probability previous word 
result encoding sentence reduced bits bits 
ira fighting british rule northern ireland linguistic context previous word context linguistic intuition 
sentence man dog spoke selection spoke determined man independent previous word dog 
follows context word better determined linguistic relations fixed pattern 
assumption lexical attraction models word depends word sentence necessarily adjacent word ngram models 
lexical attraction models possible define context word terms syntactic relations 
words direct syntactic relation strong dependencies 
chomsky defines dependencies selectional relations 
subject verb example selectional relation verb object 
subject object hand assumed chosen independently 
noted independence approximation 
sentences doctor examined patient lawyer examined witness show subject strong influence choice object 
diagram gives information content words example sentence direct syntactic relations lexical attraction models language ira fighting british rule northern ireland arrows represent head modifier relations words 
information content word computed conditional probability head 
marked verb governing auxiliary noun governing preposition may look controversial linguists 
information theory perspective mutual information content words higher function words 
model favor function word heads 
probabilities estimated counting occurrences pair relative position 
linguistic dependencies reduce encoding words sentence bits compared bits bigram model 
note number excludes encoding dependency structure 
symmetry lexical attraction lexical attraction words symmetric 
mutual information matter direction dependency goes 
directly follows bayes rule 
obvious choice head word corresponding dependency directions imposes effect joint probability sentence 
joint probability determined choice pairs words linked 
consider northern ireland example northern ireland northern ireland case conditional probability northern word ireland 
second case conditional probability ireland previous word northern 
cases encoding words bits fact log joint probability northern ireland 
natural representation link direction label shows number bits gained mutual information northern ireland generalize result representation sentence 
lexical attraction pair words defined mutual information gained link connecting 
ira fighting british rule northern ireland parsing algorithm section describe algorithm finds dependency structure highest probability sentence lexical attraction model 
straightforward way involve comparison exponentially structures 
efficient procedure exists perform calculation steps word sentence 
leftmost word best possible structure may linked words sentence furthest right 
assume word wr 
consider case words wr link words wr break link crossing principle 
link word wr wr presumed final word linked 

wr 
wn divided sentence parts overlap word wr 
multiply lexical attraction products parts get product sentence 
part fewer words sentence recursion guaranteed 
point matter trying picking best possibility link wn 
consider case linked wn 
words wn ultimately link ultimately link wn 
groups disjoint dependency structure cyclic 
suppose wq rightmost word links ultimately 

wq wq 
wn divided sentence parts 
wq wq 
wn single link joining parts 
multiply lexical attraction products parts extra factor wn link get product sentence 
call dependency structures linked wn covered uncovered 
find best dependency structure checking covered uncovered structures picking best 
computation performed bottom starting short segments sentence inductively computing best structures longer segments 
base induction consecutive word pairs linking words possible structure 
notation 
dij best dependency structure segment wi 
wj 
ij best covered structure wi 
wj 
lexical attraction models language ij best uncovered structure wi 
wj 
total lexical attraction union links dependency structures 
link wi wj 
dij inductively computed follows 

arg maxk dc ik 
du ij dc ir 
arg maxk dik dk 
dc ij dq 
dij arg maxd du ij dc ij steps compute best uncovered structure 
note part dc ir covered linked steps compute best covered structure 
note lexical attraction factor link needed step fixed partitions 
final step chooses best structures 
order compute steps performed pair steps require steps find best partition 
algorithm requires steps find best structure sentence 
learning experiments section presents line unsupervised learning algorithm results training lexical attraction models 
algorithm learning processing input sentence manner find structure input sentence current state model 
update model assuming structure correct 
experiments test results obtained words annotated wall street journal text penn treebank corpus 
phrase structure annotations converted dependency structure links results give percentage links guessed correctly experiment 
unsupervised training sample plain wsj text 
experiments run benchmarking purposes 
experiment experiment sentences parsed random model 
model random number generator generate lexical attraction values 
resulting accuracy 
experiment annotated test data supervised training 
resulting model tested data get upper bound performance lexical attraction model resulted accuracy 
experiment sentences held test data remaining supervised training 
testing held data led accuracy 
experiment experiment started empty model giving frequency word pair 
words training data previous experiment 
parser run sentence existing model 
model updated sentence incrementing frequencies linked pairs 
resulting accuracy 
experiment starting empty model words training data parser run sentence existing model previous experiment 
model updated sentence incrementing frequencies second degree neighbors 
resulting accuracy 
accuracy content word links 
difference experiments significant 
model empty giving frequency pair parser ends linking adjacent words 
experiment positive mutual information linked pairs discovered parser starts linking adjacent 
related words seen adjacent discovered 
include objects verbs separated determiners kick ball prepositional phrases adverbials modifying verb need follow object kick ball today 
incrementing second degree neighbors experiment solves problem lets program discover related words eventually 
penn treebank data evaluation unsupervised language acquisition debatable 
conversion phrase structure representation dependency structure representation bound errors 
errors pronounced noun phrases penn treebank gives internal structure 
heuristics employed lead linkage results accuracy result correct linkage discovered program new york stock exchange composite index problem evaluation equal treatment content function words 
extraction meaning mistakes content word links significantly important mistakes function word links 
sentences illustrate difference 
sentence mistake result choosing wrong subject flying 
second sentence program detected relation kick ball way word links important 
saw mountains flying new york going kick ball lexical attraction models language program experiment discovers content word links correctly 
words training accuracy goes 
contributions basic results lexical attraction models explicitly represented linguistic knowledge likelihood pairwise relations words 
showed models unsupervised language learning 
lexical attraction perform situations grammatical constraints sufficiently restrictive analysis complex noun phrases prepositional phrase attachment general syntactic ambiguity 
shows example sentences taken various stages unsupervised learning words news material 
new york stock exchange composite index fell people died clashes west september number people 
number people increased pilot saw train flying washington driver saw airplane flying washington fig 

example sentences output model particularly challenging syntactic models internal structure complex noun phrase prepositional phrase attachment examples syntactic ambiguity 

mel uk dependency syntax theory practice 
suny inci 

sleator temperley parsing english link grammar 
technical report cmu cs cmu 
lafferty sleator temperley grammatical trigrams probabilistic model link grammar 
aaai fall symposium probabilistic approaches natural language processing 

carroll charniak learning probabilistic dependency grammars labeled text 
probabilistic approaches natural language papers aaai fall symposium 

discovery linguistic relations lexical attraction 
phd thesis mit 
grammatical bigrams 
dietterich becker eds advances neural information processing systems nips cambridge ma mit press 
klein manning corpus induction syntactic structure models dependency constituency 
proceedings nd annual meeting acl 

shannon mathematical theory communication 
bell system technical journal 
cover thomas elements information theory 
john wiley sons 

beeferman berger lafferty text segmentation exponential models 
emnlp 

chomsky aspects theory syntax 
mit press 
pearl probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
sleator temperley parsing english link grammar 
third international workshop parsing technologies 

hudson english word grammar 
blackwell 
gaifman dependency systems phrase structure systems 
information control 
harary graph theory 
addison wesley 
graham knuth patashnik concrete mathematics 
edn 
addison wesley 
eisner new probabilistic models dependency parsing 
coling 

marcus penn annotating predicate argument structure 
arpa human language technology workshop 

chelba jelinek exploiting syntactic structure language modeling 
acl 

