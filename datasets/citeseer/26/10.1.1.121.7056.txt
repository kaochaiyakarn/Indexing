fitting mixture model expectation maximization discover motifs biopolymers ucsd technical report cs timothy bailey cs ucsd edu department computer science engineering university california san diego la jolla california phone fax charles elkan elkan cs ucsd edu department computer science engineering university california san diego la jolla california phone fax algorithm described discovers motifs collection dna protein sequences technique expectation maximization fit component finite mixture model set sequences 
multiple motifs fitting component finite mixture model data probabilistically erasing occurrences motif repeating process find successive motifs 
algorithm requires set sequences number specifying width motifs input 
returns model motif threshold bayes optimal classifier searching occurrences motif databases 
algorithm estimates times motif occurs input dataset outputs alignment occurrences motif 
algorithm capable discovering different motifs differing numbers occurrences single dataset 
motifs discovered treating set sequences created stochastic process modelled mixture densities generated occurrences motif rest positions sequences 
expectation maximization estimate parameters densities mixing parameter 
keywords unsupervised learning expectation maximization mixture model consensus pattern motif biopolymer binding site 
contents finite mixture model expectation maximization finite mixture models mm algorithm computing expected likelihood model implementation mm meme algorithm output meme finding starting points mm experimental results meme discovers multiple motifs sensitivity prior sensitivity noise sensitivity subsampling discussion timothy bailey supported nih genome analysis pre doctoral training 
hg 
charles elkan supported part national science foundation award 
iri 
finding cluster numerous similar subsequences set biopolymer sequences evidence subsequences occur chance share biological function 
example shared biological function accounts similarity subset subsequences common protein binding site splice junction case dna sequences active sites related enzymes case protein sequences 
describes algorithm called mm dataset possibly related biopolymer sequences estimates parameters probabilistic model generated dataset 
probabilistic model component finite mixture model 
component describes set similar subsequences fixed width motif component describes positions sequences background 
fitting model includes estimating relative frequency motif occurrences 
estimate select threshold bayes optimal classifier finding occurrences motif databases 
mm algorithm extension expectation maximization technique fitting finite mixture models developed rubin 
related algorithm expectation maximization described lawrence reilly relaxes assumption sequence dataset contains occurrence motif 
words mm solves unsupervised learning problem intended useful discovering new motifs datasets may may contain motifs treating subsequence width dataset unlabeled sample 
hand algorithm lawrence reilly treats sequence labeled sample solves supervised learning problem 
mm related algorithm described bailey elkan 
algorithm mm estimates cluster size number occurrences motif time learning models motif background 
removes need user algorithm know advance number times motif occurs dataset 
possible search motifs datasets little known 
mm algorithm implemented option meme software discovering multiple motifs biopolymer sequences bailey elkan 
mm discover multiple motifs dataset 
briefly done repeatedly applying mm dataset probabilistically erasing occurrences discovered motif 
mm estimates number occurrences motif meme mm able find motifs different numbers occurrences single dataset 
increases usefulness meme tool exploring datasets contain motif 
rest organized follows 
section describes finite mixture model mm section gives analysis needed apply expectation maximization idea type model 
section describes implementation mm context meme 
section presents experimental results mm algorithm discover motifs dna protein datasets 
section concludes discussing strengths limitations mm algorithm 
finite mixture model mm algorithm searches maximum likelihood estimates parameters finite mixture model generated dataset biopolymer sequences 
refer dataset number sequences dataset 
sequences assumed fixed alphabet say input algorithm 
mixture model mm model dataset directly 
dataset broken conceptually overlapping subsequences length contains 
new dataset referred mm learns finite mixture model models new dataset 
model strictly speaking model original dataset practice approximation especially care taken ensure model predict overlapping subsequences new dataset generated motif 
done enforcing constraint estimated probabilities overlapping subsequences motif occurrences 
constraint enforced discussed 
model new dataset consists components model motif background non motif subsequences respectively 
motif model mm says position subsequence occurrence motif generated independent random variable describing multinomial trial parameter probability letter appearing position motif parameters estimated data 
background model says position subsequence part motif occurrence generated independently multinomial trial random variable common parameter generated background model sequence independent samples single background distribution 
model new dataset mm uses motif model probability background model words mm assumes sequence length probability chosen nature sequence length generated probability distribution governing model chosen 
summary parameters model data assumed mm mixing parameter vectors letter frequencies motif model single vector letter frequencies background model 
order find motif dataset value called erasing factor associated letter dataset 
values initially set indicate erasing occurred 
mm converges motif erasing factors adjusted downward proportion probability character part occurrence motif just 
erasing factors mm reestimation effectively remove occurrences motifs dataset 
method finding multiple motifs discussed detail bailey elkan 
motif discovered mm search motif occurrences datasets 
directly estimate conditional probability subsequence length motif 
compute likelihood ratio testing subsequence generated motif background components model 
done estimated values parameter create log odds matrix type referred specificity matrix hertz 
estimated value mixing parameter calculate optimum threshold log odds matrix classifier 
addition log odds matrix profile gaps gribskov motif applications profile analysis 
expectation maximization finite mixture models mm algorithm uses expectation maximization em discover motifs datasets sequences 
sections describe em applied problem hand likelihoods motifs calculated 
mm algorithm mm algorithm maximum likelihood estimation objective discover values parameters model maximize likelihood data 
expectation maximization algorithm finite mixture models rubin 
iterative procedure finds values model 
finite mixture model assumes data locally maximize likelihood data arises groups known distributional forms different unknown parameters 
expectation maximization em find maximum likelihood estimates unknown parameters number samples number groups distributions mixing parameters relative size th group em algorithm concept missing data 
case missing data knowledge group sample data came 
notation useful number samples number groups group 
variable gives group membership th sample 
words distribution values unknown treated em missing information estimated parameters mixture model 
definition clear prior probability particular just conditional densities data missing data written definition conditional probability write joint density sample missing information assumption independent samples missing information joint density data likelihood model parameters joint distribution data missing data defined log likelihood log likelihood log log em algorithm iteratively maximizes expected log likelihood conditional distribution missing data observed data current estimates parameters step em finds expected value log likelihood values missing data observed data current parameter values expected value sum random variables sum individual expectations log expected value log log bayes rule definitions 
defining log definition expected value substituting equation rearranging log log log log step em maximizes order find estimates say maximization involves second term solution argmax log maximize maximizing term separately solve argmax need know form log contains classes distributions class motif class background function mm algorithm assumes mixture letter th position sample background model data expected number times letter indicator appears positions generated expected number appears position occurrences motif data 
reestimate substituting right hand side yielding times letter argmax argmax argmax argmax log log log log estimating parameters multinomial random variable maximum likelihood subject boundary problems 
letter frequency prone happen small datasets value change 
brown lawrence equations replaced turns equivalent bayes estimate value squared error loss sel duffy assuming prior distribution called dirichlet distribution parameter value chosen user depending information available distribution motifs background 
choice discussed section 
factor calculation motif counts erasing factor position data 
erasing mentioned described section 
erasing factors vary set initially 
pass reduced factor representing probability position contained occurrence motif pass 
counts background scaled erasing factors values log likelihood function comparable passes 
computing expected likelihood model log likelihood reported expected log likelihood conditional distribution missing data log log log compute log likelihood particular model just data log log log log values general seen fact log log log log log log tend underestimate log tend close 
rate mm maximizes calculated output implementation mm algorithm practice described section 
remainder current section show model discovered running mm convergence approximated log log log log values reported log likelihood computed approximation 
derivation approximation clearer separate right hand side log terms log log log log split separate terms corresponding motif background components log log substituting known form motif component log log log log log recall expected counts letters positions motif 
assuming moment erasing factors substitute yielding recall equal log reestimated approximation due fact actual formula updating includes pseudo counts tend small practice approximation close actual value 
write evaluated model discovered mm recalling reestimated rewrite log approximation close mm converged 
write entirely terms log erasing factors calculation counts updated discussed section 
anticipating discussion somewhat assumption correct motif dataset discovered mm 
motif discovered values corresponding occurrences motifs far reduced effectively erasing motif occurrences dataset 
cause value calculated somewhat lower true value second motifs dataset 
effect small similar motifs 
exactly erasing factors intended prevent happening problem minimal practice 
compute value completely analogous fashion 
substitute known form background component log log log log log recall expected counts letters background component log log note value assuming mm converged log easy calculate definition 
write log log log algebra enables write desired result log log log log log log log smart prior probability background component prior motif component written log implementation mm log log log mm algorithm implemented option meme software discovering multiple motifs biopolymer sequences 
meme meme program described bailey elkan 
sections describe meme algorithm output starting points mm selected 
section discusses execution time required meme 
meme algorithm meme algorithm shown 
recall number overlapping length subsequences entire dataset number sequences dataset 
motifs width searched outer loop algorithm 
loop various values tried 
limits correspond occurrences motif half non overlapping subsequences dataset motif occurrences 
sampling done geometrically increasing series experiments showed starting points factor correct value usually sufficient mm converge results shown 
innermost loop meme uses occurring subsequences derive values see section 
best starting point determined heuristic described bailey elkan mm discover motif 
motif probabilistically erased see bailey elkan outer loop repeats find motif 
implementation mm algorithm straightforward 
lengths individual sequences dataset motif background models stored array letter frequency vectors subsequences dataset numbered overlapping length left right top bottom stored array holding value procedure meme dataset sequences width motifs search number distinct motifs search randomly replacement select subsequence dataset derive subsequence estimate goodness starting point mm 
run mm convergence best starting point 
print best motif erase occurrences best motif 
meme algorithm 
corresponding subsequence starting column sequence dataset 
mm repeatedly applies step step em update change euclidean distance falls user specified threshold default user specified maximum number iterations default reached 
step updates array equation mapping just de scribed sets length subsequence starting column sequence values sequence normalized sum window size done algorithm bailey elkan enforce constraint done strong tendency mm converge motif models generate repeated strings letters 
tendency arises overlapping substrings new dataset independent 
mm breaks actual sequences overlapping subsequences length causes repetitive sequence original dataset give rise overlapping substrings new dataset highly similar 
avoid problem ad hoc normalization procedure reduces values substrings adjacent original dataset window length sequence sum values substrings starting window equal 
step equations respectively 
pseudo counts set user specified parameter meme average frequency letter dataset 
output meme meme outputs value log likelihood function equation relative entropy column motif 
define log gives measure motif 
output meme includes log odds matrix threshold value motif 
form bayes optimal classifier duda hart zero loss function 
log odds matrix rows columns log calculated log threshold set classifier new dataset overlapping subsequence score log occurrence motif shown bayesian decision theory says classify sample log log threshold loss function easily scaling scaled threshold loss incurred log deciding class correct class class motif class background 
finding starting points mm initial values selected choosing values close occurring subsequences dataset 
subsequence dataset converted column column 
column subsequence set value chosen relative entropy respect uniform distribution letters alphabet fraction maximum user supplied value default 
set log log log log log log log log equal log log maximum relative entropy distribution relative uniform distribution 
equation solved numerically binary search value satisfies 
mm algorithm guaranteed find maximum likelihood estimates model local maximum 
running mm different starting points different initial values model parameters yield different solutions varying likelihood values 
usually necessary run mm starting points pick solution highest likelihood value 
difficult know 
case biopolymer motifs high dimension likelihood surface tends complex local maxima 
task selecting starting points mm especially important 
meme adopts approach deriving starting points actual subsequences dataset 
basic meme approach convert subsequence dataset starting point close 
mm run just iteration tentative starting point mm run convergence starting point highest likelihood iteration 
trying starting points derived subsequences dataset results algorithm execution time number sequences dataset average length width motif number overlapping length subsequences 
dataset contains occurrences motif intuition says unnecessary try subsequences starting points 
just sampling fraction high probability result finding actual occurrence motif 
intuition concrete number occurrences motif dataset 
subsequences dataset randomly sample insure sample actual occurrence probability 
randomly select replacement positions dataset positions probability select possible occurrence occurrence chosen probability choose occurrence choose happens min sample starting points dataset 
number samples need take depend size dataset minimum dataset size gives considerable time savings large datasets large values meme runs mm different values including small ones execution time approximately experimental results studied performance meme number datasets different characteristics 
datasets summarized table 
datasets consist protein sequences consist dna sequences 
contain single known motif 
contains known motifs occurs sequence 
contains known motifs occurs multiple times sequence 
contains motifs occurs half sequences 
protein datasets hth described lawrence test gibbs sampling algorithm described 
reiterate briefly 
proteins bind small hydrophobic ligands wide range biological purposes 
dataset contains divergent known structure 
positions motifs sequences dataset known structural comparisons 
hth proteins contain occurrences dna binding structures involved gene regulation 
correct locations occurrences motif known ray nuclear magnetic resonance structures substitution mutation experiments 
dataset contains protein essential components signal transduction networks 
direct structural information known proteins dataset starting positions motifs reported lawrence 
starting positions agreed results earlier sequence analysis boguski boguski 
dna datasets crp lexa described bailey elkan test meme 
contain dna sequences coli 
crp dataset contains known binding sites crp lawrence reilly sites identified similarity known motif 
lexa dataset sequences contain known binding sites lexa hertz identified similarity known sites 
dataset contains sequences crp lexa datasets 
analyze output meme motifs single run meme classify dataset learned measured defined number known occurrences motif dataset positives defined number correctly classified positives true positives number non occurrences classified occurences false positives 
statistics estimates true precision recall motif learned meme find occurrences motif different dataset 
discovered motif compared motif known occur dataset 
relative closest known motif closest means highest comparison discovered motif known motif done possible shifting known motif fixed number positions meme credited finding motif predicted occurrences displaced small fixed amount 
dataset type number sequence motif sites name sequences length avg name proven total protein hth protein hth protein crp dna crp lexa dna lexa dna crp lexa table overview contents datasets 
proven sites shown occurrences motif laboratory experiment mutagenesis structural analysis 
total sites include proven sites sites reported literature identification primarily sequence similarity known sites putative sites 
meme discovers multiple motifs meme run datasets table passes set value table sampling probability prior parameter 
known motifs pass es meme 
table summarizes output meme datasets shows analysis motifs 
left table log likelihood mixture model relative entropy column motif model pass meme shown 
right name known motif corresponding motif output meme meme motif shifted relative known motif recall precision statistics shown 
exception dataset meme finds known motifs recall precision passes 
motifs higher recall precision seen table 
motif dataset combination known motifs 
second pass relatively poor version known motif 
meme merging known motifs able find 
motif pass dataset somewhat low recall pass picks remainder known occurences 
meme splitting known motif sub motifs sub motif probably probe 
similar splitting motif occurs output meme analysis discovered motifs dataset pass log likelihood motif shift hth hth crp crp lexa lexa lexa lexa crp lexa table meme run passes datasets described table 
left columns table show log likelihood number occurrences motif sites discovered meme pass 
log likelihood values closer zero indicate motifs statistically significant 
right columns show discovered motifs match known motifs datasets 
name known motif closely matched motif output meme shown phase shift 
scores discovered motif probe occurrences known motif dataset shown 
meme representations known motifs datasets provided thresholds motifs probes 
lexa motif 
actual datasets containing unknown motifs right hand side table available researcher 
value log likelihood function indicator real biologically significant motif discovered meme 
observation motifs meme correspond known motifs tend higher likelihoods motifs passes 
passes meme find motifs likelihoods far higher passes may evidence motifs significant 
known motifs meme significant criterion 
marginal case crp motif dataset 
actual practice idea run meme passes number anticipated motifs dataset 
log likelihood motifs passes may give idea log likelihood random motif dataset hand 
case meme find motif perfect recall precision 
natural question due algorithm failing find motif highest log likelihood getting stuck local optimum known motif lower log likelihood assumptions model meme motif 
table shows motifs discovered meme significantly higher log likelihood known motifs 
particular partial merging motifs dataset mentioned yields merged motif higher log likelihood known motif remainder motif higher log likelihood motif 
case discovered motif significantly lower log likelihood 
aforementioned motif dataset 
known higher likelihood discovered clear meme got stuck local optimum case 
sensitivity prior size prior user specified parameter meme algorithm 
interest choice important performance meme 
ascertain meme run biological datasets table input parameters held constant tests showed meme extremely insensitive value chosen range 
outside range things happen 
referring figures patterns discerned 
single known motif dataset hth crp lexa datasets tends increase monotonically time decreases 
surprising large values tend blur increasing size pseudo counts added cell updated em 
blurring effect tend motif general increases decreases dataset pass motif log likelihood log likelihood difference known motif discovered motif hth hth crp crp lexa lexa lexa crp table comparison log likelihood known motifs motifs meme shows meme usually finds motifs statistical significance high known motifs 
case motif meme fail discover motif log likelihood close significantly higher known motif 
motifs dataset value discovered motifs tends decrease tends decrease discovered motifs 
decrease discovered motifs probably result known motifs getting partially merged gets large 
dataset somewhat anomaly 
value meme gets stuck local optimum finds version motif 
large values motif improves 
effects ascribed small number occurences known motifs dataset 
datasets low values sufficient ensure meme finds motifs 
sensitivity noise form noise meme may subjected sequences dataset contain motif 
bound occur new datasets studied see contain new motifs 
extremely desirable meme able find motif may large number superfluous sequences may included dataset 
related problem fact motif occurrences may short compared length sequences dataset 
longer sequences recall recall recall recall recall prior hth prior recall recall recall crp recall lexa prior recall recall recall recall hth prior recall crp prior recall crp recall lexa prior lexa value statistic motifs meme run different values size prior shows remains relatively constant values concluded meme insensitive size long small 
precision precision precision precision precision prior hth precision precision precision prior crp precision lexa prior precision precision precision precision hth prior precision crp prior precision crp precision lexa prior lexa value statistic motifs meme run different values size prior shows unaffected 
difficult meme discover relevant motif occurrences 
sense non motif occurrence positions dataset thought noise 
study problem created additional datasets adding pseudorandom sequences crp lexa datasets 
added sequences average frequencies letters generated random 
shows meme difficulty finding lexa motif pseudo random sequences length added lexa dataset 
quite impressive performance considering proven total proven putative sites length lexa dataset 
hand total sites crp dataset appear harder find 
discovered motif lowered somewhat sequences pseudo random bases added 
random sequences added meme known motif seen 
explanation table shows known crp motif lower log likelihood discovered motifs poor lexa motifs discovered noisy datasets likelihood close known motif 
concluded meme tolerate enormous amount noise motifs weaker motifs require noise dataset ensure meme successful 
sensitivity subsampling user selectable parameter meme determines number starting points tried mm affects speed accuracy meme 
series runs meme datasets described earlier varying values value cases 
effect speed meme seen 
reducing results fold speedup meme relatively small decrease accuracy motifs 
accuracy meme wide range values seen high values table 
motifs usually passes meme 
discussion mm algorithm implementation meme important advantages previously reported algorithms perform similar tasks 
section explains advantages discusses limitations mm meme lifting increase usefulness exploring collections dna protein sequences 
gibbs sampling algorithm lawrence successful existing general algorithm discovering motifs sets 
mm recall precision recall crp precision crp recall lexa precision lexa noise value statistics motifs meme run crp lexa datasets different numbers random sequences added plotted 
value noise number random sequences added crp lexa datasets 
added sequences length average base frequencies sequences dataset added 
meme finds lexa motif dataset consists random sequences 
ability meme discover crp motif degraded significantly half dataset random sequences motif occasionally dataset random sequences 
results show ability meme discover motifs datasets containing sequences motif occur 
output meme analysis discovered motifs dataset pass log likelihood motif log likelihood difference name discovered name known motif motif crp crp crp crp crp crp crp crp crp crp crp crp crp crp lex lexa lex lexa lex lexa lex lexa lex lexa lex lexa lex lexa lex lexa lex lexa table results meme datasets created adding random sequences lexa crp datasets shows crp motif hard find 
meme consistently finds motifs higher log likelihood known crp motif 
statistics runs meme shown 
analysis discovered motifs output meme prob motif shift pass log likelihood hth hth hth crp crp crp lexa lexa lexa crp lexa crp lexa crp lexa table best match discovered motif known motif meme run different values shows general tendency improve increasing expected running meme larger values causes try subsequences dataset starting points mm increasing chances finding starting point 
cases meme finds known motifs 
running meme smaller values requires cpu time seen 
execution time cpu seconds alpha alpha alpha alpha alpha alpha alpha size dataset speed execution meme different values plotted 
meme run datasets table sizes ranged letters 
increase speed gotten reducing value seen generally quadratic dependence execution time size dataset 
major advantages algorithm 
mm require input sequences classified advance biologist definitely containing motif searched 
mm estimates data times motif appears 
capability quite robust experiments show sequences dataset contain motif motif characterized see section 
second mm uses formal probabilistic model entire input dataset systematically selects values parameters model maximize likelihood model 
mm model allows compare principled way motif characterizations discovered meme characterizations obtained methods 
cases characterizations discovered meme higher likelihood 
pointed lawrence fundamental practical difficulty discovering motifs existence local optima search space alternative motif models 
mm algorithm expectation maximization applications gradient descent method escape local optimum 
meme implementation mm uses heuristics overcome problem local optima 
heuristics variations common theme useful applications 
theme search space possible starting points gradient descent systematically 
contrast gibbs sampling algorithms interleave gradient search steps random jumps search space 
algorithms spend unpredictable number iterations plateau converging mm converges predictable relatively small number iterations 
focus current research overcome significant limitations mm meme 
motifs constrained width parameter specified user 
main obstacle estimating motif width endogenously likelihood values comparable models assume different motif widths 
second limitation number different motifs dataset estimated algorithm 
plan overcome limitation generalizing component mixture model models multiple components 
deep difficulty multiple component models induced search space higher dimensionality components local optima 
current intention results meme starting points fitting models multiple components 
doing additional benefit allowing similar motifs discovered different passes meme merged likelihood increased 
rubin murray rubin 
estimation hypothesis testing finite mixture models 
journal royal statistical society 
bailey elkan timothy bailey charles elkan 
unsupervised learning multiple motifs biopolymers expectation maximization 
technical report cs department computer science university california san diego august 
boguski boguski schwartz miller 
analysis conserved domains sequence motifs cellular regulatory proteins locus control regions new software tools multiple alignment visualization 
new biologist 
boguski boguski murray powers 
novel repetitive sequence motifs alpha beta subunits protein homology alpha subunit mad gene product yeast 
new biologist 
brown michael brown richard hughey anders krogh mian david haussler 
dirichlet mixture priors derive hidden markov models protein families 
intelligent systems molecular biology pages 
aaai press 
duda hart richard duda peter hart 
pattern classification scene analysis 
john wiley sons 
gribskov michael gribskov roland david eisenberg 
profile analysis 
methods 
hertz gerald hertz george iii gary stormo 
identification consensus patterns unaligned dna sequences known functionally related 
computer applications biosciences 
lawrence reilly charles lawrence andrew reilly 
expectation maximization em algorithm identification characterization common sites unaligned biopolymer sequences 
proteins structure function genetics 
lawrence charles lawrence stephen altschul mark boguski jun liu andrew neuwald john 
detecting subtle sequence signals gibbs sampling strategy multiple alignment 
science 
duffy duffy 
statistical analysis discrete data 
springer verlag 

