learning markov random fields contrastive free energies max welling school information computer science university california irvine irvine ca usa welling ics uci edu learning markov random field mrf models notoriously hard due presence global normalization factor 
new framework learning mrf models contrastive free energy cf objective function 
scheme parameters updated attempt match average statistics data distribution distribution partially approximately relaxed equilibrium distribution 
show maximum likelihood mean field contrastive divergence pseudo likelihood objectives understood paradigm 
propose study new learning algorithm kikuchi bethe approximation 
algorithm tested conditional random field model skip chain edges model long range interactions text data 
demonstrated loss accuracy training time brought average hours bp learning minutes order magnitude improvement 
learning mrfs context machine learning classes graphical model extensively studied directed graphical model bayesian network bn undirected graphical model markov random field mrf 
models applied successfully number domains fair say learning bns reached advanced level sophistication learning mrfs 
instance hidden variable models efficiently tackled variational em algorithm bayesian inference feasible conjugate priors greedy structure learning algorithms met fully observed bns trivial depend counts 
charles sutton department computer science university massachusetts amherst ma cs umass edu success 
contrast fully observed mrf model evaluating gradient log likelihood typically intractable 
problem traced back presence global normalization term depends parameters translates intractable inference problem compute gradient clearly introducing unobserved random variables aggravates problem bayesian approaches infer posterior distributions parameters structures completely absent literature apart 
mrf models arise applications including spatial statistics computer vision naturallanguage processing feel important improve state affairs 
claim learning mrfs difficult inference problem induced global normalizer different nature harder problem computing posterior distribution hidden variables observed variables needed learning bns 
reason case enter evidence model may reasonable hope posterior peaked single solution 
mrfs need infer distribution variables unconstrained implying distribution trying infer modes 
progress field approximate inference method satisfactorily deal large number modes location unknown 
approximate required averages unconstrained model distribution instance run mcmc sampler mean field approximation 
method relatively slow need sample iteration gradient descent estimated statistics get swamped sampling variance case structure graph identify junction tree small tree width inference performed tractably compute exact learning rules 
course reduce variance samples note improves number samples 
mean field approximation plagued variance mcmc sampler tolerate certain bias estimates 
problems suffer severe problem approximate mode distribution 
argue sampler mix modes absence information location modes unrealistic hope certainly high dimensions 
piece information typically remains fact data points expected located close mode achieve learning 
idea deal mentioned modes problem run multiple mcmc chains initialized different data point 
method certain modes close data points explored samples 
effect learning get local shape local mode correct 
drawbacks modes communicate mixing modes accidental modes created particular parameterization model remain undetected samples implying force remove model 
problem undesirable effect shape mode may fit relative volume free energy modes may properly estimated 
studied mode jumping mcmc procedures proposed improve communication modes 
information location spurious modes mentioned predict extremely hard deal second problem 
running markov chains convergence data case iteration learning clearly costly business 
fortunately turns greatly improve efficiency running markov chains say steps turns uses pseudo samples step reconstructions data approximately minimize called contrastive divergence objective function :10.1.1.35.8613
apart significant increase efficiency decrease variance estimates expense increased bias 
aim current combine deterministic variational approximations ideas contrastive divergence 
idea analogous mean field learning mrfs 
mean field approach contrastive divergence 
current extend ideas general variational approximations 
particular study bethe approximation combination convergent belief optimization algorithm minimize bethe free essential chains started data cases 
energy results novel algorithm train markov random fields loopy structure 
algorithm tested conditional random field model long range interactions called skip chain crfs label tokens email messages 
demonstrate speed learning tenfold cost test performance trained model 
maximum likelihood learning intuitive way restate maximum likelihood objective minimization problem kullback leibler divergence data distribution model distribution ml arg min kl consider general case apart observed variables model may contain number unobserved variables introducing joint distribution distribution rewrite kl divergence difference free energies kl kl cf denotes free energy distribution log denotes free energy random system governed 
subscript indicates run markov chain infinitely long reach equilibrium 
data case identify random systems system free energy data case clamped observed random variables hidden variables free fluctuate 
free system free energy random variables unconstrained 
energy system defined boltzman distribution exp 
discussion general restrict exponential family distributions defined energy function fi 
analogy physical systems decompose free energy average energy term entropy term denotes averaging respect joint denotes averaging respect equilibrium distribution 
learning understood follows data case compute free energy system datum clamped observed units involves inference hidden units 
set constraints observed variables free system relax new distribution lower free energy 
process expected sufficient statistics fi change imperfect model change parameters way expected sufficient statistics better preserved iteration cf fi fi note mean statistics data point cancel equilibrium statistics property hold averaged data cases 
approximate ml learning previous section wrote likelihood function difference free energies intractable compute general 
section replace free energies approximate free energies way conceptually similar mean field approximation introduced 
idea replace objective eqn kl kl app app cf app define approximate variational distributions fully factorized mean field distributions tree structured distributions 
typically depend number variational parameters need computed separately minimizing respective terms eqn 
important simplification achieved minimizing cf app fact log partition function term log cancels terms eqn 
important constraint satisfied contrastive free energy equivalently cf 
reason change unconstrained system average similar constrained system 
ensure sample samples similar data cases 
systems energy function unconstrained system entropy free energy lower see eqn 
cost function wouldn lower bounded allowed arbitrarily large 
example choose mean field approximation eqn qi hi rj zj variational parameters satisfying hi hi zj zj computed minimizing respective kl divergence terms eqn 
easy see smaller simply degrees freedom minimize variables constrained 
convenient imagine process minimize phases clamp data case minimize set variables free continue minimization jointly variational parameters update parameters gradient cf app fi fi need access approximate marginal distributions order compute expectations eqn 
allowed consider general approximate free energies functions local marginal distributions long assert 
important example family kikuchi free energies kik approximate marginals need consistent global distribution words may exist global distribution marginals clusters nodes minimize kik contrastive kikuchi free energy expressed sum constrained local kl divergences follows kik kik cf kik kl kl set clusters consists number overlapping large clusters cover graph interaction fits clusters 
mean marginal data distribution variables cluster 
distribution fixed minimize variables term 
counting numbers sure variable interaction effectively counted 
mean field approximation marginals overlapping required satisfy certain marginalization constraints intersections similarly refer details 
fact mean field equations run sequentially variable time form coordinate descent 
note write mean variables reside cluster 
working clusters consisting edges nodes called bethe approximation emphasize formalism easily adapted general kikuchi approximations fact region graph approximations 
counting numbers case neighbors approximate learning procedure similar seen compute variational parameters minimizing respective kl divergence terms subsequently update parameters gradients cf bethe fi fi need 
free energies bethe bethe convex variational parameters class algorithms name generalized belief propagation minimize bethe free energies kl divergences eqn 
bethe free energy convex special circumstances graph loop 
general local minima reasons explained deserve recommendation run bp random local minimum 
initialize minimization procedures data cases 
clear efficiently find set messages produce prescribed set marginals implying little control initialization 
fortunately algorithms developed rely messages directly minimize bethe free energy function marginals 
general algorithms iteratively construct convex upper bound bethe free energy minimize constraints marginal consistency 
unfortunately constrained bound optimization step slow iterative algorithm linear converge general 
algorithm step learning data case computationally inefficient learning algorithm 
binary random variables pairwise interactions situation considerably better shown constraints solved analytically leaving node marginals free variational parameters 
truly efficient learning procedure currently available case confident efficient minimization algorithms general case developed near 
approximate contrastive free energies introduce second approximation ideas contrastive divergence combine variational approximations described previous section 
effect making learning algorithm computationally efficient 
recall interpretation learning contrastive free energy 
compute free energy consideration compute necessary sufficient statistics 
relax constraints variables clamped value data case system reach equilibrium compute values sufficient statistics 
system relaxed hitting data distribution transition kernel invariant distribution practice replace empirical distribution achieve relaxation running mcmc sampling procedures initialized data cases 
underlying idea contrastive divergence don wait system reached equilibrium valuable information steps relaxation process steps mcmc samplers 
population samples systematic tendency move away data immediately correct tendency changing parameters probability larger location data probability smaller location samples obtained brief mcmc run cf cd fi fi pk gradients downhill approximately minimizes contrastive divergence objective kl kl pk fk derivative objective contains term fk addition terms eqn 
usually small rarely conflict terms gradient result safely ignored :10.1.1.35.8613
clearly learning contrastive divergence results vast improvement efficiency 
data case nearby sample reduce variance estimates sufficient statistic eqn compared mcmc sampler equilibrium time may introduced bias estimates 
hard show infinite number model flexible contain true model true fixed point correct parameter value second term eqn cancel 
refer details contrastive divergence learning 
small step argue procedure combines variational approximation previous section ideas contrastive divergence 
relaxing free energy sampling relax applying minimization procedure variational distributions initialized marginals initialized 
define approximate contrastive free energy kl kl qk app app cf app app function variational distribution qk 
alternatively case kikuchi approximation eqn replace local marginals step counterparts obtained steps minimization kikuchi free energy 
definition step contrastive free energy positive discussed earlier important constraint procedure 
derivatives parameters find cf app app app app qk qk term appears didn minimize free energy fk qk 
term difficult compute don explicit expressions qk terms small rarely conflict terms gradient safely ignored see experimental evidence fact case mf 
ignoring term simplifying terms arrive gradient cf app fi fi qk course kikuchi approximation replace global distributions qk eqn local marginals eqn 
relation pseudo likelihood seen learning mrfs interpreted minimizing difference free energies data clamped observed variables variables unconstrained 
importantly free energy lower 
various methods differed way allowed relaxation free energy take place 
introduced approximate relaxations variational distributions partial relaxations don relax way equilibrium 
see pseudolikelihood estimator interpreted framework see 
pseudo likelihood pl introduced learn mrf models tractably 
fully observed mrf pl pl yk denotes variables variable yk number variables number data cases 
objective far tractable ml criterion depends dimensional normalization constants zk shown asymptotically estimator consistent efficient statistical sense mle 
rewrite minus log objective difference free energies kl pk fi log pl pl cf pl identify term average energy second average dimensional conditional partition functions 
data entropy term free energy data 
second term interpreted partially unconstrained free energy variable relaxed time conditioned final result averaged 
partial relaxations pl relaxation stays close data distribution times condition variables 
relaxed distribution data case mixture pl yn yk yj yj compared maximum likelihood pk step contrastive divergence variational qk step variational 
straightforward derive gradients cf pl fi fi pl fi fi yk considerations easily generalized include hidden variables simplicity chosen illustrate point observed variables 
denotes number nodes cluster 
light interpretation learning mrfs hard generalize pl estimator generalized pl estimator allow relaxation larger possibly overlapping clusters nodes conditioned remaining nodes graph 
leave study generalized pl estimators 
mentioned shown pl estimator asymptotically consistent efficient ml estimator 
interesting see arguments pl consistency proofs adapted cover estimators studied 
conditional random fields conditional random field crf mrf trained maximize conditional log likelihood labels input variables ml arg min kl variables appear data partitioned input nodes observed test time output nodes asked predict test time 
practice discriminatively trained models advantages trained models including ability include interdependent variables needing learn distribution 
previous considerations apply conditional case 
noted trained models free energy computed variables free fluctuate 
contrast discriminatively trained models free energy data case xn clamped input nodes 
learning rule aims match average sufficient statistics random system clamped nodes random system clamped nodes 
important consequence relaxed distributions xn different data case relaxed distributions trained models data cases principle suffice run single mcmc procedure learning iteration experiments section evaluate estimators crfs 
state art training loopy crfs practice penalized maximum likelihood training expected sufficient statistics computed bp note need visit modes markov chain practice may better run multiple markov chains initialized various data cases 
method clique clique cf bethe cf bethe cf bethe cf bethe cf mf ml mf ml bp table performance measure various training methods clique clique models 

difficulties model distribution multiple modes bp may converge different solutions depending initialization fail converge altogether requires running bp convergence step gradient ascent log likelihood expensive 
hope achieve improved training time step cf estimators introduced 
experiments fully observed training data leaving partially observed data 
data set collection mail messages announcing seminars carnegie mellon university 
messages annotated seminar starting time time location speaker 
data set due dayne freitag previous 
reasons discussed section consider binary problem word speaker name 
speaker listed multiple times message 
example speaker name included near sentence meet professor smith 
useful find mentions different information surrounding context mention example mention near institution affiliation second mentions smith professor 
solve problem wish exploit word appears multiple times message tends label 
crf represent adding edges output nodes yi yj words xi xj identical capitalized 
conditional distribution different graphical structure different input configurations input nodes describing word identity part speech tags capitalization membership domain specific lexicons described detail 
compare training time test performance dif ferent contrastive free energies ml mf corresponds maximum likelihood training mean field free en ergy ml bp corresponds maximum likelihood training bethe free energy cf mf cf bethe correspond step contrastive diver gence mean field bethe approximations respectively 
compute contrastive free energy follows 
ml bp trp schedule belief propagation messages initialized 
ml mf damped fixed point equations damping factor uniform initialization 
cf mf observed iterating fixed point equations steps decrease free energy improperly damped 
separate damping factors data case adapted keep cf positive learning 
compute cf bethe belief optimization take gradient steps bethe free energy eliminating constraints solving pairwise marginals sigmoid parameterization described 
step size gradient updates determined line search 
step contrastive divergence essential optimization required compute bethe initialized data cases 
empirical distribution derivative bethe entropy infinite 
avoid problem smooth empirical distribution xj xj 
experiments report performance measure token basis correct tokens tokens extracted correct tokens true tokens 
regularization regularization parameter 
results averaged fold cross validation 
consider clique model cliques linear chain edges yi yi skip edges yi yj input edges yi xi parameters tied instances clique type 
example linear chain edge yi yi weight 
sort parameter tying necessary conditional model observe input know output nodes connections 
table compares testing performance different training methods clique model column 
note cf ml training bethe approximation results better accuracy approximation 
expected skip exposition simpler describe models input variables xi words time reality xi vector observational tests described 
chain model contains short loops graphical structure bethe approximation appropriate mf approximation 
second bethe free energy cf bethe training results comparable accuracy ml training 
great practical significance cf bethe training average minutes train ml training belief propagation hours order magnitude improvement 
belief optimization algorithm developed binary mrfs pairwise interactions boltzmann machines crf free contain arbitrary cliques output nodes distribution contains pairwise interactions 
evaluate practical advantages models evaluate skip chain model higher order cliques 
clique model add input nodes linear chain skip chain cliques linear chain cliques yi yi xi skip cliques yi yj xi xj addition input edges yi xi 
show performance cf bethe model clique model function second column 
values higher order model performs better clique model 
best clique model best higher order clique model folds show improvement averaging folds relative reduction error rises 
unknown reason clique model trained cf bethe hits bad local maximum see behavior richer set features 
clique model ml training bp somewhat better best cf bethe model substantial variance different training sets 
differences ml bp cf bethe clique model statistically significant mcnemar test 
clique model hand cf bethe training significantly better ml bp 
summary experiments demonstrate main points step cf energy performs comparably ml vastly lower training time belief optimization developed boltzmann machines effective training models certain higher order cliques conditional setting 
offered new view parameter learning mrf models minimization contrastive free energies 
seen objectives mrf learning including likelihood function mean field learning objective contrastive divergence pseudo likelihood written positive difference free energies 
learning infer posterior distribution hidden variables clamped data vector relax system exactly approximately partially un constraining observed random variables 
update parameters computing difference average sufficient statistics 
unifying framework conceptually interesting naturally suggests hybrid schemes distributions relaxed partially approximately 
particular studied new learning algorithm contrastive kikuchi bethe free energy accompanying minimization algorithm belief optimization 
feel view rich breeding ground new approximate learning algorithms 
studies hope characterize estimators proposed asymptotic properties consistency statistical efficiency 
acknowledgments supported part center intelligent information retrieval part central intelligence agency national security agency national science foundation nsf iis 
opinions findings recommendations expressed material author necessarily reflect sponsor 
max welling likes hinton teh osindero numerous discussions topic 
besag 
efficiency pseudo likelihood estimation simple gaussian fields 
biometrika 
dayne freitag 
machine learning information extraction informal domains 
phd thesis carnegie mellon university 

consistency maximum likelihood pseudolikelihood estimators gibbs distributions 
fleming eds 
lions editors stochastic differential systems stochastic control theory applications 
new york springer 
heskes 
stable fixed points loopy belief propagation minima bethe free energy 
advances neural information processing systems volume vancouver ca 
hinton :10.1.1.35.8613
training products experts minimizing contrastive divergence 
neural computation 
hinton osindero welling teh 
unsupervised discovery non linear structure contrastive backpropagation 

hinton welling 
improve contrastive divergence 
advances neural information processing systems volume 
john lafferty andrew mccallum fernando pereira 
conditional random fields probabilistic models segmenting labeling sequence data 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 
murray ghahramani 
bayesian learning undirected graphical models approximate mcmc algorithms 
proceedings th annual conference uncertainty artificial intelligence uai san francisco ca 
morgan kaufmann publishers 
peterson anderson 
mean field theory learning algorithm neural networks 
complex systems 
charles sutton andrew mccallum 
collective segmentation labeling distant entities information extraction 
technical report tr university massachusetts july 
icml workshop statistical relational learning connections fields 
charles sutton andrew mc 
dynamic conditional random fields factorized probabilistic models labeling segmenting sequence data 
proceedings international conference machine learning icml 
ben taskar pieter daphne koller 
discriminative probabilistic models relational data 
eighteenth conference uncertainty artificial intelligence uai 
teh welling 
unified propagation scaling algorithm 
advances neural information processing systems 
teh welling osindero hinton 
energy models sparse overcomplete representations 
journal machine learning research special issue ica 
wainwright jaakkola willsky 
tree reparameterization approximate estimation loopy graphs 
advances neural information processing systems volume vancouver canada 
welling hinton 
new learning algorithm mean field boltzmann machines 
proceedings international conference artificial neural networks madrid spain 
welling teh 
belief optimization binary networks stable alternative loopy belief propagation 
proceedings conference uncertainty artificial intelligence pages seattle usa 
yedidia freeman weiss 
constructing free energy approximations generalized belief propagation algorithms 
technical report merl 
technical report tr 
yuille 
cccp algorithms minimize bethe kikuchi free energies convergent alternatives belief propagation 
neural computation 
yuille 
comment contrastive divergence 
technical report department statistics psychology ucla 
technical report 
