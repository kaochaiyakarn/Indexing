cognition elsevier science publishers rights reserved 
learning development neural networks importance starting small jeffrey elman departments cognitive science linguistics university california san diego la jolla ca usa received august final version accepted april striking fact humans greatest occurs precisely point time childhood dramatic changes occur 
report describes possible synergistic interactions change ability learn complex domain language investigated con networks 
networks trained process complex sentences involving relative clauses number agreement types verb argument structure 
training fails case networks fully formed capacity 
training succeeds networks limited working memory gradually mature adult state 
result suggests limitation developmental restrictions resources may constitute necessary prerequisite mastering certain complex domains 
specifically successful learning may depend starting small 
humans differ species dimensions particularly noteworthy 
humans display exceptional capacity learn humans remarkable unusually long time takes reach maturity 
adaptive advantage learning clear may argued culture learning created basis non genetically transmission behaviors may accelerate evolution species 
adaptive am grateful elizabeth bates cathy harris mark johnson annette smith virginia jay mcclelland domenico parisi stimulating discussions issues discussed 
supported contract army avionics ft john catherine foundation 
elman cognition consequences lengthy development hand purely negative 
infancy childhood times great vulnerability young severely restrict range activities adults care protect young 
difficult understand evolutionary pressures prune long period immaturity species 
important remember evolution selects fitness individuals value isolated traits enhanced ability learn duration infancy 
adaptive success individuals determined joint interaction traits 
may understand trait apparently negative consequences lengthy period immaturity need consider possible interac tions traits ability learn 
reason suspect interaction priori humans greatest learning occurs precisely point time childhood undergoing dramatic tional changes 
fact propose humans learning development interact important non obvious way 
changes may provide enabling conditions allow learning effective 
argument indirect findings obtained artificial neural network models learning 
circumstances models best cases forced start small undergo developmental change resembles increase working memory occurs time children 
effect occurs learning mechanism systems specific shortcomings neatly compensated initial learning phase takes place restricted capacity 
light long period development plays positive role acquisition behavior 
divided major sections 
reporting results simulations artificial neural networks 
goal simulations train networks process complex sentences order test ability learn represent part relationships embedded clauses 
networks able learn task handicapped forced severe memory limitations 
effect restricting range data exposed initial learning 
importance starting small 
result contrasts findings connectionist ture 
known instance problems best learned entire data set available network harris 
network subset data fails learn correct generalization remains stuck local error minimum 
result just opposite starting small finding necessary start big 
apparent paradox leads second part 
attempt elman cognition understand deeper principles underlie learning general class connectionist systems rely error driven gradient descent techniques 
principles explain necessary start small times start big 
basically see principles learning interact characteristics human development beneficial manner 
importance starting small studied domains humans learn language 
theoretically problematic domains understanding learning part called projection problem 
problem just task language learner underlying regularities grammar responsible language hears data available learner may sufficient uniquely determine correct grammar problem apparent insufficiency data discussed contexts baker pinker wexler simplest demonstrations comes gold 
gold shows language learner positive data regular languages learned regular languages languages generated finite state automata 
rub hand natural languages appear belong powerful class chomsky evidence children receive negative data learning brown see pinker discussion competing claims 
gold advances suggestions order account fact despite findings children learn language 
children appear receive explicit negative evidence may receive indirect negative evidence 
possibly children know innate need infer grammar solely basis positive data certainly possibilities outlined gold true extent 
child unconstrained learning mechanism sense able learn possible languages 
innate tions narrow range learned 
course open controversial question exactly form innate knowledge takes 
number investigators proposed direct negative evidence may available subtler forms negative evidence 
example say may hinges exactly believes nature input bare sentence strings 
strings accompanied semantic interpretations 
strings accompanied informa tion environment uttered 
gold mentions third possibility text ordered positive presentation sufficient learn complex set languages considers 
details proposal developed 
elman cognition non occurrence expected form constitutes indirect sort negative evidence 
just far sort evidence challenged pinker 
indirect evidence plausibly solution learnability problem contribution known remains controversial 
suggest may third factor helping account apparent ability learners go data 
factor hinges simple fact language learners children undergoing significant developmental changes precisely time learning language 
language learning developmental changes completed far successful 
attributed passing critical period language learning 
restatement facts 
consider question called critical period facilitate learning language 
interestingly learnability theory neglects fact learning development occur 
especially relevant exception shall see newport proposal consistent results obtained 
typical assumption learning device training input static 
wonder consequences having learning device network child input data constant learning 
plunkett shown basic influences type token frequency phonological predictability similar condition non incremental learning better learning achieved training corpus connectionist model allowed slowly grow size 
ask consequences learning mechanism changing 
allowing networks reconfigure dynamically acquire additional nodes shown facilitate learning ash fahlman lebiere schmidt 
section report effect staged input learning connectionist model 
network fails learn task entire data set succeeds data incrementally 
show similar effects obtained realistic assumption input held constant learning mechanism undergoes developmental changes 
examine network see mechanism allows happen suggest conditions necessary incremental learning useful 
simulations originally motivated interest studying ways connectionist networks distributed representations encode complex hierarchically organized information 
mean just sort relationships elman cognition typically occur language 
example sentence girls teacher picked play produced month practice afternoon events described 
backgrounded subordinate main event 
subordination grammatical con sequences 
main verb practice plural agrees girls teacher play 
picked transitive verb takes direct object noun appears verb direct object girls mentioned 
sorts facts specifically recursive nature embedded relative clauses led linguists conclude natural language modeled finite state grammar chomsky statistical inference viable mechanism learning language miller chomsky 
connectionist networks finite state machines 
reason able wonder connectionist networks rely heavily exclusively statistical inference possess requisite computational properties modeling aspects natural language processing capacity finite state automata 
address question constructed semi realistic artificial language crucial properties problematic finite state automata statistical learning attempted train neural network process sentences language 
particular network employed simple recurrent network elman jordan servan schreiber cleeremans mcclelland 
salient property architecture internal states case hidden unit activation patterns fed back time step provide additional input 
recurrence gives network dynamical properties possible network process sequential inputs 
exactly sequential information represented internal states determined advance outset training connection weights activation states random 
network discover underlying temporal structure task learn encode structure internally 
network architecture shown fig 

input corpus consisted sentences generated grammar certain critical properties subject nouns verbs agreed number verbs differed regard argument expectations verbs required direct objects optionally permitted objects precluded direct objects sentences contain multiple embeddings form relative clauses head subject object subordinate clause 
existence relative clauses considerably cated set agreement verb argument facts 
sample sentences boys chase dogs see girls 
girl boys feed cats walk 
elman cognition output input context 
schematic simple recurrent network simulations 
rectangles indicate blocks units number units block indicated side 
forward connections dotted lines trainable solid downward connections hidden context units fixed link units basis 
cats chase dogs 
mary feeds john 
dogs see boys cats mar feeds chase 
important aspect training words represented manner convey information grammatical category meaning number 
word encoded vector single bit randomly set 
grammatical structure nature different inputs obscure network 
network trained take word time predict word 
predictions depend grammatical structure may involve multiple embeddings prediction task forces network develop internal representations encode relevant grammatical information 
see elman details language 
results trials quite disappointing 
network failed master task training data 
performance uniformly bad 
sentences network correctly coordinate number main clause subject mentioned early sentence number main clause verb mentioned embedded relative clauses 
fail get agreement correct relative clause subjects verbs close 
example predict elman cognition boys girl chase see dog getting number agreement boys see right failing proximal presumably easier girl chases 
failure course exactly predicted chomsky miller gold 
incremental input attempt understand breakdown occurring just complex language network able learn devised regimen training input organized corpora increasing complexity network trained simplest input 
phases 
phase sentences consisting solely simple sentences 
network trained exposures epochs database 
phase training data discarded network exposed new set sentences 
second phase sentences simple complex sentences included 
network trained epochs performance quite high complex sentences 
phase mixture simple complex sentences epochs 
phase mixture simple complex 
phase network trained complex sentences 
prediction task grammar non deterministic best measure performance extent literal prediction correct measured error require network memorize training data degree network predictions match unconditional probability distributions training data 
performance metric phases training including final phase 
final performance yielded error network output measured empirically derived likelihood estimates 
alternatively measure cosine angle vectors 
mean cosine training perfect performance 
furthermore network high performance generalized variety novel sentences systematically test capacity predict grammatically correct forms range different structures 
result contrasts strikingly earlier failure network learn full corpus outset put simply network unable learn complex grammar trained outset full adult language 
training data selected result earlier failure replicated times different starting conditions variety different architectures various settings learning parameters learning rate momentum bounds random weight initialization 
elman cognition simple sentences network succeeded mastering going master complex sentences 
sense pleasing result behavior network partially resembles children 
children mastering adult language complexity 
simplest structures build incrementally achieve adult language 
important way network trained way children learn language 
simulation network placed environment carefully constructed encountered simple sentences 
learning performance progressed environment gradually enriched inclusion complex sentences 
model situation children learn language 
evidence adults modify language extent interacting children clear modifications affect grammatical structure adult speech 
network children hear exemplars aspects adult language 
true child environment changes radically simulation true child changes period learning language 
realistic network model constant learning environment aspect network undergo change learning 
incremental memory developmental change plausibly relevant learning gradual increase memory attention span characteristic children 
network analog memory supplied access network recurrent connections prior internal states 
network limited memory access periodically feedback 
network limited temporal window patterns processed 
second simulation carried goal seeing effect staging input limited memory gradually increasing memory span 
rationale scenario closely resembles conditions children learn language 
simulation network trained outset full adult language target corpus previously shown able 
network modified phase recurrent feedback eliminated elman cognition third fourth word randomly second phase network continued set sentences drawn adult language set discarded simply network able memorize importantly memory window increased words 
third phase memory window increased words fourth phase words fifth phase feedback interfered 
conditions turned phase extended longer previous simulation order achieve comparable level performance epochs purposes comparison performance measured simple sentences network trained complex sentences 
initially prolonged stage learning learning proceeded quickly remaining stages epochs stage 
performance training data wide range novel data prior simulation 
learning mechanism allowed undergo changes case increasing memory capacity learning outcome just environment gradually complicated 
discussing implications finding important try understand exactly basic mechanism results apparently paradoxical finding learning improved conditions limited capacity 
know example outcome expected result obtained special stances 
looking way network eventually solved problem representing complex sentences 
network available 
form hidden unit patterns high dimensional space internal representations 
known networks internal representations play key role solution problem 
things internal representa tions permit network escape tyranny form interpretation world 
form input reliable indicator treated 
put way appearances deceive 
cases network uses hidden units construct functionally representational scheme 
similarity structure internal representations reliable indicator meaning similarity structure bare inputs 
simulation network utilized various dimensions internal state represent number different factors relevant task 
include individual lexical item grammatical category noun verb relative pronoun number singular vs plural grammatical role subject vs object level embedding main clause subordinate verb argument done setting context units values 
elman cognition type transitive intransitive optional 
principal component analysis gonzalez identify specific dimensions associated factor 
internal representations specific sentences visualized movements state space looks selected dimen sions planes chosen illustrate factor interest 
example trajectories sentences shown figs 

think plots network equivalent graphs eeg 
time plot movement dimension hidden unit activation space second principal component successfully trained network processes sentences boy boys chase chases boy vs boys boys chase chase boy 
second principal component encodes distinction main clause subject 
chases elman cognition pca 
plot movement dimensions hidden unit activation space third principal components network processes sentences boy chases boy boy sees boy boy walks sentence endings indicated 
nouns occupy right portion plane verbs occupy left side axis running top left bottom right encodes verb argument expectations 
activity recorded human subjects process various types sentences 
example shows singular plural distinction main clause subject encoded reserved embedded relative clause 
shows differences verb argument structure encoded grammar chases requires direct object sees optionally permits walks intransitive 
demonstrates way network represents embedded relative clauses 
elman cognition chases pca 
plot movement dimensions hidden unit activation space eleventh principal components network processes sentences boy chases boy boy chases boy chases boy 
depth embedding encoded shift left trajectory canonical simple sentences 
visualize representational space globally having network process large number sentences recording positions state space word displaying positions 
done 
dimensions total shown coordinates encode depth embedding coordinate encodes number see elman details 
outset learning course dimensions assigned functions 
passes sentences network prior training internal representations discernible structure 
elman cognition 
sample points visited hidden unit state space successfully trained network processes randomly chosen sentences 
sample points visited hidden unit state space network failed learn task processes randomly chosen sentences 
internal representations important outcome learning necessary basis performance 
state space graph shown fig 
produced conditions incremental training seen crucial successful learning 
state space look conditions failure train fully mature network adult corpus 
shows plot 
fig 
fig 
reveals clearly organized state space 
far greater variability words noisier internal representa tions 
see kind sharp distinctions associated encoding number verb argument type embedding network succeeded mastering language 

network confronted entire adult elman cognition corpus problem 
relatively small number sources variance number grammatical category verb argument type level embedding 
sources variance interact complex ways 
interactions involve fairly long distance dependencies 
example difficult understand sentence girl dogs chased block frightened ran away evidence verb frightened transitive bit obscure direct object girl occur verb normal position direct object simple english sentences occurs words earlier nouns verbs 
people simple recurrent networks perfect memory 
network able find solution task works time yield reasonable performance solution imperfect results set internal representations reflect true underlying sources variance 
learning proceeds incremental fashion environ ment altered network initially handicapped result network sees subset data 
input staged data just simple sentences 
network limited temporal window data full adult language effective data sentences portions sentences fall window 
simple sentences 
see initial phase learning takes bit longer condition network wade great deal input essentially noise 
subset data simple sentences contain sources variance grammatical category number verb argument type long distance dependencies 
result network able develop internal representations encode sources variance 
learning advances new input improvements network memory capacity give larger temporal window additional changes constrained early commitment basic grammatical factors 
effect early learning constrain solution space smaller region 
solution space initially large contains false solutions network parlance local error minima 
chances stumbling correct solution small 
selectively focusing simpler set facts network appears learn basic distinction noun verb relative pronoun singular plural form necessary basis learning difficult set facts arise complex sentences 
seen light early limitations memory capacity assume positive character 
predicted powerful network greater ability learn complex domain 
appears case 
domain sufficient complexity abundant false solutions opportunities failure great 
elman cognition required way artificially constrain solution space just region contains true solution 
initial memory limitations fill role act filter input focus learning just subset facts lay foundation success 
ii 
networks learn turn intriguing problem 
answering problem require seek deeper understanding principles constrain learning networks sort studied ways network learning may differ classical learning systems 
problem 
just seen conditions network appears better learning problem begins restricted subset data 
starting small result 
know conditions starting small disastrous restricting training data cases results network learning wrong generalization 
simple example exclusive function xor 
boolean function inputs 
inputs identical function maps false inputs different function maps true input output function learned layer networks sigmoidal tion functions requires additional intermediate layer 
function learned successfully 
particularly important network see patterns outset 
fourth pattern example withheld late training network typically fail learn xor 
learn logical compatible patterns 
worse having learned network unable modify weights way accommodates final pattern 
harris shown similar results hold parity function xor reduced case 
general experience neural networks suggests systems thrive training data may require large data sets order learn difficult function 
results appear sharp variance results earlier 
effects arise consequence fundamental elman cognition properties learning connectionist models consider properties differ approaches learning relevance understand ing interaction learning development humans 
properties focus number relevant discussed bates elman mcclelland press 
properties consider statistical basis learning problem small sample size representation experience constraints new hypotheses ability learn changes time 
property imposes small constraint learning taken characteristics sharply limit power networks 
shall see effect embedding learning system develops time starts small compensate exactly limitations 
property statistics basis learning problem sample size neural network learning algorithms driving force inference statistics 
nature statistical inference complex topic importance statistics learning neural networks engendered certain amount controversy 
large extent inadequacy statistically learning arise claims advanced connection language learning 
known miller chomsky argued certain properties natural language statistically learning infeasible 
problem exemplified sentences people say want rent house summer away europe california 
note exists dependency number plural people early sentence number plural second occurrence verb words 
suppose learner confronted task determining conditions 
miller chomsky argued learner able occurrence statistics inordinate number sentences sampled 
dependency viewed matter statistical occurrence structural fact relating subject verb agreement sentences may contain embedded relative clauses learner previously precise clear properties discussed associated specific approach learning gradient descent specific class connectionist networks involving distributed representations populations units continuously valued non linear activation functions 
variety architectures learning algorithms studied literature 
principles proposed necessarily extend approaches 
elman cognition sampled possible sentences contain people separated possible word combinations 
number astronomical number seconds individual lifetime orders magnitude 
miller chomsky concluded statistically learning inadequate account language acquisition 
important distinguish statistics driving force learning statistics outcome learning 
learning mechanism merely compiling lookup table occurrence facts statistics output approach doomed 
neural networks presumably humans 
neural networks function approximators compilers lookup tables 
goal learning discover function underlies training data 
learning algorithm statistically driven highly sensitive statistics training data 
outcome learning closer miller chomsky called rule system lookup table statistics 
practically means networks able extrapolate training data ways obviate need example see possible combinations words sentences 
words networks generalize 
problem associated statistical learning turns relevant 
problem arises statistics computed small sample sizes 
general smaller sample size sample statistics provide estimate population statistics 
small may large number reasonable generalizations compatible data hand grows new data typically exclude generalizations 
principle infinite number generalizations compatible data set size practice effect additional data constrain highly number reasonable generalizations 
case xor cited earlier example 
consider data patterns classified categories pattern assume network trained data 
novel pattern elman cognition question classified member class class 
answer depends generalization network extracted training data 
multiple possibilities consistent limited observa tions 
network discovered patterns class symmetrical center bits mirror image case network assign test pattern class non symmetrical 
alternatively network discov ered members class parity classify test pattern class parity 
network extracted generalization members class fifth bit position class patterns position 
case text item belongs class 
course outcome symmetry know test item network making classification basis symmetry contents fifth bit position probes needed 
effect limited data impose minimal constraints nature generalizations possible 
increasing data set may restrict range generalizations network extract 
problem neural networks 
certainly early stages learning may limited amount training data problem disappear continued exposure new data 
understand problem persists need consider remaining properties learning 
property representation experience importance data models learning basic question arises lifetime data form prior training examples stored 
question rarely addressed explicit way 
models learning dunbar lea simon osherson weinstein assumed data accumulate available veridical form long may needed 
current hypothesis rejected lacking new hypothesis modification generated old data plus new information prompts revision 
data preserved long needed maintained unprocessed form 
exemplar models assumption quite explicitly estes medin schaffer nosofsky press claim experiences stored individually retrievable exemplars 
connectionist models sort described different assump tions regarding lifetime representational form training examples 
network training pattern learning algorithm results elman cognition small changes connection strengths weights nodes 
weights implement function network learned point 
pattern processed network updated data effect immediate results modification knowledge state network 
data persist implicitly virtue effect network knows 
data lost available learning algorithm reprocessing way allow learning mechanism generate alternative hypotheses 
leads property learning constraints generation new hypotheses 
property constraints new hypotheses continuity search consider space possible hypotheses system entertain 
traditional learning models hypotheses usually take form symbolic propositions rule sets 
space possible hypotheses system consists rules conform grammar system 
connectionist models hand hypotheses implemented values weights connections nodes 
consider trajectory time search spaces 
traditional system trajectory may continuous gradual deletion change rules need 
successive hypotheses need particularly similar 
hypothesis discarded succeeding hypothesis may differ wildly 
part follows consequence having faithful enduring record prior evidence see property 
evidence may rearranged novel ways unconstrained temporal history learning 
neural networks employing gradient descent picture quite different 
search hypothesis space necessarily continuous 
somewhat clearer introduce concepts useful discussion imagine simple network shown top fig 

network input nodes hidden output weights input hidden output 
network shown hypothetical graph error produced hypothetical data set specify systematically vary values weights 
different values weights shown axes error convenience simulations randomly recycle patterns points training confused internal storage explicit manipulation data 
important point data individually represented network 
elman cognition input hidden output 
hypothetical error surface bottom associated network trainable weights top 
coordinate indicates error produced network weights corresponding values coordinates low regions surface correspond low error 
produced network different possible weight values shown axis 
error mean discrepancy network output response training data compared correct output data 
knew error surface advance course set network combination weights produces lowest error marked point 
knowing surface looks determine empirically systematically sweeping possible combina elman cognition tions weights testing network point 
course quite tedious particularly networks larger number weights 
gradient descent learning algorithms provide techniques exploring error surface efficient manner hopefully allows determine combination weights yielding minimum error 
weights chosen randomly uniform distribution near centroid importance apparent 
possible starting point shown fig 
point data processed learning algorithm lets small adjustments current weights way leads lower error follow error gradient 
goal proceed manner find global error minimum point 
succeed get trapped local minimum point small change weights increase error current error non zero depends number factors including big steps weight space allow shape error surface looks error surface joint function network architecture problem hand 
return shortly issues purposes note nature network hypothesis testing qualitatively different character traditional systems 
previously noted approaches learning permit succession radically different hypotheses entertained 
network hand begins randomly chosen hypothesis initial weight settings allowed small incremental changes settings 
plot trajectory weight settings explored gradient descent see looks curve fig 

new hypotheses required similar old hypotheses note similar hypotheses may differ dramatically output produce compare error points 
similar hypotheses may give rise different behaviors 
important point gradient descent approach learning imposes constraint prevents network generating wildly different hypotheses moment 
learning occurs smooth small changes hypotheses 
unfortunately network falls local minimum point constraint may prevent escaping forever believe wrong hypothesis 
turn final property constrains nature learning networks 
property ability learn changes time early flexibility versus late rigidity networks described backpropagation error learning algorithm 
algorithm permits modify weights network elman cognition response errors produced training data 
general terms algorithm understood way credit blame assignment 
somewhat specifically change involves weight adjustment equation equation says weight change units indexes receiver unit sender unit product terms 
term scaling constant referred learning rate typically small value learning occurs small increments 
consider happen fig 
point large weight change oscillate forever points missing terrain 
term aj activation sender implements credit blame aspect algorithm 
middle term si wish focus 
calculated net error error case output unit simply represents discrepancy target output unit actual output net net input 
logistic function determine node activations 
term weight change equation slope function node activations saturate slope asymptotically approaches 
elman cognition derivative receiver unit activation function current net input 
activation function networks sigmoidal shown fig 

activation function important properties input squashed unit resulting activation lies net input results activation middle unit activation range positive inputs result activations greater negative inputs yield activations activation function monotonic non linear 
range greatest sensitivity input node response saturates large magnitude inputs positive negative 
activation function interesting consequences far learning concerned 
recall customary initialize networks small random values lying near centroid weight space region near center space fig 

means outset learning net input node typically close small negative positive weights act multipliers inputs randomly determined mean tend cancel 
net inputs close lie range unit greatest sensitivity outset learning nodes activated region sensitive 
secondly see derivative slope activation function shown fig 
greatest mid range 
near extremes slope diminishes asymptotically 
recalling actual weight change product terms containing slope activation function input see weight changes tend decrease unit activation saturates 
true regardless unit actual error 
large error times vanishingly small slope small number early stages learning input units tends mid range consequence units sensitive input onset learning weights easily modified 
learning progresses weights network tend grow net input increases 
brings units activation range sensitive small differences input leads behavior tends categorical 
earlier malleability gives way increasing rigidity network able respond effectively mistakes early learning learning progresses 
note result independent process maturation direct result learning 
system knows right wrong harder learn new 
considered properties learning connectionist networks 
cross entropy error measure mean squared error problem avoided output units occurs non output units 
elman cognition seen properties way constrains limits ability networks learn 
summarizing main networks rely representativeness data sets 
small sample size network may discover generalization characterizes larger population 
problem serious early stages learning sample size necessarily smaller 
networks sensitive early period learning 
learning progresses networks able modify weights 
taken observation network inclined information point learning early stage information may reliable 
gradient descent learning difficult network dramatic changes hypotheses 
network committed erroneous generalization may unable escape local minimum 
taken second observation problem gets worse learning proceeds 
picture emerges system highly constrained outcome learning may far optimal 
recipe disaster 
approach differs markedly models learning principle deterministic inputs success rate 
achieved basis information available learner estes 
networks fail learn task reasons described 
far perfect learners 
say story terribly interesting 
fact strategies may ameliorate limitations 
arrange better initial data arrange worse initial data 
oddly 
incremental learning strategy employed simulations described example system learn complex domain having better initial data 
language problem hard network learn crucial primitive notions lexical category subject verb agreement obscured complex grammatical structures 
difficult learn primitive representations 
catch problem network unable learn complex grammatical structures lacks primitive representations necessary encode 
difficulties compounded network early commitment erroneous hypotheses tendency time 
incremental learning solves problem presenting network just right data data permit network learn basic representational categories just right time early network plasticity greatest 
key aspect solution far possible relevance human case elman cognition natural mechanism available doing filtering 
starting immature impoverished memory allows system process simple sentences network constructs scaffolding learning 
time progresses gradual improvement memory capacity selects complex sentences processing 
interestingly exactly opposite strategy employed arrange worse initial data 
happen data noisier outset learning network learning capacity greatest early stages time training data limited network runs risk committing wrong generalization 
initial data corrupted noise hand increased variability may learning keep network state flux data reasonable approximations true generalization 
note effects may achieved mechanism developmental schedule initial capacity reduced relative mature state 
perspective limited capacity infants assumes positive value 
limited capacity acts protective veil shielding infant stimuli may irrelevant require prior learning interpreted 
limited capacity reduces search space young learner may able entertain small number hypotheses world 
immature nervous system may encourage generalizations require larger sample size 
empirical evidence support positive interaction limitations language learning 
newport suggested early resource limitations explain apparent critical period languages learned native proficiency 
newport calls hypothesis newport 
known late learners language second exhibit poorer performance relative early native learners 
newport suggests tion performance early native learners comparable level late learners early learning particularly revealing 
gross error scores may similar nature errors groups differs 
late learners tend incomplete control morphology rely heavily fixed forms internal morphological elements frozen place 
young native learners contrast commit errors omission frequently 
newport suggests differences differential ability analyze compositional structure utterances younger language learners advantage 
occurs reasons 
newport points introduced example adding random gaussian noise stimulus patterns 
elman cognition combinatorics learning form meaning mappings underlie morph ology considerable grow exponentially number forms meanings 
supposes younger learner handicapped reduced short term memory reduces search space child able perceive store limited number forms 
adult greater storage computational skills disadvantage 
secondly newport hypothesizes close correspondence perceptually salient units morphologically relevant segmentation 
limited processing ability expect children attentive relationship adults attentive perceptual cues inclined rely computational analysis 
newport similar suggested network performance situations constraints play positive role learning 
counter intuitively problems simply solved start small 
desired 
kenny argued developmental limitations may adaptive individual current state immature motor system newborn animal prevents wandering away mother may assist neurogenesis provide basis perceptual development 
example consider problem size constancy 
major prerequisite infant maintaining order world projected size object may change dramatically distance infant 
kenny suggest problem learning size constancy may easier fact initially infant depth field restricted objects close 
means problem size constancy effectively arise stage 
period infant able learn relative size objects absence size constancy 
knowledge possible learn size constancy 
consistent hypothesis observation size constancy comes months develops objects close mckenzie day 
leads intuitively obvious perspective development biological systems particular value early limitations 
tempting view early stages development negative light 
infancy seen period gotten 
certainly negative consequences having sensorimotor cognitive systems fully developed 
individual perceive threats flee face danger adaptive disadvantage care requirements imposed adults species limit activities consume considerable time energy 
surprising evolutionary forces selected individuals born fully functional 
counter intuitively complex species greater tendency long periods infancy 
humans extreme elman cognition extreme example significant fraction individual lifetime spent childhood 
common explanation prolonged infancy selected humans infancy compromise traits increased brain size upright posture separately significant adaptive value odds 
example positive advantage upright posture frees limbs manipulative purposes 
biomechanical reasons upright posture tends force narrowing pelvis females leads constriction birth canal 
cross purposes larger associated higher primates 
solve problem females evolved slightly wider males partially accommodates larger brain cases pass birth canal 
infant reduced size relative adult 
price paid longer period immaturity may negative consequences outweighed positive advantages conferred ability walk upright larger adult brain 
results provide reason reject hypothesis 
current findings suggest additional reason lengthy period development may adaptive 
focused limitations learning kind learning mechanism afforded neural network quite powerful reasons mechanism favored evolution 
perfect system 
subject various limitations described 
addition environment humans function highly complex domains language may large number false solutions solutions fit examples lead correct generalizations 
shortcomings style learning neatly compensated limitations early child hood 
generally lesson suggests wish understand developmental phenomena aspects human learning important study ways interact 
isolation see learning prolonged development characteristics appear undesirable 
working result combination highly adaptive 
ash 

dynamic node creation backpropagation networks 
connection science 
baker 

syntactic theory projection 
linguistic inquiry 
elman cognition bates elman 

connectionism study change 
mark johnson ed bruin development cognition reader 
oxford blackwell publishers 


negative evidence problem children avoid constructing overly general grammar 
hawkins ed explaining language universals 
oxford blackwell 


types models internalization grammars 
ed grammar theoretical perspective 
new york academic press 
brown 

derivational complexity order acquisition child speech 
hayes ed cognition development language 
new york wiley 
chomsky 

syntactic structures 
hague mouton 
dunbar 

developmental differences scientific discovery processes 
eds st carnegie mellon symposium cognition complex information processing impact herbert simon 
hillsdale nj erlbaum 
elman 

finding structure time 
cognitive science 
elman 

distributed representations simple recurrent networks grammatical structure 
machine learning 
estes 

array models category learning 
cognitive psychology 
man lebiere 

cascade correlation learning architecture 
touretzky ed advances neural information processing systems pp 
san mateo ca morgan kaufmann 
gold 

language identification limit 
information control 
gonzalez 

digital image processing 
reading ma addison wesley 
harris 

parallel distributed processing models metaphors language development 
ph dissertation university california san diego 
jordan 

serial order parallel distributed processing approach 
institute cognitive science report university california san diego 


development memory 
new york freeman 
lea simon 

problem solving rule induction 
simon ed models thought 
new haven ct yale university press 
mcclelland 
press 
parallel distributed processing implications cognition develop ment 
morris ed parallel distributed processing implications psychology neurobiology 
oxford oxford university press 
mckenzie day 

development visual size constancy year human infancy 
developmental psychology 
medin schaffer 

context theory classification learning 
psychological review 
miller chomsky 

finitary models language users 
bush eds handbook mathematical psychology vol 
ii 
new york wiley 
newport 

constraints learning role language acquisition studies acquisition american sign language 
language sciences io 
newport 

constraints language learning 
cognitive science 
nosofsky 
press 
exemplars prototypes similarity rules 
healy kosslyn shiffrin eds learning theory connectionist theory essays honor william estes vol 

hillsdale nj erlbaum 
osherson weinstein 

systems learn learning theory cognitive computer scientists 
cambridge ma mit press 
pinker 

learnability cognition 
cambridge ma mit press 
plunkett 

rote learning system building 
center research language tr university california san diego 
pollack 

language acquisition strange automata 
proceedings annual conference cognitive science society 
hillsdale nj erlbaum 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart mcclelland eds parallel distributed processing explorations microstructure cognition vol 

cambridge ma mit press 
elman cognition servan schreiber cleeremans mcclelland 

encoding sequential structure simple recurrent networks 
cmu technical report cmu cs computer science department carnegie mellon university 
schmidt 

cascade correlation model balance scale phenomenon 
proceedings thirteenth annual conference cognitive science society 
hillsdale nj erlbaum 
kenny 

limitations input basis neural organization perceptual development preliminary theoretical statement 
developmental 
wexler 

forma principles language acquisition 
cambridge ma mit press 
