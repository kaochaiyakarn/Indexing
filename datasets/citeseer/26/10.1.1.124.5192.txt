infinite pcfg hierarchical dirichlet processes liang petrov michael jordan dan klein computer science division eecs department university california berkeley berkeley ca petrov jordan klein cs berkeley edu nonparametric bayesian model tree structures hierarchical dirichlet process hdp 
hdp pcfg model allows complexity grammar grow training data available 
addition presenting fully bayesian model pcfg develop efficient variational inference procedure 
synthetic data recover correct grammar having specify complexity advance 
show techniques applied full scale parsing applications demonstrating effectiveness learning state split grammars 
probabilistic context free grammars pcfgs core modeling technique aspects linguistic structure particularly syntactic phrase structure treebank parsing charniak collins 
important question learning pcfgs grammar symbols allocate learning algorithm amount available data 
question clusters symbols tackled bayesian literature dirichlet process dp mixture models antoniak 
dp mixture models extended hierarchical dirichlet processes hdp hmms teh beal applied different types clustering induction problems nlp johnson 
hierarchical dirichlet process pcfg hdp pcfg nonparametric bayesian model syntactic tree structures dirichlet processes 
specifically hdp pcfg defined infinite number symbols dirichlet process dp prior penalizes symbols supported training data 
note nonparametric mean parameters means effective number parameters grow adaptively amount data increases desirable property learning algorithm 
models increase complexity uncertainty parameter estimates 
regime point estimates unreliable take account fact different amounts uncertainty various components parameters 
hdp pcfg bayesian model naturally handles uncertainty 
efficient variational inference algorithm hdp pcfg structured mean field approximation true posterior parameters 
algorithm similar form em inherits simplicity modularity efficiency 
em algorithm able take uncertainty parameters account incorporate dp prior 
develop extension hdp pcfg grammar refinement hdp pcfg gr 
treebanks generally consist context free tree structures maximumlikelihood treebank grammar typically poor model overly strong independence assumptions 
result generative approaches parsing construct refinements treebank grammar suitable modeling task 
lexical methods split pre terminal symbol word focus smoothing sparse lexical statis tics collins charniak 
methods refine grammar conservative fashion splitting non terminal pre terminal symbol smaller number klein manning petrov 
apply hdp pcfg gr model automatically learn number symbol 
models dirichlet processes heart hdp pcfg dirichlet process dp mixture model antoniak nonparametric bayesian counterpart classical finite mixture model 
order build understanding hdp pcfg review bayesian treatment finite mixture model section 
consider dp mixture model section building block developing nonparametric structured versions hmm section pcfg section 
presentation highlights similarities models step progression reflects key differences 
bayesian finite mixture model describing bayesian finite mixture model establish basic notation carry complex models consider 
bayesian finite mixture model dirichlet 
draw component probabilities component 
draw component parameters data point 
zi multinomial choose component xi generate data point model components prior distribution specified 

dirichlet hyperparameter controls uniform distribution increases increasingly components equal probability 
mixture component 
parameters component drawn prior 
model parameters data points generated choosing component generating data model parameterized component 
document clustering example data point xi document represented vector 
component cluster multinomial parameters specifies distribution words 
customary conjugate dirichlet prior dirichlet 
multinomial parameters interpreted adding pseudocounts word 
dp mixture model consider extension bayesian finite mixture model nonparametric bayesian mixture model dirichlet process 
focus stick breaking representation sethuraman dirichlet process stochastic process definition ferguson chinese restaurant process pitman 
representation captures dp prior explicitly allows extend finite mixture model minimal changes 
enable readily define structured models form similar classical versions 
furthermore efficient variational inference algorithm developed representation section 
key difference bayesian finite mixture model dp mixture model countably infinite number mixture components predefined note infinite number mixture components longer sense consider symmetric prior component probabilities prior component probabilities decay way 
stick breaking distribution achieves follows 
write gem mean 
distributed stick breaking distribution 
concentration parameter controls number effective components 
draw gem generate countably infinite collection stick breaking proportions 
uz beta 
stick breaking weights defined terms stick proportions uz uz 
procedure generating viewed iteratively breaking remaining portions unit 
sample gem 
length stick 
component probabilities decay exponentially expectation probability getting smaller component larger 
parameter determines decay probabilities larger implies slower decay components 
component probabilities rest dp mixture model identical finite mixture model dp mixture model gem draw component probabilities component 
draw component parameters data point 
zi multinomial choose component xi generate data point hdp hmm way hdp pcfg hdp hidden markov model hdp hmm beal teh 
hmm consists set hidden states state thought mixture component 
parameters mixture component emission transition parameters 
main aspect distinguishes flat finite mixture model transition parameters specify distribution states 
just top level mixture model states collection mixture models state 
developing nonparametric version hmm number states infinite need ensure transition mixture models state share common inventory possible states 
achieve tying mixture models hierarchical dirichlet process hdp teh 
stick breaking representation hdp defined follows top level stick breaking weights drawn stick breaking prior 
new set stick breaking weights generated dp distribution dp characterized terms finite partition property partitions positive integers sets 
am 
am dirichlet 
am resulting distribution positive integers similarity controlled concentration parameter hdp hmm gem draw top level state weights state 
dirichlet draw emission parameters dp draw transition parameters time step 
xi emit current observation zi multinomial choose state state associated emission parameters addition associated transition parameters specify distribution states 
transition parameters drawn dp centered top level stick breaking weights equations 
assume fixed special start state need generate 
hdp pcfg hdp pcfg focus 
simplicity consider chomsky normal form cnf grammars types rules emissions binary productions 
consider grammar symbol mixture component parameters rule probabilities symbol 
general know appropriate number grammar symbols strategy number grammar symbols infinite place dp prior grammar symbols 
note property specific instance general stochastic process definition dirichlet processes 
hdp pcfg gem draw top level symbol weights grammar symbol 
dirichlet draw rule type parameters dirichlet draw emission parameters dp draw binary production parameters node parse tree ti multinomial choose rule type ti emission xi multinomial emit terminal symbol ti binary production zl zr multinomial generate children symbols definition graphical model hdp pcfg 
parse trees unknown structure convenient way representing visual language traditional graphical models 
show simple fixed example tree 
node children observed terminal child 
denote left right children node hmm transition parameters state specify distribution single states similarly binary production parameters grammar symbol specify distribution pairs grammar symbols children 
adapt hdp machinery tie binary production distributions 
key difference tie distributions pairs grammar symbols distributions single grammar symbols 
difference hmm time step transition emission pcfg binary production emission chosen 
grammar symbol distribution type rule apply 
cnf pcfg types rules easily generalized include unary productions parsing experiments 
summarize parameters grammar symbol consists distribution finite number rule types emission distribution terminal symbols binary production distribution pairs children grammar symbols 
describes model detail 
shows generation binary production distributions draw dp centered product distribution pairs symbols 
result doubly infinite matrix probability mass con gem dp left child state state right child state left child state right child state generation binary production probabilities top level symbol probabilities 
drawn stick breaking prior dp model 
outer product formed resulting doubly infinite matrix matrix 
base distribution generating binary production distribution dp centered 
upper left just top level distribution note replaced general zi pair dirichlet multinomial zi specialize natural language difficulty working parse trees arbitrary non multinomial observations sophisticated word models 
natural language applications hard distinction pre terminal symbols emit word non terminal symbols rewrite non terminal pre terminal symbols 
accomplished letting forces draw assign probability rule type 
alternative definition hdp pcfg follows symbol draw distribution left child symbols lz dp independent distribution right child symbols rz dp 
define binary production distribution cross product yields distribution symbol pairs defines different type nonparametric pcfg 
model simpler require additional machinery hdp hmm 
modeling assumptions imposed alternative unappealing assume left child right child independent parent certainly case natural language 
hdp pcfg grammar refinement important motivation hdp pcfg refining existing treebank grammar alleviate unrealistic independence assumptions improve parsing accuracy 
scenario set symbols known know allocate symbol 
introduce hdp pcfg grammar refinement hdp pcfg gr extension hdp pcfg task 
essential difference collection hdp pcfg models symbol operating level 
hdp pcfgs independent prior coupled interactions parse trees 
completeness included unary productions essentially pcfg counterpart transitions hmms 
node parse tree involves pair si zi needs specify distribution child symbols 
handled finite dirichlet distribution symbols known observed handled dirichlet process machinery number unknown 
hdp pcfg grammar refinement hdp pcfg gr symbol gem draw weights 
sz dirichlet draw rule type parameters sz dirichlet draw emission parameters sz dirichlet unary symbol productions sz dirichlet binary symbol productions child symbol dp unary prod 
pair children symbols dp binary node parse tree ti multinomial iz choose rule type ti emission xi multinomial iz emit terminal symbol ti unary production sl multinomial iz generate child symbol zl multinomial iz child ti binary production sl sr mult iz children symbols zl zr mult iz variational inference inference algorithm hdp pcfg model described section adapted hdp pcfg gr model bit bookkeeping 
previous inference algorithms dp models involve sampling escobar west teh 
chose variational inference blei jordan provides fast deterministic alternative sampling avoiding issues diagnosing convergence aggregating samples 
furthermore variational inference algorithm establishes strong link past pcfg refinement induction traditionally employed em algorithm 
em step involves dynamic program exploits markov structure parse tree step involves computing ratios expected counts extracted step 
variational algorithm resembles em algorithm form ratios step replaced weights reflect uncertainty parameter es approximate true posterior parameters latent parse trees structured mean field distribution distribution parameters completely factorized distribution parse trees unconstrained 

procedural similarity method able exploit desirable properties em simplicity modularity efficiency 
structured mean field approximation denote parameters hdp pcfg denotes top level symbol probabilities denotes rule probabilities 
hidden variables model training parse trees denote observed sentences goal bayesian inference compute posterior distribution 
central idea variational inference approximate intractable posterior tractable approximation 
particular want find best distribution defined def argmin kl tractable subset distributions 
structured mean field approximation meaning consider distributions factorize follows def 
restrict dirichlet distributions allow multinomial distribution 
constrain degenerate distribution truncated posterior grammar infinite number symbols exponential decay dp prior ensures probability mass contained symbols james 
variational approximation truncated actual pcfg model 
increases approximation improves 
coordinate wise ascent optimization problem defined equation intractable nonconvex simple coordinate ascent algorithm iteratively optimizes factor turn holding fixed 
algorithm turns similar form em ordinary pcfg optimizing analogue step optimizing analogue step optimizing analogue em 
summarize updates see liang complete derivations 
parse trees distribution parse trees summarized expected sufficient statistics rule counts denote zl zr binary productions emissions 
compute expected counts dynamic programming step em 
classical step uses current rule probabilities mean field approximation involves entire distribution 
fortunately handle case replacing rule probability weight summarizes uncertainty rule probability represented define weight sequel 
common perception bayesian inference slow needs compute integrals 
mean field inference algorithm counterexample represent uncertainty rule probabilities single numbers existing pcfg machinery em modularly imported bayesian framework 
rule probabilities ordinary pcfg step simply involves ratios expected particular variational distance distribution truncated version decreases exponentially truncation level increases 
counts zl zr zl zr 
variational hdp pcfg optimal standard posterior update dirichlet distributions dirichlet matrix counts rules lefthand side distributions summarized multinomial weights necessary quantities updating iteration zl zr def exp eq log zl zr zl zr zr digamma function 
emission parameters defined similarly 
inspection equations reveals difference maximum likelihood mean field update applies exp function counts 
truncation large zl zr near right hand sides zl zr exp effect counts 
subtraction affects large counts small counts rich get richer effect rules large counts preferred 
specifically consider set rules left hand side 
weights rules differ numerator equation applying exp creates local preference right hand sides larger counts 
note rule weights normalized sum equal exactly degenerate 
lack normalization gives extra degree freedom maximum likelihood estimation creates global preference left hand sides larger total counts 
top level symbol probabilities recall restrict optimizing equivalent finding single best truncated top level symbol weights dp prior reduces finite dirichlet distribution 
exp exp function computing multinomial weights mean field inference 
effect reducing larger fraction small counts large counts 
closed form expression optimal objective function equation convex apply standard gradient projection method bertsekas improve local maxima 
part objective function equation depends follows log gem eq log dirichlet see liang 
derivation gradient 
practice optimization little effect performance 
suspect objective function dominated contribution minor 
experiments empirical evaluation hdp pcfg gr model variational inference techniques 
give illustrative example ability hdp pcfg recover known grammar results experiments large scale treebank parsing 
recovering synthetic grammar section show hdp pcfg gr recover simple grammar standard xi xi ai bi ci di ai bi ci di synthetic grammar uniform distribution rules 
grammar generates trees form shown right 
pcfg fails built control grammar complexity 
grammar generated trees 
terminal symbols subscript collapsed xi training data 
trained hdp pcfg gr truncation iterations 
set hyperparameters 
shows hdp pcfg gr recovers original grammar contains leaving unused 
standard pcfg allocates fit exact occurrence statistics left right terminals 
recall rule weight defined equation analogous rule probability standard pcfgs 
say rule effective weight left hand side posterior general rules weight smaller safely pruned affect parsing accuracy 
standard pcfg uses explain data resulting effective rules contrast hdp pcfg uses resulting effective rules 
threshold relaxed rules effective corresponds exactly true grammar 
parsing penn treebank section show variational hdp pcfg scale real world data sets 
ran experiments wall street journal wsj portion penn treebank 
trained sections section tuning hyperparameters tested section 
trees treebank follows non terminal node symbol posterior posterior standard pcfg hdp pcfg posteriors standard pcfg roughly uniform posteriors hdp pcfg concentrated true number symbols grammar 
right branching cascade new nodes symbol result node children 
cope unknown words replace word appearing fewer times training set unknown word tokens derived word form features 
goal learn refined grammar symbol training set split 
compare ordinary pcfg estimated maximum likelihood hdp pcfg estimated variational inference algorithm described section 
parse new sentences grammar compute posterior distribution rules span extract tree maximum expected correct number rules petrov klein 
hyperparameters hyperparameters hdp pcfg gr model set manner uniform distribution ies versus binaries uniform distribution terminal words number different unary binary righthand sides rules left hand side treebank grammar 
important hyperparameters govern sparsity right hand side unary binary rules 
set performance probably gained tuning individually 
turns single works truncation levels shown table 
top level distribution uniform value corresponding uniform prior pairs children interestingly optimal appears superlinear subquadratic truncation best uniform table truncation level report yielded highest score development set 
pcfg pcfg smoothed hdp pcfg size size size table shows development grammar sizes number effective rules increase truncation values experiments 
results regime bayesian inference important training data scarce relative complexity model 
train just section penn treebank 
table shows hdp pcfg gr produce compact grammars guard overfitting 
smoothing ordinary pcfgs trained em improve increases start overfit 
simple add smoothing prevents overfitting cost sharp increase grammar sizes 
hdp pcfg obtains comparable performance smaller number rules 
trained sections demonstrate methods scale achieve broadly comparable results existing state theart parsers 
truncation level standard pcfg smoothing obtains score effective rules hdp pcfg gr obtains score effective rules 
expect see greater benefits hdp pcfg larger truncation level 
related question select appropriate grammar complexity studied earlier 
known complex models necessarily higher likelihood penalty imposed complex grammars 
examples penalized likelihood procedures include stolcke omohundro asymptotic bayesian model selection criterion petrov 
split merge algorithm procedurally determines switch grammars various complexities 
techniques model selection techniques heuristics choose competing statistical models contrast hdp pcfg relies bayesian formalism provide implicit control model complexity framework single probabilistic model 
johnson 
explored nonparametric grammars give inference algorithm recursive grammars grammars including rules form bc da 
recursion crucial aspect pcfgs inference algorithm handle 
finkel 
independently developed nonparametric model grammars 
model hierarchical dirichlet processes similar different inference algorithm sampling 
kurihara sato kurihara sato applied variational inference pcfgs 
algorithm similar consider nonparametric models 
hdp pcfg nonparametric bayesian model pcfgs efficient variational inference algorithm 
primary contribution elucidation model algorithm explored important empirical properties hdp pcfg demonstrated potential variational hdp pcfgs full scale parsing task 
antoniak 

mixtures dirichlet processes applications bayesian nonparametric problems 
annals statistics 
beal ghahramani rasmussen 

infinite hidden markov model 
advances neural information processing systems nips pages 
bertsekas 

nonlinear programming 
blei jordan 

variational inference dirichlet process mixtures 
bayesian analysis 
charniak 

tree bank grammars 
association advancement artificial intelligence aaai 
charniak 

maximum entropy inspired parser 
north american association computational linguistics naacl pages 
collins 

head driven statistical models natural language parsing 
ph thesis university pennsylvania 
escobar west 

bayesian density estimation inference mixtures 
journal american statistical association 
ferguson 

bayesian analysis nonparametric problems 
annals statistics 
finkel manning 

infinite tree 
association computational linguistics acl 
griffiths johnson 

contextual dependencies unsupervised word segmentation 
international conference computational linguistics association computational linguistics coling acl 
james 

gibbs sampling methods stick breaking priors 
journal american statistical association 
johnson griffiths 

adaptor grammars framework specifying compositional nonparametric bayesian models 
advances neural information processing systems nips 
klein manning 

accurate parsing 
association computational linguistics acl pages 
kurihara sato 

application variational bayesian approach probabilistic contextfree grammars 
international joint conference natural language processing workshop shallow analyses 
kurihara sato 

variational bayesian grammar induction natural language 
international colloquium grammatical inference 
liang petrov jordan klein 

nonparametric pcfgs dirichlet processes 
technical report department statistics university california berkeley 
miyao tsujii 

probabilistic cfg latent annotations 
association computational linguistics acl 
petrov klein 

learning inference hierarchically split pcfgs 
human language technology north american association computational linguistics hlt naacl 
petrov barrett klein 

learning accurate compact interpretable tree annotation 
international conference computational linguistics association computational linguistics coling acl 
pitman 

combinatorial stochastic processes 
technical report department statistics university california berkeley 
sethuraman 

constructive definition dirichlet priors 
statistica sinica 
stolcke omohundro 

inducing probabilistic grammars bayesian model merging 
grammatical inference applications 
teh jordan beal blei 

hierarchical dirichlet processes 
journal american statistical association 
