direct gradient reinforcement learning ii 
gradient ascent algorithms experiments jonathan baxter research school information sciences engineering australian national university jonathan baxter anu edu au lex weaver department computer science australian national university lex weaver anu edu au peter bartlett research school information sciences engineering australian national university peter bartlett anu edu au september introduced algorithm computing arbitrarily accurate approximations performance gradient parameterized partially observable markov decision processes :10.1.1.125.2542
algorithm chief advantages requires single sample path underlying markov chain uses free parameter natural interpretation terms bias variance trade requires knowledge underlying state 
addition algorithm applied infinite state control observation spaces 
conjugate gradient ascent algorithm uses subroutine estimate gradient direction 
uses novel line search routine relies solely gradient estimates robust noise performance estimates 
line gradient ascent algorithm 
chief theoretical advantage gradient approach approaches reinforcement learning guarantees improvement performance policy step 
show advantage real give experimental results optimize simple state markov chain controlled linear function twodimensional puck controlled neural network call admission queueing problem variation classical mountain car task 
cases algorithm rapidly optimal near optimal solutions 
function approximation necessary avoid curse dimensionality associated large scale dynamic programming reinforcement learning problems 
dominant paradigm function approximate state state action values 
algorithms seek minimize form error approximate value function true value function usually simulation see comprehensive overviews :10.1.1.32.7692
multitude empirical successes approach see name lacks fundamental theoretical guarantees performance policy generated approximate value function see section discussion :10.1.1.8.3850:10.1.1.22.3540
motivated difficulties introduced new algorithm computing arbitrarily accurate approximations performance gradient parameterized partially observable markov decision processes :10.1.1.125.2542
algorithm essentially extension williams algorithm similar algorithms :10.1.1.129.8871
specifically suppose parameters controlling example parameters approximate neural network value function generates stochastic policy form randomized look ahead parameters approximate function stochastically select controls denote average reward parameter setting computes approximation single continuous sample path underlying markov chain 
accuracy approximation controlled parameter proved theorem trade preventing choosing arbitrarily close variance estimates increase bright side theorem showed approximation error proportional subdominant eigenvalue markov chain underlying rapidly mixing significantly estimates performance gradient acceptable bias variance obtained 
provided sufficiently accurate approximation fact need adjustments parameters form small step size guarantee improvement average reward stochastic policies strictly necessary framework policy differentiable sense exists 
case gradient optimization algorithms gradient estimate guaranteed improve average reward step 
case table lookup value function approaches reinforcement learning guarantee 
see analysis case demonstration performance degradation course training neural network backgammon player 
conjugate gradient ascent algorithm uses estimates provided critical successful opera tion novel line search subroutine reduces noise relying solely gradient estimates 
line variant algorithm updates parameters time step 
similar finds near algorithms proposed 
algorithms applied variety problems simple state markov decision process mdp controlled linear function true gradient exactly computed 
show rapid convergence gradient estimates true gradient case large range values simple system able illustrate bias variance tradeoff associated selection find policy mdp 
reliably finds near optimal policy iterations markov chain order magnitude faster demonstrate effectiveness training neural network controller control puck dimensional world 
task case reliably navigate puck starting configuration arbitrary target location minimum time applying discrete forces tions 
direc third experiment train controller call admission queueing problem treated 
case optimal solutions iterations underlying queue 
fourth final experiment train switched controller dimensional variation classical mountain car task example 
rest organized follows 
section introduce definitions needed understand section describe gradient line search subroutine section experimental results 
algorithm partially observable markov decision process consists state space observation space control space state deterministic reward results guarantee convergence case finite arbitrary algorithm applied regardless nature restrict cardinality consider case discrete control determines stochastic matrix giving transition probability state state :10.1.1.125.2542
state observation generated independently distribution kernel policy distribution distribution probability observations denote probability randomized policy simply function mapping observations probability distributions controls observation distribution controls denote probability control observation continuous giving probability density transitions probability density function density probability density function density randomized corresponds markov chain state transitions generated selecting observation state selecting control generating transition state probability dealing fixed parameterize function parameterize policies set parameters observation markov chain corresponding state transition matrix technical assumptions required operation assumption 
derivatives exist assumption 
ratios uniformly bounded assumption 
magnitudes rewards uniformly bounded states assumption 
unique stationary distribution average reward simply expected reward stationary distribution assumption starting state equal expected long term average reward expectation sequences states markov chain specified algorithm reproduced algorithm algorithm computing approximation theorem proved satisfying approximation theorem 
note relies single sample path pomdp 
require knowledge transition probability matrix observation process requires knowledge randomized policy 
algorithm parameters algorithm 
randomized policy satisfying assumptions 
rewards satisfying assumption controlled generates stochastic matrices satisfying assumption 
arbitrary unknown starting state 
set 
observe generated generate control observe 
state generated set set return set arbitrarily close variance estimate increases increasing natural interpretation terms bias variance trade small values give lower variance estimates higher bias may far values close yield small bias correspondingly larger variance 
bias variance trade illustrated experiments section 
stochastic gradient ascent algorithms section introduce algorithms variant polak conjugate gradient algorithm see fully line algorithm updates parameters iteration algorithm described algorithm version polak algorithm designed operate noisy possibly biased estimates gradient objective function example estimates provided 
novel feature linesearch subroutine uses gradient information find local maximum search direction 
gradient information ensures provides initial step size robust noise performance estimates 
applied stochastic optimization problem noisy possibly biased gradient estimates available 
argument falls argument terminates 
algorithm key successful operation linesearch algorithm algorithm 
uses gradient information bracket maximum direction quadratic interpolation jump maximum 
gradients bracket maximum far robust function values 
bracket maximum function values points sign sign lying direction need estimate 
access noisy estimates example estimates obtained simulation regardless magnitude variance variance sign approaches maximum possible approaches reliably bracket maximum noisy estimates need able reduce variance estimates close 
case means running simulation estimates derived longer longer periods time 
alternative approach bracketing maximum direction find points direction maximum lie advan tage approach estimates noisy variance algorithm possibly noisy biased estimate gradient objective function maximized 
starting parameters set maximum return 
initial step size gradient resolution sign tween particular grow points approach 
disadvantage possible detect extreme overshooting maximum gradient estimates 
careful control line search find problem 
sign independent distance algorithm lines bracket maximum finding parameter setting second parameter setting expressions provide robustness errors estimates reason prevents algorithm stepping local maximum direction note determine terminate due small gradient line 
provided signs gradients bracketing points show maximum quadratic defined points lies line jump maximum 
algorithm simply jumps midpoint updating parameters time step operates iteratively choosing uphill directions searching local maximum chosen direction 
argument optimization involve iterations underlying algorithm possibly noisy biased estimate gradient objective function 
starting parameters set maximum return 
search direction initial step size inner product resolution step back bracket maximum repeat step forward bracket maximum repeat parameter updates 
alternative approach similar spirit algorithms described adjust parameter vector iteration underlying algorithm presents algorithm lines 
currently working convergence proof algorithm 
algorithm initial parameter values randomized parameterized policies satisfying assumptions 
rewards satisfying assumption controlled generates stochastic matrices satisfying assumption 
step sizes satisfying arbitrary unknown starting state 
set 
observe generated 
generate control observe state generated 
set set return experiments section sets experimental results 
sec tion refer mean argument 
set experiments consider system controller select actions state markov decision process 
system able compute true gradient exactly matrix equation origin destination state probabilities state action table transition probabilities state mdp transition matrix underlying markov chain controller parameters set stationary distribution corresponding written row vector matrix row stationary distribution column vector rewards see derivation 
compare estimates generated iterations true gradient function number function discount parameter optimize performance controller line algorithm reliably converges near optimal policy iterations line method requires approximately iterations 
contrasted training linear value function system shown converge value function step lookahead policy suboptimal :10.1.1.132.7760
second set experiments consider simple puck world problem small puck navigated dimensional world applying thrust directions 
train hidden layer neural network controller puck optimality 
controller reliably converges near third set experiments optimize admission thresholds call admission problem considered 
final set experiments train switched controller dimensional variant mountain car task example 
state mdp section consider state state choice actions table shows transition probabilities function states actions 
state associated dimensional feature vector reward detailed table 
clearly optimal policy select action leads state highest probability table means selecting action odd choice feature vectors states ensures value function linear features trained observing optimal table state rewards features 
policy implement suboptimal step greedy lookahead policy see proof 
contrast gradient approach system training linear value function guaranteed produce worse policy starts observing optimal policy 
training controller goal learn stochastic controller system implements optimal near optimal policy 
parameter vector generate policy follows 
state probability choosing action state probability choosing action ratios needed algorithms gradient estimates parameter vector estimates generated various values measure progress true gradient calculated value angle relative error angles relative errors plotted figures 
recorded 
angle degrees angle degrees beta beta angle degrees angle degrees beta beta angle true gradient estimate state markov chain various values discount parameter generated algorithm 
averaged independent runs 
note higher large variance larger values error bars standard deviation 
graphs illustrate typical trade algorithm small values give higher bias estimates larger values give higher variance bias shown norm deviation small measure angular deviation 
said bias introduced having small system 
worst case final gradient direction indistinguishable true direction relative deviation training conjugate gradient ascent argument train parameters controller described previous section 
low bias observed experiments previous section argument set small amount experimentation arguments set respectively 
values critical extremely large initial step size considerably reduce time required controller converge near optimality 
initial values parameter vector chosen similar results 
note generates suboptimal policy 
relative norm difference relative norm difference beta beta relative norm difference relative norm difference beta beta plot state markov chain various values discount parameter generated algorithm 
averaged independent runs 
note large higher variance larger values error bars standard deviation 
tested performance range values argument uses determine sign inner product gradient search direction need run 
de iterations parameter follows 
initially somewhat arbitrarily value set value called value 
obtain estimate gradient direction 
desired search direction doubled called generate new estimate procedure repeated doubled times 
negative process iterations searched local maximum direction number doubled iteration direction generated overly noisy estimates 
shows average reward final controller produced function total number simulation steps underlying markov chain 
plots represent average independent runs note average reward optimal policy 
parameters controller uniformly randomly initialized range relative norm difference beta beta beta beta graph showing final bias estimate measured function state markov chain 
generated algorithm 
note axes log scales 
call call average reward resulting controller computed exactly calculating stationary distribution controller 
optimality reliably achieved approximately iterations markov chain 
training directly line controller trained line algorithm fixed step sizes reducing step sizes form tried caused slow convergence 
shows performance controller measured exactly previous section function total number iterations markov chain different values step size graphs averages runs controller weights randomly initialized range start run 
convergence optimal order magnitude slower achieved best step size step sizes greater failed reliably converge optimal policy 
puck world section experiments described train hidden layer neural network controllers navigate small puck final reward markov chain iterations performance state markov chain controller trained function total number iterations markov chain 
performance computed exactly stationary distribution induced controller 
average reward optimal policy 
averaged independent runs 
error bars computed dividing results separate bins depending mean computing standard deviation bin 
average reward average reward markov chain iterations markov chain iterations average reward average reward markov chain iterations markov chain iterations performance state markov chain controller function number iteration steps line algorithm algorithm fixed step sizes error bars computed 
dimensional world 
world puck unit radius unit mass section cylinder constrained move plane region units square 
puck internal dynamics rotation 
collisions region boundaries inelastic tunable coefficient set experiments reported 
puck controlled applying unit force positive negative direction unit force positive negative direction giving different controls total 
control changed second simulator operated granularity second 
puck force due air resistance speed friction puck ground 
puck reward decision point second equal distance puck designated target point 
encourage controller learn navigate puck target independently starting state puck state reset simulated seconds random location random velocities range time target position set random location 
note size state space example essentially infinite order precision precision floating point precision machine bits 
controller hidden layer neural network input nodes hidden nodes output nodes generate probabilistic policy similar manner controller state markov chain example previous section 
inputs set raw locations velocities puck current time step differences puck location target location respectively 
location inputs scaled lie velocity inputs scaled speed units second mapped value hidden nodes computed squashing function output nodes linear 
hidden output node usual additional offset parameter 
output nodes exponentiated normalized markov chain example produce probability distribution controls units thrust direction units thrust direction 
controls selected random distribution 
conjugate gradient ascent trained neural network controller gradient estimates generated experimentation chose parameters supplied value scheme discussed section determine number iterations call due saturating nature neural network hidden nodes exponentiated output nodes tendency network weights converge local minima infinity 
weights grow rapidly early simulation suboptimal solution 
large weights tend imply small gradients network stuck suboptimal solutions 
observed similar behaviour training neural networks pattern classification problems 
fix problem subtracted small quadratic penalty term performance estimates small correction gradient calculation decreasing schedule quadratic penalty weight arrived experimentation 
initialized tenth eration performance improved value iterations ago reduced factor 
schedule solved nearly local minima problems expense slower convergence controller 
plot average reward neural network controller shown function number iterations graph average independent runs parameters initialized randomly range technique capacity control pattern classification technique goes name weight decay 
condition optimization problem 
average reward iterations performance neural network puck controller function num ber iterations puck world trained performance estimates generated simulating iterations 
averaged independent runs excluding bad runs 
start run 
bad runs shown omitted average gave misleadingly large error bars 
note optimal performance neural network controller class problem due fact puck target locations reset simulated seconds fixed fraction time puck away target 
see final performance puck controller close optimal 
runs get stuck suboptimal local minimum 
cases caused overshooting see prevented adding extra checks illustrates behaviour typical trained controller 
purpose illustration target location puck velocity randomized seconds puck location 
call admission control section report results experiments applied task training controller call admission problem treated chapter 
average reward iterations plots performance neural network puck controller runs converged substantially suboptimal local minima 
target illustration behaviour typical trained puck controller 
problem call type bandwidth demand arrival rate average holding time reward table parameters call admission control problem 
call admission control problem treated chapter models situation telecommunications provider wishes sell bandwidth communications link customers way maximize long term average reward 
specifically problem queuing problem 
different types call call arrival rate bandwidth demand average holding time arrivals poisson distributed holding times exponentially distributed 
link maximum bandwidth units 
call arrives sufficient available bandwidth service provider choose accept reject call available bandwidth call rejected 
accepting call type service provider receives reward units 
goal service provider maximize long term average reward 
parameters associated call type listed table 
settings optimal policy dynamic programming accept calls type assuming sufficient available bandwidth accept calls type available bandwidth 
policy average reward accept policy average reward controller controller parameters type call 
arrival call type controller chooses accept call probability currently bandwidth 
class controllers studied 
conjugate gradient ascent gradient estimates range values influence train controller generating performance trained controllers marginal set gave discrepancy average rewards quoted 
probably due discrepancy way state transitions counted clear discussion 
final reward class optimal beta total queue iterations performance call admission controller trained function total number iterations queue 
performance computed simulating controller iterations 
average reward globally optimal policy average reward optimal policy class plateau performance graphs averages independent runs 
lowest variance estimates 
value calls varied controller started parameter setting done 
value initial policy graph average reward final controller produced function total number iterations queue shown 
performance reliably achieved large iterations queue 
note optimal policy achievable controller class incapable implementing threshold policy accept reject policies 
provably optimal parameter setting suitably large values chose generates close optimal policy controller class average reward shows probability accepting call type policy function available bandwidth 
controllers produced sufficiently essentially accept controllers average reward optimum achievable class 
produce policies nearer optimal policy performance keep close starting value gradient estimate produced acceptance probability call type call types available bandwidth probability accepting call type call admission policy near optimal parameters note calls type essentially accepted 
relatively small component 
shows plot normalized function sufficiently large ensure low variance starting parameter setting starts high value explains produces accept controllers negative value variance large shown moderately relatively high 
plot performance 
approximately half remaining performance obtained setting sufficiently large choice gives remaining performance 
problem huge difference gaining optimal performance achieved iterations queue gaining optimal requires order queue iterations 
similar convergence rate final approximation error case reported line algorithms chapter results run case 
mountainous puck world mountain car task studied problem reinforcement learning literature example 
shown task drive car top dimensional hill 
car powerful accelerate directly hill gravity successful controller learn oscillate back forth normalized delta delta delta delta beta plot components call admission problem function discount parameter parameters set set note negative correct sign final reward class optimal beta total queue iterations final reward class optimal beta total queue iterations performance call admission controller trained function total number iterations queue 
performance calculated simulating controller iterations 
graphs averages independent runs 
classical mountain car task apply forward reverse thrust car get crest hill 
car starts bottom power drive directly hill 
builds speed crest hill 
section describe variant mountain car problem example section 
problem task navigate puck valley plateau northern valley 
mountain car task puck sufficient power accelerate directly hill learn oscillate order climb valley 
able reliably train near optimal neural network controllers problem generating gradient estimates 
world world dimensions physics puck dynamics controls identical flat puck world described section puck subject constant gravitational force units maximum allowed thrust units height world varied follows height units thrust unit mass puck accelerate directly valley 
simulated seconds puck initialized zero velocity bottom valley random location 
puck reward variant mountain car problem task navigate puck valley northern plateau 
puck starts bottom valley power drive directly hill 
valley southern plateau reward northern plateau speed puck 
speed penalty helped improve rate convergence neural network controller 
controller experimentation neural network controller reliably trained navigate northern plateau stay northern plateau difficult combine controller surprising tasks quite distinct 
overcome problem trained switched neural network controller puck controller valley southern plateau switched second neural network controller northern plateau 
controllers hidden layer neural networks input nodes hidden nodes output nodes 
inputs normalized valued puck locations normalized locations relative center northern wall puck velocities 
outputs generate policy fashion controller section 
average reward iterations performance neural network puck controller function num ber iterations mountainous puck world trained performance estimates generated simulating iterations 
averaged independent runs 
conjugate gradient ascent switched neural network controller trained scheme discussed section time discount factor set plot average reward neural network controller shown function number iterations graph average independent runs neural network controller parameters initialized randomly range start run 
case run failed converge near optimal performance 
see puck performance nearly optimal total iterations puck world 
may high put perspective note random neural network controller takes iterations reach northern plateau standing start base valley 
iterations equivalent trips top random controller 
note puck converges final average performance indicates spending time northern plateau 
observation puck final behaviour shows behaves nearly optimally terms oscillating back forth get valley 
showed performance gradient estimates generated algorithm optimize average reward parameterized optimization relies robust line search algorithm uses gradient estimates value estimates bracket maximum :10.1.1.125.2542
perform quite distinct problems optimizing controller state optimizing neural network controller navigating puck dimensional world optimizing controller call admission problem optimizing switched neural network controller variation classical mountain car task 
line version state call admission problems able provide graphic illustrations bias variance gradient estimates traded varying low variance high bias high variance low bias 
relatively little tuning required generate results 
addition controllers operated direct simple representations state contrast complex representations usually required value function approaches 
interesting avenue research empirical comparison value function methods algorithms domains known produce results 
despite success experiments described line algorithm advantages settings 
particular applied multi agent reinforcement learning gradient computations parameter updates performed distinct agents communication global distribution reward signal 
idea led biologically plausible parameter optimization procedure spiking neural networks see currently investigating application line algorithm multi agent reinforcement learning problems 
bartlett baxter 
preparation 
september 
baxter bartlett :10.1.1.125.2542
direct gradient reinforcement learning gradient estimation algorithms 
technical report research school information sciences engineering australian national university july 
baxter tridgell weaver :10.1.1.125.2542
learning play chess temporal differences 
machine learning 
appear 
bertsekas tsitsiklis 
neuro dynamic programming 
athena scientific 

cao 
wan 
algorithms sensitivity analysis markov chains potentials perturbation realization 
ieee transactions control systems technology 
fine 
feedforward neural network methodology 
springer new york 
kimura miyazaki kobayashi 
reinforcement learning pomdps function approximation 
fisher editor proceedings fourteenth international conference machine learning icml pages 

simulation methods markov decision processes 
phd thesis information decision systems mit 
tsitsiklis 
simulation optimization markov reward processes 
technical report mit 
samuel 
studies machine learning game checkers 
ibm journal research development 
singh bertsekas 
reinforcement learning dynamic channel allocation cellular telephone systems 
advances neural information processing systems proceedings conference pages 
mit press 
sutton :10.1.1.132.7760
learning predict method temporal differences 
machine learning 
sutton barto :10.1.1.32.7692
reinforcement learning 
mit press cambridge ma 
isbn 
tesauro 
practical issues temporal difference learning 
machine learning 
tesauro 
td gammon self teaching backgammon program achieves master level play 
neural computation 
weaver baxter 
reinforcement learning state temporal differences 
technical report department computer science australian national university may 
anu edu au jon papers std full ps gz 
williams :10.1.1.129.8871
simple statistical gradient algorithms connectionist reinforcement learning 
machine learning 
zhang dietterich 
reinforcement learning approach job shop scheduling 
proceedings fourteenth international joint conference artificial intelligence pages 
morgan kaufmann 

