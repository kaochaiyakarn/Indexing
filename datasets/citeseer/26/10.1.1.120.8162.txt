semantic extraction named entities diana maynard bontcheva hamish cunningham dept computer science university sheffield sheffield dp uk diana dcs shef ac uk discuss new challenges posed progression information extraction content extraction demonstrated ace program 
explore traditional approaches sufficient describe adaptation generic system kind application 
results suggest deeper level processing necessary achieve excellent results areas rule systems produce results reasonable quality small amount adaptation 
particular task entity detection tracking texts varying genre quality challenging 
government initiatives muc saic tipster arpa paved way development current information extraction systems 
short space time systems able recognise named entities precision recall scores th percentile narrow domains newswires terrorist attacks 
challenge adapting systems new domains extended tasks template filling question answering summarisation 
years demand growing commercial applications perform tasks 
main consequences systems need adaptable portable second level detail analysis important 
longer able recognise classify text string level order classified entities need able recognise semantic level 
systems need robust able deal issues degraded texts perfect transcriptions correct spelling punctuation grammar 
interesting challenge existing methods information extraction sufficient deal new demands deeper processing techniques full syntactic semantic parsing deeper forms knowledge dictionaries ontologies pragmatics real world knowledge necessary 
discuss new challenges posed systems ace automatic content extraction program describe generic system adapted task 
discuss extent system modified order perform deeper level analysis necessary advent challenges means new techniques may necessary 
findings reveal adaptation ace task relatively straightforward order gain equivalent scores achieved muc current techniques may sufficient 
ace program ace program began september administered nsa nist cia 
designed program develop technology extract characterise meaning human language 
formal evaluations ace algorithm performance held approximately month intervals open sites wish participate results evaluations closed 
reason publish details internal evaluations official scores 
ace includes entity detection tracking edt relation detection characterisation rdc 
edt broadly comparable muc named entity ne task rdc broadly comparable muc template elements task ace tasks challenging muc 
shall limit discussion edt task inclusion rdc task ace supports claim complexity content extraction 
main objective ace produce structured information entities events rela tions 
program aims encourage powerful new generation robust retargetable nlp applications promoting faster system development richly annotated corpora 
aims promote design general purpose linguistic resources development general purpose standalone systems 
potential uses ace output include precise forms information retrieval data mining development large knowledge bases automatic large scale annotation semantic web 
data scope ace program broader muc texts varying source quality ranging standard newswires degraded texts produced automatic speech recognition asr optical character recognition ocr output 
ace focuses core extraction task asr ocr algorithms low quality texts provide unique challenge systems 
entity detection tracking edt edt set tasks involves detecting unique entity specified types mentioned source text tracking mentions 
mentions entity form name description pronoun recognised classified entity 
recognition subtasks edt entities person organization location facility gpe entity attributes type name nominal pronominal entity mentions entity tracking mention roles mention optional role associated person organization location gpe mention extents detection np span geo political entity essentially kind location government city country information vs content extraction reason popularity muc may high scores achievable particularly named entity task systems typically able achieve scores th percentile 
meant soon incentive sites push systems new developments particularly comparing muc scores soon gold standards system evaluation 
ace program designed partly fulfil need challenge new advances obtained development robust systems capable fast adaptation new tasks deeper analysis language 
deeper analysis main differences ace muc muc ne task dealt linguistic analysis text ace deals semantic analysis 
muc ne task tagged selected segments text text represented name entity 
ace names viewed mentions underlying entities 
main task detect infer entities selected attributes shown 
entity detection output form unique id entity set entity attributes information way entity mentioned document id mention level name nominal pronominal mention head mention extent 
example entity output shown 
varied text muc texts related specific domain comprised texts particular source type ace news texts encompass wide variety domains sport politics business religion popular culture cover differing genres styles broadcast news newspapers 
may trivial practice high impact results obtained maynard 
human annotation difficulty level edt task confirmed experience human annotators 
muc ne task human annotators typically id unique id key type person organization gpe location facility class generic specific level name nominal pronoun origin database corpus subtype country city continent asia europe countries country egypt australia cities names titles mentions entity attributes entity id ft airlines jul generic false entity type organization entity mention id type name string national air traffic services entity mention entity mention id type name string nats entity mention entity mention id type pro string entity mention entity mention id type name string nats entity mention entity sample entity output achieve precision recall marsh 
experiment conducted bbn team annotators experienced ace style annotation achieved results corpus words worth value scoring false alarms misses substitutions 
adapting generic ne system content extraction section describe development mace system adapted annie system available part gate cunningham 
robust design flexible architecture gate annie meant tuning system deal multiple domains text types necessary ace straightforward 
contrast adaptation linguistic semantic analysis text involved substantial effort terms understanding complexities task programming time skill 
system overview annie mace consists number processing resources run sequentially sentence splitter part speech tagger gazetteer lookup jape semantic grammar orthographic 
modified gazetteers semantic grammar orthographic modules added new modules perform genre identification pronominal coreference nominal coreference 
added switching controller mechanism enable automatic selection different processing resources 
modules described section 
named entity detection relatively new program training data available ace muc manual annotation time consuming 
respect traditional rule systems advantage machine learning techniques bbn identifinder bikel require large quantities annotated data 
hand rule systems suffer problems 
entity types different 
annie recognises standard muc entity types person location organisation personal communication date time money percent plus additional entity types address identifier 
ace additional types facility subsumes entities usually considered organisations locations gpe subsumes entities usually person location organisation 
entails lumping splitting standard entity types means rules completely rewritten 
example annie rule recognises locations appear gazetteer lists rewritten mace certain types locations rivers mountains continents remain locations cities countries tagged 
need broken subtypes role assignment 
means annie rule separate rules mace 
modular nature gate architecture relatively straightforward adapt processing resources grammar gazetteer lists firstly procedural declarative knowledge gate kept separate secondly processing resources foreground background information largely distinguished 
means background knowledge required tokenisation name matching remain untouched foreground information specific domain application needs modified 
example changes specific parts grammar ensuring remaining parts unaffected 
enormous benefit reducing time effort spent adapting system new application 
semantic grammars written jape cunningham consist phases run sequentially compiled finite state transducers 
annie ne recognition grammar contains phases total rules annotation types average rules entity type 
mace grammars consist phases total rules entity types average rules entity type 
experienced jape user may able write dozen new rules minutes number new rules significant increased complexity new mace rules important 
org gpe loc fac name nom pro table ace application entity tracking entity tracking part edt task requires detection coreference pronominal nominal entity mentions coreference proper names 
theory pronominal nominal entities exist right named mention entity practice nominal entities fairly rare pronominal entities extremely rare 
tracking name mentions handled annie module modifications 
tracking nominal pronominal mentions handled new coreference modules 
shall discuss resources due space restrictions details ref 

evaluation differences way evaluation carried muc ace 
muc uses known metrics information retrieval precision recall measure 
measure balance precision recall adjusted usually equally weighted gives single percentage easy compare scores different systems 
ace uses cost scoring metric favours certain entity types potentially biased different types error 
detailed discussion metrics cost algorithm see ref 
withheld 
ace application parameters applied entity type level chosen perceived importance shown table 
cost parameters false alarm error equal 
aside metric major factors related task definitions impact scores muc ace mean directly compared 
way coreference handled directly affects score 
muc coreference treated separate task ne recognition scored separately 
means entities correctly identified classified missed generate perfect score ne task 
ace entity tracking essentially coreference integral part entity recognition task 
separate score entity mention recognition subtasks related entity mentions recognised related entity spurious entity generated negatively affect entity score 
say strings text bush george bush system correctly identifies person entity fails coreference 
muc get perfect score ne recognition get score coreference 
ace generating entity mentions generate entities mention 
get perfect score entity recognition spurious entity generated counts 
ace poor coreference resolution lower entity score 
muc scores coreference task quite low explains coreference impact results ace 
second factor impact metonymy problematic entities 
muc issues allowing multiple possible entity types optional annotations case unclear 
getting possibilities correct sufficient 
ace correct answer cases unclear leeway different interpretation official annotators 
results mace system ace evaluation closed release official figures ace score 
internal evaluations ace data precision recall gave figures precision recall newswire texts 
performed experiments muc data order get system performs comparison 
minor modifications system order recognise different entity types muc different guidelines example muc deal metonymy ace 
score system dates numbers previous experiments system ref 
shown text ace muc table comparison results ace muc texts annie system mace system text ace news table comparison results ace news texts achieve near perfect precision recall types corresponding entities ace 
results evaluation detailed table 
note scores muc texts enamex 
scored timex nu mex results improved 
best systems muc achieved mid tuning domain achieved higher score original system entered muc ref 
mace tuned task domain achieved higher score 
compared default annie system mace system set news texts news official ace texts ace 
news texts blind test set articles business news taken web similar annie system trained 
ace texts broadcast news texts september ace evaluation blind set 
table shows results precision recall measure 
aim see improvement mace system produced annie ace mace system worked muc style annotations news 
ran systems modifications example mace systems recognised news locations 
responses measured key response annotation type include cases system produced gpe annotations key gpe annotations running ace results news texts 
ace mace system achieved fmeasure compared annie system 
initial adaptation took approximately person weeks excluding new modules coreference system improved months second ace evaluations adding nominal coreference module rules lists 
news annie system achieved measure mace achieved 
main reason greater difference score systems news ace mace ways specific system designed specifically ace flexible multi purpose system 
discussed earlier difference semantics location entities systems caused great deal error running systems non respective texts 
ran experiment modified mace grammars transform gpe entities locations reran experiment news 
scores increased precision recall giving measure realistic measure differences systems 
results show substantial improvements baseline system relatively short space time account quite substantial differences requirements systems 
mace system described entirely rule contrast majority current systems combine rule learning mechanisms 
bikel borthwick day 
rulebased systems perform equivalent levels mixed case text bikel rule construction generally time consuming 
hand learning mechanisms hidden markov models hmms generally require large volumes training data 
mace demonstrates system underlying architecture designed effort needed adaptation generic system necessarily expensive 
debate remains open intrinsic superiority particular method intricate language pro cessing tasks ace program 
currently experimenting adding approach hmms top existing rule system preliminary results suggest bring improvements score 
ne tasks muc clear winner tasks emerging difficult humans bring new surprises certainly new challenges 
clearly single score single evaluation prove system superiority 
real world factors considered speed processing ease adaptability portability new unknown languages domains text types play important role 
user important factors may system performance ease minimally trained users adapt system needs assistance developers 
arpa advanced research projects agency 
proceedings tipster text program phase ii 
morgan kaufmann california 
bikel bikel schwartz weischedel 
algorithm learns name 
machine learning special issue natural language learning feb 
borthwick borthwick sterling agichtein grishman 
description mene named entity system muc 
proceedings muc conference nyu 
cunningham cunningham maynard bontcheva tablan 
gate framework graphical development environment robust nlp tools applications 
proceedings th anniversary meeting association computational linguistics 
cunningham cunningham maynard bontcheva tablan 
gate user guide 
gate ac uk 
day day robinson vilain yeh 
mitre description alembic system muc 
proceedings seventh message understanding conference muc 
marsh marsh 
muc evaluation technology overview results 
proceedings seventh message understanding conference muc 
www itl nist gov related projects muc index html 
maynard maynard tablan cunningham saggion bontcheva wilks 
architectural elements language engineering robustness 
journal natural language engineering special issue robust methods analysis natural language data 
saic saic 
proceedings seventh message understanding conference muc 
www itl nist gov related projects muc index html 
