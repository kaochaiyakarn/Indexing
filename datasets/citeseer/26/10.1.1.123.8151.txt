ieee transactions pattern analysis machine intelligence vol 
january statistical pattern recognition review ani jain fellow robert duin mao senior member primary goal pattern recognition unsupervised classification 
various frameworks pattern traditionally formulated statistical approach intensively studied practice 
neural network techniques methods imported statistical learning bean receiving increasing attention 
design recognition system requires attention issues definition pattern classes sensing environment pattern representation feature extraction selection cluster analysis classifier design learning selection training test samples performance evaluation 
spite years research development field general problem recognizing complex patterns arbitrary orientation location scale remains unsolved 
new emerging applications 
data mining web searching retrieval multimedia data face recognition cursive handwriting recognition require robust pattern recognition techniques 
objective review summarize compare known methods various stages pattern recognition system identify research topics applications forefront exciting challenging field 
index terms statistical pattern recognition classification clustering feature extraction selection error estimation classifier combination 
neural networks 
time years old children recognize digits letters 
small characters handwritten machine printed rotated easily young 
characters map written cluttered background ir partially occluded 
take ability fnr wc task teaching machine 
pattern thc study haw machines environment learn distinguish patterns background sound reasonable decisions patterns 
jn spite years research design general recognizer remains elusive goal 
best pattern instances humans yr understand humans recognize patterns 
ross emphasizes simon central finding pattern critical decision making tasks relevant patterns disposal better decisions 
hopeful news proponents artificial computers surely taught patterns 
successful 
computer programs help banks score credit applicants help doctors diagnose disease help pilots land airplanes depend way pattern recognition 
need pay explicit teaching pattern recognition 
goal introduce pattern recogni tion best way utilizing available sensors processors domain knowledge decisions 
pattern 
automatic grouping patterns important problems variety engineering scientific disciplines rs biology psychology medicine marketing computer vision artificial aid remote sensing 
pattern 
watanabe defines pattern chaos entity vaguely defined 
fingerprint image cursive word human face ui speech signal pattern recognition classi may consist tasks supervised classification discriminant analysis input pattern identified member predefined 
class classification clustering pattern assigned hitherto unknown class 
recognition problem posed ns classification tion task classes defined system designer supervised classification similarity patterns classification 
interest area pattern recognition duc ko applications challenging computationally demanding see table 
applications include data mining identifying pattern correlation outlier millions document classification efficiently searching text documents financial forecasting organization retrieval multimedia biometrics personal identification statistical pattern recognition review various physical attributes hcc aiid fingerprints 
picard application pattern called affective computing give computer ability aiid emotions respond tr human emotion employ contribute making 
number applications available typically arc usually suggested domain extracted optimized data driven procedures 
rapidly growing available computing power enabling faster processing nf hug dah facilitated ot elaborate methods data analysis classification 
time demands automatic pattern recognition rising dire availability databases stringent speed accuracy mid cost 
emerging applications single approach classification optimal multiple methods 
combining sensing classifiers pattern recognition 
thc pattern recognition system tlic aspects data acquisition aiid pi data representation mid decision making 
problem domain thc choice preprocessing scheme making model 
tt table examples pattern recognition applications defined sufficiently recognition pro small variations large pattern simple making 
set nf training set important desired systems 
best known approaches pattern recognition matching statistical classification syntactic struc tural matching neural networks 
arc independent pattern recognition method exists different 
attempts design hybrid involving multiple models 
brief description comparison summarized iin table 
template matching nf earliest approaches pattern template matching 
matching operation pattern recognition tlw similarity entities curves shapes type 
tn template matching template typically il shape pattern recognized available 
recognized matched stored template allowable translation rotation scale changes 
hc similarity coi may available training sct 
template learned training sct 
template matching computationally dc availability faster processors ieee transactions pattern analysis machine intelligence vol 
january table pattern recognition models iei typical 
ai cl approach feasible 
rigid template matching domains number disadvantages 
instance fail patterns duc imaging viewpoint change large variations tlie pat terns 
deformable template models match patterns thc easily explained directly 
statistical approach thc statistical approach represented terms ri ements viewed hs point dimensional space 
tho goal choose features allow pattern vectors belonging different ies occupy compact disjoint regions rl feature space 
hc space feature sct determined fi om different bc separated set training patterns class objective establish decision boundaries feature space separate patterns different classes 
statistical decision theoretic approach decision boundaries probability distribu ions tlie patterns belonging class musk specified learned 
discriminant analysis classification form tlie decision boundary linear quadratic specified best decision boundary lhc specified form classification patterns 
cart constructed example squared error criterion 
hc direct boundary approaches supported philosophy pr amount solving problem try solve problem directly general problem intermediate step 
possible available information sufficient direct solution solving general intermediate problem 

hp cr ii iim ic tu error syntactic approach problems involving patterns appropriate adopt viewed composed simple ace built simpler 
simplest terns bc called thc complex pattern represented jn terms ships primitives 
syntactic recog nition formal analogy drawn tlie structure patterns syntax 
patterns arc viewed sentences language primitives arc viewed alphabet cif language sentences generated grammar 
complex patterns described small number primitives rules 
lhc pattern class inferred available 
structural pattern recognition intuitively appealing addition classification approach provides description pattern constructed primitives 
paradigm patterns definite structure bc captured terms set textured analysis contours 
implementation syntactic approach tics primarily haw noisy patterns detect primitives inference training data vu notion cif attributed grammars syntactic statis tical pattern recognition 
approach yield combinatorial explosion possibilities investigated large training computational efforts 
neural networks neural networks bc 
viewed massively parallel computing systems consisting large simple processors 
neural network models principles learning adap tivity fault tolerance statistical pattern recognition review computation network weighted graphs thc arc artificial neurons directed edges weights connections 
main characteristics neural networks ability learn complex nonlinear input output relationships training adapt tlic data 
commonly family neural networks pattern classification tasks lhc fwd forward network perceptron radial basis function 

networks organized layers unidirectional connections layers 
popular network self organizing map kohonen network mainly dah mapping 
learning process involves updating network network efficiently perform specific classification clustering task 
lie increasing network models solve recognition primarily due seemingly low dependence domain specific knowledge relative approaches due io availability efficient learning algorithms practitioners usc 
neural networks provide new suite nonlinear algorithms tor hidden layers classification multilayer 
add classification algorithms mapped network architectures efficient hardware 
different underlying principles thc wellknown neural network models implicitly equivalent similar classical statistical pattern recognition methods scc 
anderson discuss relationship neural networks statistical pattern recognition 
anderson point arc statistics 
nns conceal statistics user 
sc offer advantages unified approaches feature extraction classification flexible finding moderately nonlinear solutions 
scope remainder primarily review statistical pattern representation classification 
appropriate discuss closely algorithms neural networks literature 
omit body li classification clustering 
thc written books fuzzy pattern recognition ic 
tlic st tho various approaches methods summarized tables easy quick reader 
due space constraints able provide wc omit approaches associated 
goal emphasize approaches extensively demonstrated bc useful new trends ideas 
literature pattern recognition vast scattered numerous applied statistics learning neural works signal image processing 
quick scan table contents issues kee pattern md machine nce ik publication january reveals approximately papers deal recognition 
approximately papers covered statistical approach broadly subtopics dimensionality dimen reduction classifier design classifier lo error classification 
addition textbooks duda yl devroye bishop ripley point excellent survey papers written nagy ill kanal 
nagy described early roots pattern recognition shared researchers artificial intelligence 
large part ol nagy number potential applications pattern recognition feature tion application domain knowledge 
ile emphasized thc techniques polynomial discriminant functions potential similar kernel functions 
ey time kanal wrote survey papers hall books recognition 
placed emphasis applications modeling design pattern recognition systems 
chc discussion automatic feature various class functions thc result ing error bounds 
kanal review contained large section methods pattern 
state pattern recognition field nagy kanal thc mid today number commercial pattern recognition systems available individuals buy personal usc printed isolated word 
possible various technological developments resulting availability inexpensive sensors top computers 
tlw field pattern recognition large review skip detailed descriptions various applications thc knowledge structural pattern recognition sys tems 
hc starting point review basic elements statistical methods pattern recognition 
apparent tion real world objects choice strongly influences classification results 

stork 
ieee transactions pattern analysis machine intelligence vol 
january topic probabilistic distance cur rently important ago cs te func high dimensional feature spaces 
thc procedures resulting accuracy large 
curse rs ss danger thu consequences complex classifier 
stood problems extent bc circumvented regularization bp resolved proper design classification procedures 
study machines svms discussed section largely real world arc high dimensional tion linear subspaces 
nonlinear procedures subspace popular section md building classifiers 
neural networks offer powerful tools purposes 
widely accepted wil solve classification 
arc admissible ap proaches capable patterns certain feature 
tlic combination classifiers ii heavily topic section 
approaches thc error classifier section 
thc topic classification section 
section frontiers nf pattern recogn lion 
goal parts bc field fig 
model pattern recognition 
table links statistical neural network recognition 
purpose haw included il illustrate va algorithms 
realize due space limitations haw able introduce concepts 
places rely background knowledge bc available 
statistical pattern recognition statistical recognition successfully design systems 
tn pattern pattern set features attributes viewed feature vector 
known concepts decision zed orn classes 
system operated modes training learning sting see 

role module pattern background remove normalize pa 
operation defining compact representation pattern 
training mode feature extraction module finds thc appropriate features input patterns classifier trained partition feature space 
path allows optimize feature extr ion strategies 
mode hind classifier assigns thc input pattern thc classes features 
test fc preprocessing pattern cat sc training preprocessing extraction pattern statistical pattern recognition review thc decision making process statistical pattern supervised nonparametric method constructs recognition summarized follows pattern decision boundary 
tu bc assigned categories wa 
dichotomy statistical pattern recognition feature xr 
thu learning labeled training features dre assumed haw probability density mass unsupervised learning unlabeled training sam features continuous ples 
label oc training pattern represents function conditioned pattern class 
category 
vector belonging class viewed classes observation drawn randomly class conditional structure class 
probability function 
number known various appear statistical decision rules including bayes decision rule shown tree structure fig 

maximum rule bc traverse tree top arid left right particular bayes rule thc neyman pearson available rule available define decision boundary 
result difficulty classification problems 
optimal bayes decision rule minimizing thc risk approaches statistical expected value loss function stated pattern recognition leaf nodes tree fig 
follows assign input pattern class wi attempting implement tlic bayes decision rule 
conditional risk field analysis deals decision making problems nonparametric learning mode 
analysis categories clusters may minimum wf uj loss incurred deciding ut true class uj probability mi 
case loss conditional risk conditional misclassification 
specified task discover reasonable data exists 
analysis algorithms various visualizing projecting data referred dah methods 
dichotomy statistical recognition bc decision arc obtained directly geometric approach probabilistic density approach shown fig 

choice loss function bayes decision rule simplified follows maximum probabilistic approach requires density functions construct posteriori map rule assign input pattern class wi functions specify decision boundaries 
hand geometric approach constructs decision directly optimizing certain cost various strategies arc utilized design classifier functions 
wc point certain statistical pattern recognition depending kind assumptions density functions approaches information available class conditional densities 
equivalent 
sec category class conditional densities 
optimal hayes decision rule matter classification decision rule design classifier 
class conditional trained training samples 
densities usually known practice result classifier depends learned available training patterns 
form available training samples thc class conditional densities known multivariate values samples 
time thc goal gaussian parameters densities designing recognition system lo classify test mean covariance matrices arc un samples arc different training known wc parametric decision problem 
samples 
optimizing classifier common strategy kind problem performance training set may result thc unknown density functions desired set 
generalization ability values resulting bayes plug classifier refers performance classifying test classifier 
optimal bayesian strategy situation patterns thc stage 
requires additional information form prior poor generalization ability classifier attributed distribution thc unknown parameters 
form factors number features thc class conditional densities known relative number training curse operate nonparametric 
case dimensionality unknown density function window parameters classifier approach directly construct decision boundary polynomial classifiers large neural training data nearest neighbor rule 
classifier intensively fact multilayer perceptron training set analogous eee transactions analysis machine intelligence 
vol 

january bayes decision theory known class conditional supervised learning parametric nonparametric density approaches fig 

various approaches pattern recognition 
phenomenon overfitting regression free parameters 
overtraining investigated theoretically classifiers minimize apparent error training set 
classical studies vapnik classifier capacity provide understanding mechanisms overtraining 
complex having independent may large capacity able represent dichotomies dataset 
measure capacity thc vapnik chervonenkis vc 
results bc prove interesting ties example certain classifiers see devroye 
practical results classifier complexity initially limited proposed bounds required number training samples wcre conservative 
jn support vector machines results proved quite useful 
thc pitfalls adaptation estimators thc set observed stages pattern recognition dimensionality reduction density estimation classifier design sound independent set evaluation 
order necessity having test sets estimators arc rotated subsets data preserving different parts data optimization evaluation 
examples arc optimization parzen kernel unknown decision 
geometric approach unsupervised learning parametric nonparametric resolving analysis discriminant analysis hl thc bootstrapping hir designing classifiers error estimation 
classification meth ods bc illustrated simple experiments data sets dataset artificial dataset consisting classes 
gaussian density para meters intrinsic overlap densities 
dataset iris dataset consists patterns classes patterns iris setosa iris iris virginica 
dataset digit dataset consists handwritten numerals collection dutch utility maps 
patterns class total available form binary 
characters represented terms feature sets fourier coefficients character 
profile correlations 
karhunen coefficients 
pixel averages windows 
zernike moments 
morphological features 
statistical pattern recognition review details dataset 
imum discrimination classes 
experiments cif parameter densities patterns cor various subs remaining ml 
training 
trunk considered thc twr refer digit dataset just loeve item meant stated 
curse dimensionality peaking phenomena performance classifier depends tlic sample sizes features classifier complexity 
naive table partitioning feature space cells associating class label cell training data points function thc dimension 
phenomenon termed curse dimensionality peaking scc discussion design 
known probability misclassification decision rule number increases long class conditional densities completely known equivalently thc number training arbitrarily large representative oc thc 
observed practice added features degrade classifier training samples design ho classifier small number features 
paradoxical behavior thc 
peaking phenomenon 
simple frr follows commonly parametric classifiers estimate thc unknown parameters plug true parameters note features arc statistically discriminating power lhc successive features monotonically providing max thu mean known 
situation optimal ap rule loss function tu construct decision boundary 
probability error function cm easy pt 
words perfectly discriminate arbitrarily increasing features 
mean vector labeled training samples arc available 
trunk maximum likelihood fi thc 
plug decision fi ni optimal decision 
probability error function written tlic class coi densities 
sample size trunk showed 
cl re implies probability error approaches tlw class problem 
number features demonstrates case arbitrarily number unknown parameters number features reliability decreases 
class conditional densities arc estimated il performance resulting plug classifiers number training samples 
practical implication fixed sample may degrade thc system designer number features 
try small number salient features trunk provided example lo illustrate confronted wi th limited training set 
curse dimensionality 
class classification problem equal commonly classifiers including multilayer feed forward networks suffer curse oe prior dimensional multivariate dimensionality 
relationship thc sian covariance foi 
class 
mean vectors tlw classes misclassification number training samples true thc class conditional densities difficult establish jd guidelines suggested ratio tlic sample size dimensionality 
accepted teen times training samples class number practice follow classifier 
complex thc larger ratio sample size dimensionality bc avoid curse 
dimensionality reduction 
hc university machine vw ci ii ml rn mll arc main reasons keep html 
ci thc rest ut nut cn tlw cul sc pattern representation number thc pt 
small possible measurement cost ieee transactions pattern analysis machine intelligence vol 
january accuracy 
limited set simplifies pattern classifiers built selected representation 
consequently classifier faster memory 
earlier small number alleviate curse ne number training samples limited 
lhc hand reduction thc may lead thc power lower accuracy resulting rr system 
watanabe supports need careful choice thc features possible arbitrary patterns encoding sufficiently large number redundant features 
important distinction selection feature extraction 
thc term selection refers algorithms hopefully best input feature set 
create new combinations original set called extraction algorithms 
feature selection extraction literature 
note feature extraction precedes selection features extracted data princi pal component discriminant analysis low discrimination ability arc discarded 
choice selection feature depends thc application domain specific training data available 
feature savings cost features discarded arid selected original physical interpretation 
add retained features may important physical process generates thc 
hand transformed features generated provide butter discriminative ability thc best features new features linear nonlinear combination features may haw physical meaning 
situations useful obtain projection multivariate data ri pattern matrix permit visual examination data 
graphical techniques exist visually obser multivariate data objective exactly pattern picture degrees freedom features 
example represents cartoon face facial characteristics nose mouth eye size correspond individual features 
fig 
shows faces corresponding mean vectors iris setosa iris iris virginica classes tlie iris data patterns class 
note face associated iris setosa looks quite faces implies setosa bc separated dimensional feature space evident plots data fig 

main issue dimensionality reduction criterion function 
commonly criterion classification error 
thc classification error reliably estimated ratio size tlie features small 
addition ch function wt 
determine appropriate dimensionality tlic 
answer question embedded thc notion tlw intrinsic data 
intrinsic dimensionality essentially determines tlic rl described adequately rl subspace dimensionality pnr patterns reasonably smooth curve intrinsic value note intrinsic dimensionality tlic thr 
property data involving tlic significant eigenvalues uf thc covariance matrix data 
algorithms available estimate intrinsic dimension mi indicate identified easily identified 
briefly discuss commonly methods feature extraction feature 
feature extraction methods determine appropriate sub space dimensionality ir il nonlinear way original feature space 
linear transforms principal component analysis factor analysis li discriminant analysis pursuit widely pattern recognition extraction reduction 
best known linear feature extractor thc component analysis pca karhunen lo cr thc na largest eigenvectors matrix riis 
linear transformation defined xu 
thc matrix derived ii rii pattern matrix thc matrix columns eigenvectors 
ita usc eigenvalues effectively approximates data subspace squared mor criterion 
methods projection pursuit independent component analysis ca xi art 
mort appropriate non gaussian distributions rdy thc second data 
ica successfully blind ex trac ti rig nea feat ii re comb nations sources 
possible sources gaussian 
pca feature extraction method discriminant analysis uses thc category information associated pattern 
linearly extracting discriminatory 
discriminant analysis separation emphasized replacing total covariance pca separability measure fisher criterion finding thc sb product scatter matrix aiid class statistical pattern recognition review 
setosa virginica fig 
chernoff faces corresponding mean vectors iris iris iris virginica 
scatter matrix sb 
criterion non class conditional densities patrick fisher density estimates ways define nonlinear extraction 
method directly related pca called pca 
tlic basic idea pca map input data mc new typically nonlinear degree perform pca mapped 
space high dimension 
avoid computing tlie mapping explicitly kernel pca kernels dot pi stress introduced 

problem mds give explicit mapping function place pattern map computed training sot repeating mapping 
techniques investigated address range tion training neural network 
possible redefine mds algorithm directly map new test patterns 
fwd forward neural network offers integrated procedure extraction arid classification hidden may interpreted set new nonlinear features thc output result thc kernel space 
metric 
examples kernels include order kernel classification 
sense multilayer networks serve feature ml 
networks fukushima ct le haw called shared weigh layers arc fact filters extracting features dimensional images 
lii pattern matrix zero mean pattern matrix space 
thc linear pca solves correlation matrix ip tlw matrix 
kernel pcr arc define training tlie filters tuned data classification performance 
neural networks bv directly extraction unsupervised mode 
fig 
shows thc architecture able find pca 
sigmoids neurons linear transfer 
network inputs il outputs lhc number 
inputs mation matrix 
size ri til represents thc desired number features 
patterns arc targets forcing output layer reconstruct input space hidden layer 
hc nodes ij represented hidden capture tlie principal training sct measured feature 
note ti mi 
nonlinear layers sigmoidal hidden wits arc see pig 
tors 
may needed depending function subspace middle layer called lhc pca pca sot eigenvectors layer 
limited size represents thc original space 
thc kernel layers 
sn autoassociative function bc chosen application nonlinear ita offer train ail open issue 
describe nonlinear subspaces 
oja shows scaling mds nonlinear autoassociative networks tca 
extraction 
aims tu multi ing map som map dataset dimensions feature extraction 
som distance matrix original dimensional arc arranged dimensional grid 
space faithfully possible projected usually 
bach input space 
various functions hir measuring units 
weights connections form mapping popular dimensional weight 
training patterns arc ieee pattern analysis intelligence vol 
january fig 

autoassociative networks finding dimensional subspace 
linear nonlinear connections shown 
random 
different sensor modalities form presentation winner weight vector closest feature vector large ii timber input 
thc neurons integration multiple data models data tlw grid thc modeled different approaches model updated vectors mow thc parameters serve features aiid tlw input 
training done different bc yield high weight vectors neighboring neurons grid arc vector 
input patterns close original bc thc sct cardinality rl feature space 
topology map kt nt number features 
tlw grid original thc selected subset criterion grid stressed hc sct 
ect lis density training data 
som higher value indicates better feature ri map connectivity natural tho function bc 
som different 
classification 
learning vector quantization lvq neighborhood lvq 
table summarizes feature extraction projection 
note adjective nonlinear may bc tlw mapping cing nonlinear function original features criterion non gaussian data 
fig 
shows different projections iris 
fig 
fig 
shc mappings fig 
fig 
depict nonlinear mappings 
thc mapping fig 
thc category information main reason mapping best thr 
categories 
feature selection thc feature selection defined follows set features select subset size rl smallest error 
interest applying feature selection methods due tlw situations multisensor fusion features computed pc criterion function feature selection procedures specific thc thc training test 
thc straightforward app re selection problem possible si mn thc thc st 
grows making impractical moderate values rl 
cover 
van showed feature procedure thc 
thy fur hi ordering classification errors feature subsets 
hc optimality say subset available features approximately bc 
lic optimal terms class ol monotonic ci ns feature selection method exhaustive search branch bound 
avoids results fur obtaining bounds final criterion value 
key statistical pattern recognition review 










fig 

dimensional mappings iris dataset iris iris iris virginica 
pca fisher mapping sammon mapping kernel pca second order polynomial kernel 
monotonicity property criterion function features subsets xi xz xl xy 
words feature subset improve feature added 
commonly criterion functions satisfy monotonicity property 
argued feature typically done line thc execution time ii particular algorithm rs critical optimality thc subset generates 
true feature sets tc size particu data mining document classification thousands features 
cases thc computa tional requirement selection algorithm important 
number subset evaluations may easily prohibitive large sizes number suboptimal techniques essentially tradeoff optimality selected subset computational efficiency 
lists known selection methods bem literature 
thc methods guarantee optimal 
strategies suboptimal due fact best pair contain best single feature general larger sets necessarily thc small sets 
result thc simple method selecting just best individual may fail dramatically 
step individually features decreasing hundreds 
selection advanced methods take account 
evaluating growing sets forward shrinking backward selection 

method sfs sbs adds feature 
sophisticated techniques arc thi 
plus take away strategy sequential floating search methods sffs 
methods backtrack long find improvements compared sets thc mint sim 
large feature selection problem methods perform better straight searches sfs sbs 
sws methods gid sets features feature set evaluations may easily 
factor io 
cii pursuit wa table feature extraction projection methods ra er ty addition thc strategy user needs select appropriate evaluation criterion 
arid specify value wt 
selection methods classification error feature subset evaluate 
donc example table feature selection methods leave method error estima tion 
different classifier estimating error rate lo ii different subset selected 
ferri jain feature pattern recognition review selection algorithms terms classification error run time 
general sequential forward 
patterns floating search method performs branch bound algorithm demands lower computational resources 
proposed algorithm shown superior 
feature methods known classifiers 
hit multilayer forward network fm pattern classification thc priming method determines tlic optimal feature subset tlic optimal network classifier 
train salient input hidden layers 
network followed removal raining patterns salient node 
repeated desired trade classification size 
pruning input 
oo 

features node corresponding 
fiy 

classification error vs number features floating reliable thc selection results search feature selection technique see text 
ratio thc available number training samples features small 
suppose tlie mahalanobis number possible approaches 
practice choice ss tlie feature criterion 
classifier difficult problem inverse average class covariance classifier happen available nr bust known matrix 
imprecision estimate small sample size tho 
situations result optimal feature identify designing different optimal subset classifier 
lh intuitive obtained thc covariance matrix 
jain design concept similarity zongker tis phenomenon class patterns re assigned tci lion gaussian class 
tu define class conditional densities data similarity classified template trunk tlw distance classifier prototypes 
expected quality selected class 
choice thc metric feature small training sets poor improves crucial success approach 
thc nearest mean set size increases 
example classifier selecting simple robust patterns hc training set branch bound algo class single rithm subset features included mean vector tlic training patterns common ideal subset features class 
mow ng densities known 
patterns tlic vector quantization learning vector training set branch hmd data subset wrong feature 
ciated decision rule nn 
shows example ol thc feature selection floating search ca editing 
ii nn ride tlw digit dataset different training set benchmark tlic classifiers sizes 
thc sct size fixed patterns 
provide reasonable classification performance tlw spaces ranging 
nn bayes plug classifier designed require user parameters assuming gaussian densities equal covariance find nearest evaluated wt 
feature selection distance commonly classification thc minimum pairwise mahalanobis distance 
results arc implementation 
tlw sni ll sample case total training classification problems expected mc desired 
observed 
tlw optimal number example characters character equals loo thc training patterns 
thumb recognition character location classification 
tlic rr lo features tlw sak side general 
thc input pattern invariance tlic character may classifiers multiple positions 

positions define nnc subspace 
classification finds invariants considered proper representation classifier subspace correspondingly 
template matching ieee transactions pattern analysis machine intelligence vol 
january tlie nearest mean classifier viewed finding subspace 
second main designing pattern classifiers thc approach 
thc optimal bayes decision 
loss function assigns pattern tlie class maximum posterior probability 
bc modified take account costs associated different types misclassifications 
nr known class densities thc bayes decision rule thc classifier hat prior loss function class condi tional densities decision lower risk value loss function example probability error 
thc prior class probabilities 
loss function bayes decision rule maximum likelihood decision rule exactly coincide 
practice rule plug rule estimates cif thc es wed true densities 
density estimates parametric 
commonly parametric arc multivariate gaussian distributions continuous binomial distributions binary features distributions valued features 
critical issue gaussian distributions assumption covariance matrices 
tlic covariance matrices different classes arc identical plug called pi linear decision boundary 
assumed bi 
resulting hayes plug call bayes quad quadratic decision boundary 
addition commonly maximum covariance matrix various regularization available obtain robust estimate small size ou estimator available minimizing bias 
logistic classifier li approach suited mixed data types 
class problem classifier cl posterior probability class denotes set unknown parameters ith training class 
function tho parameter vector posterior derived arc logistic functions 
linear discriminants easily 
equations may bc estimating class conditional probabilities optimizing training set 
relationship discriminant function posterior probabilities follows wc know log discriminant function decision rule posterior probabilities qa assume bc optimized approximate haw solving 
log ql rj qj log 
ill qy 
qj results 
tlic known nonparametric decision rules neighbor nn rule mid thc classifier class replaced window approach similar give different results 
essentially free parameter thc neighbors parzen optimized leave estimate error 
classifiers tho distances test pattern patterns thc training 
thc way avoid large numbers systematic reduction training set vector quantization tech possibly optimized metric hl 
possibilities table look br bound arc efficient 
third decision approach fig 
directly certain error criterion 
approach chosen classifiers type may approximate bayes classifier asymptotically 
hc driving training procedure minimization criterion apparent error ir thc error mse classifier output preset target 
classical example fisher linear discriminant thc mse classifier output desired labels 
thc single layer perceptron thc iteratively distances misclassified patterns hyperplane 
thc function combination thc mse criterion forward 
nets called perceptron may show io 
important note networks arc trained 
layers multilayer boundaries increase tlw number network layers neurons pcr layer added 
thc neural networks may bc 
mechanisms arc built slow training combination early stopping 
include addition noise weigh decay 
jain statistical pattern recognition review thc interesting multilayer addition tu classifying input pattern provide confidence approximation 
confidence may rejecting test pattern cast 
doubt 
radial basis function better suited sigmoid transfer function handling outliers 
radial basis network usually differently multilayer 
gradient search weights hidden neurons arc added preset performance 
classification comparable class density represented slim cif gaussians scc 
special type classifier tree trained iterative st individual node tree 
criteria feature tree tlic information node fisher criterion 
classification just con needed test pattern feature selection implicitly built 
commonly tree classifiers binary nature single node resulting decision boundaries parallel tn feature axes 
decision trees suboptimal lor applications 
thc main advantage tree classifier possibility interpret decision rule terms individual features 
decision attractive experts 
neural networks decision 
easily avoided stage 
decision tree classification systems cart arc available iti public il benchmark 
interesting developments classifier tlie vector authors 
primarily class classifier 
optimization criterion width margin tlic classes area tlic decision boundary defined distance lo training 
called support vectors thc classification 
number minimized maximizing thc margin 
tlic decision function class problem derived support classifier bc written follows kernel tc new pattern classified training pattern 
xi yo ti support vector set training set xi fl ihe label object 
optimized training 
www md ck id la constrained zj vx training set 
diagonal matrix containing thc labels aj matrix ic stores kernel function ic xi pairs training 
set slack variables allow class overlap penalty weight 
overlap allowed 
equation thc dual form maximizing margin phis penalty term 
optimization cri hc support vectors 
sn support vectors ones arc 
needed 
ad hoc character tlw penalty term error computational training quadratic minimization problem arc thc method 
various training proposed including osuna decomposition method sequential 
appropriate ti pca section needs 
simple form just dot product tho input pattern member tlic support set xi xi xi resulting il linear classifier 
nonlinear kernels ich xi xi 
result pth order polynomial classifier 
radial basis functions 
important advantage thc support vector classifier offers ei possibility train classifiers high small training sei 
large training sets selects small support set necessary designing tlie classifier requirements 
support vector classifier understood terms matching techniques 
hc support vectors replace prototypes main characterize classes decision boundary 
decision boundary just amore general possibly non near combination 
wc summarize commonly table 
fact family classifiers allow th hc user associated criterion 
classifiers thc sense problems cor best 
extensive large set classifiers different statlog showed large variability relative performances illustrating hing optimal rule 
differences decision boundaries obtained different classifiers arc fig 
dataset class problem gaussian derisi tics 
note small isolated pig 
nn rule 
neural network classifier fig 
shows ghost region seemingly data 
probable hidden cost poorer class tion 
ieee transactions pattern machine 
vol 

january larger hidden layer may result overtraining 
illustrated fig 
network neurons hidden layer 
training test set error training set error initially equal certain point epochs test set error starts increase training error keeps decreasing 
final classifier adapted noise dataset tries separate way contribute generalization ability 
classifier combination arc reasons multiple classifiers solve classification problem 
listed 
may haw number nf different classifiers developed different context 
onr thr 

training data 
table classification methods entirely different ti ii desc tlw 
tlic ti persons face handwriting 
training sct available collected different time different environment 
training sets may usp different features 
classifiers trained thr 

data differ global may show strong differences 
classifier haw space performs best 
classifiers neural networks show different initializations due randomness inherent training procedure 
selecting best network discarding combine various networks jain statistical pattern recognition review 
fig 

decision boundaries bivariate gaussian distributed classes class 
classifiers bayes normal quadratic normal ann feed neural network hidden layer containing neurons 
regions ri jf classes respectively classifying points dimensional feature space 

attempts learn data 
summary may different feature sets different training sets different classification methods different training sessions resulting set classifiers outputs may combined thc hope improving thc classification accuracy 
set classifiers fixed problem focuses thc combination function 
possible io fixed optimize thc sct input classifiers see section 
large number combination schemes proposed 
typical combination consists set individual classifiers combiner combines thc individual classifiers final decision 
individual invoked interact determined architecture combination scheme 
various combination schemes may differ thc characteristics selection individual classifiers 
various schemes combining multiple grouped main categories architecture parallel cascading combination hierarchical tree 
parallel architecture individual classifiers invoked independently arc combined combiner 
combination schemes literature belong category 
rn variant outputs individual classifiers arc weighted gating device combined 
cascading architect ire classifiers invoked linear sequence 
classes pattern gradually classifiers invoked 
sake efficiency inaccurate cheap classifiers low demands followed expensive classifiers 
hierarchical architecture individual classifiers arc combined structure similar decision tree classifier 
tree nodes associated complex classifiers demanding large number 
thc advantage high efficiency ieee transactions pattern analysis machine intelligence vol january 



set error es 
error 


number training epochs fig 
classification error neural network classifier hidden units trained levenberg marquardt rule lor epochs classes patterns dataset 
test set error independent set patterns 
exploiting thp discriminant power different types 
basic build complicated classifier combination systems 
selection training classifiers combination especially useful dual classifiers arc largely independent guaranteed usc different training sets various techniques rotation bootstrapping may tr artificially create differences 
examples stacking bagging boosting arcing 
stacking outputs individual classifiers train stacked classifier 
final outputs thc stacked classifier conjunction outputs individual classifiers 
bagging different datasets arc created boot versions original combined fixed averaging 
generating training data sets 
distribution particular training set sequence patterns misclassified earlier classifiers sequence 
tn boosting individual classifiers trained learn complex regions feature space 
original algorithm proposed schapire showed possible combination weak classifiers slightly khan random guessing achieve error rate arbitrarily small training data 
cluster analysis may separate individual classes tho training set subclasses 
consequently simpler classifiers linear may combined generate instance linear 
building different classifiers nf training different feature sets may 
explicitly thc individual classifiers contain independent information 
example random method 
combiner individual classifiers selected need lo combined called tlw combiner 
various combiners bc 
distinguished adaptivity aid output individual classifiers 
combiners voting aver aging sum borda count static training required trainable 
trainable may load better improvement static combiners cost additional training requirement additional training data 
combination arc adaptive sense combiner weighs decisions individual input pattern 
nonadaptive combiners treat patterns 
adaptive combination schemes exploit detailed error characteristics expertise individual classifiers 
examples adaptive combiners include adaptive weighting associative switch mixture local experts mlb hierarchical mle 
different combiners expect different types flf output individual classifiers 
xu expectations levels con rank 
level classifier outputs numerical class indicating belief probability thc input pattern belongs class 
rank level classifier assigns rank class highest rank choice 
rank value highest rank necessarily mean high confidence classification 
tho level classifier outputs unique class class labels case classes equally 
confidence level conveys richest information thc level contains amount information thu decision 
table lists number representative combination schemes characteristics 
means list 
theoretical analysis combination schemes large number studies shown classifier combination improve recognition accuracy 
exist experimental results 
apply simplest schemes restrictive 
rigorous theories classifier combination kleinberg sl 
popular analysis nf combination schemes known bias variance dilemma 
classification ci ror decomposed bias variance term 
unstable classifiers nr classifiers high complexity capacity trees neighbor classifiers large networks universally low bias large variance 
hand stable ni classifiers low capacity haw low variance large bias 
turner provided quantitative analysis improvements classification accuracy combin ing multiple neural networks 
showed combining jain statistical pattern recognition review nc order statistics combiner thc variance thc actual decision boundaries op boundary 
absence bias lhc reduction added error bayes error directly proportional variance 
linear combination unbiased neural networks distributed distributions reduce variance uf glance result approaches infinity zt rc 
unfortunately realistic assump tion large similarly cooper showed tho zero mean independence assumption misfit 
output actual output averaging outputs neural networks mean error mse factor mse ol thc neural networks 
mse 
principle arbitrarily small 
mentioned assumption increases 
cooper proposed optimal linear combiner ci ror sense 
generalized weights derived thc matrix neural networks 
shown mse thc generalized smaller mse best neural network thc 
assumptions thc rows columns tho correlation matrix linearly table classifier combination schemes 
independent tho error correlation matrix reliably 
assumptions break 
kittler com non theoretical framework class combination schemes individual classifiers distinct tlic probabilities input pattern 
introduced sensitivity analysis explain sum average outperforms rules class 
thy showed sum rule error individual classifiers posterior probabilities 
sum ride appropriate combining estimates posterior resulting different classifier case section 
product rule appropriate combining preferably erl probabilities resulting estimated densities different sels schapire el 
proposed exp tlw effectiveness nf voting weighted fact methods 
thc margin difference thc score class combined score thc classes 
established generalization error bounded tail probability margin distribution training data plus term function ol thc complexity single classifier ieee transactions patern analysis machine intelligence vol 
january combined classifier 
algorithm effectively improve margin distribution 
finding similar property thc support vector classifier importance patterns near margin margin thc area class conditional densities 
example illustrate characteristics different classifiers combination digit classification dataset see section 
lhc classifiers experiment designed matlab wow data set 
digit dataset section enabling illustrate performance various classifier combining uvw classifiers different feature sets 
confidence values ou ts classifiers computed directly posterior probabilities logistic output function discussed section 
outputs obtain versions intrinsically class discriminants fisher linear discriminant thc support classifier svc 
classifiers total discriminants arc classes sct cif thc classes 
test pattern classified class tor discriminant highest confidence 
classifiers arc see table plug assuming normal distributions different bayes quadratic equal covariance matrices normal linear nearest mean nm rule lan fisher binary decision tree purity criterion early pruning feed forward neural networks thc matlab neural network cif ann ann neurons arid tlic linear svc quadratic svc quadratic 
tlic number neighbors nn rule tlic smoothing tlic arc op tho result tho ono error estimate tlie training set 
classifiers median product voting rules trained classifiers nm nn 
thc training set individual classifiers 
classifier com bii 
classifiers listed table trained training patterns feature sets tested test 
resulting classification errors percentage arc reported feature set tlie best result classifiers printed bold 
individual classifiers single feature set combining rules median product voting nearest mean 
example voting rule row bhc set number column yields error percent 
underlined indicate result better performance individual classifiers feature set 
tlie outputs classifier classifier combination er feature sets combined thc combination fiw columns 
fur voting rule decision tree classifiers row yields 

combination result better thc results tree 
bottom right part table presents tlie feature sets classifier combination schemes separate sets 
tlic classifiers example decision tree data 
neural network poor optimal probably duc training sessions 
simple tho nn plug give ent classifiers ary substantially sets 
due small training uf large feature tho normal quad ati classifier outperformed lhc linear ono svc quadratic generally performs better svc linear 
shows svc classifier find solutions increasing thc risk 
tlic ts appears classifier arc fixed 
thc bcst percent error obtained trained combination rule nearest 
tlic different thc sct columns table slightly improves best individual classification 
hc best combination rule dataset voting 
thc product poorly cc expected oil tlic feature set provide 
ct results obtained different feature sets rows table bcst individual classifier 
substantial tlie case 
decision tree 
product rule better occasionally performs surprisingly bad similar combination neural network classifiers 
combination rule minimum maximum experiment sensitive poorly trained individual 
observe combining network results trained combination classification errors percent percent comparison thc fixed errors percent 
error estimation classification error simply thc ri tlw ultimate measure performance classifier 
competing bc evaluated emir probabilities 
performance cost thc computa tional requirements tlw 
easy define probability error terms thc class conditional il obtain closed form 
thc cast 
cif densities unequal il possible write simple pattern recognition review analytical thc 
rate 
analytical expression error available wed decision rdc study tlic behavior 
function features true parameter values nf thu densities training samples prior class probabilities 
fnr consistent training rules value 
approaches error increasing sample 
families distributions tight bounds error may obtained 
finite sample unknown distributions bounds arc 
practice error rate recognition system tlic available samples split training test sets 
designed training samples classification tlic kst 
tho percentage test samples taken estimate error 
order estimate reliable predicting 
classification training set test sct sufficiently large training samples 
training test samples overlooked practice 
important point keep mind thc error estimate classifier thc specific training test random 
classifier suppose test samples total 
density function il binomial distribu tion 
hc maximum estimate ec pc unbiased estimator 
random variable interval associated cp 
percent confidence interval fe ls 
interval shrinks test samples increases plays important role comparing competing classifiers cl 
suppose total uf samples available cl misclassify respectively samples 
gi better 
confidence intervals true classifiers fl oo 
confidence intervals overlap say performance cl superior 
analysis pessimistic duc positively correlated estimates hc test set 
samples split tu form training test 
training set small thc resulting classifier robust low generalization 
hand test sct small thc confidence error rate 
various methods commonly error rate table 
methods differ utilize available samples training sets 
available extremely large say 
methods lead thc estimate error rate 
example known thc method provides optimistically biased estimate thc error rate tlie bias smaller ratio training samples class tlie dimensionality feature vector gets larger larger 
guidelines available available training test fli provides arguments favor samples testing thc classifier designing classifier 
ni data split training test clear different splits specified size training test sets di error estimates 
ieee transactions analysis machine intelligence vol 
january table error estimation methods fig 
shows classification error bayes plug pattern applications linear classifier digit dataset function adequate characterize performance classifier number training patterns 
tho test sct error gradually error rate approaches tlie training set error resubstitution error 
consider evaluating number training samples increases 
relatively fingerprint matching system different large difference error rates arc interest 
thu false training patterns class indicates bias ratio number pairs different wo wrw bc reduced enlarging thc rpi ints arc system training set 
curves represent total number match attempts 
false reject rate experiments training sets frr thc ratio number pairs size randomly drawn test set patterns fingerprint system thc 
total number match attempts 
fingerprint matching thc leave ane rml rotation methods arc system tuned setting appropriate threshold versions tlic 
cross validation approach 
onc thc matching score operate desired value far 
main cross try decrease far system cially small size increase zk vice versa 
receiver available samples 
training classifier 
operating characteristic roc plot uf far extreme cases cross validation hold versus frr permits system designer assess uno method suffer performance recognition system various operating large bias large variance respectively 
overcome points lds decision rule 
sense koc limitation tho bootstrap method provides comprehensive performance measure proposed rate 
thc 
say tlw error rate system frr pal 
resamples available patterns replacement fig 
shows thc roc thc thc fake data sets typically plug th hc size training set 
class 
examples usc roc analysis arc new training sets bc tn classifiers feature selection bias resubstitution tu addition error rate useful called bootstrap estimates error 
measure classifier reject rate 
suppose experimental results shown thc test pattern falls near decision boundary estimates outperform thc classes 
decision rule may able thc resubstitution estimates error rate 
correctly classify classification statistical pattern recognition oo test cmr training set low 
better alternative bc reject doubtful patterns assigning thc categories consideration 
test pattern 
thc bayes rule known reject option reject pattern maximum posteriori probability threshold larger threshold higher thc reject rate 
reject option reduces thc rate reject thc smaller tlic mor rate 
relationship error reject trade curve bc set desired point tho classifier 
fig 
error cct digit dataset bayes plug linear classifier 
monotonically increasing rejecting patterns reduces rate keeps 
choice hc reject rate tlie costs associated incorrect decisions scc applied example nf error reject curves 
unsupervised classification applications pattern recognition extremely difficult impossible reliably label training true category 
consider example tlie land classification sensing 
obtain ground truth information pixel tlic image specific site associated pixel 
bc visited extracted information system onc available 
classification situations construct decision boundaries training data 
unsupervised classification known data clustering generic label procedures designed find natural groupings clusters data similarities patterns 
category labels information 
ol 



og false fig 

roc curve tho bayes plug linear classifier forthe digit dataset 
data influence interpretation clusters 
unsupervised classification clustering data different shapes sizes see fig 

compound number clusters data depends fins vs view data 
ono clustering detection region ii high patterns compared background 
definitions cluster include patterns cluster mom similar belonging different clusters cluster relatively high points fi om clusters relatively low 
functional cluster easy operational definition 
ill appropriate measure define wl ich data cluster shape context 
cluster analysis important useful que 
reliability algorithm cm organize large amounts data applications data mining mi retrieval image signal coding learning 
clustering algorithms haw proposed tlie algorithms continue appear 
algorithms arc rm thc clustering techniques square agglomerative hierarchical ring 
hierarchical organize data groups displayed form dendrogram 
square error partitional algorithms attempt clus tcr scatter cluster scatter 
guarantee optimum solution obtained possible partitions nf tlic 
ieee transactions analysis machine intelligence vol 
january reject rate fig 

error reject curve bayes plug linear classifier digit dataset 
dimensional patterns ji clusters computationally feasible 
various heuristics arc thc 
optimality 
clustering techniques fre hierarchical techniques pattern recognition applications restrict coverage partitional 
cluster analysis user algorithm issues mind clustering algorithm find data subjected tests clustering tendency applying clustering algorithm followed validation clusters generated best clustering algorithm 
user advised try algorithms 
data data normalization cluster validity important choice clustering strategy 
problem partitional clustering formally stated follows patterns dimensional space determine partition patterns fc clusters patterns cluster similar different clusters mi 
thc value may nr may specified 
clustering criterion global local adopted 
criterion error represents cluster assigns patterns clusters similar 
clusters local structure data 
example clusters cm formed identifying high regions thc assigning pattern nearest neighbors thc cluster 
tlie clustering techniques implicitly assume con valued vectors patterns viewed embedded metric space 
features nominal ordinal scale euclidean distances cluster arc meaningful clustering methods mally applied 
wong wang proposed clustering fig 

clusters different shapes sizes 
valued data 
technique clustering learning bc symbolic descriptors 
thu group patterns intn simple classes 
concepts defined terms attributes patterns arranged hi classes described concepts 
subsections briefly summarize popular approaches clustering square error clustering arid mixture decomposition 
square error clustering method bc viewed particular case mixture decomposition 
point difference clustering criterion clustering algorithm 
clustering algorithm particular criterion 
sense large number square error clustering algorithms minimizing thc error criterion differing choice algorithmic parameters 
known clustering algorithms listed table 
square error clustering commonly square error criterion 
tlw obtain partition lor fixed number clusters minimizes thc 
suppose set tl patterns clusters ca ill patterns pattern cluster el 
mean cl defined tlic centroid tlie id ith pattern belonging cluster ck 
square error cluster tlie sum squared distances pattern ca cluster 
tlic cl variation thc thc containing ic clusters tlic sum thc variations jain statistical pattern review mt mutual square tn find pai ic ic 
resulting partition referred ko variance partition 
general algorithm partitional 
ihm foi 
stcp initial wih clusters 
tlic cluster stabilizes 

partition closest cluster center 
stcp new clusters 

steps ail optimum value thc criterion 
ski 
adjust tho mei ging splitting clusters small 
table clustering algorithms step known thc means 
steps algorithm bc supplied parameters implicitly hidden thc computer 
arc crucial thc succ ss tlw program 
big frustration clustering lack pi el ng initial partition updating adjusting clusters stopping thc simple ic efficient gives surprisingly tlw clusters arc shape separated iri space 
mahalanobis distance lh wen detect clusters 
numerous haw performance basic ic means incorporating fuzzy function lis resulting fuzzy lit algorithm genetic algorithms annealing tabu ieee transactions patern analysis machine intelligence vol 

january optimize resulting partition llo mapping neural possibly implementation 
called enhancements algorithm arc computationally demanding quire additional parameters general arc 
show combination algorithmic distribution nf computations workstations cluster hundreds thousands multidimensional patterns just rl 
ii 
bo concepts clustering ti bhc sm algorithm 
easy lhc algorithm thc compression domain equivalent kmeans algorithm 
vq cr 
tl dimensional vq consists mappings encoder maps input alphabet channel symbol set maps symbol sc mj ti lhc output kr distortion measure sl yj specifies cost associ quantization usually quantizer thc distortion size constraint problem vector posed clustering problem 
output alphabet iji 
goal fr lind referred partition thc ic gr rithm tlic dimensional space thc average distortion mean tlic input 
quantization widely ii aiid coding applications coding coding symbols thc output alphabet cluster centers thc signal 
quantization density estimation 
kernel approach mixture kernels kernel placed thc probability thr training 
major vq oi output alphabet size 
number techniques minimum th mdl principle see section 
supervised vq called tion lvq 
mixture decomposition finite mixtures flexible aiid powerful probabilistic tool 
statistical recognition main mixtures defining formal model hi 
iip reason adequately model situations pattern ha onc set alternative kept strict tu interpretation bc class models able arbitrarily complex probability 
mixtures class condit densities scenarios see 
finite 
feature selection tool 

basic definitions consider scheme random samples 
ti random probability mass density zr cor rii 
ii 
time sample bc wp choose ccs probabilities 
sample 
thc stage generating 
mixture distribution formally function ir rj ul ki 

components functional form example multivariate gaussian 
fitting mixture model set observations qy consists nf set parameters best describe data 
mixtures ui built majority oc res 
thc arising mixture fitting 
defining model thc 
thc question answer qr cln io mx iir tion em algorithm der mild conditions converges maximum likelihood mi tht 
authors ad demanding method 
proposed section 
nok lint nf tlic thc component distributions 
em algorithm thc algorithm interprets observations missing part set associated wih jl dh missing zc indicates rc zip 
ic thc complete log likelihood thc em alternatively applying statistical patern recognition review stcp compute thc conditional complete lag od yo 
la linear missing variables tlie step hr mixtures reduces conditional missing variables 
mi step update parameter tho mixing probabilities thc consists mean vector covariance matrix arc updated versions weights uf standard ml estimates 
main difficulties em mixture model fitting current research topics local nature critically dependent initialization possibility convergence point boundary parameter unbounded likelihood onc approaches corresponding 
ril 
sample full rl distribution ic included unknown 
despite formal think mcmc far demanding useful recognition applications 
fig 
shows example mixture decomposition ic selected modified mdl si 
thc data consists cif dimensional distributed gaussian components mean vector different covariance matrices dense cloud points inside cloud sparse points 
level curve contours constant distance thc true underlying mixture mixture superimposed data 
details see sl 
nok algorithm ic means able due substantial nf components 
discussion early stage statistical recogni tion focused mainly core bayesian decision rule derivatives quadratic discriminant density tion cif dimensionality problem aiid estimating number components mi criterion thc number likelihood function making useless model selection criterion selecting value ic case 
particular instance problem classical xy hypothesis bc thc regularity conditions estimation 
limited computing thc pattern employed simple tech applied tn small scale problems 
early ros statistical rapid growth 
frontiers haw expanding 
rapid largely driven tlie forces 

haw proposed arc 
increasing collaboration different disciplines including neural networks approaches tlic fixed em algorithm machine statistics obtain ce estimates range biology 
nary efforts values ic ki fostered ideas defined minimizer cost function niques enrich tlw traditional statistical pattern recognition paradigm 
fast thc internet large cost function maximized loglikelihood function plus penalize ic 
choice class minimum description mdi criterion lo model criteria proposed bayesian inference ic thu memory 
possible complex searching optimization algorithms feasible ago 
allows tackle large scale real world recognition may involve millions samples high dimensional minimum length mml criterion akaike spaces thousands features 
information criterion aic 
emerging applications data mining resampling schemes cross validation type document taxonomy inn iii 
approaches su techniques applications brought new closer stochastic algorithms foster il interest statistical methods previous paragraph 
stochastic pattern recognition research 
approaches markov chain monte carlo east need principled mcmc sampling arc far computationally ad hoc approach solving intensive em 
mcmc different pattern recognition problems predictable way 
ways implement model selection criteria example concepts neural ic mtm ful bayesian flavor iwi inspired ral ieee transactions pattern analysis machine intelligence vol 
january 


fig mixture decomposition example directly treated principled wr statistical pat tern recognition 
frontiers pattern recognition able es topics mi opinion ale thc frontiers pattern 
wc sce table fundamental research problems thc 
model field continues grow 
model selection important avoiding curse ty topic research 
common practice model selection relies cross validation rotation best model selected performance thc validation sct 
validation set training method fully utilize precious data training especially undesirable training set small 
avoid rl number model selection schemes proposed including methods minimum description mdl akaike information criterion aic likelihood 
various regularization schemes prior model structure parameters haw proposed 
structural risk minimization notion vc wed model selection best model thc bcst worst upper bound generalization 
methods tlic search best 
typically complexity possible model set models 
assumptions parameter independence order simplify evaluation 
model selection tic applied feature selection jn supervised learning unsupervised pruning decision table frontiers pattern recognition mina ae full um uv data likelihood far 
statistical pattern recognition review 
best cif clusters automatically 
example mixture em algo rithm sc proposed popular approach density clue thc computing power available today 
thc years ii concepts aiid bcm introduced 
maximum margin context support vector machines risk minimization 
large margin separating classes small vc dimension yields generalization performance 
svms demonstrated superiority objective 
function 
boosting algorithm improves margin dis tion 
maximum 
consid ered special ai cost function inverse margin tho classes 
ri cost functions weight decay weight haw context neural networks 
due svms programming optimization techniques studied classification 
programming credited nice tlie decision fully boundary patterns linear programming thc tlie margin small set oe optimal solution obtained 
thc topic local boundary learning received lot attention 
tts primary emphasis boundary classes modify thc boundary 
boosting algorithm misclassified patterns mar decision subsampled correctly classified patterns new training sat subsequent classifiers 
combination local experts related local experts learn local decision boundaries accurately global methods 
ln classifier ref boundary respect decision reduced improved recognition 
data arise real world problems speech line handwriting 
sequential recognition important topic pattern 
hidden markov hmm popular statistical tool modeling arid recognizing data particular speech data mi 
large number hmms proposed literature including hybrids hmms neural networks output hmms weighted transducers variable hmms markov switching models switching space models 
thc growth sensor enriched availability data ways 
real world high 
physical haw il finite complexity highly 
explains models spatial structure speech ap proaches general important compress data physically meaningful improving classification learning requires training sample labeled category 
collecting amount labeled data 
practice haw small amount data amount data 
unlabeled data lor training classifier important problem 
svm perform learning 
pattern desirable applications face recognition 
early research statistical pattern recognition invariant bc difficult task 
activity designing invariant require invariant 
examples tangent distance aid matching mi 
di ly small amounts linear tions deformations 
hey intensive 
simard ct pic posed named 
tangent op minimise derivative tlic outputs distortion parameters improve tlic properly classifier selected 

known thc human recognition context 
tho contextual information difficult patterns thc major differentiator abilities human beings aiid 
contextual information success fully recognition ocr sensing 
commonly step mistakes initial rl trend bring contextual earlier stages word il recognition system 
context compound decision theory theory iir markovian models 
successful application classifier recognition web page dramatically improved ter category hat point pointed 
concluding remarks watanabe wrote book ed ted cn ti fro pnt ern 
ii tlw pat recognition il fast moving 
easy krr forin balanced aiid summary view thc field 
harder vision progress ieee transactions patern analysis machine intelligence vol 
january acknowledgments authors reviewers 
conceptual system tts application browsing 
vol 
pp 

drs til ho iterative algorithm neural lt ec vo 

pp 
critical constructive comments 
rhc nc uu 
chou optimal classification trees 
fahmy neural maximum vol 

pp apr 
clustering vol 
nu 
pp 
comon component analysis concept 
sip vol 
ii pp 

akaike new statistical ieee ea 
nd gray vol 
pp 
fur pwc 
ieee vol 
xi 
stability analysis pp sept 
learning algorithms fur blind source net cover mil 
pp 

applications anderson logistic 


vol 
pp 
june 
eds vol 
pp 
cover best arc amsterdam north 
wo sys mmr tud vol 
aid pp 


cambridge mass mit cover van bayes mem error eel 
mori voi 
pp sept 
nci vol 
pp 
july 
hid 
maximum li 
rvi ak arbitrarily tight upper lower data em roy soc ou ds probability ol error ieee 
vol 
pp 
iy 
vol 
nn 
pp 
jan 
ti 
cu si tc iri ai 
version mathworks natick mass 
hall 
de ridder duin sammon mapping 
matching proc vol pp ci vol 

pp 
fl 
minimum description coding modeling ieee 
devijver kittler rw 
vol 
nu 
pp 
oct 
lt hll 

information ap recognition study blind separation vu 
pp 
error 


vol 
nu 
pp 

data 

vol 
pp 

www icsi 
pni hi berlin springer 
cdu jagota ncs 
fast algorithm ihe nearest semi supervised vector machines idc neighbor classifier 
andy irc 
ii rf ion systems 
vol 
pp 


john wiley 

boosting rml vol 
nu 
pp 

new york plenum press 
ti 
id 
methods york john wiley sons 
str iir um 
pal eds cs press hart mid 
stork 

second cl new york john sons 

clustering informa duin note comparing classifiers kee 
spt ins iii vol 
vol 
pp 


pp 
kks jir 
oxford duin de ridder 
tax experiments approach ili 
vol 

pp 

aia examples machine learning arl cid ria 
hc jdk th kt fi plm nos 
pp 

philadelphia siam 

mid 
groenen mi oi berlin 
knowledge 
mining unifying framework breiman predictors re lcm vol 
cot 
fi aug pp 

friedman olshen stone wri ip aiid trees 
calif 
large tutorial nn support pattern irr practice iv gelsema eds pp 

discovery vol 
nu 

jain mixture pp 

models gy irt vision cardoso statistical 

eda springer ieee vol 
pp 

verlag jain 
pattern recognition review ieee transactions pattern analysis machine intelligence vol 

january ilan fr sj jain sample sisc 
new york sons 
statistical pattern practi lo mehta aid ag mi rcc pmc 
cui itr 

iyi 
montreal aii 
metz basic principles roc analysis vol 
viii 
pp 
anc single neuron single classifiers rk vol 
ui 
pp 
lo michalski stepp ra dp duin expected classification error thc ieee umts 
mrd classifier ntf ni vol 
nos 
pp 

pi 
io newd wow york 
icl aiid 
oil analysis mixtures 
soc 
pp 
io mishra study thr ripley methods lor clustering imi probabilistic 
gelsema eds ani hall 
pp 


wk tlic art 
il lc vol 
mass univ 
pp 


coi 
nagy practical world scientific 

row vol 

pp 

rol neal nr nr ks 
new york spring pwr 
vol 
pp 


ross flash genius pp 
nov 

linear mappings jr ri vol 
pp 
analysis ieee 
vol 
pp 

si gray image ani tlw learnability iwd im 
vol 
pp 
vol 
pp 
ly 

schapire 
lcc boosting lh oja ew explanation lor research studies press 
methods qf oja minor 
support ii 
thesis networks nt vol 

pp 
berlin 

nonlinear ita imc muller nonlinear analysis analysis vol 
pp 
pp 


osuna improved sung girosi niyogi algorithm machines proc 
workshop vapnik 
comparing support machines fur 
function cla morgan wilson cds new york 
vol 
pp 
park design vf vel 
pp dii new york wiley 


applic population mixture pavlidis 
new york springer ictl 
vol 
pp 
cif complexity si tit classifier design mutual information 
pp 
hdb 
hh vol 
pp 
apr nd cooper networks liu neural ieee hybrid hw 
ni tm ni rk vol 
nv 
pp 

inp 

cd sklansky algorithms chap 
large scale tire selection rrs vol 

hst support vector pp 

sequential 
un efficient pattern 

aj iif cds mass mrl 
ig cowan 
mi 

cds morgan 
hl 

rop selection vol 
fnr pp 

network arl ix 
ki 
cds pp 
thc class flip organ 
ppc 
pp 
dik adaptive 
search fcn selection trees tk voi 
tl pp vol 
pp 


makov jou 

san 
sons calif 


tutorial markov sp cch yk vol 
pp 
feb 

aiid size combining estimators non constant functions iir cds vol 
mit press 
classification complexity recognition lel 
si trunk 
mid pnc vol 
nn 
vol 
pp 

pp 
july 
statistical patern recognition review decision classifiers vol 
pp 

dom selection applications tu roc 
sixth ht cor 
june 
van dum tax den digit recognition classifiers yl vol 
pp 
vapnik oj di dnm 
vapnik 
new york john wiley 

new york wiley 
uf 
cd new york press 
ai webb scaling iterative radial basis vol 

pp 

kulikowski 
mu gan 
ratios validating analysis 
st vol 
pp 


stacked vol 
pp 

wong valued data clustering algorithm ieee 
wi pp 
woods jr 
classifiers local accuracy vol 
apr 
laszlo fur design 
ysis vd 
pp 

xu suen lor multiple character ieee sys cni mmr vol 
pp 

toussaint nf recognition vol 
pp 

mao recognition aj 
pp 
sons haralick llr making context 
rtti wi 

pp 
mar 
nom categorization hyperlinks ac jain university distinguished professor department computer science engineering michigan slate university research interests include statis tical pattern recognition markov random fields texture analysis neural networks 
document image analysis 
fingerprint matching object recognition 
received best awards certificates outstanding contributions pattern recognition society 
received ef transactions neural outstanding award 
editor eee transaction pattern analysis machine intelligence 
author algorithms clustering data edited lhe book rod time object measurement classification 
books aoa ysis range images markov random fields neural pattern recognition object recognition biometrics personal 
received fulbright research award 
fellow ieee 
robert duin studied applied physics delft university technology lands 
received phd degree thesis accuracy statistical pattern recognizers 
research included various aspects automatic interpretation mea learning classifiers 
studied devel oped hardware architectures software con interactive image analysis 
associate professor faculty sciences delft university technology 
research interest design evaluation learning algorithms pattern recognition applications 
includes particular neural classifiers vector classifiers classifier combining strategies 
began studying possibilities relational methods pattern recognition 
mao received phd degree computer science michigan stab univer sity east lansing 
research member ibm almaden research center san jose california 
research interests include recognition machine neural networks information retrieval web mining document image analysis ocr handwriting recognition image proces sing computer vision 
award pattern recognition society fee transactions neural networks outstanding award 
ak received ibm research awards 
ibm outstanding technical achievement award 
ho guest editor special issue ieee networks 
served associate editor neural editorial board member pattern analysis applications 
served program committee member international conferences 
senior member tho ieee 
