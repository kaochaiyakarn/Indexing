variational approximation bayesian networks discrete continuous latent variables kevin murphy computer science division univ california berkeley ca cs berkeley edu show variational approximation logistic function perform approximate inference bayesian networks containing discrete nodes continuous parents 
essentially convert logistic function gaussian facilitates exact inference iteratively adjust variational parameters improve quality approximation 
demonstrate experimentally approximation faster sampling comparable accuracy 
introduce simple new technique handling evidence allows handle arbitrary observed nodes achieving significant speedup networks discrete variables large cardinality 
probabilistic models naturally contain discrete continuous variables 
models called hybrid 
unfortunately exact inference possible continuous variables gaussian discrete children 
want allow discrete children continuous parents model threshold phenomena standard approach discretize variables fg kk resort sampling grs 
problem discretization get accuracy quantize finely inference slow problem especially high dimensional state spaces 
problem sampling similar get accuracy take samples slow 
introduce variational approximation handle case discrete children continuous parents faster accurate distributions handled exactly handled exactly 
introduce new approach dealing evidence allows handle arbitrary distributions observed nodes 
results context junction tree algorithm widely considered efficient general inference algorithm graphical models sas 
particular allows compute marginals families prerequisite efficient parameter structure learning passes graph query driven goal directed algorithms bucket elimination dec spi cf cf take passes 
addition junction tree algorithm allows handle graphs undirected cycles previous networks continuous variables dm aa restricted polytrees 
structure follows 
start describing popular conditional probability distributions cpds nodes hybrid networks 
section give brief overview junction tree algorithm sections review aspects specific hybrid networks 
section explain variational approximation section introduce new approach handling evidence section discuss computational complexity inference hybrid bns section experimental results assess quality approximation 
cpds hybrid networks directed graphical model define conditional distribution node parents see table examples 
discrete nodes discrete parents simplest representation table called conditional probability ta pr ble cpt defines 
note nomenclature represent discrete parent represent discrete child represent continuous parent represent continuous child 
multiple parents multidimensional table requires specifying parameters assuming simplicity discrete node binary 
representations require fewer parameters noisy neural networks easier learn don discuss 
consider case continuous nodes continuous parents 
loss generality assume child continuous parent aggregate single vector valued node 
simplest example gaussian mean linear function parent value def child parent discrete continuous discrete tabular noisy decision tree probit logistic softmax continuous conditional gaussian linear gaussian table popular conditional probability distributions 
node discrete continuous parents create mixture distribution 
weight regression matrix exp normal gaussian distribution normalizing constant number rows columns ensures 
networks variables kind linear gaussian distribution studied sk 
continuous child discrete parents specify gaussian value discrete parents called conditional gaussian cg distribution 
note cg distribution approximate arbitrary continuous distributions 
consider case discrete nodes continuous parents 
popular models discrete binary variable continuous vector valued parent called logistic probit defined follows logit def log probit def def sigmoid function probit distributions similar see differ tails essentially cumulative normal dies sigmoid dies slowly pr exp cdf standard normal 
logit nice interpretation noisy threshold unit iff logistic distribution advantages motivated statistical viewpoint jor 
efficient method fitting parame ters called iterative reweighted squares irls algorithm mn jj form newton raphson 
approximation method converting potential form see section 
generalizes multi valued discrete variables fol lows pr exp exp called softmax function 
note softmax binary variables equivalent crop network 
circles represent continuous scalar nodes squares represent discrete binary nodes 
example 
pr logistic function softmax function normal vector th decision boundary offset 
magnitude determines steepness curve large magnitude corresponds hard threshold steep curve small magnitude corresponds soft threshold 
limit sigmoid approaches step function limit sigmoid approaches uniform distribution 
turns linear gaussians softmax special cases generalized linear models see mn jj details 
cpds observed nodes see section general difficult hidden nodes restrict exact inference 
example simple example distributions described consider network 
model price certain crop say wheat assumed decrease linearly amount crop produced year assumption glut reduces prices 
government artificially prices price raised fixed amount 
addition consumer buy price drops units see 
model experiments section parameter values shown 
node distribution params 
cpt gaussian cg logistic price price vs availability supported supported crop yield prob 
buy vs price logistic probit left expected price decreases linearly crop yield shifted constant price artificially supported right probability buy crop decreases price rises threshold 
plot price 
junction tree algorithm section give brief overview junction tree algorithm see hd details discussing aspects specific hybrid networks :10.1.1.125.4164
summary meant provide road map rest 
junction tree algorithm perform graph theoretic steps order 
original graph connect parents share common child drop directionality arcs 
result undirected graph choose elimination ordering heuristics discussed 
nodes initially unmarked 
node order mark join unmarked neighbors 
result triangulated graph find maximal cliques call build undirected weighted graph nodes cliques weight edge clique clique maximal spanning tree jj 
add separator node edge pick arbitrary node root 
section discuss changes need steps case hybrid networks 
building junction tree shell numerical steps order 
steps involve potentials associated clique separator represent operate potentials discussed section 
clique separator initialize poten tial identity element 
node find clique contains parents convert cpd potential see section multiply potential 
optionally perform global propogation convert potentials joint form saved reuse avoid repeating initialization step 
approach evidence discuss section possible propogation evidence arrived 
node evidence find clique contains multiply evidence see section 
clique postorder children parents absorb parent 
called collect evidence phase 
absorbs separator performing operations potential superscript denotes new updated potential represents marginalization operator multiplication operator division operator 
say sends message clique preorder parents children absorb child 
called distribute evidence phase 
hybrid clique potentials variables clique discrete represent potential table multidimensional array variables gaussian represent potential quadratic form variables discrete gaussian table quadratic forms 
explain quadratic form representation see lw lau ole lau details 
gaussian clique potential represented familiar moment form exp convenient canonical form exp convert canonical moment form provided full rank follows log log log convert moment canonical form 
cg potential just list gaussian potentials value discrete variables 
note logarithmic representation constant factor assuming non zero 
get need additionally store indicator variable iff discrete value positive support 
advantage logarithmic representation underflow lot evidence 
define fundamental operations extension multiplication division marginalization cg potentials 
extension operation ensuring potentials defined set variables 
continuous variables sure size vector matrix inserting necessary 
discrete variables sure potentials number table entries duplicating necessary 
define multiplication cg potentials follows 
convert potentials canonical form necessary 
extend domain necessary 
compute discrete entry division similar marginalization harder 
consider case pure gaussian potentials 
suppose want compute convert canonical form necessary partition components kept components marginalized new canonical characteristics follows log log turn cg case 
marginalize continuous variables discrete ones 
assume canonical ordering entries vector matrix table 
easier marginalize moment form just extract relevant components possible convert moment form 
necessarily reduce size number table entries cg potential 
example consider potential continuous scalar variables discrete binary variables mixture dimensional gaussians 
marginalize result mixture dimensional gaussians marginalization potential smaller terms number discrete components 
suppose multiply potential scalar binary variable result mixture dimensional gaussians just value contains gaussians 
propagate messages potentials mixtures components 
avoid exponential blow size potentials adopt standard approximation collapsing mixture gaussians single gaussian formulas called weak marginalization 
example collapse mixture gaussians single gaussian value best cg approximation sense minimizing kl divergence true marginal see lau proof 
particular gives correct second order moments var weak marginal true marginal 
note parameters gaussian independent discrete variable marginalized example discrete variable parent child gaussian variables just happens live clique process exact called strong marginalization 
junction trees strong roots non closure cg potentials marginalization discrete variables means careful construct junction tree 
particular need able convert moment form perform discrete 
relevant theory discussed lau just summarize main results 
define strong root node junction tree satisfies property pair neighbors tree closer continuous variables discrete variables 
words separator tween neighboring cliques purely discrete variables clique furthest away root separator continuous 
graph triangulated paths discrete vertices passing continuous vertices path form strong root lei graphs called decomposable marked graphs marked just means types nodes 
example consider 
moralization adds arc resulting graph triangulated cliques junction tree note forbidden path strong root 
add extra arc tion step eliminate forbidden path junction tree strong root 
sufficient condition ensure strong root eliminate continuous nodes discrete ones triangulating 
example elimination order get strong junction tree 
general adding extra links remove forbidden paths increase size cliques saw 
reason need junction tree strong root ensure send messages root collect evidence phase strong subsequently send messages back root distribute evidence phase neighboring potentials consistent 
reason pass results easy see pair neighbors tree closer strong root compute performing integrations variables continuous 
marginalize discrete variable say integrate variables depend say compute avoid need collapse mixture gaussians 
reason second pass results consistent potentials easy see 
suppose absorbed pass backwards pass compute note justified pulling ratio outside sum marginalization strong 
converting cpds potentials discrete nodes discrete parents convert cpd cpt compute pr distribution specified implicitly convert tabular potential 
course transformation lose local conditional independence information exploited speed inference 
kinds cpds noisy causal independence rd ways expose local structure graphically easier exploit junction tree framework don discuss issue 
convert gaussian distribution canonical potential follows 
exp exp log set canonical characteristics log generalizes result lau case nodes 
scalar case log see rank may able represent initial potential clique moment form propagated evidence potential represents joint probability density converted moment form 
conditional gaussian distribution get triple form value discrete parents 
convert logistic function canonical gaussian potential variational lower bound jj see appendix derivation general variational methods pr exp notice quadratic represent canonical potential log call representation vg variational gaussian 
discrete node discrete parents get exact approx prior exact approx prior exact approx posterior exact approx posterior pr variational approximation gets poorer logistic function steeper deterministic 
left plot exact solid approximate dotted logistic function top bottom optimal value 
right plot pr pr pr pair discrete parent value resulting potential mixture vgs 
quadratic function poor approximation sigmoid joint probability gaus sian logistic approximated gaussian see 
increase quality approximation iteratively adjusting variational parameter em nh set value maximizes expected complete data log likelihood :10.1.1.33.2557
results update see appendix derivation posterior bound tight update equation specify take positive negative square root 
ambiguity turns matter pr symmetric choosing initial estimate important see section 
procedure follows 
walk graph compute mean variance node continuous probable value discrete evidence assignments 
get logistic node look parents plug equation example consider crop network suppose observed 
set node mixture component corresponding probable value turned better collapsing mixture gaussians distribution weights 
derive upper bound variational parameter def log log binary en tropy function 
lower bound exp tighter second order approximation learning want maximize lower bound likelihood nh :10.1.1.33.2557
upper bound conjunction lower bound filter runs mcmc result marginals fall outside bounds jj 
note exploit quadratic approximation fit parameters logistic node linear regression slower irls procedure noted tip 
finding variational approximation softmax distribution problem currently working 
consider logistic distribution binary nodes 
binary nodes encode distributed fashion value single node possible values bits parent see tip 
new approach handling evidence traditional approach junction tree framework follows see hd :10.1.1.125.4164
start considering case potentials discrete 
create junction tree potentials initialized multiply cpds 
evidence arrives observe find clique contains multiply potential form th position 
sets entries incompatible evidence 
propogation restore global consistency 
clique potential contains joint probability variables evidence normalized obtain pr likelihood pr evidence posterior pr pr gaussian potentials initialize follow similar procedure multiply potential including separators contains nodes evidence dimensionality vectors matrices reduced 
example suppose observe exp called hard evidence 
soft virtual evidence consist distribution possible values 
exp generalizes equation lau case vector valued nodes 
problems traditional approach discrete variables possible values hmms large codebooks may create huge initial clique potentials subsequently set entries zero 
technique evidence shrinkage hd zero compression ja help reduce inefficiency manipulating sparse potentials better create place :10.1.1.125.4164
need way converting cpd node potential form 
impossible kinds distributions 
want create conditional pr model associate parameters observed input linear regression 
possible traditional approach 
annoying asymmetry handling observations discrete continuous nodes 
need modify potential modify potentials contain observed nodes 
addition difficult book keeping change size gaussian potential 
simple solution problems create initial clique potentials evidence arrived 
potentials defined hidden nodes observed nodes just contribute constant factor value clique potential don take space 
example consider softmax node parent value observed convert cpt discrete potential computing softmax similarly pr consider hmm gaussian output 
create evidence specific observation matrices computing pr hidden state time step sense arbitrary conditional densities observed nodes 
type potential need junction tree determined type hidden nodes left 
hidden nodes clique discrete represent potential table gaussian gaussian mixture gaussians mg 
potential type type potentials converted type mg compatibility absorb 
similarly type mg type converted mg 
cliques raised common ancestor type hierarchy mg 
disadvantage new approach handling evidence incremental new evidence arrives just update small part junction tree combine new old evidence rerun inference algorithm 
addition new approach handle retraction evidence soft evidence 
hand simple combine new technique old core findings handled new way nodes soft evidence want just temporarily instantiate handled old way assuming cpd right form 
computational complexity inference possible marginalize multiply divide gaussian potential time size potential number scalar variables clique operations discrete potential take time linear number entries table exponential number discrete variables clique 
large cliques impose high computational cost contain discrete variables 
reason people able exactly solve large linear gaussian models kalman filters having resort kinds approximations discrete bayes net community 
new approach handling evidence need worry cliques contain hidden discrete variables 
precisely partition nodes hidden observed set discrete continuous set cost inference hybrid network set cliques number values node take discrete length continuous valued vector 
see ma detailed discussion complexity junction tree algorithm discrete networks 
experimental results see accurate variational approximation compared junction tree algorithm implemented bnt gibbs sampling implemented bugs network shown 
generated random examples joint distribution encoded network exact setting computed posterior distributions hidden variables possible pattern evidence junction tree bugs 
bugs burn iterations sampled iterations 
similar results achieved burn just plus iterations samples likelihood weighting 
junction tree updated variational parameters bayes net toolbox 
see www cs berkeley edu bayes bnt html 
bayesian inference gibbs sampling 
see www mrc cam ac uk bugs 
relative change log likelihood dropped observed unimodal distribution took iterations hidden bimodal distribution took iterations 
bugs implemented compiled modula took minutes real time cases respectively junction tree implemented interpreted matlab took minutes 
times sun ultra sparc 
results shown table 
averaged trials similarly expected values computed junction tree bugs respectively 
standard deviation brackets 
dash means variable observed inference necessary 
note binary random variable pr similarly observed perform inference exactly non zero values due finite sampling effects bugs 
hidden need variational approximation non zero values partly due errors incurred approximation partly due finite sampling effects source error driven samples 
results indicate variational approximation cases hidden rows 
case posterior bimodal peak near near corresponding respectively 
furthermore observed hard tell apart prior course 
note observing 
particularly helpful separated case rarely deviates mean cov bimodal difficult choose initial estimate cause algorithm converge poor local maximum lower bound log likelihood 
cheated starting variational algorithm correct value true values surprisingly variational method 
currently working ways improve quality approximation case distribution parents logistic node multimodal 
want extend variational approximation softmax nodes 
intend apply techniques hybrid dynamic bayesian networks extension traditional switching kalman filter model bsl 
fact better exact method optimal leaked information true values example case observations posteriors equal priors cheating method computed different estimates quantities example estimates closer true values exact priors 
derivation quadratic lower bound logistic function section derive quadratic lower bound sigmoid function details see 
consider log def function concave function log symmetric concave easy see def tangent line function upper bound tight def variational parameter indicating location tangent substitution tanh log find log get lower bound logistic function log log fact pr pr derivation update formula variational parameter get final result 
find optimal value iteratively maximize lower bound expected complete data log likelihood em nh :10.1.1.33.2557
term depends log log expectation observed data previous iteration 
differentiating get log table experimental results crop network junction tree 
cols 
means variable observed means hidden 
cols 

averaged trials similarly expected values computed jtree super exact method respectively 
standard deviation brackets 
dash means variable observed inference necessary 
see text details 
monotonically increasing obtained tr maximum taken data previous note derivation slightly general jj allow hidden net result turns 
aa agogino 
inference message propogation topology transformation vector gaussian continuous networks 
proc 
conf 
uncertainty ai 
binder koller russell kanazawa 
adaptive probabilistic networks hidden variables 
machine learning 
bsl bar shalom li 
estimation tracking principles techniques software 
artech house 
cf chang fung 
symbolic inference continuous variables 
proc 
conf 
uncertainty ai pages 
cf chang fung 
symbolic probabilistic inference discrete continuous variables 
ieee trans 
systems man cybernetics 
dec dechter 
bucket elimination unifying framework probabilistic inference 
jordan editor learning graphical models 
kluwer 
dm driver 
implementation continuous bayesian networks sums weighted gaussians 
proc 
conf 
uncertainty ai pages 
fg friedman goldszmidt 
discretizing continuous attributes learning bayesian networks 
machine learning proceedings international conference 
grs gilks richardson spiegelhalter 
markov chain monte practice 
chapman hall 
hd huang darwiche :10.1.1.125.4164
inference belief networks procedural guide 
intl 
approx 
reasoning 
ja jensen andersen 
approximations bayesian belief universes knowledge systems 
proc 
conf 
uncertainty ai 
jaakkola 
variational methods inference estimation graphical models 
phd thesis mit 
jordan ghahramani jaakkola saul 
variational methods graphical models 
jordan editor learning graphical models 
kluwer 
jj jensen jensen 
optimal junction trees 
proc 
conf 
uncertainty ai 
jj jordan jacobs 
hierarchical mixtures experts em algorithm 
neural computation 
jj jaakkola jordan 
variational approach bayesian logistic regression problems extensions 
ai statistics 
jj jaakkola jordan 
variational methods qmr dt database 
technical report computational cognitive science technical report april 
jor jordan 
logistic function 
tutorial discussion probabilities neural networks 
technical report mit computational cognitive science report august 
kjaerulff 
triangulation graphs algorithms giving small total state space 
technical report dept math 
comp 
sci aalborg univ denmark 
available www cs auc dk uk 
kk koller 
nonuniform dynamic discretization hybrid networks 
proc 
conf 
uncertainty ai 
lau lauritzen 
propagation probabilities means variances mixed graphical association models 
am 
stat 
assoc december 
lau lauritzen 
graphical models 
oup 
lei 

triangulated graphs marked vertices 
annals discrete mathematics 
lw lauritzen wermuth 
graphical models associations variables qualitative quantitative 
annals statistics 
ma mceliece aji 
generalized distributive law 
ieee trans 
info 
theory 
submitted 
mn mccullagh nelder 
generalized linear models 
chapman hall 
nh neal hinton :10.1.1.33.2557
new view em algorithm justifies incremental variants 
jordan editor learning graphical models 
kluwer 
ole olesen 
causal probabilistic networks discrete continuous variables 
ieee transactions pattern analysis machine intelligence 
rd rish dechter 
impact causal independence 
technical report information computer science uci 
sas shachter andersen szolovits 
global conditioning probabilistic inference belief networks 
proc 
conf 
uncertainty ai 
sk shachter 
gaussian influence diagrams 
managment science 
tip tipping 
probabilistic visualization highdimensional binary data 
neural info 
proc 
systems 
