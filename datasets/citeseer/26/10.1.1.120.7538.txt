information bottleneck gaussian variables gal chechik amir naftali tishby yair weiss tishby cs huji ac il school computer science engineering interdisciplinary center neural computation hebrew university jerusalem israel authors contributed equally problem extracting relevant aspects data addressed information bottleneck ib method soft clustering variable preserving information relevance variable 
interesting question addressed current extension ideas obtain continuous representations embeddings preserve relevant information discrete clusters 
give formal definition general continuous ib problem obtain analytic solution optimal representation important case multivariate gaussian variables 
obtained optimal representation noisy linear projection eigenvectors normalized correlation ma trix basis obtained canonical correlation analysis 
gaussian ib compression tradeoff parameter uniquely determines dimension scale eigenvector 
introduces novel interpretation solutions different ranks lie continuum parametrized compression level 
analysis provides analytic expression optimal tradeoff information curve terms eigenvalue spectrum 
extracting relevant aspects complex data fundamental task machine learning statistics 
problem data contains structures difficult define relevant unsupervised manner 
example speech signals may characterized volume level pitch content pictures ranked luminosity level color saturation importance regard task 
problem principally addressed information bottleneck ib approach :10.1.1.39.9882
joint distribution source variable relevance variable ib operates compress preserving information variable implicitly defines relevant isn formally cast variational problem min represents compression conditional distributions information maintains captured 
positive parameter determines tradeoff compression preserved relevant information lagrange multiplier constrained optimization problem min const 
information bottleneck approach applied far mainly categorical variables discrete represents soft clusters proved useful range applications documents clustering gene expression analysis see review 
general information theoretic formulation restricted terms variables compression variable naturally extended nominal continuous variables dimension reduction clustering techniques 
goal current 
general treatment ib continuous yields set self consistent equations obtained :10.1.1.39.9882
solving distributions generalized blahut algorithm proposed turn coupled eigenvector problems logarithmic log log functional derivatives respectively 
solving equations general turns difficult challenge 
cases problem turns analytically tractable joint multivariate gaussian variables shown 
optimal compression gaussian information bottleneck gib defined terms compression relevance tradeoff determined parameter 
turns noisy linear projection subspace dimension determined tradeoff parameter 
subspaces spanned basis vectors obtained known canonical correlation analysis cca method exact nature projection determined unique way tradeoff parameter 
specifically increases additional dimensions added projection variable series critical points structural phase transitions time relative magnitude basis vector rescaled 
process continues relevant information captured demonstrates ib formalism provides continuous measure model complexity information theoretic terms 
idea maximization relevant information taken imax framework output neuron ya serves define relevance output neighboring network yb 
imax aims maximize mutual information ya yb shown related canonical correlation analysis cca 
setting ya yb invariant scaling translation compression output representation modeled explicitly 
contrast ib framework aims characterize dependence solution explicit compression term scale sensitive measure transformation noisy 
view compressed representation inputs useful dealing neural systems stochastic nature limited response amplitudes noise characteristics constrained finite 
gaussian information bottleneck formalize problem information bottleneck gaussian variables 
jointly gaussian variables dimensions nx ny denote covariance matrices xy cross covariance matrix goal gib compress variable stochastic transformation variable nx preserving information gaussian optimal bound gaussian 
intuition second order correlations exist joint distribution distributions higher order moments carry additional information 
rigorously shown application entropy power inequality 
note explicitly limit dimension show effective dimension determined value 
random variables jointly gaussian distribution ax gaussian independent formalize problem minimization min noisy linear transformations parametrized transformation noise covariance 
normally distributed xa 
optimal projection main result characterization optimal function theorem optimal projection ax tradeoff parameter ix 
vt 
vt vt 
vt vt 
vt nx left eigenvectors sorted corresponding ascending eigenvalues 
nx critical values coefficients defined iri ri xvi nx dimensional row vector zeros semicolons separate rows matrix theorem asserts optimal projection consists eigenvectors combined interesting manner values smaller smallest critical point compression important information preservation optimal solution degenerated 
increased goes series critical points eigenvector added rank increases transition points changes smoothly function critical point coefficient vanishes 
parameterizes continuous rank projection 
illustrate form solution plot landscape target function solution simple problem case single non zero row thought row vector simplicity assume full rank reduced proper dimensionality 

target function function possible projections scalar projection parameters values xy 
optimal solution degenerated solution 
eigenvector norm theorem superimposed optimal 
length projects scalar shows target function function projection example 
therefor zero solution optimal corresponding eigenvector feasible solution target function manifold contains mirror minima 
increases minima starting single unified minimum zero split diverge apart 
turn prove theorem start rewriting formula entropy dimensional gaussian variable log denotes determinant 
schur complement formula calculate covariance conditional variable ty yt yat target function factor written log xa log log ya function noise projection easily shown pair projection dv allows simplify calculations replacing noise covariance matrix identity matrix 
identify minimum differentiate projection algebraic identity log aca ac holds symmetric matrix equating derivative zero rearranging obtain necessary conditions internal minimum ya id xa id 
equation shows multiplication reside span rows means spanned eigenvectors represent projection mixture rows left normalized eigenvectors mixing matrix weights eigenvectors 
remaining section characterize nature mixing matrix details proofs technical report 
theorem holds full rank limit generality discussion low rank matrices yield infinite values suboptimal 
lemma optimal mixing matrix diagonal matrix form diag 

krk vt 
vt 
nx eigenvectors eigenvalues 

proof write dv diagonal matrix elements corresponding eigenvalues denote diagonal matrix th element ri xvi 
nx substitute equation fact full rank obtain dr 
uniquely characterize note substitute target function equation properties eigenvalues log ri log ri th element diagonal shows depends norm columns matrices satisfy yield target function 
choose take diagonal matrix square root dr prove case nx consider matrix matrix padded zeros mixes eigenvectors 
case calculation similar gives solution nx zero rows 
complete proof remains shown solution capture extremum points 
point detailed due space considerations 
characterized set minima turn identify achieve global minima 
corollary global minimum obtained satisfying proof substituting optimal equation equation yields log log 
minimized eigenvalues satisfy taken observations prove value optimal projection obtained eigenvectors eigenvalues satisfy setting norm completes proof theorem 
gib information curve information bottleneck targeted characterizing tradeoff information preservation accuracy relevant predictions compression 
interestingly structure problem reflected information curve log 
gib information curve obtained eigenvalues 
information critical points designated circles 
comparison information curves calculated smaller number eigenvectors depicted curves calculated slope curve point corresponding tangent zero slope super imposed information curve 
maximal value relevant preserved information accuracy function complexity representation measured 
curve related rate distortion function lossy source coding limit channel coding side information 
shown concave general precise functional form depends joint distribution reveal properties hidden structure variables :10.1.1.121.8031
analytic forms information curve known special cases bernoulli variables intriguing self similar distributions 
analytic characterization gaussian ib problem allows obtain closed form expression information curve terms relevant eigenvalues 
substitute optimal projection isolate function ni log ni ni ni products ni eigenvalues obey critical condition cni cni cni ni log ni gib curve illustrated continuous smooth built segments increases additional eigenvectors projection 
derivative curve easily shown continuous decreasing yielding gib information curve concave 
value curve bounded tangent slope 
generally ib data processing inequality yields upper bound slope origin gib obtain tighter bound 
asymptotic slope curve zero reflecting law diminishing return adding bits description provide accuracy interesting relation spectral properties covariance matrices raises interesting questions special cases said spectrum patterns neural network learning problems 
relation works canonical correlation analysis imax gib projection derived uses weighted eigenvectors matrix ix xy yx eigenvectors canonical correlations analysis cca method descriptive statistics finds linear relations variables 
cca finds set basis vectors variable correlation coefficient projection variables basis vectors maximized 
bases eigenvectors matrices yx xy xy yx square roots corresponding eigenvalues canonical correlation coefficients 
cca shown special case continuous imax imax networks limited linear projections 
gib cca involve spectral analysis matrices inherent differences 
gib characterizes eigenvectors norm way depends trade parameter 
cca depends correlation coefficient compressed projected versions normalized measure correlation invariant rescaling projection vectors 
contrast value gib choose particular rescaling equation 
cca symmetric sense projected ib non symmetric variable compressed 
interesting gib cca eigenvectors projection multiterminal information theory information bottleneck formalism shown closely related problem source coding side information :10.1.1.121.8031
discrete variables encoded separately rates rx ry aim perfectly reconstruct bounds achievable rates case obtained ib information curve 
considering continuous variables lossless compression finite rates longer possible 
mutual information continuous variables longer interpretable terms encoding bits serves optimal measure information variables 
ib formalism coinciding coding theorems discrete case general sense reflects tradeoff compression information preservation concerned exact reconstruction 
reconstruction considered introducing distortion measures relevant question finding representations capture information variables 
discussion applied information bottleneck method continuous jointly gaussian variables continuous representation compressed variable derived analytic optimal solution general algorithm problem gib solely spectral properties covariance matrices problem 
solution gib characterized terms trade pa rameter compression preserved relevant information consists eigenvectors matrix continuously adding weaker compression complex models allowed 
provide analytic characterization information curve relates spectrum relevant information intriguing manner 
clean analytic structure gib offers new way analyzing empirical multivariate data correlation matrices estimated 
extends provides new information theoretic insight classical canonical correlation analysis method 
handling real world data relevance variable contains mul tiple structures correlated irrelevant 
information bottleneck side information alleviates problem side information form irrelevance variable information removed :10.1.1.134.9488
aims minimize 
functional analyzed case gaussian variables gaussian ib side information similar way analysis gib 
results generalized eigenvalue problem involving covariance matrices 
detailed solution problem function tradeoff parameters remains investigated 
categorical variables ib framework shown closely related maximum likelihood latent variable model 
interesting see gib cca equivalence extended give general understanding relation ib statistical latent variable models 
extension ib continuous variables reveals common principle regularized unsupervised learning methods ranging clustering cca 
remains interesting challenge obtain practical algorithms ib framework dimension reduction continuous gaussian assumption example adding non linearities projections 
tishby pereira bialek :10.1.1.39.9882
information bottleneck method 
proc 
th allerton conference communication computation 
slonim 
information theory applications 
phd thesis hebrew university jerusalem 
hotelling 
predictable criterion 
journal educational psychology 
becker hinton 
self organizing neural network discovers surfaces random dot stereograms 
nature 
becker 
mutual information maximization models cortical self organization 
network computation neural systems pages 
berger abd zamir 
semi continuous version berger yeung problem 
ieee transactions information theory pages 
chechik 
information bottleneck linear projections gaussian processes 
technical report hebrew university may 
wyner 
source coding side information decoder 
ieee trans 
info theory 
gilad bachrach tishby :10.1.1.121.8031
information theoretic tradeoff complexity accuracy 
proceedings colt washington 
chechik tishby :10.1.1.134.9488
extracting relevant structures side information 
becker thrun obermayer editors advances neural information processing systems 
slonim weiss 
maximum likelihood information bottleneck 
becker thrun obermayer editors advances neural information processing systems 
mika ratsch weston scholkopf smola muller 
invariant feature extraction classification kernel spaces 
solla leen muller editors advances neural information processing systems 
bell sejnowski 
information maximization approach blind blind deconvolution 
neural computation 
