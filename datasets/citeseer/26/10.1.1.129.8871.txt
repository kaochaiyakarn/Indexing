machine learning kluwer academic publishers boston 
manufactured netherlands 
simple statistical gradient algorithms connectionist reinforcement learning ronald williams ccs northeastern edu college computer science cn northeastern university huntington ave boston ma 
article presents general class associative reinforcement learning algorithms connectionist networks containing stochastic units 
algorithms called reinforce algorithms shown weight adjustments direction lies gradient expected reinforcement immediate reinforcement tasks certain limited forms delayed reinforcement tasks explicitly computing gradient estimates storing information estimates computed 
specific examples algorithms bear close relationship certain existing algorithms novel potentially interesting right 
results show algorithms naturally integrated backpropagation 
close brief discussion number additional issues surrounding algorithms including known limiting behaviors considerations help develop similar potentially powerful reinforcement learning algorithms 
keywords 
reinforcement learning connectionist networks gradient descent mathematical analysis 
general framework reinforcement learning encompasses broad variety problems ranging various forms function optimization extreme learning control 
research individual areas tends emphasize different sets issues isolation effective reinforcement learning techniques autonomous agents operating realistic environments address issues jointly 
remains useful research strategy focus limited forms reinforcement learning problems simply keep problems tractable important keep mind eventual solutions challenging problems probably require integration broad range applicable techniques 
article analytical results concerning certain algorithms tasks associative meaning learner required perform input output mapping limited exception involve immediate reinforcement meaning reinforcement payoff provided learner determined input output pair 
delayed reinforcement tasks obviously important receiving deserved attention lately widely approach developing algorithms tasks combine immediate reinforcement learner adaptive predictor critic temporal difference methods sutton 
actor critic algorithms investigated barto sutton anderson sutton clearly form learning algorithm watkins barto sutton watkins 
williams assumption learner search behavior necessary component form reinforcement learning algorithm provided means ran input output behavior learner 
common way achieve desired exploratory behavior worth noting strategies available certain cases including systematic search consistent selection ap parent best alternative 
strategy works situations goodness alter native actions determined estimates overly optimistic realistic continued experience occurs example search nilsson 
addition results framed terms connectionist networks main focus algorithms follow estimate relevant gradient 
algorithms known number limitations number reasons study useful 
experience backpropagation lecun parker rumelhart hinton williams werbos shown gradient provide powerful general heuristic basis generating algorithms simple implement surprisingly effective cases 
second sophisticated algorithms required gradient computation serve core algorithms 
extent certain existing algorithms resemble algorithms arising gradient analysis understanding may enhanced 
distinguishing property algorithms described statistically climbing appropriate gradient manage explicitly computing estimate gradient storing information estimate directly computed 
reason called simple title 
informative adjective non model 
point discussed section 
adopt connectionist perspective noted certain aspects analysis performed carry directly ways implementing adaptive input ouput mappings 
results apply general learner input output mappings consists parameterized input controlled distribution function outputs randomly generated corresponding algorithms modify learner distribution function basis performance feedback 
gra approach restriction potential applicability results certain obvious differentiability conditions met 
number results appeared various form earlier technical reports conference papers williams 

reinforcement learning connectionist networks specified assume learning agent feedforward network consisting individual units learning agent 
making additional assumption units operate stochastically useful consider case deterministic units net 
network operates receiving external input environment propagating gradient algorithms corresponding activity net sending activity produced output units environment evaluation 
evaluation consists scalar reinforcement signal assume broadcast units net 
point unit performs appropriate modification weights particular learning algorithm cycle begins 
notation follows yi denote output ith unit network denote pattern input unit 
pattern input vector individual elements typically denoted xj outputs certain units network sending output directly ith unit certain inputs environment unit happens connected receives input directly environment 
output yi drawn distribution depending weights input lines unit 
denote weight vector consisting weights denote weight matrix consisting weights wij network 
general setting viewed collec tion parameters behavior ith unit agent depends collection parameters behavior entire collection agents depends 
addition gi wi xi er yi wi xi gi probability mass function determining value yi function parameters unit input 
ease exposition consistently terminology notation appropriate case set possible output values yi discrete results de rived apply continuous valued units gi taken corresponding prob ability density function 
vector contains network parameters input output behavior ith unit depends just defined gi gi wi xi pr yi iw 
note quantities named yi de pend time generally convenient sequel suppress explicit time dependence understanding quantities appear single equation represent values time step assume new time step begins just external input network 
context immediate reinforcement tasks call time step cycle network environment interaction trial 
illustrate definitions introduce useful subclass define stochastic semilinear unit output yi drawn probability distribu tion mass function single parameter pi turn computed pi si differentiable squashing function si wij xj williams inner product viewed semilinear unit widely connectionist networks followed singly parameterized random number generator 
noteworthy special case stochastic semilinear unit bernoulli semilinear unit output yi bernoulli random variable parameter pi means possible output values pr yi pi pr yi pi 
bernoulli semilinear unit gi wi xi pi pi computed equations 
type unit common networks stochastic units appears example boltzmann machine hinton sej reinforcement learning networks explored barto colleagues barto anderson barto jordan barto sutton brouwer 
name bernoulli semilinear unit may appear simply fancy new name quite familiar term intended emphasize membership potentially general class 
particular form squashing function commonly logistic function si si stochastic semilinear unit bernoulli random number generator logistic squashing function called bernoulli logistic unit 
observe class bernoulli semilinear units includes certain types units computation alternatively described terms linear threshold computation additive input noise noisy threshold 
observation useful formulation barto colleagues barto barto anandan barto anderson barto sutton anderson barto sutton brouwer sutton investigations 
specifically assume unit computes output yi yi drawn randomly distribution 
see unit may viewed bernoulli semilinear unit si wi observe gradient thms pr pr yi si pr si pr lsi pr si 
long differentiable unit bernoulli semilinear unit squashing function fi si si 

expected reinforcement performance criterion order consider gradient learning algorithms necessary performance measure optimize 
natural immediate reinforcement learning pro associative expected value reinforcement signal conditioned particular choice parameters learning system 
reinforcement learning network performance measure denotes expectation operator reinforcement signal network weight matrix 
need expected values potential randomness environment choice input network network choice output correspon ding particular input environment choice reinforcement value particular input output pair 
note sense discuss dependently time assume third sources randomness deter ned stationary distributions environment choice input pattern net determined independently time 
absence assumptions expected value time step may function time history system 
tacitly assume stationarity dependence conditions hold 
note assumptions defined deterministic function cout unknown learning system 
formulation objective reinforcement learning system search space possible weight matrices point maximum 

reinforce algorithms consider network facing associative immediate reinforcement learning task 
recall weights adjusted network receipt reinforcement value trial 
suppose learning algorithm network trial parameter wij network incremented amount williams ij bij eij learning rate factor bij reinforcement baseline eij gi wij called characteristic eligibility suppose reinforcement baseline bij conditionally independent yi rate factor ij nonnegative depends 
typically ij taken constant 
ning algorithm having particular form called reinforce algorithm 
name acronym reward increment nonnegative factor offset reinforce ment characteristic eligibility describes form algorithm 
class algorithms interesting mathematical result theorem 
reinforce algorithm inner product aw nonnegative 
furthermore inner prod uct zero 
oz independent aw 
results relates gradient weight space performance measure aw average update vector weight space reinforce algorithm 
specifically says algorithm average date vector weight space lies direction performance measure 
sentence theorem equivalent claim weight wij quantity bq gi represents unbiased estimate 
proof theorem appendix number interesting special cases algorithms coin algorithms proposed explored literature novel 
showing existing algorithms reinforce algorithms follows immediately theorem applies 
consider novel algorithms belonging class 
consider bernoulli unit having input suppose parameter adapted pi pr yi 
equivalent action stochastic learning automaton narendra actions labeled 
probability mass function gi gi yi pi pii yi yi follows characteristic eligibility parameter pi yi gi yi pi pi pi yi pi assuming pi equal 
yi pi pi pi gradient algorithms particular reinforce algorithm unit obtained choosing reinforcement baseline rate factor pi pi 
gives rise algorithm having form api di yi pi result 
special case algorithm reinforcement signal limited coincides action version linear reward inaction lr stochastic learning automaton narendra 
network consisting unit constitutes team learning automata individual learning rate 
behavior teams lr automata investigated narendra wheeler wheeler narendra 
consider bernoulli unit 
case gi yi wi xi righthand side pi expressed terms equations 
compute characteristic eligibility particular parameter wij chain rule 
differentiating equations yields dpi dsi si osi xj 
noting gi opi yi wi xi right hand side multiply quantities find characteristic eligibility weight wij gi yi wi xi yi pi si xj pi pi long pi equal 
special case logistic function equation pi equal si pi pi characteristic eligibility simply gi yi wi xi yi pi xj 
ow consider arbitrary network bernoulli logistic units 
setting bij gives rise reinforce algorithm having form ar yi pi xj result 
interesting compare associative reward penalty algorithm barto barto anandan barto anderson barto jordan uses learning rule wij yi pi yi pi xj 
positive learning rate parameter 
called associative reward inaction ar algorithm see learning rule reduces equation case 
ar applied network bernoulli logistic units reinforce algorithm 
williams examples considered far reinforcement baseline 
reinforcement comparison sutton consistent reinforce 
strategy maintains adaptive estimate upcoming reinforce ment past experience 
particular example network bernoulli logistic units may learning rule yi pi xj reinforce algorithm long computation current value yi current value 
common approach computing exponential averaging scheme 
sophisticated strategies consistent reinforce framework including making function current input pattern unit 
analytical results offer basis comparing various choices reinforcement baseline reinforce algorithms generally believed reinforcement comparison leads algorithms having superior performance general 
discuss questions reinforce algorithm performance greater length 

episodic reinforce algorithms consider reinforce class algorithms extended certain learning problems having temporal credit assignment component may occur network contains loops environment delivers reinforcement values unknown possibly variable delays 
particular assume net trained episode episode basis episode consists time steps units may recompute outputs environment may alter non reinforcement input system time step 
single reinforcement value delivered net episode 
derivation algorithm unfolding time mapping yields arbitrary network operating fixed period time network having cycles exhibiting corresponding behavior 
unfolded network obtained duplicating time step 
formally amounts associating time dependent variable corresponding time indexed set variables values depend time property appropriate particular weight wij gives rise weights values happen equal value wij assumed wij constant episode 
form algorithm considered problem follows episode parameter wij incremented gradient algorithms ij bij eij notation defined earlier eij representing characteristic eligibility evaluated particular time definition eli sense acyclic network 
example completely interconnected recurrent network bernoulli logistic units updated synchronously eij yi pi 
quantities assumed satisfy conditions required reinforce algorithm particular reinforcement baseline bij independent output values yi rate factor depends episode number 
call algorithm form intended learning problem episodic reinforce algorithm 
example network consists bernoulli logistic units episodic reinforce algorithm prescribe weight changes rule ij bij yi pi xj 
result proved appendix theorem 
episodic reinforce algorithm inner product nonnegative 
furthermore inner product zero 
aij ot independent aw av 
noteworthy algorithm plausible line implementation single accumulator parameter network 
purpose ac cumulator form eligibility sum term depends operation network runs real time reinforcement signal eventually received 
general formulation episodic learning task possible reinforcement delivered network time step episode just 
case appropriate performance measure 
way create statistical gradient algorithm case simply replace interesting note causal depends network inputs outputs earlier times potentially better way perform necessary credit assignment 
roughly idea treat learning prob lem time step interval different overlapping episodic learning problems starting episode 
omit discussion details approach 

reinforce multiparameter distributions interesting application reinforce framework development learn ing algorithms units determine scalar output stochastically multiparameter williams distributions single parameter distributions stochastic semilinear units example 
way unit may compute fashion form deterministic computation weights input obtain values parameters controlling random number generation process draw put randomly appropriate distribution 
particular example normal distribu tion parameters mean standard deviation unit determining output distribution compute values draw output normal distribution mean equal value standard deviation equal value potentially useful feature gaussian unit mean variance output individually controllable long separate weights inputs determine parameters 
interesting control tantamount control unit exploratory behavior 
general random units multiparameter distributions potential control degree exploratory behavior independently choose explore single parameter distributions 
note reinforce algorithms unit easily derived particular case gaussian unit example 
commit particular means determining mean standard deviation unit output input weights simply treat unit mean standard deviation served adaptable parameters unit 
general functional dependence parameters actual adaptable parameters input unit simply requires application chain rule 
particular approach computation parameters separate weighted sums common set input lines somewhat different learning rule explored gullapalli 
simplify notation focus single unit omit usual unit index subscript 
unit set possible outputs set real numbers density function determining output single trial characteristic eligibility characteristic eligibility oa reinforce algorithm unit form gradient algorithms 
aa bo chosen appropriately 
reasonable algorithm obtained setting suitably small positive constant letting bo determined ding reinforcement comparison scheme 
interesting note resemblance equation giving characteristic eligibility parameter normal distribution equation giving characteristic eligibility parameter bernoulli distribution 
mean variance corresponding bernoulli random variable equations form 
fact characteristic eligibility mean parameter form wider variety distributions stated result proposition 
suppose probability mass density function form 
ok exp 
ok iz 
ok functions 
parameters mean distribution 
variance distribution 
mass density functions having form represent special cases exponential families distributions rohatgi 
easily checked number familiar distribu tions poisson exponential bernoulli normal distributions form 
proof proposition appendix 
compatibility backpropagation useful note reinforce reinforcement learning algorithms networks stochastic units works essentially measuring correlation variations local behavior resulting variations global performance reinforcement signal 
algorithms information williams effect connectivity units ignored unit network tries determine effect changes output changes reinforcement independently effect units directly connected 
contrast backpropagation algorithm works making fact entire chains effects predictable knowledge effects individual units 
backpropagation algorithm appropriate supervised learning networks deterministic units sense term backpropagation single component algorithm determines relevant partial derivatives means backward pass 
sense simply computational implementation chain nile 
mean ing term consider backpropagation integrated statistical gradient reinforcement learning algorithms investigated giving rise algorithms take advantage relevant knowledge network connec tivity appropriate 
examine ways backpropagation 

networks deterministic hidden units consider feedforward network having stochastic output units deterministic hidden units 
network reinforcement learning system sense hav ing randomness limited output units allows necessary exploration take place 
denote vector network input denote network output vector 
define pr probability mass function describing input output behavior entire network 
fact output network generally vector valued scalar valued arguments derive reinforce algorithms apply virtually unchanged global local perspective taken 
particular simple extension arguments prove theorem case vector valued output shows weight network bij wij represents unbiased estimate oe 
denote index set output units 
randomness output units randomness independent units er ii pr yk wk xk pattern appearing input kth unit result presentation pattern network 
note depends deterministically gradient algorithms xk aln gk wk xk clearly sum may computed backpropagation 
example output units bernoulli semilinear units parameters intermediate variables write characteristic eligibility weight wij gi op opt efficiently computed injecting yt pk pk just kth unit squashing function performing standard backward pass 
note weight attached output unit tion computation simply gives rise result derived earlier 
result essen tially backpropagated characteristic eligibility bernoulli parameter pi sub units consisting summer 
restricted attention networks having stochastic output units hard see result generalized network containing arbitrary mixture stochastic deterministic units 
algorithm case consists correlation style reinforce computation stochastic unit output unit backpropagation compute pre estimate relevant partial derivatives 
furthermore difficult prove general compatibility com putation unbiased estimates necessarily reinforce tion deterministic functions 
result essentially set variables depends deterministically second set variables backpropagating unbiased estimates partial derivatives respect set variables gives rise unbiased estimates partial derivatives respect second set variables 
intuitively reasonable true omit rigorous mathematical details result 
williams 
backpropagating random number generators form algorithm just described backpropagation deter portions network requires correlation style computation necessary obtain partial derivative information input side random number generator 
suppose possible random number generator 
see mean consider stochastic semilinear unit suppose function having deterministic dependence output yi 
example situation unit output unit reinforcement depending network output correct 
roughly able compute oj opi knowledge oj randomness expect deterministic relationship quantities 
reasonable property ask pi determined oy pi 
unfortunately property fails hold general 
example bernoulli unit straightforward check nonlinear function yi need particular relationship quantities 
output random number generator written differentiable function parameters approach just described backpropagating deterministic computation applied 
illustration consider normal random number generator gaussian unit 
output randomly generated parameters cr 
may write rz standard normal deviate 
see oy ol oy oa example may combine backpropagation gaussian hidden units reinforce output units 
case characteristic eligibility unit set equal computed output value characteristic eligibility parameter obtained multiplying worth noting particular results way depend fact mean standard deviation identical result applies represents translation parameter scaling parameter distribution 
generally tech nique obviously output expressed function parameters auxiliary random variables long dependence parameters differentiable 
gradient algorithms note argument results obtained backpropagation computing characteristic eligibility reinforce algorithm necessarily limited particular tion 
true backpropagation preserves un gradient estimates general form argument applied yield statistical gradient algorithms backpropagation variety situations network continuous valued stochastic units 
application supervised training networks 

algorithm performance issues convergence properties major limitation analysis performed immediately lead prediction asymptotic properties reinforce algorithms 
algorithm converge expect converge local maximum need convergence 
clear need analytical characterization asymptotic behavior reinforce algorithms results available ing simulation studies primary source understanding behavior algorithms 
give overview relevant simulation results reported literature currently preliminary 
sutton studied performance number algorithms single bernoulli unit networks facing nonassociative associative immediate reinforcement tasks 
algorithms investigated lr equations just reinforce reinforcement comparison 
studies reinforce reinforcement comparison outperform algorithms investigated 
williams peng investigated number variants reinforce nonassociative function optimization tasks networks bernoulli units 
studies demonstrated algorithms tend converge local optima expect gradient algorithm 
variants examined incor modifications designed help defeat undesirable behavior 
par interesting variant incorporated entropy term reinforcement signal helped enable certain network architectures perform especially tasks certain amount hierarchical organization search desirable 
preliminary studies carried networks bernoulli units single gaussian units 
gaussian unit studies described 
network studies involved multilayer recurrent networks facing supervised learning tasks ing reinforcement feedback 
case recurrent networks objective learn trajectory episodic reinforce 
noteworthy results studies required careful selection reinforcement func tion obtain solutions reinforce 
surprising turns obvious reinforcement functions select problems tend ave severe false maxima 
contrast ar generally succeeds finding solutions williams simpler reinforcement functions 
ar reinforce gen slow succeeds 
episodic reinforce especially slow surprising performs temporal credit assignment essentially spreading credit blame uniformly past times 
reinforce algorithm asymptotic behavior reasonably understood analytically action lr simulation experience obtained date number reinforce algorithms suggests range possible limiting behaviors may fact similar 
lr algorithm known converge single deterministic choice action probability 
noteworthy convergence spite fact expected motion direction best action follows theorem nonzero probability converging inferior choice action 
simpler example exhibits kind behavior biased random walk integers absorbing barriers 
motion biased particular direction nonzero probability absorbed barrier 
general reasonable conjecture consistent known analytically simple reinforce algorithms lr simulations sophisticated reinforce algorithms depending choice rein baseline algorithm converge local maximum expected reinforcement function nonzero typically com small probability convergence points lead zero variance net behavior 
discussion role reinforcement baseline see 

gaussian unit search behavior gaussian unit studies mentioned problems considered nonassociative involving optimization function single real variable adaptable parameters taken equations clear reinforcement comparison version reinforce unit behaves follows value sam leads higher function value obtained past moves similarly moves away points giving lower function values 
interesting tr adapted 
sampled point gives rise higher func tion value obtained past decrease tr increase change corresponds required 
corresponding behavior opposite direction sampled point leads lower value 
terms search amounts narrowing search better point suitably close mean worse point suitably far mean broadening search worse point suitably close mean better point suitably far mean 
sampled points roughly twice lie standard deviation mean follows sits top local hill sufficient breadth respect tr narrows allow convergence local maximum 
true local maximum flat top decrease point sampling worse values extremely gradient algorithms changing 
simulation studies deterministic noisy reinforce ment confirm behavior 
demonstrate nonnegative rein comparison reinforce may cause converge moved top hill 
viewed generalization potential convergence suboptimal performance described earlier ln 
interesting compare reinforce unit alternative algorithm adaptation proposed gullapalli 
approach adapted essentially manner reinforce adapted quite different manner 
reinforcement values assumed lie taken proportional strategy sense takes point view parameter controlling scale search performed optimum value function unknown 
situations known unsatisfactory formance achieved reasonable broaden scale order take coarse grained view search space identify broad region optimum reasonable chance 
relevant schmidhuber huber reported successful results networks having gaussian output units control tasks involving backpropagating model jordan rumelhart 
tion random number generators allow learning model learn ing performance proceed simultaneously separate phases 

choice reinforcement baseline important limitation analysis offers basis choosing various choices reinforcement baseline reinforce algorithms 
theorem applies equally choice extensive empirical investigation algorithms leads inescapable adaptive reinforcement baseline incorporating reinforcement comparison strategy greatly convergence speed cases lead big difference qualitative behavior 
example gaussian unit studies described 
simpler example provided single bernoulli semilinear unit bias weight input output affecting reinforcement 
positive easy see obtains kind biased random walk behavior leading nonzero probability convergence inferior output value 
con trast reinforcement comparison version lead values lying possible values leads motion better output value 
behavior occur choice lying possi ble values additional considerations applied distinguish wide variety possible adaptive reinforcement baseline schemes 
possibility considered briefly williams investigated fully dayan pick reinforcement baseline minimizes variance individual weight changes time 
turns yield mean reinforcement usual reinforcement comparison approach quantity difficult estimate effectively 
dayan simulation results suggest reinforcement baseline offers williams slight improvement convergence speed mean reinforcement convincing advantage remains demonstrated 

alternate forms eligibility reinforce reinforcement comparison prescribes weight changes pro product reinforcement factor depends current past reinforcement values factor called characteristic eligibility 
straightforward way obtain number variants reinforce vary form factors 
simulation study performed sutton involved variety algorithms obtained systematically varying factors 
par interesting variant having form included earlier study examined investigators rich sutton personal communication phil madsen personal communication williams peng promising 
studies conducted nonassociative tasks form algorithm describe 
furthermore principled basis deriving algorithms particular form developed somewhat unclear ex extended associative case 
consider specifically case bernoulli logistic unit having bias weight bias input standard reinforcement comparison version reinforce prescribes weight increments form aw computed exponential averaging scheme tz 
alternative algorithm rule aw updated updating particular algorithm generally converge faster reliably corresponding reinforce algorithm 
clear algorithms bear strong similarities 
variant obtained simply replacing viewed reasonable priori estimates output furthermore corresponding strategy generate variants reinforce number cases 
example randomness unit uses distribution proposition applies reinforce algorithm adjusting mean parameter involve factor simply replace gradient algorithms 
algorithm adapting mean gaussian unit tested behave 
arguments rich sutton personal communication suggest potential advantages ofy algorithms complete analysis performed 
interestingly possible analytical justification algorithms may considerations discussed 

local gradient estimates senses sense call reinforce algorithms simple implied title 
clear examples algorithms simple form 
simple derive essentially form random unit computation 
significant fact sense theorems climb appropriate gradient ex computing estimate gradient storing information estimate directly computed 
clearly alternative ways estimate gradients useful understand various techniques integrated effectively 
help distinguish variety alternative approaches define ter 
barto sutton watkins introduced term model describe essentially correspond indirect algorithms adaptive control field goodwin sin 
algorithms explicitly estimate relevant parameters underlying system controlled learned model system compute control actions 
corresponding notion immediate reinforcement learning system attempts learn explicit model reinforcement function learning system input output model guide parameter adjustments 
parameter adjustments gradient expected reinforce ment reinforce model yield estimates gradient 
algorithm backpropagation model proposed studied munro 
form model approach uses global model reinforcement function derivatives local model approach possible 
involve attempting estimate unit expected value reinforcement func tion input output unit gradient algorithm reinforce desired derivatives expected reinforcement 
studied sastry stochastic learning automata keeps ack average reinforcement received action general form 
learning watkins viewed involving learning local meaning case state models cumulative reinforcement 
reinforce fails model local sense may worthwhile consider algorithms attempt generate explicit gradient estimates lead algorithms having clearly identifiable strengths 
interesting possibility applies nonassociative case perform unit linear sion reinforcement signal output unit 
suspected algorithms williams form eligibility described may related approach fully analyzed 

analyses variety simulation experiments performed author suggest reinforce algorithms useful right importantly may serve sound basis developing effective reinforcement learning algorithms 
major advantage reinforce approach represents prescription devising statistical gradient algorithms reinforcement learning networks units compute random output essentially arbitrary fashion 
gradient approach gradient computation techniques backpropagation 
main disadvantages lack general convergence theory applicable class algorithms gradient algorithms apparent susceptibility convergence false optima 
acknowledgments benefitted numerous discussions rich sutton andy barto various aspects material 
preparation supported national science foundation iri 
notes 
detail aw vectors having dimensionality coordinate oe corresponding coordinate ofe aw 

strictly speaking choice algorithm guaranteeing negative normal distribution tails truncated necessarily case practice 
ap proach take adaptable parameter leads algorithm guaranteed keep positive 
appendix appendix contains proofs theorems reinforce episodic rein force algorithms respectively 
addition notation introduced text sym sets interest letting denote set possible output values yi ith unit xi denoting set possible values input vector unit 
critical assumption take xi discrete sets 
denote index set elements wij parameter system 
gradient algorithms remarked interest brevity assertions proved appendix convention unbound variable implicitly assumed universally quantified appropriate set values 
example appear considered arbitrary subject 

results reinforce algorithms fact 
oe yi gi wi xi wij wij proof 
conditioning possible values output yi may write yi pr yi er yi gi wi xi 
note specification value yi causes influence ultimate value means yi depend wij 
result follows differentiating sides equation respect wij 
fact 
proof 
ogi wi xi 
yi ge pr lw yi result follows differentiating respect 

oe ow proof note characteristic eligibility wri en williams eij gi og gi fails defined gi case defined reinforce algorithm long discrete 
gi wi xi means value zero probability occurrence value output yi 
yi pr yi gi lij wt bij xi yi gi xi aij ze yi ogi yi ow ij ij jl yi ogi wi xi yi making fact depend particular value output yi 
fact term expression aij oe xi 
consider remaining term 
bij yi bij xi assumption bij yi ogi bij ogi eyi eyi fact lemma proved 
fact 
oe oe pr 
proof 
conditioning possible input patterns may write gradient algorithms pr 
note weight lies downstream computation performed determine means pr depend result follows differentiating sides equation wij 
emma 
reinforce algorithm proof oe pr oe pr xf xi owl ij ow equality ob ined conditioning possible input patterns second equ follows lena follows assump tion depend input unit equality follows fact 
establishing result just lena conditioning input unit removed sides equation key step 
relates quantities lena quite messy compute explicitly general pr quite complicated 
lemma main result follows easily 
theorem 
reinforce 
fur en equality holds 
proof xw wi williams lemma result immediate 

results episodic reinforce algorithms analysis episodic reinforce algorithm unfolding time mapp ing associates original net unfolded time acyclic net 
key observation having face learning problem equivalent having face corresponding associative learning problem 
denote weight matrix individual components denoted 
weight rd corresponds weight tth time step correspondence nets noted specifying equivalent specifying 
correspondence learning problems consider reinforcement problems 
fact 

oe cg proof 
chain rule oe ow vd wij oe oe oe om ij ol iij ij emma 
episodic reinforce algorithm oe wij ij iy proof aw aij bij tk aw note represents reinforce algorithm follows lemma oe gradient algorithms aw jl lw oe oe equality follows fact 
theorem 
episodic reinforce algorithm aw 
furthermore equality holds 
proof aw aw oe oe ei lemma result immediate 
note proof theorem identical theorem 
theorem uses lemma theorem uses lemma lemmas 
appendix appendix devoted proof result proposition 
suppose probability mass density function form exp ok 
williams functions parameters mean distribution 
variance distribution 
proof consider case probability mass function corresponding argument density function 
denote support general og zg old ol ol 
combining fact yg find iz old og zy ol introduce shorthand notation oq od 
hypothesis proposition oq tz gradient algorithms tz 
ol ot tz zg combining see cw follows 
ng 
oh tz ae barto 

learning statistical cooperation self interested neuron computing elements 
human neurobiology 
barto anandan 

pattern recognizing stochastic learning automata 
ieee transactions systems man cybernetics 
barto anderson 

structural learning connectionist systems 
proceedings seventh annual conference cognitive science society pp 

irvine ca 
barto sutton anderson 

neuronlike elements solve difficult learning con trol problems 
ieee transactions systems man cybernetics 
barto sutton brouwer 

associative search network reinforcement learning associative memory 
biological cybernetics 
barto jordan 

gradient back propagation layered networks 
proceedings annual international conference neural networks vol 
ii pp 

san diego ca 
barto sutton watkins 

learning sequential decision making 
gabriel moore eds learning computational neuroscience foundations adaptive networks 
cambridge ma mit press 
williams dayan 

reinforcement comparison 
touretzky elman sejnowski hinton eds proceedings connectionist models summer school pp 

san mateo ca morgan 
goodwin sin 

adaptive filtering prediction control 
englewood cliffs nj prentice hall 
gullapalli 

stochastic reinforcement learning algorithm learning real valued functions 
neural networks 
hinton sejnowski 

learning relearning boltzmann machines 
rumelhart mcclelland eds parallel explorations microstructure cognition 
vol foundations 
cambridge ma mit press 
jordan rumelhart 

forward models supervised learning distal teacher 
occasional 
cambridge ma massachusetts institute technology center cognitive science 
lecun 

une procedure apprentissage pour learning procedure asymmetric threshold networks 
proceedings 
munro 

dual back propagation scheme scalar reward learning 
proceedings ninth annual conference cognitive science society pp 

seattle wa 
narendra 

learning automata 
englewood cliffs nj prentice hall 
narendra wheeler jr 

player sequential stochastic game identical payoffs 
ieee transactions systems man cybernetics 
nilsson 

principles artificial intelligence 
palo alto ca tioga 
parker 

learning logic 
technical report tr 
cambridge ma massachusetts institute tech nology center computational research economics management science 
rohatgi 
probability theory mathematical statistics 
new york wiley 
rumelhart hinton williams 

learning internal representations error tion 
rumelhart mcclelland eds parallel distributed processing explorations micro structure cognition 
vol foundations 
cambridge mit press 
schmidhuber huber 

learning generate focus trajectories attentive vision 
technical report 
technische institut informatik 
sutton 

temporal credit assignment reinforcement learning 
ph dissertation dept com puter information science university massachusetts amherst ma 
sutton 

learning predict methods temporal differences 
machine learning 
sastry 

new approach design reinforcement schemes learning automata 
ieee transactions systems man cybernetics 
wheeler jr narendra 

decentralized learning finite markov chains 
ieee transactions automatic control 
watkins 

learning delayed rewards 
ph dissertation cambridge university cambridge england 
werbos 

regression new tools prediction analysis behavioral sciences 
ph dissertation harvard university cambridge ma 
williams 

reinforcement learning connectionist networks mathematical analysis 
technical report 
san diego university california institute cognitive science 
williams 

reinforcement learning connectionist systems 
technical report nu ccs 
boston ma northeastern university college computer science 
williams 

class gradient estimating algorithms reinforcement learning neural networks 
proceedings annual international conference neural networks vol 
ii pp 

san diego ca 
williams 

backpropagation associative reinforcement learning 
proceedings second annual international conference neural networks vol 
pp 

san diego ca 
williams 

theory reinforcement learning connectionist systems 
technical report nu ccs 
boston ma northeastern university college computer science 
williams peng 

function optimization connectionist reinforcement learning algorithms 
connection science 

