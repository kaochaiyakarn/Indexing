technical report department statistics university toronto factor analysis delta rule wake sleep learning radford neal department statistics department computer science university toronto radford stat utoronto ca peter dayan department brain cognitive sciences massachusetts institute technology dayan ai mit edu july describe linear network models correlations real valued visible variables real valued hidden variables factor analysis model 
model seen linear version helmholtz machine parameters learned wake sleep method learning primary generative model assisted recognition model role fill values hidden variables values visible variables 
generative recognition models jointly learned wake sleep phases just delta rule 
learning procedure comparable simplicity oja version hebbian learning produces somewhat different representation correlations terms principal components 
argue simplicity wake sleep learning factor analysis plausible alternative hebbian learning model activity dependent cortical plasticity 
activity dependent plasticity vertebrate brain typically modeled terms hebbian learning hebb weight changes covariance presynaptic post synaptic activity von der malsburg linsker miller keller 
models derive support neurobiological evidence long term potentiation see example bliss review davis 
seen performing reasonable function extracting statistical structure collection inputs terms principal components 
suggest statistical technique factor analysis interesting alternative principal components analysis show implement algorithm demands synaptic plasticity local hebb rule 
factor analysis model real valued data correlations explained postulating presence underlying factors 
factors play role latent hidden variables directly observable allow dependencies visible variables expressed convenient way 
everitt gives latent variable models general factor analysis particular 
models widely psychology social sciences way exploring observed patterns data explainable terms small number unobserved factors 
interest models stems potential way building high level representations sensory data 
oja version hebbian learning oja karhunen oja particularly convenient counterpoint 
rule applies linear unit weight vector computes output real valued input vector convenience assumed mean zero 
presentation input vector weights unit changed amount proportionality term weight increment hebbian form 
second term tends push weights zero balancing positive feedback plain hebbian learning increase magnitude weights bound 
wyatt give explicit analysis learning equation showing reasonable starting conditions converges principal eigenvector covariance matrix inputs converges unit vector pointing direction highest variance input space 
extracting subsidiary eigenvectors covariance matrix inputs somewhat challenging requiring form inhibition successive output units sanger ldi plumbley 
linsker views hebbian learning way maximising information retained simplifying assumption distribution inputs gaussian setting output unit projection input principal component input covariance matrix conveys information possible average see plumbley 
goal reasonable early stages sensory processing information bottlenecks optic nerve may plausibly 
note implicitly assumes information equally important 
maximizing information transfer compelling goal subsequent levels processing sensory signals reached cortex 
computational goals suggested stage upwards including factorial coding barlow sparsification olshausen field various methods encouraging cortex respect reasonable invariances translation scale invariance visual processing li atick 
pursue suggestion hinton zemel see grenander mumford dayan hinton neal zemel cortex constructing hierarchical stochastic generative model input top connections implementing bottom connections recognition model sense inverse generative model 
recognition model provides high level interpretations sensory data serve basis behavior plays role learning generative model 
suggestion correct activity successive cortical areas represent input terms hierarchy latent variables interpretable encoding hidden causes observed input 
purpose code preserve raw information possible input support probabilistic model inputs matches closely possible actual distribution 
refer combination generative recognition models helmholtz machine 
attractive method learning models wake sleep algorithm hinton dayan frey neal 
simplest form helmholtz machine just layers linear units gaussian noise models 
simple helmholtz machine equivalent factor analysis model 
show empirically models learned wake sleep algorithm 
wake sleep phases learning procedure simple local delta rule 
wake sleep algorithm previously applied difficult task learning non linear models binary latent variables mixed results 
results obtained consistently wake sleep algorithm learn factor analysis models 
reason typical situation binary latent variables recognition model factor analysis perfectly invert generative model 
section describe factor analysis model 
section explains algorithm roots expectation maximisation em algorithm dempster laird rubin 
section presents experimental evidence factor analysis models learned wake sleep method appear situations algorithm operate perfectly 
discuss implications section 
factor analysis factor analysis single hidden factor generative model distribution vector real valued visible inputs single real valued factor assumed gaussian distribution mean zero variance 
vector factor loadings refer generative weights expresses relationships visible variable hidden factor 
vector means simplicity presentation taken zero stated 
noise assumed gaussian diagonal covariance matrix write called represent portion vari ance visible variable unique explained common factor 
refer generative variance parameters confused variance hidden factor generative model fixed 
model parameters define joint distribution hidden factor visible inputs helmholtz machine generative model accompanied recognition model represents conditional distribution hidden factor particular input vector recognition model simple form vector recognition weights gaussian distribution mean zero variance factor analysis model extended include hidden factors usually assumed independent gaussian distributions mean zero variance discuss possibilities section 
factors jointly produce distribution visible variables follows vector hidden factor values matrix generative weights factor loadings noise vector diagonal covariance 
linear recognition model represent conditional distribution hidden factors values visible variables 
note redundancy definition multiple factor model caused symmetry distribution generative weights generative model 
model unitary matrix produce distribution hidden factors presence multiple solutions causes difficulties interpreting results factor analysis statistical applications probably problem neurobiological modeling 
special circumstances factor analysis equivalent principal component analysis techniques general quite different jolliffe 
loosely speaking principal component analysis pays attention variance covariance factor analysis looks covariance 
particular components corrupted large amount noise principal eigenvector covariance ma trix inputs substantial component direction input 
hebbian learning result output dominated noise 
contrast fac tor analysis models noise affects component means large amount noise simply results corresponding large effect output furthermore principal component analysis essentially unaffected rotation coordinate system input vectors factor analysis privileges particular coordinate system data assumed equation coordinate system components noise independent 
factor analysis wake sleep algorithm section describe wake sleep algorithms learning factor analysis models starting simplest model having single hidden factor 
provide intuitive justification believing algorithms resemblance em algorithm 
complete theoretical proof wake sleep learning factor analysis works empirical evidence usually section 
maximum likelihood em algorithm methods performing factor analysis proposed origins technique early century 
computationally feasible algorithms developed performing factor analysis statistically attractive method maximum likelihood 
maximum likelihood learning parameters model chosen maximize probability density assigned model data observed likelihood 
factor analysis model single factor likelihood observed data independent cases obtained integrating possible values hidden factors take case model parameters described previously write probability densities conditional probability densities 
equation distribution hidden factor gaussian mean zero variance conditional distribution gaussian mean covariance implied equation 
wake sleep learning viewed approximate implementation expectation maximization em algorithm dempster laird rubin general approach maximum likelihood estimation variables case values hidden factors unobserved 
applications em factor analysis discussed dempster rubin thayer find em produces results identical method including getting stuck essentially local minima starting values parameters 
em iterative method iteration consists steps 
step finds conditional probability density unobserved variables observed data current estimates model parameters 
cases independent conditional distributions unobserved variables different cases dependent 
single factor model distribution hidden factor case observed data step finds new parameters values maximise expected loglikelihood complete data unobserved variables filled conditional distribution step factor analysis conditional distribution equation gaussian quantity expectation respect distribution required quadratic expectation easily computer 
matrix opera tions involved appear particularly plausible model learning brain 
wake sleep approach obtain learning procedure greater potential neurobiological interest eliminate explicit maximization step quantity defined terms expectation replacing gradient learning procedure expectation implicitly combining updates values stochastically generated appropriate conditional distributions 
furthermore avoid direct computation conditional distributions step learning produce recognition model trained tandem generative model 
approach results wake sleep learning procedure wake phase playing role step em sleep phase playing role step 
names phases metaphorical proposing neurobiological correlates wake sleep phases 
learning consists interleaved iterations phases operate follows wake phase observed values visible variables randomly fill values hidden factors conditional distribution defined current recognition model 
update parameters generative model filled case 
sleep phase observation randomly choose values hidden factors fixed generative distribution randomly choose fantasy values visible variables conditional distribution defined current parameters generative model 
update parameters recognition model fantasy case 
general scheme applied factor analysis model models multiple factors 
generative connections hidden factor visible variables recognition connections factor linear helmholtz machine 
connections generative model shown solid lines recognition model dashed lines 
weights connections wake sleep learning single factor model helmholtz machine implements factor analysis single hidden factor shown 
connections linear network implements generative model equation shown solid lines 
generation data network starts selection random value hidden factor gaussian distribution mean zero variance 
value th visible variable multiplying connection weight adding random noise vari ance 
bias value added produce non zero means visible variables 
recognition model connections shown dashed lines 
connections implement equation 
values visible input variables recognition network produces value hidden factor forming weighted sum inputs weight th input adding gaussian noise mean zero variance 
inputs non zero means recognition model include bias hidden factor 
parameters generative model learned wake phase follows 
values visible variables obtained external world set training case drawn distribution wish model 
current version recognition network stochastically fill corresponding value hidden factor equation 
generative weights updated delta rule follows small positive learning rate parameter 
generative variances updated exponentially weighted moving average learning rate parameter slightly 
recognition model correctly inverted generative model wake phase updates correctly implement step em algorithm limit 
update equation improves likelihood increment proportional derivative log likelihood training case filled follows averaging operation generative variances learned close lead maximum likelihood values filled values parameters recognition model learned sleep phase real data fantasy cases produced current version generative model 
values hidden factor visible variables fantasy case stochastically generated equation described section 
connection weights recognition model updated delta rule follows small positive learning rate parameter wake phase 
recognition variance updated follows learning rate parameter slightly 
aim sleep phase learning improve recognition model ability produce conditional distribution computed step em 
contrast exact correspondence wake phase step em operation sleep phase correspond obvious way exact implementation step 
problem justifying wake sleep procedure discussed section 
experiments reported section wake phase learning steps cases taken turn training set alternated sleep phase learning steps learning rate usually phases 
balanced operation phases essential convergence correct solution 
contrasts wake sleep phases learning boltzmann machines hinton sejnowski phases exactly balanced learning follow appropriate gradient 
wake sleep learning multiple factor models helmholtz machine hidden factor trained wake sleep algorithm way described single factor model 
generative model simply extended including hidden factor 
straightforward case values factors chosen independently random fantasy case generated 
new issue arises recognition model 
hidden factors visible variables recognition model form assuming means zero matrix contains weights recognition connections dimensional gaussian random vector mean covariance matrix general arbitrary generative model diagonal 
generation random covariance easily done computer cholesky decomposition method intended consideration neurobiological model prefer local implementation produces effect 
way producing arbitrary covariances hidden factors include chain connections recognition model link hidden factor hidden factors come earlier arbitrary ordering 
helmholtz machine architecture shown 
wake phase values hidden factors filled sequentially random generation gaussian distributions 
mean distribution picking value factor weighted sum inputs connections visible variables hidden factors values chosen earlier 
variance distribution factor additional recognition model parameter connections hidden factors variances associated factors independent degrees freedom accordingly recognition model form exists perfectly invert generative model 
method reminiscent proposals allow hebbian learning extract principal component sanger plumbley various order hidden units 
proposals connections remove correlations units represent different facets input 
helmholtz machine factors come represent different facets input jointly separately engaged capturing statistical regularities input 
connections hidden units capture correlations distribution hidden factors conditional values visible variables may induced joint responsibility modeling visible variables 
generative connections correlation inducing recognition connections recognition connections helmholtz machine implementing model hidden factors recognition model chain correlation inducing connections 
omits generative connections hidden factors visible variables recognition connections visible variables hidden factors indicated ellipses 
note connections hidden factors shown part recognition model 
generative connections hidden factors 
approach possible covariance matrix hidden factors generative model rotationally symmetric case identity matrix assumed far 
may freely rotate space factors unitary matrix making corresponding changes generative weights changing distribution visible variables 
generative model rotated corresponding recognition model rotate 
ro tation recognition covariance matrix diagonal represented just variances factor 
amounts forcing factors independent factorial long suggested goal early cortical processing barlow generally assumed non linear versions helmholtz machine 
hope learn multiple factor models wake sleep learning exactly way learn single factor models equations specifying values factors combine predict input vice versa 
seen section helmholtz machine capacity represent recognition variances correlation inducing connections usually able find rotation recognition model sufficient invert generative model 
note counterpart hebbian learning second approach connections hidden factors 
hebbian units connected manner extract single principal component input 
empirical results run number experiments synthetic data order test algorithm applied factor analysis finds parameter values locally maximize likelihood limit small values learning rates 
experiments provide data small learning rates practice reveal situations learning relatively slow 
report section results applying wake sleep algorithm real dataset everitt 
experimental procedure systematic experiments described done randomly generated synthetic data 
models various numbers visible variables various numbers hidden factors tested 
model sets model parameters generated generate sets training data cases second cases 
models learned training sets wake sleep algorithm 
models generate data randomly constructed follows 
initial values generative variances drawn independently exponential distributions mean initial values generative weights factor loadings drawn independently gaussian distributions mean zero variance 
parameters re scaled produce variance visible variable single factor model new generative variances set new generative weights chosen new variances visible variables equal 
experiments wake sleep learning procedure started generative recognition variances set generative recognition weights biases set zero 
note symmetry broken stochastic nature procedure 
learning done line cases training set time fixed sequence 
learning rates generative recognition models usually set values investigated described 
models generate data variables mean zero variance knowledge built learning procedure 
bias parameters allowing network learn means visible variables training data close zero exactly zero due finite training set 
compared estimates produced wake sleep algorithm maximum likelihood estimates training sets produced function plus version release uses modification method 
generative variances generative weights wake sleep learning single factor model variables 
graphs show progress generative variances weights course presentations input vectors drawn sequentially training set size learning parameters wake sleep phases 
training set randomly generated single factor model parameters picked random described section 
maximum likelihood parameter estimates plus shown horizontal lines 
implemented em factor analysis algorithm examine detail cases wake sleep disagreed plus 
stochastic learning algorithms fixed learning rates implies convergence best distribution parameter values highly concentrated near stable point 
general reduce learning rates time ensure stronger forms convergence 
software experiments may obtained internet 
experiments single factor models tried wake sleep algorithm learn single factor models visible variables 
shows progress learning typical run applied training set size 
shows progress presentations input vectors presentations training cases 
generative variances generative weights seen converge maximum likelihood estimates plus small amount random variation expected stochastic learning follow links author home page www cs utoronto ca radford procedure 
runs data generated random models training sets size showed similar convergence plus estimates presentations usually earlier 
remaining run training set size converged different local maximum plus maximization routine started values wake sleep 
convergence maximum likelihood estimates slower variables 
minimum number visible variables single factor model identifiable true values parameters data apart ambiguity signs weights 
runs training sets size runs training sets size failed clearly converge presentations convergence seen runs extended presentations case different local maximum initially plus 
slowest convergence training set size maximum likelihood estimate generative weights close zero making parameters nearly 
runs done learning rates generative recognition models 
tests training sets done learning recognition model slowed situation speculate cause problems due recognition model able keep generative model 
problems seen learning course slower 
experiments multiple factor models tried wake sleep algorithm learn models hidden factors synthetic data generated described section 
systematic experiments done inducing recognition weights models correlation cases mod els correlation inducing recognition weights 
experiments learning done behavior wake sleep learning similar model correlation inducing recognition connections model connections 
runs datasets converged presentations initially performed required iterations convergence apparent 
datasets size wake sleep learning converged different local maxima plus ones plus started parameter estimates wake sleep 
training set size maximum likelihood estimate generative variances close making model minimum number visible variables identifiability produced understandable difficulty convergence wake sleep estimates agreed fairly closely maximum likelihood values plus 
worrying discrepancy arose training sets size smallest generative variances wake sleep learning differed somewhat maximum likelihood estimates plus close zero 
plus started values wake sleep find similar local maximum simply estimates default initial values 
full em algorithm started estimates wake sleep barely changed parameters iterations 
explanation local maximum vicinity reason plus 
possible explanation likelihood extremely flat region 
case discrepancy cause worry regarding general ability method learn multiple factor models 
possible instance problem arose clearly connection everitt crime data reported section 
runs models correlation inducing recognition connections performed datasets runs converged presentations 
run training set size converged different local maximum plus 
tried runs model datasets higher learning rates 
higher learning rates produced higher variability bias parameter estimates datasets results generally correct 
runs models correlation inducing recognition connections performed datasets converged fine cases alternative local maxima 
datasets size small apparently real difference seen estimates smallest generative variances wake sleep maximum likelihood estimates plus 
case similar situation plus converge lo cal maximum vicinity started wake sleep estimates 
em algorithm moved slowly started wake sleep estimates possible explanation wake sleep having apparently converged point 
possible fundamental prevented convergence local maximum likelihood discussed connection similar results section 
experiments everitt crime data tried learning factor model dataset example everitt visible variables rates types crime cases american cities 
learning procedure experiments visible variables normalized mean zero variance bias parameters accordingly omitted generative recognition models 
fifteen runs different random number seeds done models correlation inducing recognition connection factors models connection 
runs produced results fairly close maximum likelihood estimates plus match results everitt 
runs small clearly real discrepancies notably smallest generative variances wake sleep estimates nearly zero maximum likelihood estimate zero 
behavior similar seen runs discrepancies systematic experiments section 
discrepancies arose frequently correlation inducing recognition connection runs connection included runs 
shows runs discrepancies model correlation inducing connection 
shows run differing random seed find maximum likelihood estimates 
sole run model correlation inducing recognition connection converged maximum likelihood estimates shown 
close comparison shows run convergence connection 
particular generative variance variable approaches zero slowly 
plus solution discrepant runs alternative local maximum 
furthermore extending runs iterations reducing learning rates large factor eliminate problem 
problem merely slow convergence due likelihood nearly flat vicinity hand em started discrepant wake sleep estimates movement maximum likelihood estimates slow iterations estimate generative variance variable moved third way wake sleep estimate maximum likelihood estimate 
runs discrepancies result local basin attraction wake sleep learning lead local maximum likelihood 
discrepancies eliminated model correlation inducing connection ways 
way reduce learning rate generative parameters wake phase leaving recognition learning rate unchanged 
discussed section theoretical reason think method lead maximum likelihood estimates 
generative learning rates reduced setting maximum likelihood estimates test runs 
second solution impose constraint prevents generative variances falling 
worked runs 
methods produce little improvement connection omitted recognition model successes runs smaller generative learning rates successes runs constraint generative variances 
models correlation inducing connections appears learning easier reliable 
tried learning model data normalizing visible variables mean zero variance biases included generative recognition models handle non zero means 
sensible thing means variables dataset far zero learning best take generative variances generative variances generative variances wake sleep learning factor models everitt crime data 
horizontal axis shows number presentations millions 
vertical axis shows generative variances maximum likelihood values indicated horizontal lines 
runs model correlation inducing recognition connection converge maximum likelihood estimates 
runs find maximum likelihood estimates 
run maximum likelihood estimates model correlation inducing connection 
long time biases slowly adjusted 
fact wake sleep learning fails spectacularly 
generative variances immediately quite large 
soon recognition weights depart significantly zero recognition variances quite large point positive feedback ensues weights variances diverge 
instability occur generative recognition weights variances large appears fundamental aspect wake sleep dynamics 
interestingly dynamics may operate avoid unstable region parameter space instability occur everitt data learning rate set low 
larger learning rate stochastic aspect learning produce jump unstable region 
discussion shown empirically wake sleep learning involves simple applications local delta rule implement statistical technique maximum likelihood factor analysis 
situations wake sleep method leads estimates locally maximize likelihood learning rate set small 
situations parameter estimates wake sleep deviated slightly maximum likelihood values 
needed determine exactly occurs note estimates wake sleep reasonably situations 
just usually computationally efficient implement principal components analysis standard matrix technique singular value decomposition hebbian learning factor analysis probably better implemented computer em rubin thayer second order newton methods wake sleep algorithm 
view application algorithm factor analysis interesting possible model activity dependent plasticity cortex simple successful example wake sleep learning 
discuss possible relating areas 
theoretical analysis wake sleep algorithm straightforward analysis wake sleep algorithm possible show wake sleep learning changes parameters generative recognition models fashion leads minimum function minima correspond local maxima likelihood generative model 
candidate function cost number bits takes code visible vector procedure employs distribution hidden variables defined recognition model 
show updates generative parameters wake phase produce decrease cost function 
difficulties arise sleep learning rule phase derived imagining network reversed 
intuitively reasonable defined hinton 
equation derived considering goal sleep learning recognition model invert generative model 
formally learning occurs sleep corresponds minimizing kullback leibler divergence distributions appear wrong order distribution visible variables produced generative model external world hinton 
algorithm correctly performs stochastic gradient descent recognition parameters correct cost function exist dayan hinton unfortunately involves reinforcement learning methods convergence usually extremely slow 
derived convergence results 
relies fact wake sleep learning applied factor analysis model single factor multiple factors having correlation inducing recognition connections adaptation recognition parameters ultimately lead recognition model exactly inverts generative model generative model kept fixed process 
follows wake sleep learning recognition learning rate larger generative learning rate sufficiently small method equivalent em 
second convergence result wake sleep learning second order lyapunov stable near stable point recognition weights correct 
offers weak guarantee actual solution unstable providing assurance solution 
empirical results reported section indicate general theoretical proof correctness come caveats regarding unstable regions parameter space 
proof convergence may possible discrepancies seen experiments due false basin attraction effect finite learning rates 
empirical finding difficult learn multiple factor models correlation inducing recognition connections included 
possible reason connections arbitrary generative model may perfectly invertible equivalent generative model perfectly inverted exists 
possible avenue theoretical analysis show wake sleep learning leads consistent estimates estimates converge true parameters unlimited amounts data generated model 
true wake sleep learning produce exact maximum likelihood estimates 
criterion correctness may case relevant true line context training case seen 
possible hierarchical extensions limits linear factor analysis model capture dependencies observed variables 
possible extension mixture factor analysers implemented em style algorithm 
applied successfully domain character recognition hinton revow dayan mixtures fac tor analysis models perform better mixtures principal component models 
augment mixture model allowing mixing proportions vary control gating network jacobs jordan nowlan hinton 
possibility build hierarchical model ghahramani hinton personal communication rao ballard 
willsky colleagues chou willsky benveniste chou willsky krim willsky karl willsky built sophisticated multi resolution tree architecture images combines interconnected factor analysers different spatial resolutions 
advantage tree step em done single bottom pass followed single top pass 
closely related rauch tung filtering smoothing algorithm 
course system linear remains incapable capturing second order correlations inputs prior covariance matrix image sophisticated structure 
single bottom topdown passes calculating true recognition distribution place start designing local schemes propagating information hierarchical factor analyser 
suggestive mean field methods saul jaakkola rao ballard exact linear version 
combining bottomup top information important applications generative model rightly police combination 
extracting second order structure requires non linearities system 
price paid guarantees recognition distribution tractable approximations typically 
realm helmholtz machine hinton mean field methods bayesian networks saul jaakkola sparse coding proposal olshausen field show promise worth proven empirically 
implications activity dependent plasticity main motivation original development helmholtz machine understand activity dependent development hierarchical structures cortex activation learning rules local hebb rule 
course progress modeling activity dependent plasticity simple hebbian learning rules embedded linear networks limited nonlinearities synaptic normalisation prevent weights growing bound 
networks model formation observable anatomical structures primary sensory topographic maps willshaw von der malsburg von der malsburg willshaw ocular dominance stripes nearby groups cells respond preferentially eye miller keller orientation domains groups nearby cells respond bars light nearby position orientation retina miller 
hebbian learning theoretically unsatisfying respects 
justified maximising transfer shannon information network 
may undesirable situations information really noise 
maximising infor mation appropriate goal stage processing information second layer network cortex layer layer forms entire input 
course developmental models generally impose restrictions connectivity allowing higher layers extract information combining cells lower layers receptive fields completely overlap 
second unsatisfying characteristic hebbian learning models accord role prominent features cortex notably top connections ubiquitously follow bottom projections 
helmholtz machine associated wake sleep learning algorithm potential alternative theory cortical self organisation may able avoid problems 
goal build statistical model inputs simply communicate information helmholtz machine pay undue attention noise higher layers intrinsically futile role 
furthermore helmholtz machine generative model embodied top connections parallels recognition model embodied bottom connections 
weights sets connections learned purely local delta rule cortical required construct predictions equation prediction errors equation 
rules equivalent delta rule conventional classical conditioning wagner sutton barto suggested underlying cortical plasticity montague sejnowski 
course wake sleep learning rule requires phases activation different connections primarily responsible driving cells phase 
suggestive evidence hasselmo bower implementational clearly required 
hebbian models activity dependent development key role played lateral local intra cortical connections longer range excitatory connections develop 
usually chosen form mexican hat nearby neurons tending excite distant ones tending inhibit 
encourages nearby cells similar tuning properties distant ones different properties 
instance miller keller constructed model development ocular dominance stripes bands layer iv striate cortex containing cells tuned just eye 
model position peak fourier transform local interconnections largely determines width stripes 
lateral connections generative models similar helmholtz machines inference done mean field methods olshausen field rao ballard 
lateral connections linear helmholtz machines far described play role inducing correlations hidden factors generative model recognition model 
lateral generative connections active sleep phase force correlation structure hidden factors mexican hat form 
recognition model able adapt accommodate correlation structure recognition models assume independence factors longer viable model lacks rotational symmetry needed ensure model showing independence exits 
structures ocular dominance stripes form network architecture 
lateral connections recognition model replace chain correlation inducing connections shown 
interesting investigate connections able support correlations generative recognition models 
purely linear models suffice fully explain intricacies information processing cortical hierarchy 
understanding factor analysis model learned simple local operations step understanding complex statistical models learned complex architectures involving non linear elements 
geoffrey hinton useful discussions 
research supported natural sciences engineering research council canada mh national institute mental health united states 
opinions expressed authors 
barlow 
unsupervised learning neural computation 
davis 
long term potentiation debate current issues volume cambridge massachusetts mit press 
chou willsky benveniste 
multiscale recursive estimation data fusion regularization ieee transactions automatic control 
chou willsky 
multiscale systems kalman filters riccati equations ieee transactions automatic control 
bliss 
receptors role long term potentiation trends neurosciences 
dayan hinton 
varieties helmholtz machine neural networks press 
dayan hinton neal zemel 
helmholtz machine neural computation 
dempster laird rubin 
maximum likelihood incomplete data em algorithm proceedings royal statistical society 
everitt 
latent variable models london chapman hall 
ldi 
adaptive network optimal linear feature extraction proceedings international joint conference neural networks washington dc 
grenander lectures pattern theory ii iii pattern analysis pattern synthesis regular structures berlin springer verlag 
hasselmo bower 
memory trends neurosciences 
hinton dayan frey neal 
wake sleep algorithm selforganizing neural networks science 
hinton revow dayan 
recognizing handwritten digits mixtures linear models tesauro touretzky leen editors advances neural information processing systems 
cambridge ma mit press 
hinton sejnowski 
learning relearning boltzmann machines rumelhart mcclelland pdp research group parallel distributed processing explorations microstructure cognition 
volume foundations cambridge massachusetts mit press 
hinton zemel 
minimum description length helmholtz free energy cowan tesauro alspector editors advances neural information processing systems san mateo california morgan kaufmann 
jacobs jordan nowlan hinton 
adaptive mixtures local experts neural computation 
jolliffe 
principal component analysis new york springer verlag 

contributions maximum likelihood factor analysis psychometrika 

general approach confirmatory maximum likelihood factor analysis psychometrika 

factor analysis squares maximum likelihood methods ralston wilf editors statistical methods digital computers volume mathematical methods digital computers new york wiley 
krim willsky karl 
multiresolution models random fields statistical image processing proceedings workshop information theory statistics new york ieee 
li atick 
theory striate cortex neural computation 
linsker 
basic network principles neural architecture proceedings national academy sciences 
linsker 
self organization perceptual network computer 
willsky 
likelihood calculation class multiscale stochastic models application texture discrimination ieee transactions image processing 
miller 
model development simple cell receptive fields ordered arrangement orientation columns activity dependent competition center inputs journal neuroscience 
miller keller 
ocular dominance column development analysis simulation science pp 
montague sejnowski 
predictive brain temporal coincidence temporal order synaptic learning mechanisms learning memory 
mumford 
neuronal architectures pattern theoretic problems koch davis editors large scale theories cortex cambridge ma mit press 
olshausen field 
sparse coding natural images produces localized oriented bandpass receptive fields nature press 
oja 
neural networks principal components subspaces international journal neural systems 
oja 
principal components minor components linear neural networks neural networks 
oja karhunen 
stochastic approximation eigenvectors eigenvalues expectation random matrix journal mathematical analysis applications 
plumbley 
efficient information transfer anti hebbian neural networks neural networks 
rao ballard 
dynamic model visual memory predicts neural response properties visual cortex technical report department computer science rochester new york 
wagner 
theory conditioning effectiveness reinforcement non reinforcement black editors classical conditioning ii current research theory pp 
new york appleton century crofts 
rubin thayer 
em algorithms ml factor analysis psychometrika 
sanger 
optimal unsupervised learning single layer linear feedforward neural network neural networks 
von der malsburg 
self organization orientation sensitive cells striate cortex 
von der malsburg willshaw 
label nerve cells interconnect ordered fashion proceedings national academy sciences 
willshaw von der malsburg 
patterned neural connections set self organisation proceedings royal society london 
willshaw von der malsburg 
marker induction mechanism establishment ordered neural mappings application problem philosophical transactions royal society pp 
wyatt jr 
time domain solutions oja equations neural computation 

