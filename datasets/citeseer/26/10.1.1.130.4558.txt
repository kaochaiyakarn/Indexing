neural networks statistical models proceedings nineteenth annual sas users group international conference april publicity ability artificial neural networks learn generalize 
fact commonly artificial neural networks called multilayer perceptrons nonlinear regression discriminant models implemented standard statistical software 
explains neural networks translates neural network jargon statistical jargon shows relationships neural networks statistical models generalized linear models maximum redundancy analysis projection pursuit cluster analysis 
neural networks wide class flexible nonlinear regression discriminant models data reduction models nonlinear dynamical systems 
consist large number neurons simple linear nonlinear computing elements interconnected complex ways organized layers 
artificial neural networks main ways models biological nervous systems intelligence real time adaptive signal processors controllers implemented hardware applications robots data analytic methods concerned artificial neural networks data analysis 
development artificial neural networks arose attempt simulate biological nervous systems combining simple computing elements neurons highly interconnected system hoping complex phenomena intelligence emerge result self organization learning 
alleged potential intelligence neural networks led research implementing artificial neural networks hardware vlsi chips 
literature remains confused artificial neural networks supposed realistic biological models practical machines 
data analysis biological plausibility hardware implementability irrelevant 
alleged intelligence artificial neural networks matter dispute 
artificial neural networks rarely neurons human brain neurons 
networks comparable human brain complexity far capacity fastest highly parallel computers existence 
artificial neural networks statistical methods capable processing vast amounts data making predictions surprisingly accurate warren sas institute cary nc usa intelligent usual sense word 
artificial neural networks learn way statistical algorithms estimation usually slowly statistical algorithms 
artificial neural networks intelligent statistical methods considered intelligent 
published works provide insight relationship statistics neural networks ripley probably best account date 
weiss kulikowski provide elementary discussion variety classification methods including statistical neural methods 
interested statistical aspects neural networks hinton offers readable inflated claims common popular accounts 
best book neural networks hertz krogh palmer consulted regarding neural net issues explicit citations 
hertz cover networks hopfield networks boltzmann machines 
masters source practical advice neural networks 
white contains reprints useful articles neural networks statistics advanced level 
models algorithms neural networks henceforth nns adjective artificial implied data analysis important distinguish nn models nn algorithms 
nn models similar identical popular statistical techniques generalized linear models polynomial regression nonparametric regression discriminant analysis projection pursuit regression principal components cluster analysis especially emphasis prediction complicated explanation 
nn models useful 
nn models learning vector quantization self organizing maps precise statistical equivalent may useful data analysis 
nn researchers engineers physicists psychologists computer scientists know little statistics nonlinear optimization 
nn researchers routinely reinvent methods known statistical mathematical literature decades centuries fail understand methods specht 
common implementations nns biological engineering criteria easy fit net chip established statistical optimization criteria 
standard nn learning algorithms inefficient designed implemented massively parallel computers fact usually implemented common serial computers ordinary pcs 
serial computer nns trained efficiently standard numerical optimization algorithms nonlinear regression 
nonlinear regression algorithms fit nn models orders magnitude faster standard nn algorithms 
reason inefficiency nn algorithms designed situations data stored observation available transiently real time environment 
transient data inappropriate types statistical analysis 
statistical applications data usually stored repeatedly accessible statistical algorithms faster stable nn algorithms 
practical data analysis applications usual nn algorithms useful 
need know nn training methods backpropagation nns 
jargon nn models similar identical known statistical models terminology nn literature quite different statistics 
example nn literature variables called features independent variables called inputs predicted values called outputs dependent variables called targets training values residuals called errors estimation called training learning adaptation selforganization 
estimation criterion called error function cost function lyapunov function observations called patterns training pairs parameter estimates called synaptic weights interactions called higher order neurons transformations called functional links regression discriminant analysis called supervised learning data reduction called unsupervised learning encoding autoassociation cluster analysis called competitive learning adaptive vector quantization interpolation extrapolation called generalization statistical terms sample population nn equivalents 
data divided training set test set cross validation 
network diagrams various models displayed network diagrams shown illustrates nn statistical terminology simple linear regression model 
neurons represented circles boxes connections neurons shown arrows circles represent observed variables name shown inside circle 
boxes represent values computed function arguments 
symbol inside box indicates type function 
boxes corresponding parameter called bias 
arrows indicate source arrow argument function computed destination arrow 
arrow usually corresponding weight parameter estimated 
long parallel lines indicate values fitted squares maximum likelihood estimation criterion 
input independent variable perceptrons output predicted value simple linear regression target dependent variable simple linear combination inputs possibly intercept bias term called net input 
possibly nonlinear activation function applied net input produce output 
activation function maps real input usually bounded range 
bounded activation functions called squashing functions 
common activation functions linear act identity act logistic tanh hyperbolic act tanh tangent act threshold gaussian act symbols network diagrams various types neurons activation functions shown 
perceptron outputs 
output separate bias set weights 
usually activation function output possible different activation functions 
notation formulas perceptron follows number independent variables inputs independent variable input bias output layer weight input output layer net input output layer predicted value output values act dependent variable training values residual error observed variable sum inputs power input linear combination inputs logistic function linear combination inputs threshold function linear combination inputs radial basis function inputs arbitrary value symbols neurons perceptrons trained squares attempting minimize summation outputs training set 
perceptron linear activation function linear regression model myers possibly multiple multivariate shown 
input independent variables output predicted values target dependent variables simple linear perceptron multivariate multiple linear regression perceptron logistic activation function logistic regression model shown 
input independent variables output predicted value target dependent variable simple nonlinear perceptron logistic regression perceptron threshold activation function linear discriminant function hand mclachlan weiss kulikowski 
output called shown 
multiple outputs threshold perceptron multiple discriminant function 
threshold activation function useful multiple logistic function estimate conditional probabilities class 
multiple logistic function called softmax activation function nn literature 
input independent variables output predicted value target binary class variable linear discriminant function activation function perceptron analogous inverse link function generalized linear model mccullagh nelder 
activation functions usually bounded inverse link functions identity reciprocal exponential functions 
inverse link functions required monotone implementations restriction computational convenience 
activation functions gaussian trigonometric functions 
fitted maximum likelihood variety distributions exponential class 
perceptrons usually trained squares 
maximum likelihood binomial proportions perceptrons target values usually number binomial trials assumed constant case criterion called relative entropy cross entropy 
occasionally criteria train perceptrons 
theory perceptrons thing practice overlap great theory 
polynomial regression represented diagram form shown arrows inputs polynomial terms usually constant weight 
nn terminology type functional link network pao 
general functional links transformations type require extra parameters activation function output identity model linear parameters 
elaborate functional link networks applications image processing perform variety impressive tasks iris group 
multilayer perceptrons functional link network introduces extra hidden layer neurons layer weights estimated 
model includes estimated weights inputs input independent variable functional hidden layer polynomial terms output predicted value target dependent variable functional link network polynomial regression hidden layer hidden layer uses nonlinear activation functions logistic function model genuinely nonlinear nonlinear parameters 
resulting model called multilayer perceptron mlp 
mlp simple nonlinear regression shown 
mlp multiple inputs outputs shown 
number hidden neurons number inputs outputs shown 
useful variation allow direct connections input layer output layer called main effects statistical terminology 
input independent variable hidden layer output predicted value target dependent variable multilayer perceptron simple nonlinear regression input independent variables hidden layer output predicted values target dependent variables multilayer perceptron multivariate multiple nonlinear regression input independent variables hidden layer output predicted values target dependent variables multilayer perceptron nonlinear regression notation formulas mlp follows number independent variables inputs number hidden neurons independent variable input bias hidden layer weight input hidden layer net input hidden layer hidden layer values act bias intercept output weight hidden layer output net input output layer predicted value output values act dependent variable training values residual error act act activation functions hidden output layers respectively 
mlps general purpose flexible nonlinear models hidden neurons data approximate virtually function desired degree accuracy 
words mlps universal approximators white 
mlps little knowledge form relationship independent dependent variables 
vary complexity mlp model varying number hidden layers number hidden neurons hidden layer 
small number hidden neurons mlp parametric model provides useful alternative polynomial regression 
moderate number hidden neurons mlp considered quasi parametric model similar projection pursuit regression friedman stuetzle 
mlp hidden layer essentially projection pursuit regression model mlp uses predetermined functional form activation function hidden layer projection pursuit uses flexible nonlinear smoother 
number hidden neurons allowed increase sample size mlp nonparametric sieve white provides useful alternative methods kernel regression smoothing splines eubank wahba 
mlps especially valuable vary complexity model simple parametric model highly flexible nonparametric model 
consider mlp fitting simple nonlinear regression curve input linear output hidden layer logistic activation function 
curve hidden neurons number hidden neurons estimation tends difficult case 
simple mlp acts polynomial regression leastsquares smoothing spline eubank 
polynomials linear parameters fast fit numerical accuracy problems try fit 
smoothing splines linear parameters don numerical problems high order polynomials splines problem deciding locate knots 
mlps nonlinear activation function genuinely nonlinear parameters take computer time fit polynomials splines 
mlps may numerically stable high order polynomials 
mlps require specify knot locations may suffer local minima optimization process 
mlps different extrapolation properties polynomials polynomials go infinity mlps flatten weird things extrapolated 
methods raise similar questions fit 
splines polynomials mlps easy extend multiple inputs multiple outputs exponential increase number parameters 
mlps usually trained algorithm called generalized delta rule computes derivatives simple application chain rule called backpropagation 
term backpropagation applied training method network trained manner 
confusion symptomatic general failure nn literature distinguish models estimation methods 
generalized delta rule slow tedious requiring user set various algorithmic parameters trial error 
fortunately mlps easily trained general purpose nonlinear modeling optimization programs procedures sas stat software model sas ets software nlp sas software various nlp routines sas software 
extensive statistical theory regarding nonlinear models bates watts cramer edwards gallant ross wild 
statistical software produce confidence intervals prediction intervals diagnostics various graphical displays rarely appear nn literature 
unsupervised learning nn literature distinguishes supervised unsupervised learning 
supervised learning goal predict target variables input variables 
supervision consists target values training 
supervised learning usually form regression discriminant analysis 
mlps common variety supervised network 
unsupervised learning nn literature claims target variable network supposed train extract features independent variables shown 
conceptualization wrong 
fact goal forms unsupervised learning construct feature variables observed variables really input target variables predicted 
unsupervised hebbian learning constructs quantitative features 
cases dependent variables predicted linear regression feature variables 
wellknown statistical theory optimal feature variables principal components dependent variables hotelling jackson jolliffe rao 
variations oja rule sanger rule just inefficient algorithms approximating principal components 
statistical model principal component analysis shown 
model inputs 
boxes input output unsupervised hebbian learning containing indicate values neurons computed way whatsoever provided squares fit model optimized 
course proven optimal values boxes principal component scores computed linear combinations observed variable 
model expressed observed variables shown inputs target values 
input layer hidden layer model unsupervised learning model 
rest implied unsupervised hebbian learning fact rarely acknowledged nn literature 
principal components predicted values principal component analysis dependent variables unsupervised competitive learning constructs binary features 
binary feature represents subset cluster observations 
network output neuron activated output output neurons forced 
neurons type called winner take neurons kohonen neurons 
input dependent variables output principal components predicted values dependent variables principal component analysis alternative model winner usually determined neuron largest net input words neuron weights similar input values measured inner product similarity measure 
inner product similarity measure useful usually necessary normalize weights neuron input values observation 
case inner product similarity equivalent euclidean distance 
normalization requirement greatly limits applicability network 
generally useful define net input euclidean distance synaptic weights input values case competitive learning network similar means clustering hartigan usual training algorithms slow 
superior clustering algorithms developed statistics numerical taxonomy fields described countless articles numerous books everitt massart kaufman anderberg sokal hartigan titterington smith makov mclachlan basford kaufmann rousseeuw spath 
adaptive vector quantization avq inputs acknowledged target values predicted means cluster observation belongs 
network essentially winner take activation functions 
words avq leastsquares cluster analysis 
usual avq algorithms simply compute mean cluster approximate mean iterative algorithm 
far efficient variety algorithms cluster analysis procedure 
feature mapping form nonlinear dimensionality reduction statistical analog 
varieties feature mapping kohonen self organizing map som best known 
methods principal components multidimensional scaling map continuous high dimensional space continuous low dimensional space 
som maps continuous space discrete space 
continuous space higher dimensionality necessary 
discrete space represented array competitive output neurons 
example continuous space inputs mapped output neurons array set input values turn outputs 
neurons neighbors output array correspond sets points input space close 
hybrid networks hybrid networks combine supervised unsupervised learning 
principal component regression myers example known statistical method viewed hybrid network layers 
independent variables input layer principal components independent variables hidden unsupervised layer 
predicted values regressing dependent variables principal components supervised output layer 
networks widely touted hybrid networks learn rapidly backpropagation networks 
networks variables divided sets say goal able predict variables variables variables variables 
network effectively performs cluster analysis variables 
predict particular observation compute distance observation cluster mean variables find nearest cluster predict mean variables nearest cluster 
method predicting obviously reverses roles usual algorithm usual inefficient 
far efficient procedure clustering impute option predictions 
offers advantage predict subset variables disjoint subset variables 
practice bidirectional prediction done rarely needed 
usually prediction direction 
form nonparametric regression smoothing parameter number clusters 
training unidirectional estimator tukey bins determined clustering input cases 
bidirectional training input target variables forming clusters clusters adaptive local slope regression surface create problems data smoothness estimate depends local variance target variables 
bidirectional training adds complication choosing relative weight input target variables cluster analysis 
clearly advantages discontinuous regression functions ineffective discounting independent variables little predictive value 
continuous regression functions improved additional smoothing 
nn literature usually uses interpolation kernel smoothing superior cases 
kernel smoothed variety binned kernel regression estimation clusters bins similar clustered form specht 
learning vector quantization lvq kohonen supervised unsupervised aspects hybrid network strict sense having separate supervised unsupervised layers 
lvq variation nearest neighbor discriminant analysis 
finding nearest neighbor entire training set classify input vector lvq finds nearest point set prototype vectors class 
lvq differs edited condensed nearest neighbor methods hand prototypes members training set computed algorithms similar avq 
somewhat similar method proceeds clustering class separately cluster centers prototypes 
clustering approach better want estimate posterior membership probabilities lvq may effective goal simply classification 
radial basis functions mlp net input hidden layer linear combination inputs specified weights 
radial basis function rbf network wasserman shown hidden neurons compute radial basis functions inputs similar kernel functions kernel regression 
net input hidden layer distance input vector weight vector 
weight vectors called centers 
distance usually computed euclidean metric weighted euclidean distance inner product metric 
usually bandwidth associated hidden node called sigma 
activation function variety functions nonnegative real numbers maximum zero approaching zero infinity outputs computed linear combinations hidden values identity activation function 
input independent variables radial basis functions kernel functions output predicted values radial basis function network target dependent variables comparison typical formulas mlp hidden neuron rbf neuron follows mlp rbf region near rbf center called receptive field hidden neuron 
rbf neurons called localized receptive fields locally tuned processing units potential functions 
rbf networks closely related regularization networks 
modified kanerva model prager fallside rbf network threshhold activation function 
restricted coulomb energy tm system cooper reilly threshold rbf network classification 
discrete variant rbf networks called cerebellum model articulation controller cmac miller kraft 
hidden layer values normalized sum moody darken commonly done kernel regression nadaraya watson 
observation taken rbf center weights taken target values outputs simply weighted averages target values network identical known nadaraya watson kernel regression estimator 
method reinvented twice nn literature specht ler hartmann 
specht popularized kernel regression calls general regression neural network kernel discriminant analysis calls probabilistic neural network 
specht claim effective samples sparse data multidimensional space directly contradicted statistical theory 
parametric models error prediction typically decreases proportion sample size 
kernel regression estimators error prediction typically decreases proportion number derivatives regression function number inputs 
kernel methods tend require larger sample sizes methods especially multidimensional spaces 
rbf network viewed nonlinear regression model weights estimated usual methods nonlinear squares maximum likelihood yield vastly model observation rbf center 
usually rbf networks treated hybrid networks 
inputs clustered rbf centers set equal cluster means 
bandwidths set nearest neighbor distance center moody darken idea nearestneighbor distances excessively variable works better determine bandwidths cluster variances 
centers bandwidths determined estimating weights hidden layer outputs reduces linear squares 
method training rbf networks consider case potential center select subset cases usual methods subset selection linear regression 
forward stepwise selection method called orthogonal squares ols chen 
adaptive resonance theory nns explicitly neurophysiology 
adaptive resonance theory art best known classes networks 
art networks defined algorithmically terms detailed differential equations terms recognizable statistical model 
practice art networks implemented analytical solutions approximations differential equations 
art estimate parameters useful statistical sense may produce degenerate results trained noisy data typical statistical applications 
art doubtful benefit data analysis 
art comes varieties unsupervised simplest called art 
moore pointed art basically similar iterative clustering algorithms case processed 
finding nearest cluster seed prototype template case 
updating cluster seed closer case nearest closer defined hundreds different ways 
art differs clustering methods uses stage lexicographic measure nearness 
inputs seeds binary 
binary similarity measures defined terms table giving numbers matches mismatches input seed prototype template example hamming distance number mismatches jaccard coefficient number positive matches normalized number features matters slightly art defines nearest seed seed minimum value satisfies requirement exceeds specified vigilance threshold 
input seed satisfy vigilance threshold said resonate 
input fails resonate existing seed new seed identical input created hartigan leader algorithm 
input existing seed seed updated logical operator feature updated seed input seed updating 
seed represents features common cases assigned 
input contains noise form seeds tend degenerate zero vector clusters proliferate 
art network quantitative data 
differs art mainly having elaborate iterative scheme normalizing inputs 
normalization supposed reduce cluster proliferation plagues art allow varying background levels visual pattern recognition 
fuzzy art carpenter grossberg rosen bounded quantitative data 
similar art uses fuzzy operators min max place logical operators 
artmap carpenter grossberg reynolds artistic variant supervised learning 
art jargon 
example data called arbitrary sequence input patterns 
current observation stored short term memory cluster seeds long term memory 
cluster maximally compressed pattern recognition code 
stages finding nearest seed input performed attentional subsystem orienting subsystem performs hypothesis testing simply refers comparison vigilance threshhold hypothesis testing statistical sense 
multiple hidden layers mlp hidden layer universal approximator exist various applications hidden layer useful 
highly nonlinear function approximated fewer weights multiple hidden layers hidden layer 
maximum redundancy analysis rao van den linear mlp hidden layer dimensionality reduction shown 
nonlinear generalization implemented mlp adding hidden layer introduce nonlinearity shown 
linear hidden layer bottleneck accomplishes dimensionality reduction 
independent variables redundancy components predicted values dependent variables linear multilayer perceptron maximum redundancy analysis principal component analysis shown linear model dimensionality reduction inputs targets variables 
nn literature models inputs targets called encoding autoassociation networks hidden layer 
hidden layer sufficient improve principal components seen 
nonlinear generalization principal components implemented mlp hidden layers shown 
third hidden independent variables dependent variables nonlinear transformation redundancy components nonlinear components nonlinear nonlinear analog principal components predicted values dependent variables nonlinear maximum redundancy analysis predicted values dependent variables layers provide nonlinearity second hidden layer bottleneck 
nonlinear additive models provide compromise complexity multiple linear regression fully flexible nonlinear model mlp high order polynomial tensor spline model 
generalized additive model gam hastie tibshirani nonlinear transformation estimated nonparametric smoother applied input values added 
procedure fits nonlinear additive models splines 
topologically distributed en coding geiger uses gaussian basis functions 
nonlinear additive model implemented nn shown 
input connected small subnetwork provide nonlinear transformations 
outputs subnetworks summed give output complete network 
network reduced single hidden layer additional hidden layers aid interpretation results 
adding linear hidden layer gam network projection pursuit network constructed shown 
network similar projection pursuit regression friedman stuetzle subnetworks provide nonlinearities nonlinear smoothers 
goal creating artificial intelligence lead fundamental differences philosophy neural engineers statisticians 
ripley provides illuminating discussion philosophical practical differences neural statistical methodology 
neural engineers want networks black boxes requiring human intervention data predictions 
marketing hype claims neural networks experience automatically learn required course nonsense 
doing simple linear regression requires nontrivial amount statistical expertise 
independent variables independent variables nonlinear projection transformation predicted value generalized additive network nonlinear transformation projection pursuit network dependent variable predicted value dependent variable multiple nonlinear regression model mlp requires knowledge experience 
statisticians depend human intelligence understand process study generate hypotheses models test assumptions diagnose problems model data display results comprehensible way goal explaining phenomena investigated 
vast array statistical methods analysis simple experimental data experience judgment required choose appropriate methods 
applied statistician may spend time defining problem determining appropriate questions ask statistical computation 
applied statistics reduced automatic process expert system foreseeable 
artificial neural networks supersede statistical methodology 
neural networks statistics competing methodologies data analysis 
considerable overlap fields 
neural networks include models mlps useful statistical applications 
statistical methodology directly applicable neural networks variety ways including estimation criteria optimization algorithms confidence intervals diagnostics graphical methods 
better communication fields statistics neural networks benefit 
anderberg 
cluster analysis applications new york academic press 
bates watts 
nonlinear regression analysis applications new york john wiley sons 

model discrimination nonlinear regression models new york marcel dekker 
carpenter grossberg reynolds 
artmap supervised real time learning classification nonstationary data self organizing neural network neural networks 
carpenter grossberg rosen 
fuzzy art fast stable learning categorization analog patterns adaptive resonance system neural networks 
chen orthogonal squares algorithm radial basis function networks ieee transactions neural networks 
cooper reilly 
self organizing general pattern class separator identifier patent 
cramer 
econometric applications maximum likelihood methods cambridge uk cambridge university press 
edwards likelihood cambridge uk cambridge university press 
everitt 
cluster analysis nd edition london educational books 
simultaneous linear prediction psychometrika 
friedman stuetzle 
projection pursuit regression journal american statistical association 
gallant 
nonlinear statistical models new york john wiley sons 
geiger 
storing processing information connectionist systems ed advanced neural computers amsterdam north holland 

nonlinear multivariate analysis chichester uk john wiley sons 
hand 
discrimination classification new york john wiley sons 

applied nonparametric regression cambridge uk cambridge university press 
hartigan 
clustering algorithms new york john wiley sons 
hastie tibshirani 
generalized additive models london chapman hall 
hertz krogh palmer 
theory neural computation redwood city ca addison wesley 
hinton 
neural networks learn experience scientific american september 
hotelling 
analysis complex statistical variables principal components journal educational psychology 

applied logistic regression new york john wiley sons 
huber 
projection pursuit annals statistics 
jackson 
user guide principal components new york john wiley sons 
jolliffe 
principal new york springer verlag 
jones sibson 
projection pursuit journal royal statistical society series 
kaufmann rousseeuw 
finding groups data new york john wiley sons 
mccullagh nelder 
generalized linear models nd ed london chapman hall 
mclachlan 
discriminant analysis statistical pattern recognition new york john wiley sons 
mclachlan basford 
mixture models new york marcel dekker massart kaufman 
interpretation analytical chemical data cluster analysis new york john wiley sons 
masters 
practical neural network recipes new york academic press 
miller iii kraft iii 
cmac associative neural network alternative backpropagation proceedings ieee 
moore 
art pattern clustering touretzky hinton sejnowski eds proceedings connectionist models summer school san mateo ca morgan kaufmann 
myers 
classical modern regression applications boston duxbury press nadaraya 
estimating regression theory probab 
applic 

pao adaptive pattern recognition neural networks reading ma addison wesley 
prager fallside 
modified kanerva model automatic speech recognition computer speech language 
rao 
interpretation principal component analysis applied research series 
ripley 
statistical aspects neural networks barndorff nielsen jensen kendall eds networks chaos statistical probabilistic aspects london chapman hall 
ross 
nonlinear estimation new york springer verlag 
ler hartmann 
mapping neural network derived parzen window estimator neural networks 
wild 
nonlinear regression new york john wiley sons 
sokal 
numerical taxonomy san francisco freeman 
iris group fast learning invariant object sixth generation breakthrough new york john wiley sons spath 
cluster analysis algorithms chichester uk ellis horwood 
specht 
generalized regression neural network ieee transactions neural networks nov 
titterington smith makov 
statistical analysis finite mixture distributions new york john wiley sons 
tukey 
curves parameters touch estimation proceedings th berkeley symposium 
van den 
redundancy analysis alternative canonical correlation analysis psychometrika 
wasserman 
advanced methods neural computing new york van nostrand reinhold 
watson 
smooth regression analysis series 

applied linear regression new york john wiley sons 
weiss kulikowski 
computer systems learn san mateo ca morgan kaufmann 
white 
artificial neural networks approximation learning theory oxford uk blackwell 
sas ets sas sas sas stat registered trademarks sas institute usa countries 
indicates usa registration 
brand product names respective companies 

