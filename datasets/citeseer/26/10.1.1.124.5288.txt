ideal spatial adaptation wavelet shrinkage david donoho iain johnstone department statistics stanford university stanford ca june revised april ideal spatial adaptation oracle furnishes information best adapt spatially variable estimator piecewise constant piecewise polynomial variable knot spline variable bandwidth kernel unknown function 
estimation aid oracle ers dramatic advantages traditional linear estimation nonadaptive kernels priori unclear performance obtained procedure relying data 
describe new principle spatially adaptive estimation selective wavelet reconstruction 
spline ts piecewise polynomial ts equipped oracle select knots dramatically powerful selective wavelet reconstruction oracle 
develop practical spatially adaptive method works shrinkage empirical wavelet coe cients 
mimics performance oracle selective wavelet reconstruction possible 
new inequality normal decision theory oracle inequality shows attained performance di ers ideal performance factor logn sample size 
estimator give better guarantee 
class spatially adaptive procedures essentially optimal 
relying data comes factor log performance piecewise polynomial variable knot spline methods equipped oracle 
contrast unknown piecewise polynomial methods function denied access oracle forced rely data 
keywords minimax estimation subject doing point orthogonal wavelet bases compact support piecewise polynomial tting variable knot spline 
suppose data yi ti ei ti independently distributed asn andf unknown function wewould 
measure performance estimate terms quadratic loss sample points 
detail letf ti ti denote vectors true estimated sample values respectively 
denote usual squared norm measure performance risk ek fk wewould small possible 
suggests function real equally spaced sample spatially adaptive methods particularly interested variety spatially adaptive methods proposed statistical literature breiman friedman olshen stone turbo friedman silverman mars friedman kernel methods muller 
methods presumably introduced expected better job recovery functions occurring real data traditional methods xed spatial scale series methods xed bandwidth kernel methods linear spline smoothers 
informal conversations leo breiman jerome friedman con rmed assumption 
describe simple framework encompasses important spatially adaptive methods allows develop main theme ciently 
consider estimates de ned wheret formula spatial smoothing parameter andd data adaptive choice spatial smoothing parameter clearer picture intend emerges examples 

piecewise constant 
nite list say numbers de ning partition il il pl 
note variable 
reconstruction formula tpc lx ave yi ti piecewise constant reconstruction mean data piece estimate pieces 

piecewise pp 
interpretation reconstruction uses polynomials 
pp lx pd determined applying squares principle data arising ti yi min 

variable knot 
de nes partition interval partition reconstruction formula polynomial reconstruction continuous derivatives 
detail left endpoint ofi reconstruction chosen piecewise satisfying dt dt fork subject constraint solves nx ti yi min 

variable bandwidth kernel 
function represents bandwidth kernel att smoothing ac function compact support probability density nx ti re ned versions formula boundary ects 

variable bandwidth high order 
local bandwidth reconstruction formula isac function integrating vanishing intermediate moments dt asd nonnegative 
reconstruction techniques equipped appropriate selectors spatial smoothing parameter duplicate essential features certain known methods 
piecewise constant reconstruction equipped choice partition recursive partitioning cross choice pruning constant described breiman friedman olshen stone results method cart applied dimensional data 
spline reconstruction spl equipped backwards deletion scheme models methods friedman silverman friedman applied dimensional data 
kernel equipped variable bandwidth selector described gasser herrmann results heidelberg variable bandwidth smoothing method 
compare scott 
schemes computationally feasible intuitively appealing 
little known theoretical performance adaptive schemes level uniformity inf andn 
ideal adaptation oracles avoid messy questions abandon study speci selectors study ideal adaptation 
ideal adaptation performance smoothing aid oracle 
oracle tell tell best choice true 
oracle response conceptually selection satis es rn denotes ideal risk rn rn measures performance selection full knowledge data dependent represents ideal expect attain 
target shall consider 
ideal adaptation ers principle considerable advantages traditional nonadaptive linear smoothers 
consider case functionf piecewise polynomial nite number il say lx assume thatf discontinuities break points risk ideally adaptive piecewise polynomial ts essentially oracle supply information il partition 
traditional squares theory says data traditional linear independently distributed asn traditional squares estimator satis es number parameters variance noise applying setting tting function form requires tting pieces degree parameters ek fk advertised 
hand risk spatially non adaptive procedure far worse 
consider kernel smoothing 
discontinuities kernel smoother xed varying bandwidth attains tending zero faster kernel 
result holds estimates orthogonal series polynomials sinusoids smoothing splines knots sample points squares smoothing splines knots equispaced 
strikingly piecewise polynomial ts equal width pieces wehave breakpoints form subset breakpoints happen lim short oracles er improvement ideally risk better performance expected usual parametric rate estimating nite dimensional parameters 
approach ideal performance estimators data 
selective wavelet reconstruction spatially adaptive method new principle spatially adaptive estimation developed wavelets ideas 
introductions historical accounts may books daubechies meyer chui frazier jawerth weiss 
orthonormal bases compactly supported wavelets provide powerful complement traditional fourier methods permit analysis signal image localised oscillating components 
statistical regression context spatially varying decomposition build algorithms adapt ective window width amount local oscillation data 
decomposition terms orthogonal basis analytic study closed form possible 
purposes discuss nite discrete wavelet transform 
transform careful treatment boundary correction described cohen daubechies jawerth vial related meyer 
focus attention main themes employ simpler version nite discrete wavelet transform main exposition 
version yields exactly orthogonal transformation data wavelet coe cient domains 
brief comments minor changes needed boundary corrected version section 
suppose yi various combinations number vanishing moments support width andj low resolution cuto may construct ann orthogonal matrix nite wavelet transform matrix 
matrices depending special lters addition original daubechies wavelets coi ets daubechies 
gures 
vanishing moments support lengths 
matrix yields wavelet coe cients ofy wy matrix orthogonal inversion hasn elements 
elements scheme wj remaining element 
coe cients denote th row ofw 
inversion tw yi wj sum basis coe call wavelets 
plotted function ofi looks localized wiggle name wavelet 
forj andk bounded away extreme cases conditions approximation xed wavelet sense usual wavelet transform meyer daubechies 
approximation improves 
oscillating function compact support usually called mother wavelet 
speak localized spatial positions frequencies near wavelet smooth visual appearance ands chosen su ciently large favorable choices called quadrature mirror lters construction matrix daubechies described particular construction smoothness number derivatives proportional purposes details vanishing moments jj supported 
spatial localization wavelet bases wavelet coe cients allow easily answer question signi cant change function looking wavelet coe cients spatial jt 
coe cients large answer 
figures displays functions bumps blocks heavisine doppler caricature spatially variable functions arising imaging spectroscopy scienti signal processing 
gures article 
depicts wavelet transforms functions 
large coe cients occur exclusively near areas major spatial activity 
property suggests spatially adaptive algorithm principle selective wavelet reconstruction 
nite list pairs de wj provides reconstructions selecting subset empirical wavelet coe cients 
motivation proposing principle twofold 
spatially inhomogeneous function action concentrated small subset space 
second noise model underlying noise wavelet coe cients equally 
noise ei assumed white noise orthogonal white noise 
consequently empirical wavelet coe cient wj zj wf wavelet transform noiseless ti empirical wavelet coe cient contributes noise variance wavelet coe cients contribute signal 
heuristic method 
ideal spatial adaptation de ned selective wavelet reconstruction obvious way 
risk measure ideal risk rn sw optimal spatial parameter list indices attaining rn rn sw figures depict results ideal wavelet adaptation functions displayed 
shows noisy versions functions interest noise ratio nis 
shows noisy data wavelet domain 
shows reconstruction selective wavelet reconstruction oracle shows situation wavelet domain 
oracle helps select important wavelet coe cients reconstructions high quality 
theoretical bene ts ideal wavelet selection seen case piecewise polynomial 
suppose wavelet basis parameter md properties imply wavelet coe cients vanish coe cients coarse levels ii coe cients jj associated interval contains breakpoint 
xed number coe cients satisfying resolution level breakpoints satisfying ii 
consequently number pieces 
orthogonality wj squares estimate certain depending linearly ons onf 
rn sw logn piecewise polynomial 
nearly bound ideal piecewise polynomial adaptation considerably better usual nonadaptive linear methods 
near ideal spatial adaptation wavelets course calculations ideal risk point bene ideal spatial adaptation prompt question nearly approach ideal performance oracle available rely data side information 
bene wavelet framework answer questions precisely 
section develop new inequalities multivariate decision theory furnish estimate knowledge noise level obeys rn logn sw complete generality possible come logn factor performance ideal wavelet adaptation 
small factor logn replaced constant smaller ifn 
hand radically better performance possible get inequality valid change constant hold measurable estimator sequence 
illustrate implications figures show situation basic examples estimator fn implemented computer described section 
result slightly noisier ideal estimate quality requires oracle 
theoretical properties interesting 
method property piecewise polynomial withl pieces rn logn logn wherec andc result merely combination 
special case actual estimator coming log ideal piecewise polynomial ts 
universality spatially adaptive procedure calculation essentially limited piecewise polynomials holds 
insection show depending onf rn sw rn pp 
interpret result saying selective wavelet reconstruction essentially powerful variable partition piecewise constant ts variable knot squares splines piecewise polynomial ts 
suppose functionf furnished oracle piecewise polynomials piecewise constants knot splines improve rate convergence traditional xed bandwidth kernel methods say rate xed bandwidth ton 
furnished oracle selective wavelet adaptation ers improvement nn essentially bene level rates 
know proof existing procedures tting piecewise polynomials variable knot splines current statistical literature attain performance ideal methods 
contrast selective wavelet reconstruction easy er performance comparable oracle estimator wavelet selection oracle ers advantages spatially variable methods 
main assertion theoretical perspective cleaner elegant abandon ideal tting piecewise polynomials optimal partitions turn wehave theoretical results algorithm 
contents section discusses problem mimicking ideal wavelet selection section shows wavelet selection ers advantages piecewise polynomial ts section discusses variations relations 
section contains certain proofs 
related manuscripts authors currently publication review available latex les anonymous ftp stanford edu cited text tex 
decision theory spatial adaptation section solve new problem multivariate normal decision theory apply function estimation 
oracles diagonal linear projection consider problem multivariate normal decision theory 
wi wi independent identically distributed asn known noise level object interest 
wish estimate loss de ne risk measure consider family diagonal linear projections ek tdp iwi estimators keep kill coordinate 
suppose oracle supply coe cients dp optimal diagonal projection scheme 
ideal coe cients fj ij meaning ideal diagonal projection consists estimating larger noise level 
supplied coe cients attain ideal risk dp nx ij min 
general ideal risk dp attained estimator linear nonlinear 
surprisingly simple estimates come remarkably close 
motivated idea wavelet coe cients contribute signal consider threshold rules retain observed data exceeds multiple noise level 
de ne hard soft threshold non linearities sgn jwj hard threshold rule reminiscent subset selection rules model selection return 
focus soft thresholding 
theorem assume model 
estimator satis es ek wi logn logn oracular notation nx min ir logn dp ir denotes mean squared loss estimating parameter inequality says performance oracle plus extra parameter factor essentially logn 
short proof appears appendix 
natural revealing look optimal thresholds yield smallest possible constant place logn soft threshold estimators 
give result outline approach section 
theorem assume model 
minimax threshold de ned solving yields estimator satis es ek nf wi nx min ir coe cient de ned satis es logn threshold logn asymptotically logn logn table shows constant smaller logn order 
forn get 
logn upper bound sharp 
holds extend soft thresholds allow completely arbitrary estimator sequences contention 
theorem inf sup ir ek min logn asn inequality form valid estimator sequence logn place sense oracle diagonal projection mimicked essentially faithfully soft thresholding rules suggested prior multivariate normal decision theory bickel tex 
worth mentioning traditional hard threshold estimator exhibits asymptotic performance 
theorem sequence su ciently close logn threshold estimator wi satis es logn inequality lnf nx min ir su ciently close logn means log logn logn 
adaptive wavelet shrinkage apply preceding results function estimation 
letn denote wavelet transform mentioned section 
orthogonal transformation particular iff fi fi vectors transforms parseval relation kf fk yi wy discrete wavelet transform 
wj zj kj de ne selective wavelet reconstruction 
observe tdp sense realized wavelet transform followed diagonal linear projection shrinkage followed inverse wavelet transform 
parseval relation fk denotes nonlinear estimator fk ek conclude immediately corollary nf rn sw estimator satisfy better inequality inthe sense measurable estimator inequality hold replaced logn 
inequality holds estimator wt derived hard thresholding place achieved simple means essentially best spatial adaptation possible wavelets 
implementation developed computer software package runs numerical computing environment matlab 
addition implementation nason language available anonymous ftp statlib lib stat cmu edu implementations development 
implement modi cation de nition denote estimator wavelet domain obtained wj kj wj jj estimator name estimator emphasises shrinkage wavelet coe cients performed soft thresholding mean squared error risk approach taken specify threshold 
alternative choices threshold lead estimators visushrink introduced section sureshrink discussed report tex 
rationale rule follows 
vanishing means corresponding coe cients generally cluster zero 
coe cients xed number independent ofn shrunken zero 
sw denote selective wavelet reconstruction levels shrunk 
evidently risk bound nf rn sw course rn sw rn sw dramatically worse better functions having non zero average values 
shows reconstructions test functions shows situation wavelet domains 
evidently methods job adapting spatial variability functions 
reader note occasionally reconstructions exhibit ne scale noise artifacts 
extent inevitable hypothesis smoothness underlying function 
proof outline theorem suppose single 
de ne function st see bickel 
qualitatively st increases maximum 
explicit formulas properties appendix 
main idea de ne minimax quantities inf sup st min largest attaining key inequality follows immediately rst assume 
set wi 
ek nx st nx nf wi rescaling min nx ek st min inequality follows 
consequently theorem follows asymptotics obtain consider analogous quantities supremum interval replaced supremum endpoints 
inf sup st min largest attaining appendix show strictly increasing strictly decreasing solution st st equation de nes uniquely shown appendix leads logn log log log log complete outline note balance condition st gives logn piecewise polynomials powerful wavelets wenow show wavelet selection oracle closely piecewise polynomial tting oracle 
theorem andn depending wavelet transform rn sw rn pp 
function wavelets supplied oracle ideal risk di ers logarithmic factor ideal risk piecewise polynomial estimate 
variable knot splines piecewise polynomials rn sw rn spl note constants necessarily appearance see proof 
piecewise constant ts piecewise polynomials rn sw rn pc willing neglect factors logn selective wavelet reconstruction oracle methods oracles 
note expect get better logn worst case ratio essentially reasons section 
iff piecewise polynomial perfectly suited piecewise polynomial ts wavelets expected perfectly suited wavelets polynomials 
hand iff precisely nite wavelet sum expect piecewise polynomials perfectly suited di erences di erent spatially adaptive schemes inevitable 
theorem compares ideal risks 
course ideal risk wavelet selection nearly attainable 
know parallel result ideal risk piecewise polynomials 
event get corollary estimator satis es log logn rn pp comes factor log ideal piecewise polynomial ts 
way oracle piecewise polynomials abandon piecewise polynomial ts wavelet shrinkage 
proof theorem 
partition supplied oracle piecewise polynomial ts 
suppose optimal partition 
squares partition noiseless data 
variance decomposition ideal risk pp kf sk ws wavelet transform ofs 
piecewise polynomial argument leading tells wavelet coe cients 

consider spatial parameter selective wavelet reconstruction 
comparing jjf sjj gr pp theorem follows assumption de nition rn pp pp rn sw observe optimal variable knot spline noiseless data certainly piecewise polynomial jjf sjj jjf sjj depends unknown parameters noisy data variance term times 
rn pp rn spl establishes 
discussion variations choice oracle alternate family estimators multivariate normal estimation problem diagonal linear tds iwi estimators shrink coordinate di erent coordinates possibly treated di erently 
oracle ds family estimators provides ideal yield ideal risk ds nx nx oracle inequality diagonal shrinkage 
say theorem soft thresholding estimator threshold satis es nf nx ir logn 
ii generally asymptotic inequality continues hold soft threshold sequences hard threshold estimators threshold sequences satisfying respectively log logn logn log logn logn iii theorem continues hold fortiori denominator pn min replaced pn diagonal shrinkage mimicked factor logn closely 
appendix proof theorem covers soft hard threshold estimators dp ds oracles 
proof establishes theorem asymptotic version theorem thresholds range speci ed 
results carried adaptive wavelet shrinkage just section de ning wavelet shrinkage case analog tws tds corollary extends immediately case 
variations choice threshold optimal thresholds 
theorem studied minimax threshold soft threshold nonlinearity comparison projection oracle 
total minimax quantities may de ned considering various combinations threshold type soft hard oracle type projection shrinkage 
computer programs calculating tabulate forj cf 
table 
embedded look tables software mentioned earlier 
implementation optimal thresholds require computational ort tabulate thresholds various values ofn 
computational ort far greater cases case studied essentially analog simpli cation occurs replacing 
optimal thresholds threshold precisely optimal combinations may asymptotically optimal combinations 
comparing shows hard thresholding oracle logn 
universal thresholds 
alternative minimax thresholds simply employ universal sequence logn sequence easy remember implementation software requires costly development look tables asymptotically optimal combinations threshold nonlinearity oracle discussed 
fact nite risk bounds may developed threshold examining closely proofs theorems 
theorem st logn fn st logn fn ht logn fn ht logn fn drawback simple threshold formula samples order dozens hundreds mean square error performance minimax thresholds noticeably better 
visushrink 
hand important visual advantage noise free character reconstructions 
explained follows 
wavelet transform noiseless objects portrayed gure sparse lled essentially zero coe cients 
contamination noise coe cients nonzero 
sample noiseless case ought zero noisy case nonzero character preserved reconstruction reconstruction annoying visual appearance contain small clean background 
threshold logn avoids problem fact zi isa white noise sequence logn high probability sample wavelet transform underlying signal exactly zero estimated zero 
displays results threshold noisy data figures 
noise free character plots striking 
de nition denote estimator wavelet domain obtained wj kj wj logn jj visushrink estimator wt method better visual quality asymptotic risk bounds worse logn rn sw estimator discussed report tex 
estimating noise level 
software estimates noise level median absolute deviation wavelet coe cients nest divided 
experience empirical wavelet coe cients nest scale small fraction exceptions essentially pure noise 
naturally perfect get estimate su ers upward bias due presence signal level 
median absolute deviation bias ectively controlled 
incidentally upward bias disastrous estimate biased upwards say bounds hold logn place logn 
adaptation bases considerable amount literature example pinsker seq 
concerns terms called mimicking oracle fourier basis 
improvement respects 
type objects considered wavelet oracle powerful fourier oracle 
fourier oracle give faster discontinuous object wavelet oracle achieve rates fast logn certain discontinuous objects 
displays results fourier domain oracle basic functions compared 
evidently wavelet oracle visually better case 
better mean square 

pinsker access oracle inequality di erent approach thresholding grouping blocks adaptive linear damping blocks 
approach obey risk bounds oracle inequality easily ideal risk larger logarithmic factors 
minimax sobolev balls point view pinsker designed adaptive linear damping essentially optimal compare comments report tex section 
actual reconstructions pinsker method data show better spatial adaptation see tex 
numerical measures table contains average location squared error various estimates test functions noise realisation reconstructions shown figures 
apparent ideal wavelets reconstruction dominates ideal fourier genuine estimate soft threshold comes factor ideal error predicted forn table 
logn threshold visually preferable cases uniformly worse squared error re ects known divergence usual numerical visual assessments quality table shows results small simulation comparison techniques sample size varied replications case 
features noted table extend sample sizes 
addition notes expected average squared errors decline rapidly sample size smoother signals heavisine doppler blocks bumps adaptive properties estimator proposed number optimality properties minimax decision theory 
consider problem single discussed believe thatf holder class sure exponent constant class 
adaptive sense achieves logarithmic factor best risk bounds class known logarithmic factor necessary class unknown brown low 
near minimax properties described detail report tex 
boundary correction described cohen daubechies jawerth vial introduced separate boundary lters correct non orthogonality restriction basis functions intersect preserve important property orthogonality polynomials preconditioning transformation data necessary 
transform may represented orthogonal transformation built quadrature mirror lters boundary versions cascade algorithm 
preconditioning transformation ects left right elements block diagonal diag pl ji jpr 
key point size content boundary depend onn parseval relation modi ed jj jj jj jj constants correspond smallest largest singular values pr depend onn ideal risk inequalities remain valid additional dependence constants particular concerning logarithmic mimicking oracles unchanged 
relation model selection may statisticians automatic model selection method picks subset wavelet vectors ts model consisting wavelets subset data ordinary squares 
results show method gives performance mean square error attain knew advance model provided minimum mean square error 
results apply equally orthogonal regression 
suppose independent identically distributed asn andx 
suppose predictor variables orthogonal ip 
theorem shows estimator achieves risk worse rp dp factor logp 
point view amusing consequences 
example hard thresholding estimator amounts backwards deletion variable selection retains nal model variables scores larger original squares full model 
small corresponds current practice signi cance rule near minimax sense theorem forp 
lack space pursue model selection connection length comments 

george foster proved results model selection interesting compare theorem 
language show oracle log hard thresholding log show call hard thresholding nonlinearity threshold give case performance ratio call variance ation factor asymptotically smaller logn asn 

compare bickel 
results di er attempt powerful oracles attain optimal mean squared errors 
increase power oracles expressed 
intuitively oracles achieve signi cant risk savings oracle case true parameter vector coordinates nearly precisely zero 
dean foster ed george calling attention interesting describes connections classical model selection bic criterion 

alan miller described model selection procedure equal number pure noise variables column vectors independent ofy appended matrix 
stops adding terms model point term added arti cial pure noise variables 
simulation method sets implicitly threshold maximum collection ofn gaussian random variables 
orthogonal regression case maximum behaves logn 
compare 
miller method probably far respect mse oracle 
completed donoho leave university california berkeley supported nsf nasa 
johnstone supported part nsf nih 
helpful comments referee gratefully acknowledged 
grateful carl carried simulations reported table 
appendix proofs proof theorem verify univariate case multivariate case follows summation 
sgn jxj 
fact show log log regard right minimum functions note rst pr jxj ex log get proof complete verify symmetric pr pr log log sup jg calculus shows log sup jg sup jx log 
mean squared error properties univariate thresholding 
systematic summary recording st ht standard gaussian density cumulative 
lemma st ht soft thresholding may taken hard thresholding 
inequalities st ht proof 
soft thresholding follow respectively 
fact st monotone increasing follows st follows st 
forg st establishes 
inequality follows alternating series bound gaussian tails turning hard thresholding formula follows expectations fjy consider 
range fjy obtain pr jy prove su ces st bound 
di erentiating twice inequalities nally ands obtain sup follows 
theorem asymptotics rst half denote distribution density 
quantity root 
note continuous function zero 
furthermore pn 
pn note term brackets negative expression negative 
standard inequality veri es happens logn forn implies zero logn claim veri ed direct computation 
second half de ne su ciently log loglog log standard asymptotic result follows pn converges resp 
resp 
implies 
theorem equivalence prove de ned sup st min attains maximum 
consider ranges 
numerator st ing denominator constant 
monotone increas apply st 
argument similar shows thatp forn nn equation preceding conclude 
combining thatl attains maximum establishing required equivalence 
theorem main idea random variable prior distribution chosen randomly selected subset logn coordinates size roughly logn derive information bayes risk prior 
consider varying loss pni resulting risk ln rn ln prior distribution nally rn rn inf rn denote bayes risk prior call corresponding bayes rule minimax theorem statistical decision theory applies loss ln mn denote left side mn sup consequently theorem proved exhibit sequence priors consider point prior distribution logn denotes dirac mass atx 

de ne su ciently small standard normal density 
log reports tex tex tex considered prior scalar problem usual squared error bayes risk apply results problem select logn logn log logn consider prior prior easily calculated bayes risk vector zi usual loss ln 
applying aim fact get lower bound bayes risk 
consider random fi binomial distribution set logn de ne ng 
chebyshev inequality anp 
denote bayes rule respect loss ln 
step justi ed ne ln ne ln nn logn chosen arbitrarily large proves 
justify wemust verify jj jj ac logn focus trickier terme jj jj ac denote joint distribution andx 
setp nn 
turns conditional expectation representation cauchy schwartz jensen inequalities nd jj jx jj jj jx ng fep pr jj jj jj nn log 
theorems pr logn logn give proof covers soft hard thresholding oracles 
fact consider long st ht 
logn uniformly log logn logn logn depends way explicit proof 
st require ht 
logn numerator bounded denominator bounded logn logn 
logn bound numerator obtain logn logn log logn logn follows logn wherec soft thresholding andc hard thresholding 
expansion shows range includes theorem logn bounds logn logn previous section simply logn logn logn forn 
bounds follow direct evaluation 
note bounds improved slightly considering cases separately 
bickel 

minimax estimation normal mean subject doing point 
advances statistics rizvi eds academic press new york 
breiman friedman olshen stone 

cart classi cation regression trees 
wadsworth ca 
gasser herrmann 

locally adaptive bandwidth choice kernel regression estimators 
appear 
amer 
statist 
assoc 
brown low 

ciency lack adaptability nonparametric functional estimation 
appear annals statistics 
cohen daubechies jawerth vial 

multiresolution analysis wavelets fast algorithms interval 
comptes rendus acad 
sci 
paris 

chui 
wavelets 
academic press boston ma 
daubechies 

orthonormal bases compactly supported wavelets 
communications pure applied mathematics nov pp 

daubechies 

lectures wavelets siam philadelphia 
daubechies 

orthonormal bases compactly supported wavelets ii variations theme 
siam math 
anal 
yu 
pinsker 

learning algorithm nonparametric ltering 
automat 

russian 
frazier jawerth weiss 

littlewood paley theory study function spaces 
nsf cbms regional conf 
ser mathematics 
american math 
soc providence ri 
friedman silverman 

flexible parsimonious smoothing additive modeling 
discussion 
technometrics 
friedman 

multiple additive regression splines discussion 
annals statistics 
george foster 
risk ation variable selection regression 
technical report university chicago 


problem adaptive estimation white gaussian noise 



russian 
theory probability appl 
english 


ondelettes sur algorithmes 
orsay 
meyer 

ondelettes ondelettes hermann cie paris 
meyer yves 
ondelettes sur 
revista matematica 
miller 

selection subsets regression variables discussion 
statist 
soc 

miller 

subset selection regression 
chapman hall 
london new york 
muller hans georg ulrich 

variable bandwidth kernel estimators regression curves 
ann 
statist 
scott 

variable kernel density estimation 
annals statistics 
table related quantities logn logn table average square errors jj fjj figures blocks bumps heavisine doppler fig fig noise fig ideal wavelets fig ideal fourier fig threshold fig threshold logn table average square errors jj jj replications ideal fourier ideal wavelets threshold threshold log blocks bumps heavisine doppler list figures 
spatially variable functions 
formulas 

functions wavelet domain 
nearly symmetric daubechies wavelet 
wavelet coe cients depicted forj 
coe cients level constant plotted series vast majority coe cients zero ectively zero 

functions gaussian white noise rescaled signal noise ratio sd 
noisy functions wavelet domain 
compare fig 

small number coe cients stand noise background 

ideal selective wavelet reconstruction 
compare figs 

ideal reconstruction wavelet domain 
compare figs 

coe cients set zero 
retained 
reconstruction soft thresholding mimicking oracle relying data 

wavelet domain 
compare figs 
visushrink reconstructions soft thresholding logn notice noise free character compare figs 

ideal selective fourier reconstruction 
compare fig 

superiority oracle evident 
formulas test functions blocks 
bumps 
tj sgn tj hj tj wj jtj tj blocks hj wj heavisine 
doppler 
sin sgn sgn sin 
