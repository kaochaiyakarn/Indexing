regularized winnow methods tong zhang mathematical sciences department ibm watson research center yorktown heights ny watson ibm com theory winnow multiplicative update certain advantages perceptron additive update irrelevant attributes 
effort enhancing perceptron algorithm regularization leading class linear classification methods called support vector machines 
similarly possible apply regularization idea winnow algorithm gives methods call regularized 
show resulting methods compare basic similar way support vector machine compares perceptron 
investigate algorithmic issues learning properties derived methods 
experimental results provided illustrate different methods 
consider binary classification problem determine label associated input vector useful method solving problem linear discriminant functions consist linear combinations components input variable 
specifically seek weight vector threshold label label 
training set labeled data number approaches finding linear discriminant functions advanced years 
especially interested families online algorithms perceptron winnow 
algorithms typically fix threshold update weight vector going training data repeatedly 
mistake driven sense weight vector updated algorithm able correctly classify example 
perceptron algorithm update rule additive linear discriminant function misclassifies input training vector true label update component weight vector wj wj yi parameter called learning rate 
initial weight vector taken 
winnow algorithm positive weight update rule multiplicative linear discriminant function misclassifies input training vector true label update component weight vector wj wj exp jy learning rate parameter initial weight vector taken wj 
winnow algorithm belongs general family algorithms called exponentiated gradient descent unnormalized weights 
variants 
called balanced winnow equivalent embedding input space higher dimensional space 
modification allows positive weight winnow algorithm augmented input effect positive negative weights original input modification normalize norm weight wj leading normalized winnow 
theoretical properties multiplicative update algorithms extensively studied winnow 
linearly separable binary classification problems perceptron winnow able find weight separate class vectors class vectors training set finite number steps 
number mistakes updates finding weight different 
difference suggests algorithms serve different purposes 
linearly separable problems vapnik proposed method optimizes perceptron mistake bound called optimal hyperplane method see 
method appeared statistical mechanical learning literature see referred achieving optimal stability 
non separable problems generalization optimal hyperplane proposed introducing soft margin loss term :10.1.1.15.9362
derive regularized winnow methods constructing optimal hyperplanes minimize winnow mistake bound perceptron mistake bound svm 
derive soft margin version algorithm non separable problems 
simplicity shall assume 
restriction cause problems practice append constant feature input data offset effect formulation amenable theoretical analysis 
svm fixed threshold allows simple perceptron numerical algorithm described chapter 
complex non fixed introduce fundamental difficulty 
organized follows 
section review mistake bounds perceptron winnow 
bounds show regularized winnow methods derived mimicking optimal stability method svm perceptron 
discuss relationship newly derived methods related methods 
section investigate learning aspects newly proposed methods context similar known svm results 
example section illustrate methods 
svm regularized winnow perceptron svm review derivation svm perceptron serves derivation regularized winnow 
consider linearly separable problems weight separates class vectors class vectors training set 
known perceptron algorithm computes weight correctly classifies training data updates proof kwk maxi kx mini weight vector minimizes right hand side bound called optimal hyperplane optimal stability hyperplane 
optimal hyperplane solution quadratic programming problem min non separable problems introduce slack variable data xi yi compute weight vector solves min wt parameter 
known converges weight vector optimal hyperplane 
write kkt condition optimization problem lagrangian multiplier wt elimination obtain dual optimization problem dual variable see chapter details max weight optimal solution 
solve problem modification perceptron update algorithm see chapter data xi yi update maximize dual objective functional gives max min exact maximization dual objective functional 
learning rate set corresponds winnow regularized winnow similar perceptron problem linearly separable positive weight winnow computes solution correctly classifies training data updates wj ln maxi mini wt kwk learning rate maxi 
technique deriving bound developed see earlier results 
detailed proof specific bound employed techniques 
note perceptron mistake bound bound learning rate dependent 
depends prior initial value problems separable positive weights obtain optimal stability hyperplane associated winnow mistake bound consider fixing kwk kwk 
natural define optimal hyperplane positive weight solution convex programming problem min wj ln wj similar derivation svm non separable problems introduce slack variable data compute weight vector solves min wj ln wj parameter 
note derive methods assumed kwk fixed kwk parameter 
implies derived methods fact regularized versions normalized winnow 
practice ignore normalization constraint derived methods correspond regularized versions unnormalized winnow 
worth mentioning appearance entropy regularization condition natural exponentiated gradient methods investigated readily observed theoretical results 
regularized normalized winnow closely related maximum entropy discrimination studied methods identical linearly separable problems 
framework maximum entropy discrimination winnow connection quite non obvious 
derivation motivated minimizing winnow mistake bound shall show possible derive interesting learning bounds formulations connected winnow mistake bound 
similar svm formulation non separable formulation regularized winnow approaches separable formulation 
shall focus nonseparable case 
similar svm write kkt condition lagrangian multiplier elimination obtain algebra resembles chapter shall skip due limitation space dual formulation regularized unnormalized winnow max exp jy th component weight exp optimal solution 
regularized normalized winnow kwk obtain max ln exp yi weight exp optimal solution 
exp similar perceptron update rule dual svm formulation possible derive winnow update rules regularized winnow formulations 
data fix update maximize dual objective functionals 
shall try derive analytical solution gradient ascent method learning rate ld ld denote dual objective function maximized 
fixed small number computed newton method 
hard verify obtain update rule regularized unnormalized winnow max min 
gradient ascent dual variable gives wj exp rule 
compared svm dual update rule soft margin version perceptron update rule method naturally corresponds soft margin version unnormalized winnow update 
similarly obtain dual update rule regularized normalized winnow max min wj exp exp 
rule rule naturally regarded soft margin version normalized winnow update 
note regularized normalized winnow normalization constant needs carefully chosen data 
example data infinity norm bounded appropriate choose hyperplane kwk achieve reasonable margin 
problem crucial unnormalized winnow norm initial weight affects solution 
maximum entropy discrimination closely related regularized normalized winnow earlier attempt derive large margin version unnormalized winnow heuristics 
algorithm purely mistake driven dual variables algorithm compute optimal stability hyperplane winnow mistake bound 
addition include regularization parameter practice important non separable problems 
statistical properties regularized section show similar case svm able derive learning bounds formulations minimize winnow mistake bound 
possible bounds interests 
result analogy leave cross validation bound separable svms theorem 
theorem expected misclassification error true distribution hyperplane obtained linearly separable unnormalized regularized winnow algorithm training samples bounded err min pj wj ln wj right hand side expectation taken random samples xn yn 
number support vectors solution 
optimal solution samples dual 
weight obtained setting max kwk kw kwn 
proof sketch 
describe major steps due limitation space 
denote weight obtained optimal solution removing yk training sample 
similar proof theorem need bound leave oneout cross validation error note leave cross validation error jfk wk kx gj 
inequalities wk wk wj wj ln wj wk wj wj ln wj wk wj wj ln wj inequality obtained comparing dual objective functionals corresponding kkt condition dual problem 
remaining problem reduced proving jfk wk wj wj ln wk wj gj wj ln wj dual formulation summing index kkt order condition respective dual multiplied obtains wj ln wj need show wk wj wj ln wk wj 
checked directly taylor expansion 
technique obtain bound regularized normalized winnow shall skip save space 
disadvantage bound expectation random estimator better leave cross validation error observed data difficult generalize specific technique nonseparable problems unbounded 
bound convey useful information example observe expected misclassification error learning curve converges rate long wj ln wj sup kxk reasonably bounded 
difficult obtain interesting pac style bounds covering number result entropy regularization ideas 
pac analysis imply slightly suboptimal learning curve log linearly separable problems bound provides probability confidence generalized non separable problems 
state example non separable problems justifies entropy regularization 
bound direct consequence theorem theorem 
note square root removed data dependent 
theorem data infinity norm bounded kxk consider family wj ln hyperplanes kwk denote err misclassification error true distribution 
constant probability random samples satisfies err ac ln nab ln jfi gj number samples margin text categorization example text categorization application show regularized winnow enhance winnow just svm enhance perceptron 
text categorization example quite interesting comparison svm regarded state art method unnormalized winnow applied problem results :10.1.1.161.6020
arguments concerning irrelevant features support winnow relevant features support svm 
comparison implies argument relevant regularization truly helps matter underlying algorithm perceptron winnow 
theory differences approaches example see practical differences remain carefully investigated 
standard test set text categorization mod apte split reuters data set available www research att com lewis reuters html 
training data test data 
report results largest categories binary classification problem compared 
performance usually measured precision recall classification error 
precision percentage correctly classified data data classified class recall percentage correctly classified data data truly class 
adjust threshold linear classifier training trade precision recall popular performance metric break point threshold chosen precision equals recall see 
binary word occurrences documents features append constant feature offset effect shall report balanced versions unnormalized save space performances normalized versions similar appropriate normalization conditions 
consistency comparison purposes fix learning rates iterations training data algorithms 
initial values 
average table micro average 
results comparable enhanced svm result micro averaged break top categories 
note svm regularized winnow consistently outperform perceptron winnow compatible results 
implies regularization helps practical difference perceptron winnow application 

adaptive perceptron algorithm 
europhys 
lett 
cortes vapnik :10.1.1.15.9362
support vector networks 
machine learning 
dagan karov roth 
mistake driven learning text categorization 
proceedings second conference empirical methods nlp 
dumais platt heckerman sahami 
inductive learning algorithms representations text categorization 
proceedings acm th category perceptron svm winnow regularized winnow earn acq money fx grain crude trade interest ship wheat corn avg top table break performance largest categories international conference information knowledge management pages 
grove littlestone schuurmans 
general convergence results linear discriminant updates 
proc 
th annu 
conf 
comput 
learning theory pages 
tommi jaakkola marina meila tony jebara 
maximum entropy discrimination 
solla leen 
ller editors advances neural information processing systems pages 
mit press 
jaakkola mark haussler 
discriminative framework detecting remote protein homologies 
journal computational biology appear 
joachims 
text categorization support vector machines learning relevant features 
european conference machine ecml pages 

statistical mechanics perceptron maximal stability 
lecture notes physics volume pages 
springer verlag 
kivinen warmuth 
additive versus exponentiated gradient updates linear prediction 
journal information computation 
littlestone 
learning quickly irrelevant attributes abound new algorithm 
machine learning 
mangasarian 
successive overrelaxation support vector machines 
ieee transactions neural networks 
opper 
learning times neural networks exact solution perceptron algorithm 
phys 
rev 
rosenblatt 
principles neurodynamics perceptrons theory brain mechanisms 
spartan new york 
bernhard scholkopf christopher burges alex smola editors 
advances kernel methods support vector learning 
mit press 
shawe taylor bartlett williamson anthony 
structural risk minimization data dependent hierarchies 
ieee trans 
inf 
theory 
vapnik 
statistical learning theory 
john wiley sons new york 
tong zhang 
analysis regularized linear functions classification problems 
technical report rc ibm 
early nips pp 

