size neural network gives optimal generalization 
convergence properties backpropagation steve lawrence lee giles ah chung tsoi nec research institute independence way princeton nj department electrical computer engineering university queensland st lucia australia technical report umiacs tr cs tr institute advanced computer studies university maryland college park md june revised august important aspects machine learning paradigm scales problem size complexity 
task known optimal training error pre specified maximum number training updates investigate convergence backpropagation algorithm respect complexity required function approximation size network relation size required optimal solution degree noise training data 
general solution worse function approximated complex oversized networks result lower training generalization error certain cases committee ensemble techniques beneficial level noise training data increased 
experiments performed obtain optimal solution case 
support observation larger networks produce better training generalization error face recognition example network parameters training points generalizes better smaller networks 
keywords local minima generalization committees ensembles convergence backpropagation smoothness network size problem complexity function approximation curse dimensionality 
institute advanced computer studies university maryland college park md 
statements regarding training generalization error mlps similar occur neural network literature community 
bp gradient method guarantee absolute minimum reached 
spite theoretical researchers involved bp applications know serious problem 
bp leads global minimum possible meet practical stopping criteria 
local minima rare system learns reasonable period time 
backpropagation works avoiding non optimal solutions 
network parameters number data points available statements say local minima expected affect quality solution greatly occur infrequently effect ignored practice breiman comment local minima neural net people worry landing local minima 
statement expresses intuition degrees freedom model total number data points available training 
show solution near optimal solution obtained 
relative quality solution obtained investigated function variables complexity required function approximation size network relation size required optimal solution degree noise data 
results indicate solution worse function approximated complex oversized networks result lower training generalization error certain cases committee ensemble techniques beneficial amount noise training data increased 
support observation larger networks certain cases produce better training generalization error provided face recognition example network times parameters training points generalizes better smaller networks 
techniques control generalization order illustrate case 
local minima shown error surface backpropagation network hidden layer hidden units local minima network trained arbitrary set containing different inputs yu 
practice features error surface plateaus baldi hornik difficulty optimisation 
example error functions shown gori local minima 
function left expected difficult optimise gradient descent 
purposes criterion interest considered best solution practical time limit large may impractical network large order ensure local minima 

examples possible error functions dimension gori 
abscissa corresponds value single parameter ordinate corresponds error function 
functions contains local minima function left expected suitable gradient descent optimisation due flat regions 
prior error surface small networks characterized previously xor network 
practical networks contain hundreds thousands weights general theoretical empirical results small networks scale large networks 
reason may attributed interference effect training process 
consider backpropagation training algorithm hidden layer neurons saturation gradients evaluated hidden layer neurons coupled update parameter generally affects parameters 
network hidden layer neurons interference effect expected pronounced 
caruana tutorial nips caruana generalization results variety problems size networks varied small large 
small large related number parameters model consideration distribution data error surface 
caruana reported large networks rarely worse small networks problems investigated 
results partially correlate observation 
caruana suggested backprop ignores excess parameters 
crane pearson real valued data generated random target network attempted training new networks data order approximate number minima error surface varying conditions 
random target networks fashion referred student teacher problem saad solla 
motivated similar technique order evaluate quality local minima backpropagation function various parameters 
saad solla student teacher problem analyze effect noise line learning 
networks weights speech phoneme recognition bourlard morgan 
case training examples corrupted additive output noise analyze model noise saad solla shown small noise levels may shorten symmetric phase learning larger values may lengthen phase 
generalization error increases noise level increased 
asymptotic case training fixed learning rate results non vanishing asymptotic generalization error 
show learning rate decay schemes remove effects additive output noise asymptotically 
ller finke schulten murata amari randomly generated teacher networks order create training examples student networks 
perform detailed study generalization function number training samples classification tasks networks weights demonstrate strong overfitting small number training examples region generalization error scales number training examples asymptotic scaling number weights network 
theories determining optimal network size nic network information criterion amari generalization aic akaike information criterion akaike akaike widely statistical inference generalized final prediction error gpe proposed moody vapnik chervonenkis vc dimension maass abu mostafa bartlett measure expressive power network nic relies single defined minimum fitting function unreliable local minima ripley 
little published computational experience nic gpe 
evaluation prohibitively expensive large networks 
vc bounds calculated various network types cohn tesauro 
early vc dimension handles case discrete outputs 
case real valued outputs general notion dimension required 
pseudo dimension defined considering loss function measures deviation predictions target values maass 
vc bounds conservative provide generalization guarantees simultaneously probability distribution training algorithm 
computation vc bounds practical networks difficult 
apart small examples unaware systematic procedures evaluation vc bounds typical practical networks 
addressing local minima number samples required respect generalization include baum haussler antsaklis haines hecht nielsen yu gori 
approaches limited due assumptions typical limitations include applicability linearly separable problems consideration true local minima opposed regions gradient descent stuck plateaus consideration limits training time 
respect results reported overfitting behaviour classification tasks expected different due training patterns asymptotic targets 
final prediction error fpe alternative method determining order dynamical process originally proposed akaike generalized neural network setting moody 
briefly largest set examples shattered network set examples shattered network possible ways dividing samples disjoint sets exists function computable network output members output members binary classification problem 
artificial task investigate empirical performance chosen artificial task know optimal solution carefully control various parameters 
task follows similar procedure crane 
mlp input nodes hidden nodes output nodes denoted referred data generating network initialized random weights uniformly selected specified range range weights network biases constant 
results reported specified 
bias weights initialized small random values range increased complexity function mapping increased discussed detail section 
data points created selecting random inputs zero mean unit variance propagating network find corresponding outputs 
dataset forms training data subsequent simulations 
procedure repeated create test dataset points 
simulations reported 
choice zero mean unit variance inputs unrealistic inputs mlp normalised zero mean unit variance distribution may normal le cun 

training data set train new mlps known subsequently trained networks architecture certain tests varied initial weights new networks set procedure suggested haykin equal weights network create dataset 
initialized node node basis uniformly distributed random numbers range fan neuron theoretically optimal training set error zero case noise added data 
shows process graphically 
methodology exploring convergence artificial task explore convergence networks varying certain parameters controlled manner 
training generalization performance investigated 
baseline network topology chosen represent typical network number inputs greater number hidden nodes specific values chosen total training time simulations reasonable 
methodology 
parameters simulations varied time maximum value setting weights generating network size trained networks 
process creating data sets 
size training dataset amount zero mean gaussian noise added training data zero standard deviation standard deviation input data 

configuration mlp tested simulations different starting condition random weights 

stopping criterion 
stopping criterion method controlling generalization maximum number updates order demonstrate case 
networks trained identical number stochastic updates 
expected overfitting occur 
standard mlp output neuron layer number neurons layer weight connecting neuron layer neuron layer bias hyperbolic tangent function 
number weights network standard backpropagation stochastic update update training point 
batch update investigated convergence poor training times extended order magnitude 
quadratic cost function desired value th output neuron th training sample training data set value th output neuron mlp response th training sample 
learning rate 
ways controlling generalization early stopping weight decay weight elimination pruning optimal brain damage le cun denker solla obs optimal brain surgeon hassibi stork 
simulation results results varying network size training set size function complexity amount noise added training data sections 
network size section investigates training generalization behavior networks generating network size fixed trained network size increasing 
cases data created generating network architecture random weight maximum value 
trained networks architecture varied 
theoretically optimal training set error networks tested zero networks trained obtained optimal error backpropagation updates considering networks hidden units contain degrees freedom necessary zero error reasonable expectation performance worse average number hidden units increased 
shows training test set error number hidden units trained network varied 
number training points 
average better solution larger networks compared hidden units networks 
best mean training generalisation error occurs networks hidden units 
trend varies generating network size number inputs hidden nodes outputs nature target function example optimal size networks perform best certain tasks cases advantage larger networks greater 
shows results case case training points 
similar results obtained training points average better solution larger networks compared hidden units networks 
best mean training generalisation error occurs networks hidden units case 
data points best mean training error occurs hidden units best mean generalisation error occurs hidden units 
case generalisation error quite poor networks number data points probably small accurately characterise target function cf 
curse dimensionality 
number parameters networks greater case hidden units shown table 
leads question networks smaller generating network generalise better 
case answer networks hidden units resulted worse performance 
alternative optimization techniques conjugate gradient improve convergence cases 
tech niques lose advantage larger problems 
train nmse test nmse number hidden nodes number hidden nodes 
error networks topology training points 
graph top training error 
graph bottom test error 
abscissa corresponds number hidden nodes 
result averaged simulations 
box whiskers plots shown left case mean plus minus standard deviation shown right case 
number hidden nodes number parameters table 
number parameters networks number hidden nodes varied 
interest observe effect noise problem 
shows results case training points gaussian noise added input data standard deviation equal standard deviation input data 
similar trend observed 
best generalization error average obtained networks containing hidden nodes case 
results section taken indicate oversized networks 
results indicate oversized networks may generalize training successful larger networks possible larger generalize better smaller networks 
observations 
remains desirable find solutions smallest number parameters 
train nmse test nmse train nmse test nmse number hidden nodes number hidden nodes number hidden nodes number hidden nodes training points training points training points training points 
error trained networks shown training points training points 
top bottom training points training error training points test error training points training error training points test error 
train nmse test nmse number hidden nodes number hidden nodes 
error trained networks shown training points noise added input data 
noise gaussian standard deviation equal standard deviation original data 
top bottom training test errors 

similar result expected globally optimal solution small networks hidden unit networks trained zero error expected networks extra degrees freedom result worse performance 

distribution results important 
example observe advantage larger networks training points decreased considering minimum error mean error 

number trials important 
sufficiently trials performed possible find near optimal solution optimal size networks limit infinite number random starting points finding global optimum guaranteed 
advantage larger size networks expected disappear 

note deliberately control generalization capability networks validation set weight decay maximum number updates 
solutions fit training data generalize 
contrary expected results indicate possible oversized networks provide better generalization 
successive pruning retraining larger network hassibi stork may arrive network similar size smaller networks improved training generalization error 

terms computation serial machines may desirable investigate performance number individual weight updates number iterations times number weights equal number training iterations training iterations done time smaller networks 
results show local optimum larger networks may better doing number updates weight may correspond better generalization says nature error surface network size increased extra degrees freedom may help avoid poor local minima 
experiments section face recognition problem show possible observe phenomenon smaller networks trained greater number iterations 
degrees freedom rules degrees freedom model proposed selecting topology mlp number parameters network significantly number examples parameter mlp comfortably store bits information 
network tend memorize data cmu folklore 
rules aim prevent overfitting unreliable optimal number parameters depend factors quality solution distribution data points amount noise nature function approximated 
specific rules mentioned commonly believed accurate 
stipulation number parameters number examples typically believed true common datasets 
results indicate case 
face recognition example section presents results real data 
shows results training mlp classify people images faces training set contains images person total training patterns test set contained different set images person 
small window stepped images image samples point quantized dimensional self organizing map kohonen 
outputs self organizing map image sample inputs mlp subset images shown 
case networks trained updates 
networks contain parameters number training points shown table best training error best generalization error corresponds largest model 
note generalization controlled example validation set weight decay overfitting expected sufficiently large networks 
simulated serial machines larger networks require longer training times number updates 
interest compare happens smaller networks trained longer proposed intelligent face recognition technique 
database orl database contains set faces taken april april olivetti research laboratory cambridge available different images distinct subjects database 
variations facial expression facial details 
images taken dark homogeneous background subjects right frontal position tolerance tilting rotation degrees 
variation scale 
images greyscale levels resolution 
test error 
subset face images experiments 
number hidden nodes 
face recognition experiments showing optimal number weights network larger number data points 
smallest model hidden nodes times parameters training points parameters 
largest model hidden nodes times parameters training points parameters 
test error percentage examples incorrectly classified 
number hidden nodes number weights table 
number parameters face recognition network number hidden nodes increased 
larger networks 
presents results training networks twice long 
observed results change significantly 
experiments mlp networks speech phoneme recognition suggested better performance obtained number parameters cross validation set terminate training renals morgan cohen franco robinson 
test error function complexity number hidden nodes 
networks trained twice long 
section investigates quality solution backpropagation maximum random weight generating network increased 
shows results target generating networks topology difficult visualize target function increased 
simple method provides indication complexity plotted created follows observed function complexity increases increased 
may considered parameter controlling function complexity precise definition complexity 
optimal training set error networks zero increases corresponding target function complex observed solution significantly worse 
worse generalization may expected increases focus training error compared optimal error zero 
behaviour explained considering error surface increased 
mlp error surfaces areas shallow slopes multiple dimensions hecht nielsen 
typically large weights correspond target functions smooth example case fitting function sech sigmoids li sech weights indefinitely large approximation improves 

plots indicating complexity target function varying networks creating plots topology 
rows correspond inputs networks 
column corresponds case inputs set zero remaining columns correspond cases inputs set random values pseudo code 
abscissa individual plot corresponds value input row ranges 
ordinate individual plot corresponds network output 
result weighted sum hidden nodes large causes output hidden nodes insensitive small changes respective weights unit operating tail sigmoid function slope small 
increases optimal solution requires nodes operate region slope small 
result highlights point regarding application mlp models nature target function considered detail 
consideration implicit bias smooth models help preprocessing efforts directed formulating required approximation better suit mlp 
additionally expected required approximation smoother weights network driven large values nodes saturated generalization performance may expected improve 
ensembles networks noisy training data committees ensemble networks known able improve generalization performance jacobs drucker cortes jackel le cun vapnik krogh vedelsby perrone train nmse test nmse maximum random weight maximum random weight 
error networks topology maximum random weight generating network increased 
graph top training error 
graph bottom test error 
abscissa corresponds value result averaged simulations plus minus standard deviation error bars plotted point 
cooper 
section investigates effect committees amount noise added training data increases 
simple weighted ensemble networks 
consider bias variance dilemma geman bienenstock doursat mse may decomposed bias variance components mse mse represents expectation respect training set approximating function 
finite training data reducing bias generally increases variance vice versa 
multilayer perceptron variance term due convergence local minima reduced ensembles effect reduction greater individual networks larger variance see intrator horn 
increasing noise levels resulting poorer con vergence may induce condition 
expected ensemble technique may beneficial noise level increased 
shows results committee networks standard deviation zero mean gaussian noise added training data varied standard deviation input data 
observed networks ensemble appear successful noise increased 
networks topology trained updates 
result averaged different starting conditions 
test nmse test nmse test nmse number committee networks number committee networks number committee networks noise noise noise 
test error number committee networks increased plot 
top bottom noise gaussian noise standard deviation standard deviation input data respectively 
discussion artificial task optimal training error known sample real world problem pre specified maximum number training updates shown 
solution gradient descent practical training time significantly worse globally optimal solution 

certain cases best generalization occur number parameters network greater number data points 
occur larger networks easier train 
result taken indicate oversized networks 
result indicate certain cases training successful larger networks consequently possible larger networks result improved generalization 
remains desirable find solutions smallest number parameters 

solution gradient descent significantly worse function approximated complex 

ensemble techniques increasingly beneficial level noise training data increases 

set functions particular mlp approximate pre specified maximum number training updates certain functions harder approximate backpropagation 
network size degrees freedom simple explanation larger networks provide improved training generalisation error extra degrees freedom aid convergence addition extra parameters decrease chance stuck local minima plateaus 
kr se van der 
section provides plots function approximated trained networks network weights indicate function approximated oversized networks remains relatively smooth training extra degrees freedom larger networks contribute function approximated minor way 
plots section created smaller task order aid visualisation generating networks topology trained networks hidden units 
training data points random weight maximum value 
figures provide indication function approximated networks network size hidden units amount noise gaussian noise standard deviation input standard deviation data varied 
plots generated described section 
dotted lines show target function generating network solid line shows function approximated trained network 
observations training network approximation tends accurate optimal size network approximation appears relatively smooth cases 
figures show weights trained networks network size hidden units amount noise gaussian noise standard deviation input standard deviation hidden units hidden units hidden units 
function approximated networks hidden units case noise 
plots generated described section 
dotted lines show target function generating network solid line shows function approximated trained network 
data varied 
diagram plotted follows columns correspond weights hidden units hidden units hidden units 
function approximated networks hidden units case gaussian noise standard deviation standard deviation inputs 
plots generated described section 
dotted lines show target function generating network solid line shows function approximated trained network 
hidden units hidden units hidden units 
function approximated networks hidden units case gaussian noise standard deviation standard deviation inputs 
plots generated described section 
dotted lines show target function generating network solid line shows function approximated trained network 
hidden nodes bias input nodes 
rows organised groups space group 
number groups equal number hidden nodes trained network 
rows group top row corresponds generating network bottom row corresponds trained network 
idea compare weights generating trained networks 
couple difficulties arise comparison resolved follows 
firstly reason hidden node generating network correspond hidden node trained network problem resolved finding best matching set weights trained network hidden unit generating network euclidean distance weight vectors matching hidden nodes trained generating networks accordingly 
additionally best matches ordered respective distances weight vectors top rows shows generating network hidden node best approximated hidden node trained network 
likewise worst match bottom 
second problem trying match weights hidden nodes input nodes take account output layer weights exactly hidden node function computed different weights hidden nodes weights scaled output layer weights scaled accordingly 
case output considered solution simple hidden layer weights scaled respective output layer weight 
individual weight scaled appropriate output weight plotted follows square shaded proportion magnitude weight white equals black equals maximum value weights networks 
negative weights indicated white square inside outer black square surrounds weight 
observations generating network weights matched closely larger networks consider fourth fifth best matching groups rows extra weights larger networks contribute final approximation minor way results indicate pruning optionally retraining larger networks may perform 
backpropagation result network resources certain cases parameters may ineffective partially effective due sub optimal convergence 
reporting contained hanson stated cherkassky hanson oja language approximation theory overfitting statistical estimation bias vs variance clear parameters nonparametric models neural networks parameters improve things phenomena arise uniquely neural network applications focus statisticians anomaly ignored 
section investigated phenomenon controlled conditions discussed phenomenon may arise 
phenomenon observed states find practice networks parsimonious number neurons hard train states slightly larger optimal size networks improved performance back states synapses give lower mse variance exact order synapses context modelling nonlinear systems fir iir mlp networks 
hidden units hidden units hidden units 
weights training networks hidden units case noise 
case results shown networks different random starting weights 
plotting method described text 
pair rows top row corresponds generating network bottom row corresponds trained network 
observe generating network weights matched closely larger networks compare fourth fifth set rows extra weights larger networks contribute final approximation minor way 
occam razor results showing larger optimal size networks generalize better certain cases contradiction occam razor 
occam razor advocates simpler number possible solutions applicable situation solution different quality larger hidden units hidden units hidden units 
weights training networks hidden units case gaussian noise standard deviation standard deviation inputs 
case results shown networks different random starting weights 
plotting method described text 
pair rows top row corresponds generating network bottom row corresponds trained network 
observe generating network weights matched closely larger networks compare fourth fifth set rows extra weights larger networks contribute final approximation minor way 
networks provide improved generalization performance typically happens larger networks better models training data 
hidden units hidden units hidden units 
weights training networks hidden units case gaussian noise standard deviation standard deviation inputs 
case results shown networks different random starting weights 
plotting method described text 
pair rows top row corresponds generating network bottom row corresponds trained network 
observe generating network weights matched closely larger networks compare fourth fifth set rows extra weights larger networks contribute final approximation minor way 
learning theory results contradiction statistical learning theory 
vapnik states machines small vc dimension required avoid overfitting 
states difficult approximate training data problem mlp approximation goal find appropriate network size order minimize tradeoff overfitting poor approximation 
vapnik suggests priori knowledge required small training error small generalisation error 
case linear output neurons barron derived bound total risk mlp estimator absolute moment fourier magnitude distribution target function measure complexity tradeoff observed accuracy best ap proximation requires larger avoidance overfitting requires smaller ratio 
take account limited training time different rates convergence different left hand term approximation error corresponds error target function closest function mlp implement 
artificial task approximation error zero equation selected optimal network size note results reported sigmoidal linear output neurons 
bartlett correlates results reported 
bartlett comments vc bounds loose neural networks perform successfully training sets considerably smaller number network parameters 
bartlett shows classification number training samples needs grow ignoring log factors avoid overfitting bound total weight magnitude neuron number layers network 
result explicit weight decay implicit bias smaller weights leads phenomenon observed larger networks may generalize better generalization possible larger networks trained successfully smaller networks 
curse dimensionality 
regression hypersurface 
arbitrarily complex unknown dense samples required approximate function accurately 
hard consider obtain dense samples high dimensions 
curse dimensionality friedman 
relationship sampling density number points required friedman dimensionality input space number points 
number points sampling density dimension order keep density dimensionality increased number points increase kolmogorov theorem shows continuous function dimensions completely characterized dimensional continuous function 
specifically kolmogorov theorem friedman kol states continuous definitions curse dimensionality exist definition friedman 
universal constants depend universal transformations depend continuous dimensional function totally characterises typically highly nonsmooth dimensional continuous function characterises continuous function arguments 
see problem dimensionality complexity function high dimensional functions typically potential complex friedman curse dimensionality essentially says high dimensions data points simpler function order represent accurately 
free lunch theorem wolpert macready shows assumptions regarding target function algorithm performs better average 
words need assumptions 
convenient useful assumption corresponds common sensory data instances smoothness 
demonstrated smoother functions correspond faster convergence 
intuitively reasonable complex functions correspond greater degree saturation nodes backpropagated error approaches zero saturated regions 
weight distributions certain cases standard backpropagation lead implicit bias smaller weights experiment shows 
networks trained data generated network initialised cf 
curse dimensionality points required complex target functions generalization control iterations 
shows box whiskers plots distribution weights training networks hidden nodes 
observe weights trained networks average smaller generating network 
weight magnitude generating 
box whiskers plots showing weight magnitude distribution generating network hidden nodes left trained networks hidden nodes 
averaged trials case 
universal approximation results negative terms possibility training large homogeneous mlps parsimoniously represent arbitrary functions 
case relatively small maximum weights network seen convergence may difficult parsimonious solutions 
large mlps successful pattern recognition speech phoneme recognition bourlard morgan suggest difficult find parsimonious solutions employ appropriate internal representations 
biological neural networks reason learn things critically linked pre wiring human brain 
example know lot difficulty training chimpanzee learn human language mlps 
applies homogeneous mlp type universal approximation approach learning example 
class algorithms learning example operate basis looking regularities incorporating regularities model grammar induction algorithms 
contrast computational capacity mlps static 
quote minsky papert epilogue perceptrons early years cybernetics understood hill climbing available working easy problems impractical problems larger sizes complexities 
pleased discover perceptron convergence represented hill climbing fact led wonder procedures generalized limited class multi layer machines named perceptrons 
situation changed seen contemporary connectionist publication casts new theoretical light situation 
backpropagation gradient descent popular years 
part widely applicable yield new results problems small scale 
reputation gains think forms shares albeit lesser degree biological plausibility perceptron 
fear reputation stems manner hill climbing methods deteriorate confronted larger scale problems 
minsky papert assertion local minima related difficulties problem appears valid 
agree appear standard mlp networks trained backpropagation scaled arbitrarily large problems 
certain fundamental limitations performance class learning algorithms mlps produced better results notable alternatives perceptrons threshold units werbos 
imposition sigmoid non linearities mlps allows gradient descent optimisation empirical results suggests error surface relatively speaking quite suitable gradient descent optimization process 
appendix generalization overfitting section provides brief overview generalization overfitting 
generalization refers model performs unseen data model trained training set generalization corresponds expected performance model new patterns 
mathematically goal mlp training formulated minimization cost function bengio local cost function function implemented mlp input model desired output model corresponds weights network represents probability distribution 
objective training optimise parameters minimised randomly chosen practice known 
set training number patterns approximation minimised called empirical error vapnik training error bengio generalization error bengio expected performance mlp new patterns quadratic relative entropy cost functions examples error function 
important question model trained minimise generalises low 
important low performance training set necessarily mean low expected performance new patterns 
mlp provides function mapping input values desired output values 
mapping generally smooth sense defined nature activation function topology network training algorithm allows interpolation training points 
consider simple case input dimension shown 
training patterns marked cross contain noise 
true underlying function mapping may shown middle graph 
controlling scheme mlp may seriously left hand graph overfit right hand graph data 
observe average error training samples highest underfitting graph lowest overfitting graph 
case overfitting error training samples may low error test samples may high consider test points training points overfitting graph mlp training continued past correct fit point generalization performance may decrease 
known bias variance tradeoff geman underfitting case mlp estimator produces estimates high bias low variance estimator said biased average estimated value different expected value 
overfitting case bias estimator low variance high 
exists optimum extremes 
degree overfitting possible related number training patterns number parameters model 
general fixed number training patterns overfitting occur model parameters degrees freedom 
illustrates idea polynomial approximation 
training dataset created contained points equation uniformly distributed random variable 
equation evaluated dataset fit polynomial models rivlin press teukolsky vetterling flannery orders 
order approximation poor shown 
order approximation reasonably 
order number parameters increases significant overfitting evident 
order approximated function fits training data interpolation training points poor 

underfitting overfitting 
shows results mlp approximate training set polynomial case smallest network hidden unit weights including bias weights approximate data 
hidden units weights approximation reasonably 
contrast polynomial case networks hidden units weights hidden units weights resulted reasonably approximations 
particular simple example mlp networks trained backpropagation lead large degree overfitting times parameters data points 
certainly true overfitting serious problem mlps 
example highlights possibility mlps trained backpropagation may biased smoother approximations 
shows different example significant overfitting seen larger mlp models 
equation previous example equation evaluated creating data points 
shows results mlp models hidden nodes 
example hidden node cases produce approximation expected result worse generalization 
test dataset created evaluating equation noise intervals 
tables show results test set models trained second example respectively 
example largest network provided best generalization 
second example network hidden nodes provided best generalization larger networks resulted worse generalization due overfitting 
hidden nodes training mse test mse table 
results mlp interpolation function range 
best generalization corresponded largest network tested hidden nodes 
training details follows 
single hidden layer mlp backpropagation stochastic training updates learning rate schedule initial learning rate 
approximation training data target function noise approximation training data target function noise order order approximation training data target function noise approximation training data target function noise order order 
polynomial interpolation range order model increased uniformly distributed random variable 
significant overfitting seen orders 
function hidden nodes training mse test mse table 
results mlp interpolation function range 
best generalization corresponded hidden nodes larger networks resulted higher error due overfitting 
acknowledgments acknowledge useful discussions comments caruana hanson heath horne lin 
ller 
opinions expressed authors 
partially supported australian research council act australian telecommunications electronics research board sl 
olivetti research laboratory compiling maintaining orl database 
approximation training data target function noise approximation training data target function noise hidden node hidden nodes approximation training data target function noise approximation training data target function noise hidden nodes hidden nodes 
mlp interpolation range number hidden nodes increased uniformly distributed random variable 
large degree overfitting observed 
function abu mostafa 
vapnik chervonenkis dimension information versus complexity learning neural computation 
akaike 
statistical predictor identification annals institute statistical mathematics 
akaike 
information theory extension maximum likelihood principle petrov eds proceeding nd international symposium information theory budapest pp 

akaike 
new look statistical model identification ieee transactions automatic control 
amari 
learning statistical inference arbib ed handbook brain theory neural networks mit press cambridge massachusetts pp 

back 
new techniques nonlinear system identification neural networks linear systems phd thesis department electrical engineering university queensland 
approximation training data target function noise approximation training data target function noise hidden node hidden nodes approximation training data target function noise approximation training data target function noise hidden nodes hidden nodes 
mlp interpolation range number hidden nodes increased uniformly distributed random variable 
network hidden nodes provides best generalization performance larger networks resulted worse generalization due overfitting 
function baldi hornik 
neural networks principal component analysis learning examples local minima neural networks 
barron 
complexity regularization application artificial neural networks ed nonparametric functional estimation related topics kluwer academic publishers dordrecht netherlands pp 

barron 
neural net approximation proceedings seventh yale workshop adaptive learning systems yale university new haven ct pp 

bartlett 
vapnik chervonenkis dimension bounds layer networks neural computation 
bartlett 
sample complexity pattern classification neural networks size weights important size network technical report australian national university 
baum haussler 
size net gives valid generalization neural computation 
bengio ed 
neural networks speech sequence recognition thomson 
bourlard morgan 
speech recognition hybrid approach kluwer academic publishers boston ma 
breiman 
discussion neural networks related methods classification journal royal statistical society 
li 
feedforward networks learn polynomials neural computation 
caruana 
generalization vs net size neural information processing systems tutorial denver cohn tesauro 
tight vapnik chervonenkis bounds neural computation 
crane pearson 
characterizing neural network error surfaces sequential quadratic programming algorithm machines learn snowbird 

article comp ai neural nets message id ko sjx ix com july 
drucker cortes jackel le cun vapnik 
boosting ensemble methods neural computation 
friedman 
computational learning statistical prediction tutorial neural information processing systems denver geman bienenstock doursat 
neural networks bias variance dilemma neural computation 
gori 
computational seminar university queensland brisbane australia 
gori 
problem local minima backpropagation ieee transactions pattern analysis machine intelligence 

analysis error surface xor network hidden nodes technical report macquarie university sydney australia 
hassibi stork 
second order derivatives network pruning optimal brain surgeon giles hanson cowan eds advances neural information processing systems vol 
morgan kaufmann san mateo ca 
cherkassky hanson oja 
panel neural networks statistical models international conference neural networks icnn ieee piscataway nj pp 

haykin 
neural networks comprehensive foundation macmillan new york ny 
hecht nielsen 
neurocomputing addison wesley new york 
jacobs 
methods combining experts probability assessments neural computation 
kohonen 
self organizing maps springer verlag berlin germany 
kolmogorov 
representation continuous functions variables superpositions continuous functions variable addition dokl 
krogh vedelsby 
neural network ensembles cross validation active learning tesauro touretzky leen eds advances neural information processing systems vol 
mit press pp 

kr se van der eds neural networks fifth edn university amsterdam 

kolmogorov theorem relevant neural computation 

kolmogorov theorem arbib ed handbook brain theory neural networks mit press cambridge massachusetts pp 

le cun 
efficient learning second order methods tutorial neural information processing systems 
le cun denker solla 
optimal brain damage touretzky ed advances neural information processing systems vol 
denver morgan kaufmann san mateo pp 

maass 
vapnik chervonenkis dimension neural networks arbib ed handbook brain theory neural networks mit press cambridge massachusetts pp 

haines hecht nielsen 
back propagation error surfaces local minima international joint conference neural networks vol 
washington ieee new york 
minsky papert 
perceptrons expanded version original edn mit press cambridge ma 
moody 
effective number parameters analysis generalization regularization nonlinear learning systems moody hanson lippmann eds advances neural information processing systems vol 
morgan kaufmann san mateo ca pp 

ller finke schulten murata amari 
numerical study learning curves stochastic multi layer feed forward networks neural computation press 
intrator horn 
training single networks optimal ensemble performance gerald tesauro david touretzky ed advances neural information processing systems mit press cambridge massachusetts 
perrone cooper 
networks disagree ensemble method neural networks ed neural networks speech image processing chapman hall 
press teukolsky vetterling flannery 
numerical recipes second edn cambridge university press cambridge 
renals morgan cohen franco 
connectionist probability estimation decipher speech recognition system proceedings ieee international conference acoustics speech signal processing pp 

ripley 
statistical ideas selecting network architectures invited presentation neural information processing systems 
rivlin 
approximation functions publishing waltham massachusetts 
robinson 
application recurrent nets phone probability estimation ieee transactions neural networks 
saad solla 
exact solution line learning multilayer neural networks physical review letters 
saad solla 
learning corrupted examples multilayer networks technical report ncrg aston university uk 

comp ai neural nets frequently asked 
antsaklis 
simple method derive bounds size train multilayer neural networks ieee transactions neural networks 
questions 
personal communication 
vapnik 
estimation dependencies empirical data springer verlag berlin 
vapnik 
nature statistical learning theory springer 
werbos 
regression new tools prediction analysis behavioral sciences phd thesis harvard university 
wolpert macready 
free lunch theorems search technical report sfi tr santa fe institute 
yu 
backpropagation error surface local minima ieee transactions neural networks 

