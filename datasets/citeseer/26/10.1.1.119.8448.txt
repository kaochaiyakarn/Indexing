letter communicated andrew barto multiple model reinforcement learning kenji doya doya atr jp human information science laboratories atr international seika soraku kyoto japan crest japan science technology seika soraku kyoto japan kawato dynamic brain project japan science technology seika soraku kyoto japan nara institute science technology nara japan atr jp human information science laboratories atr international seika soraku kyoto japan kawato dynamic brain project japan science technology seika soraku kyoto japan ken ichi ne jp atr human information processing research laboratories seika soraku kyoto japan nara institute science technology nara japan kawato kawato atr jp human information science laboratories atr international seika soraku kyoto japan kawato dynamic brain project japan science technology seika soraku kyoto japan nara institute science technology nara japan propose modular reinforcement learning architecture nonlinear nonstationary control tasks call multiple model reinforcement learning mmrl 
basic idea decompose complex task multiple domains space time predictability environmental dynamics 
system composed multiple modules consists state prediction model reinforcement learning controller 
responsibility signal softmax function prediction errors weight outputs multiple modules gate learning prediction models reinforcement learning controllers 
formulate mmrl discrete time nite state case continuous time case 
performance mmrl demonstrated discrete case neural computation massachusetts institute technology kenji doya nonstationary hunting task grid world continuous case nonlinear nonstationary control task swinging pendulum variable physical parameters 
big issue application reinforcement learning rl real world control problems deal nonlinearity nonstationarity 
nonlinear high dimensional system conventional discretizing approach necessitates huge number states learning slow 
standard rl algorithms perform badly environment nonstationary hidden states 
problems motivated modular hierarchical rl architectures singh dayan hinton littman cassandra kaelbling wiering schmidhuber parr russel sutton precup singh morimoto doya 
basic problem modular hierarchical rl decompose complex task simpler subtasks 
article presents new rl architecture multiple modules composed state prediction model rl controller 
architecture nonlinear nonstationary control task decomposed space time local predictability environmental dynamics 
mixture experts architecture jacobs jordan nowlan hinton applied nonlinear nonstationary control tasks kawato cacciatore nowlan 
success modular architecture depends strongly capability gating network decide modules recruited particular moment 
alternative provide experts prediction model environment prediction errors selecting controllers 
narendra balakrishnan model smallest prediction error xed set prediction models selected associated single controller control 
prediction models trained little prior knowledge task decomposition initially far optimal 
hard competition lead suboptimal task decomposition 
bayesian statistical framework pawelzik ller proposed annealing soft competition network time series prediction segmentation 
tani nol similar mechanism hierarchical sequence prediction 
softmax function module selection combination originally proposed tracking control paradigm multiple paired forward inverse models wolpert kawato wolpert kawato wolpert kawato 
multiple model reinforcement learning schematic diagram mmrl architecture 
reformulated modular selection identi cation control mosaic wolpert ghahramani wolpert kawato 
article apply idea softmax selection modules paradigm reinforcement learning 
resulting learning architecture call multiple model reinforcement learning mmrl learns decompose nonlinear nonstationary task competition cooperation multiple prediction models reinforcement learning controllers 
section formulate basic mmrl architecture section describe implementation discrete time continuous time cases including multiple linear quadratic controllers 
rst test performance mmrl architecture discrete case hunting task multiple preys grid world section 
demonstrate performance mmrl continuous case nonlinear nonstationary control task swinging pendulum variable physical parameters section 
multiple model reinforcement learning shows organization mmrl architecture 
composed modules consists state prediction model reinforcement learning controller 
basic idea modular architecture decompose nonlinear nonstationary task multiple domains space time domains environmental dynamics predictable 
kenji doya action output rl controllers learning rates predictors controllers weighted responsibility signal gaussian softmax function errors outputs prediction models 
advantage module selection mechanism areas specialization modules determined bottom fashion nature environment 
furthermore area module specialization design control strategy facilitated availability local model environmental dynamics 
consider discrete time nite state environment 
ng 
mg discrete states actions continuous time continuous state environment px state action vectors noise 
actions policy stochastic deterministic 
reward function state action 
goal reinforcement learning improve policy rewards acquired long run 
basic strategy reinforcement learning estimate cumulative reward current policy value function state improve policy value function 
de ne value function state current policy kd discrete case sutton barto ds continuous case doya parameters discounting reward :10.1.1.38.7461:10.1.1.38.7461
multiple model reinforcement learning responsibility signal 
purpose prediction model module predict state discrete time temporal derivative state continuous time observation state action 
responsibility signal li wolpert kawato relative goodness predictions multiple prediction models 
uni ed description denote new state discrete case temporal derivative state continuous case px 
basic formula responsibility signal bayes rule li pn jd prior probability selecting module likelihood model observation 
discrete case prediction model gives probability distribution new state ox previous state action ox fi ox 

prior knowledge module selection take priors uniform responsibility signal li fi pn jd fj newly observed state 
continuous case prediction model gives temporal derivative state fi 
assuming prediction error gaussian variance responsibility signal gaussian softmax function li kpx jd kpx px observed state change 
kenji doya module weighting responsibility signal 
mmrl architecture responsibility signal li purposes weighting state prediction outputs gating learning prediction models weighting action outputs gating learning reinforcement learning controller state prediction outputs prediction models weighted responsibility signal li 
discrete case prediction state ox nx li fi ox 
id continuous case predicted state derivative nx li 
id predictions model rl algorithms annealing described 
prediction model learning responsibility signal li weighting parameter update prediction models 
general realized scaling error signal prediction model learning li 
action output outputs reinforcement learning controllers linearly weighted li action output 
discrete case probability action nx li gi 
id continuous case output interpolation modular outputs nx li ui id nx id li gi 
reinforcement learning li weighting learning rl controllers 
actual equation parameter update varies choice rl algorithms detailed section 
temporal difference td algorithm barto sutton anderson sutton doya td error multiple model reinforcement learning discrete case continuous case weighted responsibility signal di li learning ith rl controller :10.1.1.38.7461:10.1.1.38.7461:10.1.1.132.7760
weighting factor li training prediction models rl controllers helps rl controller learn appropriate policy value function context paired prediction model valid predictions 
responsibility predictors 
prior knowledge belief module selection incorporate responsibility predictors wolpert kawato 
assuming outputs proportional prior probability module selection equation responsibility signal li 
modular decomposition task desired modules switch frequently 
enforced incorporating responsibility priors assumption temporal continuity spatial locality module activation 
temporal continuity 
continuity module selection incorporated previous responsibility signal responsibility prediction signal 
discrete case take responsibility prediction previous responsibility li parameter controls strength memory effect 
equations responsibility signal time product likelihoods past module selection li ty kd ak denotes normalizing factor pn qt jd kd ak 
kenji doya continuous case choose prior li dt dt arbitrarily small time difference note coincides dt 
likelihood module gaussian px kpx recursion equation responsibility signal time li dt kd px kdt dt pt dt kd kpx kdt kdt gaussian softmax function temporally weighted squared errors 
limit dt equation represented li ei jd ej ei low pass ltered prediction error pei log ei kpx 
low pass ltered prediction responsibility prediction helpful avoiding chattering responsibility signal pawelzik 
spatial locality 
continuous case consider gaussian spatial prior ci ci jd cj cj ci center area specialization mi covariance matrix speci es shape denotes transpose 
parameters updated approximate distribution input state weighted responsibility signal pci ci pmi mi ci ci gc gm update rates 
multiple model reinforcement learning implementation mmrl architecture rl controllers mmrl generally possible model free rl algorithms actor critic learning 
prediction models environmental dynamics intrinsic components architecture advantageous prediction models just module selection designing rl controllers 
describe model rl algorithms discrete time continuous time cases 
special implementation continuous time case multiple linear quadratic controllers derived linear dynamic models quadratic reward models 
discrete time mmrl 
consider implementation mmrl architecture discrete time nite state nite action problems 
standard way predictive model rl action selection step search arg max ox predicted immediate reward ox state predicted current state candidate action order implement algorithm provide module reward model ori value function vi dynamic model fi ox 
candidate action evaluated ox nx li ori id nx vi ox fi ox 
sake exploration stochastic version greedy action selection equation action selected gibbs distribution pm controls stochasticity action selection 
parameters updated error signals weighted responsibility signal li fi dynamic model 
zero li ori reward model li value function model 
continuous time mmrl 
consider continuous time mmrl architecture 
model rl algorithm continuous time kenji doya continuous state system see equation derived hamilton jacobi bellman hjb equation max time constant reward discount doya :10.1.1.38.7461
assumptions system linear respect action action cost convex greedy policy vector representing steepest ascent direction value function matrix representing input gain dynamics sigmoid function shape determined control cost doya :10.1.1.38.7461
implement hjb algorithm provide module dynamic model fi value model vi 
outputs dynamic models equation compared observed state dynamics px calculate responsibility signal li equation 
model outputs linearly weighted li state prediction nx id value function estimation li fi nx li vi 
id derivatives dynamic models fi calculate action module ui value models vi fi vi 
weighted li equation actual action 
learning weighted prediction errors li px dynamic models li value function models 
multiple model reinforcement learning multiple linear quadratic controllers 
modular architecture mmrl universal nonlinear function approximators large numbers degrees freedom problematic lead undesired solution single module tries handle task domain 
linear models prediction models controllers reasonable choice local linear models shown properties quick learning generalization schaal atkeson 
furthermore reward function locally approximated quadratic function linear quadratic controller see bertsekas rl controller design 
local linear dynamic model ai local quadratic reward model ori qi module xd center local prediction state reward respectively 
bias quadratic reward model 
value function quadratic form vi xvi pi 
matrix pi solving riccati equation pi pi pi qi 
center bias value function qi xv qi 
optimal feedback control module linear feedback ui pi 
action output weighting controller outputs responsibility signal li nx li ui 
id kenji doya parameters local linear models ai bi quadratic reward models qi ri updated weighted prediction px li ori respectively 
assume update models slow riccati equations may recalculated intermittently 
call method multiple linear quadratic controllers 
simulation discrete case order test effectiveness mmrl architecture rst applied discrete mmrl architecture nonstationary hunting task grid world 
hunter agent tries catch prey torus grid world 
states representing position prey relative hunter 
hunter chooses possible actions east south west 
prey moves xed direction trial 
trial movement directions nw se swg randomly selected prey placed random position grid world 
hunter catches prey stepping grid prey reward 
step movement costs 
trial terminated hunter catches prey fails catch steps 
order compare performance mmrl conventional methods applied standard learning compositional learning cq singh task 
major difference cq mmrl criterion modular decomposition cq uses consistency modular value functions mmrl uses prediction errors dynamic models 
cq gating network component learning modules trained composite value approximates action value function entire problem 
original cq singh output gating network augmenting bit explicitly signaled change context 
goal agent learn appropriate decomposition task explicit cue modi ed cq see appendix details algorithm parameters 
results 
shows performance difference standard qlearning cq mmrl hunting task 
modi ed cq perform signi cantly better standard learning 
investigation modular functions cq revealed simulation runs modules appropriately differentiate different kinds preys 
hand performance mmrl approached close theoretical optimum 
modules successfully specialized kinds prey movement 
multiple model reinforcement learning comparison performance standard learning gray line modi ed cq dashed line mmrl thick line hunting task 
average number steps needed catching prey trial epochs simulation runs plotted 
dash dotted line shows theoretically minimal average steps required catching prey 
shows examples value functions prediction models learned mmrl 
output prediction models fi seen modules specialized prey moving ne nw sw se respectively 
landscapes value functions vi accordance movement directions prey 
possible reason difference performance cq mmrl task dif culty module selection 
cq prey far hunter differences discounted values different kinds prey minor 
dif cult differentiate modules solely values 
mmrl hand module selection state change case prey movement relatively easy prey far hunter 
simulation continuous case order test effectiveness mmrl architecture control applied algorithm described section task swinging pendulum limited torque see doya :10.1.1.38.7461
driving torque limited max max max 
pendulum back forth bottom build momentum successful swing 
kenji doya example value functions prediction models learned mmrl trials 
slot grid shows position prey relative hunter state 
state value functions vi 
prediction model outputs fi ox current state prey xed shown circle action xed stay state space dimensional ph joint angle means pendulum hanging 
action reward height tip negative squared torque cosh rt 
trial started random joint angle angular velocity 
devised automatic annealing process parameter softmax function responsibility signal equation skc sk denotes number trial ek average state prediction error kth trial 
parameters initial value set 
task decomposition space nonlinear control 
rst modules linear dynamic model see equation quadratic reward model see equation 
centers local linear dynamic models initially placed randomly angular component 
multiple model reinforcement learning trial started random position pendulum lasted seconds 
shows example swing performance bottom position 
initially rst prediction model predicts pendulum motion better second responsibility signal close 
output rst rl controller bottom position control 
pendulum driven away bottom second prediction model predicts movement better higher second rl controller takes stabilizes upright position 
shows changes linear prediction models quadratic reward models learning 
linear prediction models approximated nonlinear gravity term 
rst model predicted negative feedback acceleration equilibrium state pendulum hanging 
second model predicted positive feedback acceleration unstable equilibrium pendulum raised 
reward models approximated cosine reward function parabolic curves 
shows dynamic reward models modules 
modules specialized bottom position modules specialized near top position modules centered 
result shows proper modularization possible redundant modules 
compares time course learning modules actor critic doya :10.1.1.38.7461
learning fastest modules 
addition redundant modules resulted variability time course learning 
multiple possible ways modular decomposition due variability sample trajectories took longer modular decomposition stabilize 
learning module faster architecture 
interesting feature strategy qualitatively different controllers derived solutions riccati equations 
controller bottom positive feedback controller equilibrium reward minimal controller top typical linear quadratic regulator stabilizes upright state 
important feature modules switched simply prediction errors 
successful swing achieved top planning complex sequence 
task decomposition time nonstationary pendulum 
tested effectiveness mmrl architecture nonlinear nonstationary control tasks mass length pendulum changed trial 
kenji doya example swing performance 
dynamics ml rh sinh ph physical parameters max 
trajectory initial state rad rad 
start goal 
solid line module 
dashed line module 
time course state top action middle responsibility signal bottom 
multiple model reinforcement learning development state reward prediction models 
outputs state prediction models learning 
outputs reward prediction model learning 
solid line module 
dashed line module dotted line targets rx 
centers spatial responsibility prediction outputs modules 
state prediction models 
reward models 
kenji doya learning curves pendulum swing task 
cumulative reward dt trial shown simulation runs 
modules 
modules 
modules 
architecture 
modules linear dynamic model see equation quadratic reward model see equation 
centers xi local linear prediction models initially set randomly 
trial started random position lasted seconds 
implemented responsibility prediction tc tm tp 
parameters annealing initial value 
rst trials physical parameters xed fm 
shows change position gain fa rh prediction models 
control performance shown 
figures show outputs prediction models section fh 
initial position gains set randomly see 
trials modules specialized bottom region learned similar prediction models 
modules learned prediction model top region see 
accordingly rl controllers modules learned reward model minimum near multiple model reinforcement learning time course learning changes prediction models 
changes coef cient rh prediction models coef cient angle 
change average reward trial 
thin lines results simulation runs 
thick line average simulation runs 
note average reward new longer pendulum lower successful learning longer period swinging 
linear prediction models section fh learning trials xed parameters trials changing parameters 
slopes linear models correspond shown destabilizing feedback policy equations 
modules learned reward model peak near implemented stabilizing feedback controller 
trials parameters pendulum switched fm fm trial 
rst degenerated modules tried follow alternating environment see swing successful new longer pendulum 
performance shorter pendulum disturbed see 
trials prediction models gradually specialized new learned dynamics see successful swing achieved shorter longer 
kenji doya similar module specialization simulation runs 
runs due bias initial module allocation modules aggregated domain top bottom model covered domain stationary condition 
trials nonstationary condition module specialization shown achieved 
discussion proposed mmrl architecture decomposes nonlinear nonstationary task space time local predictability system dynamics 
tested performance mmrl nonlinear nonstationary control tasks 
shown simulations pendulum swing task multiple prediction models successfully trained corresponding model controllers derived 
modules specialized different domains state space 
con rmed nonstationary pendulum swing task available modules allocated different domains space time task demands 
modular control architecture multiple prediction models proposed wolpert kawato computational model cerebellum wolpert wolpert kawato 

showed fmri experiments novel tool large area cerebellum activated initially smaller area remains active long training 
proposed local activation spots neural correlates internal models tools 
suggested internal models different tools represented separated areas cerebellum 
simulation results nonstationary environment provide computational account fmri data 
new task introduced modules initially compete learn 
repetitive learning subset modules specialized recruited new task 
argue reinforcement learning architecture uses lq controllers calculated line 
linear dynamic models quadratic reward models learned line simulations entire system realizes reinforcement learning 
limitation architecture reward function helpful gradients modular domain 
method backpropagating value function successor module effective reward predecessor module development 
order construct hierarchical rl system appears necessary combine top bottom approaches task decomposition 
mmrl architecture provides solution bottom approach 
combination bottom mechanism top mechanism subject ongoing study 
multiple model reinforcement learning appendix modi ed compositional learning time step gating variable gi prior probability module selection case assumption temporal continuity equation gi li 
lj composite values state computed oq nx gi qi id action selected eb 
reward acquired state changes td error module ei max oq qi 
gaussian assumption value prediction error likelihood module ei ei responsibility signal posterior probability selecting module li gi ei gj ej 
values module updated weighted td error li ei error signal 
discount factor set greediness parameters mmrl cq 
decay parameter temporal responsibility predictor mmrl 
tried different values cq success 
value figures 
kenji doya acknowledgments daniel wolpert chris atkeson jun tani kimura helpful discussions 
barto sutton anderson 

neuronlike adaptive elements solve dif cult learning control problems 
ieee transactions systems man cybernetics 
bertsekas 

dynamic programming optimal control 
belmont ma athena scienti cacciatore nowlan 

mixture controllers jump linear non linear plants 
cowan tesauro alspector eds advances neural information processing system 
san mateo ca morgan kaufmann 
dayan hinton 

feudal reinforcement learning 
giles hanson cowan eds advances neural information processing systems pp 

san mateo ca morgan kaufmann 
doya 

reinforcement learning continuous time space 
neural computation 
kawato 

recognition manipulated objects motor learning modular architecture networks 
neural networks 
wolpert kawato 

multiple paired models human motor learning control 
kearns solla cohen eds advances neural information processing systems pp 

cambridge ma mit press 
wolpert kawato 

mosaic model sensorimotor learning control 
neural computation 
sasaki tz kawato 

separated modules visuomotor control learning cerebellum functional mri study 
toga mazziotta eds neuroimage third international conference functional mapping human brain vol 

copenhagen denmark academic press 
sasaki tz kawato 

human cerebellar activity re ecting acquired internal model new tool 
nature 
jacobs jordan nowlan hinton 

adaptive mixtures local experts 
neural computation 
littman cassandra kaelbling 

learning policies partially observable environments scaling 
prieditis russel eds machine learning proceedingsof th international conference pp 

san mateo ca morgan kaufmann 
morimoto doya 

acquisition stand behavior real robot hierarchical reinforcement learning 
robotics autonomous systems 
multiple model reinforcement learning narendra balakrishnan 
june 
adaptation learning multiple models switching tuning 
ieee control systems magazine 
parr russel 

reinforcement learning hierarchies machines 
jordan kearns solla eds advances neural pp 
cambridge ma mit press 
pawelzik ller 

annealed competition experts segmentation classi cation switching dynamics 
neural computation 
schaal atkeson 

isolation cooperation alternative view system experts 
touretzky mozer hasselmo eds advances neural information pp 

cambridge ma mit press 
singh 

transfer learning composing solutions elemental sequential tasks 
machine learning 
sutton 

learning predict methods temporal difference 
machine learning 
sutton barto 

reinforcement learning 
cambridge ma mit press 
sutton precup singh 

mdps semi mdps framework temporal abstraction reinforcement learning 
arti cial intelligence 
tani nol 

learning perceive world articulated approach hierarchical learning sensory motor systems 
neural networks 
wiering schmidhuber 

hq learning 
adaptive behavior 
wolpert ghahramani 

computational principles movement neuroscience 
nature neuroscience 
wolpert kawato 

multiple paired forward inverse models motor control 
neural networks 
wolpert kawato 

internal models cerebellum 
trends cognitive sciences 
received june accepted november 
