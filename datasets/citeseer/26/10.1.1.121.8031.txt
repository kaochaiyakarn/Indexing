information theoretic tradeoff complexity accuracy ran gilad bachrach amir naftali tishby school computer science engineering interdisciplinary center neural computation hebrew university jerusalem israel tishby cs huji ac il 
fundamental question learning theory quantification basic tradeoff complexity model predictive accuracy 
valid way quantifying tradeoff known information bottleneck measure complexity model prediction accuracy shannon mutual information 
show information bottleneck framework answers defined known coding problem time provides general relationship complexity prediction measured mutual information 
study nature complexity accuracy tradeoff discuss theoretical properties 
furthermore relations classical information theoretic problems rate distortion theory cost capacity tradeoff source coding side information 
learning human machine known related ability representations data 
supervised learning done choosing hypothesis summarizes training data learning clusters low dimensional features play role 
cases interested concise description preserves data 
learning process deal basic tradeoff complexity conciseness data representation avail able best accuracy goodness fit complexity enables 
measures complexity accuracy may change task 
theory complexity commonly measured vc dimension covering metric entropies class description coding representation 
accuracy measured generalization error mistake bound clusters purity feature efficiency various ways choose study nature tradeoff complexity accuracy information theoretic concepts 
main advantage model powerful properties 
measure representation complexity minimal number describe data sample known rate 
choose measure accuracy amount information data representation target variable 
precise nature target variable depends task labels supervised learning categories noisy data weakly dependent random variable 
statistical dependence data general property target universally mutual information 
fact complexity mutual information proposed information bottleneck ib framework enables quantify general precise way shown study information bottleneck ib method introduced tishby pereira bialek years ago 
relevant information signal respect defined mutual information variable provides 
think asthe minimal number binary questions average asked values variable order reduce possible value variable 
examples include relationship document category words statistics face features person identity speech sounds spoken words gene expression levels tissue samples cases problem map variables considered asthe source signal concise reproduction signal information relevant predicted signal information bottleneck useful various learning applications 
slonim clustering data 
note ib uses infor mation theoretic point view suffer basic flaws geometric algorithms kleinberg 
ib 
poupart studying pomdps baram evaluating expected performance active learning algorithms comprehensive study ib see 
ib related classic problems information theory distortion cost capacity 
rate distortion problem goal encode source way minimizes code length average distortion 
cost capacity problem cost assigned symbol channel alphabet 
task minimize ambiguity constraint average cost 
ib rate distortion formulation non fixed distortion measure atthe time take form cost capacity problem cost function 
combines problems way arbitrary nature distortion measure channel cost 
formally related problem source coding side information setting problem different solution happen similar 
see section detailed discussion relationship 
summary results define ib coding problem ib optimization problem sec tion 
discuss relationship ib problem source side information section 
show ib optimization problem provides tight lower bound forthe ib coding problem section 
define ib tradeoff information curve assigns tion minimal possible value free vari ables conditional distributions 
section study optimization utilize formal relation source coding side information prove ib curve smooth monotone con vex function 
furthermore show sufficient achieve best encoding section show information bottleneck optimization locally equivalent rate distortion problem adequate distortion mea sure considered rate distortion problem variable distortion measure 
show information curve locally equivalent rate distortion functions 
section dual representation ib problem 
representation ib takes form cost capacity problem non fixed cost function 
optimum primal rate dual cost capacity equivalent 
preliminaries notation notation random variables finite respectively 
instances variables 
denote entropy random variable mutual information denoted dkl kullback liebler kl divergence distributions 
specified logarithms base 
problem setting assume random variables joint distribution encode reproduction alphabet way keeps maximum information rate minimal rate value iy information usually done information theory block encoding discuss average rate average information 
show tradeoff tween values discovered optimization problem definition 
going introduce definitions measure average information 
note definition similar rate distortion code 
difference thatthe average distortion replaced information 
assumption proving convexity information curve lemma proofs 
definition rate information code 
nr rate information encoding function fn 
nr decoding function gn nr 
information associated nr defined fn gn nx ii gamma gn ffi fn xn delta gamma delta ij information element calculated respect distri bution defined code coordinate follows pi xn xn xn xn rate information pair iy said achievable exists se quence rate information codes fn gn asymptotic rate asymptotic information larger equal iy limn fn gn iy rate information region closure set achievable rate infor mation pairs iy 
definition rate information function 
rate information function iy infimum rates iy isin rate information region constraint information iy definition ib function 
ib function iy random vari ables defined iy min iy minimization normalized distributions 
note constraint depends appear minimization problem px follows markov chain minimum exists continuous function minimization compact set possible define dual function max show functions equivalent curve 
theorem shows curve iy thesame 
refer curve ib curve 
see illustration curve tishby lagrange multipliers analyze ib optimization problem 
proved conditional distribution minimum exponential form stated theorem 
iy iy fig 

typical ib curve 
graphs iy left right 
curve computed empirically joint distribution size 
theorem tishby 
optimal assignment ib minimization problem definition satisfies equation fi fi lagrange multiplier corresponds constraint fi normalization function 
exponent bayes rule markov note solution formal solution exponent isdefined implicitly terms assignment mapping 
relation source coding side information problem source coding side information decoder information theory community mid seventies 
known wyner ahlswede problem 
lately closely related information bottleneck 
order explore relations frameworks give problem 
framework study situation variable way allow reconstruct presence information variable 
formally non independent random variables 
encoded separately rates accordingly 
codes available decoder 
rates achievable allows exact reconstruction sense 
referred side information 
see illustration independently time minimal achievable rate constraint min adding constant get defined surprising equivalence explained follows value measures rate add top side information inorder fully reconstruct compliment quantity amount known side information measures quantization accuracy ib framework 
similarity rx decoder encoder encoder fig 

network correspond tothe problem problems possible share knowl edge optimization problem 
example algorithms de veloped ib valid 
anyway despite technical similarity motivation applications different kind arises different 
example coding theorems ib related different 
coding theorem section state prove main results ib function coding problem definition 
theorem 
rate information function sampling associated ib function 
iy iy min iy ib function lower bounds iy show code achieves information larger iy iy 
proof similar converse proof rate distortion theorem see sake completeness proof uses properties iy insection 
complete proof theorem section 
proof lower bound theorem 
consider nr code defined func tions fn gn 
xn xn xn gn ffi fn xn reproducing xn 
joint distribution induced code xn xn ae gn ffi fn xn nr elements elements xn fact conditioning reduces entropy nr xn xn xn xn xn xn xn xn xn nx xi nx xi xn nx xi nx xi xi nx xi xi definition convexity nx xi xi nx xi yi nx xi yi nr nx xi yi 
marginals exactly pi defined fact non decreasing nr nx xi yi 
nr fn gn nr iy combining get stated result 
ut ib function prove ib function iy function tight bound 
series codes rn constructed 
enhance rate distortion ways encode respect multiple distortion measures time 
require thatthe distortion coordinate block close distortion induced rate distortion average distortion counts selecting appropriate distortion measures complete proof 
introduce definitions lemmas 
definition multi distortion jointly typical 
joint prob ability distribution 
dk set distortion measures ffl pair sequences xn xn said multi ffl typical log xn log xn log xn xn dj xn xn fififi ffl dj xn xn defined pni dj xn xn 
set denoted ffl lemma 
xi xi drawn 
pr ffl result follows central limit theorem 
lemma 
finite set bounded 
dk exists series codes fn gn asymptotic rate dj average distortion code converges uniformly ep dj 
ffl exist dj xn gn ffi fn xn ep dj fififi ffl proof 
random codebook mapping multi distortion joint typical ity follows generation codebook randomly generate codebook consisting ni sequences xn drawn 
index codewords 
ni 
encoding encode xn exist 
xn xn ffl send 
rest proof showing probability greater zero construction achieves required distortion inthe rate distortion theorem example pp 
ut definition distortion coordinate 
source dis measure code fn gn 
distortion coordinate epn xn xn gn ffi fn xn note total distortion code fn gn average 
lemma 
setting lemma possible add requirement distortion measure dj distortion coordinates 
proof 
achieve additional demand making code symmetric start code satisfies requirements lemma 
xn inthe code add cyclic permutations codebook 
enlarge codebook factor change asymptotic rate 
oe cyclic permutation note xn xn ffl implies oe xn oe xn ffl possible change encoding hold oe gn ffi fn xn gn ffi fn oe xn sacrificing average distortion 
fix distortion measure dj ibe coordinates code 
cyclic permutation oe epn xn xn gn ffi fn xn xn xn xn gn ffi fn xn xn oe xn oe xn gn ffi fn oe xn xn xn xn oe gn ffi fn xn oe epn xn xn oe gn ffi fn xn oe equalities follows equation xn oe xn 
ut ready complete proof theorem 
proof theorem 
suffices show joint distri bution possible construct series codes fn gn asymptotic rate asymptotic information define pair distortion measure follows dx ae ep dx ep xn dx xn gn ffi fn xn nx pi pi defined distortion measures construction lemma equations build series codes fn gn asymptotic rate large pi ffl information code continuous function respect pi follows information convergence 
ut properties ib curve section study ib function iy poses 
similarity problem see section adopt results note iy minimum iy increases 
iy non decreasing function iy second note contrary rate distortion scenario elements get meaning distortion measure scenario matter 
clear solution may optimal lemma shows bigger 
lemma 
cardinality sufficient achieve optimal constraint iy case convexity lemma 
ib function iy convex function iy concave function lemmas proved 
note prove different setting modifications straightforward 
curve continuous interior monotonic convex smooth mild conditions stated lemma 
lemma 
slope iy continuous approaches iy approaches proof 
point curve achieves point 
convexity know straight line pass curve lies upper half line 
denote fi slope line 
fii iy optimal exponential form fi assume line slope fi fi properties thatthe curve smooth point 
true fi fi dkl fact dkl finite fi fi fi fi dkl left hand side independent right hand side follow 
ut easy see assumption zeros lemma necessary example curve correspond block matrix constant finite slope 
bb cca corollary 
equation follows optimal solution lagrangian fii fi slope iy point 
lemma 
ib function iy continuous function 
proof follows continuity mutual information iy function iy 
information bottleneck rate distortion section show information bottleneck locally equivalent distortion problem adequate distortion measure considered non fixed distortion measure 
show curve envelope locally equivalent curves 
definition 
define dkl lemma 
fixed px proof lemma follows markov chain manipulation 
define rate distortion minimization problem minp hd ibi lemma 
note rate distortion problem distortion measure note generally lagrange multipliers optimum point gradient lagrangian vanishes 
fixed 
fixed means depends minimization parameter 
dependency follows dkl dkl lemma 

conditional distribution define rate distortion problem minp dkl proof lemma omitted due space limitation 
corollary 
px rate distortion problems source distortion measure dkl 
hold ib curve switched axis rate distortion curve 
correspond point ib curve rate tangent ib curve point 
ib curve tight lower bound envelope rate form 
demonstration corollary 
information bottleneck cost capacity previous section shown information bottleneck rate distortion problem non fixed distortion measure 
section consider dual representation 
representation takes cost capacity problem non fixed cost function 
show dual problems equivalent 
definition 
noisy channel cost function ir pa cost capacity function defined maxp distortion iy rate rate fig 

ib curve envelope rate distortion curves random matrix 
bold line ib curve switched axis 
dashed lines rate distortion curves source different distortion measures induced substituting different dkl 
right shows normalized graph curve divided ib curve 
consider defining noisy channel define cost function log log ip maxp max note defines fixed sound show maximization problem really dual original minimization problem optimization problems defines 
lemma 
optimization problems iy defines thesame curve switched axes iy proof 
prove iy iy vice verse exists iy iy 
suffices show properties hold relevant iy possible achieve larger iy equal smaller iy relevant possible achieve smaller equal larger property clear convexity iy impossible achieve 
second property follows concavity follows possible move point achieve smaller concavity possible maximal possible value possible ut research provided rigorous formulation information information theoretic question lower bound rate code preserves mutual information vari able 
method successfully applied finding efficient representations numerous learning problems distribution estimated far proper information theoretic justification 
showed ib method solves natural information theoretic prob lem formally related source coding side information 
defined sense problem unifies aspects rate distortion theory cost cases effective distortion channel cost function simultaneously determined joint statistics data single tradeoff parameter 
proved joint distribution tight achievable convex smooth bound representation length variable mutual information representation maintains variable 
problem continuous function lemma expect calculating bound sampled version joint distribution give approximation 
representation cardinality exceed cardinality original variable 
finding compact representation crucial known component learning occam razor mdl 
information theoretic tools order quantify quality representations accuracy mutual information 
natural ideas connect bounds generalization error bounds common learning theory 
related interesting issues left outside ib curve specific joint distributions simpler conditions ensure convexity smoothness 
know joint distributions analytic expressions curve general distributions finding curve computationally diffi cult 
intriguing remaining questions optimal tradeoff bounded computational complexity 
ii whatis nature deviations optimal finite sample joint distribution sample complexity fitting pro 
iii similar coding theoretic formulation multivariate ib fact network information theoretic tradeoff 
acknowledgment eyal krupka noam invaluable discussions ideas 
rb supported clore foundation 
supported horowitz foundation 
partly israel science foundation 

ahlswede korner 
source coding side information converse degraded broadcast channels 
ieee transaction information theory november 

baram el yaniv luz 
online choice active learning algorithms 
submitted publication 

cardinal 
compression side information 
ieee international conference multimedia expo 

cover thomas 
elements information 
wiley interscience 

kleinberg 
impossibility theorem clustering 
proc 
th conference neural information processing systems 

poupart boutilier 
value directed compression pomdps 
proc 
th conference neural information processing systems 

shannon 
mathematical theory communication 
bell system technical journal july october 

slonim 
information bottleneck theory applications 
phd thesis hebrew university 

slonim tishby power word clustering text classification 
proc 
rd european colloquium information retrieval research 

slonim tishby 
objective classification galaxy spectra information bottleneck method 
monthly notes royal astronomical society 

slonim tishby 
document clustering word clusters information bottleneck method 
proc 
rd annual international acm sigir conference research development information retrieval 

tishby pereira bialek 
information bottleneck method 
proc 
th annual allerton conference control computing pages 

wyner 
conditional entropy bound pair discrete random variables 
ieee transaction information theory september 

wyner 
source coding side information decoder 
ieee transaction information theory may 
