wow self organizing wide area overlay networks virtual workstations ganguly agrawal oscar renato figueiredo advanced computing information systems laboratory university florida gainesville florida email renato acis ufl edu describes wow distributed system combines virtual machine overlay networking peerto peer techniques create scalable wide area networks virtual workstations high throughput computing 
system architected facilitate addition nodes pool resources system virtual machines vms self organizing virtual network links maintain ip connectivity vms migrate network domains users applications environment functionally identical local area network cluster workstations 
describe novel extensible user level decentralized technique discover establish maintain overlay links tunnel ip packets different transports including udp tcp firewalls 
report experiments conducted testbed wow deployment router nodes planetlab vmware vm nodes distributed domains 
experiments show latency joining wow network order seconds set trials nodes self configured routes seconds established direct connections nodes seconds 
experiments show testbed delivers performance unmodified representative benchmarks drawn life sciences domain 
testbed wow achieves throughput jobs minute pbs scheduled executions meme application average single job sequential running time parallel speedup pvm application 
experiments demonstrate system capable seamlessly maintaining connectivity virtual ip layer typical client server applications nfs ssh pbs vms migrate wan 
virtualization techniques address key challenges deployment wide area distributed computing environments 
system virtual machines vmware xen fully decouple execution environment exposed applications guest vm host allowing nodes distributed multiple domains configured managed consistent software base 
virtual networks decouple management address spaces provide full tcp ip connectivity nodes network address translation nat firewall routers 
describes distributed system integrates virtual machine self organized virtual networking create scalable wide area networks virtual workstations 
system described architected scale large number nodes facilitate addition nodes system self configuration virtual network links maintain ip connectivity virtual machines migrate domains users applications environment functionally identical local area network cluster workstations 
doing wow nodes deployed independently different domains wow distributed systems managed programmed just local area networks reusing unmodified subsystems batch schedulers distributed file systems parallel application environments familiar system administrators users 
wow techniques variety scenarios design driven needs arisen collaborative environments 
specifically techniques allow users community easily pool resources form wow configured typical cluster nodes configured homogeneous software distribution connected private network 
adding vm guest hosted typical linux windows platform running wow involves simple time setup local system administrator requires minimal changes host install suitable vm monitor clone base vm image instantiate 
run unmodified operating systems applications inside vms applied facilitate deployment existing successful middleware frameworks reach wide variety hosts 
instance base wow vm image installed condor binaries quickly replicated multiple sites host homogeneously configured distributed condor pool vm image installed globus binaries configured network interface public network serve gatekeeper pool 
key contributions 
describe novel extensible user level framework discover establish maintain overlay links tunnel ip packets different transports including udp tcp decentralized manner 
second report experiments conducted realistic deployment demonstrate current implementation wow supports middleware configurations typical cluster environments provides performance high throughput sequential jobs parallel applications 
contribution extended brunet node node node org ufl edu planetlab nodes wan edu node node lsu edu node gru net node edu fig 

wow testbed experiments 
wow compute nodes hosted universities level nat firewall routers nodes florida illinois northwestern louisiana node virginia north carolina unc 
node home network multiple nats vmware wireless router isp provider 
total router nodes run planetlab hosts part overlay network 
protocol ipop virtual network system support demand establishment direct overlay links communicating wow nodes 
direct connections allow nodes bypass intermediate overlay routers communicate directly nodes ip packet exchanges nodes detected overlay 
technique similar topology adaptation described achieve purely decentralized manner capable traversing nat firewall routers hole punching techniques decentralized manner 
second contribution evaluated performance wow prototype quantitatively variety experiments 
prototype uses ipop ip overlay support decentralized establishment direct connections vmware hosted vms 
experiments conducted testbed router nodes deployed planetlab compute nodes deployed different domains 
experiments designed demonstrate ability implementation support existing middleware applications quantify performance benefits direct overlay connections 
related publications investigated overhead machine network virtualization perspectives 
experimental results show nodes joining wow autonomously establish connections ip typically seconds establish direct connections nodes communicate seconds 
performance analyses realistic batch sequential parallel applications show node compute cluster sustains throughput batch jobs jobs minute meme sequential runs average run time sec node node delivers parallel speedups pvm 
support decentralized forming direct connections provides higher job throughput meme better parallel performance pvm compared setup direct connections supported 
report ability wow autonomously reestablish virtual network links vm migrates wide area network 
experiments show wow nodes successfully resume execution tcp ip client server applications migration including pbs scheduled job uses data nfs mounted file system ssh file transfer manner completely transparent applications 
remainder organized follows 
section ii discusses related 
section iii describes configuration deployment wow virtual machine nodes 
section iv details connection establishment techniques form virtual network links wow nodes 
section describes experimental setup presents analyses performance wow prototype deployment 
section vi presents directions concludes 
ii 
related years related approaches investigated high performance distributed computing commodity computers networks 
berkeley project beowulf early successful efforts direction today commodity clusters high performance computing widely disseminated 
wow shares motivation building systems commodity machines networks case commodity machines virtualized commodity network self configuring ip overlay top internet 
supporting tightly coupled parallel computation cluster network aim wow support computing cross domain collaborations 
project aims study applications programming environments wide area cluster computers 
cluster demand moore propose partition cluster dynamic virtual clusters independent installed software name spaces access controls network storage volumes 
das project built distributed cluster homogeneously configured commodity nodes dutch universities 
related ibis project lets applications span multiple sites grid copes firewalls local ip addresses secure communication tcp bandwidth problems 
efforts large scale distributed computing focused aggregating wide area resources support computing expense requiring applications designed scratch 
legion system designed scale large numbers machines cross administrative domains 
globus provides security infrastructure federated systems supports services resource data information management 
condor highly successful delivering high throughput computing large numbers users 
differs approaches approach providing fully connected widearea cluster environment running unmodified applications 
wow virtual machines networks enables grids run arbitrary applications tied particular application middleware framework sense fundamentally different systems 
virtualization approach preclude systems 
quite contrary virtualization enables run unmodified systems software readily reuse existing mature middleware implementations applicable rapidly integrate techniques 
researchers started applying techniques computational grids wide area clusters 
iamnitchi investigated discovery computational resources grid applications 
cao proposed approach task scheduling computational grids 
related jalapeno organic grid ourgrid projects pursue decentralized computing technology 
system currently applies techniques solve different problem self configuration virtual network links enable efficient easy deploy virtualized clusters 
overlays support legacy applications described context 
approach dht system chord redirect packets potentially mobile hosts nated hosts 
main difference approach providing virtual network layer exactly emulate ip order run existing ip grid software 
contrast introduces new programming apis nodes communicate 
project developed proxies udp tcp traffic 
contrast approach assign virtual ip addresses ip traffic communicated node virtual ip address works exactly ip network 
addition virtual network provided wow completely isolated physical network 
related projects violin vnet vine recognized utility network overlays wide area distributed environments 
approach fundamentally different nodes join leave overlay completely decentralized self organizing fashion 
contrast vnet violin necessary administrators set overlay links centralized entity needed orchestrate network adaptation 
iii 
wow node configuration deployment background motivations commodity machines connected local area networks flexible cost efficient resources run computing parallel applications 
scaling lan grid efforts address important issues arise resources shared virtual organizations 
resource level system configuration heterogeneity difficulty establish connectivity machines due increasing nats firewalls substantially hinders sharing resources 
designed facilitate aggregation resources environment systems different domains different hardware software configurations subject different machine network administration policies 
resources aggregated managed existing unmodified grid middleware scheduling access control allowing site specify usage policies necessary 
virtualization allows isolated flexible efficient multiplexing resources shared distributed infrastructure 
vms native preferred software environment applications instantiated physical host 
vms replicated form virtual clusters enabling instantiation homogeneous execution environments top heterogeneous systems 
vms offer unique opportunities load balancing fault tolerance build growing support checkpointing live migration running vms monitors xen 
complementary vms virtual networking enables isolated multiplexing private networks wit familiar tcp ip environment communication nodes 
vm configuration deployed manner shares common characteristics cluster computing environments configuring deploying wow process system administrators part familiar 
base computing node configured desired execution environment particular linux distribution ancillary libraries system utilities application software 
node independent computer ip address private network 
typical lan cluster physical node disk loaded desired execution environment nodes cluster rack connected hardware switches wow virtual disk configured copied hosts nodes wan interconnected software overlay 
system works vms provide nat virtual network interface vmware xen require allocation physical ip address vm 
software needed vm additional typical cluster environment ipop virtual network 
current implementation ipop requires components mono net runtime environment tap device short tens lines configuration script launch ipop setup vm ip address overlay 
configuration script specifies location ipop node public internet establish connections nodes 
currently overlay deployed planetlab purpose 
deployment usage scenarios goal addition node pool grid resources simple instantiating pre configured vm image easily disseminated file transfer distribution physical media 
envision wow deployments vm appliance configured copied deployed resources facilitating deployment open environments grid computing similar nature efforts open science grid osg 
wow allows participants add resources fully decentralized manner imposes little administrative overhead 
high throughput computing workloads primary target system benefit large number nodes wow provides regardless differences speeds 
loosely coupled parallel applications benefit wow 
architecture designed facilitate setup collaborative environments example support large numbers users execute interactive batch applications 
iv 
wow overlay connection management core wow architecture ability overlay links nodes join distributed system 
section describes adaptive virtual ip networking wow 
builds user level framework provided brunet protocol suite 
extended brunet ipop virtual networking system discover establish maintain direct overlay links frequently communicating nodes 
term connection refer overlay link nodes packets routed 
section describe different connection types supported brunet protocols connection setup 
section iv describe adaptive algorithm support direct structured near structured far multi hop overlay path address ordering 
fig 

peer peer connections brunet 
nodes form structured near connections neighboring nodes ring structured far connections created brunet reduce average number overlay hops 
connection establishment communicating nodes communicate single overlay hop 
refer direct connections shortcut connections 
shortcut connections alleviate issues geography load unaware routing described 
section describe system handles overlay network connections 
particular describe connection linking protocols enable new nodes join network connections formed nodes nats decentralized creation shortcut connections traffic inspection 
description system brunet maintains structured ring nodes ordered bit brunet addresses 
node maintains connections nearest neighbors address space called structured near connections 
new node joins network find right position existing ring form structured near connections nearest neighbors address space 
point onwards node communicate node network vice versa 
node maintains connections distant nodes address space called structured far connections reduces average number overlay hops nodes log network size algorithm 
brunet uses greedy routing packets structured near far connections overlay hop packet gets closer destination address space 
packet eventually delivered destination destination delivered nearest neighbors address space 
connections brunet nodes abstracted may operate transport 
information transport protocol physical endpoint ip address port number contained inside uniform resource indicator uri brunet tcp 
note node may multiple uris multiple network interfaces levels network node node 
sends ctm request 
receives ctm reply initiates linking protocol node node 
sends ctm reply overlay network node node 
initiates linking protocol physical network 
connected shortcut node fig 

shortcut connection setup nodes initiates process connect ctm request send ctm reply overlay initiates linking protocol directly uri list received step 
receiving ctm reply starts linking protocol handshake uri list 
hop connection established linking protocol 
address translation 
encapsulation provided uris provides extensibility new connection types currently implementations tcp udp transports 
connection setup shows steps involved connection setup nodes 
mechanism connection setup nodes consists conveying intent connect resolution addresses uris followed linking handshake summarized follows connection protocol connection protocol nodes convey intent setup connection list uris node 
node wishes connect node network sends connect ctm request message node 
ctm request contains list uris initiating node 
message routed network delivered target node turn sends ctm reply containing uris back source node initiates linking protocol describe 
source node getting ctm reply initiates linking protocol target node 
noted target node prospective connection may case ctm request delivered nearest neighbors target linking 
linking protocol node initiates linking protocol gets ctm request gets reply ctm request sent 
case learned uris node side upcoming connection 
node starts linking handshake sending sequence link request messages target node physical network trying uris learnt 
target node simply responds link reply messages physical network 
exchange sequence linking handshake messages nodes record new connection state subsequently routing packets network 
nodes keep idle connection state alive periodically exchanging ping messages involves resending pings exponential back offs resends 
ping message perceived node going network disconnection node discards connection state 
ping messages incur bandwidth processing overhead nodes restricts number connections node maintain 
noted linking protocol initiated peers leading potential race condition broken favor peer succeeding failing 
node records active linking attempt target sending link request 
current node gets link request target responds link error message stating target give active attempt current node go ahead protocol 
target node gives connection attempt eventually current node succeed 
possible nodes current target initiate active linking get link error messages give attempts case restart exponential backoff reduce likelihood race condition 
joining existing network acquiring connections new node initialized uris nodes network 
new node creates call leaf connection initial nodes directly linking protocol 
case linking unidirectional target passively participates protocol 
initial nodes typically public new node nat discovers records uris corresponding nat assigned ip port 
leaf connection established leaf target ring acts forwarding agent new node 
stage guarantee packet originating arbitrary point network addressed new node delivered 
new node identify correct position ring form structured near connections left right neighbors 
sends ctm request addressed network leaf target 
ctm request routed structured network new node ring eventually delivered nearest neighbors 
ctm replies received forwarding node passed back new node 
node knows uris nearest left right neighbors vice versa form structured near connections 
point node fully routable 
measured time taken new node joining existing network nodes fully routable order seconds see 
node right place ring acquires structured far connections random nodes ring protocols described section iv 
nat traversal description uris earlier follows node inside private network nat multiple uris corresponding private ip port nat assigned ip port communicates nodes public internet 
initially nodes nats know private ip port connection setup public nodes learn nat assigned ip port 
furthermore uris communicate 
uris usable depend locations communicating nodes nature nats 
example nodes nat support hairpin translation communicate uris corresponding private ip port fail nat assigned ip port 
contrast nodes hairpin nats communicate uris private nat assigned 
public node communicate node inside private network uri corresponding nat assigned ip port 
udp transport implementation brunet designed deal nat traversal large class nat devices practical deployments 
bi directionality connection linking protocols enables nat technique succeed 
described earlier node private network nat may multiple uris due firewalls nats nodes uris 
linking protocol nodes try uris time find send receive handshake messages 
linking handshake involves resending exponential back resend interval link requests responded certain interval 
node get responses link requests sends maximum number retries restarts linking handshake uri list eventually succeeds gives adaptive shortcut creation latencies high ms observed ipop nodes connected node planetlab network 
high latencies due multi hop overlay routing highly loaded planetlab nodes 
section describes method decentralized adaptive shortcut creation enables setup single hop overlay links demand traffic inspection 
show section logic choosing random address scope 
currently back factor number retries chosen conservatively brunet account highly loaded nodes environments planetlab lead delays order seconds giving bad uri trying 
shortcuts greatly reduce latency improve bandwidth virtual network improve job throughput grid compute nodes 
brunet library extensible system allows developers add new routing protocols connection types 
brunet connection type node connection ensures node right number connections 
support shortcut connections implemented brunet library 
node tracks communication nodes metric called score 
algorithm queueing system 
number packets arrive th unit time ai 
constant service rate queue 
score amount remaining left virtual queue 
score time si rate service si max si ai higher score destination node communication 
nodes virtual queue longest nodes connect 
establishes maintains shortcut connections nodes scores exceed certain threshold 
conceivable create shortcuts node network large overlays bandwidth processing overhead incurred connection setup maintenance poses practical limit number shortcuts node maintain 
plan investigate models capturing relationship number shortcut links cost associated maintaining processing bandwidth better select score threshold currently constant 
section experimental results compare bandwidth latency virtual network adaptive shortcuts 
measure time required network adapt create shortcuts communicating nodes 
experimental setup experiments deployed virtual cluster vms configured debian linux providing homogeneous software environment cluster 
vms instantiated top highly heterogeneous physical environment consisting hosts running different operating systems linux windows different vm monitors located different network domains subject different firewall policies 
table details configuration various compute nodes testbed illustrated 
nodes planetlab network provide bootstrap overlay running public address internet nodes nodes firewalls connect 
planetlab nodes run ipop software routing tap virtual network interfaces attached pick inject packets table configuration wow testbed depicted 
wow guests run debian linux ode number virtual ip physical domain host cpu host vm monitor vmware node ufl edu xeon ghz linux smp node node ufl edu xeon ghz linux smp gsx node node northwestern edu xeon ghz linux smp gsx node node lsu edu xeon ghz linux gsx node org pentium iii ghz linux node edu xeon ghz linux gsx node gru net pentium ghz windows xp sp host ipop prototype brunet library written run time environment mono 
brunet configured establish udp tcp links tunnel virtual network traffic udp 
exception org firewall single udp port open allow ipop traffic firewall changes needed implemented system administrators 
furthermore wow nodes vmware nat devices require ip address allocated site administrator 
remaining section analyzes performance testbed perspectives 
section brevity sake refer northwestern university site nwu university florida site ufl 
shortcut connections latency bandwidth initial analysis latency throughput ipop links lan wan environments previous 
experiment study process joining ipop node existing overlay network experiment 
ipop node instantiated priori ufl virtual ip address remained fixed experiment 
proceeded iterative process starting ipop node host nwu sending icmp echo packets second intervals terminating ipop node process repeated different virtual ip addresses mapping different locations ring experiments conducted ip address resulting total trials 
experiments conducted scenarios nodes nwu nodes ufl 
summarizes results experiment 
focusing initially ufl nwu case analyzing data initial tens icmp packets shows different regimes see 
icmp requests average packets dropped short period node establish route nodes 
icmp sequence numbers average planetlab router nodes investigate effectiveness approach large overlay network 
deployed overlay routing distributed ipop running compute nodes 
due axis scale chosen initial packets appear plot 
percentage lost packets steadily drops average round trip latency drops ms ms standard deviation ms values indicate period newly joined node routable may established shortcut connection node icmp sequence numbers percentage lost packets drops round trip latencies drop ms standard deviation ms indicating shortcut connection established 
ufl ufl case observe regimes timings differ 
takes icmp ping packets node routable network 
furthermore takes icmp packets shortcut connections formed 
high delay nature ufl nat current implementation ipop linking protocol follows 
ufl nat support hairpin translation discards packets sourced private network destined public ip port 
described section iv linking handshake involves nodes trying target uris finding send receive handshake messages 
ipop nodes attempt uris corresponding nat assigned public ip port linking handshake connection setup 
conservative estimates re send interval back factor number retries udp tunneling nodes take seconds giving uri trying list private ip port succeed 
nwu nwu case nodes inside private network vmware nat network different hosts 
vmware nat supports hairpin translation uris node connection setup 
ufl nwu case linking protocol succeeds uri tries short cut connections setup icmp packets 
evaluated bandwidth achieved wow nodes communicating virtual network provided ipop 
test tcp ttcp utility measure bandwidth achieved transfers large files 
trial experiment observed sharp fall icmp latency soon shortcut connection established 
time elapsed shortcut formed varied length regime different trials averaging trials shows gradual decline latency likelihood packets dropped 
average icmp echo round trip latency ms round trip latencies wow node join ufl ufl ufl ufl ufl nwu nwu nwu ufl nwu nwu nwu icmp sequence number non replied icmp messages dropped packets wow node join ufl ufl ufl nwu nwu nwu ufl ufl ufl nwu nwu nwu icmp sequence number fig 

profiles icmp echo round trip latencies left dropped packets right 
experiment considers combinations location node joining network node communicates ufl ufl ufl nwu nwu nwu 
left plot shows latencies averaged trials reported ping application packets dropped 
right plot shows percentage lost packets trials icmp sequence number reported ping application 
non replied icmp messages dropped packets wow node join ufl nwu icmp packets icmp sequence number fig 

regimes percentage dropped icmp packets observed new node 
new node routable network 
node routable may formed short cut connection node 
new node formed short cut connection node results summarized table ii 
factors routing path limit bandwidth nodes bandwidth overlay links second load machines hosting intermediate ipop routers reduces processing throughput user level implementation 
shortcut connections nodes communicated hop communication path traversing heavily loaded planetlab nodes low bandwidth recorded 
shortcuts enabled nodes communicate single overlay hop achieving higher bandwidth 
virtual machine migration vms provide unique opportunity migrate unmodified applications hosts 
vm migrates carries connection state 
connection state accumulate inside hosts communicating 
forces vm retain network identity turn hampers vm migration subnets 
virtual networking provides opportunity table ii average bandwidth standard deviation measurements wow nodes shortcuts 
experiment considers ttcp transfers files different sizes mb mb mb scenarios location nodes ufl ufl nwu ufl 
shortcuts enabled shortcuts disabled bandwidth std dev bandwidth std dev kb kb kb kb ufl ufl ufl nwu maintaining consistent network identity vm migrates different network 
network virtualization layer wow designed vm migrates subnet connection state virtual network interface continues remain valid 
physical network state involving overlay connections needs invalidated 
case ipop done simple manner simply killing restarting user level ipop program 
ipop node rejoins overlay network autonomously creating structured connections ring process described section iv 
clearly packets get routed dropped node rejoins network short period approximately minutes node network setup 
tcp transport applications resilient temporary network outages experiments show 
ssh scp file transfer experiment set client vm vmware nat interface nwu 
file server located ufl private network 
client vm established ssh scp file transfer session server started download mbyte file 
transfer elapsed transfer time migration initiated ipop process file server killed fig 

profile size mb file transferred ssh scp client migration ssh scp server 
axis shows file size client local disk 
client vm nwu server vm suspended ufl resumed nwu 
file transfer resumes successfully requiring application restarts 
sustained transfer bandwidths migration mb mb respectively 
vm suspended virtual memory disk copy logs transferred host nwu vm resumed 
vm resumed virtual eth network interface restarted nats vm connected different ufl nwu vm acquired new physical address eth destination 
virtual tap interface need restarted remained identity overlay network 
ipop restarted seconds scp server vm routable network established shortcut connection scp client vm eventually scp file transfer resumed point stalled 
summarizes results experiment 
pbs job submission experiment setup environment migration vm executes sequential job scheduled pbs 
pbs head node vm configured ufl private network worker vms configured different hosts ufl private network 
communication nodes performed ipop virtual network 
experiment simulates case applying migration improve load balancing introduced background load vm host observed increase execution time applications executing vm guest migrated vm guest ufl different host nwu 
ipop restarted guest vm resume 
observed job running vm continued problems eventually committed output data nfs mounted home directory account experiment 
runtime job transit migration increased substantially due wan migration delay pbs started submitting jobs vm running unloaded host observed decrease job runtimes respect loaded host 
experiment showed nfs pbs client server implementations tolerant period lack connectivity 
summarizes results experiment 
fig 

profile execution times pbs scheduled meme sequential jobs migration worker node 
job ids run vm ufl 
job ib id vm migrated nwu 
job id impacted wide area migration latency hundreds seconds completes successfully 
subsequent jobs scheduled pbs run successfully migrated vm requiring application reconfiguration restart 
application experiments experiment chose representative life science applications benchmarks meme version version 
applications ran modifications node wow scheduling data transfer parallel programming run time middleware ran unmodified including version pvm version ssh rsh nfs version 
experiments designed benchmark implementation classes target applications wow independent tasks parallel applications high computation communication ratios 
specifically goals experiments show deliver throughput parallel speedups quantify performance improvements due shortcut connections provide qualitative insights deployment stability system realistic environment 
pbs batch application meme application implements algorithm discover motifs collection dna protein sequences 
experiment consider execution large number short running meme sequential jobs approximately queued scheduled pbs 
jobs run set input files arguments submitted frequency job second pbs head node 
jobs read write input output files nfs file system mounted head node 
scenario shortcut connections enabled wall clock time finish jobs average throughput wow jobs minute 
shows detailed analysis distribution job execution times cases wow shortcut connection establishment enabled left disabled right 
differences execution times shown histogram qualitatively explained help table physical machines wow prototype ghz pentium cpus couple nodes noticeably slower noticeably faster nodes 
slower nodes running substantially smaller number jobs frequency occurrence execution time histogram pbs meme shortcuts enabled wall clock time frequency occurrence execution time histogram pbs meme shortcuts disabled wall clock time fig 

distributions pbs meme job wall clock times overlay network self organizing shortcuts enabled left disabled right 
wall clock time average standard deviation shortcuts enabled shortcuts disabled 
table iii execution times speedups execution pvm nodes wow 
parallel speedups reported respect execution time node 
sequential execution parallel execution node node nodes nodes shortcuts enabled shortcuts disabled shortcuts enabled execution time seconds parallel speedup respect node fastest nodes node runs jobs node runs 
shows shortcut connections decreases average relative standard deviation job execution times 
shortcuts reduced queuing delays pbs head node resulted substantial throughput improvement jobs minute shortcut connections jobs minute shortcut connections 
note throughput achieved system depends performance overlay performance scheduling data transfer software runs pbs nfs 
choice different middleware implementations running inside wow condor globus lead different throughput values comprehensive analysis comparison performance scope 
vm impacts performance 
meme application observed average virtual physical overhead 
pvm parallel application program maximum likelihood inference phylogenetic trees dna sequence data 
parallel implementation pvm master workers model master maintains task pool dispatches tasks workers dynamically 
high computation ratio due dynamic nature task dispatching tolerates performance heterogeneities computing nodes 
table iii summarizes results experiment 
taxa input dataset reported 
parallel execution wow reduces significantly execution time 
fastest execution achieved nodes wow shortcut connections enabled faster nodes shortcuts enabled faster node execution 
computation ratio task shortcuts resulted substantial performance improvements 
profiled time spent application execution time increase execution times explained fact application needs synchronize times execution select best tree round tree optimization 
sequential execution times reported different nodes node node show differences hardware configuration individual nodes wow result substantial performance differences 
modeling parallel speedups heterogeneous environment difficult report speedups respect node hardware setup common network 
parallel speedup computed assumption comparison speedup reported approximately achieved homogeneous ibm rs sp cluster lan 
discussion prototype wow setup deployed month 
discuss qualitative insights obtained practical usage system 
deployment wow greatly facilitated packaging software vm requiring nat network availability free vm monitors notably 
base vm image created replicating instantiating new cluster nodes quite simple 
nat interface convenient believe host interface improve isolation wow nodes physical network working implementation supports interface 
overlay network running deployed experiments quite stable 
occasion restart entire overlay network despite fact physical nodes shut restarted period time 
overlay network resilient changes nat ip port translations happen behavior noticed vm hosted broadband connected home desktop 
ipop dealt translation changes autonomously detecting broken links re establishing connection techniques discussed section iv 
vi 
functionality enabled powerful experimental results show viable technology high throughput computing applications 
aspects system call better performance flexibility focus going research 
terms performance virtual network overhead relatively large local area cluster network 
significant component overhead due multiple user kernel copies involved handling virtual network traffic 
solutions support shared user kernel buffers wow vms requiring host kernel changes high speed remote dma network interfaces tunnel overlay traffic reduce amount user kernel copying provide better lan performance 
goal support substantially larger wow setups performance evaluations 
overlay ip routing infrastructure wow algorithms designed scale large systems 
middleware runs wow tasks scheduling distributed file systems client server models may scale large numbers 
plan investigate approaches decentralized resource discovery scheduling data management suitable large scale systems 
believe provide powerful flexible infrastructure allows novel evolutionary revolutionary middleware approaches deployed side side existing systems 
vii 
acknowledgments effort sponsored nsf eia eec aci ani sci sci carried component coastal ocean observing prediction program initiative southeastern universities research association 
funding support provided office naval research award noaa ocean service award na nos 
authors acknowledge sur ibm 
opinions findings recommendations expressed material authors necessarily reflect views sponsors 
authors peter dinda justin davis vladimir david forrest steve thorpe providing access host resources 
distributed net 
distributed net 
home 
edu 
open science grid web site 
www org 
web site 
www org 
chawla figueiredo fortes virtualized resources virtual computing grids vigo system 
generation computing systems special issue complex problem solving environments grid computing apr 
teahan 
decentralized peer topeer computing system 
proc 
rd intl 
symp 
algorithms models tools parallel computing heterogenous networks jul 
anderson cobb 
seti home experiment public resource computing 
communications acm 
anderson culler patterson case network workstations 
ieee micro february 
andrade costa cirne 
peer peer grid computing ourgrid community 
proc 
rd brazilian symp 
computer networks may 
bal 
efficient dynamic communication system heterogenous grids 
proc 
ccgrid cardiff uk may 
bailey elkan 
fitting mixture model expectation maximization discover motifs biopolymers 
proc 
second intl 
conference intelligent systems molecular biology pages menlo park ca 
aaai press 
barham fraser hand xen art virtualization 
proc 
th acm symposium operating systems principles pages bolton landing ny 
becker sterling savarese 
beowulf parallel workstation scientific computation 
proc 
intl 
conference parallel processing icpp 
kong 
brunet software library brunet ee ucla edu brunet 
calder chien wang yang 
entropia virtual machine desktop grids 
cse technical report cs university california san diego san diego ca oct 
cao kwong wang cai 
peer peer approach task scheduling computation grid 
intl 
journal grid utility computing 
baumgartner 
organic grid self organizing computation peer peer network 
ieee transactions systems man cybernetics may 
chase irwin moore 
dynamic virtual clusters grid site manager 
proc 
th intl 
symp 
high performance distributed computing seattle wa jun 
gupta 
peer peer discovery computational resources grid applications 
proc 
th ieee acm workshop grid computing seattle wa november 
chun culler roscoe peterson bowman 
planetlab overlay testbed broad coverage services 
acm sigcomm computer communication review 
clark fraser hand hansen jul pratt warfield 
live migration virtual machines 
proc 
nd symp 
networked systems design implementation nsdi boston ma may 
denis bal 
wide area communication grids integrated solution connectivity performance security problems 
proc 
th intl 
symp 
high performance distributed computing honolulu hawaii jun 
bal distributed asci supercomputer project 
acm special interest group operating systems review vol oct 
figueiredo dinda fortes 
case grid computing virtual machines 
proc 
rd ieee intl 
conference distributed computing systems icdcs providence rhode island may 
ford 
peer peer communication network address translators 
proc 
usenix annual technical conference usenix anaheim california apr 
fortes figueiredo 
virtual computing infrastructures simulation 
proc 
ieee aug 
foster iamnitchi 
death taxes convergence peer peer grid computing 
proc 
nd intl 
workshop peer peer systems iptps berkeley ca feb 
foster kesselman 
globus metacomputing infrastructure toolkit 
intl 
journal supercomputer applications 
ganguly agrawal figueiredo 
ip enabling self configuring virtual ip networks grid computing 
appear proc 
ieee intl 
parallel distributed processing symp 
ipdps rhodes greece jun 
grimshaw wulf 
legion flexible support widearea computing 
proc 
th acm sigops european workshop ireland 
guha takeda francis 
sip approach udp tcp connectivity 
proc 
special interest group data communications sigcomm workshops portland pages aug 
iamnitchi foster 
peer peer approach resource location grid environments 
proc 
th symp 
high performance distributed computing edinburgh uk aug 
jiang xu 
violin virtual internetworking overlay infrastructure 
proc 
nd intl 
symp 
parallel distributed processing applications dec 
kannan 
supporting legacy applications 
computer science division university california berkeley jun 
technical report 
ucb csd 
keahey foster freeman zhang 
virtual workspaces grid 
proc 
europar lisbon portugal sep 
bal maassen van jacobs 
project parallel application support computational grids 
proc 
st european grid forum workshop pages poznan poland apr 
kleinberg 
nature 
satyanarayanan 
internet suspend resume 
th ieee workshop mobile computing systems applications wmcsa june ny usa pages 
ieee computer society 
litzkow livny mutka 
condor hunter idle workstations 
proc 
th ieee intl 
conference distributed computing systems icdcs jun 
lo zappala zhou liu zhao 
cluster computing fly scheduling idle cycles internet 
proc 
third intl 
workshop peer peer systems iptps san diego ca feb 
olsen overbeek 
tool construction trees dna sequences maximum likelihood 
comput 
appl 
biosci 
raman livny solomon 
matchmaking distributed resource management high throughput computing 
proc 
th ieee intl 
symp 
high performance distributed computing hpdc chicago il jul 
rosenberg weinberger huitema 
rfc simple traversal user data protocol protocol network address translators 
www ietf org rfc rfc txt 
arpaci dusseau livny 
deploying virtual machines grid 
proc second usenix workshop real large distributed systems worlds pages san francisco ca dec 
chandra pfaff chow lam rosenblum 
optimizing migration virtual computers 
proc 
usenix operating system design implementation osdi 
smith nair 
virtual machines versatile platforms systems processes 
morgan kaufmann 
son allcock livny 
firewall traversal cooperative demand opening 
proc 
th intl 
symp 
high performance distributed computing hpdc 
stewart hart berry olsen fischer 
parallel implementation performance program maximum likelihood phylogenetic inference 
proc 
ieee acm supercomputing conference sc 
stoica zhuang shenker surana 
internet indirection infrastructure 
ieee acm transactions networking vol apr stoica morris liben nowell karger kaashoek dabek balakrishnan 
chord scalable peer peer lookup protocol internet applications 
ieee acm trans 
netw 
lim 
virtualizing devices vmware workstation hosted virtual machine monitor 
proc 
usenix annual technical conference jun 
dinda 
virtual networks virtual machine grid computing 

rd usenix virtual machine research technology symp san jose ca may 
gupta dinda 
dynamic topology adaptation virtual networks virtual machines 
proc 
seventh workshop languages compilers run time support scalable systems lcr oct 
sunderam dongarra geist manchek 
pvm concurrent computing system evolution experiences trends 
parallel computing apr 
tannenbaum wright miller livny 
beowulf cluster computing linux chapter condor distributed job scheduler 
mit press 
bengtsson 
jalapeno decentralized grid computing peer peer technology 
proc 
nd conference computing frontiers italy 
fortes 
virtual network vine architecture grid computing 
appear proc 
ieee intl 
parallel distributed processing symp 
ipdps rhodes greece jun 
zhang keahey foster freeman 
virtual cluster workspaces grid applications 
technical report anl mcs argonne national laboratories apr 
