optimizing result prefetching web search engines segmented indices extended study process search engines segmented indices serve queries 
particular investigate number result pages search engines prepare query processing phase 
search engine users observed browse pages results queries submit 
behavior users suggests prefetching results processing initial query efficient prefetched results requested user initiated search 
policy abandons result prefetching favor retrieving just page search results optimal system resources 
argue certain behavior users engines prefetch constant number result pages query 
define concrete query processing model search engines segmented indices analyze cost prefetching policies 
costs show determine constant optimizes prefetching policy 
results applicable local index partitions inverted files applicable processing short queries global index architectures 
permission copy fee part material granted provided copies distributed direct commercial advantage vldb copyright notice title publication date appear notice copying permission large data base endowment 
copy republish requires fee special permission endowment 
proceedings th vldb conference hong kong china ronny lempel shlomo moran department computer science technion haifa israel email moran cs technion ac il sheer size www efforts search engines index significant portions caused search engines partition inverted index web disjoint segments partial indices 
partitioning index impacts manner engines process queries 
engines form query result caching results queries served cached time 
particular query results may prefetched anticipation user requests 
scenarios occurs engine retrieves certain query results initially returned user 
examine efficient prefetching policies search engines 
policies depend architecture search engine turn affects query processing scheme behavior patterns search engine users 
search engine users users submit queries search engines 
user point view engine answers query linked set ranked result pages typically results page 
users browse page results results deemed engine ranking scheme relevant query scan additional result pages usually natural order pages 
studies analyzed manner users query search engines view result pages study jansen queries submitted search engine excite study markatos queries submitted excite study silverstein queries submitted search engine altavista findings studies share particularly relevant www excite com www altavista com queries submitted www search engines short averaging terms query half queries containing just terms 
results reported 
studies define query terms somewhat differently reported term counts may loosely interpreted number words query 
users browse result pages 
mentioned studies differ reported distribution page views agree users view page top results users browse result pages 
number distinct information needs users large seen huge variety queries submitted search engines 
popular queries repeated times popular queries account queries submitted engines 
caching prefetching search results commonly believed major search engines perform sort search result caching prefetching 
caching results noted brin page description prototype search engine google important optimization technique search engines 
markatos demonstrated caching search results lead hit ratios close 
addition storing results requested users cache search engines may prefetch results predict requested shortly 
immediate example prefetching second page results new query submitted user 
studies indicate second page results requested shortly new query submitted cases search engines may prepare cache result pages query 
index structure query processing models inverted indices inverted lists files regarded widely applied indexing technique believed major search engines :10.1.1.111.6826
search engines index hundreds millions web pages size inverted indices measured terabytes 
ribeiro neto barbosa mention hardware configurations handle large digital libraries powerful central machine parallel machine high speed network machines workstations high desktops 
considering size indices search engines main www google com tain growth rate web large number queries search engines answer day network machines considered cost effective scalable architecture 
networks operate shared memory organization machine processing power cpus memory secondary storage 
machines communicate passing messages high speed network connects 
studied schemes partitioning inverted index machines global index organization 
scheme inverted index partitioned terms 
machine holds posting lists distinct set terms terms may partitioned lexicographic order example 
posting list term holds entries documents include local index organization 
scheme inverted index partitioned documents 
machine responsible indexing distinct set documents hold posting lists terms appeared set documents 
works ribeiro neto barbosa tomasic garcia molina compared run time efficiency schemes 
parallel generation global index studied system crawls web builds distributed local index 
evaluated computational performance local indices variety workloads hawking examined scalability issues local index organizations 
prototype google reported global index partitioning 
mentioned works describe essentially model processing queries systems segmented indices user queries arrive certain designated machine call query integrator qi 
machine called home site central broker user interface connection server 
qi issues query separate index segments manner depends partitioning scheme index 
local index partitioning qi send query submitted user segments 
global index partitioning qi sends segment partial query consisting set terms posting lists stored segment 
qi waits relevant segments return result sets merges result sets respect system ranking scheme 
index partitioning schemes imply different merge operations 
local index partitioning usually assumed segment ability calculate global score document local index respect queries 
result sets returned different segments disjoint merging various result sets straightforward 
global index partitioning relevant segment returns ranked document list may overlap lists returned segments score reflects score document respect partial query segment received 
qi may need perform set operations partial result sets queries containing boolean operators need weigh scores returned segment differently example different idf values terms partial query 
qi returns merged results users 
consider cache augmented process qi maintains query result cache 
receiving query user qi checks cache contains results query 
cached results returned user forwarding query segments 
query answered cache qi processes query described completion caches merged results considering query processing model described context web search engines note merged results returned users small batches typically time decreasing order relevance ranked search engine 
qi may prepare results populate batch cache 
raises issue optimizing number prefetched results systems cost processing uncached queries increases number results fetched prefetching large number results query costly may pay user request additional batches results cached 
note cost prefetching associate cache space occupied prefetched results 
assuming fixed size cache increasing number prefetched results query may decrease number queries results simultaneously cached 
maintenance cache considered 
particular examine cached entries replaced freshness results maintained 
may lead lower cache hit ratios increase load engine 
issue arising query processing model relationship number results qi decides prefetch query number results ask segment 
example consider engine uses local partitioning segments policy prefetch results query 
top results denoted qi collect segment query 
may happen top results reside particular segment 
order certain top results obtained necessary collect top results segment setting 
assuming documents partitioned randomly independently segments qi may able collect considerably results segment high probability obtain top results 
optimizing number prefetched results behavior respect considered 
tradeoff amount cost result prefetching possibility serving subsequent queries cache main topic 
popular search engines process millions queries day efficient prefetching policies help reduce hardware requirements response time engines 
rest organized follows 
section formally presents problems studied notations notations summarized table 
model search engine query service process users behavior 
define cost prefetching number results terms cost function analyzed optimized sections 
section presents algorithm optimizes prefetch cost function special cases 
case deals inverted indices fit single machine 
single machine scenario models serving queries quite common web globally partitioned index 
second special case deals scenario engine guarantees users receive absolutely optimal results worst case assumptions distribution relevant documents local index partitions 
main body contained section presents algorithms solve approximately solve optimization problem locally partitioned indices arbitrary number segments documents randomly distributed 
sections tackles combinatorial problem setting number results retrieved segment order provide quality merged results users 
section discusses practical impact results may search engine engineering 
conclu sions suggestions research brought section 
notations formal model user requires model way search engine users view result pages searches 
studies reported aspects user behavior examining query logs search engines 
purposes analysis altavista log report sufficient detail exact distribution result pages views citing percentages users viewing pages 
addition statistics reported considered requests additional results arrived minutes previous request user 
study excite users brings elaborate distribution result page views query 
page views result page views second result page views result pages views conformed geometric distribution parameter 
chose model number result pages users view query geometric random variable 
model users view result pages natural order probability user viewing exactly result pages 
viewing result pages equals words viewing result page user requests page probability important property geometric distribution fact memoryless assume complexity retrieving ranked results memoryless meaning complexity retrieving results rank places 
depends number results retrieved see assumption holds identity result ranks place known 
memoryless behavior users memoryless cost retrieval implies optimal number result pages ropt prefetched query independent number result pages requested far time query served cache qi prepare ropt result pages 
index architecture complexity processing queries model refer local index partitioning scheme network 
index partitioned segments 
assume documents urls partitioned segments random process assigns document segment uniform distribution independently documents 
partitioning achieved hashing url fixed size document id mapping ids segments 
scheme mentioned context building url repositories technique applied assigning pages segments inverted index 
number documents considered hundreds millions considerably smaller square root number documents segments contain roughly number documents high probability 
query processing model described section 
discussion consider processing broad topic query matches documents segment larger number results user browse 
denote number results engine presents result page typical value 
results prefetched page units number prefetched results query multiple follows examine cost prefetching ra results query subsequent sections able optimize value number prefetched result pages 
denote user query pair search topic ordinal number result page requested 
query start search new topic ask additional results existing search 
discussion addresses query types 
preliminaries receiving query answered cache qi needs fetch results task set value number results retrieve segments 
denote set documents engine ideally retrieve query documents attain best scores engine ranking function documents retrieved queries denote set documents retrieved query segments returns relevant previously matches ideally contain ensuring means setting equal assume engine employs quality policy probability qi sets value respect special case discussed section 
words qi collect previously quality results segment probability top retrieved results best previously results entire index 
relationship studied section 
time suffices note assumption documents uniformly distributed segments probability depends values independent topic lq denote minimal number documents retrieved segments quality criterion satisfied lq min collecting results qi sends segment topic request lq top results query 
batch results retrieved segment receives si score lowest ranking document contributed results 

estimate cost serving requests 
assumption query matches documents segment larger number results users browse consequently larger lq lq bounded bounded number results users browse 
assume identifying sized set candidate documents done time linear assumption holds inverted index structure number query terms small case broad topic queries web see section 
recall segment receives score lowest ranking document retrieved far query discard previously retrieved results set candidates 
top scoring lq documents remaining candidates 
segment spend lq log processing steps query order return lq sorted results qi 
merging results qi receives sorted result sets length lq 
reading buffering sets takes lq operations 
partially merges results identifies top ra retrieved results populate result pages 
tree selection sorting sorted result lists hanging leaves tree merge accomplished time log 
complexity step lq log 
assume results query cached query arrives 
caching results result pages cached pages returned user 
scores 
sm noted 
space complexity ra 
complexity query processing model model requires messages passed qi segments qi sends query segment segment returns lq results qi 
total number results received qi lq amount data impacts time complexity 
allowed rounds communication managed sending qi results lowering complexity merge step log 
chose minimizing communication rounds machines expense sending larger messages improve performance distributed computations 
note complexity retrieval model described memoryless see discussion section 
model implies computational loads various resources engine policy prefetching result pages query qi performs lq ra ra log computation steps 
index segment performs lq ra log computations 
cache space required ra 
additionally introduce non negative coefficients allow assign different weights resources consumed query processing 
specifically multiply computations qi multiply cache space required tuning values emphasize memory cache limitations computational bottlenecks qi vs segments response time query 
section 
ready formulate expected cost policy prefetches pages geometric users parameter result pages ir ir 
termed th batch result pages 
ease notation introduce lq lq ra 
computational loads expressed notation 
concreteness simplicity consider expressions exact complexities 
allows avoid tedious notations affect ensuing analysis results 
caching overhead preparing batch batch preparation complexity ar ir lq log ra log ar lq log pr ra log pr rearranging terms ignoring constant additive term depend affect optimization get ar log lq log ease notation define constants log log notation ar clq dr number documents segment match query typically large number typically smaller 
proportionality constants typical values large tens thousands relatively small typically 
mission way locally segmented index geometric users quality criterion determine ropt integral value minimizes 
doing determine lq ropt 
qi prepare ropt result pages query answered cache asking segments retrieve top lq ropt results score certain threshold query processed 
strive obtain exact exact values ropt lq ropt 
simple special cases section show problem single segment problem multiple segments behave similarly cases ropt log ropt steps 
index stored single segment ignore terms complexity function deal merging results symbol denotes shorthand shorthand shorthand log shorthand log number segments index probability viewing result page viewing page quality criterion qi number result pages fetch ropt optimal integral value number results result page number relevant results segment required fetching result pages query lq number results fetch segment best ra results collected probability equals lq ra multiplies computations qi multiplies required caching space table summary notations different segments terms involving 
addition lq ra regardless value 
ar log note index partitioned globally segment holds posting lists distinct set terms single term queries effectively queries single segment described 
studies indicate percentage single term queries web quite large 
case ra complexity function takes form ar ca cases imply complexity function form ar derivative negative zero increases 
positive values decreases reaching unique minimal value increases 
relying behavior optimal integral value ropt applying procedure 
find minimal natural number 

find optimal value binary search range 
exceed log ropt complexity finding ropt log ropt 
solution way segmented local index section study problem setting optimal value quality criterion engine architecture parameters complexity parameters 
subsection presents algorithm determining optimal value minimizes retrieval complexity function 
subsection presents approximation algorithm finds value approximately optimal 
optimizing indices segments recall complexity function section ar clq dr clearly behavior depends behavior lq 
show precisely calculate lq section purpose subsection suffices note lq lq 
order facilitate search ropt set forth find value upper bound set definition function called restrictive 
example restrictive ar ag 
consequently ropt larger 
restrictive functions bound search space ropt 
seek restrictive function better providing tighter bounds size search space 
proposition proved full version proposition function restrictive 
pr clq dr note function reflects architectural parameters search engine index user behavior represented desired quality criterion displays algorithm op setting optimal value steps calculation lq trivial calculation topic section 
correctness algorithm follows restrictiveness proposition need iterate values known higher values seen 
set finite 
initializations wmin ropt limit 
limit calculate lq value set 
limit limit 
wmin wmin ropt 

print ropt 
algorithm op optimizing prefetch policy complexity algorithm algorithm op needs executed relatively times configuring prefetching policy search engine see discussion section 
complexity impact performance engine 
prove running time polynomial 
bounding rmax maximal number iterations op may require course 
ca note assumptions relative values see section small constant 
lq bound number iterations 
rmax bounded log log 
bound rmax 
log lemma rmax log proof log log 
pr pr clq dr clq dr cra dr obviously lq ra log log complete analysis complexity algorithm op finding ropt show section calculating values lq 
rmax requires max steps regardless value 
bounded rmax simple functions bounds complexity algorithm follow 
table brings sample results algorithm 
combination ropt act max highest value calculated execution shown 
plots function calculated algorithm values 
displayed results table ropt act max values function approximating optimal solution previous subsection shown determine ropt number pages minimizes complexity function 
willing settle nearly optimal solutions finding ropt values small values algorithm 
rmax log log 
find value range 
rmax minimizes 
note rmax depends user behavior modeled independent engine architecture quality policy modeled 
furthermore algorithm applicable function increasing function note satisfies condition ar clq dr positive constants functions lq nondecreasing functions correctness approximation algorithm relies proposition 
proposition positive function pr increasing function pt 
proof corollary proof increases get corollary pt ps min 
ropt proof ropt claim holds 
result implied corollary ropt 
log substituting rmax log corollary yields approximation algorithm min 
ropt log log ropt ropt log log log table shows values rmax 

mentioned earlier calculating values lq 
rmax requires max compu tational steps time complexity approximation algorithm log log 
note results subsection may practice improve running time algorithm op checking steps wmin set ting limit terminating algorithm 
proposition asserts iterations larger values result greater values op safely terminate output current value ropt 
calculating lq section brings recursive formulae lq calculated time polynomial model distribution top results segments random process ra different balls top results query thrown randomly independently different cells segments ni balls inserted cell ni 
model querying process min ni balls cell 
denote en number excess balls remain cells querying process completed 
section calculate probability en 
corresponds case cell function contains balls qi managed collect top results segments 
full version calculate probability en corresponds case qi managed collect just top results 
study case qi may choose employ relaxed quality policy requiring high probability necessarily top results returned user 
subsection briefly reviews previous related issues 
rough bound lq may suffice precise calculations essen tial 
clearly lower bound lq 
show lq need larger max log en probability exactly results inserted segment ne ne ne mi sharper known asymptotic bounds lq discussed section 
table rmax function probability results inserted segment bounded ne mi ne ne max log en sion bounded ne ne expres log union bound probability segments contains results smaller results follow 
precise calculation lq turn precise calculation lq 
calculate probability en probability throwing different balls different cells cell contains balls 
size problem space counting number ways throw different balls different cells cell contains balls table lq various values recursive formulae may calculate values recursion naturally fits algorithm op section jl 
jl choose cells exactly balls 
choose balls populate cells multinomial coefficient terms 
remaining jl balls distributed remaining cells cell collecting balls 
grows subsequent iterations op value lq 
recursion naturally uses results previous iterations iterations 
initial values 




denoting nmax mod value iteration op lmax value iteration total time spent calculating values lq lmax 
table shows sample values lq 
previous stochastic properties process randomly throws balls cells studied extensively 

properties studied distribution maximum number balls cell denote 
example ln balls cells ln ln probability 
ln asymptotically ln ln probability 
distribution examined regard behavior ratio ln 
separate results obtained cases ln ln ln 
shown distribution may approximated distribution max sj sj sj independent variable degrees freedom 
theory practice section attempts bridge gap theory practice highlighting possible practical implications model results 
complexity function revisit assumptions formalizing section 
assumptions pertain manner users view result pages memoryless query processing scheme 

users view search result pages memoryless geometric process 
assumption extremely simplistic studies cited section indicate reasonably approximate aggregate behavior users 

request result page arrives result page cached 
assumption send segment score lowest result contributed page 
turn allowed formulate memoryless query processing scheme 
ignoring cache management issues consideration justifies intuition assumption aim policy prefetches pages numbered 
processing request result page query rapidly answer cache subsequent requests pages 
query 
prefetching policy implicitly assumes life expectancy cached entries allow page cached requested 
words policy prefetches pages assumes pages cached long subsequent requests 
require pages cached subsequent requests 
assumptions allowed formulate exact complexity function concrete query processing model 
section complexity function abbreviated form ar clq dr claim abbreviated form results accommodate retrieval model incurs costs prefetching pages cache space linear number prefetched result pages 
retrieval complexity sum term depends query breadth number matching results term linear lq term linear results may apply index structures query processing schemes differ model 
furthermore results section apply complexity function increasing function results section determined number results retrieved segment lq applicable search engine uses locally segmented index documents partitioned uniformly independently 
implementing prefetching policy implementing prefetching policy engines locally segmented indices framework research requires preprocessing steps setting parameters approximate value derived analyzing engine query logs parameter set quality policy values set engine resources 
systems small caches set high value qi heavily loaded set high value range query range values parameter algorithm optimizing approximating ropt executed 
qi segment loaded tables containing values ropt lq ropt values range 
receiving query segment estimates query breadth value ct corresponds 
done ways local index implementations incorporate global term statistics segment order facilitate term scoring 
statistics may help estimating breadth certain types queries 
assumption segment contain approximately number results broad topic queries 
segment process query number matches finds estimate estimating ct segment forwards lq ropt ct results qi merges retrieved results produce ropt ct result pages 
examined search engines prefetch search results user queries 
started presenting concrete query processing model search engines locally segmented inverted indices 
argued model assumes number result pages users view distributed geometrically optimal engine policy prefetch constant number result pages expressed computational cost policy prefetches pages suggested algorithm finding optimal value minimizes expected cost 
suggested find values imply policies cost approximately optimal 
extensions model ignores overlaps information needs different users 
consider example popular queries may submitted multiple users short time span increasing probability user requesting additional results 
query popularity account may find popular queries warrant result prefetching rare queries 
address cache replacement policies particular suggest result pages removed cache prefetching results new query 
noted context buffering posting lists knowledge access patterns query cache considered setting replacement policy 
example users usually browse result pages natural order 
assuming result pages query cached evicted natural remove second page results lru policy suggest 
results applicable locally segmented indices 
single term queries global indices considered 
additional research required order extend results multi term queries global indices 
acknowledgments andrei broder maghoul altavista useful discussions insights problems covered 
arasu cho garcia molina paepcke raghavan 
searching web 
acm transactions internet technology 
yossi azar andrei broder anna karlin eli upfal 
balanced allocations 
siam journal computing 
sergey brin lawrence page 
anatomy large scale hypertextual web search engine 
proc 
th international www conference 
kathryn mckinley lu 
evaluating performance distributed architectures information retrieval variety workloads 
acm transactions information systems 
volker 
randomized allocations processes 
proc 
th ieee symposium foundations computer science pages 
david hawking 
scalable text retrieval large digital libraries 
european conference digital libraries 
bernard jansen amanda spink saracevic 
real life real users real needs study analysis user queries web 
information processing management 
soo jeong edward omiecinski 
inverted file partitioning schemes multiple disk systems 
ieee transactions parallel distributed systems 
johnson young 
applications approximations multinomial distribution 
biometrika 
norman johnson samuel kotz 
urn models application 
john wiley sons 
andrei broder currently ibm research bj rn th nsson michael franklin divesh srivastava 
interaction query evaluation buffer management information retrieval 
sigmod proceedings acm sig mod international conference management data seattle washington usa pages june 
donald knuth 
art computer programming volume 
addison wesley publishing 
valentin boris vladimir 
random allocations 
winston sons 
steve lawrence lee giles 
searching world wide web 
science april 
evangelos markatos 
caching search engine query results 
proceedings th international web caching content delivery workshop may 
sergey melnik sriram raghavan beverly yang hector garcia molina 
building distributed full text index web 
proc 
th international www conference 
ribeiro neto barbosa 
query performance tightly coupled distributed digital libraries 
proc 
acm digital libraries conference pages 
ribeiro neto navarro ana ziviani 
parallel generation inverted files distributed text collections 
proc 
th international conference computer science society 
craig silverstein monika henzinger hannes marais michael 
analysis large altavista query log 
technical report compaq systems research center october 
tomasic garcia molina 
performance inverted indices shared distributed text document information retrieval systems 
proc 
second international conference parallel distributed information systems pages 
