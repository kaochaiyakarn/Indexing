learning structured prediction models large margin approach ben taskar taskar cs berkeley edu computer science uc berkeley berkeley ca vasco cs stanford edu daphne koller koller cs stanford edu computer science stanford university stanford ca carlos guestrin guestrin cs cmu edu computer science carnegie mellon university pittsburgh pa consider large margin estimation broad range prediction models inference involves solving combinatorial optimization problems example weighted matchings 
goal learn parameters inference model reproduces correct answers training data 
method relies expressive power convex optimization problems compactly capture inference solution optimality structured prediction models 
directly embedding structure learning formulation produces concise convex problems efficient estimation complex diverse models 
describe experimental results matching task connectivity prediction showing significant improvements state art methods 

structured prediction problems arise tasks multiple interrelated decisions weighed arrive globally satisfactory consistent solution 
natural language processing need construct global coherent analysis sentence corresponding part ofspeech sequence parse tree translation language 
computational biology analyze genetic sequences predict structure proteins find global alignment related dna strings recognize functional portions genome 
computer vision segment complex objects cluttered scenes appearing proceedings nd international conference machine learning bonn germany 
copyright author owner 
reconstruct shapes stereo video track motion articulated bodies 
prediction tasks modeled combinatorial optimization problems example alignment shapes weighted bipartite point matching belongie connectivity prediction weighted non bipartite matchings baldi clustering spanning trees graph cuts duda combinatorial graph structures 
define structured model broadly scoring scheme set combinatorial structures method finding highest scoring structure 
score model function weights vertices edges parts structure weights represented parametric functions set input features 
focus task learning parametric scoring function 
training data consists instances labeled desired combinatorial structure matching cut tree set input features parameterize scoring function 
informally goal find parameters scoring function highest scoring structures close possible desired structures training instances 
lines maximum margin estimation probabilistic models collins altun taskar discriminative estimation framework structured models large margin principle underlying support vector machines 
criterion provides alternative probabilistic likelihood estimation methods concentrating directly robustness decision boundary model 
framework defines suite efficient learning algorithms rely expressive power convex optimization problems compactly capture inference solution optimality structured learning structured prediction models large margin approach models 
extensive experiments connectivity protein structure prediction showing superior performance state art methods 

structured models particularly simple relevant example consider modeling task assigning reviewers papers maximum weight bipartite matching problem weights represent expertise reviewer 
specifically suppose reviewers reviewer assigned papers 
reviewer score sjk indicating qualification level reviewer evaluating objective find assignment reviewers papers maximizes total weight 
represent matching set binary variables yjk set reviewer assigned 
score assignment sum edge scores jk 
define set bipartite matchings number papers reviewers maximum weight bipartite matching problem arg maxy solved combinatorial algorithm linear program max jk yjk yjk yjk 
lp guaranteed integral solutions long integers scoring function schrijver 
quality assignment depends critically choice weights define objective 
simple scheme measure expertise percent word overlap reviewer home page 
want weight certain words heavily words relevant subject infrequent 
constructing tuning weights problem difficult time consuming process perform hand 
webpage denote bag words occurring home page reviewer denote bag words occurring xjk denote intersection bag words occurring webpage 
score sjk simply sjk wd xjk weighted combination overlapping words indicator function 
define fd xjk xjk fd jk yjk xjk number times word web page reviewer matched represent objective weighted combination set features set parameters set features 
develop large margin framework learning parameters model training data example reviewer assignments previous years 
general consider prediction problems input arbitrary structured object output vector values 
example matching cut graph 
assume length lx structure depend deterministically input bipartite matching example output space defined number papers reviewers denote output space input entire output space 
assume define output space structured example set constraint functions gd ir 
class structured prediction models consider linear family hw arg max vector functions ir formulation general clearly pairs finding optimal intractable 
focus attention models optimization problem solved polynomial time 
problems include probabilistic models certain types markov networks context free grammars combinatorial optimization problems min cut matching convex optimization problems linear quadratic semi definite programming 
intractable cases certain types matching graph cuts approximate polynomial time optimization procedure provides upper lower bounds solution 

max margin estimation input consists set training instances instance consists structured object graph target solution matching 
develop methods finding parameters arg max 
note solution space depends structured object example space possible matchings depends precise set nodes edges graph parameters learning structured prediction models large margin approach describe general approaches solving problem apply bipartite matchings 
approaches define convex optimization problem finding parameters formulation provides effective exact polynomial time algorithm variants problem 
reduction task standard convex optimization problem allows highly optimized shelf software 
framework extends max margin formulation markov networks taskar taskar context free grammars taskar similar formulations altun tsochantaridis 
univariate prediction measure error prediction loss function 
structured problems jointly predicting multiple variables loss just simple loss 
structured prediction natural loss function kind hamming distance number variables predicted incorrectly 
express requirement true structure optimal solution respect instance min fi fi fi 
interpret fi fi margin constraints enforce fi fi minimizing maximizes smallest margin scaled loss 
assume simplicity features rich satisfy constraints analogous separable case formulation svms 
add slack variables deal non separable case omit details lack space 
problem eq 
linear constraints generally exponential li number variables yi 
equivalent formulations avoid exponential blow important structured models 

min max formulation rewrite eq 
substituting single max constraint linear ones min fi max fi formulation convex quadratic program max fi convex maximum affine functions convex function 
key solving eq 
efficiently inference max fi 
optimization problem precisely form prediction problem parameters trying learn max fi additional term corresponding loss function 
max fi solved polynomial time convex optimization tractability inference depends form loss term 
assume loss function decomposes variables natural example loss function hamming distance simply counts number variables candidate solution differs target output assume reformulate loss augmented inference convex optimization problem terms set variables objective fi concave subject convex constraints gi max fi max gi fi 
call formulation concise number variables constraints gi polynomial li number variables note max gi fi convex eq 
likewise assume feasible bounded eq 
example hamming loss bipartite matchings counts number different edges matchings jk yjk jk rn jk jk equality follows fact valid matching training example reviewers papers 
loss augmented matching problem written lp similar eq 
constant term rn max jk jk jk jk jk jk jk 
terms eq 
fi gi affine fi rn jk jk jk jk gi jk jk jk 
learning structured prediction models large margin approach generally express max lp similar lp loss augmented inference di max fiw ci ai bi appropriately defined di fi ci ai bi depend 
note appears objective 
cases formulate loss augmented inference convex optimization problem eq 
formulation concise lagrangian duality see boyd vandenberghe excellent review define joint concise convex problem estimating parameters lagrangian associated eq 
li fi gi vector lagrange multipliers constraint function gi 
assume fi concave bounded nonempty set gi strong duality max gi fi min max li 
forms write lagrangian dual min max li explicitly min hi qi hi qi convex 
folded qi brevity 
original problem polynomial size dual polynomial size 
example dual lp eq 
di min fiw ci hi di qi fiw ci 
plugging eq 
eq 
get min fi min qi hi 
combine minimization minimization reason right hand side minimum constraint tighter necessary leading suboptimal solution optimizing jointly produce solution optimal 
min fi hi qi 
joint concise convex optimization program estimating exact form program depends strongly example qp linear constraints min fi di fiw ci 
formulation generalizes idea taskar 
provide polynomial time estimation procedure certain family markov networks derive max margin formulations taskar 
taskar 


certificate formulation previous section assumed concise convex formulation loss augmented max eq 

important combinatorial problems allow polynomial time solution concise convex optimization formulation 
example maximum weight spanning tree perfect matching problems expressed linear programs exponentially constraints polynomial formulation convex optimization problem known schrijver 
problems solved polynomial time combinatorial algorithms 
cases find concise certificate optimality guarantees arg maxy fi expressing loss augmented inference concise convex program 
intuitively verifying assignment optimal easier finding 
simple example consider maximum weight spanning tree problem 
basic property spanning tree cutting edge tree creates disconnected sets nodes vj jk vk jk vj jk vk jk 
spanning tree optimal respect set edge weights edge tree connecting vj jk vk jk weight larger equal weight edge graph vj jk vk jk schrijver 
conditions expressed linear inequality constraints weights 
generally suppose find concise convex formulation conditions polynomial li set functions qi jointly convex auxiliary variables fixed qi fi fi 
min qi joint convex program computes max margin parameters 
learning structured prediction models large margin approach expressing spanning tree optimality require additional variables problems perfect matching optimality see auxiliary variables needed 
consider problem finding matching non bipartite graph considering perfect matchings node exactly neighbor provide reduction non perfect case node neighbor 
perfect matching complete undirected graph 
alternating cycle path respect edges alternate belong 
alternating cycle augmenting respect score edges matching smaller score edges matching theorem edmonds perfect matching maximum weight perfect matching augmenting alternating cycles 
number alternating cycles exponential number vertices simply enumerating infeasible 
rule cycles considering shortest paths 
negating score edges discussion assume edge score sjk modified way 
refer score sjk length edge jk 
alternating cycle augmenting length negative 
condition ruling negative length alternating cycles stated succinctly type distance function 
pick arbitrary root node denote length shortest distance alternating path edge path 
shortest distances defined negative alternating cycles 
constraints capture distance function 
sjk sjk jk sjk sjk jk theorem exists distance function satisfying constraints eq 
augmenting alternating cycles exist 
proof taskar chap 

learning formulation assuming hamming loss loss augmented edge weights jk jk xjk jk jk term front negates score edges matching 
di vector distance variables fi gi matrices coefficients qi vector fiw qi represents constraints eq 
example joint convex program di playing role computes max margin parameters min fiw qi 
features rich predict training data perfectly introduce slack variable vector allow violations constraints 
case non perfect matchings handled reduction perfect matchings follows schrijver 
create new graph making copy nodes edges adding edges node corresponding node copy 
extend matching replicating edges copy unmatched node introduce edge copy 
define xjk edges original copy 
perfect matchings graph projected original graph correspond non perfect matchings original graph 

duals kernels min max formulation certificate formulation produce primal convex optimization problems 
solving directly consider dual versions 
particular svms kernel trick dual efficiently learn high dimensional feature spaces 
consider example primal problem eq 

dual training example li li variables edge jk kj constraints edge primal 
vector dual variables example dual quadratic problem min gi occurrence feature vectors expansion squared norm term objective jk kj jk jk apply kernel trick jk mn jk mn 
prediction time kernel trick compute xmn jk kj jk jk xmn 
dual min max eq 

learning structured prediction models large margin approach 
pdb protein ans amino acid sequence structure graph potential bonds 
actual connectivity shown yellow model graph potential bonds 

experiments apply framework task connectivity prediction 
proteins containing residues form intra chain bonds known bridges 
bonds important feature protein structure enhance conformational stability reducing number configurational states decreasing entropic cost folding protein native state baldi 
knowledge exact bonding pattern protein provides information protein structure possibly function evolution 
furthermore connectivity pattern imposes structural constraints reduce search space protein folding prediction protein structure prediction 
development efficient scalable accurate methods prediction bonds important practical applications 
increasing interest applying computational techniques task predicting connectivity baldi 
lines predict connectivity pattern finding maximum weighted matching graph vertex represents residue edge represents attraction strength connects 
example ans protein fig 
bonds 
parameterize attraction scoring function linear combination features include protein sequence residues evolutionary information form multiple alignment profiles secondary structure solvent accessibility information learn weights different features set solved protein structures connectivity patterns known 
datasets 
datasets containing sequences experimentally verified bonding patterns sp baldi 
frasconi 
report results experimental settings bonding state known know bonds unknown 
known state setting corresponds non bipartite matchings unknown imperfect matchings 
case bonding state known order compare previous focus proteins containing bonds covers entire sp dataset dataset 
order avoid biases testing adopt dataset splitting procedure previous frasconi baldi 
models 
experimental results report dual formulation eq 
rbf kernel xjk exp xjk 
commercial qp software cplex train models 
training time took minutes examples 
set features xjk candidate pair jk local windows size centered 
simplest approach encode window actual sequence binary vector entries denote particular amino acid occurs particular position 
common strategy compensate sparseness features multiple sequence alignment profile information 
multiple alignments computed running psi blast default settings align sequence sequences nr database altschul 
input features model profile consist proportions occurrence amino acids alignments position local windows 
second model profile ss augments profile model secondary structure solvent accessibility information 
program dataset publicly available baldi 

consists non redundant proteins pdb berman may contain intra chain bonds 
sequences annotated secondary structure solvent accessibility information sander 
split sp different subsets constraint proteins sequence similarity belong different subsets 
ran rigorous smith waterman local pairwise alignment blosum gap penalty extension considered pairs aligned residues dissimilar 
learning structured prediction models large margin approach svm profile dag rnn profile profile ss profile dag rnn table 
numbers indicate precision accuracy known bonded state setting precision recall accuracy unknown bonded state 
denotes true number bonds 
best results row bold 
profile vs svm dag rnn model baldi sp 
profile vs profile ss models dataset 
profile vs dag rnn model sp unknown state 
produces types secondary structure augment local window size additional length binary vector length binary vector representing solvent accessibility position 
utilizes true structure information assigning secondary structure model unknown proteins 
performance useful upper bound potential prediction improvement features derived accurate secondary structure prediction algorithms 
known bonded state 
bonding state known evaluate algorithm metrics accuracy measures fraction entire matchings predicted correctly precision measures fraction correctly predicted individual bonds 
apply model sp dataset fold cross validation replicates experimental setup baldi 
frasconi 
evaluate advantage learning formulation provides baseline approach features profile model ignore constraints learning phase simply learn svm labeling bonded pairs positive examples non bonded pairs negative examples 
svm learned weights score edges 
results summarized table column svm 
profile model uses certificate formulation directly incorporating matching constraint learning phase 
achieves significant gains svm bond numbers illustrating importance explicitly modelling structure parameter estimation 
compare model dag rnn model baldi 
current top performing system uses recursive neural networks set input features 
performance better bond numbers 
final experiment examine role secondary structure solvent accessibility information plays model profile ss 
table shows gains significant especially sequences bonds 
highlights importance developing richer features complex kernels 
unknown bonded state 
evaluated learning formulation case bonding state unknown graph copy reduction described sec 

additional evaluation metric recall measures correctly predicted bonds fraction total number bonds 
apply model sp dataset proteins bonds assuming knowledge bonded state 
sequences contain computational efficiency training take bonded including ones participate bonds randomly selecting additional protein available 
testing 
results summarized table comparison dag rnn model currently model tackle challenging setting 
results compare favorably having better precision recall bond numbers connectivity pattern accuracy slightly lower bonds 

discussion framework learning wide range structured prediction models set outcomes class combinatorial structures matchings graph cuts 
formulations structured max margin estimation define concise convex optimization problem 
formulation min max relies ability express inference model concise convex optimization problem 
second certificate requires expressing optimality desired solution model 
illustrate apply formulations problem learning match bipartite non bipartite matchings 
formulations learning structured prediction models large margin approach applied min cuts max flows trees colorings combinatorial problems 
closely related large margin methods probabilistic models max margin formulation ties body called inverse combinatorial convex optimization survey see 
inverse optimization problem defined instance optimization problem maxy set nominal weights target solution goal find weights closest nominal norm target solution optimal 
min solution depends critically choice nominal weights example trivially optimal solution 
approach different goal learning parameterized objective function depends input generalize prediction new instances 
approach attempts find weights obtain particular target solution training instance 
unique solution reasonable assumption domains bonding may equally target solutions 
interesting extend approach accommodate multiple target solutions 
estimation problem tractable exact prediction problem formulated concise convex optimization problem polynomial time combinatorial algorithm concise convex optimality conditions 
intractable models tripartite matching quadratic assignment max cut framework learn approximate parameters exploiting approximations provide upper lower bounds optimal structure score provide certificate optimality large neighborhood desired structure 
shelf convex optimization code learning formulation convenient flexible problem specific methods combinatorial subroutines outperform generic solvers 
design algorithms open problem 
addressed estimation models discrete output spaces 
similarly consider range problems prediction variables continuous 
problems natural generalizations regression involving correlated real valued outputs 
examples include learning models metabolic flux cells obeys constraints learning game payoff matrices observed equilibria strategies 
learning framework addresses large class prediction models rich interesting structure 
hope continued research vein help tackle sophisticated prediction problems 

supported nsf dbi 
darpa program subcontract sri 
altschul madden schaffer zhang miller lipman 
gapped blast psi blast new generation protein database search programs 
acids res 
altun tsochantaridis hofmann 

hidden markov support vector machines 
proc 
icml 
baldi cheng 

large scale prediction bond connectivity 
proc 
nips 
belongie malik puzicha 

shape matching object recognition shape contexts 
ieee trans 
pattern anal 
mach 
intell 
berman westbrook feng bhat bourne 

protein data bank 
boyd vandenberghe 

convex optimization 
cambridge university press 
collins 

discriminative training methods hidden markov models theory experiments perceptron algorithms 
proc 
emnlp 
duda hart stork 

pattern classification 
new york wiley interscience 
nd edition 
edmonds 

maximum matching polyhedron vertices 
journal research national bureau standards 


prediction connectivity proteins 
bioinformatics 


inverse combinatorial optimization survey problems methods results 
journal combinatorial optimization 
sander 

dictionary protein secondary structure pattern recognition geometrical features 
schrijver 

combinatorial optimization polyhedra efficiency 
springer 
taskar 

learning structured prediction models large margin approach 
doctoral dissertation stanford university 
taskar koller 

learning associative markov networks 
proc 
icml 
taskar guestrin koller 

max margin markov networks 
proc 
nips 
taskar klein collins koller manning 

max margin parsing 
proc 
emnlp 
tsochantaridis hofmann joachims altun 

support vector machine learning interdependent structured output spaces 
proc 
icml 
frasconi 

connectivity prediction recursive neural networks evolutionary information 
bioinformatics 
