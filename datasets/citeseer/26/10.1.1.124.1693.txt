convex repeated games fenchel duality shai shalev shwartz yoram singer school computer sci 
eng hebrew university jerusalem israel google parkway mountain view ca usa describe algorithmic framework game term convex repeated game 
show various online learning boosting algorithms derived special cases algorithmic framework 
unified view explains properties existing algorithms enables derive new interesting algorithms 
algorithmic framework stems connection build notions regret game theory weak duality convex optimization 
problem setting problems arising machine learning modeled convex repeated game 
convex repeated games closely related online convex programming see discussion section 
convex repeated game players game performed sequence consecutive rounds 
round repeated game player chooses vector wt convex set second player responds convex function gt player suffers instantaneous loss gt wt 
study game viewpoint player 
goal player minimize cumulative loss gt wt 
motivate setting cast familiar setting online learning convex repeated game 
online learning performed sequence consecutive rounds 
round learner receives question cast vector xt required provide answer question 
example xt encoding email message question email spam 
prediction learner performed hypothesis ht set questions set possible answers 
aforementioned example stands spam email stands benign 
predicting answer learner receives correct answer question denoted yt suffers loss loss function ht xt yt 
cases hypotheses prediction come parameterized set hypotheses hw 
example set linear classifiers answering questions defined hw sign 
saying round learner chooses hypothesis say learner chooses vector wt hypothesis 
note environment chooses question answer pair xt yt loss function function hypotheses space equivalently set parameter vectors redefine online learning process follows 
round learner chooses vector wt defines hypothesis prediction 
environment chooses pair xt yt induces loss function set parameter vectors gt hw xt yt 
learner suffers loss gt wt xt yt 
described process online learning convex repeated game 
assess performance player notion regret 
number rounds fixed vector define regret player excess loss consistently playing vector gt wt gt main result algorithmic framework player guarantees low regret respect vector specifically derive regret bounds take form gt wt gt 
informally function measures complexity vectors scalar related generalized lipschitz property functions 
gt defer exact requirements impose sections 
algorithmic framework emerges representation regret bound eq 
optimization problem 
specifically rewrite eq 
follows gt wt inf gt 
average loss player bounded minimum value optimization problem jointly minimize average loss complexity measured function note optimization problem right hand side eq 
solved hindsight observing entire sequence loss functions 
writing regret bound eq 
implies average loss player forms lower bound minimization problem 
notion duality commonly convex optimization theory plays important role obtaining lower bounds minimal value minimization problem see example 
generalizing notion fenchel duality able derive dual optimization problem optimized incrementally game progresses 
order derive explicit quantitative regret bounds immediate fact dual objective lower bounds primal objective 
reduce process playing convex repeated games task incrementally increasing dual objective function 
amount dual increases serves new natural notion progress 
doing able tie primal objective value average loss player increase dual 
rest organized follows 
sec 
establish notation point mathematical tools 
main tool deriving algorithms playing convex repeated games generalization fenchel duality described sec 

algorithmic framework sec 
analyzed sec 

generality framework allows utilize different problems arising machine learning 
specifically sec 
underscore applicability framework online learning sec 
outline analyze boosting algorithms framework 
conclude discussion point related sec 

mathematical background denote scalars lower case letters vectors bold face letters 
inner product vectors denoted sets designated upper case letters 
set non negative real numbers denoted 
set integers 
denoted 
norm vector denoted 
dual norm defined sup 
example euclidean norm dual norm xi dual norm maxi xi 
recall definitions convex analysis 
reader familiar convex analysis may proceed lemma thorough see example 
set convex vectors line set open point neighborhood lying set closed complement open set 
function closed convex scalar level set closed convex 
fenchel conjugate function defined sup closed convex fenchel conjugate 
fenchel young inequality states vector sub gradient function differential set denoted set sub gradients differentiable consists single vector amounts gradient denoted 
sub gradients play important role definition fenchel conjugate 
particular lemma states fenchel young inequality holds equality 
lemma closed convex function differential set proof appendix continuous function strongly convex convex set respect norm contained domain 
strongly convex functions play important role analysis primarily due lemma 
lemma norm dual norm 
strongly convex function fenchel conjugate 
differentiable arg maxx 
furthermore proof appendix notable examples strongly convex functions follows 
example function strongly convex respect norm 
conjugate function 
example function wi log wi strongly convex probabilistic simplex rn respect norm 
conjugate function log exp 
generalized fenchel duality section derive main analysis tool 
start considering optimization problem inf gt non negative scalar 
equivalent problem inf wt gt wt wt introducing vectors 
vector lagrange multipliers equality constraint wt obtain lagrangian 
wt 
gt wt wt dual problem task maximizing dual objective value 
inf 
wt 
wt sup sup wt gt wt wt exposition sec 

fenchel conjugate functions 
gt generalized fenchel dual problem sup 
note duality called fenchel duality 
template learning algorithm convex repeated games section describe template learning algorithm playing convex repeated games 
mentioned study convex repeated games viewpoint player shortly denote 
recall learning algorithm achieve regret bound form eq 

start rewriting eq 
follows gt wt inf gt sublinear term cumulative loss lower bounds optimum minimization problem right hand side eq 

previous section derived generalized fenchel dual right hand side eq 

construction weak duality theorem stating value dual problem smaller optimum value primal problem 
algorithmic framework propose derived incrementally ascending dual objective function 
intuitively ascending dual objective move closer optimal primal value performance similar performance best fixed weight vector minimizes right hand side eq 

initially elementary dual solution assume gt imply 

assume addition strongly convex 
lemma function differentiable 
trial uses prediction vector wt 
predicting wt receives function gt suffers loss gt wt 
updates dual variables follows 
denote differential set gt wt gt gt wt wt 
new dual variables 
set set vectors satisfy conditions 



ii 

section show condition ensures increase dual trial proportional loss gt wt 
second condition ensures calculate dual trial knowledge seen loss functions gt 
gt conclude section update rules trivially satisfy conditions 
update scheme simply finds set second update defines 

argmax 

analysis section analyze performance template algorithm previous section 
proof technique monitoring value dual objective function 
main result lemma gives upper lower bounds final value dual objective function 
lemma strongly convex function respect norm set assume minw 

gt sequence convex closed functions gt 
suppose dual incrementing algorithm satisfies conditions eq 
run complexity function sequence 
gt 
wt sequence primal vectors algorithm generates 
final sequence dual variables 
exists sequence sub gradients 
gt wt 
inf gt proof second inequality follows directly weak duality theorem 
turning left inequality denote 


rewritten note 

equality follows fact 

definition update implies 



subgradient denoting rewrite lower bound lemma definition wt get wt 
assume gt closed convex apply lemma get wt gt wt 
plugging equality eq 
summing obtain gt wt combining inequality eq 
concludes proof 
regret bound follows direct corollary lemma 
theorem conditions lemma 
denote gt wt gt particular obtain bound gt wt application online learning gt sec 
cast task online learning convex repeated game 
demonstrate applicability algorithmic framework problem instance ranking 
analyze setting prediction problems including binary classification multiclass prediction multilabel prediction label ranking cast special cases instance ranking problem 
recall online round learner receives question answer pair 
instance ranking question encoded matrix xt dimension kt answer vector yt kt semantic yt follows 
pair yt yt say yt ranks th row xt ahead th row xt 
interpret yt yt confidence th row ranked ahead th row 
example row xt encompasses representation movie yt movie rating expressed number stars movie received movie reviewer 
predictions learner determined weight vector wt rn defined yt xt wt 
define loss functions ranking generalize hinge loss binary classification problems 
denote set yt yt 
define pair hinge loss xt yt yt yt xt xt max xt xt respectively th th rows xt 
note zero ranks xt higher xt sufficient confidence 
ideally wt xt yt zero case penalized combination pair losses example set xt yt average pair losses avg xt yt xt yt loss suggested authors see example 
popular approach see example penalizes maximal loss individual pairs max xt yt max xt yt apply algorithmic framework sec 
ranking gt avg xt yt max xt yt 
theorem provides sufficient condition regret bound thm 
holds ranking 
theorem strongly convex function respect norm 
denote lt maximum xt xt 
gt avg xt yt gt max xt yt regret bound holds boosting game gt wt gt pt lt section describe applicability algorithmic framework analysis boosting algorithms 
boosting algorithm uses weak learning algorithm generates weak hypotheses performances just slightly better random guessing build strong hypothesis attain arbitrarily low error 
adaboost algorithm proposed freund schapire receives input training set examples 
xm ym xi taken instance domain yi binary label yi 
boosting process proceeds sequence consecutive trials 
trial booster defines distribution denoted wt set examples 
booster passes training set distribution wt weak learner 
weak learner assumed return hypothesis ht average error slightly smaller 
exists constant def wt xi 
goal boosting algorithm invoke weak learner times different distributions combine hypotheses returned weak learner final called strong hypothesis error small 
final hypothesis combines linearly hypotheses returned weak learner coefficients 
defined sign hf hf ht coefficients 
determined booster 
ad initial distribution set uniform distribution 

iter ation value set log 
distribution updated rule wt wt exp yi ht xi zt zt normalization factor 
freund schapire shown assumption eq 
error final strong hypothesis exp 
authors proposed view boosting coordinate wise greedy optimization process :10.1.1.135.1357:10.1.1.30.3515:10.1.1.156.2440
note hf errs example iff hf 
exp loss function defined exp hf smooth upper bound zero error equals hf 
restate goal boosting minimizing average exp loss hf training set respect variables 
simplify derivation sequel prefer say boosting maximizes negation loss max exp yi tht xi 
view boosting optimization procedure iteratively maximizes eq 
respect variables 
view boosting enables hypotheses returned weak learner general functions reals ht see instance :10.1.1.156.2440
view boosting convex repeated game booster weak learner 
motivate construction note boosting algorithms define weights different domains vectors wt assign weights examples weights weak hypotheses 
terminology weights wt primal vectors show sequel weight hypothesis ht related dual vector particular show eq 
exactly fenchel dual primal problem convex repeated game algorithmic framework described far playing games naturally fits problem iteratively solving eq 

derive primal problem fenchel dual problem eq 
denote vt vector ith element vt xi 
set gt function gt vt intuitively gt penalizes vectors assign large weights examples predicted accurately xi 
particular ht xi wt distribution examples case adaboost gt wt reduces see eq 

case minimizing gt equivalent maximizing error individual hypothesis ht examples 
consider problem minimizing gt relative entropy example see eq 

derive fenchel dual note exists see appendix 
addition define goal maximize dual restrict take form vt get 
vt log pt xi 
minimizing exp loss strong hypothesis dual problem primal minimization problem find distribution examples relative entropy uniform distribution small possible correlation distribution vt small possible 
correlation vt inversely proportional error ht respect obtain primal problem trying maximize error individual hypothesis dual problem minimize global error strong hypothesis 
intuition finding distributions retrospect result large error rates individual hypotheses alluded :10.1.1.30.3515
apply algorithmic framework sec 
boosting 
describe game parameters underscore case vt game booster sets dual variables zero 
trial boosting game booster constructs primal weight vector wt rm assigns importance weights examples training set 
primal vector wt constructed eq 
wt ivi 
weak learner responds presenting loss function gt vt booster updates dual variables increase dual objective function 
possible show range ht update eq 
equivalent update min log 
obtained variant adaboost weights capped 
disadvantage variant need know parameter 
note passing limitation lifted different definition functions gt 
omit details due lack space 
analyze game boosting note conditions lemma holds left hand side inequality lemma tells gt wt 
definition gt weak learnability sumption eq 
imply wt vt gt wt wt vt implies vt recall vt xi 
assuming range ht get 
combining left hand side inequality lemma get 

definition see eq 
value rearranging terms recover original bound adaboost yi pt ht xi related discussion new framework designing analyzing algorithms playing convex repeated games 
framework analysis known algorithms online learning boosting settings 
framework paves way new algorithms 
previous suggested duality design online algorithms context mistake bound analysis 
contribution fold briefly discuss 
generalize applicability framework specific setting online learning hinge loss general setting convex repeated games 
setting convex repeated games formally termed online convex programming gordon 
voluminous amount unifying approaches deriving online learning algorithms 
refer reader closely related content 
generalizing previously studied algorithmic framework online learning automatically utilize known online learning algorithms norm algorithms setting online convex programming 
note algorithms derived special cases algorithmic framework setting parallel independently gordon described algorithmic framework online convex programming closely related potential algorithms described cesa bianchi lugosi 
gordon considered problem defining appropriate potential functions 
generalizes theorems providing somewhat simpler analysis 
second usage generalized fenchel duality lagrange duality enables analyze boosting algorithms framework 
authors derived unifying frameworks boosting algorithms 
general framework connection game playing fenchel duality underscores interesting perspective online learning boosting 
believe viewpoint potential yielding new algorithms domains 
despite generality framework introduced resulting analysis distilled earlier analysis reasons 
usage lagrange duality restricted notion generalized fenchel duality appropriate general broader problems consider 
ii strongly convex property employ simplifies analysis enables intuitive conditions theorems 
various possible extensions pursue due lack space 
framework naturally analysis settings repeated games see 
applicability framework online learning extended prediction problems regression sequence prediction 
conjecture primal dual view boosting lead new methods regularizing boosting algorithms improving generalization capabilities 
borwein lewis 
convex analysis nonlinear optimization 
springer 
boyd vandenberghe 
convex optimization 
cambridge university press 
cesa bianchi lugosi 
prediction learning games 
cambridge university press 
collins schapire singer 
logistic regression adaboost bregman distances 
machine learning 
crammer dekel keshet shalev shwartz singer 
online passive aggressive algorithms 
jmlr mar 
freund schapire 
decision theoretic generalization line learning application boosting 
eurocolt 
freund schapire 
game theory line prediction boosting 
colt 
friedman hastie tibshirani 
additive logistic regression statistical view boosting 
annals statistics 
gordon 
regret bounds prediction problems 
colt 
gordon 
regret algorithms online convex programs 
nips 
grove littlestone schuurmans 
general convergence results linear discriminant updates 
machine learning 
kivinen warmuth 
relative loss bounds multidimensional regression problems 
journal machine learning 
mason baxter bartlett frean 
functional gradient techniques combining hypotheses 
advances large margin classifiers 
mit press 
nesterov 
primal dual subgradient methods convex problems 
technical report center operations research econometrics core catholic university louvain ucl 
schapire singer :10.1.1.156.2440
improved boosting algorithms confidence rated predictions 
machine learning 
shalev shwartz singer 
online learning meets optimization dual 
colt 
weston watkins 
support vector machines multi class pattern recognition 
esann april 

online convex programming generalized infinitesimal gradient ascent 
icml 
fenchel conjugate pairs section describe useful fenchel conjugate pairs 
euclidean norm squared norm dual norm 
domain example pp 

norm dual 
proof see hinge loss rn rn conjugate show recall sup rn 
need consider cases 
case note case objective maximize eq 

denote inner product variable take value obtain max sup sup turn second case exist case rewrite equal zero 
setting av objective eq 
gives tends 
relative entropy logarithm sum exponentials rn wi wi wi log fenchel conjugate proof see 
log exp 
binary entropy logistic loss rn wi wi log wi wi log wi fenchel conjugate log 
fenchel pair correspondence shown fact differentiable functions inverse function 
effect scaling shifting conclude section useful property conjugate pairs 
function fenchel conjugate 
fenchel conjugate af af proof see page 
technical proofs proof lemma know equivalently sup right hand side equals 
assumption closed convex implies fenchel conjugate sup combining eq 
gives inequalities hold equality concludes proof 
proof lemma lemma know convex gradient lipschitz continuous constant 
arbitrary positive integer 
define convex addition holder inequality eq 
get combining get concludes proof 

