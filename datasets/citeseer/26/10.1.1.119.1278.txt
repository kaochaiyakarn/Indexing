ieee transactions pattern analysis machine intelligence vol 
august candid covariance free incremental principal component analysis weng member ieee zhang student member ieee hwang member ieee appearance image analysis techniques require fast computation principal components high dimensional image vectors 
introduce fast incremental principal component analysis ipca algorithm called candid covariance free ipca compute principal components sequence samples incrementally estimating covariance matrix covariance free 
new method motivated concept statistical efficiency estimate smallest variance observed data 
keeps scale observations computes mean observations incrementally efficient estimate wellknown distributions gaussian highest possible efficiency guaranteed case unknown sample distribution 
method real time applications allow iterations 
converges fast high dimensional image vectors 
links ipca development cerebral cortex discussed 
index terms principal component analysis incremental principal component analysis stochastic gradient ascent sga generalized hebbian algorithm gha orthogonal complement 
class image analysis techniques called appearance approach popular 
major reason leads popularity statistics tools automatically derive features relying humans define features 
principal component analysis known technique sirovich kirby appear technique directly characterization human faces image considered simply high dimensional vector pixel corresponding component 
turk pentland representation face recognition :10.1.1.12.7580
technique extended object recognition sign recognition autonomous navigation image analysis problems 
known computational approach pca involves solving eigensystem problem computing eigenvectors eigenvalues sample covariance matrix numerical method power method qr method 
approach requires training images available principal components estimated 
called batch method 
type method longer satisfies coming new trend computer vision research visual filters incrementally derived long online real time video stream motivated development animal vision systems 
online development visual filters requires system perform new sensory signals flow 
dimension image high computation storage complexity grow dramatically 
example eigenface method moderate gray image rows columns results dimensional vector 
symmetric covariance matrix requires elements amounts entries 
clever saving method number images smaller authors department computer science engineering michigan state university east lansing mi 
mail weng cse msu edu 
manuscript received feb revised oct accepted oct 
recommended acceptance beveridge 
information obtaining reprints article please send mail tpami computer org ieeecs log number 
ieee published ieee computer society number pixels image 
online developing system observe open number images number larger dimension observed vectors 
incremental method required compute principal components observations arriving sequentially estimate principal components updated arriving observation vector 
covariance matrix allowed estimated intermediate result 
evidence biological neural networks incremental method perform various learning hebbian learning 
ipca techniques proposed compute principal components covariance matrix 
ran convergence problems facing highdimensional image vectors 
explain article 
propose new method candid covariance free ipca oja karhunen sanger 
motivated known statistical concept called efficient estimate 
amnesic average technique dynamically determine retaining rate old new data fixed learning rate 
derivation algorithm eigenvector suppose sample vectors acquired sequentially possibly infinite 
dimensional vector large 
loss generality assume zero mean mean may incrementally estimated subtracted 
ut covariance matrix known allowed estimated intermediate result 
definition eigenvector matrix satisfies ax corresponding eigenvalue 
replacing unknown sample covariance matrix replacing estimate time step obtain illuminating expression nth step estimate see soon equation motivated statistical efficiency 
estimate easy get eigenvector eigenvalue 
question estimate 
considering may choose jjv jj leads incremental expression jjv jj set direction data spread 
incremental estimation written recursive form ut jjv jj weight estimate weight new data 
proven algorithm largest eigenvalue covariance matrix fu corresponding eigenvector 
derivation motivated statistical efficiency 
unbiased estimate parameter said efficient estimate class distribution functions distribution density function variance squared error reached minimal value ieee transactions pattern analysis machine intelligence vol 
august fig 

intuitive explanation incremental pca algorithm 
fig 

correctness correlation represented dot products eigenvectors computed sga gha proposed amnesic parameter 
ieee transactions pattern analysis machine intelligence vol 
august fig 

correctness eigenvalue du log right side called cram rao bound 
says efficient estimate variance real parameter variance bounded cram rao bound 
example sample mean pn efficient estimate mean gaussian distribution known standard deviation 
vector version cram rao bound reader referred pp 

define ut viewed mean samples 
exactly method motivated statistical efficiency averaging 
words statistically method tends converge quickly estimate smallest error variance currently observed samples 
course necessarily drawn gaussian distribution independently estimate sample mean strictly efficient 
estimate high statistical efficiency fairly low error variance show experimentally 
cram rao lower error bound estimate error variance equivalently convergence rate gaussian distribution model proposed experimented weng section 
reasonable estimate near optimal statistical efficiency 
weng demonstrated actual error variance sensitive distribution uniform gaussian distributions 
error estimator especially useful estimate roughly samples needed tolerable error variance 
ipca algorithms studied researchers 
early rigorous proof convergence oja oja karhunen introduced stochastic gradient ascent sga algorithm 
sga computes vi vi vi vi vi vj vi estimate ith dominant eigenvectors sample covariance matrix vi new estimate 
practice done standard gram schmidt procedure 
parameter stochastic approximation gain 
convergence sga proven assumptions 
sga essentially gradient method associated problem choosing learning rate 
simply speaking learning rate appropriate second term correction term right side comparable term large small 
practice depends 
nature data usually requires error procedure impractical online applications 
oja gave suggestions typically multiplied constants 
procedure mercy magnitude observation term unit norm second take magnitude 
small magnitude second term small changes new estimate 
large magnitude case high dimensional images second term dominate right side large number small reached 
case updating inefficient convergence slow 
contrasted sga term right side normalized 
effect converges eigenvalue eigenvector 
statistical efficiency realized keeping scale estimate order new observations second terms properly weighted right side get sample mean allows full observation terms statistical efficiency 
note coefficient important learning rate second term realize sample mean 
close large important fast convergence early samples 
point estimate converge harder pull back large 
need worry nature observations 
reason candid naming new algorithm 
true series parameters sga manually tuned offline application takes account magnitude 
predefined accomplish statistical efficiency matter tuned 
true observations term contribute estimate weight statistical efficiency contribute unequally due normalization term damage efficiency 
manual tuning suited online learning algorithm user predict signals advance 
online algorithm automatically compute data sensitive parameters 
improvement procedure 
samples jjv jj weighted equally 
generated far away real value early estimation stage sample large noise small 
speed convergence estimation preferable give smaller weight early samples way implement idea amnesic average changing ieee transactions pattern analysis machine intelligence vol 
august fig 

absolute values eigenvalues 
ut jjv jj positive parameter called amnesic parameter 
note modified weights sum 
presence larger weight new samples effect old samples fade gradually 
typically ranges 
intuitive explanation intuitive explanation procedure follows consider set dimensional data gaussian probability distribution function physically arising distribution consider orders statistics pca 
data ellipse shown fig 

geometrical meaning eigenvectors know eigenvector aligned long axis ellipse 
suppose th step estimation eigenvector 
noticing scalar know jjv jj ut jjv jj essentially scaled vector 
weighted combination estimate scaled vector 
geometrically speaking obtained pulling small amount 
line orthogonal divides plane halves upper lower ones 
point ul lower half plane obtuse angle ut negative scalar 
ul may written utl ul jjv jj jjv jj ul upper half plane point obtained rotating ul degrees origin 
ellipse centrally symmetric may rotate lower half plane points upper half plane consider pulling effect upper half plane points 
points uu upper half plane pure force pull direction data points right side left side 
long eigenvalues different pulling force exists pulling direction eigenvector corresponding larger eigenvalue 
moving aligned pulling forces sides balanced 
words converge eigenvector 
imagine larger ratio eigenvalue second eigenvalue unbalanced force faster pulling convergence 
ellipse degenerates circle 
movement algorithm converge 
vector circle represent eigenvector hurt converge 
get back cases equal eigenvalues section 
higher order eigenvectors procedure estimates dominant eigenvector 
way compute higher order eigenvectors sga start set vectors update suggested iteration step recover orthogonality 
real time online computation need avoid time consuming 
breaking orthogonality slows convergence compared keeping orthogonality 
know eigenvectors orthogonal 
helps generate observations complementary space computation higher order eigenvectors 
example compute second order eigenvector subtract data projection estimated order eigenvector shown jjv jj jjv jj 
obtained residual complementary space serves input data iteration step 
way orthogonality enforced convergence reached exactly early stages 
effect better uses sample available speeds convergence 
similar idea researchers 
proposed algorithm finds eigenvector fig 

effect amnesic parameter 
correctness eigenvectors computed amnesic parameter 
comparison fig 

ieee transactions pattern analysis machine intelligence vol 
august fig 

longer data stream 
correctness eigenvectors computed sga gha amnesic parameter respectively epochs 
method equivalent sga subtracts component samples computing component 
sanger suggested algorithm called generalized hebbian algorithm gha idea components computed time 
case statistical efficiency considered 
new saves computations 
may notice expensive steps sga dot products high dimensional data space 
requires extra dot product vi principal component estimation step 
sga new estimates eigenvectors totally dot products 
average number dot product saved sga eigenvector 
equal eigenvalues consider case equal eigenvalues 
suppose ordered eigenvalues equal explanation section vector estimate converge larger eigenvalue 
estimate eigenvectors ei affected anyway 
vector estimates el em converge subspace spanned corresponding eigenvectors 
eigenvalues equal shape distribution fig 
hypersphere subspace 
estimates multiple eigenvectors converge set orthogonal basis subspace 
converges depends mainly early ieee transactions pattern analysis machine intelligence vol 
august fig 

eigenfaces obtained batch pca amnesic parameter epoch amnesic parameter epochs shown images 
samples averaging effect contribution new data gets infinitely small increases bound 
exactly want 
convergence eigenvectors fast general case 
algorithm summary combining mechanisms discussed candid covariance free ipca algorithm follows procedure 
compute dominant eigenvectors vk directly 
followings steps 


minfk ng initialize ith eigenvector vi ui 
vi vi ui ut vi jj ui ui ut jj jj mathematical proof convergence founded 
vi empirical results convergence performed experiments study statistical efficiency new algorithm existing ipca algorithms especially high dimensional data images 
define sample ratio number samples dimension sample space 
lower ratio generally harder statistical estimation problem 
results feret face data set 
data set frontal views subjects 
subjects views views view results data set images 
size image pixels dimensions 
hard problem low sample dimension ratio 
computed eigenvectors batch pca qr method ground truth 
program batch pca adapted recipes 
real mean image data unknown incrementally estimated sample mean vi nth sample image 
data entering ipca algorithms scatter vectors 
record intermediate results divided entire data set subsets 
data went ipca algorithms estimates eigenvectors saved subset passed 
sga learning rate suggested 
suggested extrapolated give gha set amnesic parameter set 
correlation estimated unit eigenvector computed batch method normalized represented inner product larger correlation better 
jjv jj iff 
see fig 
sga converge fed images 
gha shows trend converge estimates far correct ones 
contrast proposed converges fast 
higher order eigenvectors converge slower earlier ones th reaches percent extremely low sample dimension ratio 
see th principal component represents percent total data variance 
percent correlation correct means percent total data variance lost 
examine convergence eigenvalues ratio length estimated eigenvector divided estimate computed recipe batch method 
results eigenvalues show similar pattern fig 

conciseness shown eigenvalue result proposed fig 
fig 
showing eigenvalues 
ratio summation eigenvalues variance data percent means percent data variance falls subspace spanned eigenvectors 
demonstrate effect amnesic parameter show result eigenvector estimate 
comparing fig 
fig 
see amnesic parameter help achieve faster convergence 
amnesic parameter vary sail robot development program due space limit subject scope 
show performance algorithm longer data stream 
statistics real world image stream may necessarily stationary example mean ieee transactions pattern analysis machine intelligence vol 
august table average execution time estimating eigenvectors new data variance may change time changing mean variance convergence evaluation difficult 
avoid effect simulate statistically stable long data stream feeding images feret data set repeatedly algorithms 
fig 
shows result epochs 
expected ipca algorithms converge quickest 
shown fig 
eigenfaces estimated batch pca amnesic parameter epoch epochs respectively 
corresponding eigenfaces computed different methods similar 
average execution time sga gha estimation step shown table 
independent data 
doing procedure gha run significantly faster sga 
computational advantage gha saving normalization 
observed similar efficiency difference data sets speech data 
general readership experiment done lower dimension data set 
extracted pixel subimages right eye area image feret data set estimated sample covariance matrix matlab generate samples gaussian distribution dimensional space 
sample dimension ratio 
original subimage sequence statistically stationary person eye area image necessarily distribution defined early persons data 
matlab generated data avoid nonstationary situation 
turned eigenvectors estimated reached percent correlation actual ones 
discussions short concentrates challenging issue computing dominating eigenvectors eigenvalues incrementally arriving high dimensional data stream computing corresponding covariance matrix knowing data advance 
proposed algorithm fast convergence rate low computational complexity 
results showed concept efficient estimate plays dominating role convergence speed high dimensional data 
amnesic average technique implemented improve convergence rate 
importance result potentially apparent technical scope interesting computer vision community 
discussed human brain just computing processing data importantly fundamentally developing computing engine real world online sensory data streams 
lot studies remain done open questions waiting answered incremental development processor plays central role brain development 
processor closely related procedure widely appearance vision inner product input scatter vector eigenvector neuron sigmoidal nonlinearity 
relationship ipca brain 
clear answer available rubner schulten proved known mechanisms biological hebbian learning lateral inhibition nearby neurons pp 
result incremental way computing pca 
claim computational steps proposed physiologically brain link incremental pca developmental mechanisms brain probably intimate fully appreciate 
acknowledgments supported part national science foundation 
iis darpa eto contract 
darpa ito 
dabt 
authors chen codes batch pca 
sirovich kirby low dimensional procedure characterization human faces optical soc 
am 
vol 
pp 
mar 
turk pentland eigenfaces recognition cognitive neuroscience vol :10.1.1.12.7580
pp 

murase nayar visual learning recognition objects appearance int computer vision vol 
pp 
jan 
cui weng appearance base hand sign recognition intensity image sequences computer vision image understanding vol 
pp 

chen weng state indoor visual navigation ieee trans 
neural networks vol 
pp 

golub matrix computations 
baltimore md johns hopkins univ press 
proc 
nsf darpa workshop development learning weng eds apr 
hertz krogh palmer theory neural computation 
addison wesley 
oja subspace methods pattern recognition 
research studies press 
oja karhunen stochastic approximation eigenvectors eigenvalues expectation random matrix math 
analysis application vol 
pp 

sanger optimal unsupervised learning single layer linear feedforward neural network ieee trans 
neural networks vol 
pp 

zhang weng convergence analysis complementary candid incremental principal component analysis technical report msu cse dept computer science eng michigan state univ east lansing aug 
probability theory mathematical statistics third ed 
john wiley sons 
weng huang ahuja motion structure image sequences 
springer verlag 
adaptive data orthogonalization proc 
ieee int conf 
acoustics speech signal processing pp 
apr 
thompson adaptive spectral analysis technique unbiased frequency estimation presence white noise proc 
th asilomar conf 
circuits systems computers pp 

advanced engineering mathematics 
wiley 
phillips moon rauss rizvi feret evaluation methodology face recognition algorithms proc 
ieee conf 
computer vision pattern recognition pp 
june 
press flannery teukolsky vetterling numerical recipes second ed 
cambridge univ press 
weng hwang zhang yang smith developmental humanoids humanoids develop skills automatically proc 
ieee ras int conf 
humanoid robots sept 
rubner schulten development feature detectors self organization biological cybernetics vol 
pp 

principles neural science third ed 
kandel schwartz eds conn appleton lange 
information computing topic please visit digital library computer org publications dlib 
