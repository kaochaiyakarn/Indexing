ieee transactions neural networks vol 
january fast iterative nearest point algorithm support vector machine classifier design keerthi bhattacharyya murthy give new fast iterative algorithm support vector machine svm classifier design 
basic problem treated allow classification violations 
problem converted problem computing nearest point convex polytopes 
suitability classical nearest point algorithms due gilbert mitchell studied 
ideas algorithms combined modified derive fast algorithm 
problems require classification violations allowed violations quadratically penalized idea due cortes vapnik convert problem classification violations 
comparative computational evaluation algorithm powerful svm methods platt sequential minimal optimization shows algorithm competitive 
index terms classification nearest point algorithm quadratic programming support vector machine 
years seen rise support vector machines svm powerful tools solving classification regression problems :10.1.1.117.3731
variety algorithms solving problems emerged 
traditional quadratic programming algorithms modifications chunking algorithm fact number support vectors usually small percentage total training set tried 
algorithms require enormous matrix storage expensive matrix operations 
overcome problems fast iterative algorithms easy implement suggested platt smo algorithm important example 
algorithms bound widely increase popularity svm practitioners 
contribution direction 
transforming particular svm classification problem formulation problem computing nearest point convex polytopes hidden feature space give fast iterative nearest point algorithm svm classifier design competitive smo algorithm 
smo algorithm quite straightforward implement 
pseudocode algorithm 
pseudocode actual running manuscript received may revised november 
keerthi department mechanical production engineering national university singapore singapore mail guppy mpe nus edu sg 
bhattacharyya murthy department computer science automation indian institute science bangalore india mail csa ernet 
publisher item identifier 
code developed short time 
fortran code obtained free authors noncommercial purpose 
basic problem addressed category classification problem 
denote input vector support vector machine denote feature space vector related transformation svm designs assume known computations done kernel function denotes inner product space 
simplest svm formulation allow classification violations 
training set input vectors 
index set class index set class 
assume define svm design problem violations 
svm nv assumption 
assumption exists pair constraints svm nv satisfied 
assumption holds svm nv optimal solution turns unique 
bounding hyperplanes separating classes 
margin svm nv consists finding pair parallel hyperplanes maximum margin pairs separate classes 
deal data linearly inseparable space purpose improving generalization need problem formulation classification violations allowed 
popular approach doing allow violations satisfaction constraints svm nv penalize violations linearly objective function ieee svm vl indexes run elements positive inverse regularization constant chosen give correct relative keerthi fast iterative nearest point algorithm support vector machine classifier design weighting margin maximization classification violation 
wolfe duality theory svm vl transformed equivalent dual problem :10.1.1.117.3731
svm vl dual computationally easy handle problem directly kernel calculations 
platt smo algorithm ways solving svm vl dual 
smo particularly simple time impressively fast 
simple terms iterative scheme appropriately chosen variables adjusted time improve value objective function 
need adjusting variables time caused presence equality constraint 
useful point wolfe dual svm nv svm vl dual set algorithm designed solving svm vl dual easily solve dual svm nv solve svm nv 
suggestion cortes vapnik explored sum squared violations cost function 
svm vq preliminary experiments shown formulation promising 
svm vl need include nonnegativity constraints reason 
suppose optimal solution svm vq negative resetting remain feasible strictly decrease cost 
negative values occur optimal solution 
pointed nice property svm vq doing simple transformation converted instance svm nv 
denote dimensional vector th component components zero 
define easy see svm vq transforms instance svm nv 
note presence variables svm vq feasible space nonempty resulting svm nv automatically feasible 
note denotes kernel function svm vq problem kernel function transformed svm nv problem zero 
pair training vectors modified kernel function easily computed 
main aim give fast algorithm solving svm nv 
main idea consists transforming svm nv problem computing nearest point convex polytopes carefully chosen nearest point algorithm solve 
algorithm easily solve svm vq 
algorithms smo solve svm vq 
empirical testing algorithm efficient solving svm vq smo way 
typical computational cost solving svm vl smo compared solving svm vq algorithm find algorithm competitive 
deriving inspiration algorithm designing hopfield nets mangasarian successive overrelaxation idea suggested inclusion extra term objective functions various formulations 
done computational simplicity mind 
added objective functions primal svm problems svm nv svm vl svm vq turns equality constraint dual problems gets eliminated 
easy give iterative algorithm improving dual objective function simply adjusting single time opposed adjustment variables required smo 
applied kernel algorithm solve svm nv applied svm vq mangasarian applied successive overrelaxation scheme solve svm vl 
basic algorithmic ideas follows choose determine unconstrained step size bounds clip step size updated satisfies bounds 
term included objective functions svm vq svm nv problems easily transformed problem computing nearest point single convex polytope origin problem simpler finding nearest distance convex polytopes 
nearest point algorithm mentioned earlier simplifies considerably simpler problem 
testing described shows simplified algorithms perform algorithms solve problems term 
problems entire kernel matrix computed stored memory useful 
organized follows 
section ii reformulate svm nv problem computing nearest point convex polytopes 
section iii discuss optimality criteria nearest point problem 
section iv derives simple check stopping nearest point algorithms ieee transactions neural networks vol 
january get guaranteed accuracy solution svm nv 
main algorithm derived sections vi 
classical algorithms doing nearest point solution due gilbert mitchell combined modified derive fast algorithm 
comparative computational testing algorithm platt smo algorithm taken section vii 
conclude closing remarks section viii 
ii 
reformulation svm nv nearest point problem set denote convex hull set convex combinations elements definition svm nv 
finite sets convex polytopes 
consider generic problem computing minimum distance 
npp easily noted solution npp may unique 
rewrite constraints npp algebraically equivalence problems svm nv npp easily understood studying geometry shown fig 

assumption equivalent assuming nonintersecting 
implies optimal cost npp positive 
denotes solution svm nv denotes solution npp facts maximum margin easily derive relationship solutions svm nv npp theorem states relationship formally 
theorem solves svm nv exist solves npp holds 
direct geometrically intuitive proof keerthi geometrical problem robotics 
bennett proved somewhat close result context learning algorithms 
give discussion follows traditional wolfe dual approach employed svm literature 
main reason doing fig 

pairs parallel hyperplanes separate classes pair largest margin normal direction pair closest points note chosen apa 
show relationships npp variables wolfe dual svm nv variables 
wolfe duality theory transform svm nv equivalent dual problem :10.1.1.117.3731
svm nv dual implies introduce new variable rewrite constraints define svm nv dual rewritten 
formulation convenient optimize keeping constant optimize outer loop 
optimizing respect yields keerthi fast iterative nearest point algorithm support vector machine classifier design substituting simplifying see equivalent 
problem equivalent problem 
define matrix columns easy note satisfies constraints equivalent npp 
wolfe duality theory yields immediately verify expression :10.1.1.117.3731
expression easily understood geometry 
discussion points important fact simple redundancy svm nv dual formulation gets removed npp formulation 
note npp equality constraints svm nv dual number variables 
quadratic programming algorithms applied svm nv equivalent 
iii 
optimality criteria npp give definitions concerning support properties convex polytope 
compact set define support function see fig 

denote solution satisfies consider case convex polytope known maximum linear function convex polytope attained extreme point 
means determined simple enumeration inner products fig 

definition support properties 
extreme vertex direction distance hyperplanes equal aa 
provides simple procedure evaluating support properties convex polytope define function follows adapt general definitions derive optimality criterion npp 
simple geometrical analysis shows pair solves npp words solves npp equivalently optimal define function nonnegative functions follows optimal theorem states results related ones formally 
theorem suppose hold 
point satisfies point line segment point satisfies point line segment solves npp proof follows 
define real variable function describes variation generic point varies line segment joining 
result follows 
proof similar lines 
wehave ieee transactions neural networks vol 
january fig 

situation satisfying 
subtracting inequalities get optimal npp 
hand optimal npp set results show iv 
stopping criteria npp algorithms numerical algorithm employed solve problems mentioned earlier approximate stopping criteria need employed 
svm design dual formulation svm nv dual npp solved primal svm nv computational ease 
kept mind primal solution final interest care needed ensure numerical algorithm reached approximate primal solution guaranteed specified closeness optimal primal solution 
easy give example show simply stopping dual approximate solution reached dual optimal solution accuracy dangerous far solution svm nv concerned 
suppose algorithm npp iteratively improves approximate solution way solution npp 
recall function defined 
part theorem fact continuous function assumption suppose specified accuracy parameter satisfying desired algorithm pair feasible svm nv optimal solution svm nv 
show possible dual algorithm satisfies fig 
geometrically depicts situation 
margin corresponding direction clearly note determination consistent feasibility svm nv easily setting choosing get equation central separating hyperplane form easily get expression fact get derive bound note proved important result 
theorem solution npp optimal solution svm nv 
suppose holds 
defined feasible svm nv particularly interested getting bound cost function svm nv easily obtained similar bound obtained discussions point important fact effectively terminate numerical algorithm solving npp 
iterative algorithms npp npp studied literature number algorithms 
best general purpose algorithms npp wolfe algorithm terminate finite number steps require expensive matrix storage matrix operations step unsuitable large keerthi fast iterative nearest point algorithm support vector machine classifier design svm design 
interesting point active set method kaufman solve svm identical wolfe algorithm solve svm nv 
iterative algorithms need minimal memory memory size needed linear number training vectors reach solution asymptotically number iterations goes infinity better suited svm design 
section take algorithms investigation 
gilbert algorithm gilbert algorithm algorithms suggested solving npp 
originally devised solve certain optimal control problems 
modifications pattern recognition robotics 
section briefly describe algorithm point adapted svm classifier design 
definition npp 
denote minkowski set difference clearly npp equivalent minimum norm problem mnp npp problem unique solution 
mnp npp simpler problem npp superficially 
convex polytope set points see take general points representation write note fact general possible vertices 
large definitely idea form define 
gilbert algorithm method solving mnp require explicit formation steps require evaluations support properties fortunately functions computed efficiently computation requires time 
optimality criterion mnp easily derived section iii 
stated theorem 
omit proof lines proof theorem 
function applied plays key role 
theorem suppose hold 
point point line segment satisfying 
solves mnp gilbert algorithm results theorem 
suppose start solution mnp 
hand satisfies part theorem searching line segment connecting yields point norm smaller observations yield gilbert algorithm 
gilbert algorithm solving mnp choose compute set compute point line segment joining norm set go back step 
efficient algorithm computing point norm line segment joining points appendix 
gilbert showed algorithm step finite number iterations asymptotically number iterations goes infinity 
adapt gilbert algorithm svm design important note represented form need stored maintained order represent maintaining updating cache inner products improves efficiency 
details 
implemented gilbert algorithm tested performance variety classification problems 
algorithm rapid movement solution initial iterations problems slow approached final solution 
suppose gets picked algorithm initial stages needed representing final solution algorithm slow driving corresponding zero 
reasons gilbert algorithm efficient solving npp arise svm classifier design 
ieee transactions neural networks vol 
january fig 

definition 
fa ea 
mitchell dem algorithm years gilbert mitchell independently suggested new algorithm mnp 
refer algorithm mdm algorithm 
gilbert algorithm mdm algorithm fundamentally uses representation basic operation 
clear condition automatically holds 
approximate point situation different 
point close exist small 
efficiency representation algorithm performance idea eliminate representation mind define function see fig 
clearly iteration mdm algorithm attempts decrease gilbert algorithm tries decrease seen geometrically fig 

mdm algorithm tries crush total slab zero gilbert algorithm attempts push lower slab zero 
essential difference algorithms 
note satisfying mdm algorithm better potential quickly eliminate representation 
describe main step mdm algorithm 
index reduction mind mdm algorithm looks improvement norm value direction easy see moving strictly decrease representation decreases movement limited interval defined define basic iteration mdm algorithm consists finding point minimum norm line segment joining fig 

illustrating various ideas improvement 
gilbert line segment mdm line segment parallel ef triangle triangle fy quadrilateral efe triangle 
implemented tested mdm algorithm 
works faster gilbert algorithm especially stages approaches algorithms faster mdm algorithm designed observations easy combine ideas gilbert algorithm mdm algorithm hybrid algorithm faster working directly space located easy find just elements modify time keep essence hybrid algorithm mentioned 
section describe ideas hybrid algorithm 
take details final algorithm incorporating observations section vi 
hybrid algorithm careful look mdm algorithm shows main computational effort iteration associated computation determination updating inner products cache 
quite afford remaining steps iteration little complex provided leads gain disturb determination 
gilbert algorithm mainly requires computation anyway computed mdm algorithm possible easily insert main idea gilbert algorithm mdm algorithm improve 
number ideas emerge attempt 
describe ideas comment goodness 
situation typical iteration shown fig 

take representation loss generality assume index 
equal include set corresponding zero affecting representation anyway 
points respectively shown 
point mdm algorithm shown idea step iteration mdm algorithm take point minimum norm triangle formed keerthi fast iterative nearest point algorithm support vector machine classifier design algorithm computing point minimum norm triangle appendix 
line segment connecting mdm algorithm finds minimum point part idea locally perform terms decreasing norm mdm algorithm 
note minimum norm point expressed linear combination cost updating inner products cache mdm algorithm 
define clearly point obtained removing representation doing renormalization 
define see fig 

follows lies line segment joining idea step iteration mdm algorithm take point minimum norm triangle formed contains idea produce norm equal produced idea 
carry idea generate ideas 
point intersection line joining line joining easily checked point obtained removing indexes representation doing renormalization denote quadrilateral formed convex hull triangle formed clearly lines earlier argument easy show idea step iteration mdm algorithm take point minimum norm idea step iteration mdm algorithm take point minimum norm implemented tested ideas 
idea gives improvement mdm algorithm idea improves little 
performance idea performs somewhat worse compared idea idea performs 
clear explanation performances 
basis empirical observations recommend idea best modifying mdm algorithm 
vi 
fast iterative algorithm npp section give algorithm npp directly space located 
key idea motivated considering gilbert algorithm 
approximate solutions iteration say index satisfies condition suppose satisfies 
point line segment joining closest relation appendix get appreciable decrease objective function npp parallel comment satisfies 
hand unable find index satisfying recall definitions section iii get theorem way stopping 
observations lead generic algorithm gives ample scope generating number specific algorithms 
efficient algorithm describe soon special instance 
generic iterative algorithm npp choose set find index satisfying index approximate optimality criterion satisfied 
go step 
choose convex polytopes compute pair closest points minimizing distance set go back step 
suppose step choose line segment joining choose line segment joining algorithm close spirit gilbert algorithm 
note index plays role iteration gilbert algorithm requires indexes define similar spirit appropriately choosing various ideas section extended involving indexes 
discuss detailed algorithm prove convergence generic algorithm 
theorem generic iterative algorithm terminates step satisfying finite number iterations 
proof easy 
note holds way chosen step 
assumption uniformly bounded zero uniform decrease iteration 
minimum distance ieee transactions neural networks vol 
january nonnegative iterations terminate finite number iterations 
consider situation step index satisfying available 
suppose algorithm maintains representations refer support vector consistent terminology adopted literature 
define follows see fig 
redundant stated conditions stress importance 
numerical implementation index having occur numerically 
indexes regularly cleaned best keep positivity checks 
number ways combining index doing operations ideas similar section create variety possibilities implemented tested promising ones empirically arrived final choice describe detail 
set follows step carried cases depending see fig 
geometric description cases 
case point obtained removing representation choose triangle formed case point obtained removing representation choose triangle formed case choose line segment joining line segment joining 
case choose line segment joining line segment joining 
defines basic operation algorithm 
efficient algorithms doing nearest point computations involving triangle line segments appendix fig 

illustration 
fig 

geometric description cases 
cases cases 
tremendous improvement efficiency achieved doing things maintaining updating caches variables interchanging operations support vector vector sets 
ideas directly inspired platt smo algorithm 
go details 
discuss need maintaining cache variables 
look shows important variables 
variables computed scratch expensive example computation single requires evaluation kernel computations computation number elements compute repeated times 
important maintain caches variables 
define cache variables vectors 
algorithm spends operation adjusting keerthi fast iterative nearest point algorithm support vector machine classifier design support vectors appropriate bring vectors rarely check satisfaction optimality criteria 
case efficient maintain indexes compute quantities scratch needed 
mentioned paragraph important spend iterations mind define types loops 
type loop goes sequentially choosing time checking condition satisfied doing iteration step generic algorithm 
type ii loop operates doing iterations times certain criteria met 
platt smo algorithm iterations run support vector index time adopted different strategy type ii loop 
define follows 
efficiently computed mainly caching basic type ii iteration consists determining doing iteration iterations continued approximate optimality criteria satisfied support vector set early stages algorithm say rounds nearly accurate set support vector indexes get identified 
happens wasteful spend time doing type ii iterations 
adopted strategy 
point entry type ii loop percentage difference number support vectors compared value previous entry greater limit number type ii iterations total number training pairs problem 
done roughly cost type type ii loops equal 
case useful limit number type ii iterations algorithm stopped holds causing algorithm exit type ii loop type loop follows satisfying condition full details concerning actual implementation full algorithm pseudocode algorithm 
extremely easy develop working code short time 
vii 
computational experiments section empirically evaluate performance nearest point algorithm npa described section 
table properties data sets platt smo algorithm currently fastest algorithms svm design compare npa smo 
apart comparison computational cost compare algorithms generalize 
implemented algorithms fortran ran mhz pentium machine 
smo implemented exactly lines suggested platt 
compared methods number problems report representative performance benchmark problems checkers data uci adult data 
created checkers data generating random set points checkers grid 
adult data set taken platt web page 
case adult data set inputs represented special binary format platt testing smo 
study scaling properties training data grows platt staged experiments adult data 
data fourth seventh stages 
training exactly sets platt 
validation testing subsets large validation test sets platt 
gaussian kernel experiments 
values employed dimension input sizes training validation test sets table values table chosen follows 
adult data values platt experiments smo checkers data chose suitably get generalization 
applied algorithms training sets compared computational costs 
algorithms apply different svm formulations smo solves svm vl npa solves svm vq 
proper cross comparison methods 
denote final margin obtained solution solution space 
data set chose different ranges values way roughly cover range values 
may noted inverse relationship ran methods bunch values sampled ranges 
study important reason 
particular method svm design usually unknown chosen trying number values validation set 
fast performance method range values important 
particular method performance gets revealed clearly experiments 
common stopping criterion chosen methods see details 
ieee transactions neural networks vol 
january fig 

checkers data cpu time seconds shown function margin fig 

adult data cpu time seconds shown function margin various datasets figs 
show variation computing cost measured cpu time seconds function margin average smo npa perform quite closely 
small values large values npa better smo indicates better method solving svm nv 
higher values algorithms equally efficient 
performance adult datasets indicates npa scales large size problems smo 
table ii gives rough idea variation number support vectors various combinations datasets algorithms 
linear violation formulation smo results smaller number support vectors compared quadratic violation formulation npa 
difference small checkers dataset difference prominent adult datasets especially high small values 
difference occurs particular problem linear formulation advantage svm classifier inference design process 
training values studied generalization properties svm classifiers designed smo fig 

adult data cpu time seconds shown function margin fig 

adult data cpu time seconds shown function margin npa 
combination algorithm dataset corresponding validation set choose best value 
adult dataset experiments large computing times involved choosing optimal values validation set 
classifier corresponding best value performance percentage misclassification test set evaluated 
results table iii 
clearly classifiers designed smo npa give nearly performance 
viii 
developed new fast algorithm designing svm classifiers carefully devised nearest point algorithm 
comparative performance algorithm smo algorithm number benchmark problems excellent 
performance studies done done indicate svm classifier formulation quadratically penalizes classification violations worth considering 
smo algorithm keerthi fast iterative nearest point algorithm support vector machine classifier design table ii number support vectors 
numbers representative values 
smo values item number open interval number respectively 
npa value number non zero table iii percentage misclassification test set algorithm quite straightforward implement indicated pseudocode 
nearest point formulation algorithm special classification problems svm regression 
appendix line segment triangle algorithms appendix give useful algorithms computing nearest point origin line segment computing nearest point origin triangle 
consider problem computing nearest point line segment joining points origin 
expressing terms single real variable minimizing respect easy obtain expression optimal 
easy derive expression optimal value 
furthermore suppose condition holds consider cases 
case case case get algebra finite 
points polytope bound exists 
get combining cases get consider problem computing nearest point triangle joining points origin 
numerically robust compute minimum distance origin edges triangle minimum distance origin interior triangle minimum exists take best values 
distances computed line segment algorithm described 
consider interior 
done computing minimum distance origin dimensional affine space formed testing nearest point lies inside triangle 
setting gradient zero solving yields clearly minimum point affine space lies inside triangle case nearest point square minimum distance adaptive perceptron algorithm europhys 
lett vol 
pp 

ieee transactions neural networks vol 
january barr efficient computational procedure generalized quadratic programming problem siam contr vol 
pp 

bennett bredensteiner geometry learning dept math 
sci polytechnic inst troy ny tech 
rep 
bennett mangasarian robust linear programming discrimination linearly inseparable sets optimization methods software vol :10.1.1.117.3731
pp 

burges tutorial support vector machines pattern recognition data mining knowledge discovery vol :10.1.1.117.3731

cortes vapnik support vector networks machine learning vol 
pp 

fletcher practical methods optimization nd ed 
new york wiley 
cristianini campbell kernel algorithm fast simple learning procedure support vector machines proc 
th int 
conf 
machine learning san mateo ca 
support vector networks kernel bias soft margin univ sheffield dept automat 
contr 
syst 
eng sheffield tech 
rep 
gilbert minimizing quadratic form convex set siam contr vol 
pp 

gilbert johnson keerthi fast procedure computing distance complex objects dimensional space ieee robot 
automat vol 
pp 

joachims making large scale support vector machine learning practical advances kernel methods support vector machines sch lkopf burges smola eds 
cambridge ma mit press dec 
kaufman solving quadratic programming problem arising support vector classification advances kernel methods support vector machines sch lkopf burges smola eds 
cambridge ma mit press dec 
keerthi bhattacharyya murthy 
mar fast iterative nearest point algorithm support vector machine classifier design 
intell 
syst 
lab dept comput 
sci 
automat indian inst 
sci bangalore india 
online 
available guppy mpe nus edu sg lawson hanson solving squares problems 
englewood cliffs nj prentice hall 
lay convex sets applications 
new york wiley 
fast computation distance line segments inform 
processing lett vol 
pp 

mitchell dem finding point polyhedron closest origin siam contr vol 
pp 

mangasarian successive overrelaxation support vector machines computer sciences dept university wisconsin madison wi tech 
rep 
platt fast training support vector machines sequential minimal optimization advances kernel methods support vector machines sch lkopf burges smola eds 
cambridge ma mit press dec 
adult web datasets platt 
online 
available www research microsoft com keerthi computation certain measures proximity convex polytopes complexity viewpoint proc 
ieee int 
conf 
robot 
automat nice france pp 

smola sch lkopf tutorial support vector regression royal holloway college london neurocolt tech 
rep tr 
spirals data online 
available ftp ftp cs cmu edu pub neural bench bench spirals tar gz uci machine learning repository online 
available www ics uci edu mlearn mlrepository html vapnik estimation dependences empirical data 
berlin germany springer verlag 
vapnik nature statistical learning theory 
new york springer verlag 
wisconsin breast cancer data online 
available ftp pub machine learning databases breast cancer wisconsin wolfe finding nearest point polytope math 
programming vol 
pp 

keerthi received bachelor degree mechanical engineering rec university madras india master degree mechanical engineering university missouri ph degree control engineering university michigan ann arbor 
working year applied dynamics international ann arbor doing real time simulation joined faculty department computer science automation indian institute science bangalore april 
academic research covered areas geometric problems robotics algorithms dynamic simulation multibody systems nonholonomic motion planning neural networks 
joined control division department mechanical production engineering national university singapore may associate professor 
published papers leading international journals conferences 
received sc eng 
degree indian institute science 
currently department computer science automation indian institute science 
research interests include algorithms neural networks pattern recognition 
bhattacharyya received degree electrical engineering indian institute science 
currently ph student department computer science automation indian institute science 
research interests lie belief networks mean field theory neural networks 
murthy received degree systems science automation indian institute science 
currently ph student department computer science automation indian institute science 
interests information filtering machine learning neural networks 
