neurocomputing www elsevier com locate general framework barbara hammer alessio micheli alessandro sperduti marc research group lnm department mathematics computer science university germany dipartimento di informatica universita di pisa pisa italy dipartimento di matematica ed universita degli studi di padova padova italy self organization constitutes important paradigm machine learning successful applications data mining 
approaches processing data nite dimensional vector space 
article focus extensions general data structures sequences tree structures 
various modi cations standard self organizing map som sequences tree structures literature temporal kohonen map recursive som som structured data 
methods enhance standard som utilizing recursive connections 
de ne general recursive dynamic article provides recursive processing complex data structures recursive computation internal representations context 
soms structures special cases proposed general dynamic 
furthermore dynamic covers supervised case recurrent recursive networks 
general framework ers uniform notation training mechanisms hebbian learning 
transfer computational alternatives vector quantization neural gas algorithm structure processing networks easily achieved 
formulate general cost functions corresponding vector quantization neural gas modi cation som 
cost functions compared hebbian learning interpreted approximation stochastic gradient descent 
comparison derive exact gradients general cost functions 
elsevier rights reserved 
keywords self organizing map kohonen map recurrent networks som corresponding author 
tel fax 
mail address hammer informatik uni de hammer 
research done author visiting university pisa 
groups padua pisa siena warm hospitality stay 
see front matter elsevier rights reserved 
doi hammer neurocomputing 
neural networks constitute particularly successful approach machine learning allows learn unknown regularity set training examples 
deal tasks outputs classes data points available network learn assign input data appropriately correct class supervised case 
alternatively prior information classes known andthe network extract useful information classes data 
naturally task di cult notion useful information depends context 
results hard evaluate automatically experts respective eld 
task processing occurs areas application explicit teacher information available data web mining bioinformatics text categorization name just topics 
context neural networks approaches deal nite dimensional vectors inputs 
areas interest time series prediction speech processing bioinformatics chemistry theorem proving data sequences trees graphs 
data require appropriate preprocessing cases important features simple vector representation 
preprocessing usually domain dependent time consuming 
loss information inevitable 
ort done derive neural methods deal structured data directly 
supervised scenario various successful approaches developed neural networks constitute modeling sequential data language processing time series prediction 
naturally networks complex data structures tree structures graphs dealt 
symbolic terms possess tree representation generalization successfully various areas symbolic structures arise theorem proving chemistry image processing natural language processing 
training recursive networks standard backpropagation time 
important theoretical investigations neural networks recursive networks 
alternative important paradigm neural networks successfully data mining see 
additional structural information available possible applications self organizing maps soms transfer standard unsupervised learning methods sequences complex tree structures valuable 
approaches sequences som constitutes metric applied directly structured data data comparison de ned notion adaptation data space 
proposed 
various approaches alternatively recurrent dynamics leaky integrators general recurrent connections allow recursive processing hammer neurocomputing sequences 
examples temporal kohonen map tkm recursive som approaches 
som constitutes recursive mechanism capable processing tree sequences 
alternative models unsupervised time series processing example hierarchical network architectures 
overview important models 
focus models recursive dynamics structured data derive generic formulation recursive self organizing maps 
propose general framework transfers idea recursive processing complex data networks 
general framework covers tkm andthe 
methods share basic recursive dynamic di er way structures internally neural map 
tkm andthe standard som appropriate choice internal representations general framework 
dynamic networks general framework 
approaches slight variations parts framework 
obtain uniform formulation allows possible learning algorithms properties important approaches literature soms recurrence 
usually hebbian learning 
general formulation allows formalize hebbian learning uniform manner immediately transfer alternatives neural gas algorithm vector quantization existing approaches 
neural gas hebbian learning approximately interpreted stochastic gradient descent appropriate error function 
uniformly formulate analogous cost functions general framework structural self organizing maps investigate connection hebbian learning 
turns hebbian learning approximation gradient mechanism contributions substructures discarded 
exact gradient mechanism includes recurrent neural network training special case formulae comparable backpropagation time real time recurrent learning derived unsupervised case 
gives hints understanding dynamics unsupervised network training rst step general theory unsupervised recurrent networks 
de ne general framework formally show soms recursive dynamics literature special cases general framework 
show hebbian learning formulated approach 
relate hebbian learning alternative training methods energy functions popular methods eld directly general framework methods 
hammer neurocomputing 
structure processing self organizing maps rst clarify notation term self organizing map literature refers paradigm neural system learns self organizing fashion andthe speci successful self organizing map proposed kohonen 
order distinguish meanings refer speci architecture kohonen som 
speak self organization general paradigm 
som kohonen biologically network learns hebbian learning topological representation data distribution examples 
assume data taken real vector space rn euclidian metric 
som de ned set neurons nn neighborhood structure neurons nh determined regular lattice structure neurons ni nj direct neighbors nh ni nj directly connected lattice 
neurons nh ni nj re ects minimum number direct connections needed link ni nj 
dimensional lattice ers possibility easy visualization data mining applications 
neuron ni weight wi rn represents corresponding region data space 
set training patterns ad rn weights neurons adapted hebbian learning including neighborhood cooperation weights wi represent training points accurately possible andthe topology neurons lattice matches topology induced data points 
precise learning rule intuitive repeat choose ai random compute neuron ni minimum distance ai wi ai wj adapt wj wj nh nj ni ai wj nh nj ni learning rate maximum winner nj ni decreasing neurons nj direct neighbors winner ni 
form nh nj ni exp nh nj ni possibly adding constant factors term 
incorporation topology learning rule allows adapt winner neighbors training step 
training som dynamic pattern rn map computes winner neuron smallest distance wj weight respectively 
allows identify new data point learned prototype 
starting winning neuron map traversal reveals similar known data 
popular alternative self organizing algorithms vector quantization vq andthe neural gas algorithm ng 
vq aims learning representation data points topology preservation 
case andthe learning rule adapts winner step repeat choose ai random compute neuron ni minimum distance ai wi ai wj adapt wi wi ai wi hammer neurocomputing rate 
order data representation caused topological defects ng pose topological constraints neural arrangement 
de training data 
recursive update reads repeat choose ai random compute distances ai wj adapt wj wj rk ai wj rk denotes rank neuron nj distances ai wj number neurons nk ai wk ai wj holds 
rk function maximum values larger numbers 
rk exp rk possibly additional constant factors 
results closest neuron adapted neurons adapted distance data point 
respective order neurons respect training point determines current neighborhood 
eventually neurons neurons closest data point neighbored 
infer data adapted longer regular lattice training way preserves topology data space 
exist various possibilities extending self organizing maps deal alternative data structures simple vectors interesting line research deals adaptation self organizing maps qualitative variables euclidian metric directly 
article particularly complex discrete structures sequences 
article provides overview self organizing networks proposed processing spatio temporal patterns :10.1.1.41.7128
naturally common way processing structured data self organizing mechanisms relies adequate preprocessing sequences 
features nite dimensional vector space standard pattern recognition methods 
simple way sequence representation time window data 
method yields large dimensions som standard euclidian metric su ers curse dimensionality methods adapt metric example advisable 
hierarchical adaptive preprocessing methods involve soms various levels websom approach document retrieval 
self organizing algorithms immediately arbitrary metrical structures standard euclidian metric alternatively de ne complex metric adapted structured data adopting complex data preprocessing 
soms equipped edit distance constitute example 
data structures discrete space real vector space approaches 
case specify additionally weights neurons adapted 
edit distance dealt perform operations transform actual weight neuron data structure example 
uni cation done operations order need unique 
methods hammer neurocomputing propose representation complex data structures som complex patterns activation neuron 
activation patterns arise recursive processing appropriate decaying blocking activations 
alternative methods lines 
interested approaches equip som additional recursive connections recursive processing recursive data structure possible 
particular prior metric preprocessing chosen similarity structures evolves recursive comparison single parts data 
various approaches literature 
deal sequences real vectors input data 
approach proposed general tree structures including sequences special case 
shortly summarize important recursive dynamics 
sequences entries denoted 
denotes empty sequence 
trees labels denoted pre notation tk denotes label root tk subtrees 
empty tree denoted 
temporal kohonen map temporal kohonen map chappell extends som recurrent self connections neurons neurons act leaky integrators 
sequence neuron ni weight wi di aj wi constant determines integration context information winner computed 
formula form leaky integrator integrates previous distances neuron ni entries sequence 
neuron winner weight close data point addition exponentially previous entries sequence weight small 
obviously alternative recursive formalization di neuron jth time step di aj wi di di 
training tkm hebbian learning time step weights wi adapted time step respect current input aj standard som update rule 
winner neuron leaky integration formula 
recurrent som rsom de ned example uses similar dynamic 
integrates directions deviations weights 
activation di element recursively di di aj wi di 
winner determined neuron smallest smallest di naturally procedure stores information tkm 
hebbian learning time step tkm 
alternative direct training method som evaluates condition optimum hammer neurocomputing weight space derivative quantization error zero 
quadratic equation yields solutions optimum weights wi 
apart sequence recognition tasks models successfully applied learning motion sensitive maps visual cortex 

recursive som recursive som mechanism sequence prediction 
symbols sequence recursively 
neuron weight wi additionally context vector ci stores activation pro le map indicating sequential context vector wi 
sequence activation neuron ni time step de ned di di aj wi exp exp dn ci constants control mediation amount pattern match versus context match 
respective symbol weights ai 
addition computed context activation map previous time step match context ci neuron ni neuron winner 
comparison contexts done involving exponential function exp order avoid numerical explosion 
exponential transform included activation di huge distances respect components context 
sequence recognition mackey glass time series recognition sequences 
training done hebbian learning cases weights wi neurons contexts ci 
parameters wi ci recursively neuron ni adapted current input aj andthe recursively neighbors ni adapted accordingly smaller learning rate 
shows di erentiating input sequences 
winners represent di erent sequences training 

som structured data som processing 
speci case covers sequences 
neurons rectangular dimensional lattice structure 
neurons tuples id nd ni nd neuron weight context vectors rd represent context subtrees andthe indices respective winners 
index winning neuron tree root label tk hammer neurocomputing recursively de ned tk argmin tk respective context label tree winners subtrees 
empty tree index lattice 
weights neurons consist compact representations trees prototypical labels winner indices represent subtrees 
starting leaves winner recursively entire tree 
hebbian learning 
starting leaves index winner subtrees computed 
attached weights direction tk recursive processing step ti denotes winning index subtree ti currently tree 
updated direction smaller learning rate 
classi cation arti cially tree 
learning representation pictures emerges groups pictures described similarly similar labels 
pictures generated plex grammar ships houses various shapes 
capable properly arranging objects categories houses 
clusters di erentiation respect involved features 
basedon clustering cation accuracy objects attaching appropriate classes nodes 

general framework dynamic main idea general framework models derived observation models share basic recursive dynamics 
di er way tree structures sequences respectively internally respect recursive processing weights neurons 
models deal sequences real vector space fan convenience shortly introduce data structures formally 
assume set labels taken 
practice embedded real vector space 
sequences refer objects ai denotes length sequence 
empty sequence dealt 
sequences considered trees fan generally trees fan consist empty tree root node labeled element subtrees tk empty 
tree tk 
root node parent root nodes tk andthe root nodes tk referred children root node nodes tree empty subtrees leaves 
sequence st corresponds hammer neurocomputing fig 

example trees 
empty children omitted 
left tree fan right fan 
labels taken 
pre notation left tree resp 
empty trees presentation 
right omitting empty trees 
tree st st element sequence taken root tree 
note terms logical formulas domains chemistry games naturally tree structures 
trees representing terms depicted fig 

simplicity focus binary trees labels set node children 
generalization trees fan particular sequences trees fan obvious 
labels trees may origin arbitrary set real vector space discrete set labels 
denote set binary trees labels pre notation single parts tree represents tree root label 
empty tree denoted type data processed unsupervised fashion 
de ne basic ingredients andthe basic dynamic functionality map 
anda new input tree winner tree computed 
general dynamic special case includes models described tkm canonic formulation hebbian learning section 
processing dynamic main ingredients de ne general som short set labels similarity measure dw usually images non negative real numbers 
set formal representations trees similarity measure dr image usually non negative numbers 
denote representation empty tree adequate formal representation trees computed 
set neurons self organizing map assume enumerated simplicity 
weight function attaches weight neuron 
neuron consists map triple consisting prototype label formal representations 
hammer neurocomputing representation mapping rep rn maps vector activations neurons formal representation 
idea ingredients follows tree processed iteratively starting leaves information root reached 
step comparison context information resulting processing subtrees taken account 
neuron map proper representation entire tree rst part weight neuron represents properly represent correct contexts 
similarity measure dw directly 
formal descriptions contexts pointers reduced description context respectively 
purpose iteratively context activation neurons map 
activation formal description context representation mapping rep comparison output ci possible metric dr formal representations 
far verbal description formalized de nition allows compute recursive similarity activation node ni tree ni dw ni dr ni dr ni tj rj rep tj tj nn weighting factors 
refer dynamic generalized som structured data short 
choice determines importance proper context comparison correct root label representation 
set enables precise notation trees internally map 
function rep constitutes interface maps activity internal representations trees 
recursive similarity yields activation neurons input tree 
applying rep obtain formal representation tree 
order resulting map example information storing recovering determine neuron highest responsibility neuron res winner input tree 
neuron highest lowest recursive similarity depending meaning depending fact dot products distances computation similarities 
picture explains processing recursive step fig 

note labels input trees come proper subset discrete data processed representations neurons lie real vector space 
dw squared standard euclidian metric induced metric point need special properties dw hammer neurocomputing rep rep rep rep fig 

recursive processing step tree distance neuron weighted distances andthe distances respectively representations representations processing trees applying rep pro le map 
formal representations trees compact way nite dimensional vector space 
example chosen simply index neuron best recursive similarity tree processed organizing map 
idea distributed representation tree emerges processing tree map 
formal description pointer activity pro le 
basedon assumption compare trees comparing formal descriptions 
neurons prototypes trees root label subtrees 
formal representations 
note introduce lattice topological structure neurons point 
de nition topology self organizing map ect recursive similarity neurons tree 
topological structure training 
addition prove bene cial speci applications dimensional neighborhood structure example allows appropriate visualization map 
hammer neurocomputing mapping rep maps recursively yields activity pro le neurons formal description trees 
simply identity appropriate cation example 
longer symbolic tree representation connectionist description activation 
obviously easily de ned trees fan 
particular case sequences included 
considering trees fan weight function form lk contexts neurons corresponding fan recursive processing reads tk ni dw ni dr ni dr rk lk ni tj rj rep tj tj nn de nition captures approaches structure processing selforganizing networks 
prove fact case binary input trees 
transfer sequences obvious 
theorem 
includes dynamic som tkm appropriate choices proof 
som som choose dr 
context available labels root trees elements respectively taken account 
tkm tkm ingredients follows de ne dw metric 
explicitly stores activation neurons tree processed denotes number neurons 
similarity measure dr dot product 
representation empty tree vector 
weights neurons special form neuron ni ni rst parameter may arbitrary value training 
contexts coincide unit vector position 
context neurons particularly simple structure 
think context focus neuron looks activation processing tree care global activation produced tree 
mathematically implemented dot product dr context unit vector neuron 
rep simply identity reduction takes place 
hammer neurocomputing recursive dynamic reduces computation ni ni ni ni choose case sequences just leaky integrator de ned 
analogous dynamic results tree structures involve global context processing focuses activation single neuron 
winner neuron smallest value de ne de ne dw metric 
de ne number neurons self organizing map 
dr squared euclidian metric 
formal description tree merely identical activation neurons self organizing map 
reduction respect dimensionality takes place 
representation empty tree origin 
neurons lattice approach arrangement ect processing dynamics 
choose rep rep rep xn exp exp xn 
exponential function introduced stability reasons 
hand prevents similarities blowing recursive processing hand scales similarities nonlinear fashion small similarities getting close zero 
noise disturb computation due large dimensionality way 
information lost application exponential function 
de nitions lead recursive formula ni ni ni ni rj exp tj exp tj nn restrict fan get dynamic rec som 
winner neuron smallest value choose de ne dw metric 
real vector space contains set indices neurons self organizing map 
neurons lie dimensional lattice dr squared euclidian distance lattice points 
addition representation empty tree singular point outside lattice 
neurons topological structure dimensional lattice 
weighting attaches appropriate values neurons 
weights hebbian learning 
note neurons enumeration basedon lattice elements nd 
choose rep rep follows input domain rep set denotes number neurons identi ed nd reps maps vector similarities index neuron best similarity rep nd smallest input 
note unique 
hammer neurocomputing assume ordering neurons 
case multiple optima assume implicitly rst optimum chosen 
de nition obtain general formula ni ni ni ni recursive similarity non empty trees indices neurons winners respectively 
winner map neuron smallest note examples corresponds distance 
standard som input neurons compute activation measures distance respective input 
winner determined neuron smallest distance 
note interpretation distance common generally similarity activation neurons input signal 
general networks included framework seen argument 
various di erent dynamics discrete time recurrent neural networks rnn literature 
models simulated simple elman dynamic ref 

assume sequences entries dealt 
neurons nn weights wi rn denote tanh hyperbolic tangent 
activation neurons sequence tanh tanh denotes dot product vectors 
linear function added order obtain desired output rnns neural networks deal tree structures 
introduce dynamic binary trees labels neurons wi wi rn activation recursively tanh tanh linear function added obtain nal output 
theorem 
formulated dynamic 
hammer neurocomputing proof 
choose recursive computation ingredients rn dw dot product 
rn stores input net neurons dr dot product 
neurons weights ni wi ni wi ni wi 
rep xn tanh tanh xn 
way obtain net neurons recursive similarity tanh tanh denotes component wise application hyperbolic tangent 
substitute computation nal winner function tanh possibly added linear function obtain processing dynamic recurrent networks dynamic 
allow activations neurons general simple dimensional vectors corresponding distances 
dimensionality activation vector neurons chosen rl dr dw chosen dimensional vectors 
case sum dw dr dr denotes scalar multiplication addition vectors 
setting allows model rsom example tkm stores leaky integration input directions integration distances 
obtain analog rsom binary trees choose rn ni ni ei ei ith unit vector rn dr rep identity dw 
recursive dynamic ni ni ni ni 
fan recursive formula rsom 
course computation combination similarities general simple possible 
case computation dw dr dr rl mapping rl maps inputs actual andthe weights neuron new activation neuron 
general mechanism example model feature map self organizing temporal pattern recognizer 
som wij neuron variant temporal hebbian learning 
input sequence neuron ni active neuron smallest distance actual pattern addition weighted sum activations previous time step weights wji larger certain threshold may happen winner sequence known 
corresponds fact neuron smallest distance active due small support previous time step 
computation appropriate choice dimensional neuron rst part storing current distances second part storing fact neuron nally active 
hammer neurocomputing activates step neuron minimum distance currently neurons lying activated neuron bene winner easily 
modeled storing bias neuron adapted neuron winner gradually decreases time 
bias subtracted current distance time step ective winner neuron smallest distance largest bias 
dynamic easily modeled framework states neurons dimensional storing respective distance actual bias neuron chosen appropriately 
focus dynamics states neurons dimensional distances similarity values speci form de ned 
discuss hebbian learning learning energy function 
describe concrete setting binary trees 
methods rely reasonable contexts winner exponentially entire map respectively 
seen prototypical mechanisms di erent degree information compression stores winner stores activity pro le due exponential transform focuses winners 
describe concrete setting tkm mechanism respect capability tree recognition local contexts tkm su cient task storing trees arbitrary depth 

hebbian learning order map assume nite set training data td 
neural map training data represented accurately possible neural map 
discuss training methods adapt weights neurons triples neurons 
means assume function training simplicity 
naturally parts adaptive weighting terms computation formal representations rep similarities dw dr note learning algorithms discussed immediately transferred adaptation additional parameters rep various ways learning self organizing maps 
simplest intuitive learning paradigms hebbian learning 
got advantage producing simple update formulas require properties functions alternatives objective learning process explicitly terms cost function optimized 
gradient descent methods optimization techniques genetic algorithm applied 
gradient descent requires involved functions di erentiable real vector spaces 
hammer neurocomputing learning algorithms simple vectors heuristic method interpreted stochastic gradient descent appropriate cost function 
paradigms yield learning rules simple vectors 
investigate situation 
rst discuss hebbian learning structured data adapt cost functions self organizing maps approach 
proper representation trees ti training set subtrees neural map 
computation recursive similarities tree ti uses representation subtrees adequate representation ti neural map subtrees ti properly processed 
assume tree contained subtrees 
property essential cient hebbian learning 
property bene cial learning cost function 
implicitly assume training set contain subtrees ti hebbian learning characterizes idea iteratively making weights neuron res response speci input similar input 
purpose notation moving direction required 
assume functions function allows adaptation weights rst position neuron direction input degree ifw real vector space function usually just addition vectors 
discrete weights embedded real vector space dealt adaptation discrete decision compared edit distance example speci number operations transform value standard som 
function allows adapt formal representations way 
formal representations real vector space recursive som example addition vectors 
alternatively discrete transformations consists discrete set 
note handle discrete formal representations way 
rst formulate hebbian training analogy som training 
additional ingredient neighborhood structure topology assume nh hammer neurocomputing measures distance neurons respect topology 
lattice structure priori lattice neurons data training 
consist dimensional lattice andthe neurons accordingly 
dimensional lattice nh distance indices example 
lattice structure complex hexagonal grid exponentially increasing number neighbors 
case algorithm written follows initialize weights random repeat choose training pattern subtrees inverse topological order compute ni neurons ni compute neuron ni greatest similarity adapt weights neurons ni simultaneously ni ni nh ni ni ni ni nh ni ni ni ni nh ni ni tj rj rep tj tj nn monotonically decreasing function 
exp learning rate decreased step order ensure convergence term determines size taken account 
usually decreased training 
dependence requires recomputation recursive similarity subtrees changing weights 
course costly 
recursive similarity usually subtree tree values uniformly update 
approximation change training small learning rates 
hebbian learning successfully applied 
see example literature classi cation time series images 
point scaling term nh ni ni substituted general function depends winner lattice gets input distances ni neurons ni 
general setting covers updates soft min function ranking neurons example means algorithm 
alternatives som neural networks literature vector quantization vq gas ng section 
obtain analogy vq case choose nh ni nj hammer neurocomputing yields learning algorithm handle sequences trees respective context topology preservation takes place 
lattice structure vq algorithm 
vq alternative som training 
metric dr internal representations values vq initialization map andthe order presentation examples 
consequence argumentation adaptation representations necessary apply vq case 
assume neurons alternative xn vector entry place xi xj repr ect decoupling indices 
note easily apply form hebbian learning representation 
fast computation look choose similarity measure dr dot product restrict rn ri 
store formal representations neuron form rn global scaling factor components representation 
tree neuron winner neurons winners respectively adapt weights way hebbian learning component representing respectively global scaling factors representations consequence obtain implicit normalization formal representations 
neural gas algorithm section determines respective training example distances neurons 
update algorithm tree tj ni ni rk ni ni rk ni ni rk rk denotes rank neuron ni ordered current distance input tree tj currently processed rk denotes number neurons nk tj ni tj nk 
example possess form exp 
update sure neurons similar activation adapt respective weights direction 
neurons neighbors data driven implicit lattice constitute rst data point 
naturally resulting regular 
accordance lattice proposes alternative som data 
alternative lattice 
case vector quantization representation requires adaptation 
distance indices dynamically computed training algorithm ranking respect data point 
behavior obtained choose rn dr metric choose representation function ith component xn number xj xj xi xn rk rk xn rk xi denotes number xk xk xi 
hammer neurocomputing 
learning cost minimization som ng vq orts derive hebbian learning rules alternatively stochastic gradient methods appropriate cost functions simple vector data 
assume di erentiable cost function written form ai ai ai denoting training pattern stochastic gradient descent respect weights form initialize weights random repeat choose training pattern ai update ai learning rate 
note hebbian learning similar update 
desirable prove hebbian learning obeys stochastic gradient descent appropriate choice holds simple vectors example 
consequence prior mathematic objective identi optimization methods particular global optimization methods simulated annealing alternative 
exist known guarantees convergence stochastic gradient descent learning rate may vary time ful lls certain properties see 
simple computation shows learning introduced interpreted exact gradient descent method 
update formula hebbian learning form ni ni nh ni ni nh ni ni nh nh depends neighborhood lattice data driven ranking characteristic function winner neuron respective cases 
metric term lk ni rk nh chosen lk ni nh lk ni rk 
general metric dr chosen lk ni nh dr lk ni rk lk ni 
formula stochastic gradient descent theorem schwartz hold nh dr lk ni rk lk ni ll nm nh dr ll nm rl ll nm lk ni weights ll nm 
partial derivatives separate summands 
rst depends derivative respective neighborhood nh 
borders receptive elds usually contribute depending respective choice neighborhood equality terms left hammer neurocomputing case simple vector data 
derivative yields terms nh dr lk ni rk lk ni ll nm left hand side right obtain nh dr ll nm rl ll nm lk ni 
note rl depends lk ni rk depends ll nm 
terms usually coincide rl rk entirely di erent functions weights 
vq example term nh nonvanishing case second derivative yields non vanishing contribution product vanishes choices hebbian learning structured data general correspond gradient descent 
investigate learning rules result appropriate cost functions relate hebbian learning 
rst look som vq simple vectors 
denote simple labels vectors training ai 
weights map denoted wj 
cost function vq case simple vectors form ev ai wj ai wj ai wj denotes characteristic function receptive eld wj winner ai 
derivatives respect weights wj yields learning rule vq 
note derivatives computed function 
ignore point simple vectors provide detailed derivation formulas including borders function ng constants form 
cost en rk ai wj rk denotes rank neuron nj neurons distance data point 
som possess cost function continuous case data distribution nite training set considered 
discrete case nite data set energy function constructed 
article proposes cost function slightly modi ed version som continuous case es nh nj nk ai wk denotes characteristic function receptive eld winner slightly modi ed de nition winner nh nj nk ai wk nh nj nk ai wk hammer neurocomputing winner cost function modi ed learning rule som neuron smallest distance neuron smallest averaged distance respect local neighborhood 
obviously cost functions form ai ai wn function chosen speci setting function equals ai ai wn ai wj ai wj vq 
general scheme allows immediate transfer cost functions structured case term ai wj respective recursive similarity neurons tree structure 
neural map neurons ni anda training set td complete respect subtrees general cost function form ti ti ti denotes vector ti ti nn 
chosen corresponding cost functions vq ng som measure taken objective 
derivatives obtain formulas stochastic gradient descent structured case 
shown hebbian learning introduced constitutes approximation stochastic gradient descent cost functions ng vq som disregards contributions error function due substructures 
start computing derivatives general function respect weights neurons 
purpose assume real vector space assume including rep dr dw di erentiable 
discuss point 
derivative respect weight li nj yields ti li nj ti ti nj ti nj li nj rst part depends respective cost function choice involves functions computing derivatives built characteristic functions 
follows de ne xn xn xi 
fm vector functions term xn denotes vector xn ifm xn 
xi vector xi xim dene xn xn xi xn xim 
abbreviation rj rep tj tj nn tj hammer neurocomputing denote corresponding derivative respect variable tj rj rep tj tj nn tj nl 
nj nj jj dw nj dr nj nj dr nj nj rst components weights ij kronecker symbol ij denotes dot product 
formal representations attached neuron nd nj nj jj dr nj dr nj nj dr nj nj nj nj jj dr nj dr nj nj dr nj nj derivatives computed recursively depth tree 
starting leaves obtain formulas derivatives respective activation respect weights neurons 
complexity order denoting number neurons number labels tree dimensionality weights neuron dimensionality note rst summands derivatives yield terms occur hebbian learning dr dr coincide squared euclidian metric terms respectively 
get original hebbian learning rules consider rst summands involve respective terms arising ng vq som 
hebbian learning disregards contribution subtrees drops summands recursive formulas derivatives 
simple vector case hebbian learning stochastic gradient descent coincide structured data causes di erences methods 
factor usually smaller weighting factor additional terms gradient formulas vanishes exponentially depth recursive step small factor hebbian hammer neurocomputing learning seen cient approximation precise stochastic gradient descent 
formulation cost minimization allows formulate tasks framework 
assume yi tree ti 
train approximately yields output yi ti minimize yi ti stochastic gradient descent 
shown networks dynamic 
error function introduces training methods cases 
respective formulas case essentially equivalent known real time recurrent learning 
hebbian approximation gradient 
note known problem long term dependencies di culty latch information recursive layers arises context :10.1.1.13.634:10.1.1.41.7128
problem long term dependencies obviously avoided hebbian learning map substructures structure data set 
allows drop terms gradients include vanishing gradients 
rtrl proposes formulate gradient descent error functions recurrent network training backpropagation time structure respectively bptt 
bptt lower complexity respect number neurons 
idea bptt bottlenecks computation parameterization cost function including variables substitute contribution activation neurons respective activation neurons time steps 
chain rule error signals ciently 
assume representation function rep dimensional components rep repr consider tree simplicity refer nodes labels ai identify subtree root ai andthe root label notation 
subtrees node ai denoted left ai right ai parent node denoted parent ai 
denote formal representation tree ai ai components rj ai 
weight li nj times recursive computation 
assume corresponding number identical copies li nj ak li nj ak refers copy recursive step computing ak 
part cost function comes labels tree denoted ak ak 
derivative respect li nj sum derivatives respect copy li nj ak 
bottleneck computation derivatives corresponds inputs net bptt case rnns yields respective bptt formula similarities output dr dw computation bottleneck 
de ne dw aj ni di aj ni dr left aj ni dr right aj ni hammer neurocomputing copy li nj ak dw ak nj dr left ak nj li nj ak dri ak nj dr right ak nj ak nj li nj ak dw ak nj dr left ak nj dr right ak nj derivative respect di ak nj backpropagation ak nj ak ak nj di ak nj ak ak nj di parent ak nl dr ak li nl rep ak ak left parent ak 
ak ak nj depends speci cost function 
backpropagation order 
note function form rep xn rep xn complexity reduces derivative xn reduces vector non zero entry 
complexity case reduces order complexity bptt 
possible bottleneck dimensionality rep low 
backpropagation done internal representations structured data 
complexity method depends dimensionality explicit formulas 
nally look speci cost functions vq approximate som ng structural case order show explicitly hebbian learning interpreted approximate stochastic gradient descent cases 

energy function vq cost function vector quantization case ev ti nj ti nj ti nj minimal ti nj hammer neurocomputing ti form ti nj ti nj 
term derivatives speci cost function vq ak ak nj 
derivative term appendix 
disregard terms general formulas derivatives obtain hebb formulas 
note terms contain usually small 
dw dr constitute euclidian metric truncated hebbian formulas look nl nl ti nl nl nl nl ti nl nl nl nl ti nl nl denotes current label ri internal representations subtrees 
possible transfer gradient calculations directly formal representation trees lies real vector space andthe di erentiable 
component number repr xn yields exp xl components zero 
reps di erentiable expected reps encodes winners unary fashion referring lattice location 
di erentiable approximated desired degree accuracy di erentiable function soft min function exp xn exp xl exp xn exp xl controls quality approximation 
original function situations minimum unique 

energy function som adapt variation proposed uses slightly di erent notation winner 
done original learning rule proposed kohonen possess cost function continuous case discrete case energy function exists 
corresponding cost function structured data form es ti nj nh nj nk ti nk nh describes neighborhood function lattice exp provides scaling factor update neighbors nh nj nk ti nk ti nj minimal hammer neurocomputing determines winner 
winner neuron optimum weighted distance respect neighborhood 
tree ti cost function induces function ti ti nj nh nj nk ti nk derivative respect ti nl appendix 
disregarding terms yields standard hebb terms euclidian metric dr dw nl nl nl nl nl nl ti nk nh nk nl nl ti nk nh nk nl nl ti nk nh nk nl nl speci adaptations recursive som follow line previous case 
note alternatives 
function reps di erentiable 
approximate function rep desired degree 
denote indices neurons 
representation function computes index minimum input approximated soft min function exp xi reps xn exp xl controls quality approximation 
approximation di erentiable 
original function situations minimum unique 
note lattice neurons chosen priori 
dimensional regular appropriate restricted data sets due incompatibility grid topology complex data 
experiments show di erent structures stored di erent clusters regular 
lattice erent structures partially due combinatorial explosion possible neighbors exponential increase observed small tree trees complex structures 
lattices re ect property exponentially increasing neighborhood hyperbolic som choice 

energy function ng cost function ng form en rk ti nj hammer neurocomputing rk equals number neurons ti nk ti nj 
exp scaling factor 
put cost function general form function ti rk ti nj derivative respect ti nl appendix 
hebbian updates result discarded euclidian metric dr dw nl nl rk nl nl nl rk nl nl nl rk nl distance indices dynamically computed training algorithm ranking respect respective data point 
representation function xn rk rk xn xj xn xj rk xi denotes rank xi values xn ordered size 
function di erentiable approximate di erentiable function substitute sigmoidal approximation sgd sgd sgd exp note amount position ranking formulation 
expected position winning units important rank neurons far away 
formal representation trees winning neurons 
possibility include exponential weighting rep xn exp rkk exp rkk xn 
ect deviations neurons close currently neurons away 

discussion general framework structured data main idea recursive processing recursive structured data 
purpose dynamics supervised recurrent recursive networks directly 
special approaches tkm recurrent networks framework 
key issue dynamics notion internal representations context enables networks store activation pro les recursively processed substructures nite vector space 
hammer neurocomputing allows comparison arbitrary size 
general framework allows formulate training mechanisms uniform manner 
hebbian learning various topologies vq ng topology immediately formulated 
turns case processing hebbian learning approximation gradient dynamics appropriate cost functions 
cost functions general framework ways precisely compute gradients 
hebbian learning disregards contribution substructures cases costly precise approaches 
precise formulation allows recover mechanisms general framework 
appropriate choice cost functions yields algorithms networks bptt 
formulation proposes transfer di erent learning paradigms quantization recursive case learning vector quantization lvq constitutes self organizing learn prototype data hebb style learning rules 
approach proposes cost function variants lvq called glvq 
stable learning additional features automatic metric adaptation developed 
methods rely cost functions depend squared euclidian distance patterns weights neurons immediately included framework formulas calculating corresponding derivatives 
starting general formulation uniform investigation properties recurrent self organizing maps possible 
demonstrated possibility respect investigation training algorithms 
directions research take general properties rep andthe internal representation trees consideration design general criteria respective functions possibilities evaluate approaches 
steps direction noise tolerance various representations representation capability andthe notion topology preservation speci emphasis respective internal representation 
note standard issues self organizing maps include problem convergence ordering notion topology preservation andthe magni cation 
analogous issues uniformly general recursive self organizing maps 
appendix 
derivative vq ak ak nj vq computed 
note ti nj ti nl ti nj hammer neurocomputing heaviside function derivative function symmetric function nonvanishing 
nd ti ti nl ti nj ti nl ti nj ti nj ti nj ti nl second term yields standard hebb factor vq yields contribution ti nl general formulas derivatives 
ti nk ti nj ti nk ti nl rst summand equals ti nj ti nl ti nk ti nj ti nj vanishes symmetric andthe terms involving non vanishing role ti nk ti nj changed 

derivative approximate som derivative ti nj nh nj nk ti nk ti nl ti nj nh nj nk ti nk ti nl second term yields usual som update slightly di erent notion winner section 
contribution formulas ti nj nh nj nl 
rst term vanishes seen follows identity ti nj nh nl nk ti nk nh nj nk ti nk hammer neurocomputing rst sum equals nh ns nt ti nt nh nj nt ti nt nh nt ti nt nh nj nt ti nt ti nk nh nj nk nh nl nh nj nl vanishes symmetric andthe terms non vanishing nj substituted 

derivative ng derivative rk ti nj ti nl rk ti nj ti nl second term yields usual update rule ng contribution rk formulas 
rst term vanishes explicitly identity rk ti nj ti nk rst term rk ti nj ti nk ti nj ti nj rk ti nl ti nk ti nl vanishes due properties baldi brunak frasconi pollastri soda exploiting past protein secondary structure prediction bioinformatics 
barreto araujo time self organizing maps overview models int 
comput 
res 

bengio simard frasconi learning long term dependencies gradient descent di cult ieee trans :10.1.1.41.7128
neural networks 
micheli sperduti application cascade correlation networks structures chemistry appl 
intell 

chappell taylor temporal kohonen map neural networks 
costa frasconi soda incremental parsing natural language recursive neural networks appl 
intell appear 
cottrell fort paget theoretical aspects som algorithm neurocomputing 
hammer neurocomputing cottrell verleysen ed european symposium arti cial neural networks side publication pp 

cottrell rousset kohonen algorithm powerful tool analysing multidimensional quantitative qualitative variables proceedings springer berlin pp 

erwin obermayer schulten self organizing maps convergence properties functions biol 
cybern 

principe spatiotemporal memory basedon soms activity di usion oja kaski eds kohonen maps elsevier amsterdam 
farkas miikkulainen modeling self organization directional selectivity primary visual cortex proceedings international conference arti cial neural networks springer berlin pp 

frasconi gori sperduti eld guide dynamical recurrent networks kolen kremer eds sequences data structures theory applications ieee md pp 

frasconi gori sperduti general framework adaptive processing data structures ieee trans 
neural network 
frasconi gori sperduti learning ciently neural networks theoretical comparison representations ecai proceedings th european conference arti cial intelligence ios press amsterdam 
giles kuhn williams special issue dynamic recurrent neural networks ieee trans 
neural networks 
gori mozer tsoi watrous special issue recurrent neural networks sequence processing neurocomputing 
gunter validation indices graph clustering 
eds proceedings third iapr tc workshop graph representations pattern recognition italy pp 

sperduti tsoi self organizing map adaptive processing structured data ieee trans 
neural networks 
tsoi sperduti organizing map allison yin slack eds advances self organizing maps springer berlin pp 

hammer learning recurrent neural networks lecture notes control sciences vol 
springer berlin 
hammer micheli sperduti general framework self organizing structure processing neural networks technical report tr dipartimento di informatica universita di pisa 
hammer villmann learning vector quantization neural networks 
hammer villmann mathematical aspects neural networks verleysen ed european symposium arti cial neural networks pp 

heskes self organizing maps vector quantization modeling ieee trans 
neural networks 
hochreiter schmidhuber long short term memory neural 
comput 

extended kohonen feature map sentence recognition kappen eds proceedings icann international conference artif 
neural networks springer berlin pp 

james miikkulainen self organizing feature map sequences tesauro touretzky leen eds advances neural information processing systems vol 
mit press cambridge ma pp 

kangas time delayed self organizing maps proceedings ieee inns international joint conjecture neural networks vol 
pp 

kaski bankruptcy analysis self organizing maps learning metrics ieee trans 
neural networks 
hammer neurocomputing kohonen learning vector quantization arbib ed handbook brain theory neural networks mit press cambridge ma pp 

kohonen self organizing maps springer berlin 
kohonen kaski lagus honkela large level som browsing newsgroups von der malsburg von seelen eds proceedings icann springer berlin pp 

kaski recurrent som local linear models time series prediction verleysen ed sixth european symposium arti cial neural networks de facto 
kremer spatio temporal connectionist networks taxonomy neural comput 


hornik convergence learning algorithms constant learning rates ieee trans 
neural networks 
kushner clark stochastic approximation methods systems springer berlin 
martinetz schulten neural gas network vector quantization application time series prediction ieee trans 
neural networks 
martinetz schulten topology representing networks neural networks 
pearlmutter gradient calculations dynamic recurrent neural networks survey ieee trans 
neural networks 
ritter self organizing maps non euclidean spaces oja kaski eds kohonen maps springer berlin pp 

ritter martinetz schulten neural computation organizing maps addison wesley reading ma 
asymptotic behaviour self organizing maps non uniform stimuli distribution technical report universitat kaiserslautern fb mathematik germany july 
sato yamada generalized learning vector quantization tesauro touretzky leen eds advances neural information processing systems vol 
mit press cambridge ma pp 

sinkkonen kaski clustering conditional distribution auxiliary space neural comput 

sperduti neural networks adaptive processing structured data dor ner bischof hornik eds icann springer berlin pp 

sperduti supervised neural networks classi cation structures ieee trans 
neural networks 
som local models time series prediction proceedings workshop self organizing maps helsinki finland pp 

villmann der herrmann martinetz topology preservation self organizing maps exact de nition measurement ieee trans 
neural networks 
context quantization self organizing maps proceedings international joint conference neural networks vol 
pp 

recursive self organizing maps neural networks 
recursive self organizing maps allison yin slack eds advances self organizing maps springer berlin pp 

