statistical learning humanoid robots vijayakumar usc edu souza usc edu computer science neuroscience kawato dynamic brain project university southern california los angeles ca usa shibata tom atr jp kawato dynamic brain project japan science technology kyoto japan rg ini phys ethz ch univeristy eth zurich 
ch zurich stefan schaal usc edu computer science neuroscience kawato dynamic brain project university southern california los angeles ca usa 
complexity kinematic dynamic structure humanoid robots conventional analytical approaches control increasingly unsuitable systems 
learning techniques offer possible way aid controller design insufficient analytical knowledge available learning approaches mandatory humanoid systems supposed completely autonomous 
research neural networks statistical learning focused learning finite data sets stringent constraints computational efficiency learning humanoid robots requires different setting characterized need real time learning performance essentially infinite stream incrementally arriving data 
demonstrates high dimensional learning problems kind successfully dealt techniques nonparametric regression locally weighted learning 
example describe application advanced algorithms locally weighted projection regression lwpr line learning problems humanoid motor control learning inverse dynamics models model control learning inverse kinematics redundant manipulators learning oculomotor reflexes 
examples demonstrate fast seconds minutes learning convergence highly accurate final 
conclude real time learning complex motor system humanoid robots possible appropriately tailored algorithms increasingly autonomous robots massive learning abilities achievable near 

necessity adaptive control apparent scale control systems gets increasingly complex instance advanced robotics factory automation control 
humanoid robots typical example 
humanoid high dimensional kluwer academic publishers 
printed netherlands 
tex vijayakumar movement systems classical system identification control techniques insufficient due unknown sources nonlinearities inherent systems 
learning techniques possible way overcome limitations aiding design appropriate control laws involve multitude sensors actuators 
learning viable research approach generation flexible autonomous perform multiple tasks schaal hope creating completely autonomous humanoid robot point 
characteristics motor learning problems humanoid robots high dimensional input spaces potentially redundant irrelevant dimensions nonstationary input output distributions essentially infinite training data sets representative validation sets need continual learning 
learning tasks fall domain regression problems learning dynamics models involve regression problems learning policy reinforcement learning 
interestingly class line learning regression problems characteristics far conquered new developments statistical learning 
bayesian inference bishop usually computationally expensive real time application complete joint densities data 
framework structural risk minimization vapnik advanced form support vector machines excels classification finite batch learning problems show compelling performance regression incremental learning 
techniques nonparametric regression particular methods locally weighted learning atkeson advanced meet requirements real time learning high dimensional spaces schaal 
advanced locally weighted learning algorithms locally weighted projection regression lwpr challenging learning humanoid robotics inverse dynamics model dof anthropomorphic robot ii inverse kinematics map redundant iii bio mimetic gaze stabilization humanoid oculomotor system 
sections explain lwpr algorithm introduce illustrate learning results real time learning actual robots tasks 
best knowledge time real time learning complex models accomplished robot control 
tex statistical learning humanoid robots 
locally weighted projection regression core concept learning approach isto approximate nonlinear piecewise linear models atkeson 
learning system automatically determines appropriate number local models parameters hyperplane model region validity called receptive field rf model formalized gaussian kernel wk exp ck dk ck query point linear model prediction yk kx 
total output learning system weighted mean linear models wk illustrated fig 

learning system involves determining linear regression parameter distance metric dk 
center ck rf 
local created needed section 

local dimensionality reduction despite appealing simplicity piecewise linear modeling approach brittle computationally expensive high dimensional input spaces 
empirical observation high dimensional data lie locally low dimensional distributions possible develop efficient approach exploit property 
ordinary linear regression fit local hyperplanes suggest employ partial squares pls wold frank friedman 
pls recursively computes orthogonal projections input data performs single variable regressions projections residuals previous iteration step 
table illustrates pls pseudocode global linear model input data isin matrix corresponding dimensional output data vector key ingredient pls direction maximal correlation residual error input data projection direction regression step 
additionally pls regresses inputs previous step projected inputs sr order ensure orthogonality projections ur step 
additional regression avoided replacing ur similar techniques principal tex vijayakumar table pseudocode pls projection regression 
initialize res 
projections res res res res res res tr 
component analysis sanger 
regression step leadsto better performance algorithm 
fact pls chooses effective projections input data spherical distribution projection pls find direction gradient achieve optimal regression results 
regression step modifies input data resulting data minimal magnitude push distribution spherical 
incremental locally weighted version pls algorithm derived table ii vijayakumar schaal 
forgetting factor quickly older data forgotten various pls parameters similar recursive system identification techniques ljung :10.1.1.14.5408
variables ssr srr memory enable univariate regression step recursive squares fashion fast newton method 
pls univariate efficiently possible run locally weighted pls projection direction denoted 
optimal projection isin direction local gradient function approximated 
locally weighted input data forms spherical distribution local model single pls projection suffice find optimal direction 
distance metric weighting data need adjusted local distribution spherical 
learning rule distance metric accomplish adjustment explained 
noted steps table ii unnecessary uni projection case 

learning distance metric distance metric dk locality receptive fields learned local model individually stochastic gradient descent leave cross validation cost function 
note tex statistical learning humanoid robots table ii 
incremental locally weighted pls rf centered update local model initialization training point exp update means wy initialize res projections 

sr 
ss ss ws 
sr sr 
sr ss 
sr 
mse mse 
sz sz sr 
sz ss 
sr predicting novel data initialize repeat rsr sr sr doesnot require competitive learning completely local learning rule leave crossvalidation performed keeping data memory schaal atkeson 

mt positive definiteness cost function minimized wires mi wi wi st ij 
number training data number input dimensions 
stochastic version gradient derived cost function keeping track memory terms shown table iii 
tex vijayakumar table iii 
derivatives distance metric update mkl dij mkl stochastic update mkl mkl dij dij mkl il jl ij ifi ij 
compute projection direction cv pr pr sr rn en er ss re cv cv tr rsr hr pr 
complete lwpr algorithm er wp cv combined incremental learning scheme automatically allocates new locally linear models needed 
final learning network fig 
outline algorithm table iv 
code isa threshold create new receptive field isthe initial usually diagonal distance metric eq 
initial number projections set 
algorithm simple mechanism determining increased recursively keeping track mean squared error mse asa function number local model step incremental pls pseudocode 
mse projection doesnot decrease certain percentage algorithm adding new projections locally :10.1.1.14.5408
diagonal distance metric assumption number projections remains small computational complexity update parameters tex statistical learning humanoid robots table iv 
complete lwpr algorithm initialize lwpr receptive field rf new training sample receptive fields calculate activation eq update projections regression table ii distance metric table iii check 
projections needs increased cf 
section rf activated create new rf lwpr number input 
variant computational complexity 

real time learning humanoid robots main development lwpr model control humanoid robots analytical models result sufficient accuracy 
sections describe lwpr improve model control learned acquired rapidly real time system trying accomplish task 
results learning literature demonstrate feasibility realtime statistical learning high dimensional systems humanoid robots 

real time learning inverse dynamics common strategy robotic biological motor control convert kinematic trajectory motor inverse dynamics model 
inverse dynamics takes desired positions velocities accelerations dofs robot outputs appropriate motor commands 
case learning dof anthropomorphic robot arm see fig 
inverse dynamics model receives outputs torque commands 
derived analytically rigid body dynamics assumption compact recursive formulation inverse dynamics robot results pages compact code filled nested sine cosine terms 
line learning motor commands need tex vijayakumar correlation computation module receptive field centered weighted average output wk dk yk 
information processing unit lwpr learning module input input pls ui linear unit reg inputs generated model hz implementation 
updating learning system take place lower rate remain high possible capture data fast movements usually achieve hz updating rate 
learning regression problems high dimensional input space isa daunting problem view bias variance trade 
learning control training data generated learning system impossible assess priori structural complexity data going 
fortunately actual movement systems fill data space completely random way 
viewed locally data distributions tend low dimensional dimensional inverse dynamics schaal robot global input dimensions 
property lwpr algorithm isa key element excellent real time performance learning scheme 
tex statistical learning humanoid robots 
dof dexterous arm dof humanoid robot 
performance comparison static data set demonstrating applicability lwpr real time comparison alternative learning methods serve demonstrate complexity learning task 
collected data points dof anthropomorphic robot fig 
hz sampling frequency 
percent data excluded test set 
training data approximated different methods parameter estimation analytical rigid body ii support vector regression iii lwpr iv full lwpr 
noted ii incremental learning methods require batch learning 
parametric model suggested just approximating data global model inverse powerful method 
robot lightweight compliant know rigid body dynamics assumption fully justified 
method ii support vector regression relatively new statistical learning approach derived theory structural risk minimization 
re tex vijayakumar nmse parameter identification svm lwpr lwpr training data points 
comparison generalization error nmse traces different learning schemes cent publications support vector machines demonstrated superior learning performance previous algorithms comparison method lwpr interesting benchmark 
lwpr iii iv algorithm described 
iv learned separate model output inverse dynamics models univariate output inputs 
lwpr employed diagonal distance metric 
fig 
illustrates function approximation results shoulder motor command graphed number training iterations iteration corresponds update data point 
surprisingly rigid body parameter estimation achieved worst results 
lwpr outperformed parameter estimation fell svm regression 
full lwpr performed best 
results dofs analogous 
final result lwpr employed local models average local projections 
lwpr perform better diagonal distance metric 
abilities diagonal distance metric carve locally spherical distribution limited accomplish better results full distance metric remedy learning number inputs 
results demonstrate lwpr competitive function approximation technique 
tex statistical learning humanoid robots 
line learning implemented full lwpr robotic setup 
parallel processors system mhz powerpc processor completely devoted lookup learning lwpr 
dof lwpr learning system resulting parallel learning modules 
order accelerate lookup training times added special data structure lwpr 
local model maintained list local overlapped sufficiently 
sufficient overlap local model determined centers distance metrics 
point input space closest centers sense mahalanobis distance di dj 
inserting point eq local activation due 
local listed overlapping cf 
lwpr outline 
diagonal distance metrics overlap computation number inputs 
new data point lwpr neighborhood relation maximally activated rf 
appropriate counter local model ensures overlap local exhaustively 
nearest neighbor data structure fact movement system highly correlated data lookup learning confined rfs 
lookup update identification number maximally activated rf 
lookup update consider 
proved exhaustive lookup update strategy activated certain threshold 
lwpr trained line robot performed pseudo randomly drifting pattern front body 
lookup proceeded hz updating learning model hz 
certain intervals learning stopped robot attempted draw planar hz frequency entire pattern 
quality drawing fig 
fig 
desired pattern illustrates performed robot simulator uses perfect inverse dynamics model necessarily perfect tracking numerical integration algorithm isthe performance estimated rigid body shows results lwpr 
rigid body model worst performance lwpr obtained results comparable simulator 
fig 
illustrates speed lwpr learning 
trace inverse dynamics model just low gain pd controller 
traces tex vijayakumar displacement meters displacement meters des sim lwpr param displacement meters lwpr lwpr lwpr lwpr displacement meters 
robot effector motion traces different control schemes progress online learning lwpr control show rapidly lwpr learned pattern training denote performance seconds training 
seconds hardly distinguishable desired trace 

inverse kinematics learning movement tasks defined coordinate systems different actuator space robot coordinate transformation task actuator space performed motor commands computed 
system redundant freedom dofs inverse kinematics transformation external internal ill posed 
define intrinsic coordinates manipulator asthe dimensional vector joint angles position orientation manipulator send effector asthe dimensional vector forward kinematic function generally written need inverse relationship redundant systems robots see fig 
solutions equation non unique 
traditional inverse kinematics tex statistical learning humanoid robots determine particular solution face multiple solutions optimizing additional cost criterion 
approaches favor local optimizations compute optimal change small change integrate generate entire joint space path 
resolved motion rate control local method uses jacobian forward kinematics describe change position solved inverse non singular pseudo inverse computations minimize null space 
motivation learning learning inverse kinematics useful kinematic model robot isnot known accurately cartesian information uncalibrated camera coordinates computational complexity analytical solutions high 
instance humanoid robot observed offsets sensor readings inaccurate knowledge exact robot lead significant error accumulations analytical inverse kinematics computations hard maintain accurate calibration active vision system robot 
re calibrating entire robot frequently employ self calibrating learning approach 
additional appealing feature learning inverse kinematics avoids problems due kinematic singularities learning experienced data data correct demand impossible postures result ill conditioned matrix inversion 
major obstacle learning inverse kinematics inverse redundant kinematic chain solutions 
context eq 
means multiple learn mapping average solutions assuming different due noise 
may result invalid solution multiple lie non convex set case robot kinematics jordan rumelhart 
problem avoided specific input representation learning network bullock averaging tex vijayakumar shown averaging eq 
multiple map 
jacobian linear form redundant systems average solutions result desired asthe averaging vicinity particular 
propose learn inverse mapping function spatially localized lwpr learning system input output representation 
automatically resolve redundancy problem resorting optimization approach local average solution picked simply local average solutions experienced 
algorithm perform near singular posture mentioned generate joint experienced 

applying lwpr inverse kinematics learning order apply lwpr inverse kinematics learning humanoid robot learn separate model generate joint freedom neglecting degrees freedom eyes plus cartesian inputs mapping 
resolution redundancy requires creating optimization criterion allows system choose particular solution inverse kinematics problem 
robot humanoid robot system assume posture natural possible 
definition natural corresponds posture close possible default posture opt behavioral studies cruse br er 
total cost function training lwpr written asfollows opt represents distance current posture optimal posture opt isa diagonal weight matrix isthe current prediction lwpr 
minimizing achieved presenting lwpr target values target tex statistical learning humanoid robots targets self supervised target slightly modified component enforce optimization cost function null space jacobian eq 

exploration strategy initially output lwpr term motion opt strength number data seen largest contributing local model lwpr 
additional term meaningful importantly data generating motion joint space explored 
learn inverse kinematics fly attempting perform required task 
important aspect formulation inverse kinematics problem learning problem comprise locality local model isa function linear projection directions solely dependent cf 
eq 
encode knowledge lwpr learning process setting initial values diagonal terms distance metric eq 
correspond zero 
locality receptive model solely 
lwpr ability determine ignore locally irrelevant regression provide information normalizing input dimensions variance relevant dimensions large 
scaling results larger correlations relevant output projection directions relevant subspace 
feature scale dimensions corresponding variables regression local model primarily subspace 

experimental evaluations goal task experiments track trajectory cartesian space created simulated visual input robot 
performance system plotted analytical pseudo inverse solution eq 
available robot previous schaal 
system trained data generated small sinusoidal degree freedom randomly chosen mean space 
seconds mean repositioned 
performance nr tex vijayakumar analytical motor babbling analytical online learning joint velocity analytical task specific analytical eb learned eb joint position 
tracking learned inverse kinematics 
performance training motor babbling results improving performance data seen task performance minutes learning scratch task phase plot joint position joint velocity system training system motor babbling fig 

second experiment robot executed trained lwpr experiment 
case system allowed improve data collected performing task 
shown fig 
merely minute additional learning system performs analytical pseudo inverse solution 
final experiment started untrained system learn inverse kinematics scratch performing task 
fig 
shows progression system performance task minutes tex statistical learning humanoid robots learning 
system initially starts making slow inaccurate movements 
collects data rapidly converges desired trajectory 
minutes training performance approached seen fig 

note redundant manipulators periodic trajectory operational space imply consistency joint space trajectory followed joint space may cyclic aperiodic null space motion affect tracking accuracy 
fig 
shows phase plot joints elbow flexion extension cycles trajectory learning converged 
presence single loop cycles shows inverse kinematics solution algorithm consistent 

learning biomimetic gaze stabilization oculomotor control humanoid robot biological oculomotor systems stabilization gaze face unknown perturbations body selective attention stereo vision dealing large information processing delays 
nonlinearities geometry binocular vision possible nonlinearities oculomotor plant desirable accomplish accurate control behaviors learning approaches 
describe application lwpr learning control system oldest behaviors oculomotor control stabilization reflexes gaze 
shibata schaal described control theoretically reasonable choices control components result oculomotor control system resembles known functional anatomy primate oculomotor system 
resulting control circuitry system shown fig 

core learning system derived biologically inspired principle learning combined lwpr algorithm 
essentially blocks system fig 
middle block vestibular head velocity input linear feedforward controller conservatively low gains top block non linear feedforward controller continuously adapted lwpr vestibular lower block isthe retinal slip negative feedback controller generates delayed error signal linear fixed feedforward control non linear continuously learned feedforward circuit 
feedback error learning fel isa principle learning motor control 
employs approximate way mapping sensory errors tex vijayakumar 
control diagram vor learning system 
lowest box corresponds negative feedback circuit middle box corresponds linear feedforward model top box corresponds continuously learned non linear feedforward circuitry motor errors subsequently train neural network supervised learning 
viewpoint adaptive control fel isa model adaptive controller 
controller equipped priori stabilizing linear feedback controller performance satisfactory due nonlinearities plant delays feedback signals 
feedback motor command error signal train neural network controller 
neural network receives correct inputs usually current desired state plant acquire nonlinear control policy inverse dynamics model plant nonlinear feedback controller 
kawato proved convergence control scheme advocated model learning cerebellum 
order employ lwpr learning fel scheme require presence target output see table ii 
motor learning target values motor commands rarely exist errors usually generated sensory space motor command space 
fel strategy interpreted generating pseudo target tex statistical learning humanoid robots 
vision head subsystem humanoid experimental setup motor command fb fb denotes feedback error signal predicted output 
employing lwpr algorithm line learning demonstrate humanoid robot able acquire high performance visual stabilization reflexes seconds learning despite significant nonlinearities processing delays system 

experimental setup fig 
depicts experimental system 
dof robot actuated torque control loop 
eye robot oculomotor system consists cameras wide angle degrees view angle horizontally color camera peripheral vision second camera foveal vision providing narrow view degrees horizontally color image 
foveated retinal structure primates essential artificial vision system order obtain high resolution vision objects interest able perceive events peripheral environment 
eye independent freedom pan tilt motion 
tex vijayakumar controllers implemented subsystems learning control subsystem vision subsystem operated vme rack real time operating system vxworks 
cpu boards motorola learning control subsystem cpu boards motorola provided vision subsystem 
learning control subsystem cpu boards respectively low level motor control eyes robot compute torque mode ii visuomotor learning iii receiving data vision subsystem 
communication cpu vme shared memory communication hardware isvery fast 
vision subsystem cpu board controls fujitsu tracking vision board order calculate retinal slip retinal slip velocity information eye 
ntsc video signals binocular cameras synchronized ensure processing eyes vision data 
vision data sent serial port bps learning control subsystem 
experimental demonstrations peripheral camera vor pan degree freedom 
multiple freedom camera multiple eyes just require duplication control learning circuits 
image peripheral camera image mechanically coupled foveal vision stabilized 
order mimic canal biological systems attached axis gyro sensor circuit head 
sensors circuit head angular velocity signal acquired bit board 
oculomotor head control loop hz vision control loop hz 
visual tracking optical flow calculation order acquire retinal slip retinal slip velocity respectively 
spatial averaging multiple optical flow reduce noise 
maintain hz vision processing loop rate pixels sampled dots 
due sampling effective angular resolution center image rad 

experimental results online gaze sources nonlinearities biological artificial oculomotor systems muscle nonlinearities nonlinearities added actuators usually heavy cable attached cameras ii perceptual distortion due foveal vision iii axis effects 
non coinciding rotation eye balls head require nonlinear adjustment feedforward controller asa function focal length eye head position 
note tex rectified mean retinal slip rad statistical learning humanoid robots retinal slip rad time sec 
time course mean retinal slip dashed line corresponds linear learning result solid line corresponds non linear learning lwpr inset retinal slip part learning axis effect significant nonlinearity oculomotor system 
learning experiment compare learning performance lwpr non linear online learning algorithm recursive squares rls regression linear learning system ljung 
purpose large board texture appropriate vision processing placed front robot 
distance camera board cm distance emphasized axis nonlinearities 
experiment head moved horizontally sinusoidal signal frequency hz amplitude rad 
fig 
shows time course rectified retinal slip smoothed moving average second time window 
dashed line corresponds rls learning solid line presents learning performance lwpr 
benefits nonlinear learning system clearly demonstrated learning rapid improvement time final retinal slip lwpr half remaining slip linear learning 
fig 
inset shows time course raw retinal slip signals learning 
section effective angular resolution center image rad learning fig 
satisfactory amplitude rad best result achievable visual sensing resolution 
nonlinear component generated rad head rotated rad visual stimulus tex vijayakumar distance analytical computations geometry axis vision head system 
difference consistent average difference results obtained rls lwpr suggesting lwpr able learn nonlinear component generated 

introduced locally weighted projection regression lwpr statistical learning algorithm applications real time learning highly complex humanoid robots 
update complexity lwpr number inputs statistically sound dimensionality reduction learning rules allowed reliable successful real time implementation various learning problems humanoid robotics including inverse dynamics learning inverse kinematics learning oculomotor learning 
results times complex internal models model control learned autonomously real time sophisticated robotic devices 
hope algorithms lwpr allows near equip robots massive line learning abilities come step closer realizing dream completely robots 
atkeson hollerbach 
model control robot manipulator mit press 
atkeson moore schaal 

locally weighted learning 
artificial intelligence review 
bishop 
neural networks pattern recognition 
oxford press 
bullock grossberg 
self organizing neural model motor equivalent reaching tool arm 
journal cognitive neuroscience 
cruse br wer human arm redundant manipulator control path joint angles 
biological cybernetics 
frank friedman 

statistical view regression tools 
technometrics 
jordan supervised learning distal teacher cognitive science pp 

kawato 

feedback error learning neural network supervised motor learning 
editor advanced neural computers pages 
north holland elsevier 
automatic supervisory control configuration behavior multibody 
ieee transactions systems man cybernetics 
tex statistical learning humanoid robots ljung 
theory practice recursive identification 
cambridge mit press 
sanger 

optimal unsupervised learning single layer liner feedforward neural network 
neural networks 
saunders stitson weston bottou smola 
support vector machine manual 
tr csd tr dept computer science royal holloway univ london 
schaal 

imitation learning route humanoid robots 
trends cognitive sciences 
schaal atkeson 

constructive incremental learning local information 
neural comp 
schaal atkeson vijayakumar 

real time robot learning locally weighted statistical learning 
proc 
international conference robotics automation icra 
schaal vijayakumar atkeson 

local dimensionality reduction 
proc 
neural information processing systems 
shibata schaal 

biomimetic gaze stabilization learning nonparametric regression networks 
neural networks 
vol 

pp 

li 

applied nonlinear control 
prentice hall 
schaal inverse kinematics humanoid robots 
proceedings international conference robotics automation icra san francisco ca apr 
vapnik 
nature statistical learning theory 
springer new york 
vijayakumar schaal 

locally weighted projection regression algorithm incremental real time learning high dimensional space 
proc 
international conference machine learning icml pp 
wold 

soft modeling latent variables nonlinear iterative partial squares approach 
perspectives probability statistics 
tex tex 
