tutorial give overview basic ideas underlying support vector sv machines function estimation 
furthermore include summary currently algorithms training sv machines covering quadratic convex programming part advanced methods dealing large datasets 
mention modifications extensions applied standard sv algorithm discuss aspect regularization sv perspective 
purpose twofold 
serve selfcontained support vector regression readers new rapidly developing field research 
hand attempts give overview developments field 
decided organize essay follows 
start giving brief overview basic techniques sections plus short summary number figures diagrams section 
section reviews current algorithmic techniques implementing sv machines 
may interest practitioners 
section covers advanced topics extensions basic sv algorithm connections sv machines regularization briefly mentions methods carrying model selection 
conclude discussion open questions problems current directions sv research 
results review published comprehensive presentations details new 
historic background sv algorithm nonlinear generalization generalized portrait algorithm developed russia sixties vapnik lerner vapnik chervonenkis 
extended version available neurocolt technical report tr 
australian national university canberra australia alex smola anu edu au max planck institut kybernetik bingen germany bernhard tuebingen mpg de term regression somewhat lose includes cases function estimation minimizes errors mean square loss 
done mainly historical reasons vapnik 
similar approach linear quadratic programming taken time usa mainly mangasarian 
tutorial support vector regression alex smola bernhard sch lkopf september firmly grounded framework statistical learning theory vc theory developed decades vapnik chervonenkis vapnik 
nutshell vc theory characterizes properties learning machines enable generalize unseen data 
form sv machine largely developed bell laboratories vapnik workers boser guyon cortes vapnik sch lkopf sch lkopf vapnik :10.1.1.15.9362:10.1.1.103.1189
due industrial context sv research date sound orientation real world applications 
initial focused ocr optical character recognition 
short period time sv classifiers competitive best available systems ocr object recognition tasks sch lkopf blanz sch lkopf 
comprehensive tutorial sv classifiers published burges 
regression time series prediction applications excellent performances soon obtained ller drucker stitson haykin 
snapshot state art sv learning taken annual neural information processing systems conference sch lkopf 
sv learning evolved active area research 
process entering standard methods toolbox machine learning haykin cherkassky hearst 
sch lkopf smola contains depth overview svm regression 
additionally cristianini shawe taylor herbrich provide details kernels context classification 
basic idea suppose training data 
denotes space input patterns 
instance exchange rates currency measured subsequent days corresponding econometric indicators 
sv regression vapnik goal find function deviation obtained targets yi training data time flat possible 
words care errors long accept deviation larger 
may important want sure lose money dealing exchange rates instance 
pedagogical reasons describing case linear functions form denotes dot product flatness case means seeks small way ensure minimize norm write problem convex optimization problem minimize subject yi xi xi yi tacit assumption function exists approximates pairs xi yi precision words convex optimization problem feasible 
may case may want allow errors 
analogously soft margin loss function bennett mangasarian adapted sv machines cortes vapnik introduce slack variables cope infeasible constraints optimization problem 
arrive formulation stated vapnik 
minimize subject yi xi xi yi constant determines trade flatness amount deviations larger tolerated 
corresponds dealing called insensitive loss function described 
fig 
depicts situation graphically 
points outside shaded region contribute cost insofar deviations penalized linear fashion 
turns soft margin loss setting linear svm 
cases optimization problem solved easily dual formulation 
see sec 
dual formulation provides key extending sv machine nonlinear functions 
standard dualization method utilizing lagrange multipliers described fletcher 
see smola overview ways specifying flatness functions :10.1.1.11.2062:10.1.1.11.2062
true long dimensionality higher number observations 
case specialized methods offer considerable computational savings lee mangasarian 
dual problem quadratic programms key idea construct lagrange function objective function called primal objective function rest article corresponding constraints introducing dual set variables 
shown function saddle point respect primal dual variables solution 
details see mangasarian mccormick vanderbei explanations section 
proceed follows yi xi yi xi lagrangian lagrange multipliers 
dual variables satisfy positivity constraints 
note refer follows saddle point condition partial derivatives respect primal variables vanish optimality 
maximize bl wl xi substituting yields dual optimization problem 
xi xj subject yi deriving eliminated dual variables condition reformulated eq 
rewritten follows xi xi 
called support vector expansion completely described linear combination training patterns xi 
sense complexity function representation svs independent dimensionality input space depends number svs 
note complete algorithm described terms dot products data 
evaluating need compute explicitly 
observations come handy formulation nonlinear extension 
computing far neglected issue computing done exploiting called karush kuhn tucker kkt conditions karush kuhn tucker 
state point solution product dual variables constraints vanish 
yi xi yi xi 
allows useful 
firstly lie outside samples xi yi corresponding insensitive tube 
secondly set dual variables simultaneously nonzero 
allows conclude yi xi yi xi conjunction analogous analysis max yi xi min yi xi inequalities equalities 
see keerthi means choosing way computing discussed context interior point optimization cf 
sec 

turns product optimization process 
considerations shall deferred corresponding section 
see keerthi methods compute constant offset 
final note regarding sparsity sv expansion 
follows xi yi lagrange multipliers may nonzero words samples inside tube shaded region fig 
vanish xi yi second factor nonzero zero kkt conditions satisfied 
sparse expansion terms xi need xi describe 
examples come nonvanishing coefficients called support vectors 
kernels nonlinearity preprocessing step sv algorithm nonlinear 
instance achieved simply preprocessing training patterns xi map feature space described aizerman nilsson applying standard sv regression algorithm 
brief look example vapnik 
example quadratic features consider map understood subscripts case refer components training linear sv machine preprocessed features yield quadratic function 
approach reasonable particular example easily computationally infeasible polynomial features higher order higher dimensionality number different monomial features degree dim 
typical values ocr tasks performance sch lkopf sch lkopf vapnik corresponding approximately features 
implicit mapping kernels clearly approach feasible find computationally cheaper way 
key observation boser feature map example :10.1.1.103.1189
noted previous section sv algorithm depends dot products patterns xi 
suffices know explicitly allows restate sv optimization problem maximize subject xi xj yi likewise expansion may written xi xi 
difference linear case longer explicitly 
note nonlinear setting optimization problem corresponds finding function feature space input space 
conditions kernels question arises functions correspond dot product feature space theorem characterizes functions defined 
theorem mercer suppose integral operator tk positive denotes measure finite supp 
eigenfunction tk associated eigenvalue normalized denote complex conjugate 


sup 

holds series converges absolutely uniformly 
formally speaking theorem means holds write dot product feature space 
condition conclude simple rules compositions kernels satisfy mercer condition sch lkopf 
call functions admissible sv kernels 
corollary positive linear combinations kernels denote admissible sv kernels admissible kernel 
follows directly virtue linearity integrals 
generally show set admissible kernels forms convex cone closed topology pointwise convergence berg 
corollary integrals kernels symmetric function dz exists 
admissible sv kernel 
shown directly rearranging order integration 
state necessary sufficient condition translation invariant kernels derived smola 
theorem products kernels denote admissible sv kernels admissible kernel 
seen application expansion part mercer theorem kernels observing term double sum gives rise positive coefficient checking 
theorem smola sch lkopf ller translation invariant kernel admissible sv kernels fourier transform dx nonnegative 
give proof additional explanations theorem section 
follows interpolation theory micchelli theory regularization networks girosi 
kernels dot product type exist sufficient conditions admissible 
theorem burges kernel dot product type satisfy order admissible sv kernel 
note conditions theorem necessary sufficient 
rules stated useful tools practitioners checking kernel admissible sv kernel constructing new kernels 
general case theorem 
theorem schoenberg kernel dot product type defined infinite dimensional hilbert space power series expansion ant admissible 
slightly weaker condition applies finite dimensional spaces 
details see berg smola 
examples sch lkopf shown explicitly computing mapping homogeneous polynomial kernels suitable sv kernels cf 
poggio 
observation conclude immediately boser vapnik kernels type inhomogeneous polynomial kernels admissible rewrite sum homogeneous kernels apply corollary :10.1.1.103.1189
kernel appealing due resemblance neural networks hyperbolic tangent kernel tanh 
applying theorem check kernel satisfy mercer condition 
curiously kernel successfully practice cf 
sch lkopf discussion reasons 
translation invariant kernels quite widespread 
shown aizerman micchelli boser admissible sv kernel :10.1.1.103.1189
show smola vapnik denotes indicator function set convolution operation bk ko splines order defined convolution unit admissible 
shall postpone considerations section connection regularization operators pointed detail 
cost functions far sv algorithm regression may strange hardly related existing methods function estimation huber stone hastie tibshirani wahba 
cast standard mathematical notation observe connections previous 
sake simplicity consider linear case extensions nonlinear straightforward kernel method described previous chapter 
risk functional moment go back case section 
training data 
assume training set drawn iid independent identically distributed probability distribution 
goal find function minimizing expected risk cf 
vapnik dp denotes cost function determining penalize estimation errors empirical data know distribution estimating function minimizes 
possible approximation consists replacing integration empirical estimate get called empirical risk functional remp xi yi xi 
attempt find empirical risk minimizer argmin remp function class rich capacity high instance dealing data high dimensional spaces may idea lead overfitting bad generalization properties 
add capacity control term sv case leads regularized risk functional tikhonov arsenin vapnik remp called regularization constant 
algorithms regularization networks girosi neural networks weight decay networks bishop minimize expression similar 
maximum likelihood density models standard setting sv case mentioned section insensitive loss 
straightforward show minimizing particular loss function equivalent minimizing difference 
loss functions may desirable superlinear increase leads loss robustness properties estimator huber cases derivative cost function grows bound 
hand nonconvex 
case recover mean squares fit approach standard sv loss function leads matrix inversion quadratic programming problem 
question cost function 
hand want avoid complicated function may lead difficult optimization problems 
hand particular cost function suits problem best 
assumption samples generated underlying functional dependency plus additive noise yi ftrue xi density optimal cost function maximum likelihood sense log 
seen follows 
likelihood estimate xf 
additive noise iid data xf xi xi yi yi xi 
maximizing xf equivalent minimizing log xf 
get log xf xi yi xi 
cost function resulting reasoning nonconvex 
case find convex proxy order deal situation efficiently find efficient implementation corresponding optimization problem 
hand specific cost function real world problem try find close proxy cost function possible performance wrt 
particular cost function matters ultimately 
table contains overview common density models corresponding loss functions defined 
requirement impose fixed convexity 
requirement want ensure existence uniqueness strict convexity minimum optimization problems fletcher 
solving equations sake simplicity additionally assume symmetric symmetry discontinuities derivative zero interval 
loss functions table belong class 
take form 
note similarity vapnik insensitive loss 
straightforward extend special choice general convex cost functions 
nonzero cost functions interval additional pair slack variables 
choose different cost functions ci different values sample 
expense additional lagrange multipliers dual formulation additional discontinuities taken care 
analogously arrive convex minimization problem smola sch lkopf 
simplify notation stick normalizing 
minimize subject yi xi xi yi standard lagrange multiplier techniques exactly manner case compute dual optimization problem main difference slack variable terms nonvanishing derivatives 
omit indices applicable avoid tedious notation 
yields maximize xi xj yi xi subject inf examples consider examples table 
show explicitly examples simplified bring form practically useful 
insensitive case get 
conclude inf 
case piecewise polynomial loss distinguish different cases 
case get inf second case 
inf turn yields 
combining cases 
table contains summary various conditions formulas strictly speaking different cost functions 
note maximum slope determines region feasibility sup leads compact intervals cs 
means influence single pattern bounded leading robust estimators huber 
observe experimentally loss function density model insensitive exp laplacian exp gaussian huber robust loss polynomial piecewise polynomial ct insensitive laplacian gaussian huber robust loss exp exp exp exp exp exp table common loss functions corresponding density models polynomial piecewise polynomial table terms convex optimization problem depending choice loss function 
performance sv machine depends significantly cost function ller smola 
cautionary necessary regarding cost functions insensitive 
lose advantage sparse decomposition 
may acceptable case data render prediction step extremely slow 
trade potential loss prediction accuracy faster predictions 
note reduced set algorithm burges burges sch lkopf sch lkopf sparse decomposition techniques smola sch lkopf applied address issue 
bayesian setting tipping shown cost function sacrificing sparsity 
bigger picture delving algorithmic details implementation briefly review basic properties sv algorithm regression described far 
contains graphical overview different steps regression stage 
input pattern prediction mapped feature space map 
dot products computed images training patterns map 
corresponds evaluating kernel functions xi 
dot products added weights plus constant term yields final prediction output 
process described table displays ct plugged directly corresponding optimization equations 



output weights dot product mapped vectors support vectors test vector architecture regression machine constructed sv algorithm 
similar regression neural network difference sv case weights input layer subset training patterns 
demonstrates sv algorithm chooses function approximating original data precision 
requiring flatness feature space observe functions flat input space 
due fact kernels associated flatness properties regularization operators 
explained detail section 
fig 
shows relation approximation quality sparsity representation sv case 
lower precision required approximating original data fewer svs needed encode 
non svs redundant patterns training set sv machine constructed exactly function think efficient way data compression storing support patterns estimate reconstructed completely 
simple analogy turns fail case high dimensional data drastically presence noise 
vapnik see moderate approximation quality number svs considerably high yielding rates worse nyquist rate nyquist shannon 
sinc sinc approximation sinc sinc approximation sinc sinc approximation left right approximation function sinc precisions 
solid top bottom lines indicate size tube dotted line regression 
left right regression solid line datapoints small dots svs big dots approximation 
note decrease number svs 
optimization algorithms large number implementations sv algorithms past years focus algorithms greater detail 
selection somewhat biased contains algorithms authors familiar 
think overview contains effective ones useful practitioners code sv machine 
doing briefly cover major optimization packages strategies 
implementations commercially available packages quadratic programming train sv machines 
usually numerically stable general purpose codes special enhancements large sparse systems 
feature needed sv problems dot product matrix dense huge success 
osl package written ibm 
uses phase algorithm 
step consists solving linear approximation qp problem simplex algorithm dantzig 
related simple qp problem dealt 
successive ap high price tag usually major deterrent 
bear mind sv regression may speed solution considerably exploiting fact quadratic form special structure may exist rank degeneracies kernel matrix 
close second permits quadratic objective converges rapidly starting value 
interior point algorithm added software suite 
cplex cplex optimization uses logarithmic barrier algorithm megiddo predictor corrector step see 
lustig mehrotra sun 
minos stanford optimization laboratory murtagh saunders uses reduced gradient algorithm conjunction quasi newton algorithm 
constraints handled active set strategy 
feasibility maintained process 
active constraint manifold quasi newton approximation 
matlab matlab qp optimizer delivered agreeable average performance classification tasks useful regression tasks problems larger samples due fact effectively dealing optimization problem size half eigenvalues hessian vanish 
problems addressed version 
matlab uses interior point codes 
loqo vanderbei example interior point code 
section discusses underlying strategies detail shows adapted sv algorithms 
maximum margin perceptron kowalczyk algorithm specifically tailored svs 
techniques works directly primal space take equality constraint lagrange multipliers account explicitly 
iterative free set methods algorithm kaufman bunch bunch kaufman drucker kaufman uses technique starting variables boundary adding karush kuhn tucker conditions violated 
approach advantage having compute full dot product matrix 
evaluated fly yielding performance improvement comparison tackling optimization problem 
algorithms modified subset selection techniques see section address problem 
basic notions algorithms rely results duality theory convex optimization 
happened mention basic ideas section sake convenience briefly review proof core results 
needed particular derive interior point algorithm 
details proofs see fletcher 
uniqueness convex constrained optimization problem unique minimum 
problem strictly convex solution unique 
means svs plagued problem local minima neural networks 
lagrange function lagrange function primal objective function minus sum products constraints corresponding lagrange multipliers cf 
fletcher bertsekas 
optimization seen lagrangian wrt 
primal variables simultaneous maximization wrt 
lagrange multipliers dual variables 
saddle point solution 
usually lagrange function theoretical device derive dual objective function cf 
sec 

dual objective function derived minimizing lagrange function respect primal variables subsequent elimination 
written solely terms dual variables 
duality gap feasible primal dual variables primal objective function convex minimization problem greater equal dual objective function 
svms linear constraints large noisy problems patterns substantial fraction lagrange multipliers impossible solve problem exactly due size subset selection algorithms joint optimization training set impossible 
neural networks determine closeness optimum 
note reasoning holds convex cost functions 
constraint qualifications strong duality theorem theorem satisfied follows gap vanishes optimality 
duality gap measure close terms objective function current set variables solution 
karush kuhn tucker kkt conditions set primal dual variables feasible satisfies kkt conditions solution constraint dual variable 
sum violated kkt terms determines exactly size duality gap simply compute constraint part done 
allows compute quite easily 
simple intuition violated constraints dual variable increased arbitrarily rendering lagrange function arbitrarily large 
property 
interior point algorithms nutshell idea interior point algorithm compute dual optimization problem case dual dual solve primal dual simultaneously 
done gradually enforcing kkt conditions iteratively find feasible solution duality gap primal dual objective function determine quality current set variables 
special flavour algorithm describe primal dual path vanderbei 
order avoid tedious notation consider slightly general problem specialize result svm 
understood stated variables denote vectors denotes th component 
minimize subject inequalities vectors holding componentwise convex function 
add slack variables get rid inequalities positivity constraints 
yields minimize dual maximize subject subject free ay free get kkt conditions 

necessary sufficient condition optimal solution primal dual variables satisfy feasibility conditions kkt conditions 
proceed solve iteratively 
details appendix useful tricks proceeding algorithms quadratic optimization briefly mention useful tricks applied algorithms described subsequently may significant impact despite simplicity 
part derived ideas interior point approach 
training different regularization parameters reasons model selection controlling number support vectors may happen train sv machine different regularization parameters identical settings 
parameters cnew cold different advantageous rescaled values lagrange multipliers starting point new optimization problem 
rescaling necessary satisfy modified constraints 
gets new old likewise bnew bold 
assuming dominant convex part primal objective quadratic scales linear part scales 
linear term dominates objective function rescaled values better starting point 
practice speedup approximately training time observed sequential minimization algorithm cf 
smola :10.1.1.11.2062
similar reasoning applied retraining regularization parameter different similar width parameters kernel function 
see cristianini details thereon different context 
monitoring convergence feasibility gap case primal dual feasible variables connection primal dual objective function holds dual obj 
primal obj 
seen immediately construction lagrange function 
regression estimation insensitive loss function obtains max xi yi min xi yi max yi xi min yi xi 
convergence respect point solution expressed terms duality gap 
effective stopping rule require tol primal objective precision tol 
condition spirit primal dual interior point path algorithms convergence measured terms number significant figures decimal logarithm convention adopted subsequent parts exposition 
subset selection algorithms convex programming algorithms described far directly moderately sized samples datasets modifications 
large datasets difficult due memory cpu limitations compute dot product matrix xi xj keep memory 
simple calculation shows instance storing dot product matrix nist ocr database samples single precision consume gbytes 
cholesky decomposition thereof additionally require roughly amount memory counting multiplies adds separately unrealistic current processor speeds 
solution introduced vapnik relies observation solution reconstructed svs 
knew sv set fitted memory directly solve reduced problem 
catch know sv set solving problem 
solution start arbitrary subset chunk fits memory train sv algorithm keep svs fill chunk data current estimator errors data lying outside tube current regression 
retrain system keep iterating training kkt conditions satisfied 
basic chunking algorithm just postponed underlying problem dealing large datasets dot product matrix kept memory occur larger training set sizes originally completely avoided 
solution osuna subset variables working set optimize problem respect freezing variables 
method described detail osuna joachims saunders case pattern recognition 
adaptation techniques case regression convex cost functions appendix basic structure method described algorithm 
algorithm basic structure working set algorithm 
initialize choose arbitrary working set sw repeat compute coupling terms linear constant sw see appendix solve reduced optimization problem choose new sw variables satisfying kkt conditions working set sw sequential minimal optimization algorithm sequential minimal optimization smo proposed platt puts chunking similar technique employed bradley mangasarian context linear programming order deal large datasets 
extreme iteratively selecting subsets size optimizing target function respect 
reported convergence properties easily implemented 
key point working set optimization subproblem solved analytically explicitly invoking quadratic optimizer 
readily derived pattern recognition platt simply original reasoning obtain extension regression estimation 
done appendix pseudocode smola sch lkopf 
modifications consist pattern dependent regularization convergence control number significant figures modified system equations solve optimization problem variables regression analytically 
note reasoning applies sv regression insensitive loss function convex cost functions explicit solution restricted quadratic programming problem impossible 
derive analogous non quadratic convex optimization problem general cost functions expense having solve numerically 
exposition proceeds follows derive modified boundary conditions constrained indices subproblem regression proceed solve optimization problem analytically check part selection rules modified approach regression 
content fairly technical relegated appendix main difference implementations smo regression way constant offset determined keerthi criterion select new set variables 
strategy appendix 
selection strategies focus current research recommend readers interested implementing algorithm sure aware developments area 
note just presently describe generalization smo regression estimation learning problems benefit underlying ideas 
smo algorithm training novelty detection systems class classification proposed sch lkopf 
variations theme exists large number algorithmic modifications sv algorithm suitable specific settings inverse problems semiparametric settings different ways measuring capacity reductions linear programming convex combinations different ways controlling capacity 
mention popular ones 
convex combinations norms algorithms far involved convex best quadratic programming 
think reducing problem case linear programming techniques applied 
done straightforward fashion mangasarian weston smola sv pattern recognition regression 
key replace remp denotes norm coefficient space 
uses sv kernel expansion ik xi different way controlling capacity minimizing xi yi xi 
insensitive loss function leads linear programming problem 
cases problem stays quadratic general convex may yield desired computational advantage 
limit derivation linear programming problem case cost function 
reformulating yields minimize yi subject xj xi xj xi yi classical sv case transformation dual give improvement structure optimization problem 
best minimize directly achieved linear optimizer dantzig lustig vanderbei 
weston similar variant linear sv approach estimate densities line 
show smola may obtain bounds generalization error exhibit better rates terms entropy numbers classical sv case williamson 
automatic tuning insensitivity tube standard model selection issues specify trade empirical error model capacity exists problem optimal choice cost function 
particular insensitive cost function problem choosing adequate parameter order achieve performance sv machine 
smola show existence linear dependency noise level optimal parameter sv regression 
require know noise model 
knowledge available general 
albeit providing theoretical insight finding particularly useful practice 
really knew noise model choose insensitive cost function corresponding maximum likelihood loss function 
exists method construct sv machines automatically adjust asymptotically predetermined fraction sampling points svs sch lkopf 
modify variable optimization problem including extra term primal objective function attempts minimize 
words minimize remp 
carrying usual transformation minimize subject yi xi xi yi note holds convex loss functions insensitive zone 
sake simplicity exposition stick standard loss function 
computing dual yields maximize subject xi xj yi note optimization problem similar sv target function simpler homogeneous additional constraint 
information affects implementation cf 
chang lin 
having advantage able automatically determine advantage 
pre specify number svs theorem sch lkopf 
upper bound fraction errors 

lower bound fraction svs 

suppose data generated iid distribution continuous conditional distribution 
probability asymptotically equals fraction svs fraction errors 
essentially sv regression improves sv regression allowing tube width adapt automatically data 
kept fixed point shape tube 
go step parametric tube models non constant width leading identical optimization problems sch lkopf 
combining sv regression results asymptotical optimal choice noise model smola leads guideline adjust provided class noise models gaussian laplacian known 
optimal choice denote probability density unit variance noise models generated assume data drawn iid continuous 
assumption uniform convergence asymptotically optimal value dt argmin dt polynomial noise models densities type exp may compute corresponding asymptotically optimal values 

details see sch lkopf smola experimental validation :10.1.1.11.2062
optimal optimal unit variance polynomial degree optimal various degrees polynomial additive noise 
conclude section noting sv regression related idea trimmed estimators 
show regression influenced perturb points lying outside tube 
regression essentially computed discarding certain fraction outliers specified computing regression estimate remaining points sch lkopf 
regularization far concerned specific properties map feature space convenient trick construct nonlinear regression functions 
cases map just implicitly kernel map properties neglected 
deeper understanding kernel map useful choose appropriate kernels specific task incorporating prior knowledge sch lkopf 
feature map defy curse dimensionality bellman making problems seemingly easier reliable map higher dimensional space 
section focus connections sv methods previous techniques regularization net works girosi 
particular show sv machines essentially regularization networks rn clever choice cost functions kernels green function corresponding regularization operators 
full exposition subject reader referred smola 
regularization networks briefly review basic concepts rns 
minimize regularized risk functional 
enforcing flatness feature space try optimize smoothness criterion function input space 
get remp 
denotes regularization operator sense tikhonov arsenin positive semidefinite operator mapping hilbert space functions consideration dot product space expression defined instance choosing suitable operator penalizes large variations reduce known overfitting effect 
possible setting operator mapping reproducing kernel hilbert space rkhs kimeldorf wahba saitoh sch lkopf girosi 
expansion terms symmetric function xi xj note need fulfill mercer condition chosen arbitrarily define regularization term ik xi insensitive cost function leads quadratic programming problem similar svs 
dij xi xj due length constraints deal connection gaussian processes svms 
see williams excellent overview 
get solution minimize subject kd 
unfortunately setting problem preserve sparsity terms coefficients potentially sparse decomposition terms spoiled general diagonal 
green functions comparing leads question condition methods equivalent conditions regularization networks lead sparse decompositions expansion coefficients differ zero 
sufficient condition kd full rank need kd holds image xi xj xi xj goal solve problems 
regularization operator find kernel sv machine enforce flatness feature space correspond minimizing regularized risk functional regularizer 

sv kernel find regularization operator sv machine kernel viewed regularization network problems solved employing concept green functions described girosi 
functions introduced purpose solving differential equations 
context sufficient know green functions gx satisfy gx 
distribution confused kronecker symbol ij property xi 
relationship kernels regularization operators formalized proposition proposition smola sch lkopf ller regularization operator green function mercer kernel sv machines minimize risk functional regularization operator 
exploit relationship ways compute green functions regularization operator infer regularizer kernel translation invariant kernels specifically consider regularization operators may written multiplications fourier space denoting fourier transform real valued nonnegative converging supp 
small values correspond strong attenuation corresponding frequencies 
small values large desirable high frequency components correspond rapid changes describes filter properties note attenuation takes place frequencies excluded integration domain 
regularization operators defined fourier space show exploiting xi rn xi corresponding green function satisfying translational invariance xi xj xi xj 
provides efficient tool analyzing sv kernels types capacity control exhibit 
fact special case theorem stating fourier transform positive measure constitutes positive hilbert schmidt kernel 
example gaussian kernels exposition yuille described girosi see dx laplacian gradient operator get gaussians kernels 
provide equivalent representation terms fourier properties multiplicative constant 
training sv machine gaussian rbf kernels sch lkopf corresponds minimizing specific cost function regularization operator type 
recall means derivatives penalized operator obtain smooth estimate 
explains performance sv machines case means obvious choosing flat function high dimensional space correspond simple function low dimensional space shown smola dirichlet kernels 
question arises kernel choose 
think extreme situations 

suppose knew shape power spectrum pow function estimate 
case choose matches power spectrum smola :10.1.1.11.2062

happen know little data general smoothness assumption reasonable choice 
want choose gaussian kernel 
computing time important consider kernels compact support bq spline kernels cf 

choice cause matrix elements kij xi xj vanish 
usual scenario extreme cases limited prior knowledge available 
information prior knowledge choosing kernels see sch lkopf 
capacity control reasoning far assumption exist ways determine model parameters regularization constant length scales rbf kernels 
model selection issue easily double length review area active rapidly moving research 
limit presentation basic concepts refer interested reader original publications 
important keep mind exist fundamentally different approaches minimum description length cf 
rissanen li vit nyi idea simplicity estimate plausibility information number bits needed encode reconstructed 
bayesian estimation hand considers posterior probability estimate observations 
observation noise model prior probability distribution space estimates parameters 
bayes rule 
depend maximize obtain called map estimate 
rule thumb translate regularized risk functionals bayesian map estimation schemes consider exp 
detailed discussion see kimeldorf wahba mackay neal rasmussen williams :10.1.1.31.4284
simple powerful way model selection cross validation 
idea expectation error subset training sample training identical expected error 
exist strategies fold crossvalidation leave error fold crossvalidation bootstrap derived algorithms estimate crossvalidation error 
see stone wahba efron efron tibshirani wahba jaakkola haussler details 
strictly speaking bayesian estimation concerned maximizer posterior distribution may uniform convergence bounds ones introduced vapnik chervonenkis 
basic idea may bound probability expected risk remp confidence term depending class functions criteria measuring capacity exist vc dimension pattern recognition problems maximum number points separated function class possible ways covering number number elements needed cover accuracy entropy numbers functional inverse covering numbers variants thereof 
see vapnik devroye williamson shawe taylor :10.1.1.33.8995
due quite large body done field sv research impossible write tutorial sv regression includes contributions field 
quite scope tutorial relegated textbooks matter see sch lkopf smola comprehensive overview sch lkopf snapshot current state art vapnik overview statistical learning theory cristianini shawe taylor introductory textbook 
authors hope provides overly biased view state art sv regression research 
deliberately omitted topics 
missing topics mathematical programming starting completely different perspective algorithms developed similar ideas sv machines 
primer bradley 
see mangasarian street mangasarian 
comprehensive discussion connections mathematical programming sv machines bennett 
density estimation sv machines weston vapnik 
fact cumulative distribution function monotonically increasing values predicted variable confidence adjusted selecting different values loss function 
dictionaries originally introduced context wavelets chen allow large class basis functions considered simultaneously kernels different widths 
standard sv case hardly possible defining new kernels linear combinations differently scaled ones choosing regularization operator determines kernel completely kimeldorf wahba cox sullivan sch lkopf 
resort linear programming weston 
applications focus review methods theory applications 
done limit size exposition 
state art record performance reported ller drucker stitson haykin 
cases may possible achieve similar performance neural network methods parameters optimally tuned hand depending largely skill experimenter 
certainly sv machines silver bullet critical parameters regularization kernel width state art results achieved relatively little effort 
open issues active field exist number open issues addressed research 
algorithmic development stable stage important ones find tight error bounds derived specific properties kernel functions 
interest context sv machines similar approaches stemming linear programming regularizer lead satisfactory results 
sort luckiness framework shawe taylor multiple model selection parameters similar multiple hyperparameters automatic relevance detection bayesian statistics mackay bishop devised sv machines dependent skill experimenter :10.1.1.31.4284
worth exploit bridge regularization operators gaussian processes priors see williams state bayesian risk bounds sv machines order compare predictions ones vc theory 
optimization techniques developed context sv machines deal large datasets gaussian process settings 
prior knowledge appears important question sv regression 
whilst invariances included pattern recognition principled way virtual sv mechanism restriction feature space burges sch lkopf sch lkopf clear probably subtle properties required regression dealt efficiently 
reduced set methods considered speeding prediction possibly training phase large datasets burges sch lkopf osuna girosi sch lkopf smola sch lkopf 
topic great importance data mining applications require algorithms able deal databases order magnitude larger samples current practical size sv regression 
aspects data dependent generalization bounds efficient training algorithms automatic kernel selection procedures techniques way standard neural networks toolkit considered 
readers tempted embark detailed exploration topics contribute ideas exciting field may find useful consult web page www kernel machines org 
supported part dfg ja sm 
authors peter bartlett chris burges stefan mangasarian klaus robert ller vladimir vapnik jason weston robert williamson andreas helpful discussions comments 
solving interior point equations path trying satisfy directly solve modified version thereof substituted rhs place decrease iterating 


difficult solve nonlinear system equations exactly 
interested obtaining exact solution approximation 
seek somewhat feasible solution decrease repeat 
done linearizing system solving resulting equations predictor corrector approach duality gap small 
advantage get approximately equal performance trying solve quadratic system directly provided terms small 
gi gi zi zi si si ti ti solving variables get ay denotes vector 
gn analogously 
denote vector generated componentwise product vectors 
solving get formulate reduced kkt system see vanderbei quadratic case iteration strategies predictor corrector method proceed follows 
predictor step solve system terms rhs set values substituted back definitions solved corrector step 
quadratic part affected predictor corrector steps need invert quadratic matrix 
done best manually pivoting part positive definite 
values obtained iteration step update corresponding values 
ensure variables meet positivity constraints chosen variables move initial distance boundaries positive orthant 
usually vanderbei sets 
heuristic computing parameter determining kkt conditions enforced 
obviously aim reduce fast possible happen choose small condition equations worsen drastically 
setting proven robustly 
rationale average satisfaction kkt conditions point decrease rapidly far away boundaries positive orthant variables constrained 
come initial values 
analogously vanderbei choose regularized version order determine initial conditions 
solves subsequently restricts solution feasible set max min min min ay min ay 
denotes function 
special considerations sv regression algorithm described far applied sv pattern recognition regression estimation 
standard setting pattern recognition xi xj consequently xi xj hessian dense thing compute cholesky factorization compute 
case sv regression 

xi xj xi xj ij xi xj analogously 
deal ing matrix type diagonal matrices 
applying orthogonal transformation inverted essentially inverting matrix system 
exactly additional advantage gain implementing optimization algorithm directly general purpose optimizer 
show practical implementations smola solve optimization problems nearly arbitrary convex cost functions efficiently special case insensitive loss functions 
note due fact solving primal dual optimization problem simultaneously computing parameters corresponding initial sv optimization problem 
observation useful allows obtain constant term directly setting see smola details :10.1.1.11.2062:10.1.1.11.2062
solving subset selection problem subset optimization problem adapt exposition joachims case regression convex cost functions 
loss generality assume situations treated special case 
extract reduced optimization problem working set variables kept fixed 
denote sw 
working set sf 
sw fixed set 
writing optimization problem terms sw yields maximize sw xi xj sw yi sf xi xj sw subject sw sf update linear term coupling fixed set sw sf xi xj equality constraint 
easy see maximizing decreases exactly amount 
choose variables kkt conditions satisfied objective function tends decrease whilst keeping variables feasible 
bounded 
prove convergence statement osuna algorithm proves useful practice 
methods kaufman platt deal problems quadratic part completely fit memory 
practice take special precautions avoid stalling convergence results chang indicate certain conditions proof convergence possible 
crucial part sw 
note optimality convenience kkt conditions repeated slightly modified form 
denote error current estimate sample xi mx yi xi yi xi xj 
rewriting feasibility conditions terms yields si zi 
zi si 
set dual feasible variables zi max si min max min consequently kkt conditions translated si variables violating conditions may selected optimization 
cases especially initial stage optimization algorithm set patterns larger practical size sw 
unfortunately osuna contains little information select sw 
heuristics adaptation joachims regression 
see lin details optimization svr 
selection rules similarly merit function approach el idea select variables violate contribute feasibility gap 
defines score variable si construction size feasibility gap cf 
case insensitive loss 
decreasing gap approaches solution upper bounded primal objective lower bounded dual objective function 
selection rule choose patterns largest 
algorithms zi si si zi si si 
see mutually imply 
gives measure contribution variable size feasibility gap 
note heuristics assigning sticky flags cf 
burges variables boundaries effectively solving smaller subproblems completely removing corresponding patterns training set accounting couplings joachims significantly decrease size problem solve result noticeable speedup 
caching joachims kowalczyk computed entries dot product matrix may significant impact performance 
solving smo equations pattern dependent regularization consider constrained optimization problem indices say 
pattern dependent regularization means ci may different pattern possibly different 
variables may nonzero time dealing constrained optimization problem may express terms just variable 
summation constraint obtain old old old old regression 
exploiting yields account fact may different pairs nonzero variables 
convenience define auxiliary variables case 
max cj min ci max min cj analytic solution regression max min ci max min solve optimization problem analytically 
substitute values reduced optimization problem 
particular yi kij sw sw old old kij 
auxiliary variables kii kjj kij obtains constrained optimization problem eliminating ignoring terms independent noting holds maximize old old subject 
unconstrained maximum respect 
old ii old iii old iv old problem know quadrants iv contains solution 
considering sign distinguish cases iii possible coefficients satisfy cases ii iv 
case ii iii considered 
see diagram 
iii iv ii best start quadrant test unconstrained solution hits boundaries probe corresponding adjacent quadrant ii iii 
dealt analogously 
due numerical instabilities may happen 
case set solve linear fashion directly 
negative values theoretically impossible satisfies mercer condition xi xj kii kjj kij 
selection rule regression pick indices objective function maximized 
reasoning smo platt sec 
classification mimicked 
means loop approach chosen maximize objective function 
outer loop iterates patterns violating kkt conditions lagrange multipliers upper lower boundary satisfied patterns violating kkt conditions ensure self consistency complete dataset 
solves problem choosing large step minimum looks large steps computationally expensive compute possible pairs chooses heuristic maximize absolute value numerator expressions 
index corresponding maximum absolute value chosen purpose 
heuristic happens fail words little progress choice indices looked called second choice platt way 
indices corresponding non bound examples looked searching example progress 

case heuristic unsuccessful samples analyzed example progress 

previous steps fail proceed detailed discussion see platt 
interior point algorithms smo automatically provide value chosen section having close look lagrange multipliers stopping criteria obtained 
essentially minimizing constrained primal optimization problem ensure dual objective function increases iteration step 
knows minimum value objective function lies interval dual objective primal objective steps interval maxj dual objective primal objective uses determine quality current solution 
useful especially dealing noisy data iterate complete kkt violating dataset complete self consistency subset achieved 
computational resources spent making subsets self consistent globally self consistent 
reason pseudo code global loop initiated non bound variables changed 
open question subset selection optimization algorithm devised decreases primal dual objective function time 
problem usually involves number dual variables order sample size attempt 
calculation primal objective function prediction errors straightforward 
uses kij yi definition avoid matrix vector multiplication dot product matrix 
aizerman 
braverman theoretical foundations potential function method pattern recognition learning 
automation remote control 

theory reproducing kernels 
transactions american mathematical society 
sherali shetty 
nonlinear programming theory algorithms 
wiley nd edition 
bellman 
adaptive control processes 
princeton university press princeton nj 
bennett 
combining support vector mathematical programming methods induction 
sch lkopf burges smola editors advances kernel methods sv learning pages cambridge ma 
mit press 
bennett mangasarian 
robust linear programming discrimination linearly inseparable sets 
optimization methods software 
berg christensen 
harmonic analysis semigroups 
springer new york 
bertsekas 
nonlinear programming 
athena scientific belmont ma 
bishop 
neural networks pattern recognition 
clarendon press oxford 
blanz sch lkopf burges vapnik vetter 
comparison view object recognition algorithms realistic models 
von der malsburg von seelen sendhoff editors artificial neural networks icann pages berlin 
springer lecture notes computer science vol 


lectures fourier integral 
princeton univ press princeton new jersey 
boser guyon vapnik 
training algorithm optimal margin classifiers 
haussler editor proceedings annual conference computational learning theory pages pittsburgh pa july 
acm press 
bradley fayyad mangasarian 
data mining overview optimization opportunities 
technical report university wisconsin computer sciences department madison january 
informs journal computing appear 
bradley mangasarian 
feature selection concave minimization support vector machines 
shavlik editor proceedings international conference machine learning pages san francisco california 
morgan kaufmann publishers 
ftp ftp cs wisc edu math prog tech reports ps bunch kaufman 
stable methods calculating inertia solving symmetric linear systems 
mathematics computation 
bunch kaufman 
computational method indefinite quadratic programming problem 
linear algebra applications pages december 
bunch kaufman parlett 
decomposition symmetric matrix 
numerische mathematik 
burges 
simplified support vector decision rules 
saitta editor proceedings international conference machine learning pages san mateo ca 
morgan kaufmann publishers 
burges 
tutorial support vector machines pattern recognition 
data mining knowledge discovery 
burges 
geometry invariance kernel methods 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
burges sch lkopf 
improving accuracy speed support vector learning machines 
mozer jordan petsche editors advances neural information processing systems pages cambridge ma 
mit press 
sch lkopf smola 
choosing support vector regression different noise models theory experiments 
proceedings ieee inns international joint conference neural networks ijcnn como italy 

chang 
hsu 
lin 
analysis decomposition methods support vector machines 
proceeding ijcai svm workshop 
chang lin 
training support vector classifiers theory algorithms 
neural computation 
chen donoho saunders 
atomic decomposition basis pursuit 
siam journal scientific computing 
cherkassky 
learning data 
john wiley sons new york 
cortes vapnik 
support vector networks 
machine learning 
cox sullivan 
asymptotic analysis penalized likelihood related estimators 
annals statistics 
cplex optimization cplex callable library 
manual 
cristianini shawe taylor 
support vector machines 
cambridge university press cambridge uk 
nello cristianini colin campbell john shawe taylor 
multiplicative support vector learning 
neurocolt technical report nc tr royal holloway college 
dantzig 
linear programming extensions 
princeton univ press princeton nj 
devroye gy rfi lugosi 
probabilistic theory pattern recognition 
number applications mathematics 
springer new york 
drucker burges kaufman smola vapnik 
support vector regression machines 
mozer jordan petsche editors advances neural information processing systems pages cambridge ma 
mit press 
efron 
bootstrap resampling plans 
siam philadelphia 
efron tibshirani 
bootstrap 
chapman hall new york 
el tapia tsuchiya zhang 
formulation theory newton interior point method nonlinear programming 
optimization theory applications 
fletcher 
practical methods optimization 
john wiley sons new york 
girosi 
equivalence sparse approximation support vector machines 
neural computation 
girosi jones poggio 
priors stabilizers basis functions regularization radial tensor additive splines 
memo artificial intelligence laboratory massachusetts institute technology 
guyon boser vapnik 
automatic capacity tuning large vc dimension classifiers 
hanson cowan giles editors advances neural information processing systems pages 
morgan kaufmann publishers 

applied nonparametric regression volume econometric society monographs 
cambridge university press 
hastie tibshirani 
generalized additive models volume monographs statistics applied probability 
chapman hall london 
haykin 
neural networks comprehensive foundation 
macmillan new york 
nd edition 
hearst sch lkopf dumais osuna platt 
trends controversies support vector machines 
ieee intelligent systems 
herbrich 
learning kernel classifiers theory algorithms 
mit press 
huber 
robust statistics review 
annals statistics 
huber 
robust statistics 
john wiley sons new york 
ibm 
ibm optimization subroutine library guide 
ibm systems journal 
sc 
jaakkola haussler 
probabilistic kernel regression models 
proceedings conference ai statistics 
joachims 
making large scale svm learning practical 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
karush 
minima functions variables inequalities side constraints 
master thesis dept mathematics univ chicago 
kaufman 
solving quadratic programming problem arising support vector classification 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
keerthi bhattacharyya murthy 
improvements platt smo algorithm svm classifier design 
technical report cd dept mechanical production engineering natl 
univ singapore singapore 
keerthi bhattacharyya murty 
improvements platt smo algorithm svm classifier design 
neural computation 
kimeldorf wahba 
correspondence bayesian estimation stochastic processes smoothing splines 
annals mathematical statistics 
kimeldorf wahba 
results spline functions 
math 
anal 
applic 
kowalczyk 
maximal margin perceptron 
smola bartlett sch lkopf schuurmans editors advances large margin classifiers pages cambridge ma 
mit press 
kuhn tucker 
nonlinear programming 
proc 
nd berkeley symposium mathematical statistics pages berkeley 
university california press 
lee mangasarian 
ssvm smooth support vector machine classification 
computational optimization applications 
li vit nyi 
kolmogorov complexity applications 
texts monographs computer science 
springer new york 
lin 
convergence decomposition method support vector machines 
ieee transactions neural networks 
lustig shanno 
implementing mehrotra predictor corrector interior point method linear programming 
princeton technical report sor dept civil engineering operations research princeton university 
lustig shanno 
implementing mehrotra predictor corrector interior point method linear programming 
siam journal optimization 
mackay 
bayesian methods adaptive models 
phd thesis computation neural systems california institute technology pasadena ca 
mangasarian 
linear nonlinear separation patterns linear programming 
operations research 
mangasarian 
multi surface method pattern separation 
ieee transactions information theory 
mangasarian 
nonlinear programming 
mcgraw hill new york 
haykin 
support vector machines dynamic reconstruction chaotic system 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
mccormick 
nonlinear programming theory algorithms applications 
john wiley sons new york 
megiddo 
mathematical programming chapter pathways optimal set linear programming pages 
springer new york ny 
mehrotra sun 
implementation interior point method 
siam journal optimization 
mercer 
functions positive negative type connection theory integral equations 
philosophical transactions royal society london 
micchelli 
algebraic aspects interpolation 
proceedings symposia applied mathematics 

methods solving incorrectly posed problems 
springer 

ller smola tsch sch lkopf kohlmorgen vapnik 
predicting time series support vector machines 
gerstner 
nicoud editors artificial neural networks icann pages berlin 
springer lecture notes computer science vol 

murtagh saunders 
minos user guide 
technical report sol stanford university ca usa 
revised 
neal 
bayesian learning neural networks 
springer 
nilsson 
learning machines foundations trainable pattern classifying systems 
mcgraw hill 
nyquist 
certain topics telegraph transmission theory 
trans 
pages 
osuna freund girosi 
improved training algorithm support vector machines 
principe morgan wilson editors neural networks signal processing vii proceedings ieee workshop pages new york 
ieee 
osuna girosi 
reducing run time complexity support vector regression 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 

kernels eigenvalues support vector machines 
thesis australian national university canberra 
platt 
fast training support vector machines sequential minimal optimization 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
poggio 
optimal nonlinear associative recall 
biological cybernetics 
rasmussen 
evaluation gaussian processes methods non linear regression 
phd thesis department computer science university toronto 
ftp ftp cs toronto edu pub carl thesis ps gz 
rissanen 
modeling shortest data description 
automatica 
saitoh 
theory reproducing kernels applications 
longman scientific technical harlow england 
saunders stitson weston bottou sch lkopf smola 
support vector machine manual 
technical report csd tr department computer science royal holloway university london egham uk 
svm available svm dcs rhbnc ac uk 
schoenberg 
positive definite functions spheres 
duke math 

sch lkopf 
support vector learning 
oldenbourg verlag nchen 
tu berlin 
download www kernel machines org 
sch lkopf burges vapnik 
extracting support data task 
fayyad uthurusamy editors proceedings international conference knowledge discovery data mining menlo park 
aaai press 
sch lkopf burges vapnik 
incorporating invariances support vector learning machines 
von der malsburg von seelen sendhoff editors artificial neural networks icann pages berlin 
springer lecture notes computer science vol 

sch lkopf burges smola editors 
advances kernel methods support vector learning 
mit press cambridge ma 
sch lkopf herbrich smola williamson 
generalized representer theorem 
technical report neurocolt 
appear proceedings annual conference learning theory 
sch lkopf mika burges 
ller tsch smola 
input space vs feature space kernel methods 
ieee transactions neural networks 
sch lkopf platt shawe taylor smola williamson 
estimating support high dimensional distribution 
neural computation 
sch lkopf simard smola vapnik 
prior knowledge support vector kernels 
jordan kearns solla editors advances neural information processing systems pages cambridge ma 
mit press 
sch lkopf smola 
ller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
sch lkopf smola williamson bartlett 
new support vector algorithms 
neural computation 
sch lkopf smola 
learning kernels 
mit press 
sch lkopf sung burges girosi niyogi poggio vapnik 
comparing support vector machines gaussian kernels radial basis function classifiers 
ieee transactions signal processing 
shannon 
mathematical theory communication 
bell system technical journal 
john shawe taylor peter bartlett robert williamson martin anthony 
structural risk minimization data dependent hierarchies 
ieee transactions information theory 
smola murata sch lkopf 
ller 
asymptotically optimal choice loss support vector machines 
niklasson bod ziemke editors proceedings international conference artificial neural networks perspectives neural computing pages berlin 
springer 
smola sch lkopf 
ller 
connection regularization operators support vector kernels 
neural networks 
smola sch lkopf 
ller 
general cost functions support vector regression 
downs frean gallagher editors proc 
ninth australian conf 
neural networks pages brisbane australia 
university queensland 
smola sch lkopf tsch 
linear programs automatic accuracy control regression 
ninth international conference artificial neural networks conference publications pages london 
iee 
smola 
regression estimation support vector learning machines 
diplomarbeit technische universit nchen 
smola 
learning kernels 
phd thesis technische universit berlin 
gmd research series 
smola elisseeff sch lkopf williamson 
entropy numbers convex combinations mlps 
smola bartlett sch lkopf schuurmans editors advances large margin classifiers pages cambridge ma 
mit press 
smola ri williamson 
regularization dot product kernels 
leen dietterich tresp editors advances neural information processing systems pages 
mit press 
smola sch lkopf 
kernel method pattern recognition regression approximation operator inversion 
algorithmica 
smola sch lkopf 
tutorial support vector regression 
neurocolt technical report nc tr royal holloway college university london uk 
smola sch lkopf 
sparse greedy matrix approximation machine learning 
langley editor proceedings international conference machine learning pages san francisco 
morgan kaufmann publishers 
stitson gammerman vapnik vovk watkins weston 
support vector regression anova decomposition kernels 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
stone 
additive regression nonparametric models 
annals statistics 
stone 
cross choice assessment statistical predictors discussion 
journal royal statistical society 
street mangasarian 
improved generalization tolerant training 
technical report mp tr university wisconsin madison 
tikhonov arsenin 
solution problems 
winston sons 
tipping 
relevance vector machine 
solla leen 
ller editors advances neural information processing systems pages cambridge ma 
mit press 
vanderbei 
loqo interior point code quadratic programming 
tr sor statistics operations research princeton univ nj 
vanderbei 
loqo user manual version 
technical report sor princeton university statistics operations research 
code available www princeton edu 
vapnik 
nature statistical learning theory 
springer new york 
vapnik 
statistical learning theory 
john wiley sons new york 
vapnik 
remarks support vector method function estimation 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
vapnik chervonenkis 
note class perceptrons 
automation remote control 
vapnik chervonenkis 
theory pattern recognition russian 
nauka moscow 
german translation theorie der akademie verlag berlin 
vapnik smola 
support vector method function approximation regression estimation signal processing 
mozer jordan petsche editors advances neural information processing systems pages cambridge ma 
mit press 
vapnik lerner 
pattern recognition generalized portrait method 
automation remote control 
vapnik 
estimation dependences empirical data 
springer berlin 
vapnik chervonenkis 
uniform convergence relative frequencies events probabilities 
theory probability applications 
wahba 
spline bases regularization generalized crossvalidation solving approximation problems large quantities noisy data 
ward cheney editors proceedings international conference approximation theory honour george lorenz pages austin tx 
academic press 
wahba 
spline models observational data volume cbms nsf regional conference series applied mathematics 
siam philadelphia 
wahba 
support vector machines reproducing kernel hilbert spaces randomized 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
weston gammerman stitson vapnik vovk watkins 
support vector density estimation 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
williams 
prediction gaussian processes linear regression linear prediction 
jordan editor learning inference graphical models pages 
kluwer academic 
williamson smola sch lkopf 
generalization performance regularization networks support vector machines entropy numbers compact operators 
technical report neurocolt www neurocolt com 
accepted publication ieee transactions information theory 
yuille 
motion coherence theory 
proceedings international conference computer vision pages washington december 
ieee computer society press 

