memory layout oriented run time technique locality optimization yong yan hal computer systems dell avenue campbell ca exploiting locality run time complementary approach compiler approach applications dynamic memory access patterns 
proposes memory layout oriented approach exploit cache locality parallel loops run time symmetric multi processor smp systems 
guided hints targeted cache architecture reorganizes partitions parallel loop shrinking partitioning memory access space loop run time 
generated task partitions data sharing partitions minimized data reuse partition maximized 
execution tasks partitions scheduled adaptive locality preserved way achieve balanced execution minimizing execution time applications trading load balance locality 
simulation measurement show run time approach achieve comparable performance compiler optimizations applications load balance cache locality optimized tiling program transformations 
experimental results show approach able significantly improve memory performance applications dynamic memory access patterns 
type programs usually hard optimized compilers 

increasing speed gap processor memory system techniques latency hiding reduction important uniprocessor systems multiprocessor systems 
symmetric multi research supported national science foundation ccr air force office scientific research afosr office naval research onr 
zhang zhao zhang computer science department college william mary williamsburg va processor smp systems emerged major class high performance platforms hp convex exemplar class sun sgi challenge dec alphaserver 
smps dominate server market commercial applications desktops scientific computing 
important building blocks large scale systems 
access latency processor shared memory smp usually tens times cache improving memory performance applications smps crucial successful smp systems 
techniques reducing effect long memory latency intensively investigated researchers application designers hardware architects 
proposed techniques far fall categories latency avoidance latency tolerance 
latency tolerance techniques aimed hiding effect latencies overlapping computations communications aggregating communications 
techniques reducing impact access latencies cost increasing program bandwidth requirements 
latency avoidance techniques called locality optimization techniques aimed reducing low level memory accesses software hardware approaches 
smp system reducing total number accesses low levels memory hierarchy substantial solution reduce cache coherence overhead memory contention network contention 
optimizing locality parallel computations demanding tolerating memory latency goal 

problem locality optimization paid attention researchers years 
majority existing focuses compiler optimizations :10.1.1.1.9215
compiler exploit detailed information applications comprehensive analysis techniques compiler locality optimization tech niques shown effective improving performance applications applied see 
unfortunately applications real world possess dynamic patterns analyzed compiler time 
double int brow sparse mm int start register double start start brow start break task 
sparse matrix multiplication smm dynamic data access pattern irregular computation pattern 
sparse matrix multiplication algorithm sparse source matrices dense representations 
innermost loop elements multiplied indirectly determined data arrays brow 
compiler time compiler know kind data program going process determine program accesses data 
data access pattern program determined run time input data obtained 
smp system design run time locality optimization technique challenged low overhead requirement complexities minimizing data sharing caches maximizing data reuses cache trading locality performance factors 

solution contributions data reuses application occur loop structures parallel loop major program structure scientific applications propose run time technique improve memory performance parallel loops dynamic data access patterns 
run time technique memory access patterns parallel tasks program captured run time multi dimensional memory access space sim ple application dependent hints 
abstracted memory access space cache architecture locality program optimized types transformations space shrinking space partitioning 
tasks adaptively scheduled trade locality load imbalance aiming minimizing parallel computing time 
proposed information abstraction transformations efficiently implemented acceptable overhead 
respect applications different data access patterns effectiveness proposed technique evaluated detail event driven simulator commercial smp systems 
comparisons related exploiting cache locality run time paid attention previous 
dynamic loop scheduling algorithms consider affinity loop iterations processors 
significant performance improvement acquired applications type affinity exploited approach popular relations memory different iterations considered 
proposed technique takes consideration affinity parallel tasks processors uses information underlying cache architecture memory patterns tasks minimize cache misses false sharing 
design cool language locality exploitation issue addressed language mechanisms run time system 
task affinity data affinity specified users implemented run time system 
major limit approach quality locality optimizations totally depends programmer 
complicated applications example difficult user specify affinity 
proposed technique uses simple programming interface user compiler specify simple information data complicated affinity relations 
regarding run time locality optimization sequential programs proposes memory layout oriented method 
reorganizes computation loop simple hints memory patterns loops cache architectural information 
compared uniprocessor system cache coherent shared memory system complicated factors considered locality exploitation data sharing load imbalance 
uses run time system color virtual pages program parameters summary array access patterns generated high level compiler 
approach depends compiler time static analysis data access patterns 
organization remaining contains sections 
section describes run time optimization technique detail 
section presents performance evaluation method performance results 
section conclude 

memory layout oriented optimization technique program transforming user compiling estimation memory accessing pattern task grouping task partitioning task scheduling executables hints run time library run time execution 
framework run time technique 
run time technique implemented set library functions 
presents framework run time optimization 
sequential application program transformed compiler rewritten user insert run time functions 
generated executable file encoded application dependent hints 
runtime encoded run time functions executed fulfill functionalities estimating pattern program reorganizing tasks cache affinity groups tasks group expected heavily reuse data cache partitioning groups multiple processors data sharing multiple processors minimized adaptively scheduling execution tasks 
order minimize run time overhead multidimensional hash table internally built manage set task affinity groups 
set hash functions map appropriate task affinity group hash table 
locality oriented task reorganization partitioning finished task mapping 
section describes information estimation method design hash table hash functions task reorganization partitioning task scheduling algorithm 

memory access pattern estimation loop structure referenced data usually structured arrays 
arrays accessed loop body nested data independent loop 
array usually laid contiguous memory region independent arrays 
rare cases array may laid memory pages 
run time system may handle rare cases efficiently system works memory layout cases practice 
visualizing array independent dimension memory regions arrays abstracted dimensional memory access space expressed arrays arranged selected order user 
dimensional memory access space contains memory addresses accessed loop 
similar 
order run time system precisely capture memory space information hints provided interface 
hint 
number arrays accessed tasks 
hint 
size bytes array 
run time system maintains memory access space size vector denoted mss vector size th array 
hint 
starting memory address array 
underlying run time system constructs starting address vector denoted sa vector starting memory address th array 
hint static information 
array size may static size known compiler time dynamic size determined run time data hint tell calculate size run time 
starting addresses dynamic memory addresses determined run time 
hint tells determine starting addresses run time 
determining global memory access space loop need determine parallel iteration accesses global memory access space reorganize improve memory performance 
instance loop body parallel loop parallel task 
access region task array simply represented starting address access region 
hint provided interface functions 
hint memory access vector task jn ji starting address referenced region th array 
loop structures parallel iteration may contiguously access array access region may precisely abstracted starting address 
case loop iteration split smaller iterations iteration accesses contiguous region array 
addition hint provided assist task partitioning 
hint number processors access dimension double memory layout size starting memory layout size starting hints memory layouts accessed arrays 
physical memory layout access dimension dimensional memory accessing space 

memory access space representation 
hints memory access space loop abstracted dimensional memory access space task abstracted point jn memory access space estimation memory access pattern 
presents example representation memory accesses physical memory layout arrays smm 
gives hints space 
illustrates memory layout arrays laid starting address respectively 
array element size bytes 
memory space arrays memory space accessed tasks 
memory access space represented dimensional space shown point gives pair possible starting memory access addresses respectively task 
example means task access array starting memory address access array starting physical address 

task reorganization memory access space nearby task points access nearby memory addresses memory 
grouping nearby tasks memory access space change enhance temporal locality spatial locality execute 
achieved shrinking memory access space underlying cache size 
ft ji mg set data independent tasks parallel loop memory access space parallel loop 
conceptually task mapped point memory access space starting memory addresses memory access regions 
addition number processors capacity underlying secondary cache bytes 
task reorganization consists steps 
step memory access space shifted origin point subtracting coordinates task points 
second step equal shrinking method shrink dimension shifted memory fc dimensional space resulted shrinking called dimensional bin space 
weight constant 
bin space point associated task bin hold tasks mapped task bin 
shrinking procedure space exemplified dimensional space 
shrinking original memory access space shifted origin point see 
shifting function shown 
dimension shifted memory access space shrunk new dimensional bin space 
tasks shadow square access space cache size mapped point bin space grouped execute 
access dimension access dimension shifting shifting memory access space function access dimension dimensional memory access space shrinking access dimension access dimension bin bin bin access dimension shrinking memory access space function cache bytes 

equally shrinking memory access space 

task partitioning shrinking dimensional memory access space tasks grouped locality affinity information dimensional bin space 
task partitioning aimed partitioning dimensional bin space partitions number processors partition dimensional polyhedron 
data sharing degree partitions minimized measured volume boundary spaces partitions 

partitions balanced balance refers partitions volume 
major function partitioning dimensional bin space find partitioning vector conditions satisfied 
finding optimal partition vector np complete problem propose heuristic algorithm partitioning rules 
detailed proofs 
theorem ordering rule partitioning vector decreasing order partitioning vector resulting sorting decreasing order terms sharing degree 
theorem increment rule dimensional bin space partitioning vectors better terms sharing degree corollary increment rule dimensional bin space partitioning vectors better terms sharing degree rules design efficient heuristic algorithm follows 

factor number processors generate prime factors decreasing order 
assume prime factors initially dimensional partitioning vector stored bin space 

index position 
initially 
prime factor increases increment rule determine rj put 
ordering rule best place put rj 
increment rules find better place 
increased go back increment rule put rj reorder decreasing order go back 
increment rule put rj reorder decreasing order go back 
algorithm computational complexity 
determination partitioning vector bin space partitioned multiple independent spaces reconstructed dimensional space 
procedure shown bin space produced partitioned vector 
partitions transformed independent spaces transformed dimensional space shown 
dimensional space implemented access dimension bin bin access dimension indexing partitions 
independent address space partition 
task control link list tcl task bin task bin task bin task bin task bin task bin dimensional internal representation memory access space 

partitioning bin space evenly divided partitions dimensions 
dimensional hash table task bins partition chained pointed record task control linked tcl list 
hashing tasks hash table finished space transformation functions details 

task scheduling order minimize parallel computing time partitioned tasks locality preserved adaptive scheduling las algorithm extending linearly adaptive algorithm proposed 
initially th task group chain tcl list considered local task chain processor total number processors 
task chain head tail 
initial allocation maintains minimized data sharing achieved task reorganization step processors 
number current tasks local chain processor recorded tcl counter denoted las algorithm estimate load imbalance 
addition processor chunking control variable initial value denoted processor determine tasks executed scheduling step 
scheduling algorithm works phases task bin task bin local scheduling phase global scheduling phase 
processors start local scheduling phase 
local scheduling phase processor calculates load status relative processors follows heavy light normal decreases execution control load distribution closely 
adjusts chunking control variable fol lowing load light minf ki load heavy ki processor gets remaining tasks head local task chain execute 
varying range chunking control variables shown safe balancing load 
processor finishes local tasks sets chunking control variable top enters global scheduling phase get remaining tasks heavily load processor tail task chain 

performance evaluation 
evaluation method implemented locality optimization technique simple run time library functions 
performance evaluation simulation measurement 
simulation conducted event driven simulator bus shared memory systems built mint mips interpreter 
measurements conducted commercial systems hp convex class crossbar cache coherent smp system processors sun bus cache coherent smp system processors 
selected applications dense matrix multiplication denoted dmm regular computation pattern static data access pattern adjoint convolution denoted ac irregular computation pattern static data access pattern sparse matrix multiplication non zero elements denoted smm irregular computation pattern dynamic pattern 
optimized versions exploiting locality runtime library denoted dmm lo ac lo smm lo respectively 
comparison benchmarks parallelized respectively best existing techniques follows 
dmm application parallelized blocked matrix multiplication algorithm wolf lam 
program denoted dmm wl 
ac application loop split loop reverse loop fusion transformations get balanced outer loop equally partitioned processors 
program denoted ac bf 
smm application linearly adaptive scheduling technique proposed schedule executions parallel iterations smm 
program denoted smm detailed description programs 

performance results processors rate dmm application ac applic ation smm application dmm wl dmm lo ac bf ac lo smm smm lo table 
cache rate comparison experiments conducted shrinking factor 
table presents rates benchmark programs processors processors 
regarding regular application dmm locality optimized version dmm lo run time technique higher tuned version dmm wl number cache misses table 
ac lo locality optimized program ac run time technique shown achieve slightly better cache performance ac bf tuned program 
regarding application smm run time locality technique shown effective reducing cache misses 
cache rate reduced shown table 
hp class sun sparc program size processors size processors dmm wl dmm lo ac bf ac lo smm smm lo table 
measured time seconds comparison 

table presents execution comparisons smp systems 
measured load balance table 
regarding dmm program dmm wl consistently performed little bit better dmm lo larger smp systems 
better load balance dmm wl reason 
shows runtime optimization achieve comparable perfor hp class sun sparc program size processors size processors dmm wl dmm lo ac bf ac lo smm smm lo table 
measured load imbalance terms rate time deviation mean time 

mance compiler optimization regular applications 
program ac ac lo performed better ac bf processors smp systems 
processors applied execution times close 
ac bf balanced load better due perfect initial partition 
load imbalance occurred lo larger 
shows run time optimization chance outperform optimization applications irregular computation pattern 
smm smm lo achieved better performance improvement smm reduction execution time observed test cases smp systems 
confirms effectiveness run time technique improving performance applications dynamic memory access patterns table presents run time overhead measurements 
smp systems run time overhead larger execution time cases 
shows effectiveness hash table hash functions integrate locality optimizations run time 
hp class sun sparc program size processors size processors dmm lo ac lo smm lo table 
run time overhead percentage total time 

table show effects selecting different shrinking factors change affect execution times benchmark programs 
effect big 
recommend programming ease 

run time locality optimization technique shown effectiveness optimizing memory performance applications dynamic memory access pattern 
applications application machine value dmm lo class dmm lo ac lo class ac lo smm lo class smm lo table 
effects different values execution time seconds processors 
memory performance optimized current compiler optimizations run time technique achieve comparable performance 
hash table hash functions shown effective way reduce run time overhead 
limits need studied 
access pattern task array estimated starting address 
task accesses non contiguous regions array multiple starting addresses task split small tasks 
investigation needed case 
space shrinking equal shrinking approach 
tasks accesses different arrays different ways non equal shrinking approach 
difficulty estimate array access patterns low cost 
program structure extended consider data dependence 
combining run time locality optimization technique run time parallelization technique general program structures handled 
dr douglas sending thread library 
grateful dr greg constructive suggestions help hp convex class 
appreciate neal wagner careful reading manuscript constructive comments 
brewer 
overview hp convex exemplar hardware 
technical report hewlett packard system technology division 
bugnion anderson mowry rosenblum lam 
compiler directed page coloring multiprocessors 
proceedings asplos pages oct 
burger goodman 
limited bandwidth affect processor design 
ieee micro pages november december 
multiprocessing ieee compcon pages february 
chandra gupta hennessy 
data locality load balancing cool 
proceedings ppopp pages may 
coleman mckinley 
tile size selection cache organization data layout 
proceedings pldi pages june 
culler singh gupta 
parallel computer architecture hardware software approach 
morgan kaufmann publishers 
galles williams 
performance optimizations implementation verification sgi challenge multiprocessor 
proceedings seventh hawaii international conference system sciences pages jan 
hennessy patterson 
computer architecture quantitative approach 
morgan kaufmann publishers 
eggers 
reducing false sharing shared memory multiprocessors compile time data transformations 
proceedings ppopp pages july 
ahmed pingali 
data centric multilevel blocking 
proceedings pldi pages may 
lam rothberg wolf 
cache performance optimizations blocked algorithms 
proceedings asplos pages april 
markatos leblanc 
processor affinity loop scheduling scheme shared memory multiprocessors 
ieee trans 
para 
dist syst april 
mckinley carr tseng 
improving data locality loop transformations 
acm trans 
prog 
lang 
syst july 
mckinley 
quantitative analysis loop nest locality 
proceedings asplos pages oct 
philbin douglas li 
thread scheduling cache locality 
proceedings asplos pages oct 
saltz 
run time parallelization scheduling loops 
ieee trans 
comput pages may 
harris 
alphaserver cached processor module architecture design 
digital technical journal 
veenstra fowler 
mint front efficient simulation shared memory multiprocessors 
proceedings mascots pages jan 
wolf lam 
data locality optimizing algorithm 
proceedings pldi pages june 
yan 
exploiting cache locality run time 
phd thesis computer science department college william mary may 
yan jin zhang 
adaptively scheduling parallel loops distributed shared memory systems 
ieee trans 
para 
dist syst jan 
