hierarchical dirichlet processes yee teh comp nus edu sg department computer science national university singapore singapore michael jordan jordan eecs berkeley edu computer science division department statistics university california berkeley berkeley ca usa matthew beal cse buffalo edu department computer science engineering state university new york buffalo buffalo ny usa david blei blei eecs berkeley edu department computer science princeton university princeton nj usa december yee teh lee yew postdoctoral fellow department computer science national university singapore singapore jordan professor computer science statistics uc berkeley ca matthew beal assistant professor computer science engineering suny buffalo ny david blei assistant professor computer science princeton university nj 
correspondences directed michael jordan 
partially supported intel microsoft research darpa support calo program 
authors wish acknowledge helpful discussions lancelot james jim pitman referees useful comments 
consider problems involving groups data observation group draw mixture model desirable share mixture components groups 
assume number mixture components unknown priori inferred data 
setting natural consider sets dirichlet processes group known clustering property dirichlet process provides nonparametric prior number mixture components group 
desire tie mixture models various groups consider hierarchical model specifically base measure child dirichlet processes distributed dirichlet process 
base measure discrete child dirichlet processes ily share atoms 
desired mixture models different groups necessarily share mixture components 
discuss representations hierarchical dirichlet processes terms stick breaking process generalization chinese restaurant process refer chinese restaurant franchise markov chain monte carlo algorithms posterior inference hierarchical dirichlet process mixtures describe applications problems information retrieval text modelling 
keywords clustering mixture models nonparametric bayesian statistics hierarchical models markov chain monte carlo recurring theme statistics need separate observations groups allow groups remain linked share statistical strength bayesian formalism sharing achieved naturally hierarchical modeling parameters shared groups random ness parameters induces dependencies groups 
estimates posterior distribution exhibit shrinkage current explore hierarchical approach problem model clustering grouped data 
assume data subdivided set groups group wish find clusters capture latent structure data assigned group 
number clusters group unknown inferred 
sense precise wish allow clusters shared groups 
example kind problem motivates genetics 
consider set binary markers single nucleotide polymorphisms snps localized region human genome 
individual human exhibit different patterns markers single chromosome real populations small subset patterns haplotypes observed gabriel 
model combination pair haplotypes genotype mating set observed genotypes sample human population great interest identify underlying haplotypes stephens 
consider extension problem population divided set groups african asian european subpopulations 
may want discover sets haplotypes subpopulation may wish discover haplotypes shared subpopulations 
identification haplotypes significant implications understanding migration patterns ancestral populations humans 
second example consider problem field information retrieval ir mod eling relationships sets documents 
ir documents generally modeled exchangeability assumption bag words assumption order words document ignored salton mcgill 
common view words document arising number latent clusters topics topic generally modeled multinomial probability distribution words basic vocabulary blei 
document concerned university funding words document drawn topics education finance considering collection documents may wish allow topics shared documents corpus 
example corpus contains document concerned university football topics may education sports want topic related discovered analysis document university funding 
may want extend model allow multiple corpora 
example doc uments scientific journals grouped themes empirical process theory mul statistics survival analysis interest discover extent latent topics shared documents shared groupings 
general wish consider sharing clusters multiple nested groupings data 
approach problem sharing clusters multiple related groups metric bayesian approach dirichlet process ferguson 
dirichlet process dp measure measures 
parameters scaling parameter base probability measure 
explicit representation draw dirichlet process dp sethuraman showed dp probability independent random variables distributed atom stick breaking weights random depend parameter definition provided section 
representation shows draws dp discrete probability 
discrete nature dp unsuitable general applications bayesian suited problem placing priors mixture components mixture modeling 
idea basically associate mixture component atom introducing tor variables associate data points mixture components posterior distribution yields probability distribution partitions data 
number authors studied dirichlet process mixture models antoniak escobar west maceachern ller :10.1.1.51.1747
models provide alternative methods attempt select particular number mixture components methods place explicit parametric prior number components 
consider setting data subdivided number groups 
goal solving clustering problem group consider set random measures gj group gj distributed group specific dirichlet process dp 
link clustering problems link group specific dps 
authors considered ways induce dependencies multiple dps links parameters maceachern tomlinson ller de ibrahim mallick walker james 
focusing natural proposal hierarchy measures gj conditionally independent draws single underlying dirichlet process dp parametric distribution random parameter fong 
integrating induces dependencies dps 
simple hierarchical approach solve problem observed consider ing case absolutely continuous respect lebesgue measure gaussian mean 
case draws gj arise conditionally independent draws necessarily atoms common probability 
clusters arise group discreteness draws dp atoms associated different groups different sharing clusters groups 
problem assuming lies discrete parametric family assumption overly restrictive 
proposed solution problem straightforward force discrete broad support consider nonparametric hierarchical model draw dirichlet process dp 
restores flexibility modeler choose continuous discrete 
case probability discrete stick breaking representation 
atoms shared multiple dps yielding desired sharing atoms groups 
summary consider hierarchical specification dp gj dp refer hierarchical dirichlet process 
immediate extension hierarchical dirichlet process mixture models yields proposed formalism sharing clusters related clustering problems 
related nonparametric approaches linking multiple dps discussed number authors 
approach special case general framework dependent dirichlet processes due maceachern maceachern 

framework random variables general stochastic processes indexed collections random variables allows general forms dependency dps 
hierarchical approach fits framework endow stick breaking weights second subscript indexing groups view weights jk dependent fixed value show section definition yields specific canonical form dependence weights jk 
approach special case framework referred analysis densities ande tomlinson tomlinson escobar 
ande model hierarchical model multiple dps common base measure random treating draw dp case treated draw mixture dps 
resulting continuous general antoniak discussed problem sharing clusters 
appropriate choice problem addressed tomlin son sharing statistical strength multiple sets density estimation problems 
ande framework hierarchical dp framework closely related formally inferential goal different 
see restriction discrete important implications design efficient mcmc inference algorithms 
terminology hierarchical dirichlet process ller 
describe different notion hierarchy discussed 
authors consider model coupled set random measures gj defined gj fj fj draws dps 
model provides alternative approach sharing clusters shared clusters stick breaking weights associated groups 
contrast hierarchical model draws gj underlying base measure draw assigns different stick breaking weights shared atoms associated 
atoms partially shared 
terminology hierarchical dirichlet process third way beal 
context model known infinite hidden markov model hidden markov model countably infinite state space 
hierarchical dirichlet process beal 
hierarchy bayesian sense algorithmic description coupled set urn models 
discuss model detail section show notion hierarchical dp yields elegant treatment infinite hidden markov model 
summary notion hierarchical dirichlet process explore specific example dependency model multiple dirichlet processes specifically aimed problem sharing clusters related groups data 
involves simple bayesian hierarchy base measure set dirichlet processes distributed dirichlet process 
ways couple dirichlet processes view simple canonical bayesian hierarchy particularly worthy study 
note particular appealing recursiveness definition hierarchical dirichlet process readily extended multiple hierarchical levels 
natural applications 
example application document modeling level hierarchy needed share clusters multiple documents corpus second level hierarchy needed share clusters multiple corpora 
similarly genetics example interest consider nested subdivisions populations various criteria geographic cultural economic consider flow haplotypes resulting tree 
case nonparametric bayesian methods significant component chal working hierarchical dirichlet process computational 
provide general framework designing procedures posterior inference hierarchical dirichlet process parallel available dirichlet process necessary develop analogs hi dirichlet process representations proved useful dirichlet process setting 
provide analogs section discuss stick breaking repre sentation hierarchical dirichlet process analog urn model refer chinese restaurant franchise representation hierarchical dirichlet process terms infinite limit finite mixture models 
representations background mcmc algorithms posterior inference hierarchical dirichlet process mixtures section 
experimental results section section 
setting interested problems observations organized groups assumed ex changeable group groups 
precise letting index groups index observations group assume xj xj exchangeable group assume observations exchangeable group level xj xj xj 
denote observations group exchangeable 
assuming observation drawn independently mixture model mixture component associated observation 
ji denote parameter specifying mixture component associated observation xji 
refer variables ji factors 
note variables generally distinct develop different notation distinct values factors 
ji denote distribution xji factor ji 
gj denote prior distribution factors 
associated group assume factors conditionally independent gj 
probability model ji gj gj augment specification 
xji ji ji dirichlet processes section provide brief overview dirichlet processes 
discussion basic definitions different perspectives dirichlet process stick breaking construction urn model limit finite mixture models 
perspectives analog hierarchical dirichlet process described section 
measurable space probability measure space 
positive real number 
dirichlet process dp defined distribution random probability measure finite measurable partition 
ar random vector 
ar distributed finite dimensional dirichlet distri bution parameters 
ar 
ar dir 
ar 
write dp random probability measure distribution dirichlet process 
existence dirichlet process established ferguson 
stick breaking construction measures drawn dirichlet process discrete probability ferguson 
property explicit stick breaking construction due sethuraman 
stick breaking construction independent sequences random variables beta 
define random measure probability measure concentrated 
sethuraman showed defined way random probability measure distributed dp 
important note sequence constructed satisfies probability 
may interpret random probability measure positive integers 
convenience shall write gem random probability measure defined gem stands griffiths mccloskey see pitman 
chinese restaurant process second perspective dirichlet process provided urn scheme blackwell macqueen 
urn scheme shows draws dirichlet process discrete exhibit clustering property 
urn scheme refer directly refers draws sequence random variables distributed variables conditionally independent exchangeable 
consider successive conditional distributions integrated 
blackwell macqueen showed conditional distributions form 
interpret conditional distributions terms simple urn model ball distinct color associated atom 
balls drawn ball drawn placed back urn ball color 
addition probability proportional new atom created drawing ball new color added urn 
expression shows positive probability equal previous draws 
positive reinforcement effect point drawn drawn 
clustering property explicit helpful introduce new set variables represent distinct values atoms 
define 
distinct values taken 
mk number values equal re express 
mk 
somewhat different metaphor urn scheme closely related distribution partitions known chinese restaurant process aldous 
metaphor turned useful considering various generalizations dirichlet process pitman useful 
metaphor follows 
consider chinese restaurant unbounded number tables 
corresponds customer enters restaurant distinct values correspond tables customers sit 
th customer sits table indexed probability proportional number customers mk seated case set sits new table probability proportional increment draw set 
dirichlet process mixture models important applications dirichlet process nonparametric prior parameters mixture model 
particular suppose observations xi arise follows xi denotes distribution observation xi factors conditionally independent observation xi conditionally independent observations factor distributed dirichlet process model referred dirichlet process mixture model 
graphical model representation dirichlet process mixture model shown left 
represented stick breaking construction factors take values probability may denote indicator variable zi takes positive integral values distributed interpreting random probability measure positive integers 
equivalent representation dirichlet process mixture conditional distributions gem zi xi zi zi infinite limit finite mixture models zi 
dirichlet process mixture model derived limit sequence finite mixture mod els number mixture components taken infinity neal rasmussen green richardson :10.1.1.45.9111
limiting process provides third perspective dirichlet process 
suppose mixture components 

denote mixing proportions 
note previously symbol denote weights associated atoms deliberately overloaded definition shall see closely related 
fact limit vectors equivalent random size biased permutation entries pitman 
place dirichlet prior symmetric parameters 

denote parameter vector associated mixture component prior distribution 
drawing observation xi mixture model involves picking specific mixture component probability mixing proportions zi denote component 
model dir 
zi xi zi zi 
gl show measurable function integrable respect dg dg 
consequence marginal distribution induced observations 
xn ap proaches dirichlet process mixture model 
hierarchical dirichlet processes propose nonparametric bayesian approach modeling grouped data group associated mixture model wish link mixture models 
analogy dirichlet process mixture models define appropriate nonparametric prior refer hierarchical dirichlet process 
show prior grouped mixture model setting 
analogs perspectives earlier dirichlet process stick breaking construction chinese restaurant process representation representation terms limit finite mixture models 
hierarchical dirichlet process distribution set random probability measures 
process defines set random probability measures gj group global random probability measure 
global measure distributed dirichlet process concentration parameter base probability measure dp random measures gj conditionally independent distributions dirichlet process base probability measure gj dp 
hyperparameters hierarchical dirichlet process consist baseline probability measure concentration parameters 
baseline provides prior distribu tion factors ji 
distribution varies prior amount variability governed 
actual distribution gj factors th group deviates amount variability governed 
expect variability different groups dif ferent separate concentration parameter group escobar west put vague gamma priors 
hierarchical dirichlet process prior distribution factors grouped data 
random variables distributed gj 
ji factor corresponding single observation xji 
likelihood ji gj gj xji ji ji 
completes definition hierarchical dirichlet process mixture model 
corresponding graphical model shown right 
hierarchical dirichlet process readily extended levels 
base measure draw dp hierarchy extended levels deemed useful 
general obtain tree dp associated node children node conditionally independent parent draw dp node serves base measure children 
atoms stick breaking representation node shared descendant nodes providing notion shared clusters multiple levels resolution 
stick breaking construction global measure distributed dirichlet process expressed stick breaking representation independently gem mutually independent 
support points gj necessarily support points written gj jk 
jk note weights independent gj independent 
describe weights related global weights 
ar measurable partition kl 
note 
kr finite partition positive integers 
assuming non atomic distinct probability partition positive integers cor responds partition 
gj 
gj ar dir 
ar jk jk dir 
kr kr finite partition positive integers 
independently distributed accord ing dp interpret probability measures positive integers 
non atomic weaker result holds dp gj dp distributed 
dirichlet process mixture model factor ji distributed gj takes value probability jk 
indicator variable ji xji 
obtain equivalent representation hierarchical dirichlet process mixture conditional distributions gem dp xji 
derive explicit relationship elements recall stick breaking construction dirichlet processes defines variables follows beta 
show stick breaking construction produces random probability measure dp jk beta jk jk jl 
derive notice partition 
gives jl jk jl dir 
removing element standard properties dirichlet distribution jl jk jl dir 
define jk jk jl observe obtain 
completes description stick breaking construction hierarchical dirichlet processes 
chinese restaurant franchise section describe analog chinese restaurant process hierarchical dirichlet processes refer chinese restaurant franchise 
chinese restaurant franchise metaphor chinese restaurant process extended allow multiple restaurants share set dishes 
metaphor follows see 
restaurant franchise shared menu restaurants 
table restaurant dish ordered menu customer sits shared customers sit table 
multiple tables multiple restaurants serve dish 
setup restaurants correspond groups customers correspond factors ji 

denote random variables distributed global menu dishes 
introduce variables jt represent table specific choice dishes particular jt dish served table restaurant note ji associated jt jt associated introduce indicators denote associations 
particular tji index jt associ ated ji kjt index associated jt 
chinese restaurant franchise metaphor customer restaurant sat table tji table restaurant serves dish kjt 
need notation counts 
particular need maintain counts customers counts tables 
notation denote number customers restaurant table eating dish marginal counts represented dots 
represents number customers restaurant table nj represents number customers restaurant eating dish notation mjk denotes number tables restaurant serving dish mj represents number tables restaurant represents number tables serving dish total number tables occupied 
compute marginals hierarchical dirichlet process gj integrated 
consider conditional distribution ji gj integrated 
ji 
mj jt mixture draw mixture obtained drawing terms right hand side probabilities corresponding mixing proportions 
term summation chosen set ji jt tji chosen second term chosen increment mj draw jmj set ji jmj tji mj 
proceed integrate 
notice appears role distribution variables jt 
distributed dirichlet process integrate write conditional distribution jt jt 


draw jt choosing term summation right hand side equation set jt kjt chosen second term chosen increment draw set jt kjt completes description conditional distributions ji variables 
equations obtain samples ji proceed follows 
sample ji 
new sample needed obtain new sample jt set ji jt 
note hierarchical dirichlet process values factors shared groups groups 
key property hierarchical dirichlet processes 
infinite limit finite mixture models case dirichlet process mixture model hierarchical dirichlet process mixture model derived infinite limit finite mixtures 
section apparently different finite models yield hierarchical dirichlet process mixture infinite limit emphasizing different aspect model 
consider collection finite mixture models global vector mixing proportions group specific vector mixing proportions dir 
dir xji 
parametric hierarchical prior discussed mackay peto model natural languages 
show limit model hierarchical dirichlet process 
consider random probability measures gl jk section measurable function integrable respect dg dg 
standard properties dirichlet distribution see holds finite case partitions 
dp 
clear marginal distribution finite model induces approaches hierarchical dirichlet process mixture model 
alternative finite model limit hierarchical dirichlet process mixture model 
introducing dependencies groups placing prior finite model group choose subset mixture components model wide set mixture components 
particular consider model dir 
kjt dir tji xji tji kjt kjt ji 
limit model chinese restaurant franchise process infinite limit model hierarchical dirichlet process mixture model 
inference section describe related markov chain monte carlo sampling schemes hi dirichlet process mixture model 
straightforward gibbs sampler chinese restaurant franchise second augmented representation involving chinese restaurant franchise posterior third variation second sampling scheme streamlined bookkeeping 
simplify discussion assume base distribution conjugate data distribution allows focus issues specific hierarchical dirichlet process 
case approached adapt ing hierarchical dirichlet process techniques developed dp mixtures neal 
section assume fixed values concentration parameters sampler parameters appendix 
recall random variables interest 
variables xji observed data 
xji assumed arise draw distribution ji 
factor ji associated table tji restaurant representation ji random variable jt instance mixture component kjt jt kjt prior parameters denote mixture component associated observation xji 
notation denote number customers restaurant table eating dish mjk denotes number tables restaurant serving dish marginal counts represented dots 
xji xjt xji tji tji kjt mjk 
superscript attached set variables count ji jt ji jt means variable corresponding superscripted index removed set calculation count 
examples ji xji jt kjt ji jt number observations group factor associated jt leaving item xji 
density density 
conjugate integrate mixture component parameters sampling schemes 
denote conditional density xji mixture component data items xji xji xji xji ji xj ji xj 
similarly denote xjt xjt conditional density xjt data items associated mixture component leaving xjt 
suppress variables sampled condi tional distributions follow particular omit 
posterior sampling chinese restaurant franchise chinese restaurant franchise section produce samples prior distribution ji intermediary information related tables mixture components 
framework adapted yield gibbs sampling scheme posterior sampling observations dealing ji jt directly shall sample index variables tji kjt 
ji jt reconstructed index variables representation markov chain monte carlo sampling scheme efficient cf 
neal 
notice tji kjt inherit exchangeability properties ji jt conditional distributions adapted expressed terms tji kjt 
state space consists values notice number kjt variables represented explicitly algorithm fixed 
think actual state space consisting infinite number kjt finitely associated data represented explicitly 
sampling compute conditional distribution tji remainder variables exchangeability treat tji variable sampled group 
obtain conditional posterior tji combining conditional prior distribution tji likelihood generating xji 
prior probability tji takes particular previously value pro ji jt probability takes new value say tnew mj proportional 
likelihood due xji tji previously xji xji 
likelihood tji new calculated integrating possible values kjt new xji ji tji new xji xji xji new xji xji new xji xji simply prior density xji 
conditional distribu tion tji tji ji ji jt xji kjt xji previously xji ji tji new new sampled value tji tnew obtain sample sampling kf xji xji previously xji new xji new result updating tji table unoccupied probability table zero proportional 
result may delete corresponding kjt data structure 
result deleting kjt mixture component unallocated delete mixture component 
sampling changing kjt changes component membership data items table likelihood obtained setting kjt xjt xjt conditional probability kjt kjt jt jt xjt fk xjt previously xjt knew xjt knew posterior sampling augmented representation chinese restaurant franchise sampling scheme sampling groups coupled integrated 
complicates matters elaborate models case hidden markov model considered section 
section describe alternative sampling scheme addition chinese restaurant franchise representation instantiated sampled posterior conditioned factorizes groups 
posterior sample chinese restaurant franchise representation obtain draw posterior noting dp jt table draw 
conditioning jt distributed dp explicit construction dir gu dp xji ji kjt ji 
sample posterior group factorized sampling group performed separately 
variables interest scheme chinese restaurant franchise sampling scheme gu integrated introduces couplings sampling group easily handled 
sampling identical chinese restaurant franchise sampling scheme 
novelty replace new component new instantiated draw beta set new new understand follows new component instantiated instantiated gu choosing atom gu probability weight fact sequence stick breaking weights size biased permutation weights draw dirichlet process pitman weight corresponding chosen atom gu distribution stick breaking weight beta 
sampling described dir 
posterior sampling direct assignment chinese restaurant franchise augmented representation sampling schemes data items assigned table tji tables assigned mixture component kjt 
indirect association mixture components bookkeeping somewhat involved 
section describe variation augmented representation sampling scheme directly assigns data items mixture components variable equivalent earlier sampling schemes 
tables represented terms numbers tables mjk 
sampling realized grouping terms associated ji replaced 
ji xji fk xji previously uf xji knew xji knew sampling augmented representation sampling scheme conditioned assignment data items mixture components effect variables conditional distribution 
result sufficient sample place obtain distribution mjk conditioned variables consider distribution tji assuming 
probability data item xji assigned table kjt tji kjt ji ji jt probability assigned new table component tji new kjt new ji 
equations form conditional distributions gibbs sampler equilibrium distribu tion prior distribution assignment nj observations components ordinary dirichlet process concentration parameter corresponding distribution num ber components desired conditional distribution mjk 
antoniak shown mjk jk nj nj unsigned stirling numbers kind 
definition entries computed ns 
sampling augmented sampling scheme 
comparison sampling schemes consider relative merits sampling schemes 
terms ease im plementation direct assignment scheme preferred bookkeeping straightforward 
schemes chinese restaurant franchise involve substantial effort 
ad dition augmented direct assignment schemes sample integrate result sampling groups decoupled 
simplifies sampling schemes applicable elaborate models hidden markov model section 
terms convergence speed direct assignment scheme changes component mem data items time schemes chinese restaurant franchise changing component membership table change membership multiple data items time leading potentially improved performance 
akin split merge techniques dirichlet process mixture modeling jain neal 
analogy somewhat misleading split merge methods assignment data items tables consequence prior clustering effect dirichlet process nj samples 
result expect probability obtaining successful reassignment table previ ously component small necessarily expect chinese restaurant franchise schemes dominate direct assignment scheme 
inference methods viewed steps development inference procedures hierarchical dirichlet process mixtures 
sophisticated methods split merge methods jain neal variational methods blei jordan shown promise dirichlet processes expect prove useful hierarchical dirichlet processes 
experiments describe experiments section highlight aspects hierarchical dirichlet process nonparametric nature hierarchical nature 
section third experiment highlighting ease extend framework complex models specifically hidden markov model countably infinite state space 
software experiments available www cs berkeley edu hdp 
software implements hierarchy dirichlet processes arbitrary depth 
document modeling recall problem document modeling discussed section 
standard method ology information retrieval literature salton mcgill view document bag words exchangeability assumption words document 
model words document arising mixture model mixture component topic multinomial distribution words finite known vo 
goal model corpus documents way allow topics shared documents corpus 
parametric approach problem provided latent dirichlet allocation lda model blei 

model involves finite mixture model mixing propor tions drawn document specific basis dirichlet distribution 
mixing proportions word document independent draw mixture model 
generate word mixture component topic selected word generated topic 
note assumption word associated possibly different topic differs model mixture component selected document words generated selected topic 
interesting note distinction arises population genetics multiple words document analogous multiple markers chromosome 

developed model marker probabilities selected marker model essentially identical lda 
simpler finite mixture models natural try extend lda related models dirichlet processes capture uncertainty regarding number mixture components 
somewhat difficult case simple mixture model lda model documents document specific mixing proportions 
require multiple dps document 
poses problem sharing mixture components multiple dps precisely problem hierarchical dp designed solve 
hierarchical dp extension lda takes form 
underlying measure multinomial probability vectors select random measure provides countably infinite collection multinomial probability vectors viewed set topics corpus 
jth document corpus sample gj base measure selects specific subsets topics document gj generate document repeatedly sampling specific multinomial probability vectors ji gj sampling words xji probabilities ji 
overlap random measures gj implements sharing topics documents 
fit standard parametric lda model hierarchical dp extension corpus biology abstracts see elegans edu wli 
abstracts total 
removing standard words words appearing fewer times left words total 
standard information retrieval methodology vocabulary defined set distinct words left abstracts size 
models similar possible distinction lda assumes fixed finite number topics hierarchical dirichlet process 
models symmetric dirichlet distribution parameters prior topic distributions 
parameters vague gamma priors gamma gamma 
distribution topics lda assumed symmetric dirichlet parameters number topics lda 
posterior samples obtained chinese restaurant franchise sampling scheme concentration parameters sampled auxiliary variable sampling scheme appendix 
evaluated models fold cross validation 
evaluation metric ity standard metric information retrieval literature 
perplexity held consisting words 
wi defined exp log 
wi training corpus probability mass function model 
results shown 
lda evaluated perplexity mixture component cardinalities ranging 
seen left hierarchical dp mixture approach integrates mixture component cardinalities performs best lda model doing form model selection procedure 
shown right posterior number topics obtained hierarchical dp mixture model consistent range best fitting lda models 
multiple corpora consider problem sharing clusters documents multiple corpora 
approach problem extending hierarchical dirichlet process third level 
draw top level dp yields base measure set corpus level dps 
draws corpus level dps yield base measures dps associated documents corpus 
draws document level dps provide representation document probability distribution topics distributions words 
model allows topics shared corpus corpora 
documents experiments consist articles proceedings neural information processing systems nips conference years 
original articles available books nips cc preprocessed version available www cs utoronto ca roweis nips 
nips conference deals range topics covering human machine intelligence 
articles separated sections algorithms architectures aa applications ap cognitive science cs control navigation cn imple im learning theory lt neuroscience ns signal processing sp vision sciences vs 
sections years 
sectioning earlier years differed slightly manually relabeled sections earlier years match 
treat sections corpora interested pattern sharing topics corpora 
articles total 
article modeled bag words 
culled standard words words occurring fewer times corpus 
left average slightly words article 
considered experimental setup 
set articles single nips section wish model vs section experiments report wish know value terms prediction performance include articles nips sections 
done ways lump articles regard division sections hierarchical dp approach link sections 
consider models see graphical representations models model ignores articles sections simply uses hierarchical dp mixture kind section model vs articles 
model serves baseline 
gamma gamma prior distributions concentration parameters 
model incorporates articles sections ignores distinction sections single hierarchical dp mixture model model articles 
priors gamma gamma 
model takes full hierarchical approach models nips sections multiple corpora linked hierarchical dp mixture formalism 
model tree root draw single dp articles level set draws dps nips sections second level set draws dps articles sections 
priors gamma gamma gamma 
models finite known vocabulary assumed base measure symmetric dirichlet distribution parameters 
conducted experiments set articles chosen uniformly random sections vs done balance impact different sections different sizes 
training set articles chosen uniformly random vs section additional set test articles distinct training articles 
results report predictive performance vs test articles training set consisting arti cles additional section vs training articles varies 
direct assignment sampling scheme concentration parameters sampled auxiliary variable sampling scheme appendix 
left presents average predictive performance models runs number vs training articles ranged 
performance measured terms perplexity single words test articles training articles averaged choice additional section 
seen fully hierarchical model performs best perplexity decreasing rapidly modest values small values performance quite poor performance approaches articles included vs training set 
performance partially hierarchical poorer fully hierarchical range dominated small yielded poorer performance greater 
interpretation sharing strength articles useful little information available small eventually medium large crosstalk sections preferable model separately share strength hierarchy 
results left average sections interest see sections beneficial terms enhancing prediction articles vs right plots predictive performance model data particular sections lt aa ap 
articles lt section concerned theoretical properties learning algorithms aa concerned models methodology ap concerned applications learning algorithms various problems 
seen see predictive performance enhanced prior exposure articles ap articles aa articles lt articles vs tend concerned practical application learning algorithms problems computer vision pattern transfer reasonable 
interest investigate subject matter content topics discovered hierarchical dp model 
experimental setup 
section vs aa fit model articles section 
introduced articles vs section continued fit model holding topics earlier fit fixed recording topics earlier section allocated words vs section 
table displays representations frequently occurring topics setup topic represented set words highest probability topic 
topics provide qualitative confirmation expectations regarding overlap vs sections 
hidden markov models simplicity hierarchical dp specification base measure dp distributed dp straightforward exploit hierarchical dp building block complex models 
section demonstrate case hidden markov model 
recall hidden markov model hmm doubly stochastic markov chain sequence multinomial state variables 
vt linked state transition matrix element yt sequence observations 
yt drawn independently observations conditional vt rabiner 
essentially dynamic variant finite mixture model mixture component corresponding value multinomial state 
classical finite mixtures interesting consider replacing finite mixture underlying hmm dirichlet process 
note hmm involves single mixture model set mixture models value current state 
current state vt indexes specific row transition matrix probabilities row serving mixing proportions choice state vt 
state vt observation yt drawn mixture component indexed vt 
consider nonparametric variant hmm allows unbounded set states consider set dps value current state 
dps linked want set states reachable current states amounts requirement atoms associated state conditional dps shared exactly framework hierarchical dp 
define nonparametric hidden markov model simply replacing set con ditional finite mixture models underlying classical hmm hierarchical dirichlet process mixture model 
refer resulting model hierarchical dirichlet process hidden markov model hdp hmm 
hdp hmm provides alternative methods place explicit para metric prior number states model selection methods select fixed number states stolcke omohundro 
served inspiration hdp hmm beal 
discussed model known infinite hidden markov model number hidden states hidden markov model allowed countably infinite 
beal 
defined notion hierarchical dirichlet process model hierarchical dirichlet process hier bayesian sense involving distribution parameters dirichlet process description coupled set urn models 
briefly review construction relate formulation 
beal 
considered level procedure determining transition probabilities markov chain unbounded number states 
level prob ability transitioning state state proportional number times transition observed time steps probability proportional oracle pro cess invoked 
second level probability transitioning state proportional number times state chosen oracle regardless previous state probability transitioning novel state proportional 
intended role oracle tie transition models destination states common way baseline distribution ties group specific mixture components hierarchical dirichlet process 
relate level urn model hierarchical dp framework describe represen tation hdp hmm stick breaking formalism 
particular consider hierarchical dirichlet process representation shown 
parameters representation distributions gem dp time steps 
state observation distributions vt vt vt yt vt vt assume simplicity distinguished initial state 
consider chinese restaurant franchise representation model discussed section turns result equivalent coupled urn model beal 
infinite hidden markov model hdp hmm 
unfortunately posterior inference chinese restaurant franchise representation awk ward model involving substantial bookkeeping 
beal 
mcmc inference algorithm infinite hidden markov model proposing heuristic approximation gibbs sampling 
hand augmented representation di rect assignment representations lead directly mcmc sampling schemes straightforward implement 
experiments reported section direct assignment representation 
practical applications hidden markov models consider sets sequences treat sequences exchangeable level sequences 
applications speech recognition hidden markov model word vocabulary generally trained replicates word spoken 
setup readily accommodated hierarchical dp framework simply considering additional level bayesian hierarchy letting master dirichlet process couple hdp hmms set dirichlet processes 
alice section report experimental results problem predicting strings letters sentences taken lewis carroll alice adventures comparing hdp hmm hmm related approaches 
sentence treated sequence letters spaces sequence words 
distinct symbols letters space cases punctuation marks ignored 
training sentences average length symbols test sentences average length 
base distribution symmetric dirichlet distribution symbols parameters 
concentration parameters gamma priors 
direct assignment sampling method posterior predictive inference compared hdd hmm variety methods prediction hidden markov models classical hmm maximum likelihood ml parameters obtained baum welch algo rithm rabiner classical hmm maximum posteriori map parameters priors independent symmetric dirichlet distributions transition emission probabilities classical hmm trained approximation full bayesian analysis particular variational bayesian vb method due mackay described detail beal 
classical hmms conducted experiments value state cardinality ranging 
perplexity test sentences left 
vb predictive probability intractable compute modal setting parameters 
map vb models optimal settings hyperparameters hdp hmm 
see hdp hmm lower perplexity models tested ml map vb 
right shows posterior samples number states hdp hmm 
discussion described nonparametric approach modeling groups data group characterized mixture model allow mixture components shared groups 
proposed hierarchical bayesian solution problem set dirichlet processes coupled base measure distributed dirichlet process 
described different representations capture aspects hierarchical process 
particular described stick breaking representation describes random measures explicitly representation marginals terms urn model referred chinese restaurant franchise representation process terms infinite limit finite mixture models 
representations led formulation markov chain monte carlo sampling schemes posterior inference hierarchical dirichlet process mixtures 
scheme directly chinese restaurant franchise representation second scheme represents posterior chinese restaurant franchise sample global measure third uses direct assignment data items mixture components 
clustering important activity large scale data analysis problems engineering science reflecting heterogeneity data collected large scale 
clustering problems approached probabilistic framework finite mixture models fraley raftery green richardson years seen nu examples applications finite mixtures dynamical cousins hmm areas bioinformatics durbin speech recognition huang information retrieval blei computational vision forsyth ponce 
areas provide numerous instances data analyses involve multiple linked sets clustering prob lems classical clustering methods model non model provide little way leverage 
bioinformatics alluded problem finding haplotype structure subpopulations 
examples bioinformatics include hmms amino acid sequences hierarchical dp version hmm allow motifs discov ered shared different families proteins 
speech recognition multiple hmms widely form word specific speaker specific models adhoc meth ods generally share statistical strength models 
discussed examples grouped data information retrieval examples include problems groups author language 
computational vision robotics problems involve sets descriptors objects arranged taxonomy 
examples substantial uncertainty regarding appropriate numbers clusters sharing statistical strength groups natural desirable suggest hierarchical metric bayesian approach clustering may provide generally useful extension model clustering 
posterior sampling concentration parameters mcmc samples posterior distributions concentration parameters hierarchical dirichlet process obtained straightforward extensions analogous tech niques dirichlet processes 
number observed groups equal nj observa tions th group 
consider chinese restaurant franchise representation 
concentration parameter governs distribution number jt mixture 
noted sec tion mj 
nj nj mj mj 
nj govern aspects joint distribution prior sufficient derive mcmc updates variables 
case single mixture model escobar west proposed gamma prior derived auxiliary variable update rasmussen observed log concave log proposed adaptive rejection sampling 
adaptive rejection sampler rasmussen directly applied case conditional distribution log log concave 
auxiliary variable method escobar west requires slight modification case 
assume prior gamma distribution parameters write nj nj nj wj nj 
define auxiliary variables wj sj wj variable values sj binary variable define distribution nj wj sj nj 
marginalizing gives desired conditional distribution 
defines auxiliary variable sampling scheme 
sj log wj gamma distribution parameters sj log wj 
wj sj conditionally independent distributions wj nj wj sj sj nj beta binomial distributions respectively 
completes auxiliary variable sam pling scheme 
prefer auxiliary variable sampling scheme easier implement typically mixes quickly iterations 
total number jt concentration parameter governs distribution number components 
variables independent may apply techniques escobar west rasmussen directly sampling 
aldous 
exchangeability related topics cole de probabilit de saint flour xiii springer berlin pp 

antoniak 
mixtures dirichlet processes applications bayesian metric problems annals statistics pp 

beal 
variational algorithms approximate bayesian inference ph thesis gatsby computational neuroscience unit university college london 
beal ghahramani rasmussen 
infinite hidden markov model dietterich becker ghahramani eds 
advances neural information processing systems cambridge ma mit press vol 
pp 

blackwell macqueen 
ferguson distributions urn schemes statistics pp 

blei jordan 
variational methods dirichlet process mixtures bayesian analysis pp 

blei jordan ng 
hierarchical bayesian models applications information retrieval bayesian statistics vol 
pp 


semiparametric regression count data biometrika pp 


non di scam di associative tech 
rep istituto matematica fi dell universit di torino 
de ller rosner 
anova model dependent random measures journal american statistical association pp 

durbin eddy krogh mitchison 
biological sequence analysis cam bridge uk cambridge university press 
escobar west 
bayesian density estimation inference mixtures journal american statistical association pp 

ferguson 
bayesian analysis nonparametric problems annals statis tics pp 

fong arnold bolton 
ultimatum bargaining comparing nondecreasing curves shape constraints journal business economic statistics pp 

forsyth ponce 
computer vision modern approach upper saddle river nj prentice hall 
fraley raftery 
model clustering discriminant analysis density estimation journal american statistical association pp 

gabriel 
structure haplotype blocks human genome science pp 

green richardson 
modelling heterogeneity dirichlet process scandinavian journal statistics pp 

huang hon 
spoken language processing upper saddle river nj prentice hall 
james 
computational methods multiplicative intensity models weighted gamma processes proportional hazards marked point processes panel count data journal american statistical association pp 


exact approximate sum representations dirichlet process canadian journal statistics pp 

jain neal 
split merge markov chain monte carlo procedure dirichlet process mixture model tech 
rep department statistics university toronto 
ibrahim 
semi parametric bayesian approach generalized linear mixed models statistics medicine pp 

maceachern 
dependent nonparametric processes proceedings section bayesian statistical science american statistical association 
maceachern gelfand 
spatial nonparametric bayesian mod els tech 
rep institute statistics decision sciences duke university 
maceachern ller 
estimating mixture dirichlet process models jour nal computational graphical statistics pp 

mackay 
ensemble learning hidden markov models tech 
rep cavendish laboratory university cambridge 
mackay peto 
hierarchical dirichlet language model natural language engineering 
mallick walker 
combining information experiments nonparametric priors biometrika pp 


bayesian predictive approach sequential search optimal dose parametric nonparametric models journal italian statistical society pp 

ller rosner 
method combining inference related nonparametric bayesian models journal royal statistical society pp 

neal 
bayesian mixture modeling proceedings workshop maximum entropy bayesian methods statistical analysis vol 
pp 

markov chain sampling methods dirichlet process mixture models journal computational graphical statistics pp 

pitman 
random discrete distributions invariant size biased permutation ad applied probability pp 

combinatorial stochastic processes tech 
rep department statistics uni versity california berkeley lecture notes st flour summer school 
poisson dirichlet gem invariant distributions split merge transforma tions interval partition combinatorics probability computing pp 

stephens donnelly 
inference population structure genotype data genetics pp 

rabiner 
tutorial hidden markov models selected applications speech recognition proceedings ieee pp 

rasmussen 
infinite gaussian mixture model solla leen 
ller eds 
advances neural information processing systems cambridge ma mit press vol 

salton mcgill 
modern information retrieval new york mcgraw hill 
sethuraman 
constructive definition dirichlet priors statistica sinica pp 

stephens smith donnelly 
new statistical method haplotype recon struction population data american journal human genetics pp 

stolcke omohundro 
hidden markov model induction bayesian model merging giles hanson cowan eds 
advances neural information processing systems san mateo ca morgan kaufmann vol 
pp 

tomlinson 
analysis densities ph thesis department public health sci ences university toronto 
tomlinson escobar 
analysis densities tech 
rep department public health sciences university toronto 
left representation dirichlet process mixture model graphical model 
right hierarchical dirichlet process mixture model 
graphical model formalism node graph associated random variable shading denotes observed variable 
rectangles denote replication model rectangle 
number replicates bottom right corner rectangle 
depiction chinese restaurant franchise 
restaurant represented gle 
customers ji seated tables circles restaurants 
table dish served 
dish served global menu parameter jt table specific indicator serves index items global menu 
customer ji sits table assigned 
gj ji xji perplexity perplexity test lda hdp mixture lda hdp mixture number lda topics number samples posterior number topics hdp mixture number topics left comparison latent dirichlet allocation hierarchical dirichlet process mixture 
results averaged runs error bars standard error 
right histogram number topics hierarchical dirichlet process mixture posterior samples 
gj gj ji xji vs training documents ji xji vs test documents gj ji xji additional training documents gj ji xji vs training documents gj ji xji vs test documents gj additional training documents gj vs training documents models nips data 
left right 
ji xji ji xji gj ji xji vs test documents perplexity average perplexity nips sections models additional ignored flat additional section hierarchical additional section number vs training documents perplexity generalization lt aa ap vs lt aa ap number vs training documents left perplexity single words test vs articles training articles vs section different models 
curves shown averaged sections runs 
right perplexity test vs articles lt aa ap articles respectively averaged runs 
plots error bars represent standard error 
table topics shared vs nips sections 
topics fre quently occurring vs fit constraint associated significant number words greater section 
cs ns lt aa im sp ap cn task representation pattern processing trained representations process unit patterns examples concept similarity bayesian hypotheses generalization numbers positive classes hypothesis cells cell activity response neuron visual patterns pattern single fig visual cells cortical orientation receptive contrast spatial cortex stimulus tuning signal layer gaussian cells fig nonlinearity nonlinear rate eq cell large examples form point see parameter consider random small optimal algorithms test approach methods point problems form large distance tangent image images transformation transformations pattern vectors convolution simard processing pattern approach architecture single shows simple large control motion visual velocity flow target chip eye smooth direction optical visual images video language image pixel acoustic delta lowpass flow signals separation signal sources source matrix blind mixing gradient eq approach trained test layer features table classification rate image images face similarity pixel visual database matching facial examples ii tree pomdp observable strategy class stochastic history strategies density policy optimal reinforcement control action states actions step problems goal perplexity graphical representation hierarchical dirichlet process hidden markov model 
perplexity test sentences alice ml map vb number hidden states number samples posterior number states hdp hmm number states left comparing hdp hmm solid horizontal line ml map vb trained hidden markov models 
error bars represent standard error hdp hmm small see 
right histogram number states hdp hmm posterior samples 

