journal artificial intelligence research submitted published value function approximations partially observable markov decision processes hauskrecht cs brown edu computer science department brown university box brown university providence ri usa partially observable markov decision processes pomdps provide elegant mathematical framework modeling complex decision planning problems stochastic domains states system observable indirectly set imperfect noisy observations 
modeling advantage pomdps comes price exact methods solving computationally expensive applicable practice simple problems 
focus efficient approximation heuristic methods attempt alleviate computational problem trade accuracy speed 
objectives 
survey various approximation methods analyze properties relations provide new insights differences 
second number new approximation methods novel refinements existing techniques 
theoretical results supported experiments problem agent navigation domain 

making decisions dynamic environments requires careful evaluation cost benefits immediate action choices may 
evaluation harder effects actions stochastic pursue evaluate possible outcomes parallel 
typically problem complex look 
situation worse outcomes observe imperfect unreliable indicators underlying process special actions needed obtain reliable information 
unfortunately real world decision problems fall category 
consider example problem patient management 
patient comes hospital initial set complaints 
rarely allow physician diagnose underlying disease certainty number disease options generally remain open initial evaluation 
physician multiple choices managing patient 
choose wait see order additional tests learn patient state disease proceed radical treatment surgery 
making right decision easy task 
disease patient suffers progress time may worse window opportunity particular effective treatment missed 
hand selection wrong treatment may patient condition worse may prevent applying correct treatment 
result treatment typically non deterministic outcomes possible 
addition treatment investigative choices come different costs 
ai access foundation morgan kaufmann publishers 
rights reserved 
hauskrecht course patient management decision maker carefully evaluate costs benefits current choices interaction ordering 
decision problems similar characteristics complex temporal cost benefit tradeoffs stochasticity partial observability underlying controlled process include robot navigation target tracking machine replacement 
sequential decision problems modeled markov decision processes mdps bellman howard puterman boutilier dean hanks extensions 
model choice problems similar patient management partially observable markov decision process pomdp drake astrom sondik lovejoy 
pomdp represents sources uncertainty stochasticity underlying controlled process disease dynamics patient management problem imperfect observability states set noisy observations symptoms findings results tests 
addition lets model uniform way control information gathering investigative actions effects cost benefit tradeoffs 
partial observability ability model reason information gathering actions main features distinguish pomdp widely known fully observable markov decision process bellman howard 
useful modeling perspective pomdps disadvantage hard solve papadimitriou tsitsiklis littman mundhenk goldsmith allender madani hanks condon optimal optimal solutions obtained practice problems low complexity 
challenging goal research area exploit additional structural properties domain suitable approximations heuristics obtain solutions efficiently 
focus heuristic approximation methods particular approximations value functions 
important research issues area design new efficient algorithms better understanding existing techniques relations advantages disadvantages 
address issues 
survey various value function approximations analyze properties relations provide insights differences 
second number new methods novel refinements existing techniques 
theoretical results findings supported empirically problem agent navigation domain 

partially observable markov decision processes partially observable markov decision process pomdp describes stochastic control process partially observable hidden states 
formally corresponds tuple set states set actions set observations set transition probabilities describe dynamic behavior modeled environment set observation probabilities describe relationships observations states actions ir denotes reward model assigns rewards state transitions models payoffs associated transitions 
instances definition pomdp includes priori probability distribution set initial states value function approximations pomdps ot part influence diagram describing pomdp model 
rectangles correspond decision nodes actions circles random variables states diamonds reward nodes 
links represent dependencies components 
st ot rt denote state action observation reward time note action time depends past observations actions states 
objective function pomdp goal construct control policy maximizes objective value function 
objective function combines partial stepwise rewards multiple steps various kinds decision models 
typically models cumulative expectations 
models frequently practice finite horizon model maximize rt rt reward obtained time infinite horizon discounted model maximize rt discount factor 
note pomdps cumulative decision models provide rich language modeling various control objectives 
example easily model goal achievement tasks specific goal reached giving large reward transition state zero smaller rewards transitions 
focus primarily discounted infinite horizon model 
results easily applied finite horizon case 
information state pomdp process states hidden observe making decision action 
action choices information available quantities derived information 
illustrated influence diagram action time depends previous observations actions states 
quantities summarizing information called information states 
complete information states represent trivial case 
st hauskrecht ot st influence diagram pomdp information states corresponding information state mdp 
information states represented double circled nodes 
action choice rectangle depends current information state 
definition complete information state 
complete information state time denoted consists prior belief states time complete history actions observations ot ot starting time 
sequence information states defines controlled markov process call information state markov decision process information state mdp 
policy information state mdp defined terms control function mapping information state space actions 
new information state deterministic function previous state action new observation ot ot 
update function mapping information state space observations actions back information space 
easy see convert original pomdp information state mdp complete information states 
relation components models sketch reduction pomdp information state mdp shown 
bellman equations pomdps information state mdp infinite horizon discounted case fully observable mdp satisfies standard fixed point bellman equation max 

denotes generic update function 
symbol information state space different 
value function approximations pomdps denotes optimal value function maximizing trt state expected step reward equals 
denotes expected step reward state action information state deterministic function previous information state action observation equation rewritten compactly summing possible observations max 
optimal policy control function selects value maximizing action arg max 
value control functions expressed terms action value functions functions max argmax 
function corresponds expected reward chosing fixed action step acting optimally 
sufficient statistics derive equations implicitly complete information states 
remarked earlier information available decision maker summarized quantities 
call sufficient information states 
states preserve necessary information content markov property information state decision process 
definition sufficient information state process 
information state space update function defining information process ot 
process sufficient regard optimal control time step satisfies st st ic ot ot complete information states 
easy see equations complete information states hold sufficient information states 
key benefit sufficient statistics hauskrecht easier manipulate store complete histories may expand time 
example standard pomdp model sufficient belief states assign probabilities possible process state astrom 
case bellman equation reduces max step belief state 
normalizing constant 
defines belief state mdp special case continuous state mdp 
belief state mdps primary focus investigation 
value function mappings properties bellman equation belief state mdp rewritten value function mapping form 
space real valued bounded functions ir defined belief information space ir defined 
defining value function mapping hv maxa bellman equation information states written hv known mdps isotone mapping contraction supremum norm see sobel puterman 
definition mapping isotone implies hv hu 
definition supremum norm 
mapping contraction supremum norm hv hu holds 
value iteration optimal value function equation approximation computed dynamic programming techniques 
simplest approach value iteration bellman shown 
case optimal value function determined limit performing sequence value iteration steps vi hvi vi ith approximation value function ith value function 
sequence estimates 
models belief states sufficient include pomdps observation action channel lags see hauskrecht 

note update vi hvi applied solve finite horizon problem standard way 
difference vi stands steps go value function represents value function rewards states 
value function approximations pomdps value iteration pomdp initialize repeat update hv sup return value iteration procedure 
converges unique fixed point solution direct consequence banach theorem contraction mappings see example puterman 
practice iteration reaches limit solution 
stopping criterion algorithm examines maximum difference value functions obtained consecutive steps called bellman error puterman littman 
algorithm stops quantity falls threshold 
accuracy approximate solution ith value function regard expressed terms bellman error 
theorem sup vi vi vi vi magnitude bellman error 
vi vi hold 
obtain approximation precision bellman error fall piecewise linear convex approximations value function major difficulty applying value iteration dynamic programming mdps belief space infinite need compute update vi hvi 
poses threats value function ith step may representable finite means computable finite number steps 
address problem sondik sondik smallwood sondik showed guarantee computability ith value function finite description belief state mdp considering piecewise linear convex representations value function estimates see 
particular sondik showed piecewise linear convex representation vi vi hvi computable remains piecewise linear convex 
theorem piecewise linear convex functions 
initial value function piecewise linear convex 
ith value function obtained finite number update steps belief state mdp finite piecewise linear convex equal vi max vectors size finite set vectors linear functions hauskrecht piecewise linear convex function pomdp process states 
note holds belief state 
key part proof express update ith value function terms linear functions defining vi vi max max 
leads piecewise linear convex value function vi represented finite set linear functions linear function combination actions permutations vectors size 
combination 
linear function corresponding defined jo 
theorem basis dynamic programming algorithm finding optimal solution finite horizon models value iteration algorithm finding nearoptimal approximations discounted infinite horizon model 
note result imply piecewise linearity optimal fixed point solution algorithms computing value function updates key part value iteration algorithm computation value function updates vi hvi 
assume ith value function vi represented finite number linear segments vectors 
total number possible linear functions combination actions permutations vectors size enumerated time 
complete set linear functions rarely needed linear functions dominated omission change resulting piecewise linear convex function 
illustrated 
value function approximations pomdps redundant linear function redundant linear function 
function dominate regions belief space excluded 
linear function eliminated changing resulting value function solution called redundant 
conversely linear function achieves optimal value point belief space called useful 
sake computational efficiency important size linear function set small possible keep useful linear functions value iteration steps 
main approaches computing useful linear functions 
approach generate test paradigm due sondik monahan 
idea enumerate possible linear functions test usefulness linear functions set prune redundant vectors 
extensions method interleave generate test stages early pruning set partially constructed linear functions zhang liu cassandra littman zhang zhang lee 
second approach builds sondik idea computing useful linear function single belief state sondik smallwood sondik done efficiently 
key problem locate belief points seed useful linear functions different methods address problem differently 
methods implement idea sondik pass algorithms sondik cheng methods cheng witness algorithm kaelbling littman cassandra littman cassandra :10.1.1.107.9127
limitations complexity major difficulty solving belief state mdp complexity piecewise linear convex function grow extremely fast number update steps 
specifically size linear function set defining function grow exponentially number observations single update step 
assuming initial value function linear number linear functions defining ith value function 

defining redundant useful linear functions assume linear function duplicates copy linear function kept set hauskrecht potential growth size linear function set bad news 
remarked earlier piecewise linear convex value function usually complex worst case linear functions pruned away updates 
turned task identifying useful linear functions computationally intractable littman 
means faces potential super exponential growth number useful linear functions inefficiencies related identification vectors 
significant drawback exact methods applicable relatively simple problems 
analysis suggests solving pomdp problem intrinsically hard task 
finding optimal solution finite horizon problem pspace hard papadimitriou tsitsiklis 
finding optimal solution discounted criterion harder 
corresponding decision problem shown undecidable madani optimal solution may computable 
structural refinements basic algorithm standard pomdp model uses flat state space full transition reward matrices 
practice problems exhibit structure represented compactly example graphical models pearl lauritzen dynamic belief networks dean kanazawa kjaerulff dynamic influence diagrams howard matheson 
ways take advantage problem structure modify improve exact algorithms 
example refinement basic monahan algorithm compact transition reward models studied boutilier poole 
hybrid framework combines mdp pomdp problem solving techniques take advantage perfectly partially observable components model subsequent value function decomposition proposed hauskrecht 
similar approach perfect information region subset states containing actual underlying state discussed zhang liu 
casta yost explore techniques solving large pomdps consist set smaller resource coupled independent pomdps 
extracting control strategy value iteration allow compute ith approximation value function vi 
goal find optimal control strategy close approximation 
focus problem extraction control strategies results value iteration 
lookahead design simplest way define control function value function vi greedy step lookahead arg max vi 
see survey boutilier dean hanks different ways represent structured mdps 
value function approximations pomdps direct control design 
linear function defining vi associated action 
action selected linear function function maximal 
vi represents ith approximation optimal value function question arises resulting controller really theorem puterman williams baird littman relates accuracy lookahead controller bellman error 
theorem vi vi magnitude bellman error 
la expected reward lookahead controller designed vi 
la bound construct value iteration routine yields lookahead strategy minimum required precision 
result extended step lookahead design straightforward way steps error bound la direct design extract control action lookahead essentially requires computing full update 
obviously lead unwanted delays reaction times 
general speed response remembering additional information 
particular linear function defining vi associated choice action see equation 
action byproduct methods computing linear functions extra computation required find 
action corresponding best linear function selected directly belief state 
idea illustrated 
bound accuracy direct controller infinite horizon case derived terms magnitude bellman error 
theorem vi vi magnitude bellman error 
dr expected reward direct controller designed vi 
dr direct action choice closely related notion action value function function 
analogously equation ith function satisfies vi max qi 
note control action extracted lookahead vi optimal steps go finite horizon model 
main difference vi optimal value function steps go 
hauskrecht policy graph finite state machine obtained value iteration steps 
nodes correspond linear functions states finite state machine links dependencies linear functions transitions states 
linear function node associated action 
ensure policy applied infinite horizon problem add cycle state dashed line 
qi vi 
perspective direct strategy selects action best maximum belief state 
finite state machine design complex refinement technique remember linear function vi action choice choice linear function previous step observations see equation 
idea applied recursively linear functions previous steps obtain relatively complex dependency structure relating linear functions vi vi observations actions represents control strategy kaelbling 
see model structure graphical terms 
different nodes represent linear functions actions associated nodes correspond optimizing actions links emanating nodes correspond different observations successor nodes correspond linear functions paired observations 
graphs called policy graphs kaelbling littman cassandra 
interpretation dependency structure represents collection finite state machines fsms possible initial states implement pomdp controller nodes correspond states controller actions controls outputs links transitions conditioned inputs 
williams baird give results relating accuracy direct function controller bellman error functions 
value function approximations pomdps observations 
start state fsm controller chosen greedily selecting linear function controller state optimizing value initial belief state 
advantage finite state machine representation strategy steps works observations directly belief state updates needed 
contrasts policy models lookahead direct models keep track current belief state update time order extract appropriate control 
drawback approach fsm controller limited steps correspond number value iteration steps performed 
model controller expected run infinite number steps 
way remedy deficiency extend fsm structure create cycles visit controller states repeatedly 
example adding cycle transition state fsm controller dashed line ensures controller applicable infinite horizon problem 
policy iteration alternative method finding solution discounted infinite horizon problem policy iteration howard sondik 
policy iteration searches policy space gradually improves current control policy belief states 
method consists steps performed iteratively policy evaluation computes expected value current policy policy improvement improves current policy 
saw section ways represent control policy pomdp 
restrict attention finite state machine model observations correspond inputs actions outputs hansen kaelbling 
finite state machine controller finite state machine fsm controller pomdp described set memory states controller set observations inputs set actions outputs transition function mapping states fsm memory states observation output function mapping memory states actions 
function selects initial memory state initial information state 
initial information state corresponds prior posterior belief state time depending availability initial observation 
policy evaluation step policy iteration policy evaluation 
important property fsm model value function specific fsm strategy computed efficiently number controller states key efficient computability 
policy iteration algorithm policies defined regions belief space described sondik 
hauskrecht example state fsm policy 
nodes represent states links transitions states conditioned observations 
memory state associated control action output 
fact value function executing fsm strategy memory state linear 
theorem finite state machine controller set memory states value function applying memory state linear 
value functions solving system linear equations variables 
illustrate main idea example 
assume fsm controller memory states stochastic process hidden states 
value policy augmented state space satisfies system linear equations action executed state transits seeing input observation assuming start policy memory state value policy 

idea linearity efficient computability value functions fixed fsm strategy addressed different contexts number researchers littman cassandra hauskrecht hansen kaelbling 
origins idea traced earlier 
value function approximations pomdps value function linear computed efficiently solving system linear equations 
general fsm controller start memory state choose initial memory state greedily maximizing expected value result 
case optimal choice function defined argmax value fsm policy belief state max 
note resulting value function strategy piecewise linear convex represents expected rewards strategy perform better optimal strategy hold 
policy improvement policy iteration method searching space controllers starts arbitrary initial policy improves gradually refining finite state machine fsm description 
particular keeps modifying structure controller adding removing controller states memory transitions 
old new fsm controller 
improvement step satisfy 
guarantee improvement hansen proposed policy iteration algorithm relies exact value function updates obtain new improved policy structure 
basic idea improvement observation switch back forth fsm policy description piecewise linear convex representation value function 
particular value function fsm policy piecewise linear convex linear function describing corresponds memory state controller individual linear functions comprising new value function update viewed new memory states fsm policy described section 
allows improve policy adding new memory states corresponding linear functions new value function obtained exact update 
technique refined removing linear functions memory states fully dominated linear functions 

policy iteration algorithm exploits exact value function updates works policies defined belief space earlier sondik 
hauskrecht step decision tree 
rectangles correspond decision nodes moves decision maker circles chance nodes moves environment 
black rectangles represent leaves tree 
reward specific path associated leaf tree 
decision nodes associated information states obtained action observation choices path root tree 
example belief state obtained performing action initial belief state observing observation 
forward decision tree methods methods discussed far assume prior knowledge initial belief state treat belief states equally 
initial state known fixed methods modified take advantage fact 
example finite horizon problem finite number belief states reached initial state 
case easier enumerate possible histories sequences actions observations represent problem stochastic decision trees raiffa 
example step decision tree shown 
algorithm solving stochastic decision tree basically mimics value function updates restricted situations reached initial belief state 
key difficulty number possible trajectories grows exponentially horizon interest 
combining dynamic programming decision tree techniques solve pomdp fixed initial belief state apply strategies constructs decision tree solves solves problem backward fashion dynamic programming 
unfortunately techniques inefficient suffering exponential growth decision tree size super exponential growth value function complexity 
techniques combined value function approximations pomdps way partially eliminates disadvantages 
idea fact techniques solution different sides forward backward complexity worsens gradually 
solution compute complete kth value function dynamic programming value iteration cover remaining steps forward decision tree expansion 
various modifications idea possible 
example replace exact dynamic programming efficient approximations providing upper lower bounds value function 
decision tree expanded bounds sufficient determine optimal action choice 
number search techniques developed ai literature korf combined branch bound pruning lave applied type problem 
researchers experimented solve pomdps washington hauskrecht hansen 
methods applicable problem monte carlo sampling kearns mansour ng mcallester singh real time dynamic programming barto bradtke singh dearden boutilier bonet geffner 
classical planning framework pomdp problems fixed initial belief states solutions closely related classical planning extensions handle stochastic partially observable domains particularly buridan buridan planners kushmerick hanks weld draper hanks weld 
objective planners maximize probability reaching goal state 
task similar discounted reward task terms complexity discounted reward model converted goal achievement model introducing absorbing state condon 

heuristic approximations key obstacle wider application pomdp framework computational complexity pomdp problems 
particular finding optimal solution case pspace hard papadimitriou tsitsiklis discounted case may computable madani 
approach problems approximate solution precision 
unfortunately remains intractable general pomdps approximated efficiently goldsmith mundhenk madani 
reason simple problems solved optimally near optimally practice 
alleviate complexity problem research pomdp area focused various heuristic methods approximations error parameter efficient 
heuristic methods focus 
referring approximations mean heuristics specifically stated 

quality heuristic approximation tested bellman error requires exact update step 
heuristic methods se contain precision parameter 
hauskrecht approximation methods combinations divided closely related classes value function approximations policy approximations 
value function approximations main idea value function approximation approach approximate optimal value function ir function ir defined information space 
typically new function lower complexity recall optimal nearoptimal value function may consist large set linear functions easier compute exact solution 
approximations formulated dynamic programming problems expressed terms approximate value function updates understand differences advantages various approximations exact methods sufficient analyze compare update rules 
value function bounds heuristic approximations guaranteed precision cases able say overestimate underestimate optimal value function 
information bounds multiple ways 
example upper help narrowing range optimal value function elimination suboptimal actions subsequent speed ups exact methods 
alternatively knowledge value function bounds determine accuracy controller generated bounds see section 
instances lower bound sufficient guarantee control choice achieves expected reward high bound section 
bound property different methods determined examining updates bound relations 
definition upper bound 
exact value function mapping approximation 
say upper bounds hv hv holds analogous definition constructed lower bound 
convergence approximate value iteration value function mapping representing approximate update 
approximate value iteration computes ith value function vi vi 
fixed point solution close approximation represent intended output approximation routine 
main problem iteration method general converge unique multiple solutions diverge oscillate depending initial function 
unique convergence guaranteed arbitrary mapping convergence specific approximation method proved 
definition convergence 
value iteration converges value function limn exists 
value function approximations pomdps definition unique convergence 
value iteration converges uniquely limn exists pairs limn limn 
sufficient condition unique convergence show contraction 
contraction bound properties combined additional conditions show convergence iterative approximation method bound 
address issue theorem comparing fixed point solutions value function mappings 
theorem value function mappings defined 
contractions fixed points 

isotone mapping 
holds 
note theorem require cover space value functions 
example cover possible value functions belief state mdp restricted space piecewise linear convex value functions 
gives flexibility design iterative approximation algorithms computing value function bounds 
analogous theorem holds lower bound 
control approximation value function available generate control strategy 
general control solutions correspond options section include lookahead direct function finite state machine designs 
drawback control strategies heuristic approximations precision guarantee 
way find accuracy strategies exact update value function approximation adopt result theorems bellman error 
alternative solution problem bound accuracy controllers upper lower bound approximations optimal value function 
illustrate approach prove appendix theorem relates quality bounds quality lookahead controller 
theorem vu vl upper lower bounds optimal value function discounted infinite horizon problem 
sup vu vl vu vl maximum bound difference 
expected reward lookahead controller la constructed vu vl satisfies la policy approximation alternative value function approximation policy approximation 
shown earlier strategy controller pomdp represented finite state machine fsm model 
policy iteration searches space possible policies fsms optimal near optimal solution 
space usually enormous bottleneck hauskrecht method 
searching complete policy space restrict attention subspace believe contain optimal solution approximation 
memoryless policies white scherer littman singh jaakkola jordan policies truncated histories white scherer mccallum finite state controllers fixed number memory states hauskrecht hansen examples policy space restriction 
consider finite state machine model see section quite general models viewed special cases 
states fsm policy model represent memory controller general summarize information past activities observations 
best viewed approximations information states feature states 
transition model controller approximates update function information state mdp output function fsm approximates control function mapping information states actions 
important property model shown section value function fixed controller fixed initial memory state obtained efficiently solving system linear equations 
apply policy approximation approach need decide restrict space policies judge policy quality 
restriction frequently consider controllers fixed number states say structural restrictions narrowing space policies restrict output function choice actions different controller states transitions current states 
general heuristic domain related insight may help selecting right biases 
different policies yield value functions better different regions belief space 
order decide policy best need define importance different regions combinations 
multiple solutions 
example considers worst case measure optimizes worst minimal value initial belief states 
space fsm controllers satisfying restrictions 
quality policy worst case measure max min max 
mc option consider distribution initial belief states maximize expectation value function values 
common objective choose policy leads best value single initial belief state max max mc 
finding optimal policy case reduces combinatorial optimization problem 
unfortunately trivial cases problem computationally intractable 
example problem finding optimal policy memoryless case current observations considered np hard littman 
various heuristics typically applied alleviate difficulty littman 
fully observable mdp approximations section fast informed bound approximations section value function approximations pomdps unobservable mdp approximations section randomized policies value function approximations fixed strategy approximations section curve fitting approximations section grid linear function methods section grid value interpolation extrapolation methods section value function approximation methods 
restricting space policies simplify policy optimization problem 
hand simultaneously give opportunity find best optimal policy replacing best restricted policy 
point considered deterministic policies fixed number internal controller states policies deterministic output transition functions 
finding best deterministic policy best option randomized policies randomized output transition functions usually lead far better performance 
application randomized stochastic policies pomdps introduced 
essentially deterministic policy represented randomized policy single action transition best randomized policy worse best deterministic policy 
difference control performance policies shows cases number states controller relatively small compared optimal strategy 
advantage stochastic policies space larger parameters policy continuous 
problem finding optimal stochastic policy non linear optimization problem variety optimization methods applied solve 
example gradient approach see meuleau 

value function approximation methods section discuss depth value function approximation methods 
focus approximations belief information space 
survey known techniques include number new methods modifications existing methods 
summarizes methods covered 
describe methods means update rules 
alternative value function approximations may complete histories past actions observations 
approximation methods white scherer example 
hauskrecht moves sensors test example 
maze navigation problem maze 
implement simplifies analysis theoretical comparison 
focus properties complexity dynamic programming value iteration updates complexity value functions method uses ability methods bound exact update convergence value iteration approximate update rules control performance related controllers 
results theoretical analysis illustrated empirically problem agent navigation domain 
addition agent navigation problem illustrate give intuitions characteristics methods theoretical underpinning 
results generalized problems rank different methods 
agent navigation problem maze maze navigation problem states actions observations 
maze consists partially connected rooms states robot operates collects rewards 
robot move directions north south east west check presence walls sensors 
move actions sensor inputs perfect robot moving unintended directions 
robot moves unintended direction probability neighboring directions 
move wall keeps robot position 
investigative actions help robot navigate activating sensor inputs 
investigative actions allow robot check inputs presence wall east west directions 
sensor accuracy detecting walls wall case north south wall wall case north south wall case smaller probabilities wrong perceptions 
control objective maximize expected discounted rewards discount factor 
small reward action leading bumping wall points move points investigative action large reward points achieving special target room indicated circle recognizing performing move actions 
doing collecting reward robot placed random new start position 
maze problem moderate complexity regard size state action observation spaces exact solution reach current exact methods 
exact methods tried problem include witness algorithm kaelbling incremental pruning algorithm cassandra 
anthony cassandra running algorithms 
mdp value function approximations pomdps pomdp mdp mdp mdp qmdp pomdp mdp mdp approximations fully observable version state pomdp states mdp approximation qmdp approximation 
values extreme points belief space solutions fully observable mdp 
policy iteration fsm model hansen 
main obstacle preventing algorithms obtaining optimal close optimal solution complexity value function number linear functions needed describe subsequent running times memory problems 
approximations fully observable mdp simplest way approximate value function pomdp assume states process fully observable astrom lovejoy 
case optimal value function pomdp approximated mdp mdp optimal value function state fully observable version process 
refer approximation mdp approximation 
idea approximation illustrated 
resulting value function linear fully defined values extreme points belief simplex 
correspond optimal values fully observable case 
main advantage approximation fully observable mdp solved efficiently problem discounted infinite horizon problems 
update step fully observable mdp mdp max mdp 
solution finite state fully observable mdp discounted infinite horizon criterion efficiently formulating equivalent linear programming task bertsekas mdp approximation hauskrecht mdp approximation approach equation described terms updates belief space mdp 
step strictly speaking redundant simplifies analysis comparison approach approximations 
vi linear value function described vector mdp corresponding values mdp states th value function vi vi max mdp vi 
vi described linear function components mdp mdp max mdp mdp rule rewritten general form starts arbitrary piecewise linear convex value function vi represented set linear functions vi max max application mapping leads linear value function 
update easy compute takes time 
reduces time mdp updates strung 
remarked earlier optimal solution infinite horizon discounted problem solved efficiently linear programming 
update mdp approximation upper bounds exact update vi vi 
show property theorem covers cases 
intuition get better solution information fully observable mdp upper bound partially observable case 
approximation functions qmdp variant approximation fully observable mdp uses functions littman cassandra kaelbling max mdp mdp mdp optimal action value function function fully observable mdp 
qmdp approximation piecewise linear convex linear functions corresponding value function approximations pomdps action 
qmdp update rule belief state mdp vi linear functions vi max vi 
max generates value function linear functions 
time complexity update mdp approximation case reduces time qmdp updates 
contraction mapping fixed point solution solving corresponding fully observable mdp 
qmdp update upper bounds exact update 
bound tighter mdp update vi vi vi prove theorem 
inequalities hold fixed point solutions theorem 
illustrate difference quality bounds mdp approximation qmdp method maze navigation problem 
measure quality bound mean value function values 
belief states equally important assume uniformly distributed 
approximate measure average values fixed set belief points 
points set selected uniformly random 
set chosen fixed remained tests 
shows results experiment include results fast informed bound method section 
shows running times methods 
methods implemented common lisp run sun ultra workstation 
control mdp qmdp value function approximations construct controllers step lookahead 
addition qmdp approximation suitable direct control strategy selects action corresponding best highest value function 
method special case function approach discussed section 
advantage direct qmdp method faster lookahead designs 
hand lookahead tends improve control performance 
shown compares control performance different controllers maze problem 
quality policy preference particular initial belief state measured mean value function values uniformly distributed initial belief states 
approximate measure average discounted rewards 
confidence interval limits probability level range respective average scores holds bound experiments 
relatively small include graphs 

pointed littman 
instances direct qmdp controller selects investigative actions actions try gain information underlying process state 
note observation true general qmdp controller direct action selection may select investigative actions fully observable version problem investigative actions chosen 
score mdp approximation bound quality qmdp approximation fast informed bound hauskrecht time sec mdp approximation running times qmdp approximation fast informed bound comparison mdp qmdp fast informed bound approximations bound quality left running times right 
bound quality score average value approximation set belief points chosen uniformly random 
methods upper bound optimal value function flip bound quality graph longer bars indicate better approximations 
control trajectories obtained fixed set initial belief states selected uniformly random 
trajectories obtained simulation steps long 
validate comparison averaged performance scores show scores result randomness methods statistically significantly different 
rely pairwise significance tests 
summarize obtained results score differences methods sufficient reject method lower score better performer significance levels respectively 
error bars reflect critical score difference significance level 
shows average reaction times different controllers experiments 
results show clear dominance direct qmdp controller need lookahead order extract action compared controllers 
fast informed bound method mdp qmdp approaches ignore partial observability fully observable mdp surrogate 
improve approximations account 
length trajectories steps maze problem chosen ensure estimates discounted cumulative rewards far actual rewards infinite number steps 

alternative way compare methods compute confidence limits scores inspect overlaps 
case ability distinguish methods reduced due fluctuations scores different initializations 
maze confidence interval limits probability level range respective average scores 
covers control experiments 
pairwise tests eliminate dependency examining differences individual values improve discriminative power 

critical score differences listed cover worst case combination 
may pairs smaller difference suffice 
score lookahead mdp approximation value function approximations pomdps control performance lookahead direct qmdp approximation lookahead direct fast informed bound time sec lookahead mdp approximation reaction times lookahead qmdp approximation lookahead direct direct fast informed bound comparison control performance mdp qmdp fast informed bound methods quality control left reaction times right 
quality control score average discounted rewards control trajectories obtained fixed set initial belief states selected uniformly random 
error bars show critical score difference value methods statistically different significance level 
degree partial observability propose new method fast informed bound method 
vi piecewise linear convex value function represented set linear functions new update defined max max max vi max vi 
fast informed bound update obtained exact update derivation vi max max max max max max max vi 
value function vi vi obtains update piecewise linear convex consists different linear functions corresponding action hauskrecht max 
update efficient computed time 
method outputs linear functions computation done time updates strung 
significant complexity reduction compared exact approach lead function consisting linear functions exponential number observations worst case takes time 
updates polynomial complexity find approximation finite horizon case efficiently 
open issue remains problem finding solution infinite horizon discounted case complexity 
address establish theorem 
theorem solution fast informed bound approximation solving mdp states actions discount factor 
full proof theorem deferred appendix 
key part proof construction equivalent mdp states representing updates 
finite state mdp solved linear program conversion fixed point solution fast informed bound update computable efficiently 
fast informed bound versus fully observable mdp approximations fast informed update upper bounds exact update tighter mdp qmdp approximation updates 
theorem vi corresponds piecewise linear convex value function defined linear functions 
vi vi vi vi 
key trick deriving result swap max sum operators proof appendix obtain upper bound inequalities subsequent reduction complexity update rules compared exact update 
shown 
umdp approximation included discussed section 
difference methods boils simple mathematical manipulations 
note inequality relations derived updates hold fixed point solutions theorem 
illustrates improvement bound mdp approximations maze problem 
note improvement paid increased running time complexity 
control fast informed bound outputs piecewise linear convex function linear function action 
allows build pomdp controller selects action associated best highest value linear function directly 
compares control performance direct lookahead controllers mdp qmdp controllers 
see fast informed bound leads tighter bounds value function approximations pomdps umdp update vi max max exact update vi max max fast informed bound update vi max max qmdp approx 
update vi max max mdp approx 
update max max relations exact update umdp fast informed bound qmdp mdp updates 
improved control average 
stress currently theoretical underpinning observation may true belief states problem 
extensions fast informed bound method main idea fast informed bound method select best linear function observation current state separately 
differs exact update seek linear function gives best result observation combination states 
observe great deal middle ground extremes 
design update rule chooses optimal maximal linear functions disjoint sets states separately 
illustrate idea assume partitioning sm state space new update vi max max max max sm easy see update upper bounds exact update 
exploration approach various partitioning heuristics remains interesting open research issue 
hauskrecht approximation unobservable mdp mdp approximation assumes full observability pomdp states obtain simpler efficient updates 
extreme discard observations available decision maker 
mdp observations called unobservable mdp umdp may choose value function solution alternative approximation 
find solution unobservable mdp derive corresponding update rule similarly update partially observable case 
preserves piecewise linearity convexity value function contraction 
update equals vi max max vi set linear functions describing vi 
vi remains piecewise linear convex consists linear functions 
contrast exact update number possible vectors step grow exponentially number observations leads possible vectors 
time complexity update 
starting linear function running time complexity updates bounded 
problem finding optimal solution unobservable mdp remains intractable finite horizon case np hard discounted infinite horizon case undecidable madani 
usually useful approximation 
update lower bounds exact update intuitive result reflecting fact better information 
provide insight updates related derivation proves bound property elegant way vi max max max max max max vi 
see difference exact umdp updates max sum step observations exchanged 
causes choice vectors independent observations 
sum max operations exchanged observations marginalized 
recall idea swaps leads number approximation updates see summary 
value function approximations pomdps fixed strategy approximations finite state machine fsm model primarily define control strategy 
strategy require belief state updates directly maps sequences observations sequences actions 
value function fsm strategy piecewise linear convex efficiently number memory states section 
policy iteration policy approximation contexts value function specific strategy quantify goodness policy place value function substitute optimal value function 
case value function defined belief space equals max obtained solving set linear equations section 
remarked earlier value fixed strategy lower bounds optimal value function simplify comparison fixed strategy approximation approximations rewrite solution terms fixed strategy updates max vi max vi 
value function vi piecewise linear convex consists linear functions 
infinite horizon discounted case represents ith approximation 
note update applied finite horizon case straightforward way 
quality control assume fsm strategy substitute optimal control policy 
different ways extract control 
simply execute strategy represented fsm 
need update belief states case 
second possibility choose linear functions corresponding different memory states associated actions repeatedly step 
refer controller direct dr controller 
approach requires updating belief states step 
hand control performance worse fsm control 
final strategy discards information actions extracts policy value function step lookahead 
method la requires belief state updates lookaheads leads worst reactive time 
dr strategy guaranteed worse fsm controller 
theorem relates performances controllers 
score fsm controller control performance dr controller la controller hauskrecht time sec fsm controller reaction times dr controller la controller comparison different controllers fsm dr la maze problem collection action policies control quality left response time right 
error bars control performance graph indicate critical score difference methods statistically different significance level 
theorem fsm controller 
cdr cla direct step lookahead controllers constructed 
cdr cla hold belief states prove direct controller lookahead controller better underlying fsm controller see appendix full proof theorem show similar property controllers initial belief states 
lookahead approach typically tends dominate reflecting usual trade control quality response time 
illustrate trade running maze example collection action policies generating sequence action 
control quality response time results shown 
see controller fsm fastest worst terms control quality 
hand direct controller slower needs update belief states step delivers better control 
lookahead controller slowest best control performance 
selecting fsm model quality fixed strategy approximation depends strongly fsm model 
model provided priori constructed automatically 
techniques automatic construction fsm policies correspond search problem complete restricted space policies examined find optimal near optimal policy space 
search process equivalent policy approximations policy iteration techniques discussed earlier sections 
value function approximations pomdps grid approximations value interpolation extrapolation value function continuous belief space approximated finite set grid points interpolation extrapolation rule estimates value arbitrary point belief space relying points grid associated values 
definition interpolation extrapolation rule ir real valued function defined information space bg bg set grid points bg set point value pairs 
function rg ir ir estimates point information space values associated grid points called interpolation extrapolation rule 
main advantage interpolation extrapolation model estimating true value function requires compute value updates finite set grid points vi approximation ith value function 
approximation th value function vi obtained vi rg values associated grid point bg included gi vi max vi 
grid update described terms value function mapping hg vi hg vi 
complexity update eval rg eval rg computational cost evaluating interpolation extrapolation rule rg grid points 
show section instances need evaluate interpolation extrapolation rule step eliminated 
family convex rules number possible interpolation extrapolation rules enormous 
focus set convex rules relatively small important subset rules 
definition convex rule function defined space bg bg bg set grid points bg bg bg bg bgk bgk set point value pairs 
rule rg estimating called convex value rg jf bj bj 

note convex rules special case introduced gordon 
difference minor definition includes constant independent grid points values added convex combination 
hauskrecht key property convex rules corresponding grid update hg contraction max norm gordon 
approximate value iteration hg converges unique fixed point solution 
addition hg convex rules isotone 
examples convex rules family convex rules includes approaches commonly practice nearest neighbor kernel regression linear point interpolations 
take example nearest neighbor approach 
function belief point estimated value grid point closest terms distance metric defined belief space 
point exactly nonzero parameter bg holds zero 
assuming euclidean distance metric nearest neighbor approach leads piecewise constant approximation regions equal values correspond regions common nearest grid point 
nearest neighbor estimates function value account grid point value 
kernel regression expands grid points 
adds weights contributions values distance target point 
example assuming gaussian kernels weight grid point exp bg normalizing constant ensuring bj parameter flattens narrows weight functions 
euclidean metric kernel regression rule leads smooth approximation function 
linear point interpolations subclass convex rules addition constraints definition satisfy jb belief point convex combination grid points corresponding coefficients 
optimal value function pomdp convex new constraint sufficient prove upper bound property approximation 
general different linear point interpolations grid 
challenging problem find rule best approximation 
discuss issues section 
conversion grid mdp assume find approximation value function convex rule grid update equation 
view process process finding sequence values bg bg bg grid points bg show instances sequence values computed applying interpolation extrapolation rule step 
cases problem value function approximations pomdps converted fully observable mdp states corresponding grid points call mdp grid mdp 
theorem finite set grid points rg convex rule parameters fixed 
values solving fully observable mdp states discount factor 
proof grid point write max bg max max denoting bj bgk construct fully observable mdp problem states corresponding grid points discount factor 
update step equals max bg prerequisite bj guarantees bg interpreted true probabilities 
compute values solving equivalent fully observable mdp 
solving grid approximations idea converting grid approximation grid mdp basis simple powerful approximation algorithm 
briefly key find parameters transition probabilities rewards new mdp model solve 
process relatively easy parameters interpolate extrapolate value non grid point fixed assumption theorem 
case determine parameters new mdp efficiently step grid set nearest neighbor kernel regression examples rules property 
note leads polynomial time algorithms finding values grid points recall mdp solved efficiently finite discounted infinite horizon criteria 
problem solving grid approximation arises parameters interpolation extrapolation fixed subject optimization 
happens example multiple ways interpolating value 
note similar result proved independently gordon 
hauskrecht point belief space find best interpolation leading best values grid points case corresponding optimal grid mdp single step iterative approximation solving sequence grid mdps usually needed 
worst case complexity problem remains open question 
constructing grids issue touched far selection grids 
multiple ways select grids 
divide classes regular non regular grids 
regular grids lovejoy partition belief space evenly equal size regions 
main advantage regular grids simplicity locate grid points neighborhood belief point 
disadvantage regular grids restricted specific number points increase grid resolution paid exponential increase grid size 
example sequence regular grids dimensional belief space corresponds pomdp states consists grid points 
prevents method higher grid resolutions problems larger state spaces 
non regular grids unrestricted provide flexibility grid resolution increased adaptively 
hand due irregularities methods locating grid points adjacent arbitrary belief point usually complex compared regular grids 
linear point interpolation fact optimal value function convex belief state mdps show approximation linear point interpolation upper bounds exact solution lovejoy 
kernel regression nearest neighbor guarantee bound 
theorem upper bound property grid point interpolation update 
vi convex value function 
vi hg vi 
upper bound property hg update convex value functions follows directly jensen inequality 
convergence upper bound follows theorem 
note point interpolation update imposes additional constraint choice grid points 
particular easy see valid grid include extreme points belief simplex extreme points correspond 
regular grids lovejoy triangulation 
essentially idea partition evenly dimensional subspace ir fact affine transform allows map grid points belief space grid points dimensional space lovejoy 

number points regular grid sequence lovejoy grid refinement parameter 


value function approximations pomdps 
extreme points unable cover belief space interpolation 
nearest neighbor kernel regression impose restrictions grid 
finding best interpolation general multiple ways interpolate point belief space 
objective find best interpolation leads tightest upper bound optimal value function 
belief point bj bj bj set grid value pairs 
best interpolation point min jf bj subject linear optimization problem 
solved polynomial time linear programming techniques computational cost doing relatively large especially considering fact optimization repeated times 
alleviate problem seek efficient ways finding interpolation sacrificing optimality 
way find suboptimal interpolation quickly apply regular grids proposed lovejoy 
case value belief point approximated convex combination grid points closest 
approximation leads piecewise linear convex value functions 
interpolations fixed problem finding approximation converted equivalent grid mdp solved finite state mdp 
pointed previous section regular grids specific number grid points increase resolution grid paid exponential increase grid size 
feature method attractive problem large state space need achieve high grid resolution 
focus non regular arbitrary grids 
propose interpolation approach searches limited space interpolations guaranteed run time linear size grid 
idea approach interpolate point belief space dimension set grid points consists arbitrary grid point extreme points belief simplex 
coefficients interpolation efficiently search best interpolation 
grid point defining interpolation 
value point satisfies vi min value interpolation grid point illustrates resulting approximation 
function characterized sawtooth shape influenced choice interpolating set 
find best value function solution close approximation apply value iteration procedure search best interpolation update step 

solution problem may adaptive regular grids grid resolution increased parts belief space 
leave idea 
hauskrecht value function approximation linear time interpolation approach dimensional case 
interpolating sets restricted single internal point belief space 
drawback approach interpolations may remain unchanged update steps slowing solution process 
alternative approach solve sequence grid mdps 
particular stage find best minimum value interpolations belief points reachable grid points step fix coefficients interpolations construct grid mdp solve exactly approximately 
process repeated improvement improvement larger threshold seen values different grid points 
improving grids adaptively quality approximation bound depends strongly points grid 
objective provide approximation smallest possible set grid points 
task impossible achieve known advance solving belief points pick 
way address problem build grids incrementally starting small set grid points adding adaptively places greater chance improvement 
key part approach heuristic choosing grid points added 
heuristic method developed attempts maximize improvements bound values stochastic simulations 
method builds fact interpolation grid include extreme points cover entire belief space 
extreme points values affect grid points try improve values place 
general value grid point improves precise values successor belief points belief states correspond choice observation current optimal action choice incorporating points grid larger improvement value initial grid point 
assuming initial point extreme point heuristic tends improve value point 
naturally proceed selection incorporating successor points level successors grid forth 
value function approximations pomdps generate new grid points set extreme points repeat set arg maxa select observation update add return procedure generating additional grid points bound improvement heuristic 
score mdp fast interpolation qmdp informed regular grid bound quality interpolation adaptive grid interpolation random grid improvement upper bound quality grid point interpolations adaptive grid method 
method compared randomly refined grid regular grid points 
upper bound approximations mdp qmdp fast informed bound methods included comparison 
capture idea generate new grid points simulation starting extremes belief simplex continuing belief point currently grid reached 
algorithm implements bound improvement heuristic expands current grid set new grid points relying current value function approximation shown 
illustrates performance bound quality adaptive grid method maze problem 
combination adaptive grids linear time interpolation approach 
method gradually expands grid point increments grid points 
shows performance random grid method time sec mdp qmdp fast informed interpolation regular grid hauskrecht running times interpolation adaptive grid interpolation random grid running times grid point interpolation methods 
methods tested include adaptive grid random grid regular grid grid points 
running times adaptive grid cumulative reflecting dependencies higher grid resolutions lower level resolutions 
running time results mdp qmdp fast informed bound approximations shown comparison 
new points grid selected random results grid point increments shown 
addition gives results regular grid interpolation lovejoy belief points upper bound methods mdp qmdp fast informed bound approximations 
see dramatic improvement quality bound adaptive method 
contrast uniformly sampled grid random grid approach hardly changes bound 
reasons uniformly sampled grid points concentrated center belief simplex transition matrix maze problem relatively sparse belief points obtains extreme points step boundary simplex 
grid points center simplex interpolate belief states reachable extremes step improve values extremes bound change 
drawback adaptive method running time grid size need solve sequence grid mdps 
compares running times different methods maze problem 
grid expansion adaptive method depends value function obtained previous steps plot cumulative running times 
see relatively large increase running time especially larger grid sizes reflecting trade bound quality running time 
note adaptive grid method performs quite initial steps grid points outperforms regular grid points bound quality 
note heuristic approaches constructing adaptive grids point interpolation possible 
example different approach refines grid ex score value function approximations pomdps mdp qmdp fast informed control performance interpolation interpolation regular grid adaptive grid interpolation random grid nearest neighbor adaptive grid nearest neighbor random grid control performance lookahead controllers grid point interpolation nearest neighbor methods varying grid sizes 
results compared mdp qmdp fast informed bound controllers 
differences values current grid points proposed brafman 
control value functions obtained different grid methods define variety controllers 
compares performances lookahead controllers point interpolation nearest neighbor methods 
run versions approaches adaptive grid random grid show results obtained grid points 
addition compare performances interpolation regular grids grid points mdp qmdp fast informed bound approaches 
performance interpolation extrapolation techniques tested maze problem bit disappointing 
particular better scores achieved simpler qmdp fast informed bound methods 
see heuristics improved bound quality approximations lead similar improvement qmdp fast informed bound methods terms control 
result shows bad bound terms absolute values imply bad control performance 
main reason control performance influenced relative absolute value function values words shape function 
interpolation extrapolation techniques regular grid interpolation approximate value function functions piecewise linear convex interpolations linear time interpolation technique sawtooth shaped function nearest neighbor leads piecewise constant function 
allow match shape optimal function correctly 
factor affects performance large sensitivity methods selection grid points documented example comparison heuristic random grids 
hauskrecht tests focused lookahead controllers 
alternative way define controller grid interpolation extrapolation methods function approximations value functions direct lookahead designs 
approximations solving grid mdp keeping values functions different actions separate 
approximations value functions curve fitting squares fit alternative way approximate function continuous space curve fitting techniques 
approach relies predefined parametric model value function set values associated finite set grid belief points approach similar interpolation extrapolation techniques relies set belief value pairs 
difference curve fitting remembering belief value pairs tries summarize terms parametric function model 
strategy seeks best possible match model parameters observed point values 
best match defined various criteria squares fit criterion objective minimize error yj bj bj yj correspond belief point associated value 
index ranges points sample set combining dynamic programming squares fit squares approximation function construct dynamic programming algorithm update step vi vi 
approach steps 
obtain new values set sample points vi max vi second fit parameters value function model vi new sample value pairs square error cost function 
complexity update eval vi fit vi time eval vi computational cost evaluating vi fit vi cost fitting parameters vi belief value pairs 
advantage approximation squares fit requires compute updates finite set belief states 
drawback approach combined value iteration method lead instability divergence 
shown mdps researchers bertsekas boyan moore baird tsitsiklis roy 

similar qmdp method allows lookahead greedy designs 
fact qmdp viewed special case grid method function approximations grid points correspond extremes belief simplex 
value function approximations pomdps line version squares fit problem finding set parameters best fit solved available optimization procedure 
includes line instance version gradient descent method corresponds known delta rule rumelhart hinton williams 
denote parametric value function belief space adjustable weights wk 
line update weight wi computed wi wi bj yj bj wi learning constant bj yj seen point value 
note gradient descent method requires function differentiable regard adjustable weights 
solve discounted infinite horizon problem stochastic line version squares fit combined parallel synchronous incremental gauss seidel point updates 
case value function previous step fixed new value function computed scratch set belief point samples values computed step expansion 
parameters stabilized attenuating learning rates newly acquired function fixed process proceeds iteration 
incremental version single value function model time updated compute new values sampled points 
littman 
parr russell implement approach asynchronous reinforcement learning backups sample points updated obtained stochastic simulation 
stress versions subject threat instability divergence remarked 
parametric function models apply squares approach select appropriate value function model 
examples simple convex functions linear quadratic functions complex models possible 
interesting relatively simple approach squares approximation linear action value functions functions littman 
value function vi approximated piecewise linear convex combination qi functions vi max qi qi squares fit linear function set sample points values points obtained vi 
method leads approximation linear functions coefficients functions efficiently solving set linear equations 
recall approximations qmdp fast informed bound approximations hauskrecht linear functions 
main differences methods qmdp fast informed bound methods update linear functions directly guarantee upper bounds unique convergence 
sophisticated parametric model convex function softmax model parr russell set linear functions adaptive parameters fit temperature parameter provides better fit underlying piecewise linear convex function larger values 
function represents soft approximation piecewise linear convex function parameter smoothing approximation 
control tested control performance squares approach linear function model littman softmax model parr russell 
softmax model varied number linear functions trying cases linear functions respectively 
set experiments parallel synchronous updates samples fixed set belief points 
applied stochastic gradient descent techniques find best fit cases 
tested control performance value function approximations obtained updates starting qmdp solution 
second set experiments applied incremental stochastic update scheme gauss seidel style updates 
results method acquired grid point updated times learning rates decreasing linearly range 
started qmdp solution 
results lookahead controllers summarized shows control performance direct function controller comparison results qmdp method 
linear function model performed results lookahead design better results qmdp method 
difference quite apparent direct approaches 
general performance method attributed choice function model match shape optimal value function reasonably 
contrast softmax models linear functions perform expected 
probably softmax model linear functions updated sample point 
leads situations multiple linear functions try track belief point update 
circumstances hard capture structure optimal value function accurately 
negative feature effects line changes linear functions added softmax approximation bias incremental update schemes 
ideal case identify vector responsible specific belief point update modify vector 
linear function approach avoids problem updating single linear function corresponding action 
score value function approximations pomdps lookahead direct qmdp approximation iter iter iter stoch linear function lookahead control performance iter iter iter linear function direct stoch iter iter iter stoch softmax linear functions iter iter iter stoch softmax linear functions control performance squares fit methods 
models tested include linear function model direct lookahead control softmax models linear functions lookahead control 
value functions obtained synchronous updates value functions obtained incremental stochastic update scheme define different controllers 
comparison include results qmdp controllers 
grid approximations linear function updates alternative grid approximation method constructed applying sondik approach computing derivatives linear functions points grid lovejoy 
vi piecewise linear convex function described set linear functions new linear function belief point action computed efficiently smallwood sondik littman indexes linear function set linear functions defining vi maximizes expression fixed combination optimizing function acquired choosing vector best value action vectors 
assuming set candidate linear functions resulting functions satisfies arg max 
collection linear functions obtained set belief points combined piecewise linear convex value function 
idea number exact hauskrecht new linear function incremental version grid linear function method 
piecewise linear lower bound improved new linear function computed belief point sondik method 
algorithms see section 
exact case set points cover linear functions defining new value function located hard task 
contrast approximation method uses incomplete set belief points fixed easy locate example random heuristic selection 
denote value function mapping grid approach 
advantage grid method leads efficient updates 
time complexity update polynomial equals 
yields set linear functions compared possible functions exact update 
set grid points incomplete resulting approximation lower bounds value function obtain performing exact update lovejoy 
theorem lower bound property grid linear function update 
vi piecewise linear value function set grid points compute linear function updates 
vi vi 
incremental linear function approach drawback grid linear function method contraction discounted infinite horizon case value iteration method mapping may converge lovejoy 
remedy problem propose incremental version grid linear function method 
idea refinement prevent instability gradually improving piecewise linear convex lower bound value function 
assume vi convex piecewise linear lower bound optimal value function defined linear function set linear function point computed vi sondik method 
construct new improved value function vi vi simply adding new linear function idea incremental update illustrated similar incremental methods cheng lovejoy 
method score standard approach value function approximations pomdps bound quality incremental approach time sec qmdp fast informed running times standard approach incremental approach bound quality running times standard incremental version grid linear function method fixed point grid 
cumulative running times including previous update cycles shown methods 
running times qmdp fast informed bound methods included comparison 
extended handle set grid points straightforward way 
note adding new linear functions previous linear functions may redundant removed value function 
techniques redundancy checking applied exact approaches monahan eagle 
incremental refinement stable converges fixed set grid points 
price paid feature linear function set grow size iteration steps 
growth linear number iterations compared potentially exponential growth exact methods linear function set describing piecewise linear approximation huge 
practice usually incremental updates method converges 
question remains open complexity hardness problem finding fixed point solution fixed set grid points illustrates trade offs involved applying incremental updates compared standard fixed grid approach maze problem 
grid points techniques initial value function 
results update cycles shown 
see incremental method longer running times standard method number linear functions grow update 
hand bound quality incremental method improves rapidly worse update steps 
minimum expected reward incremental method improves lower bound value function 
value function say vi create controller lookahead direct action choice 
general case say performance quality controllers regard vi 
certain conditions performance controllers guaranteed fall vi 
theorem proved appendix establishes conditions 
theorem vi value function obtained incremental linear function method starting corresponds fixed strategy 
cla cdr hauskrecht controllers vi lookahead controller direct action controller cla cdr respective value functions 
vi cla vi cdr hold 
note property holds incremental version exact value iteration 
lookahead direct controllers perform worse vi obtained incremental updates corresponding fsm controller 
selecting grid points incremental version grid linear function approximation flexible works arbitrary grid 
grid need fixed changed line 
problem finding grids reduces problem selecting belief points updated 
apply various strategies 
example fixed set grid points update repeatedly select belief points line various heuristics 
incremental linear function method guarantees value function improved linear functions previous steps kept redundant 
quality new linear function added depends strongly quality linear functions obtained previous steps 
objective select order points better chances larger improvement 
designed heuristic strategies selecting ordering belief points 
strategy attempts optimize updates extreme points belief simplex ordering heuristically 
idea heuristic fact states higher expected rewards designated goal states effects rewards locally 
desirable states neighborhood highest reward state updated distant ones 
apply idea order extreme points belief simplex relying current estimate value function identify highest expected reward states pomdp model determine neighbor states 
second strategy idea stochastic simulation 
strategy generates sequence belief points reached fixed initial belief point 
points sequence reverse order generate updates 
intent heuristic maximize improvement value function initial fixed point 
run heuristic need find initial belief point set initial belief points 
address problem heuristic allows order extreme points belief simplex 
points initial beliefs simulation part 
tier strategy top level strategy orders extremes belief simplex lower level strategy applies stochastic simulation generate sequence belief states reachable specific extreme point 
tested order heuristics tier heuristics maze problem compared simple point selection strategies fixed grid strategy set grid points updated repeatedly random grid strategy points chosen uniformly random 
shows bound quality 
restriction grid points included grid required example linear point interpolation scheme extreme points belief simplex 
score value function approximations pomdps fixed grid bound quality random grid order heuristic tier heuristic improvements bound quality incremental linear function method different grid selection heuristics 
cycle includes grid point updates 
methods update cycles cycle consists grid point updates maze problem 
see differences quality value function approximations different strategies simple ones relatively small 
note observed similar results problems just maze 
relatively small improvement heuristics explained fact new linear function influences larger portion belief space method sensitive choice specific point 
plausible explanation heuristics accurate heuristics combinations heuristics constructed 
efficient strategies locating grid points exact methods witness algorithm kaelbling cheng methods cheng potentially applied problem 
remains open area research 
control grid linear function approach leads piecewise linear convex approximation 
linear function comes natural action choice lets choose action greedily 
run lookahead direct controllers 
compares performance different controllers fixed grid points combining standard incremental updates lookahead direct greedy control update cycles 
results see illustrate trade offs computational time obtaining solution quality 
see incremental approach lookahead controller design tend improve control performance 
prices paid worse running reaction times respectively 

small sensitivity incremental method selection grid points suggest instances replace exact updates simpler point selection strategies 
increase speed exact value iteration methods initial stages suffer inefficiencies associated locating complete set grid points updated step 
issue needs investigated 
score lookahead direct qmdp lookahead direct fast informed hauskrecht control performance direct standard lookahead standard direct incremental lookahead incremental control performance different controllers grid linear function updates update cycles point grid 
controllers represent combinations update strategies standard incremental action extraction techniques direct lookahead 
running times update strategies 
comparison include performances qmdp fast informed bound methods direct lookahead designs 
score qmdp fast informed control performance fixed grid random grid order heuristic tier heuristic control performances lookahead controllers incremental approach different point selection heuristics improvement cycles 
comparison scores qmdp fast informed bound approximations shown 
illustrates effect point selection heuristics control 
compare results lookahead control approximations obtained improvement cycles cycle consists grid point updates 
test results show value function approximations pomdps bound quality big differences various heuristics suggesting small sensitivity control selection grid points 
summary value function approximations heuristic value function approximations methods allow replace hard compute exact methods trade solution quality speed 
numerous methods employ different properties different trade offs quality versus speed 
tables summarize main theoretical properties approximation methods covered 
majority methods polynomial complexity efficient polynomial bellman updates 
candidates complex pomdp problems reach exact methods 
methods heuristic approximations give solutions guaranteed precision 
despite fact proved solutions methods worse terms value function quality see 
main contributions 
currently minimal theoretical results relating methods terms control performance exception results fsm controllers fsm approximations 
key observation quality control lookahead control important approximate shape derivatives value function correctly 
illustrated empirically interpolation extrapolation methods section non convex value functions 
main challenges find ways analyzing comparing control performance different approximations theoretically identify classes pomdps certain methods dominate 
note list methods complete value function approximation methods refinements existing methods possible 
example white scherer investigate methods truncated histories lead upper lower bound estimates value function complete information states complete histories 
additional restrictions methods change properties generic method 
example possible additional assumptions able ensure convergence squares fit approximation 

pomdps offers elegant mathematical framework representing decision processes stochastic partially observable domains 
despite modeling advantages pomdp problems hard solve exactly 
complexity problem key aspect application model real world problems expense optimality 
complexity results approximability pomdp problems encouraging madani focus heuristic approximations particular approximations value functions 
hauskrecht method mdp approximation qmdp approximation fast informed bound umdp approximation fixed strategy method bound upper upper upper lower lower contraction grid interpolation extrapolation nearest neighbor kernel regression linear point interpolation upper curve fitting squares fit linear function grid linear function method incremental version start lower bound lower lower table properties different value function approximation methods bound property contraction property underlying mappings 
incremental version grid linear function method contraction converges 
method finite horizon discounted infinite horizon mdp approximation qmdp approximation fast informed bound umdp approximation np hard undecidable fixed strategy method grid interpolation extrapolation varies na nearest neighbor kernel regression linear point interpolation varies fixed interpolation best interpolation curve fitting squares fit varies na linear function na grid linear function method na incremental version na table complexity value function approximation methods finite horizon problem discounted infinite horizon problem 
objective discounted case find corresponding fixed point solution 
complexity results take account addition components pomdps approximation specific parameters size grid grid methods 
indicates open instances na methods applicable problems possible divergence 
contributions value function approximations pomdps surveys new known value function approximation methods solving pomdps 
focus primarily theoretical analysis comparison methods findings results supported experimentally problem moderate size agent navigation domain 
analyze methods different perspectives computational complexity capability bound optimal value function convergence properties iterative implementations quality derived controllers 
analysis includes new theoretical results deriving properties individual approximations relations exact methods 
general relations trade offs different methods understood 
provide new insights issues analyzing corresponding updates 
example showed differences exact mdp qmdp fast informed bound umdp methods boil simple mathematical manipulations subsequent effect value function approximation 
allowed determine relations different methods terms quality respective value functions main results 
number new methods heuristic refinements existing techniques 
primary contributions area include fast informed bound point interpolation methods including adaptive grid approaches stochastic sampling incremental linear function method 
showed instances solutions obtained efficiently converting original approximation equivalent finite state mdp 
example grid approximations convex rules solved conversion grid mdp grid points correspond new states leading polynomial complexity algorithm finite discounted infinite horizon cases section 
result dramatically improve run time performance grid approaches 
similar conversion equivalent finite state mdp allowing polynomial time solution discounted infinite horizon problem shown fast informed bound method section 
challenges directions pomdps approximations far complete 
complexity results remain open particular complexity grid approach seeking best interpolation complexity finding fixed point solution incremental version grid linear function method 
interesting issue needs investigation convergence value iteration squares approximation 
method unstable general case possible certain restrictions converge 
single pomdp problem maze support theoretical findings illustrate intuitions 
results supported theoretically related control generalized rank different methods performance may vary problems 
general area pomdps pomdp approximations suffers shortage larger scale experimental multiple problems different complexities broad range methods 
experimental especially needed study compare different methods regard control quality 
main reason theoretical results relating hauskrecht control performance 
studies help focus theoretical exploration discovering interesting cases possibly identifying classes problems certain approximations suitable 
preliminary experimental results show significant differences control performance different methods may suitable approximate control policies 
example grid nearest neighbor approach piecewise constant approximation typically inferior outperformed simpler efficient value function methods 
focused heuristic approximation methods 
investigated general flat pomdps take advantage additional structural refinements 
real world problems usually offer structure exploited devise new algorithms lead speed ups 
possible restricted versions pomdps additional structural assumptions solved approximated efficiently general complexity results pomdps approximations encouraging papadimitriou tsitsiklis littman mundhenk madani 
challenge identify models allow efficient solutions time interesting point application 
number interesting issues arise move problems large state action observation spaces 
complexity value function updates belief state updates issue 
general partial observability hidden process states allow factor decompose belief states updates transitions great deal structure represented compactly 
promising directions deal issues include various monte carlo approaches isard blake kanazawa koller russell doucet kearns methods approximating belief states decomposition boyen koller combination approaches mcallester singh 
anthony cassandra thomas dean leslie kaelbling william long peter szolovits anonymous reviewers provided valuable feedback comments 
research supported ro lm lm national library medicine dod advanced research project agency arpa contract number darpa rome labs planning initiative 
appendix theorems proofs convergence bound theorem value function mappings defined 
contractions fixed points 

isotone mapping 
holds 
value function approximations pomdps proof applying condition expanding result condition get repeating get limit proves result 
accuracy lookahead controller bounds theorem vu vl upper lower bounds optimal value function discounted infinite horizon problem 
sup vu vl vu vl maximum bound difference 
expected reward lookahead controller la constructed vu vl satisfies la proof denotes upper lower bound approximation la value function mapping corresponding lookahead policy note lookahead policy optimizes actions regard la hold 
error la bounded triangle inequality component satisfies la la 
la la la la la la la la la inequality follows fact upper lower bound 
rearranging inequalities obtain la bound second term trivial 
la mdp qmdp fast informed bounds theorem solution fast informed bound approximation solving mdp states actions discount factor 
proof linear function action defining vi 
denote parameters function 
parameters vi satisfy max 
max 
hauskrecht rewrite max max equations define mdp state space action space discount factor 
solution fast informed bound update solving equivalent finite state mdp 
theorem vi corresponds piecewise linear convex value function defined linear functions 
vi vi vi vi 
proof max max hvi max max max max vi max vi fixed strategy approximations max theorem fsm controller 
cdr cla direct step lookahead controllers constructed 
cdr cla hold belief states proof value function fsm controller satisfies maxv 
value function approximations pomdps direct controller cdr selects action greedily step chooses 
lookahead controller cla selects action step away la argmax max expanding value function step get maxv max max la 
max max la max la iteratively expanding maxx expression improved higher value expressions back obtain value functions direct lookahead controllers 
expansions lead value direct controller expansions value lookahead controller 
cdr cla hold 
note action choices la expressions different leading different step belief states subsequently different expansion sequences 
result imply dr la grid linear function method theorem vi value function obtained incremental linear function method starting corresponds fixed strategy 
cla cdr controllers vi lookahead controller direct action controller cla cdr respective value functions 
vi cla vi cdr hold 
proof initializing method value function fsm controller incremental updates interpreted additions new states fsm controller new linear function corresponds new state fsm 
ci controller step vi holds inequalities follow theorem 
hauskrecht astrom 

optimal control markov decision processes incomplete state estimation 
journal mathematical analysis applications 
baird 

residual algorithms reinforcement learning function approximation 
proceedings twelfth international conference machine learning pp 

barto bradtke singh 

learning act real time dynamic programming 
artificial intelligence 
bellman 

dynamic programming 
princeton university press princeton nj 
bertsekas 

counter example temporal differences learning 
neural computation 
bertsekas 

dynamic programming optimal control 
athena scientific 
bonet geffner 

learning sorting classification pomdps 
proceedings fifteenth international conference machine learning 
boutilier dean hanks 

decision theoretic planning structural assumptions computational leverage 
artificial intelligence 
boutilier poole 

exploiting structure policy construction 
proceedings thirteenth national conference artificial intelligence pp 

boyan moore 

generalization reinforcement learning safely approximating value function 
advances neural information processing systems 
mit press 
boyen koller 

tractable inference complex stochastic processes 
proceedings fourteenth conference uncertainty artificial intelligence pp 

boyen koller 

exploiting architecture dynamic systems 
proceedings sixteenth national conference artificial intelligence pp 

brafman 

heuristic variable grid solution method pomdps 
proceedings fourteenth national conference artificial intelligence pp 



complexity partially observed markov decision processes 
theoretical computer science 
cassandra 

exact approximate algorithms partially observable markov decision processes 
ph thesis brown university 
cassandra littman zhang 

incremental pruning simple fast exact algorithm partially observable markov decision processes 
proceedings thirteenth conference uncertainty artificial intelligence pp 

value function approximations pomdps casta 

approximate dynamic programming sensor management 
proceedings conference decision control 
cheng 

algorithms partially observable markov decision processes 
ph thesis university british columbia 
condon 

complexity stochastic games 
information computation 
dean kanazawa 

model reasoning persistence causation 
computational intelligence 
dearden boutilier 

abstraction approximate decision theoretic planning 
artificial intelligence 
doucet 

sequential simulation methods bayesian filtering 
tech 
rep cued infeng tr department engineering cambridge university 
drake 

observation markov process noisy channel 
ph thesis massachusetts institute technology 
draper hanks weld 

probabilistic planning information gathering contingent execution 
proceedings second international conference ai planning systems pp 

eagle 

optimal search moving target search path constrained 
operations research 


course triangulations differential equations deformations 
springer verlag berlin 
gordon 

stable function approximation dynamic programming 
proceedings twelfth international conference machine learning 
hansen 

improved policy iteration algorithm partially observable mdps 
advances neural information processing systems 
mit press 
hansen 

solving pomdps searching policy space 
proceedings fourteenth conference uncertainty artificial intelligence pp 

hauskrecht 

planning control stochastic domains imperfect information 
ph thesis massachusetts institute technology 
hauskrecht fraser 

planning medical therapy partially observable markov decision processes 
proceedings ninth international workshop principles diagnosis dx pp 

hauskrecht fraser 

planning treatment heart disease partially observable markov decision processes 
artificial intelligence medicine 
hauskrecht sobel 

stochastic methods operations research stochastic optimization 
mcgraw hill 
howard 

dynamic programming markov processes 
mit press cambridge 
howard matheson 

influence diagrams 
principles applications decision analysis 
isard blake 

contour tracking stochastic propagation conditional density 
proccedings conference computer vision pp 

kaelbling littman cassandra 

planning acting partially observable stochastic domains 
artificial intelligence 
kanazawa koller russell 

stochastic simulation algorithms dynamic probabilistic networks 
proceedings eleventh conference uncertainty artificial intelligence pp 

kearns mansour ng 

sparse sampling algorithm near optimal planning large markov decision processes 
proceedings sixteenth international joint conference artificial intelligence pp 

kjaerulff 

computational scheme reasoning dynamic probabilistic networks 
proceedings eighth conference uncertainty artificial intelligence pp 

korf 

depth iterative deepening optimal admissible tree search 
artificial intelligence 
kushmerick hanks weld 

algorithm probabilistic planning 
artificial intelligence 
lauritzen 

graphical models 
clarendon press 
littman 

memoryless policies theoretical limitations practical results 
cliff husbands meyer wilson 
eds animals animats proceedings third international conference simulation adaptive behavior 
mit press cambridge 
littman 

algorithms sequential decision making 
ph thesis brown university 
littman cassandra kaelbling 

learning policies partially observable environments scaling 
proceedings twelfth international conference machine learning pp 

lovejoy 

computationally feasible bounds partially observed markov decision processes 
operations research 
value function approximations pomdps lovejoy 

survey algorithmic methods partially observed markov decision processes 
annals operations research 
lovejoy 

suboptimal policies bounds parameter adaptive decision processes 
operations research 
goldsmith mundhenk 

results markov decision processes 
tech 
rep university kentucky 
madani hanks condon 

undecidability probabilistic planning infinite horizon partially observable markov decision processes 
proceedings sixteenth national conference artificial intelligence 
mcallester singh 

approximate planning factored pomdps belief state simplification 
proceedings fifteenth conference uncertainty artificial intelligence pp 

mccallum 

instance utile distinctions reinforcement learning hidden state 
proceedings twelfth international conference machine learning 
monahan 

survey partially observable markov decision processes theory models algorithms 
management science 
mundhenk goldsmith allender 

encyclopaedia complexity results finite horizon markov decision process problems 
tech 
rep cs dept tr university kentucky 
papadimitriou tsitsiklis 

complexity markov decision processes 
mathematics operations research 
parr russell 

approximating optimal policies partially observable stochastic domains 
proceedings fourteenth international joint conference artificial intelligence pp 

pearl 

probabilistic reasoning intelligent systems 
morgan kaufman 


finite memory estimation control finite probabilistic systems 
ph thesis massachusetts institute technology 


feasible computational approach infinite horizon markov decision problems 
tech 
rep georgia institute technology 
puterman 

markov decision processes discrete stochastic dynamic programming 
john wiley new york 
raiffa 

decision analysis 
introductory lectures choices uncertainty 
addison wesley 
rumelhart hinton williams 

learning internal representations error propagation 
parallel distributed processing pp 

hauskrecht lave 

markovian decision processes probabilistic observation states 
management science 
singh jaakkola jordan 

learning state estimation partially observable markovian decision processes 
proceedings eleventh international conference machine learning pp 

smallwood sondik 

optimal control partially observable processes finite horizon 
operations research 
sondik 

optimal control partially observable markov decision processes 
ph thesis stanford university 
sondik 

optimal control partially observable processes infinite horizon discounted costs 
operations research 


dynamic programming influence diagrams 
ieee transactions systems man cybernetics 
tsitsiklis roy 

feature methods large scale dynamic programming 
machine learning 
washington 

incremental markov model planning 
proceedings ieee international conference tools artificial intelligence pp 

white scherer 

finite memory suboptimal design partially observed markov decision processes 
operations research 
williams baird 

tight performance bounds greedy policies imperfect value functions 
proceedings tenth yale workshop adaptive learning systems yale university 
yost 

solution large scale allocation problems partially observable outcomes 
ph thesis naval postgraduate school monterey ca 
zhang lee 

planning partially observable markov decision processes advances exact solution method 
proceedings fourteenth conference uncertainty artificial intelligence pp 

zhang liu 

model approximation scheme planning partially observable stochastic domains 
journal artificial intelligence research 
zhang liu 

region approximations planning stochastic domains 
proceedings thirteenth conference uncertainty artificial intelligence pp 


