psychological review copyright american psychological association vol 
doi fei xu university british columbia word learning bayesian inference joshua tenenbaum massachusetts institute technology authors bayesian framework understanding adults children learn meanings words 
theory explains learners generalize meaningfully just positive examples novel word referents making rational inductive inferences integrate prior knowledge plausible word meanings statistical structure observed examples 
theory addresses shortcomings best known approaches modeling word learning deductive hypothesis elimination associative learning 
experiments adults children test bayesian account predictions context learning words object categories multiple levels taxonomic hierarchy 
results provide strong support bayesian account competing accounts terms quantitative model fits ability explain important qualitative phenomena 
extensions basic theory discussed illustrating broader potential bayesian models word learning 
keywords word learning bayesian inference concepts computational modeling learning simplest names object categories presents difficult induction problem quine 
consider typical dilemma faced child learning english 
observing competent adult speaker word dog max particular dalmatian running child infer meaning word dog 
potential hypotheses appear endless 
word refer dogs mammals animals individual max dogs plus lone ranger horse dogs spotted things running things front half dog dog parts things dogs observed monday cats observed 
despite severe authors contributed equally listed order birth 
research possible mitsubishi electric research labs paul newton chair joshua tenenbaum canada research chair national science foundation natural science engineering research council canada fei xu 
eliza sydney goddard assistance behavioral experiments 
dare baldwin paul bloom jeff elman jesse elizabeth helpful discussions nick chater susan carey terry regier valuable comments manuscript 
parents children participation 
portions annual conference cognitive science society august philadelphia pennsylvania july italy boston university conference language development november boston massachusetts november boston massachusetts biennial conference society research child development april minneapolis minnesota 
correspondence concerning article addressed fei xu department psychology university british columbia west mall vancouver british columbia canada joshua tenenbaum department brain cognitive sciences massachusetts institute technology massachusetts avenue cambridge ma 
mail fei psych ubc ca mit edu year olds remarkably successful learning meanings words examples 
particular children adults infer approximate extensions words dog relevant examples word systematic evidence words bloom carey markman regier 

broad classes proposals word learning works dominant literature hypothesis elimination associative learning 
hypothesis elimination approach learner effectively considers hypothesis space possible concepts words map leaving aside problem homonyms polysemy assumes word maps exactly concepts 
act learning consists eliminating incorrect hypotheses word meaning basis combination priori knowledge observations words refer aspects experience learner converges single consistent hypothesis 
logically possible hypotheses may ruled priori correspond natural concepts learner possesses example hypothesis dog refers things dogs observed monday cats observed 
hypotheses may ruled inconsistent examples word example rule hypothesis dog refers cats seeing example max dalmatian 
settling hypothesis eliminating incorrect amounts deductive approach logical problem word learning refer approaches deductive approaches 
hypothesis elimination roots early accounts human machine concept learning bruner austin mitchell corresponds standard paradigms considered formal analyses natural language syntax acquisition gold pinker 
related classic inferential frameworks considered philosophy science including popper induction mill bacon 
variants hypothesis elimination word learning literature 
pinker berwick siskind proposed particularly clear explicit formal models 
instance siskind efficient algorithm keeping track just necessary possible components word meaning hypotheses consistent set examples 
research word learning precise formal models easy identify inference framework guiding research 
researchers speak process eliminating ruling hypotheses word meaning pinker tracking minimal set necessary sufficient meaning components siskind take appealing kind deductive model implicitly 
way thinking word learning serves foundation substantive proposals children bring prior knowledge bear inference problem carey clark markman 
main alternatives hypothesis elimination form associative learning connectionist networks smith gasser smith regier smith similarity matching examples landau smith jones roy pentland 
internal layers hidden units appropriately designed input output representations appropriately tuned similarity metrics models able produce generalizations word meaning go simplest form direct percept word associations 
boundary hypothesis elimination approaches associative learning approaches clear 
instance siskind kept track frequencies specific words world contexts associated support rejection noise construction lexical entries 
hypothesis elimination associative learning models offer certain important insights argue approach provides adequate framework explaining people learn meanings words 
consider core phenomena highlighted literature years bloom carey smith markman regier siskind tomasello model word learning account 
word meanings learned examples 
reasonable guess just single example examples may sufficient right contexts home meaning high accuracy 

word meanings inferred positive examples examples word refers 
negative examples examples word refer may helpful necessary learner reasonable guess word meaning 

word meanings carve world complex ways entity action property relation typically labeled multiple words 
target word learning simply single partition world mutually exclusive categories xu tenenbaum word category system overlapping concepts distinct linguistic label 

inferences word meanings examples may graded varying degrees confidence reflecting level ambiguity noise learner experience 

inferences word meanings strongly affected pragmatic intentional reasoning observed examples generated relevant communicative context 
mean suggest phenomena apply case word learning pervasive central importance 
illustrate severe challenges word learning poses computational problem solved powerful inferential capacities children able bring bear solution 
satisfying framework modeling word learning natural explanations phenomena 
explain traditional approaches hypothesis elimination associative learning general best approach captures subset phenomena 
main goal article propose new approach understanding word learning principles rational statistical inference 
framework combines principal advantages deductive associative frameworks going major limitations 
key innovation bayesian inference framework 
hypotheses word meanings evaluated machinery bayesian probability theory deductive logic hypotheses simply ruled scored probability correct 
interaction bayesian inference principles appropriately structured hypothesis spaces explain core phenomena listed 
learners rationally infer meanings words label multiple overlapping concepts just positive examples 
inferences ambiguous patterns data lead graded uncertain patterns generalization 
pragmatic inferences communicative context affect generalizations word meanings changing learner probabilistic models 
plan article follows 
pointing specific difficulties faced standard approaches word learning 
core article develop bayesian framework context particular case study learning common nouns object categories animal dog main ingredients computational important approach modeling acquisition word meaning broadly associative learning tradition discuss focuses different learning task complementary focus learning word meanings examples 
problem learning associative contexts words observing words tend conversation writing 
statistical approaches look clusters words occurring similar contexts griffiths steyvers tenenbaum chater finch latent space best explains patterns word occurrence landauer dumais produced intriguing results 
interest see approaches profitably combined approaches discuss yield models word learning draws observed examples patterns linguistic usage scope 
model principles bayesian framework providing explanation key phenomena learning overlapping extensions 
predictions model tested experiments adult child learners demonstrating importance inferences context 
inferences may provide means people acquire words concepts multiple levels object kind hierarchy subordinate basic superordinate traditionally considered critical challenge early word learning markman waxman 
quantitative fit model data 
show bayesian framework potentially address difficulties faced standard approaches consider challenges facing general framework word learning 
evaluating traditional approaches case object kind labels describe bayesian approach experimental tests helpful give concrete illustrations core phenomena word learning listed explain traditional deductive associative accounts phenomena difficulties facing 
return opening question child infer meaning common noun labels object kind word dog 
numerous studies shown children reasonable guesses word meanings single labeling event 
instance markman hutchinson taught year olds new word fep familiar object german shepherd showed children preferred generalize new labels taxonomically similar objects poodle thematically matching object bone 
markman year olds interpreted novel word referring object opposed salient part object 
landau 
showed year olds preferred generalize category labels objects matching shape texture size color 
ability learn words exposures may children young months woodward markman 
rapid inferences restricted object kind labels 
fast mapping study carey bartlett adult pointed children colored prototypical blue colored unusual olive green 
adult asked bring tray blue children correct inference referred olive green color experiences sort half remembered word referent pairing weeks 
furthermore markman showed ability linguistic contrast infer word meanings applied semantic domains shape texture 
sum ability infer important aspects word meaning just single positive example referred phenomena children young years age 
children inferences word meanings sparse data 
influential proposal hypothesis elimination paradigm people come task word learning equipped strong prior knowledge kinds viable word meanings carey clark mark word learning bayesian inference man allowing rule priori logically possible unnatural extensions word 
classic constraints meanings common nouns object constraint taxonomic constraint markman 
object constraint requires words refer objects opposed parts objects attributes objects ruling word meanings front half dog dog parts 
taxonomic constraint requires words refer taxonomic classes typically tree structured hierarchy natural kind categories 
example dog taxonomic assumption rule subsets spotted things running things dogs plus lone ranger horse dogs 
cases example child learning word dog constraints useful sufficient solve inference problem 
ruling hypotheses inconsistent typical labeled example max dalmatian learner left consistent hypotheses correspond possible meanings common nouns 
infer word perceived refer max applies dogs mammals animals 
problem inference hierarchical taxonomy interesting right importantly special case fundamental challenge problem learning overlapping hypotheses phenomenon list 
interesting semantic domains natural concepts named lexicon mutually exclusive overlap structured way 
single example new word typically fall multiple categories insufficient fix class word 
example learning overlapping hypotheses arises contexts multiple dimensions object shape material composition relevant simultaneously 
consider words apply objects furniture store kinds objects table chair shelf vase kinds solid substances wood metal plastic stone 
kinds objects tend fairly reliable perceptual correlate shared shape kinds substances refer material object 
adults children capable learning word meanings orthogonal pattern overlap small number examples mintz gleitman 
article discuss case detail 
markman suggested solution dealing overlapping hypotheses case object categories people may assume new common nouns map just level taxonomy preferentially basic level categorization rosch mervis gray johnson 
categories category dogs clusters intermediate size maximize different indices category utility relative smaller subordinate categories larger superordinate categories contain animals 
children really preference map words basic level kinds controversial mccarthy preference exist clear enable names basic level categories learned seeing just single typical labeled example 
context learning words kinds objects kinds substances 
extensions words label object kind categories may overlap nested fashion accord tree structured hierarchy object kind taxonomy 
landau 
analogous suggestion children preferentially map words shape object categories simple regular shapes imai gentner carey 
assuming biases map words basic level object categories constraints mentioned appears explain hypothesis elimination learner learn word meanings just single positive example object belongs just category 
solution works basic level object labels dog fact counterproductive kinds words 
learn words know categories superordinate subordinate levels substance concepts 
admitting kind soft combination constraints reasonable alternative offered precise account biases interact observed examples novel word order support meaningful generalizations just examples 
sense goal article 
prospects better associative learning accounts trying explain word learning just positive examples 
surface associative learning suited explaining kind rapid inference typically conceived gradual process accumulating associative strength experiences 
classic xu tenenbaum models language acquisition show enduring fast mapping plunkett sinha moller potential catastrophic interference 
models tried account rapid inferences word meaning smith regier combination exemplar representations attentional learning 
clear associative models solve problem overlapping extensions 
standard mechanism associative models presence implicit negative evidence models implicitly assume positive example word negative example word 
precisely issue concerning overlapping extensions 
attempt address problem regier 
described neural network learning algorithm capable learning overlapping words positive evidence weakened form mutual exclusivity gradually strengthened thousands learning trials 
model address phenomenon learning examples 
class models li macwhinney macwhinney merriman uses competition outputs implement idea implicit negative evidence 
simple mechanism competition embodied models designed explain children learn multiple words apply single object 
associative models word learning focused idea tuning attentional biases 
example shape bias object labels result shifting attention shape opposed material set training exemplars smith 
model regier acquires attentional biases meaning space form space enable learning system form meaning mappings 
problem learning words overlapping extensions persists learners acquire words subordinate superordinate level words categories shape just examples 
models tried address question clear 
argue essential problem learning overlapping word meanings sparse positive examples solved bayesian approach word learning 
relative traditional approaches approach posits powerful framework combining prior knowledge observed examples word referents 
focus set phenomena context learning words taxonomic categories strongly suggest inferential mechanism sort 
illustrate learning problem introduced earlier observing max dalmatian labeled fep learner guided taxonomic hypothesis space preference labeling basic level categories reasonably guess fep refers dogs 
suppose learner observes objects labeled dalmatian 
additional examples consistent exactly set taxonomic hypotheses consistent example potential meanings word learning bayesian inference 
extensions words label object shape substance categories may overlap crosscutting fashion dimensions object appearance approximately independent 
ruled inconsistent inconsistent seeing dalmatian called fep 
additional examples word fep relatively refer just dogs 
intuitively inference appears suspicious coincidence quite surprising observe called fact word referred dogs examples random sample world 
intuition captured bayesian inference mechanism scores alternative hypotheses word meaning predict observed data fit learner prior expectations natural meanings 
intuitive sensitivity sorts suspicious coincidences core capacity enabling rapid word learning argue best explained bayesian framework 
previous studies shown multiple examples help children learn subordinate superordinate kind labels liu sak waxman adjectives mintz gleitman 
instance showing dog horse cow examples animals provides better evidence showing just cow showing differently shaped objects characteristic texture examples word texture provides better evidence showing just single object 
intuitively additional examples help cases ruling compelling alternative hypotheses formal account learning key part siskind hypothesis elimination model word learning 
siskind hypothesis elimination approach explain phenomenon learn ing suspicious coincidence hypotheses eliminated additional examples 
phenomenon poses challenge associative learning approaches word learning 
dalmatian labeled times fep labeled fep correlation appearance dog features word fep exactly correlation appearance dalmatian features word fep cases 
associative models attempt infer word meaning correlations perceptual feature clusters labeling events get inductive leverage coincidence 
see effects multiple examples reveal inductive logic word learning consider learner beliefs meaning fep changed examples golden retriever poodle 
presumably learner confident fep fact refers dogs relative initial belief single example dalmatian called fep 
inference meaning qualitatively similar example examples different subordinate classes confident case 
shift confidence suggests initial inference example simply due application defeasible constraint ruling basic level hypothesis dogs additional examples tell 
plausible interpretation say example probability word mapped subordinate category 
subsequent evidence weighs overly specific hypothesis shifts corresponding weight belief basic level hypothesis dogs 
experiments described show children adults behave agreement picture increase tendency generalize basic level multiple examples spanning basic level hypothesis relative base preference just single example 
explanation behavior restriction generalization examples spanning subordinate category discussed inferences word meaning purely hypothesis elimination subject hard defeasible constraints 
reflect kind statistical inference may sharper focused additional consistent data observed 
show patterns inference suspicious coincidence captured statistical framework bayesian inference 
contrast hypothesis elimination approaches hypotheses just ruled 
probability alternative hypothesis evaluated 
contrast associative learning approaches statistical information just come correlations words referents 
inference mechanism sensitive examples generated may disregard outliers uninformative examples 
bayesian framework explain general graded character generalization word learning phenomenon causes difficulties hypothesis elimination approaches particular 
learner seen just single example typically confident generalizing word new instances learner seen consistent examples 
anomalous examples may discounted outliers xu tenenbaum full weight revising learners hypotheses markman 
phenomenon word learning particularly central studies role intentional pragmatic inferences phenomenon 
phenomena pose challenge approaches word learning particularly associative tradition 
advocates tradition suggested regier mechanisms simple associative learning necessary account social factors word learning 
example baldwin colleagues baldwin baldwin markman bill desjardins irwin showed months age children speaker eye gaze joint attention infer object speaker referring 
speaker looks bucket says look fep child looking object child track speaker gaze interpret word referring object inside bucket 
consider study tomasello barton 
adult said children doll executed action followed exclamation oops second action followed exclamation children correctly inferred referred second action emotional expression experimenter 
furthermore word learning constraints mutual exclusivity markman reinterpreted pragmatic constraints 
words example new word just raw data point entered blindly matrix word object cooccurrences potentially rich communicative experience explained inference word meaning 
somewhat controversial fact intentional reasoning versus basic attentional cuing smith guides children solving word learning problems agree deciphering speaker communicative intent important component word learning 
clear children inferences cast naturally framework hypothesis elimination chains pragmatic deductions adult intended refer clear associative learning framework 
bayesian framework address inferences terms sensitivity sampling process give example capacity article 
sum traditional approaches hypothesis elimination associative learning account critical aspects word learning identified 
contrast bayesian framework propose potentially handle phenomena 
rest article lay model detail provide empirical evidence adults year old children learning words different levels taxonomic hierarchy discuss extensions implications framework 
bayesian framework model formulated bayesian framework concept learning generalization introduced tenenbaum colleagues tenenbaum tenenbaum griffiths tenenbaum griffiths kemp 
framework aims explain inductive learning level computational theory marr rational analysis anderson chater understand functional terms implicit knowledge inferential machinery guide people generalizing examples describe precisely psychological processes involved 
focus restricted problem learning single novel word examples discuss framework principle extends general problem learning lexicon large corpus experience 
denote set observed examples novel word examples drawn known domain entities assume learner access hypothesis space possible concepts probabilistic model relating hypotheses data hypothesis thought pointer subset entities domain candidate extension assume learner identify extension hypothesis entities fall 
generally hypotheses represent candidate intensions simplifying assumption intension yields unique extension version clark contrast principle focus learners infer word extension 
examples bayesian learner evaluates hypotheses candidate word meanings bayes rule computing posterior probabilities proportional product prior probabilities likelihoods 
prior including hypothesis space embodies learner expectations plausible meanings word independent examples observed 
priors may reflect conceptual lexical constraints expectations different kinds words different contexts beliefs conditional meanings previously learned words 
may innate acquired 
taxonomic constraint basic level bias incorporated naturally term 
likelihood captures statistical information inherent examples reflects expectations entities observed examples particular hypothesis meaning default assumption examples observed representative sample concept learned 
likelihood may sensitive data syntactic context examples examples words contrast 
consider possibilities article 
posterior reflects learner degree belief fact true meaning combination observations prior knowledge plausible word meanings 
proportional product likelihood prior hypothesis relative corresponding products hypotheses 
form embodies principle conservation rational belief learner believes strongly particular hypothesis meaning word learned assigns value near learner necessarily believe strongly hypotheses pick word learning bayesian inference true meaning assign values near probabilities priors likelihoods posteriors implicitly conditioned knowledge base include meanings previously learned words principles possible word meanings words tend examples typically generated 
article consider general analyses likelihoods prior probabilities change incorporate different aspects learner background knowledge 
main model done specifying likelihoods priors enter bayes rule 
considering components note additional piece machinery needed relate learner beliefs word meaning encoded generalization behavior 
learner needs way decide new object belongs extension observations learner completely sure word meaning exactly generalization trivial applies new objects 
generally learner compute probability generalization averaging predictions hypotheses weighted posterior probabilities 
evaluate equation note simply examples contained generalization probability written sum posterior probabilities hypotheses contain new object old examples tenenbaum griffiths interpret hypotheses features feature bundles define intensions hypotheses posterior probabilities feature weights equation captures intuition generalization increase proportion number weight features common classic models similarity judgment tversky shepard arabie 
hypothesis sharply picks subset entities equation produce essentially generalization posterior probability concentrates mass single hypothesis 
hypothesis space generally hypothesis space simply set hypotheses meaning novel word hypothesis points subset entities domain candidate extension purposes bayesian inference hypotheses need structured related particular way 
may simply set mutually exclusive exhaustive candidate word extensions carrying assumption word learned maps subsets world 
strong theoretical reasons practical motives typically assume structured hypothesis space 
figures show examples hypothesis spaces different large scale structures tree structured taxonomy object kinds hypotheses nested orthogonal dimensional matrix object substance categories hypotheses different dimensions overlap 
explained structured hypothesis space thought important component learner prior important component supports successful learning examples 
place candidate word learning principles enter analysis 
practically speaking assuming appropriately structured hypothesis space allow space constructed fairly automatic fashion independent behavioral data collect participants 
assuming structure hypothesis space force modelers specify hypothesis associated prior probability hand heit leading proliferation free parameters model 
model learning object kind labels assume hypothesis space corresponds taxonomy nested categories constructed automatically hierarchical clustering average linkage duda hart human participants similarity ratings see article 
hypothesis corresponds cluster tree 
emphasize intuitive taxonomy intended simple tractable approximation hypothesis space people adopt learning common object labels intended source object label hypotheses represent structure hypothesis spaces learning kinds words 
probabilistic components model kinds probabilities prior probabilities likelihoods defined hypothesis space candidate word meanings 
describe general character probabilities saving details computed applying model section model evaluation experimental sections 
likelihoods 
likelihood function comes assuming observed positive examples sampled random independently true concept learned 
consider hypothesis word extension picks finite set objects 
likelihood picking object random set size objects sampled replacement reasoning leads likelihood function size 
refer equation size principle scoring hypotheses hypotheses smaller extensions assign greater probability larger hypotheses data assign exponentially greater probability number consistent examples increases 
captures intuition dalmatian example fep dogs fairly plausible hypotheses word extension xu tenenbaum examples fep word refer dogs likelihood ratio hypotheses inversely proportional ratio sizes raised fourth power 
size principle explains learner observes examples fep happen tend infer word refers dogs hypotheses logically consistent examples encountered 
intuitively inference noticing suspicious coincidence formally reflects preference hypotheses data observed 
proposal addresses crucial shortcoming traditional deductive hypothesis elimination approaches word learning explain inferences may change encountering falsifying examples 
addresses analogous shortcoming associative approaches explain feature may preferred basis word meaning features equally correlated observed usage word 
rationality size principle depends widely applicable assumption randomly sampled examples defeasible learner confronted examples sampled importantly different ways 
size principle viewed softer statistical version subset principle berwick wexler classic deductive approach learning positive examples formal models language acquisition 
discuss connection bayesian learning subset principle detail compare alternative models experimental data 
priors 
generally prior reflect people implicit knowledge words map meanings meanings tend different contexts 
important component prior simply qualitative structure hypothesis space assumption hypotheses correspond nodes tree structured taxonomy 
assumption equivalent assigning zero prior probability vast majority logically possible hypotheses subsets objects world conform particular taxonomy 
tree structured hypothesis space necessary bayesian approach rational statistical learner interesting generalizations adopting bias assigns zero near zero prior probability logically possible hypotheses 
see consider bayesian learner assigned equal priors logically possible hypotheses subsets entities domain 
size principle likelihood function best hypothesis set examples containing just objects hypothesis calls generalization 
generalization word learning kind inductive learning possible prior concentrates mass relatively small number hypotheses 
fine grained quantitative differences prior probability necessary explain particular patterns generalization people different patterns shown different groups learners adults versus children experts versus novices 
important kind graded prior knowledge word learning may preference labeling distinctive clusters distinctive clusters priori distinguishing names 
learning common nouns paramount goal acquire linguistic handles natural kind categories 
perceptual distinctiveness cluster ready indicator cluster correspond natural kind 
distinctiveness perceptual conceptual may important right utility stability word depend part easily speakers pick entities extension 
expect kind preference distinctiveness general aspect prior probabilities word learning 
summary basic modeling framework 
priors likelihoods understood terms combination explain people successfully learn extensions new words just positive examples 
successful word learning requires constrained space candidate hypotheses provided prior ability reweight hypotheses explain set observed examples provided likelihood 
constraints imposed prior meaningful generalizations possible 
likelihood learned multiple examples simply eliminating inconsistent hypotheses 
particular priors likelihoods contribute directly main pattern generalization described look experiments just single example novel kind label generalization objects graded examples learners apply word generalizing members specific natural concept spans observed examples 
prior determines concepts count natural likelihood generates specificity preference determines strength preference sharpness generalization increases function number examples 
need strong prior knowledge constrain word learning major theme previous research tradition bloom markman pinker 
importance statistical learning multiple examples word object pairings stressed associative learning approaches smith regier 
thesis successful word learning depends prior knowledge statistical inference critically interaction 
theoretical framework understanding interaction functions support rational generalization positive examples 
turn series empirical studies mapping adults children generalize words examples followed quantitative comparisons judgments generalization patterns bayesian model 
experiment experiment tested adults word learning situation 
experiment consisted phases word learning phase similarity judgment phase 
word learning phase adults taught novel words asked generalize word objects 
variables manipulated number examples vs range spanned examples green peppers different colored peppers different kinds vegetables 
word learning bayesian inference similarity judgment phase participants asked give similarity ratings pairs objects phase 
similarity judgments yield hypothesis space subsequent computational modeling 
predictions adults show graded generalization example generalizations examples 
furthermore depending span examples expected adults generalize specific category consistent examples 
method participants 
participants students massachusetts institute technology stanford university participating pay course credit 
participants carried word learning task participated similarity judgment phase followed 
participants native speakers english normal corrected normal vision 
materials 
stimuli digital color photographs real objects 
distributed different superordinate categories animals vegetables vehicles different basic level subordinate level categories 
stimuli divided training set stimuli test set stimuli 
twelve sets labeled examples training stimuli experiment 
sets contained example dalmatian green pepper yellow truck 
sets contained examples objects single example sets dalmatian new objects matched subordinate level different postures basic level mutt superordinate level pig 
sets arose combination objects example set crossed levels matching specificity 
objects test set shown 
objects distributed superordinate level categories animals vegetables vehicles 
set constructed provide matches levels subordinate basic dogs superordinate animals cat bee seal bear objects vegetables vehicles 
note test set exactly trials set exemplars contained total subordinate level matches basic level matches dogs superordinate level matches dogs animals distractors objects 
chose include basic level superordinate level matches categories members real world actual ratio rough estimate size categories 
design procedure 
phase experiment word learning task 
stimuli 

square window color computer monitor normal viewing distance 
participants told helping puppet frog speaks different language pick objects wants 
trial participants shown pictures labeled examples novel monosyllabic word fep asked pick test set objects clicking screen computer mouse 
test items laid array order randomly permuted trial trial 
experiment began participants shown test objects time seconds familiarize stimuli 
followed instructions experimental trials 
participants additional set trials reported 
trials participants saw example new word fep 
trials saw examples new word 
set trials example sets appeared pseudorandom order content domain animal vegetable vehicle specificity subordinate basic superordinate counterbalanced participants 
trial participants asked choose objects word applied responses recorded 
phase approximately min total 
second phase experiment similarity judgment task 
participants shown pictures pairs objects word learning task asked rate similarity objects scale similar extremely similar 
instructed base ratings aspects objects important making choices word learning phase 
instruction placement similarity judgment task word learning task adopted hope maximizing information similarity judgments provide hypothesis space participants word learning 
similarity judgments took xu tenenbaum 
twelve training sets labeled objects experiment drawn domains animals vegetables vehicles test conditions example subordinate examples basic level examples superordinate examples 
circled number underneath object index object location hierarchical clustering shown 
approximately min collect 
judgments collected pairs objects domain animals vegetables vehicles including test objects training objects omitted save time 
omitted objects green peppers yellow trucks practically identical included objects treated identical constructing model learning reported 
participant rated similarity pairs animals vegetables vehicles judgments third possible cross superordinate pairs animal vegetable vegetable vehicle chosen judgments total judgments participant 
order trials order stimuli randomized participants 
trials preceded practice trials chosen randomly stimuli participants range similarities encounter encouraged develop consistent way rating scale 
encouraged entire scale spread judgments evenly scale 
ratings recorded average rating pair objects computed 
results main results experiment shown 
adults clearly differentiated example example trials sensitive span examples 
example adults showed graded generalization subordinate basic level superordinate matches 
generalization 
test set objects probe generalization word meanings experiment 
training set test set contains subordinate matches basic level matches superordinate matches 
circled number underneath object index object location hierarchical clustering shown 
ents dropped steeply basic level soft threshold test items basic level category chosen relatively superordinate matches chosen 
examples adults generalizations sharpened pattern 
generalizations examples restricted specific level consistent examples instance examples adults generalized different dogs different animals adults generalized dogs animals 
overview mind turn statistical analyses quantify effects 
formal computational model word learning task compare 
adults generalization word meanings experiment averaged domain 
results shown types example set example subordinate sub 
examples basic level examples superordinate super 
examples 
bar height indicates frequency participants generalized new objects various levels 
error bars indicate standard errors 
word learning bayesian inference data experiment quantitative detail 
analyses section tailed tests planned comparisons model predictions 
data collapsed different superordinate categories different test items level generalization subordinate basic superordinate 
kinds example sets subordinate basic level superordinate levels generalization participant received set percentage scores measuring chosen test items level generalization kind example set 
means scores participants shown 
participants time chose distractors test items outside example superordinate category subsequent analyses include scores 
questions addressed planned tests 
participants generalize example trials compared example subordinate trials versus virtually identical exemplars 
specifically adults show significant threshold generalization basic level example trials restrict generalization subordinate level example trials 
second example trials differ depending range spanned examples 
specifically participants restrict generalization specific level consistent set exemplars 
investigate question compared percentages responses matched example subordinate basic superordinate levels 
example trials participants chose subordinate basic level matches superordinate matches difference levels smaller difference levels 
contrast similar exemplars subordinate category participants chose subordinate matches basic level superordinate matches comparisons 
similar comparisons example basic level superordinate level examples 
examples basic level category participants generalized basic level compared example trials vs 
examples superordinate category participants generalized exemplars superordinate category 
model predicts discussed examples spanning single subordinate level category generalization gradient relate example trials follows equal generalization subordinate level large decrease generalization basic level small decrease generalization superordinate level 
examples spanning category generalization gradient modified follows equal generalization subordinate level increase basic level generalization small decrease superordinate level generalization 
examples spanning superordinate level category generalization function modified follows equal generalization subordinate level increases basic level superordinate level generalization 
predictions follow model general tendency relatively uncertain correct level generalization example certain examples 
set tailed tests conducted test predictions comparing mean percentages relevant pairs conditions adults generalizations 
results tests consistent model predictions difference superordinate generalization example basic level examples conditions 
difference predicted direction predicted small result surprising 
investigate second question tested series specific predictions model discussed generalizations examples certain level specificity differ 
set planned comparisons addressed question comparing percentages response level 
examples category model predicts sharp drop subordinate level generalization basic level generalization vs 
examples basic level category model predicts sharp drop basic level generalization superordinate level generalization vs 
examples superordinate category model predicts generalization include exemplars superordinate category ns 
similarity data analyzed article describe fits bayesian learning model 
similarities construct model hypothesis space 
xu tenenbaum discussion adults clearly generalized differently example example trials 
example showed graded generalization subordinate basic level superordinate matches 
addition adults showed basic level preference generalized exemplars basic level category generalized superordinate category 
examples adults generalizations manner 
restricted generalizations specific level consistent examples 
experiment experiment investigated year old children learn words subordinate basic level superordinate categories 
children taught novel words object categories asked generalize words new objects 
experiment factors manipulated number examples labeled vs range spanned examples kinds dogs kinds animals 
method participants 
participants year old children mean age years month ranging years months years months approximately evenly divided gender 
participants recruited greater boston area mail subsequent phone calls 
children middle class non hispanic white asian african american hispanic 
children received token gift study 
children excluded play game experimenter 
english primary language spoken home children 
materials 
stimuli objects experiment children real toy objects opposed photographs 
design procedure 
child randomly assigned conditions example condition condition 
children example condition saw example word children condition saw examples word 
child participated total trials superordinate categories 
trial example condition examples spanned different level generality subordinate basic level superordinate 
children introduced puppet frog told helping puppet speaks different language pick objects wants 
test array objects randomly laid front child experimenter 
experiment began dialogue follows 
experimenter held puppet said child friend frog 
say hello frog 
child says hello frog toys play game 
play game frog 
child says 
frog speaks different language different names toys 
going pick help pick ones picked okay 
child says okay novel words fep dax 
example condition 
trial experimenter picked object array green pepper labeled see 
child told frog 
experimenter said child frog wants pick toys doesn want 
remember frog wants 
pick toys child allowed choose test objects find put front frog 
child picked object experimenter reminded remember frog wants 
child picked object said encourage pick toys 
trial experimenter said child put back play game 
frog going pick toys help pick ones picks okay novel word introduced 
child participated trials example drawn superordinate categories dalmatian animal green pepper vegetable yellow truck vehicle 
order trials novel words fep dax counterbalanced participants 
example condition 
trial procedure example trial important difference 
experimenter picked object labeled child see 
fep 
picked objects time labeled child look fep look fep 
factors superordinate category animal vegetable vehicle range spanned examples subordinate basic superordinate novel word fep dax crossed counterbalanced participants 
level factor appeared equally second third trials experiment 
results patterns generalization qualitatively similar adults experiment quantitative word learning bayesian inference analyses followed essentially logic 
analyses tailed tests planned comparisons 
collapsed superordinate categories novel words trial orders 
type example set children shown received set percentage scores measuring chosen test items levels generalization subordinate basic superordinate 
means scores participants shown 
children example condition received just single set scores trials featured kind example set 
children condition received sets scores trial trial featured different kind example set examples clustering subordinate basic superordinate level 
child chose distractors subsequent analyses include scores 
questions experiment addressed planned tests 
children generalize differently example trials compared example trials case 
importance generalize differently versus virtually identical exemplars 
specifically children show significant threshold generalization basic level example trials restrict generalization subordinate level example trials 
second example trials differ depending range spanned examples 
specifically children restrict generalization specific level consistent set exemplars 
investigate question compared percentages responses matched example subordinate basic superordinate levels 
example trials participants chose subordinate basic level matches superordinate matches comparisons 
contrast similar exemplars subordinate category participants chose subordinate matches basic level superordinate matches comparisons 
similar comparisons example basic level superordinate level examples 
examples basic level category participants generalize basic level compared trials vs ns 
examples superordinate category participants gen 
children generalization word meanings experiments averaged domain 
results shown types example set example subordinate sub 
examples basic level examples superordinate super 
examples 
bar height indicates frequency participants generalized new objects various levels 
error bars indicate standard errors 
basic level superordinate level vs vs 
investigate second question tested series predictions model experiment 
set planned comparisons addressed question comparing percentages response level 
examples subordinate level category model predicts sharp drop subordinate level generalization basic level generalization vs 
examples basic level category model predicts sharp drop basic level generalization superordinate level generalization vs 
examples superordinate level category model predicts generalization include exemplars superordinate category 
children performance broad agreement predictions 
discussion year old children performance broad agreement predictions 
example trials showed graded generalization 
interesting note show strong basic level preference 
example trials children modified generalizations depending span examples 
generalizations consistent specific category included examples 
children data noisier adults 
methodological reasons may account differences 
level response lower children 
task freely choosing objects demanding children age may reluctant choose objects 
subordinate classes equally salient interesting children 
class unusually interesting children distinctive coloration fact came line toys currently popular children animated feature film 
green peppers class salient children differed members basic level class color coloration nearly striking 
experiment children objects ask response 
modification meant ensure children provide judgment test objects 
changed subordinate classes slightly order equalize salience subordinates animals vegetables vehicles 
critical prediction bayesian framework concerned learner generalization function differed labeling single example versus independent examples 
object labeled trials contained times labeling events example trials 
able tell learner kept track number independent examples labeled simply number labeling events word object pairings 
particularly important associative models smith regier suggested children word learning built keeping track occurrences words object percepts 
distinguish xu tenenbaum bayesian approach conventional associative approaches important tease apart possibilities 
study equate number labeling events example example conditions labeling single example object example condition times example object example condition labeled just 
experiment experiment sought replicate extend results experiment slight modifications stimuli important methodological changes 
equated number labeling events example example conditions 
second letting children choose target objects experimenter chose objects asked child judgment case 
method participants 
participants year old children mean age years months ranging years months years months approximately evenly divided gender 
participants recruited experiment 
children middle class non hispanic white asian african american hispanic 
materials 
stimuli objects experiment replaced green peppers replaced peppers 
members subordinate class distinguished objects basic level class shape color features moderate salience 
design procedure 
procedure identical experiment 
example condition object labeled times 
example experimenter pick green pepper show child say see 
fep put pepper floor pick saying look fep put pick third time saying fep experimenter sure child actions clear pepper labeled times 
example condition object labeled exactly 
experimenter monitored child attention ensure joint attention established labeling event object 
test objects laid front child experimenter chose objects ask 
experimenter picked objects asked child fep target set included subordinate level matches basic level matches superordinate level matches distractors 
results main results experiment shown 
significance level statistical analyses 
preliminary analyses effects gender order domain order target type 
subsequent analyses collapsed variables 
children chose distractors experiment trial analyses excluded distractor scores 
questions experiments addressed planned tests 
children behave differently example trials compared example trials 
importance generalize differently versus virtually identical exemplars 
second example trials differ depending span examples 
investigate question compared percentages responses matched example subordinate basic superordinate levels 
example trials participants chose subordinate basic level matches superordinate matches comparisons 
contrast similar exemplars subordinate category participants chose subordinate matches basic level superordinate matches comparisons 
similar comparisons example basic level superordinate level examples 
examples basic level category participants generalized basic level compared example trials vs 
examples superordinate level category participants generalized basic level superordinate level vs vs 
investigate second question tested series predictions model experiments 
seen modifications methodology children performance consistent predictions 
examples subordinate level category model predicts sharp drop subordinate level generalization basic level generalization vs 
examples basic level category model predicts sharp drop basic level generalization superordinate level generalization vs 
examples superordinate category model predicts generalization include exemplars superordinate category 
discussion simplified testing procedure children generalized new words ways looked adults experiment 
showed lower tendency basic level generalization single example suggests adults strong tendency shot basic level generalization may reflect convention acquired extensive experience learning words 
differences generalization example example conditions experiment persisted stronger number labeling events equated conditions 
finding suggests children statistical inferences word meanings computed number examples labeled just number word object pairings 
word learning bayesian inference discussion experiments order test specific predictions bayesian framework experiments investigated effects number examples vs span examples participants subordinate vs basic vs superordinate level number labeling events object labeled times vs objects labeled 
tested adults children 
experimental design features sheds new light process word learning 
varying number examples able examine effects multiple examples generalization 
word learning displays characteristics statistical inference adult child learners accurate confident generalizations number examples increased 
effect typical gradual learning curve associated statistical learning 
strong shift generalization behavior examples reflecting rational statistical principle observing span independent randomly sampled examples warrants sharp increase confidence hypothesis generalization correct 
adult child learners appear sensitive suspicious coincidences examples novel word appear cluster taxonomy candidate categories named 
varying span examples labels subordinate superordinate categories may difficult children learn suggested previous studies 
multiple examples children able learn words refer different levels taxonomic hierarchy superordinate categories animal vehicle vegetable 
special linguistic cues negative examples necessary learning words 
varying number labeling events independent number examples able explore ontological underpinning children word learning 
evidence children keep track number instances labeled simply number occurrences object percepts labels 
word learning appears fundamentally statistical inference standard associative models statistics computed ontology objects classes surface perceptual features 
interesting difference adults children extend novel words example instances basic level category adults showed greater basic level generalization children 
consistent finding children show robust basic level generalization taught unfamiliar words 
results broadly consistent suggest basic level bias may part foundations word learning 
bias may develop children learn general patterns word meanings words tend 
research broader range categories experimental paradigm developed necessary establish case developmental proposal 
research supports notion basic level bias develops experience expect development modeled instance bayesian learning people come realize basic level object labels frequently subordinate superordinate labels 
important note caveats 
argued experiments children learned words categories multiple levels taxonomic hierarchy superordinate basic subordinate open question children understand categories part hierarchically organized system kinds 
previous studies include explicit test child understanding class inclusion relations taken ultimate test understanding hierarchical structures 
smith asked year old children inference questions class inclusion year olds showed fragile statistically reliable understanding 
possible children simply span perceptual similarity approximation larger versus smaller categories akin set nested categories mature conceptual system 
alternative possibility assumes children may somewhat different hypothesis space adults having nested set categories children may mapped words regions perceptual space shepard tenenbaum griffiths broad narrow 
indication children may somewhat different hypothesis space adults pattern generalization superordinates 
examples spanned superordinate level category children chose superordinate matches time far example sets adults time experiment vs experiment 
possible explanations finding explored 
children may simply different tree structured hypothesis space adults stable hypothesis space stable superordinate level hypotheses just happen include exactly objects adults 
children stable hypothesis spaces 
variance children hypothesis spaces individual child single tree structured hypothesis space clearly articulated adult learners 
children need acquire deeper theoretical knowledge superordinate categories biologically relevant facts animals grow part intension word categories stable hypotheses generalizing word meanings 
potential concern experiments relatively familiar categories 
possible children acquired superordinate subordinate level terms simply translated words nonsense labels experiments 
waxman half year old children knew superordinate term animal vegetable vehicle commonly known 
sample year olds presumably know words year olds behave differently task 
concepts existing label dalmatian yellow truck green pepper 
children simply translated new words words knew 
broader range categories novel categories help clarify generality findings 
xu tenenbaum schmidt tenenbaum studied word learning different sets novel objects classified xu tenenbaum tree structured hierarchy object kinds behavior consistent bayesian framework 
stress say words hard learn examples saying aspects words easy learn 
experiments model addressed word meaning terms extension entities word refers 
aspects word meaning having word intension essence concept labeled word concept relates domain theory relates concepts may easily grasped just examples 
see bloom discussion differences extensions intensions word meaning 
developing models statistical inference naturally begins focusing extension words component meaning directly measurable statistical consequences 
framework limited extensions 
aspects word meaning statistical consequences word principle learned observations appropriate hypothesis space 
category labels different levels conceptual hierarchy difficult learn suggested young children early vocabulary tend see basic level category labels 
critical observation motivated standard picture children acquire kind terms multiple levels taxonomy 
factors may important explaining time lag acquiring basic level labels acquiring subordinate superordinate level labels 
subordinate superordinate level labels may require multiple examples 
example labeled different occasions spread time children may forget examples time 
second subordinate superordinate level category labels frequently adult speech relevant examples harder come 
middle class american parents tend point objects label basic level terms 
superordinates refer collections markman children may misled input interpreting words 
studies children simplified learning situation order uncover underlying inferential competence guides exclusively responsible real world performance 
evaluating bayesian model learning object kind labels section assess quantitative fit bayesian model word learning participants generalization judgments kind label learning experiments just 
consider predicted generalization patterns alternative models including weaker versions full bayesian approach number non bayesian models intended capture major hypothesis elimination associative learning approaches 
constructing hypothesis space basis participants similarity judgments experiment generated hierarchical cluster tree approximate taxonomy nested categories 
internal node tree corresponds cluster objects average similar nearby objects 
height node represents average pairwise dissimilarity objects corresponding cluster 
length branch node measures similar average cluster members objects nearest cluster distinctive cluster main classes underlying choice stimuli corresponds node tree vegetable ee vehicle hh animal jj pepper truck dog green pepper yellow truck dalmatian 
clusters highly distinctive separated clusters long branches expect targets kind terms 
easily describable nodes include cluster containing construction vehicles tractor crane cluster ii containing word learning bayesian inference 
hierarchical clustering similarity judgments yields taxonomic hypothesis space bayesian word learning 
letter codes refer specific clusters hypotheses word meaning vegetable ee vehicle hh animal jj pepper truck dog green pepper yellow truck dalmatian 
clusters labeled letter codes text needed 
numbers indicate objects located leaf node hierarchy keyed object numbers shown figures 
height cluster vertical axis left represents average cluster objects cluster 
mammals 
clusters appear correspond conceivably concepts defined subtle perceptual variation subordinate level cluster including cluster including green peppers 
take cluster correspond hypothesis exception clusters subordinate level 
doing assuming learner maintains single hypothesis space structure change new words notable exception cluster corresponding trucks barely separated highest cluster contains trucks plus long yellow school bus 
cluster fairly separated highest cluster suggesting perceptually basic category quite trucks motor vehicles learned 
assuming single tree structure sufficient model hypothesis spaces word learners 
assumptions greatly simplify modeling fundamental commitment theoretical framework expect need relaxed 
computing numerical values likelihoods priors learning common nouns taxonomic constraint geometry cluster tree suggests general purpose procedures computing likelihoods priors 
methods convenient modeling purposes view best just approximation knowledge people bring bear problem 
crucial geometrical feature height node tree scaled lie lowest node highest node measures average dissimilarity objects likelihood hypothesis function size extension 
access true size set dogs world vegetables access psychologically plausible proxy average cluster dissimilarity cluster height tree 
equating node height approximate cluster size likelihood height 
add small constant height keep likelihood going infinity lowest nodes tree height 
exact value critical 
generally find best results simulations article 
nodes shown height true height reflecting value 
larger values may appropriate situations sizes concepts distinctly learner 
likelihoods monotonically related heights cluster tree finite increasingly uniform uninformative increases 
preference cluster distinctiveness prior captured proportional branch length separating node parent height parent height 
measure maximized clusters entities high average cluster similarity relative similarity similar entities outside cluster 
example class containing dogs highly distinctive classes immediately nearly distinctive accordingly receives higher prior proportional vs 
example shows distinctiveness bias prior necessary 
terms likelihood hypothesis effectively dogs significant body area colored white typically slightly preferred hypothesis effectively dogs slightly smaller 
strong distinctiveness prior favoring ensure conceptually natural hypothesis receives higher posterior probability learner observes random examples dogs tend fall hypotheses 
general distinctiveness high basic level categories prior probability distinctiveness xu tenenbaum thing basic level bias 
distinctiveness may high conceptually natural clusters superordinate subordinate categories 
superordinate categories significantly distinctive categories accords intuition fundamental differences contrasting ontological categories occur superordinate level animal vs vehicle basic subordinate level dog vs cat dalmatian vs 
independent general preference distinctiveness people may preference map new words basic level categories mervis hirsh markman 
existence basic level bias children learning matter controversy waxman original studies rosch 
certainly provide strong reasons think bias useful preference distinctiveness introduced 
rosch spontaneous labeling objects adults basic level names 
preference extreme basic level preferences rosch reported nonlinguistic perceptual motor action criteria suggests learning kind labels appropriate adopt basic level bias general bias natural distinctive concepts 
note basic level bias reflect learners beliefs word meanings natural reflects beliefs words specifically kind labels tend 
belief rosch showed strongly supported naming statistics 
belief statistically valid majority kind labels fact pick basic level concepts subordinate kinds basic level kinds receive labels 
test utility sort basic level bias word learning consider versions model contains preference map words basic level instantiated distinctiveness prior equation contains extra bias prior probability just hypotheses corresponding basic level words english dog truck pepper 
hypotheses basic level bias implemented replacing times value equation single free numerical parameter adjusted provide best fit data 
note term basic level bias differs uses literature 
typically unclear putative word learning bias basic level bias refers behavioral tendency aspect mental representation greater prior degree belief concepts basic level kinds candidate word meanings 
interest primarily concerns reserve term bias sense empirical studies focused 
empirical phenomenon demonstrated previous studies waxman studies generalization taxonomic label single example appears follow gradient falling basic level 
children adults tend extend novel label new objects matching subordinate level time studies objects matching basic level rarely objects matching superordinate level 
referring behavioral tendency basic level bias refer shot basic level generalization distinguish possible cognitive structures proposed account 
model results consider basic bayesian model distinctiveness prior equation 
compares computed model generalization judgments adult participants figures averaged participants superordinate classes animal vehicle vegetable test items level generalization 
averaged data shown model achieves reasonable quantitative fit 
captures main qualitative features data graded generalization example generalization level specific consistent natural concept examples 
differences model generalizations people judgments model produces little generalization basic level matches example subordinate examples generalization superordinate matches basic level examples 
shows fit bayesian model incorporating bias prior favors basic level hypotheses 
strength basic level bias free parameter set 
free parameter model provides perfect fit average data 
main qualitative trends captured including accounted bayesian model basic level bias 
results suggest adults hypotheses word learning biased specifically basic level object categories general preference distinctive categories captured branch length prior equation 
different picture emerges compare versions bayesian model age children generalizations experiment figures 
ways children performance looks bayesian model predictions basic level bias particularly shift example subordinate examples 
correlation coefficients models similar word learning bayesian inference 
predictions bayesian model basic level bias compared data adults experiment children experiment 
sub 
subordinate super 
superordinate 
basic level bias basic level bias 
additional parameter contribute significantly variance accounted leads fit qualitatively worse ways results suggest child word learners may strong basic level bias adults exhibit 
tendency extend new words basic level matches weaker adults may explained simply combination bayesian hypothesis averaging equation general preference hypotheses corresponding distinctive categories equation 
return issue discussion 
comparison models illustrates respectively complementary roles played size principle equations hypothesis averaging equation bayesian framework 
size principle weight hypotheses strictly prior including basic level bias bayes reduces similarity computation suited generalization gradients observed example patterns observed examples 
mathematically corresponds replacing size likelihood equations simpler measure consistency examples consistent hypothesis 
tenenbaum griffiths called approach weak bayes uses weak binary measure consistency likelihood strong assumption randomly sampled examples implicit size principle 
essentially algorithm pro correlation values section computed judgments test items superordinate class observed examples 
participants chose test items crossed superordinate boundaries models give test items zero near zero probability generalization 
posed mitchell haussler kearns schapire shepard 
averaging predictions consistent hypotheses base generalization just single probable hypothesis bayes reduces computation 
priors including basic level bias likelihoods cooperate rank hypotheses highest ranking hypothesis probability distribution hypotheses generalization 
mathematically corresponds replacing hypothesis averaging equation simpler decision rule ify isthe hypothesis maximal posterior probability equation 
approach called maximum posteriori bayes bayes short 
shows map bayes captures qualitative trends adults children generalize multiple examples including restriction generalization subordinate examples observed 
capture graded nature generalization single example 
capture increasing confidence basic level generalization comes seeing basic level examples adults children map bayes exactly generalizations basic level examples just single example 
shows predictions alternative learning models 
models specifically proposed word learning generic approaches literature computational models learning generalization representative previous suggestions word learning viewed computationally 
explicitly bayesian varying degrees correspond special cases bayesian learning shown 
presents predictions simple exemplar similarity model computed averaging similarity exemplar 
mean similarity judgments adult participants experiment normalized scale 
set examples generalization function scaled linearly maximum 
shows predictions alternative approach exemplar similarity inspired proposals goldstone osherson smith wilkie lopez computed maximum similarity exemplars weak bayes pure version bayesian model shown exemplar similarity models give soft gradient generalization xu tenenbaum 
predictions variants bayesian model 
size principle bayesian generalization behaves exemplar similarity computation 
hypothesis averaging bayesian generalization follows pattern 
map bayes maximum posteriori bayes approach sub 
subordinate super 
superordinate 
example fail sharpen generalization appropriate level examples 
flexible similarity models category learning incorporate selective attention different stimulus attributes kruschke better able accommodate data major modification 
models typically rely error driven learning algorithms designed learn broadly generalize just positive examples negative examples low dimensional spatial representations stimuli suited representing broad taxonomy object kinds 
authors suggested associative correlational learning algorithms instantiated neural networks explain children learn meanings words smith gasser smith regier 
possible evaluate extant correlational learning algorithms consider standard approach hebbian learning hertz krogh palmer 
shows predictions hebbian learning network matched closely possible structure bayesian models 
hebbian model uses input features corresponding hypotheses bayesian models evaluating averaging hypotheses machinery bayesian inference uses hebb rule compute associative weights input feature unit output unit representing occurrence novel word learned fep 
network produces generalization patterns produced exemplar similarity models weak bayes capturing graded character shot generalization failing account generalization appropriate level examples seen 
similar predictions various models reflect underlying computational commonalities 
learning hebbian network strictly frequency input features occur observed examples exemplar leaves trace feature values weights connecting input features output unit final pattern generalization shows median pattern generalization superordinate categories mean map generalizations mean representative model predictions 
proportional average generalization similarity gradients produced exemplar individually 
second models fail converge appropriate level specificity multiple examples lack size principle reweighting multiple consistent hypotheses prefer hypothesis produced observed examples 
hebbian learning network multiple examples subordinate category correlation output unit input features specific greater correlation output input features apply dogs animals dalmatian exemplar activates dog animal units dalmatian units 
correlations independent number examples observed hebbian model explain generalization subordinate concept decreases examples lying strictly subordinate observed seeing dalmatian exemplars leads lower generalization dogs relative seeing just dalmatian exemplar 
problem powerful associative learning mechanisms standard neural networks trained back propagation errors rumelhart hinton williams associative models word learning smith regier defined solely terms statistics input output occurrence 
hebb rule associative learning algorithms modified include version size principle 
instance allow learning rates vary different input features function feature specificity 
move allow bayesian associative models word learning interact productively 
point connectionist models word learning untenable generic associative learning mechanisms purely correlations observable features sufficient explain children adults learn meanings new words 
powerful mechanisms statistical inference bayesian framework necessary 
word learning bayesian inference 
predictions alternative non bayesian models 
max maximum sim similarity sub 
subordinate super 
superordinate 
shows predictions standard learning algorithm hypothesis elimination paradigm known subset principle berwick pinker siskind see wexler discussion subset learning syntax acquisition bruner feldman analogous proposals category learning 
subset principle ranks hypotheses inclusion specificity hypothesis ranks higher hypothesis strictly includes includes object object 
subset learner eliminates hypotheses inconsistent observed examples generalizes fashion highest ranking remaining hypothesis specific hypothesis consistent observed examples 
approach intuitively sensible produces reasonable generalizations multiple examples far conservative just single example 
patterns generalization multiple examples subset principle essentially identical map bayes 
approaches just single hypothesis guide generalization chosen basis inclusion specificity posterior probability respectively 
criteria converge component posterior probability likelihood size principle likelihood proportional specificity 
posterior probabilities map bayes depend priors exert strongest role example observed 
basic level bias prior accounts map bayes generalizes differently subset principle example basic level matches just subordinate matches 
examples observed stronger influence specificity preference likelihood prior map bayes subset principle coincide 
limit infinite data map bayes full bayesian models equivalent maximum likelihood size principle equivalent subset learning 
subset principle justified rational statistical inference large numbers examples observed precisely case learnability limit focus previous uses subset principle 
bayesian models size principle viewed extensions subset principle explain dynamics learning just examples arguably important regime learning real world words concepts 
broadly various phenomena entrenchment conservatism language acquisition brooks goldberg may consistent softer statistical model hard commitments subset principle 
summary sum inductive models may seen probabilistic generalizations classic deductive approach word learning hypothesis elimination 
hypothesis elimination accounts constrained hypothesis space possible meaningful generalization examples 
contrast accounts hypotheses just ruled 
bayes rule assigned probability correct explain pattern examples observed 
assumption observed examples randomly sampled word extension provides powerful statistical lever yielding strong reliable generalizations just examples 
experiments domain words object categories showed people patterns generalization qualitatively quantitatively consistent bayesian model behavior standard models hypothesis elimination exemplar similarity associative correlational learning 
particular bayesian approach naturally explains spectrum generalization behavior observed positive examples 
graded generalization example follows straightforwardly mechanism hypothesis averaging sharpening examples follows straightforwardly size principle 
bayesian inference may offer promising framework explain speed success fast mapping 
models hypothesis elimination associative traditions extended accommodate findings 
easily think positing additional machinery inelegant fundamentally departs original spirit approaches 
deductive framework hypothesis elimination order explain sharpening generalization examples posit basic level bias just example case version subset bias choosing smallest category consistent examples just example case 
presumably want posit specific selection principle particular case 
addition positing basic level bias subordinate superordinate kind labels difficult learn 
children eventually learn names levels hypothesis elimination approaches posit provisions overriding basic level bias time incorporating linguistic cue appropriate level generalization label 
mentioned earlier possible extend associative model word learning account range generalization behavior observed building size principle hypothesis averaging learning activation rules 
xu tenenbaum extensions sufficient 
computations associative models typically defined core ontology objects object kind concepts relationships perceptual features example visual features objects sound features words regier 
focus learning correlations level perceptual features stand way appropriate generalization 
bayesian learner sees critical difference object labeled times distinct perceptually highly similar objects labeled year old children showed experiment 
cases provide observations word object pairings object features cases case provides independent samples objects concept case provides independent sample 
case provides strong evidence extent concept case children restrict generalization objects subordinate category 
associative learner appreciate difference need gain size principle learning rule kind ontology understands differences objects percepts categories keil 
need build kind taxonomic hierarchy object kind categories top ontology 
capacities currently part conventional associative models incompatible general predictive learning view smith 
adding capacities abandon core claim word learning explained sophisticated inferential mechanisms sophisticated representations world landau smith jones 
extending bayesian framework models focused inductive problem word learning simplest form learning meaning single new word fairly restricted hypothesis space observations word label entities world 
tried keep models simple possible capture fundamental insights meanings important classes words may learned limited data 
word learning real world considerably complex terms kinds hypothesis space learners entertain kinds inferences required kinds data brought bear inferences 
word learning dynamic process knowledge gained previous word learning experience specific meanings particular words abstractions general principles word meaning usage leads crucial constraints word learning baldwin bloom gleitman markman regier tomasello 
section briefly sketches possible avenues extending models handle complexities 
differently structured hypothesis space objects solid substances fundamental problem induction word learning choose multiple potential concepts hypotheses word meanings consistent observed examples new word 
far addressed problem context learning count nouns object kinds multiple consistent hypotheses come hierarchically nested kind concepts 
learning sorts words encounters different kinds inductive ambiguities hypotheses overlap ways 
sketch bayesian analysis case crosscutting object kinds solid substance kinds showing general framework developed learning kind labels applies relevant concepts conform nested hierarchy 
consider furniture store context raised 
entities furniture store may referred terms object category chair table shelf vase material wood plastic metal stone 
generally solid entity may construed modes object particular kind solid substance compose words available refer modes 
learning words solid entities poses challenge learning overlapping hypotheses nested hypotheses 
object kind categories crosscut solid substance categories kind object realizable different substances substance capable different shapes 
haskell explored conditions people construe solid entity terms object kind substance kind concept crosscutting hypotheses available 
showed people solid entities composed unfamiliar materials regular shapes complex irregular shapes asked prefer call entities choosing suggests refers object category choosing suggests substance kind 
single regularly shaped entity people tended choose object category single irregularly shaped entity people tended choose substance interpretation 
people inferences multiple examples generally consistent single example cases interesting exception 
people shown multiple essentially identical entities complex irregular shape novel material preference labeling entity set switched substance interpretation object interpretation 
bayesian framework explain inferences assume people treating examples random samples hypotheses meaning novel word object kind category substance category 
goal infer hypothesis probable examples observed 
technical details scope basic assumptions results derived 
object category organized prototypical shape 
second object categories regular shapes higher priors substance categories turn higher priors object categories irregular shapes 
consistent english word frequencies higher regularly shaped object category labels material irregularly shaped object category labels bloom landau 
third conceptually distinct shapes support object categories material properties support substance categories 
effective size word learning bayesian inference object kind hypothesis smaller substance hypothesis suspicious coincidence observe randomly sampled entities novel shape compared novel material 
speakers english tend find salient potentially differences object shapes material substances cross linguistic differences imai gentner 
ingredients allow explain 
finding shift substance interpretation novel word irregularly shaped example objectbased interpretation essentially identical examples irregular shape material 
prior initially favors substance interpretation shift generalization multiple examples comes detecting suspicious coincidence reflection size principle likelihood term 
strong coincidence observe random samples irregular shape novel word intended label substance kind suggests object kind correct 

interpreted findings similar terms arguing people interpret entity instance object kind form appears 
interpretation explains finding theirs single entity irregular shape construed instance object kind shape shown functional significance sense shape appears coincidence 
framing interpretations explicitly terms statistical inference see reflect general rational inferential mechanisms understanding learning meanings words 
mechanisms underlie people ability infer appropriate scope range generalization learning names hierarchically nested categories support ability infer appropriate directions dimensions generalization multiple plausible hypotheses crosscut 
shown extend bayesian approach learning aspects linguistic meaning differently structured hypothesis spaces appropriate learning verb frames niyogi color terms principles anaphora resolution regier 
transforming likelihood function basic bayesian framework information meaning new word contributes influence likelihood prior 
changing expanding terms address complex kinds inferences incorporate additional sources constraint learner inferences 
sources input 
earlier mentioned sources information word meaning explicitly incorporate formal analyses negative examples examples entities word apply special linguistic cues relate meaning new word familiar words saying dalmatian 
kind dog focused learning positive examples special linguistic cues negative examples relational linguistic cues crucial inferring scope new word extension 
bayesian framework naturally accommodate sources information straightforward mod likelihood function 
assign zero likelihood hypothesis includes negative examples essentially treating negative examples deductive constraint candidate word meanings 
cue dalmatian 
kind dog treated deductive constraint assigning zero likelihood hypothesis dalmatian contained extension word dog 
bayesian learner rationally infer subordinate meaning new word fep just positive example fep dalmatian additional sources input negative example fep relational cue language kind dog 
theory mind reasoning sensitivity sampling 
vital source information word meaning comes theory reasoning baldwin bloom tomasello 
fact certain kind object labeled certain word just simple perceptual feature associated corresponding object concept 
words tools intentional agents refer aspects world examples words learner observes consequences intentional acts 
inferences theory mind reasoning put opposition statistical inferences word meaning bloom regier construed bottom associative processes 
top knowledge approach statistical inference propose theory mind considerations play critical role 
making statistical inferences meanings words examples may demand learner addition abilities sensitivity intentional epistemic states speakers communicative interactions produce examples observed 
sensitivity may enter bayesian framework specifying sampling assumption determines appropriate likelihood function 
explored framework predictions settings strong theory mind demands tested children adults sensitive sampling process generating examples see labeled adjust likelihoods accordingly 
xu tenenbaum studied generalization novel object kind labels conditions labeled examples appeared sampled randomly set objects word applies essentially examples observed clearly randomly sampled word extension 
stimuli simple novel objects generated computer drawing program 
studies reported objects classified multiple levels clear salient hierarchy classes 
teacher driven condition similar trials experiments 
experimenter pointed object said child fep pointed distinct similar looking objects subordinate category labeled fep 
reasonable approximation treat examples random samples 
learner driven condition labeled example chosen experimenter said child point 
get right get case children motivated choose objects subordinate category order get labeled correct experimenter 
children learner xu tenenbaum driven condition received essentially data children teacher driven condition 
learner driven condition examples treated random sample drawn word extension child know meaning novel word fact trying choose objects similar labeled experimenter 
results xu tenenbaum showed adults sensitive sampling conditions 
teacher driven condition replicated results current experiments learners restricted generalization novel word subordinate exemplars 
condition adults children generalized broadly basic level category included examples 
just bayesian analysis predict situation examples labeled sampled independently meaning word xu tenenbaum 
likelihood reflecting size principle simply measure consistency proportional hypotheses consistent labeled examples inconsistent hypotheses 
increasing preference smaller hypotheses bayesian learner maintain basic level threshold generalization additional examples observed long consistent set hypotheses 
sensitivity sampling conditions distinctive feature bayesian approach 
predicted traditional associative deductive accounts word learning view word learning fundamentally problem making statistical inferences samples underlying explanatory hypotheses 
associative approaches typically embody implicit statistical assumptions assumptions explicit learners power inferences sampling process 
forgo important aspect rational statistical inference important contribution intentional reasoning word learning process 
transforming prior probabilities effects previously learned words 
ways word meanings learned previously constrain meanings new words learned 
way development syntax semantics mappings bias map count nouns object kinds mass nouns substance kinds smith kemp 
way lexical contrast assumption meanings words differ clark 
influences captured bayesian framework modifying learner prior probabilities 
technical details scope article sketch bayesian analysis lexical contrast 
mutual exclusivity simple form lexical contrast constraint entity label words overlapping extensions markman 
simplest way capture mutual exclusivity framework prior 
mutual exclusivity assumed hard constraint simply set prior zero hypothesis extension new word overlaps extension previously learned word 
mutual exclusivity taken soft bias hard constraint prior probability hypotheses extensions overlapping known words set fraction default value 
regier suggested alternative way mutual exclusivity enter bayesian analysis alternative formulation likelihood 
mutual exclusivity useful early stages word learning excludes cases meaning overlap studied impossible learn word meanings animal dalmatian pet just sake learning basic level kind term dog 
clark principle contrast clark weaker version lexical contrast suited mature lexicon assume words exactly meaning extensions may overlap way complete identity 
formally principle implemented just mutual exclusivity setting prior probability hypothesis corresponds extension known word zero small fraction default value softer bias called 
analysis lexical contrast effects far assumed highly idealized scenario attributing learner completely fixed lexicon previously learned words 
practice learners learning words time varying degrees experience confidence meaning 
realistic bayesian formulation word learning problem construe hypotheses data language wide structures learning individual word concept mappings 
learner evaluate hypotheses possible sets word concept mappings entire language basis full body data words language seen date 
size principle likelihood apply separately word 
prior candidate lexicons incorporate factors discussed far including principle contrast bias map words priori natural distinctive concepts 
directly implementing language wide approach computationally intractable kind online approximation usefully describe trajectory large scale vocabulary acquisition 
open issues theoretical framework aims generality important questions word learning current scope 
sketch open questions 
emphasized phenomenon fast mapping adults children showing bayesian models naturally give rise efficient learning just examples 
researchers suggested early word learning fundamentally different kind process 
characterized slow laborious enterprise 
children months require exposures single word order learn see woodward words appear drop lexicon 
unclear early word learning appears efficient learning stages reflects applicability bayesian framework earliest stages word learning 
possible reasons word learning youngest children look fast mapping behavior bayesian models 
necessary capacity bayesian word learning bayesian inference inference may available youngest children may develop simple maturation way depends development general purpose cognitive capacities 
associative models word learning smith regier focus earliest stages word learning certainly possible word learning best characterized initially associative bayesian mature state studied 
second capacity bayesian inference may available young children may weaker constrained hypothesis spaces support learning high confidence just examples 
may viewed bayesian word learners appropriate hypothesis spaces 
third young children possess domain general bayesian inference capacities able apply mechanisms task word learning 
instance grasp concepts intention necessary treat observations word object labeling events randomly sampled examples word class able set likelihood functions appropriate word learning 
youngest children possess core conceptual capacities suffer processing limitations prevent remembering words stably time fixing referent word quickly 
research necessary distinguish possible accounts earliest stages word learning 
far treated word learning mapping problem learners possess concepts hypotheses candidate word meanings independent words task map word forms concepts 
view imply concepts innate just place time words learned 
quite possible word learning concept formation proceed parallel extent levinson gentner goldin meadow xu 
terms bayesian framework observation new words mapped easily current hypothesis space candidate concepts triggers formation new concepts suitable hypotheses meanings words 
bayesian models relation word learning concept learning generally focus ongoing kemp tenenbaum 
generally questions origins learner hypothesis space clearly important targets 
questions asked levels 
deeply learner acquire knowledge certain class words map hierarchy object kinds certain kinds perceptual features typically diagnostic kind membership 
second knowledge learner construct concrete tree structured hierarchy words object kinds mapped 
principle questions addressed hierarchical bayesian framework kemp tenenbaum tenenbaum extension approach developed include hypothesis spaces multiple levels abstraction probabilistic models linking level hierarchy 
second question easier address sense addressed implicitly 
goal searching tree structured hierarchy object kinds sense perceptual features charac kinds learner just needs perform kind hierarchical clustering objects observed world 
kemp 
discussed hierarchical clustering viewed bayesian inference search simplest tree assigns high likelihood observed object features probabilistic model objects nearby tree expected look similar objects far apart tree 
question addressed logic 
learner considers different classes structures generate hypothesis space word meanings including tree structured object kind hierarchies kinds structure 
organizing principles scored predicts observed object features practice computing score quite difficult involves summing searching specific structures consistent class structures kemp 
analysis word learning focuses marr called level computational theory 
tried elucidate logic word learners inductive inferences specifying logic implemented algorithmically mind physiologically neural hardware 
claim bayesian computations implemented exactly mind brain explicitly represented probabilities 
contrary details mental neural processing correspond efficient approximation bayesian computations propose 
claim computations consciously accessible intermediate steps 
fact people typically aware considering hypotheses word meaning mean mind implicitly behave accord bayesian principles 
learning mechanism proposed specific word learning language acquisition 
research shown domains inductive learning reasoning may explained bayesian terms including causal learning griffiths tenenbaum sobel tenenbaum steyvers tenenbaum blum category induction heit kemp tenenbaum conditional reasoning chater covariation assessment mckenzie mckenzie 
word learning requires specialized mechanisms assumptions subject lively debate field cognitive language development bloom bloom waxman booth xu cote baker 
word learning may require certain language specific principles structures plausible inference mechanisms suggest domain general 
taken close look pieces big puzzle 
argued bayesian approach provides powerful computational framework explaining people solve inductive problem learning word meanings showing approach gives distinctive insights core phenomena word learning strong quantitative fits behavioral data experiments adult child learners 
xu tenenbaum course caution concluding studies 
specific experimental tasks computational models worked simplify real challenges children face ways leave aspects word learning completely unaddressed suggest number promising extensions 
think valuable lessons drawn nature word learning cognitive development generally 
accounts cognitive development typically view statistical learning sophisticated representational machinery competing mutually exclusive explanations come know world 
theoretical framework explaining aspect development word learning basis operation powerful statistical inference mechanisms defined structured mental representations 
contrast associative tradition approach critical roles conceptual hierarchies objects distinct word percept correlations linguistic communicative principles 
traditional approaches heart statistical inference knowledge word meanings graded depending probabilistic evidence provided different degrees data 
fully satisfying computational model word learning remains remote model general purpose cognition suggests bet models look 
combination sophisticated mental representations sophisticated statistical inference machinery able explain adults children learn words fast accurately 


learning words overhearing 
child development 
anderson 

adaptive character thought 
hillsdale nj erlbaum 
bacon 
new related writings 
indianapolis merrill 
original published baldwin 

infants contribution achievement joint 
child development 
baldwin 

infants ability consult speaker clues word 
journal child language 
baldwin markman bill desjardins irwin 

infants reliance social criterion establishing word object relations 
child development 


fast mapping young children extensions novel words novel facts 
developmental psychology 
berwick 

learning positive examples subset principle case studies 
carbonell michalski mitchell eds machine learning artificial intelligence approach vol 
pp 

los altos ca morgan kauffman 
bloom 

children learn meanings words 
cambridge ma mit press 
levinson 

language acquisition conceptual development 
cambridge england cambridge university press 
brooks 

verb argument structure problem avoiding grammar 
tomasello merriman eds names things young children acquisition verbs pp 

hillsdale nj erlbaum 
bruner austin 

study thinking 
new york wiley 


parents label objects young children role input acquisition category hierarchies 
child development 


development object categories inclusion relations hypotheses word meanings 
developmental psychology 
mccarthy 

children hypotheses word meanings basic level constraint 
journal experimental child psychology 
carey 

child word learner 
halle bresnan miller eds linguistic theory psychological reality pp 

cambridge ma mit press 
carey bartlett 

acquiring single new word 
papers reports child language development 
clark 

principle contrast constraint language acquisition 
macwhinney ed mechanisms language acquisition th annual carnegie symposium cognition pp 

hillsdale nj erlbaum 
smith 

lexicon expectations kinds role associative learning 
psychological review 
bloom 

specific shape bias 
child development 


children avoidance lexical overlap pragmatic account 
developmental psychology 


modelling acquisition colour words 
mckay slaney eds advances artificial intelligence pp 

berlin germany springer 


early lexical development 
cambridge england cambridge university press 
duda hart 

pattern classification scene analysis 
new york wiley 
feldman 

structure perceptual categories 
journal mathematical psychology 
gasser smith 

learning nouns adjectives connectionist approach 
language cognitive processes 
gentner goldin meadow 

language mind 
cambridge ma mit press 
gleitman 

structural sources verb meanings 
language acquisition 
gold 

language identification limit 
information control 
goldberg 

constructions new theoretical approach language 
trends cognitive sciences 
goldstone 

role similarity categorization providing groundwork 
cognition 
mervis hirsh 

early object labels case developmental lexical principles framework 
journal child language 
sobel schulz 

theory causal learning children causal maps bayes nets 
psychological review 
griffiths steyvers tenenbaum 

topics semantic representation 
manuscript submitted publication 
griffiths tenenbaum 

structure strength causal induction 
cognitive psychology 
haussler kearns schapire 

bounds sample complexity bayesian learning information theory 
machine learning 
word learning bayesian inference markman 

word learning children examination fast mapping 
child development 
heit 

bayesian analysis forms inductive reasoning 
chater eds rational models cognition pp 

oxford england oxford university press 
hertz krogh palmer 

theory neural computation 
boston addison wesley 
imai gentner 

cross linguistic study early word meaning universal ontology linguistic influence 
cognition 
markman 

learning proper common nouns inferential vs contexts 
child development 
keil 

semantic conceptual development ontological perspective 
cambridge ma harvard university press 
kemp tenenbaum 

learning domain structure 
forbus gentner regier eds proceedings th annual conference cognitive science society 
mahwah nj erlbaum 
kemp tenenbaum 

learning hierarchical bayesian models 
developmental science 
kemp tenenbaum 

theory induction 
kirsh eds proceedings th annual conference cognitive science society 
mahwah nj erlbaum 
kruschke 

exemplar connectionist model category learning 
psychological review 
landau smith jones 

importance shape early lexical learning 
cognitive development 
landau smith jones 

object shape object function object name 
journal memory language 
landauer dumais 

solution plato problem latent semantic analysis theory acquisition induction representation knowledge 
psychological review 
li macwhinney 

overgeneralization competition connectionist model learning english prefixes 
connection science 
liu sak 

cow animal young children extend words superordinate level 
child development 
macwhinney 

models emergence language 
annual review psychology 
markman 

categorization naming children 
cambridge ma mit press 
markman hutchinson 

children sensitivity constraints word meaning taxonomic versus thematic relations 
cognitive psychology 
markman 

children mutual exclusivity constrain meanings words 
cognitive psychology 
marr 

vision 
san francisco freeman 
mckenzie 

accuracy intuitive judgment strategies covariation assessment bayesian inference 
cognitive psychology 
mckenzie 

bayesian view covariation assessment 
cognitive psychology 
merriman 

competition attention young children lexical processing 
macwhinney ed emergence language pp 

mahwah nj erlbaum 
mill 

system logic ratiocinative inductive connected view principles evidence methods scientific investigation 
london parker 
mintz gleitman 

adjectives really modify nouns incremental restrictive nature early adjective acquisition 
cognition 
mitchell 

generalization search 
journal artificial intelligence 
mitchell 

machine learning 
new york mcgraw hill 
niyogi 

bayesian learning syntax semantics interface 
gray eds proceedings th annual conference cognitive science society pp 

mahwah nj erlbaum 
chater 

rational analysis selection task optimal data selection 
psychological review 
chater 

rational models cognition 
oxford england oxford university press 
osherson smith wilkie lopez 

category induction 
psychological review 
kemp tenenbaum 

modeling acquisition domain structure feature understanding 
bara barsalou bucciarelli eds proceedings th annual conference cognitive science society 
mahwah nj erlbaum 
pinker 

formal models language learning 
cognition 
pinker 

language learnability language development 
cambridge ma harvard university press 
pinker 

learnability cognition acquisition argument structure 
cambridge ma mit press 
plunkett sinha moller 

symbol grounding emergence symbols 
vocabulary growth children connectionist net 
connection science 
popper 

logic scientific discovery 
new york basic books 
haskell 

conceiving entities objects stuff 
cognition 
quine 

word object 
cambridge ma mit press 
chater finch 

distributional information powerful cue acquiring syntactic categories 
cognitive science 
regier 

human semantic potential spatial language constrained connectionism 
cambridge ma mit press 
regier 

emergent constraints word learning computational review 
trends cognitive science 
regier 

emergence words attentional learning form meaning 
cognitive science 
regier 

learning role missing evidence 
cognition 
rosch mervis gray johnson 

basic objects natural categories 
cognitive psychology 
roy pentland 

learning words sights sounds computational model 
cognitive science 
rumelhart hinton williams 
october 
learning representations back propagating errors 
nature 
schmidt tenenbaum 

word learning novel objects 
unpublished raw data 
shepard 
september 
universal law generalization psychological science 
science 
shepard arabie 

additive clustering representation similarities combinations discrete overlapping properties 
psychological review 
xu tenenbaum siskind 

computational study cross situational techniques learning word meaning mappings 
cognition 
smith 

children understanding natural language hierarchies 
journal experimental child psychology 
smith 

avoiding associations really hate 
hirsh eds word learner debate lexical acquisition pp 

oxford england oxford university press 
sobel tenenbaum 

children causal inferences indirect evidence backwards blocking bayesian reasoning 
cognitive science 
carey 

ontological categories guide young children induction word meaning object terms substance terms 
cognition 


principles object perception 
cognitive science 
steyvers tenenbaum blum 

inferring causal networks observations interventions 
cognitive science 
tenenbaum 

bayesian framework concept learning 
unpublished doctoral dissertation massachusetts institute technology cambridge ma 
tenenbaum griffiths 

generalization similarity bayesian inference 
behavioral brain sciences 
tenenbaum griffiths kemp 

theory bayesian models inductive learning reasoning 
trends cognitive sciences 
tomasello 

perceiving intentions learning words second year life 
levinson eds language acquisition conceptual development pp 

cambridge england cambridge university press 
tomasello barton 

learning words non contexts 
developmental psychology 
tversky 

features similarity 
psychological review 
waxman 

linguistic biases establishment conceptual hierarchies evidence children 
cognitive development 
waxman booth 

word learning smart evidence conceptual information affects extension novel words 
cognition 
wexler 

formal principles language acquisition 
cambridge ma mit press 
woodward markman 

rapid word learning month olds 
developmental psychology 
xu 

role language acquiring object kind concepts infancy 
cognition 
xu 

categories kinds object individuation infancy 
stowe eds building object categories developmental time papers nd carnegie symposium cognition pp 

mahwah nj erlbaum 
xu cote baker 

labeling guides object individuation month old infants 
psychological science 
xu tenenbaum 

sensitivity sampling bayesian word learning 
developmental science 
received june revision received july accepted october 
