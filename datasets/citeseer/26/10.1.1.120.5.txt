cooperative caching remote client memory improve file system performance michael dahlin randolph wang thomas anderson david patterson university california berkeley dahlin tea patterson cs berkeley edu emerging high speed networks allow machines access remote data nearly quickly access local data 
trend motivates cooperative caching coordinating file caches machines distributed lan form effective file cache 
examine cooperative caching algorithms trace driven simulation study 
simulations indicate systems studied cooperative caching halve number disk accesses improving file system read response time 
simulations conclude cooperative caching significantly improve file system read response time relatively simple cooperative caching algorithms sufficient realize potential performance gain 

cooperative caching seeks improve network file system performance coordinating contents client caches allowing requests satisfied client local memory file cache satisfied cache client 
technology trends push consider cooperative caching 
processor performance increasing rapidly disk performance 
divergence increasingly important reduce number disk accesses file system 
second emerging high speed low latency switched networks supply file system blocks network faster standard ethernet indicated 
fetching data remote memory older network times faster getting data remote disk remote memory may accessed times quickly disk increasing payoff cooperative caching 
supported part advanced research projects agency national science foundation cda california micro foundation digital equipment hewlett packard siemens sun microsystems xerox 
dahlin supported national science foundation graduate research fellowship 
anderson supported national science foundation young investigator award 
appeared proceedings symposium operating systems design implementation osdi 
existing file systems level memory hierarchy implementing limited form cooperative caching locating shared cache server memory supplement memory levels client memory server disk 
reduce disk accesses increasing fraction system ram resides server factors cooperative caching attractive physically moving memory clients server 
cooperative caching provide better performance approach improve global hit rate reduce system disk accesses cooperative caching leaves large memories clients maintain high hit rates remote memory ethernet mbit atm remote disk remote memory remote disk mem 
copy net overhead data disk total 
time service file system local cache remote memory disk slow network ethernet faster network mbit atm 
local memory copy time measured time read kb file cache dec axp 
network overhead times indicate round trip small packet latencies tcp times reported mart hewlett packard workstation 
ethernet data transfer figures unrealistically optimistic assumption data transferred full mbit link speed reality transfer times double listed ethernet 
atm transfer time assumes full mbit bandwidth attained optimistic assumption met year processor speeds continue increase 
disk transfer time measured physical disk time excluding queueing fastest systems measured real workloads ruemmler wilkes 
clients local caches saving network latencies compared going server 
second server cooperative caching system loaded satisfy requests small packets forward requests large data transfers 
third cooperative caching allows flexible memory memory physically located clients client virtual memory system demands warrant nels 
cooperative cache systems cost effective building system extremely large server cache 
example significantly cheaper add mb industry standard simm memory clients buy specialized server machine capable holding additional gb memory 
quantify trade offs centralized distributed memory detail section 
cooperative caching introduces fourth level network file system cache hierarchy 
data local memory server memory server disk client memory 
depending cooperative caching algorithm new level may client local memory server memory server memory disk 
note examining cooperative caching assuming clients cache file system data local memories local disks 
fast networks faster client fetch data client memory fetch data local disk 
assumption clients system equally secure 
believe fair assumption lan environments machines administered way 
trust stronger clients currently popular file systems nfs case client operating system compromised client issue unauthorized file system requests 
increasing availability process migration networks workstations doug zhou speed trend trust administrative domain system allows user jobs migrated machines user data may cached memories machines regardless cooperative caching 
study goals 
goal ascertain cooperative caching provide significant benefits real workloads 
trace driven simulation approach contrasts previous efforts evaluate cooperative caching synthetic workloads leff fran 
second goal evaluate range algorithms find practical algorithms implement effective cooperative caching 
previous studies focused algorithms requiring global knowledge client cache contents leff algorithms sacrifice perfor mance coordinating contents client caches fran 
primary result cooperative caching improve file read performance configurations workloads studied 
conclude algorithm called chance forwarding practical algorithm achieves nearly potential performance gains workloads 
cooperative caching designed improve cache performance file system reads 
technique address issues write performance large file performance important users file system 
study issues implementing cooperative caching part xfs project wang dahl 
cooperative caching illustrates primary design philosophy xfs vast aggregate resources system clients improve performance 
section describes cooperative caching algorithms examine 
section describes simulation methodology section examines simulation results 
compare previous efforts improve file cache performance section 
section summarizes 

cooperative caching algorithms examines variations cooperative caching detail covering range algorithm design decisions 
cooperative caching creates new level file system storage hierarchy remote client memory 
different cooperative caching algorithms manage new level different ways 
illustrates fundamental design questions relationship algorithms questions 
algorithms examine means exhaustive set cooperative caching algorithms subset contains representative examples large portion design space includes practical algorithm close optimal performance 
note algorithms examined affect data storage reliability alter write delay write back policy file system 
clients send modified data server cooperative caching server commits data disk traditional system 
rest section describes algorithms scrutiny briefly discusses algorithms 

direct client cooperation simple cooperative caching approach direct client cooperation allows active client idle client memory backing store 
active client forwards cache entries overflow local cache directly idle machine 
active client access private remote cache satisfy read requests remote machine active evicts cooperative cache 
system define criteria designating active idle clients provide mechanism locate 
direct client cooperation appealing simplicity implemented server modification 
far server concerned client utilizing remote memory appears temporarily enlarged local cache 
drawback lack server coordination active clients benefit contents active clients memories 
client data request go disk desired block longer happens limited server memory client caching block 
result performance benefits direct client cooperation limited motivating algorithm 

greedy forwarding simple cooperative caching approach called greedy forwarding treats cache memories clients system global resource may accessed satisfy client request algorithm attempt coordinate contents caches 
traditional file systems client manages local cache greedily regard contents caches system potential needs clients 
client find block local cache asks server data 
server required data memory cache supplies data 
server consults data structure listing private direct client cooperation private global coop 
cache 
greedy forwarding block location 
coordinated cache entries 
cent 
coord hash global coordination coordination static dynamic partition 
static dynamic chance client fixed weighted lru 
cooperative caching algorithm design space 
box represents design decision oval represents algorithm examined study 
focus highlighted algorithms consider detail due space constraints 
contents client caches 
client caching required data server forwards request client 
client receiving forwarded request sends data directly client request 
note block sent back server unnecessarily increase latency add server workload 
client caching data request satisfied server disk cooperative caching 
greedy forwarding change file system server needs able forward requests clients need able handle forwarded requests support needed algorithms discussed 
server forwarding implemented data structures systems implementing write consistency call backs cache disabling nels 
systems server tracks files cached client take appropriate action guarantee consistency file modified 
study assume cooperative caching extends call back data structure track individual file blocks cached client allow forwarding 
systems nfs servers maintain precise information clients caching sand implementation directory may simplified contents taken hints forwarded requests may sent clients longer caching desired block 
case client inform server mistake server forward request client get data disk 
block forwarding table larger traditional file callback lists additional server memory overhead reasonable entry allows server leverage block client cache 
instance forwarding table implemented hash table hash entry containing byte file identifier byte block offset byte client identifier byte pointer linked list collision resolution byte pointers doubly linked lru list server require bytes block client cache 
system caching kb file blocks data structure consume memory indexes 
system clients mb cache server track contents gb distributed cache mb index 
greedy forwarding appealing preserves fairness clients manage local resources local deriving benefit clients 
hand lack coordination cache contents may cause unnecessary data duplication best advantage system memory avoid disk accesses leff fran 
algorithms attempt address lack coordination 

centrally coordinated caching centrally coordinated caching adds coordination greedy forwarding algorithm statically partitioning client cache locally managed section managed greedily client globally managed section coordinated server extension central cache 
client find block locally managed cache sends request server 
server desired data memory supplies data 
server checks see stored block centrally coordinated client memory 
locates data client memory forwards request client storing data 
fails server supplies data disk 
centrally coordinated caching behaves physically moving memory clients server 
server manages globally managed fraction client cache global replacement algorithm 
server evicts block local cache room data fetched disk sends victim block replace block blocks centrally coordinated distributed cache 
server forwards client request distributed cache entry entry lru list global distributed cache 
noted simulate policy server manages client cache 
primary advantage centrally coordinated caching high global hit rate achieve global management bulk memory resources 
main drawbacks approach clients local hit rates may reduced local caches effectively smaller central coordination may impose significant load server 

chance forwarding final algorithm quantitatively evaluate chance forwarding dynamically adjusts fraction client cache managed cooperatively depending client activity 
chance algorithm modifies greedy forwarding algorithm clients cooperate preferentially cache blocks stored client cache 
chance forwarding works greedy forwarding 
chance forwarding attempts avoid discarding client memory 
client discards block checks see block copy cached client 
check may require message server may done consulting flags associated block described 
block throw block away client sets block recirculation count forwards data random peer sends server message telling block moved 
peer receives data adds block lru list block referenced 
block reaches lru list count decremented forwarded count zero case simply discarded 
client resets block recirculation count caches data normally client cooperatively caching discards block cache 
parameter indicates times allowed different clients lru lists referenced discarded 
greedy forwarding simply degenerate case algorithm 
noted simulations 
algorithm provides dynamic trade client cache allocation local data data cached client referenced global data cached aggregate system performance 
active clients tend force global data sent caches quickly local displace global data 
idle clients tend accumulate global blocks hold memory long periods time 
enhancement algorithm preferentially forward idle clients avoid disturbing active clients 
study clients forward uniformly randomly clients system 
implementation algorithm prevent ripple effect block forwarded client displaces block client 
note common case displaced block copy data ripple occurs simulate policy prevents deep recursion occurring client receiving block allowed forward block space 
client receives block uses modified replacement algorithm discarding oldest duplicated block 
cache contains duplicated blocks client discards oldest block fewest remaining 
optimizations algorithm reduce amount communication server 
cache client combines messages server updating server directory client cache contents message requests data satisfy update indicates block client discarded cache block forwarded 
second set optimizations reduces number messages asking server block cached copy client deciding block discarded 
block recirculation count set server message necessary decide fate 
non blocks client usually send message server determined block client discard forward message needed block lifetime cache 
special case client making space kicked client cache discard blocks discovers mark blocks setting recirculation count need ask server client block 
client block server forwards request client resets flag 
main advantage chance forwarding provides simple dynamic trade client private cache data data cached system 
favoring provides better performance simple greedy algorithm discarding potentially expensive discarding duplicated block duplicate satisfied client memory leff 
potential disadvantage approach block may bounced multiple caches living cooperative portion caches resulting unnecessary system load 

algorithms considered cooperative caching algorithms 
performance measurements algorithms omitted report performed similarly algorithms examined 
hash distributed caching differs centrally coordinated caching hash distributed caching partitions centrally managed cache block identifiers client managing partition cache 
central server sends blocks displaced local cache client selected hashing block identifier 
local client accesses distributed cache sending request directly appropriate client 
client supplies data currently caching block forwards request server block 
simulations indicate hash distributed caching provides nearly identical hit rates compared centrally coordinated caching fixed partitioning centrally managed cache hurt hit rate 
main advantage hash distributed caching centrally coordinated caching hash distributed caching significantly reduces server load requests satisfied cooperative cache don go server 
examined weighted lru dynamic algorithm attempts replace object globally lowest value cost ratio 
chance objects duplicated multiple client caches valuable copy discarded data may fetched client memory 
hand cached copy block valuable loss cause disk access 
opportunity cost keeping object memory cache space con time block referenced smit approximately time 
weighted lru explicitly balances keeping frequently duplicates avoid network accesses keeping frequently avoid disk accesses 
traces response time slightly worse substantially simpler chance forwarding 

simulation methodology trace driven simulation evaluate cooperative caching algorithms 
simulator tracks state caches system monitors requests hit rates seen client 
assume cache block size kb allow partial blocks allocated files smaller kb 
verified simulator synthetic workload described leff input 
calculate response times multiplying local memory remote client memory server memory disk hit rates times takes access memories 
baseline technology assumptions mbit atm columns kb block fetched local memory fetch remote memory takes additional plus network hop average disk access takes summarizes access times different resources algorithms 
section examine sensitivity results technology changes 
note include queueing delays response time figures 
attractive algorithms studied increase server load emerging high performance networks switched topology expect queueing significantly alter results 
maintain data consistency writes assume data modifications written central server client caches kept consistent write invalidate protocol arch 
focus read performance delayed write write back policy affect results 
local mem 
remote client mem 
server mem 
server disk direct greedy central chance 
access times different levels memory hierarchy different cooperative caching algorithms 
differences remote client times different algorithms depends number network hops reach data algorithm 
results traces sprite workload described detail baker bake 
sprite user community included full time part time users system 
users included operating systems researchers computer architecture researchers vlsi designers including administrative staff graphics researchers 
traces list activity client machines server day period measured sprite operating system 
contain read write block accesses accesses little day warm simulated caches 
section describes simulation results additional workload 
reporting results compare set baseline cache management assumptions unrealistic best case model 
base case assumes client cache central server cache cooperative caching 
unrealizable best case assumes cooperative caching algorithm able achieve global hit rate high client memory managed single global cache local hit rates client memory managed private local cache 
best case provides lower bound response time cooperative caching algorithms physically distribute client memory equally client lru replacement 
simulate algorithm doubling client local cache allowing clients manage half locally allowing server manage half globally centrally coordinated case 
best case assume data remote client memory fetched network hops request forward reply total remote memory hit 

simulation results section presents main results simulation studies cooperative caching 
section compares different cooperative caching algorithms base case unrealizable best case 
clarity comparison assuming particular set parameters algorithm set technology memory assumptions single workload 
section examines individual algorithms closely studying different values algorithms parameters 
section examines sensitivity results technology memory assumptions client cache size server cache size hardware performance 
section examines algorithms additional workload 
section summarizes results highlights compares cooperative caching moving system memory server 

baker traces included requests auxiliary servers 
just accesses main server trace 

comparison algorithms section compares algorithms response times hit rates server loads impact individual clients 
initial comparison algorithms fixes client caches mb client fixes server cache mb sprite workload 
direct cooperation algorithm optimistic assumption clients interfere remote caches simulate assumption allowing client maintain permanent remote cache size equal local memory size effectively doubling size client cache 
central coordination algorithm assume client cache memory dedicated cooperative cache managed locally 
chance algorithm choose recirculation count unreferenced data passed random caches purged memory 
examine appropriate parameters section 
illustrates response times algorithms examined compares times base case left best case right 
direct cooperation provides small speedup compared base case despite optimistic assumptions algorithm 
greedy forwarding shows modest significant performance gain speedup 
algorithms coordinate cache contents reduce redundant cache entries show impressive gains 
central coordination provides speedup chance forwarding provides performance response time ms ms ms ms ms local server memory ms remote client server disk ms ms ms ms base direct greedy coord best chance algorithm 
average block read time 
bar represents time complete average read algorithms 
segments bars show fraction total read time data accesses satisfied local memory server memory remote client memory server disk 

speedup performance improvement figures terminology 
speedup defined execution time slower algorithm divided execution time faster algorithm 
performance improvement percentages calculated subtracting speedup multiplying get percentage 
improvement 
coordinated algorithms ideal cooperative caching response time 
apparent 
disk accesses dominant source latency base case efforts cooperative caching improve hit rate beneficial 
second dramatic improvements performance come coordinated algorithms system effort reduce duplication cache entries improve hit rate 
provides additional insight performance algorithms illustrating access rates different levels memory hierarchy 
total height bar represents rate algorithm local cache 
base direct cooperation greedy best case algorithms manage local caches greedily identical local rates 
central coordination local rate higher baseline local rate 
algorithm local deficiency aggressive coordination memory system providing combined memory rates essentially identical achieved best case just requests going disk 
disk access rate half rate base caching scheme 
chance algorithm emphasis holding data copies hurts local rate surprisingly small amount recirculation increases local rate access rate server memory remote client server disk base direct greedy coord best chance algorithm 
fraction requests satisfied level memory hierarchy different algorithms 
total height bar local rate algorithm 
sum server disk remote client segments shows rate combined local server memories 
bottom segment shows rate memories included disk access rate 

simulated local rate lower rate measured sprite machines bake simulate larger caches average mb caches observed study larger caches service requests server 

chance provides low disk access rate 
comparison static memory partition algorithm centralized coordination dynamic partition algorithm chance forwarding illustrates local global rates considered evaluating algorithms 
static algorithm provides lower disk access rates provides low rate significant cost local cache performance 
chance algorithm coordinates smaller fraction client cache contents protecting local cache hit rate sacrificing global hits 
important metric comparison server load imposed algorithms 
cooperative caching algorithm significantly increases server load increased queueing delays reduce performance gains 
illustrates relative server loads algorithms 
primarily interested verifying cooperative caching increased server coordination doesn greatly increase server load number simplifications server load calculations 
include load write backs deletes file attribute requests sources server load load comparison 
including loads add equally load algorithm reducing relative differences 
server load load hit server memory hit remote client hit disk base direct greedy coord best chance algorithm 
server loads algorithms percentage baseline cooperative caching server load 
hit disk segment includes network disk load requests satisfied server disk 
hit remote client segment shows server load receiving forwarding requests remote clients 
hit server memory segment includes cost receiving requests supplying data server memory 
local hits generate server load 
load segment includes server overhead invalidating client cache blocks answering client queries chance asks block cached copy 
base server load calculations network messages disk transfers server algorithm 
assume network message overhead costs load unit block data transfer costs load units 
small network message costs unit network data transfer costs overhead plus data transfer total units 
charge server load units disk data transfer 
results server load measurements suggest cooperative caching algorithms significantly increase server load response time approximation ignoring queueing delay provide valid comparisons base case 
centralized coordinated algorithm appear increase server load somewhat simple assumptions 
increase centralized algorithm significantly increases local rate local misses sent server 
detailed measurements determine centralized algorithm implemented increasing server queueing delays 
final comparison algorithms examines individual client performance aggregate average performance 
illustrates relative performance individual clients cooperative direct client cooperation number client reads slowdown speedup slowdown speedup greedy forwarding number client reads centrally coordinated chance number client reads number client reads 
performance individual client 
point represents speedup slowdown seen client cooperative caching algorithm compared client performance base case 
speedups line slowdowns 
client slowdown defined inverse speedup speedup 
axis indicates number read requests client relatively inactive clients appear near left edge graph active clients appear right 
slowdown speedup slowdown speedup caching algorithm compared client performance base case 
graph positions data points clients inactive clients appear left graph active clients right 
speedups slowdowns inactive clients may significant clients spending relatively little time waiting file system case inactive clients response times significantly affected adding just disk accesses 
important aspect individual performance fairness clients significantly worse contribute resources community managing local caches greedily 
fairness important average client performance improved clients may refuse participate cooperative caching performance worse 
data suggest fairness widespread problem workload 
direct client cooperation centrally coordinated caching slow clients modest amounts 
greedy forwarding chance forwarding harm clients workload 
expect algorithms greedy client cache management fair direct client cooperation causes clients suffer worse performance additional cooperative cache memory 
clients benefit greatly cooperative cache memory lower server cache hit rates direct client cooperation base case 
lower server hit rates occur accesses server cache clients system filtered effectively larger local caches reducing correlation client access streams server 
chance centrally coordinated algorithms disturb local greedy caching significant improvements global caching provide net benefit clients 
chance forwarding hurts clients workload centrally coordinated caching damages response client 
algorithms help client working set fits completely local cache client hurt interference local cache contents 
chance forwarding interferes local caching centrally coordinated caching indicated unfair individual clients 
algorithms statically partition client memory hash distributed caching physically moving cache clients server suffer vulnerability centrally coordinated caching 

detailed algorithm analysis subsection examines cooperative caching algorithms detail evaluates sensitivity algorithm specific parameters 
direct client cooperation direct client cooperation simple achieving modest response time improvement seen may difficult 
results optimistic assumption clients recruit sufficient remote cache memory double caches interfering 
reality algorithm meet challenges provide modest gains 
difficulty direct client cooperation clients may able find remote memory significantly affect performance 
plots direct cooperation response time function amount remote memory recruited client 
instance clients recruit memory increase cache size mb response time improvement drops 
significant speedups achieved client able recruit mbs times size local cache 
interference clients limit direct client cooperation benefits 
client donating memory active flush client data memory 
client trying take advantage remote memory sees series temporary caches reducing hit rate new cache data 
studies workstation activity doug arpa suggest idle machines usually available length idle periods relatively short 
possible solution problem send evicted data new idle client discarding increase system complexity 
final challenge direct client cooperation dynamically selecting clients donate memory utilize remote memory 
prob speedup base direct client cooperation mb mb mb mb mb mb mb remote cache size client 
direct client cooperation speedup compared base case function client remote cache size 
circle indicates result mb client remote cache assumed algorithm previous section 
lem appears solvable active clients able recruit cooperative cache achieve maximum benefits available direct client cooperation trace 
hand implementation recruiting mechanism detracts algorithm simplicity may require server involvement 
greedy forwarding performance gains greedy algorithm modest greedy algorithm may attractive simplicity increase server load fair 
words performance improvement comes essentially free clients server modified forward requests server callback state expanded track individual blocks 
centrally coordinated caching centrally coordinated caching provide significant speedups high global hit rates 
hand devoting large fraction client cache centrally coordinated caching reduces local hit rate potentially increasing server load reducing performance clients 
fraction client cache treated centralized resource determines effectiveness algorithm 
plots response time centrally coordinated fraction increased 
fraction increased global hit rate improves reducing time spent fetching data disk 
time local hit rate decreases driving time spent fetching remote caches 
trends create response response time ms ms ms total disk ms centrally coordinated percent 
response time centrally coordinated caching depends percent cache centrally coordinated 
corresponds baseline cooperative caching case 
total time sum time requests satisfied disk time requests satisfied local remote memory 
rest study uses centrally coordinated fraction algorithm indicated circled points 
time plateau client local cache managed global resource 
note measurements take increased server load account increasing centrally managed cache fraction increases load central server local caches satisfy fewer requests 
effect may increase queueing delays server centrally managed fraction increased reducing speedups pushing break point smaller centrally managed fractions 
chose default centrally managed fraction appears stable part plateau different workloads cache sizes 
instance plateau runs workload mb client caches 
high centrally managed fraction tends achieve performance large disparity disk network memory access times compared gap network local memory 
network slower smaller percentage appropriate 
chance forwarding chance forwarding provides performance improving hit rates significantly reducing local hit rates 
algorithm server load fairness characteristics 
plots response time recirculation count parameter algorithm 
largest improvement comes increased zero greedy algorithm 
increasing count provides small improvement larger values little difference 
relatively low values effective data random ms ms ms total disk ms recirculation count 
response time chance algorithms depends number times unreferenced blocks random caches 
zero corresponds greedy algorithm recirculation 
total time sum time requests satisfied going disk requests satisfied local remote memory 
rest study uses recirculation count algorithm indicated circled points 
cache lands relatively idle cache remains memory significant period time flushed 
parameter random forwarding gives block relatively long period time idle cache 
higher values little additional difference blocks need third try find idle cache algorithm discards old cache items times avoid ripple effect caches 

sensitivity subsection explores sensitivity results assumptions client cache size central server cache size performance lan machines connected 
plots performance algorithms function size client local cache 
graph shows coordinated algorithms centralized coordination chance forwarding perform long caches reasonably large 
caches small coordinating contents client caches provides little benefit borrowing client memory causes large increase local misses little aggregate benefit reducing disk accesses 
simple greedy algorithm performs relatively range cache sizes 
illustrates effect varying size cache central server 
increasing server cache size significantly improves base cooperative caching case modestly improving performance cooperative algorithms global hit rates 
sufficiently large server caches cooperative caching provides benefit server cache large aggregate client caches 
large cache double system memory cost compared cooperative caching 
note server cache large centrally coordinated caching performs poorly degraded local hit rate 
response time ms ms ms ms ms ms ms base direct greedy centrally coordinated chance best ms mb mb mb mb mb client cache size 
response time function client cache memory algorithms 
graphs study assumed client cache size mb circled 
response time read response time ms ms ms ms ms ms ms base direct greedy centrally coordinated chance best ms mb mb mb mb gb server cache size 
response time total central server cache size 
circled points highlight results default mb server assumption 
ms ms ms ms base direct greedy ms centrally coordinated ms chance best ms ms ms remote memory time 
response time function network speed 
axis round trip time request receive kb packet 
disk access time held constant ms memory access time held constant rest study assumed hop plus block transfer total remote fetch time request reply excluding memory copy time indicated vertical bar 
chance best lines nearly overlap entire range graph 
emergence fast networks means time ripe utilizing cooperative caching file systems 
ethernet speed networks slow get large benefits cooperative caching emerging atm networks promise fast see significant improvements 
plots response time function network time fetch remote block 
ethernet speed network remote data access take nearly ms maximum speedup seen cooperative caching algorithm 
network fetch time reduced ms instance fast atm network peak speedup increases 
graph shows little benefit reducing network block fetch time network fast response time ms ms ms ms ms inferred local hits local server memory remote client server disk ms base direct greedy coord best algorithm chance 
response time algorithms workload 
inferred local hits segment indicates estimate amount time spent processing local hits appear incomplete traces assuming traced system local hit rate 
network times significant source delay compared constant memory disk times 
coordinated algorithm provide nearly ideal performance network fast chance forwarding appears sensitive network speed centrally coordinated caching 
centrally coordinated caching sense environments accessing remote data closer accessing local data going disk 
reduced local hit rate outweighs increased global hit rate 

berkeley workload response time results second workload called berkeley appear 
berkeley workload traces nfs file system network requests clients berkeley computer science division serviced file server 
workload interesting follows activity larger number clients includes longer period time sprite traces 
large number clients provide extremely large pool memory cooperative caching exploit 
traces cover day period include read write events warm caches 
trace taken snooping network include local hits adjust simulation account missing local accesses 
smith stack deletion method smit approximate response time results incomplete trace 
smith omitting hit small cache little difference number faults seen simulating larger cache 
actual rate accurately approximated dividing number faults seen simulating reduced trace actual number full trace 
refinement utilize read attribute requests trace accurately simulate local client lru lists 
nfs uses read attribute requests validate cached blocks referencing 
read attribute requests hint cached blocks file referenced block requests appear trace 
attribute requests provide approximation attribute cache hides attribute requests validated previous seconds read attribute requests really signify file cached blocks referenced allow infer missing block hits 
results workload approximate support results seen sprite workloads 
relative ranking algorithms workload sprite workload centrally coordinated caching chance forwarding nearly best case greedy algorithm provides significant speedups 
direct cooperation provides modest gains 
result insensitive hit rate assumed 
predicted speedup factors workload depend hit rate assumed significant wide range assumed local hit rates 

summary chance forwarding relatively simple algorithm appears provide performance wide range conditions 
centrally coordinated caching omitted hash distributed caching provide performance degrade performance individual clients depend heavily fast network performance reduced local hit rates impose 
weighted lru algorithm results omitted performs similarly chance algorithm complicated may load server requests information global state 
greedy forwarding algorithm appears algorithm choice simplicity primary concern 
direct cooperation algorithm simple satisfying demands cooperative caching interfering client activities may difficult particularly direct algorithm locate mb mb remote memory active client equal greedy algorithm performance 
consider alternative cooperative caching physically moving memory central server 

unfortunately trace indicate total number 
results assume hidden hit rate approximate rate simulated sprite trace giving maximum speedup chance forwarding 
local hit rate higher bars slightly larger constant added differences algorithms smaller local hit rate reduces chance speedup 
local hit rate lower differences magnified local hit rate gives chance speedup 
approach similar centrally coordinated algorithm provides similar performance moving client memory server yields improvements standard memory distribution sprite workloads respectively 
speedups nearly equal speedups chance algorithm fall short chance algorithm reduced local hit rates resulting smaller local caches 
moving large fractions clients caches server number disadvantages compared cooperative caching algorithm chance forwarding static allocation global local caches provide bad performance individual clients seen centrally coordinated caching 
system cache memory server clients sensitive network speed seen centrally coordinated caching 
ratio network performance local memory performance reduced moving memory server attractive 
reducing size client local caches transferring data server increase server load 
read load traditional caching system enlarged central cache higher chance forwarding sprite workload 
memory physically moved central server file system cache clients activities 
cooperative caching hand may allow client cache memory released client virtual memory system demands warrant nels 
configuring servers large amounts memory may cost effective spreading amount memory clients 
instance mb cache memory clients trace gb memory demanding extremely expandable potentially expensive server 

related evaluates performance benefits implementation issues cooperative caching 
primary contributions evaluating realistic management algorithms real file system workloads systematic exploration implementation options 
leff leff leff leff investigate remote caching architectures form cooperative caching analytic simulation models synthetic workload 
important characteristics workload access probabilities object client fixed time client knew distributions 
leff clients base caching decisions global knowledge clients caching achieve nearly ideal performance clients decisions strictly local basis performance suffered greatly 
differs leff studies number important ways 
uses actual file system traces workload allowing quantify benefits cooperative caching realizable real workloads 
second major feature study focused getting performance controlling amount central coordination knowledge required clients focusing optimal replacement algorithms 
franklin fran examined cooperative caching context client server data bases clients allowed forward data avoid disk accesses 
study synthetic workloads focused techniques reduce replication clients caches server cache 
server attempt coordinate contents clients caches reduce replication data clients 
forwarding sending dropped pages algorithm similar chance forwarding algorithm send copy block server cache client 
blaze proposed allowing file system clients supply hot data local disk file caches 
focus reducing server load improving responsiveness 
client client data transfers allowed dynamic hierarchical caching avoided store forward delays experienced static hierarchical caching systems 
idea forwarding data cache build scalable shared memory multiprocessors 
dash hardware implements scheme similar greedy forwarding dirty cache lines 
policy avoids latency writing dirty data back server shared 
optimization cooperative caching file system uses delayed writes 
cache memory architecture coma designs relied cache cache data transfers hage rost 
researchers examined idea remote client memory disk virtual memory paging 
felten zahorjan felt examined idea context traditional lans 
schilit duchamp scrutinized remote memory paging allow diskless portable computers iftode li petersen explored memory servers parallel supercomputers 
comer griffioen propose communications protocol remote paging come 

advent high speed networks provides opportunity clients closely significantly improve performance file systems 
investigated technique cooperative caching con clude cooperative caching reduce read response times nearly factor workloads studied relatively simple algorithm allows clients efficiently manage shared cache 
acknowledgments owe special david black osf working osdi shepherd 
fred douglis john howard edward lee john ousterhout anonymous osdi referees comments improved content presentation 
grateful mary baker john hartman michael kupfer ken shirriff john ousterhout making sprite traces available 
matt blaze providing tools gather traces 
arch james archibald jean baer 
cache coherence protocols evaluation multiprocessor simulation model 
acm transactions computer systems november 
arpa arpaci amin vahdat thomas anderson david patterson 
combining parallel sequential workloads network workstations 
technical report computer science division university california berkeley 
bake mary baker john hartman michael kupfer ken shirriff john ousterhout 
measurements distributed file system 
proc 
th symposium operating systems principles pages october 
matthew addison blaze 
caching large scale distributed file systems 
phd thesis princeton university january 
come douglas comer james griffioen 
efficient order dependent communication distributed virtual memory environment 
symp 
experiences distributed multiprocessor systems iii pages march 
dahl michael dahlin clifford randolph wang thomas anderson david patterson 
quantitative analysis cache policies scalable network file systems 
proc 
sigmetrics pages may 
doug fred douglis john ousterhout 
transparent process migration design alternatives sprite implementation 
software practice experience july 
felt edward felten john zahorjan 
issues implementation remote memory paging system 
technical report dept computer science university washington march 
fran michael franklin michael carey miron livny 
global memory management client server dbms architectures 
proc 
international conference large data bases pages august 
hage erik anders landin seif haridi 
ddm cache memory architecture 
ieee computer september 
john hennessy david patterson 
computer architecture quantitative approach 
morgan kaufmann publishers 
john howard michael kazar menees david nichols satyanarayanan robert sidebotham michael west 
scale performance distributed file system 
acm transactions computer systems february 
iftode kai li karin petersen 
memory servers multicomputers 
proc 
compcon pages 
leff avraham leff philip yu joel wolf 
policies efficient memory utilization remote caching architecture 
proc 
international conf 
parallel distributed information systems pages december 
leff avraham leff joel wolf philip yu 
replication algorithms remote caching architecture 
ieee trans 
parallel distributed systems november 
leff avraham leff philip yu joel wolf 
performance issues object replication remote caching architecture 
computer systems science engineering january 
lenoski laudon gharachorloo gupta hennessy 
directory cache coherence protocol dash multiprocessor 
proc 
th international symposium computer architecture pages may 
michael litzkow marvin solomon 
supporting checkpointing process migration outside unix kernel 
proc 
winter usenix pages january 
mart richard martin 
active message layer network hp workstations 
proc 
hot interconnects august 
muntz honeyman 
multi level caching distributed file systems cache ain trash 
proc 
winter usenix pages january 
matthew mutka miron livny 
available capacity privately owned workstation environment 
performance evaluation july 
nels michael nelson brent welch john ousterhout 
caching sprite network file system 
acm transactions computer systems february 
david nichols 
idle workstations shared computing environment 
proc 
th symposium operating systems principles pages october 
rost smirni wagner 
ksr experimentation modeling 
proc 
acm sigmetrics pages june 
chris ruemmler john wilkes 
unix disk access patterns 
proc 
winter usenix pages january 
sand russel sandberg david goldberg steve kleiman dan walsh bob lyon 
design implementation sun network filesystem 
proc 
summer usenix pages june 
bill schilit dan duchamp 
adaptive remote paging mobile computers 
technical report cucs dept computer science columbia university february 
smit alan jay smith 
methods efficient analysis memory address trace data 
ieee transactions software engineering se january 
smit alan jay smith 
long term file migration development evaluation algorithms 
computer architecture systems august 
marvin theimer keith 
finding idle machines workstation distributed system 
ieee transactions software engineering november 
thorsten von eicken david culler seth copen goldstein klaus erik schauser 
active messages mechanism integrated communication computation 
proc 
asplos pages may 
wang randolph wang thomas anderson 
xfs wide area mass storage file system 
fourth workshop workstation operating systems pages october 
zhou zhou wang zheng pierre delisle 
utopia load sharing facility large heterogeneous distributed computer systems 
software practice experience december 
