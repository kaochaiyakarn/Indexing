probabilistic approach full text document clustering moises goldszmidt sri international ravenswood ave menlo park ca moises erg sri com addressing issue text document clustering suitable function measuring distance documents needed 
explore function scoring document similarity probabilistic considerations similarity scored expectation words appearing documents 
score enables investigation different smoothing methods estimating probability word appearing document purposes clustering 
experimental results show different smoothing methods may effective depending degree separability clusters 
furthermore show cosine information retrieval associated particular form probabilistic smoothing model 
introduce specific scoring function outperforms cosine coefficient extensions tfidf weighting experiments document clustering tasks 
new scoring normalizing probabilistic sense cosine similarity score adding scaling factor characteristics corpus clustered 
experiments indicate model assumes asymmetry positive word appearance negative word non appearance information document clustering task outperforms standard mixture models weight information equally 
amount line information continues grow increasing rate need tools help manage information rises 
tool ability cluster documents similar content aid retrieval presentation information user 
early information retrieval ir stressed clustering means improving ability find documents relevant query van rijsbergen jardine salton 
cluster hypothesis van rijsbergen states closely associated documents tend relevant requests working assumption document collections clustered priori new queries simply matched clusters document individually 
serve speed retrieval mehran sahami gates building computer science department stanford university stanford ca sahami cs stanford edu process possibly find relevant documents explicitly contain words user query 
applications document clustering scatter gather cutting hearst pederson means allowing entire collections query retrieval results browsed easily :10.1.1.34.6746
area shown document clustering effective way give user greater sense topics set documents pirolli 
success systems hinges effectiveness clustering methods employed 
long history empirical document clustering excellent survey willett 
description scatter gather particular clustering methods reflecting results years comparative ir community continues today schuetze silverstein 
empirical document clustering advanced state art performance equivalent advancement theoretical analysis explain methods arrived experimentation 
seek provide foundational analysis document clustering tools probability theory 
way formalize assumptions models document clustering 
objective gain new insights effectiveness current clustering algorithms open door improved founded extensions 
making explicit text clustering algorithms prompted investigate issues treatment evidence different approaches density estimation 
consequently propose probability score document overlap outperformed traditional ir methods experiments text clustering 
general terms clustering problem consists finding groups data points possess similarities 
problem formalized define meant similarity 
practice formalization involves separate issues measure similarity data samples second evaluate partitioning set samples clusters 
working context document clustering propose probabilistic score measuring similarity documents evaluation clustering partition 
context generally ir commonly measure similarity obtained representing documents normalized vectors computing inner product find cosine angle vectors 
generally referred cosine coefficient salton 
dimension vector corresponds distinct word union words corpus clustered 
document represented vector containing normalized frequency counts words 
intuitively measure tries capture degree word overlap documents 
similar considerations investigate probabilistic function document overlap scores expectation words appearing documents 
score prompts investigation different smoothing methods estimating probability word appearing document 
empirical evaluation shows different smoothing methods may effective depending degree separability clusters 
show widely cosine coefficient associated particular form framework 
analysis reveals scaling factor inverse probability word appearing corpus combined probabilistic similarity score yields clustering method outperforms cosine coefficient tfidf weighting salton buckley experiments 
experiment alternative probabilistic approaches mixture models autoclass cheeseman showing generally produce inferior results 
point probabilistic score easily extended include sophisticated notions document overlap equivalence classes words synonyms phrases general function groups words corpus 
way score cleanly capture full generality probabilistic indexing fuhr techniques contexts 
parameters defining contributions different words functional characteristic documents similarity score cases learned directly data 
clear advantage probabilistic score possibility cleanly fusing information coming different modalities example video audio similarity scores multimedia domains 
issues focus current research 
probabilistic document overlap formalize problem document clustering need explicitly define notion similarity documents 
similarity function clustering establishing degree overlap pairs documents 
assume document imposes multinomial distribution set words corpus 
document associated dimensional feature vector di 
dimension vector corresponds distinct word union words corpus 
value th component vector number times word corresponding component appeared document 
vector representation documents provides sufficient statistics computing expected overlap pair documents 
documents corpus compute expected overlap terms corresponding vectors di dj 
denote expected overlap measure eo di dj compute di dj yi yj yi denotes event word selected document equal model contains information corpus including total number times word appears corpus information partitioning documents clusters 
equation intuitively appealing 
says overlap documents computed estimating probability word appears document multiplying results 
seen shortly way probability estimated greatly influence results clustering 
focus different ways estimating probabilities statistics vector di relation cosine coefficient salton 
presently provide derivation eq 

deriving probabilistic overlap investigate possible derivation eq 
reveal underlying assumptions 
start defining expected degree overlap documents corpus corresponding vectors word statistics di dj 
yi di yj dj yi yj dj rewritten yi di yj yj di dj yi yj dj event yi di denotes word assigned yi appears di non zero count events yi di yj dj clearly independent conditioned yi yj vectors statistics di dj 
value yi di depends choice yi model yi di yj dj yi yj dj note yi di yj dj jyj dj simply indicator functions limiting set words contribute sum di dj 
reduces sum di dj yi yj dj bayes theorem summation equal di dj di yj yi yj di assumption information documents independently distributed statistics different documents independent di 
probability theory write di yj yi yj yj note di dj result yi yj captured effect single word believe effect small especially documents distinct words approximation yi yj yj substituting approximation eq 
yields di yj final assumption statistics corpus part model probability drawing word different documents independent events 
yi yj yi yj substituting eqs 
eq 
yields di dj yi yj di yi dj yj yi yj equal eq 

pointed derivation embodies series assumptions probabilistic independence 
assume example probability statistics different documents di independent information assume probability statistics remain independent additional information particular word drawn 
relation establish section assumptions computation similarity cosine coefficient 
analysis merely assumptions explicit opening opportunities research verifying finding ways avoid making 
research scope 
probability estimation smoothing focus estimating term wjd eq 

initial approach take maximum likelihood ml estimate probability pml wjd number times word appears document doc represented vector 
bound poor estimate words important topic document may appear times terms may appear 
shorter documents news clips estimate prone spiky high variance 
trying control variance estimating wjd critical perform form smoothing 
simple smoothing technique context computational linguistics charniak arithmetic mean am pml wjd maximum likelihood estimate unconditional distribution pml doc pml doc doc drop subscript readability 
case ml estimate appropriate computation average documents entire corpus attenuate word spikes may appear single document 
formally arithmetic mean smoothing yields pam wjd pml wjd pml form smoothing involves geometric mean gm ml distributions pgm wjd pml wjd pml gm estimate eq 
define true probability distribution generally sum 
introduce true probability distribution geometric mean simply adding normalization factor 
gives normalized geometric mean estimate wjd pml wjd pml pml wjd pml continue pursue unnormalized gm formulation related computation similarity documents normalized vector dot product known cosine coefficient 
consider similarity score documents computed cosine represented eq 
di dj di di dj dj sum equation reduced include words di dj words documents documents influence sum 
furthermore cosine similarity score information retrieval clustering raw frequency scores features document vector 
frequencies attenuated monotone shrinkage factor log square root 
reported document clustering task square root generally appears give better performance log cutting :10.1.1.34.6746
incorporating factor eq 
yields di di dj di di dj dj dj di dj di di di dj dj dj equivalent arithmetic mean log space log pgm wjd log pml wjd log pml cast eq 
terms unnormalized gm estimate defined obtain di dj pgm yi pgm yj pml cosine similarity metric square root dampening empirical success ir community seen form geometric smoothing account high variability word appearances 
furthermore framing cosine probabilistic framework uncovers scaling factor axes word space 
intuitively scaling sense incorporates additional knowledge form frequency word usage corpus clustered 
experiments evaluate expected overlap eq 
various estimation smoothing proposals introduced section plus variant incorporates scaling factor eq 

seen best results obtained eq 
augmented scaling factor eq 
denominator 
clustering algorithms having defined similarity score documents turn problem actual document clustering algorithms 
number methods clustering exist widely applied text domains hierarchical agglomerative clustering iterative clustering techniques means rasmussen 
methods rely definition similarity score pairs documents generality refer sim doc doc subsequently instantiate measure probabilistic overlap different probability estimation methods 
hierarchical agglomerative clustering common clustering method employed information retrieval community past decade hierarchical agglomerative clustering hac frakes baeza yates 
family methods begins placing document distinct cluster 
pair wise similarities clusters computed closest clusters merged new cluster 
process computing pair wise similarities merging closest clusters repeatedly applied generating dendogram structure simply contains cluster encompassing data root 
selecting appropriate level granularity dendogram possible generate partitioning clusters desired 
criteria minimum number documents cluster prevent outlier documents considered separate cluster 
experiments heuristically set minimum cluster size documents 
depending similarity document cluster defined obtain different flavors hac common ones single link complete link group average methods 
previous ir willett pointed group average method generally produces superior results 
concentrate method 
group average method defines similarity document doc cluster average pair wise similarities doc documents sim doc doc jcj sim doc doc simple group average method document cluster equally representative cluster 
evident jcj weighting term sum 
note obtain variations hac replacing term jcj alternate distributions weight documents cluster gaussian document distance cluster centroid 
iterative clustering iterative clustering techniques referred reallocation methods attempt optimize clustering repeatedly reassigning documents cluster similar 
general form algorithms specification number clusters 
initialize clusters 

document doc compute similarity doc cluster 

assign document doc cluster similar 

goto convergence criterion satisfied 
case hac define similarity document cluster group average similarity 
exit criterion step simply run algorithm iterations observed fewer needed convergence 
note initialization step affect convergence point algorithm 
experimented various runs random initial clusters hac method find initial clustering 
results comparable cases worse 
reasons space report experiments hac determined initial set clusters 
random assignment documents clusters simple method initialization 
results objective experiments describe section test different estimation schemes computation expected overlap documents 
interested evaluating effect axis scaling expected overlap measure revealed derivation cosine coefficient 
seen results scaled score overlap performs better case dramatically better score tested including cosine coefficient tfidf weighting method commonly ir salton buckley 
realizing methods evaluating clustering algorithms controversy strategy aware limitations 
previously labeled data measure clustering recover known label structure data 
fixed number clusters number known class labels data 
clustering algorithm information true label document 
clustering completed set predicted label documents cluster true label majority documents cluster 
actual predicted label document simply compute classification error 
gives baseline maximal error dataset get instances classified majority class 
experiments conducted various subsets reuters dataset 
selecting corpus reuters news articles expect labeling reflect semantic coherence trusted evaluation 
datasets created considering documents particular subset class labels reuters collection 
applied simple pre processing feature selection datasets standard zipf law analysis eliminate words appeared fewer greater times providing little discriminating power documents 
description datasets table 
seeking characterize datasets study difficulty recovering underlying class structure measured ratio average inter label similarity average intra label similarity 
value shown table captures relative difficulty expect measure similarity dataset 
value increases indicates documents class appear similar documents outside class making recovery true class structure difficult 
values find updated version dataset reuters publically available david www research att com lewis dataset docs words categories baseline dataset nat gas soybean dlr dataset gold coffee sugar dataset yen reserves dataset gnp sugar dataset loan interest money fx table datasets clustering experiments 
dataset ml am ml am cos tfidf datasets clearly order increasing difficulty similarity measures 
datasets show relative variability reflected results experiments 
empirically evaluated measure probabilistic overlap number different estimation schemes 
computed document overlap ml am gm estimates wjd 
cases scaling axes word space runs referred indicate unscaled 
modified computation include scaling factor marginal probability word appearance yielding di dj yi yj pml table label label average similarity 
identify runs indicate scaled 
comparison performed clustering cosine coefficient similarity score square root dampening eq 

recognizing tfidf weighting ir literature salton buckley alternate means term scaling weighting scheme conjunction cosine rule square root dampening similarity score compare 
tfidf weighting commonly scheme tf idf log nw wheren total number doc uments nw number documents word appears 
error rates clustering hac table iterative clustering hac initialization table 
note results applying iterative optimization performing hac leads improved results seen looking reduction error rates tables 
focus attention second tables 
axis scaling improves performance similarity measure ml am estimates 
matter fact error rate reduced cases drastically increased cases slightly remains unchanged case 
investigate measurable characteristic datasets point benefit scaling performed chi squared test dataset 
purpose test check hypothesis marginal probabilities word dataset uniformly distributed case expect scaling help 
expected hypothesis uniformity rejected dataset error probability second important highlights utility similarity score 
general scaled probabilistic similarity measures ml am perform extremely comparison cosine tfidf similarity scores currently state art information retrieval 
significantly draw readers attention similarity score produces error rate comparable significantly cosine tfidf methods 
noting cosine equivalent scaled unnormalized gm estimate see normalization obtain true probabilities case preserve clean understood overlap measure significant beneficial impact empirical performance 
dataset ml am ml am cos tfidf table error rates hierarchical agglomerative clustering 
dataset ml am ml am cos tfidf alternative probabilistic models alternative approach text clustering modeling autoclass cheeseman 
investigations approach documents represented binary vectors word frequency counts 
autoclass cluster documents mixture independent binomial distributions word appearances 
representation immediate consequences loses word frequency information second evidence word appears document treated symmetrically 
loss word frequency estimation may remedied complex statistical models parametric distributions gaussians word frequencies fit data 
approach requires commitment particular parametric model word appearance 
initial investigation lines gaussian distributions indicates venue may promising 
context text clustering symmetrical treatment evidence problematic 
symmetry mean word appearance absence weight binomial distribution described 
expect appearance particular words text indicative particular topic absence word 
note single multinomial proposed overlap score places importance information appearance words absence 
matches intuitions word usage text 
test arguments converted datasets previously described binary representation 
objec table error rates hac seed iterative clustering 
tive compare probabilistic models fair grounds removing word frequency information 
clustered data cosine similarity scores 
hac hac followed methods 
likewise ran autoclass proper number clusters find priori initial clusters set results hac randomly 
help alleviate problems bad initial conditions random case ran autoclass multiple times different random initial clusters report results best clustering chosen autoclass model selection criterion 
results experiments table 
expected lack word frequency information generally hinders cosine non autoclass clustering regimes 
striking poor performance autoclass text datasets 
autoclass random initialization fails find real structure datasets 
furthermore reasonable provided autoclass hac cosine outputs final clusters worse 
aside note intuition asymmetry evidence important purpose text clustering categories discovered generally hold true text classification tasks categories known priori 
observed empirically successful application symmetric probabilistic models classification problems text lewis ringuette koller sahami domains friedman geiger goldszmidt 
full discussion point scope 
hac hac iter autoclass dataset cos cos cos random probability score document similarity quite effective clustering 
shown widely cosine similarity coefficient captured particular form probability estimation framework 
formulation cosine revealed scaling factor effectively harnessed yield results superior traditional ir methods 
seek extend probabilistic similarity score include arbitrary functions words documents phrases logical operations 
done expanding domain multinomial distributions currently compute expected document overlap 
way easily incorporate information word frequencies similarity score 
wish extend similarity measure problems domains video segmentation different estimation techniques appropriate 
promising initial results collaboration colleague 
long term goal plan leverage wellunderstood probabilistic semantics model develop clean fusion information different modalities aid multi media information retrieval 
charniak 
statistical language learning 
cambridge ma mit press 
cheeseman kelly self stutz taylor freeman 
autoclass bayesian classification system 
proceedings machine learning 
cutting karger pederson tukey 
scatter gather cluster approach browsing large document collections 
proceedings acm sigir 
frakes baeza yates 
information retrieval data structures algorithms 
prentice hall 
colleague remain nameless reasons anonymity 
table error rates binary data 
friedman geiger goldszmidt 
bayesian network classifiers 
machine learning 
fuhr 
models retrieval probabilistic indexing 
information processing management 
hearst pederson 
reexamining cluster hypothesis scatter gather retrieval results 
proceedings acm sigir 
koller sahami 
hierarchically classifying documents words 
proceedings machine learning 
lewis ringuette 
comparison learning algorithms text categorization 
proceedings sdair 
pirolli schank hearst diehl 
scatter gather browsing communicates topic structure large text collection 
proceedings chi 
rasmussen 
clustering algorithms 
frakes baeza yates eds information retrieval data structures algorithms 
prentice hall 
salton buckley 
term weighting approaches automatic text retrieval 
technical report cornell university computer science department 
salton 
smart information retrieval system 
englewood cliffs nj prentice hall 
schuetze silverstein 
comparison projections efficient document clustering 
proceedings acm sigir 
van rijsbergen jardine 
hierarchic clustering information retrieval 
information storage retrieval 
van rijsbergen 
information retrieval 
butterworths 
willett 
trends hierarchical document clustering critical review 
information processing management 
