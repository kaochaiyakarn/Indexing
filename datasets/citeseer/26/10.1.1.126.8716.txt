boosting algorithms gradient descent mason research school information sciences engineering australian national university canberra act australia anu edu au jonathan baxter research school information sciences engineering australian national university canberra act australia jonathan baxter anu edu au peter bartlett research school information sciences engineering australian national university canberra act australia peter bartlett anu edu au marcus frean department computer science electrical engineering university queensland brisbane qld australia elec uq edu au attention experimental theoretical focussed classification algorithms produce voted combinations classifiers 
theoretical shown impressive generalization performance algorithms adaboost attributed classifier having large margins training data 
algorithm finding linear combinations functions minimize arbitrary cost functionals functionals necessarily depend margin 
existing voting methods shown special cases algorithm 
previous theoretical results bounding generalization performance convex combinations classifiers terms general cost functions margin new algorithm doom ii performing gradient descent optimization cost functions 
experiments data sets uc irvine repository demonstrate doom ii generally outperforms adaboost especially high noise situations 
margin distribution plots verify doom ii willing give examples hard order avoid overfitting 
show overfitting behavior exhibited adaboost quantified terms proposed cost function 
considerable interest voting methods pattern classification predict label particular example weighted vote set base classifiers 
example freund schapire adaboost algorithm breiman bagging algorithm give significant performance improvements algorithms corresponding base classifiers led study related algorithms :10.1.1.29.9093:10.1.1.30.3515:10.1.1.133.1040:10.1.1.32.9399:10.1.1.49.2457:10.1.1.13.5087:10.1.1.156.2440:10.1.1.32.8918:10.1.1.131.1931
theoretical results suggest effectiveness algorithms due tendency produce large margin classifiers 
margin example defined difference total weight assigned correct label largest weight assigned incorrect label 
interpret value margin indication confidence correct classification example classified correctly positive margin larger margin viewed confident correct classification 
results show loosely speaking combination classifiers correctly classifies training data large margin error probability small :10.1.1.31.2869
mason bartlett baxter improved upper bounds misclassification probability combined classifier terms average training data certain cost function margins 
describes experiments algorithm directly minimizes cost function choice weights associated base classifier 
algorithm exhibits performance improvements adaboost suggests margin cost functions appropriate quantities optimize 
general class algorithms called anyboost gradient descent algorithms choosing linear combinations elements inner product space minimize cost functional 
component linear combination chosen maximize certain inner product 
specific case choosing combination classifiers optimize sample average cost function margin choice base classifier corresponds minimization problem involving weighted classification error 
certain weighting training data base classifier learning algorithm attempts return classifier minimizes weight misclassified training examples 
section give convergence results class algorithms 
section show general class algorithms includes special cases number popular successful voting methods including freund schapire adaboost schapire singer extension adaboost combinations real valued functions friedman hastie tibshirani logitboost :10.1.1.30.3515:10.1.1.156.2440:10.1.1.32.8918
algorithms implicitly minimize margin cost function gradient descent 
section experimental results particular implementation anyboost algorithm cost functions margin motivated theoretical results 
cost functions suggested results significantly different cost functions implicitly minimized methods described section 
experiments show new algorithm typically outperforms adaboost especially true label noise 
addition cost functions provide estimates error adaboost sense predict overfitting behaviour 
optimizing cost functions margin notation 
assume examples randomly generated unknown probability distribution theta space measurements typically space labels usually discrete set subset 
algorithm section apply different machine learning settings primary interest voted combinations classifiers form sgn tx ft sigma base classifiers fixed class wt classifier weights 
margin example respect classifier sgn defined yf 
set xm ym labelled examples generated wish construct voted combination classifiers form described pd sgn small 
probability incorrectly classifies random example small 
unknown training set take approach finding voted classifiers minimize sample average cost function margin 
training set want find mx yif xi minimized suitable cost function note symbol denote cost function real margin yf cost functional function interpretation meant clear context 
anyboost way produce weighted combination classifiers optimizes gradient descent function space idea proposed breiman 
treatment shows existing voting methods may viewed gradient descent suitable inner product space 
level view base hypotheses combinations elements inner product space 
case linear space functions contains lin set linear combinations functions inner product defined hf gi mx xi xi lin 
anyboost algorithm defined section convergence properties studied section valid cost function inner product 
suppose function lin wish find new add cost fflf decreases small value ffl 
viewed function space terms asking direction fflf rapidly decreases 
desired direction simply negative functional derivative gamma rc rc ff ff fi fi fi fi ff indicator function restricted choosing new function general possible choose gamma rc search greatest inner product gamma rc 
choose maximize gamma hrc motivated observing order ffl fflf ffl hrc greatest reduction cost occur maximizing gamma hrc preceding discussion motivates algorithm iterative algorithm finding linear combinations base hypotheses minimize cost 
note allowed base hypotheses take values arbitrary set restricted form cost inner product specified step sizes 
appropriate choices things apply algorithm concrete situations 
note algorithm terminates gamma hrc ft ft weak learner returns base hypothesis ft longer points downhill direction cost function 
algorithm terminates order step function space direction base hypothesis returned increase cost 
algorithm anyboost require ffl inner product space containing functions mapping set ffl class base classifiers ffl differentiable cost functional lin ffl weak learner accepts lin returns large value gamma hrc 
ft ft 
gamma hrc ft ft return ft choose wt 
ft ft wt ft return ft 
gradient descent view voting methods main aim optimization margin cost functionals restrict attention inner product cost sigma 
choices gamma hrc gamma mx yif xi yif xi sensible cost function margin monotonically decreasing gamma yif xi positive 
dividing gamma pm yif xi see finding maximizing gamma hrc equivalent finding minimizing weighted error xi yi yif xi pm xi case successful voting methods appropriate choice cost function step size specific cases anyboost algorithm 
table summarizes anyboost cost function step size settings needed obtain adaboost confidence rated adaboost arc logitboost algorithms :10.1.1.30.3515:10.1.1.156.2440:10.1.1.32.8918
detailed analysis algorithms specific cases anyboost full version 
convergence anyboost section provide convergence results anyboost algorithm quite weak conditions cost functional prescriptions step sizes wt results table existing voting methods viewed gradient descent optimizers margin cost functions 
algorithm cost function step size adaboost gamma yf line search arc gamma yf gamma yf line search logitboost ln gamma yf newton raphson convergence guarantees practice smaller necessary fixed small steps form line search :10.1.1.30.3515:10.1.1.133.1040:10.1.1.32.9399:10.1.1.156.2440
theorem proof omitted see supplies specific step size anyboost characterizes limiting behaviour step size 
theorem 
lin lower bounded lipschitz differentiable cost functional exists gamma rc lkf gamma lin 
sequence combined hypotheses generated anyboost algorithm step sizes wt gamma hrc ft ft anyboost halts round gamma hrc ft ft ft converges finite value lambda case limt hrc ft ft theorem proof omitted see shows weak learner find best weak hypothesis ft round anyboost cost functional convex accumulation point sequence ft generated anyboost step sizes global minimum cost 
ease exposition assumed terminating gamma hrc ft ft anyboost simply continues return ft subsequent time steps theorem 
lin convex cost functional properties theorem ft sequence combined hypotheses generated anyboost algorithm step sizes 
assume weak hypothesis class negation closed gamma round anyboost algorithm finds function ft maximizing gamma hrc ft ft 
accumulation point sequence ft satisfies sup gamma hrc inf lin experiments adaboost perceived resistant overfitting despite fact produce combinations involving large numbers classifiers 
studies shown case base classifiers simple decision stumps 
grove schuurmans demonstrated running adaboost hundreds thousands rounds lead significant overfitting number authors showed adding label noise overfitting induced adaboost relatively classifiers combination :10.1.1.131.1931
theoretical motivations described propose new algorithm doom ii specific case anyboost cost functional mx gamma tanh yif xi restricted convex combination classifiers base class adjustable parameter cost function 
henceforth refer normalized sigmoid cost function normalized weights normalized convex combination 
family cost functions parameterized qualitatively similar theoretically motivated family cost functions 
family practice may cause difficulties gradient descent procedure functions flat negative margins margins close 
normalized sigmoid cost function alleviates problem 
theoretical analysis viewed data dependent complexity parameter measures resolution examine margins 
large value corresponds high resolution high effective complexity convex combination 
choosing large value amounts belief high complexity classifier obtain large margins overfitting 
conversely choosing small value corresponds belief high complexity classifier obtain large margins overfitting 
implementation doom ii fixed small step size ffl experiments ffl 
practice fixed ffl replaced line search optimal step size round 
full details algorithm reader referred full version 
normalized sigmoid cost function non convex doom ii algorithm suffer problems local minima 
fact result shows cost functions satisfying gamma ff gamma ff algorithm strike local minimum step 
lemma 
cost function satisfying gamma ff gamma ff 
doom ii find optimal weak hypothesis time step terminate time step returning 
way avoiding local minimum remove round continue algorithm returning cost goes round 
local minimum cost guaranteed increase round 
continue step best available direction uphill direction eventually crest hill defined basin attraction classifier start decrease cost 
cost decreases classifier safely return classifier class available base classifiers 
course guarantee cost decrease classifier round 
practically problem small values cost function linear gamma case classifier corresponds global minimum anyway 
order compare performance doom ii adaboost series experiments carried selection data sets taken uci machine learning repository 
simplify matters binary classification problems considered 
experiments repeated times examples randomly selected training validation test purposes respectively 
results averaged repeats 
experiments axis orthogonal hyperplanes known decision stumps weak learner 
fixed complexity weak learner avoided problems complexity combined classifier dependent actual classifiers produced weak learner 
adaboost validation set perform early stopping 
adaboost run rounds combined classifier round corresponding minimum error validation set chosen 
doom ii validation set set data dependent complexity parameter 
doom ii run rounds optimal chosen correspond minimum error validation set rounds 
adaboost doom ii run data sets varying levels label noise applied 
summary experimental results shown 
improvement test error exhibited doom ii adaboost standard error bars shown data set noise level 
results show doom ii generally outperforms adaboost improvement pronounced presence label noise 
error advantage data set sonar cleve ionosphere vote credit breast cancer pima indians hypo splice noise noise noise summary test error advantage standard error bars doom ii adaboost varying levels noise uci data sets 
effect normalized sigmoid cost function exponential cost function best illustrated comparing cumulative margin distributions generated adaboost doom ii 
shows comparisons data sets label noise applied 
margin value curve corresponds proportion training examples margin equal value 
curves show trying increase margins negative examples adaboost willing sacrifice margin positive examples significantly 
contrast doom ii gives examples large negative margin order reduce value cost function 
margin breast cancer wisconsin noise adaboost noise doom ii noise adaboost noise doom ii margin splice noise adaboost noise doom ii noise adaboost noise doom ii margin distributions adaboost doom ii label noise breast cancer splice data sets 
adaboost suffer overfitting guaranteed minimize exponential cost function margins cost function certainly relate test error 
value proposed cost function correlate adaboost test error 
theoretical bound suggests right value data dependent complexity parameter cost function test error closely correlated 
shows variation normalized sigmoid cost function exponential cost function test error adaboost uci data sets rounds 
values curves averaged random train validation test splits 
value case chosen running doom ii various values choosing corresponding minimum error validation set 
curves show strong correlation normalized sigmoid cost right value adaboost test error 
data sets minimum adaboost test error minimum normalized sigmoid cost nearly coincide 
labor data set adaboost test error converges overfitting occur 
data set normalized sigmoid cost exponential cost converge 
vote data set adaboost initially decreases test error increases test error overfitting set 
data set normalized sigmoid cost mirrors behaviour exponential cost converges 
rounds labor adaboost test cost normalized sigmoid cost rounds vote adaboost test cost normalized sigmoid cost adaboost test error exponential cost normalized sigmoid cost rounds adaboost labor vote data sets 
costs scaled case easier comparison test error 
shown existing boosting type algorithms combining classifiers viewed gradient descent appropriate cost functional suitable inner product space 
anyboost algorithm type generating general linear combinations base hypothesis class 
prescription step sizes algorithm guaranteeing convergence optimal cost 
motivated main theoretical result derived doom ii specialization anyboost gamma tanh cost function margin experimental results uci datasets verified doom ii generally outperformed adaboost boosting decision stumps particularly presence label noise 
doom ii cost training data reliable predictor test error adaboost exponential cost 
inner product perspective boosting applied inner product space just spaces functions done 
explore possibility applying boosting machine learning settings 
acknowledgments research supported australian research council 
mason supported australian postgraduate research award 
jonathan baxter supported australian postdoctoral fellowship 
peter bartlett marcus frean supported institute advanced studies australian universities collaborative 
shai ben david stimulating discussion 
bartlett 
sample complexity pattern classification neural networks size weights important size network 
ieee transactions information theory march 
bauer kohavi :10.1.1.32.9399
empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
appear 
breiman :10.1.1.32.9399
bagging predictors 
machine learning 
breiman 
prediction games arcing algorithms 
technical report department statistics university california berkeley 
keogh blake merz 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
technical report computer science department oregon state university 
drucker cortes 
boosting decision trees 
advances neural information processing systems pages 
duffy helmbold 
geometric approach leveraging weak learners 
computational learning theory th european conference 
appear 
freund 
adaptive version boost majority algorithm 
proceedings twelfth annual conference computational learning theory 
appear 
freund schapire :10.1.1.133.1040
experiments new boosting algorithm 
machine learning proceedings thirteenth international conference pages 
freund schapire :10.1.1.32.8918
decision theoretic generalization line learning application boosting 
journal computer system sciences august 
friedman 
greedy function approximation gradient boosting machine 
technical report stanford university 
friedman hastie tibshirani :10.1.1.30.3515
additive logistic regression statistical view boosting 
technical report stanford university 
grove schuurmans 
boosting limit maximizing margin learned ensembles 
proceedings fifteenth national conference artificial intelligence pages 
maclin opitz 
empirical evaluation bagging boosting 
proceedings fourteenth national conference artificial intelligence pages 
mason bartlett baxter 
improved generalization explicit optimization margins 
machine learning 
appear 
mason baxter bartlett frean 
boosting algorithms gradient descent function space 
technical report australian national university 
quinlan 
bagging boosting 
proceedings thirteenth national conference artificial intelligence pages 
onoda 
uller 
soft margins adaboost 
technical report nc tr department computer science royal holloway university london egham uk 
schapire freund bartlett lee :10.1.1.31.2869
boosting margin new explanation effectiveness voting methods 
annals statistics october 
schapire singer :10.1.1.156.2440
improved boosting algorithms confidence rated predictions 
proceedings eleventh annual conference computational learning theory pages 
schwenk bengio 
training methods adaptive boosting neural networks 
advances neural information processing systems pages 
