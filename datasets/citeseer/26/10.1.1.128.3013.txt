learning mixtures trees marina automatic control computer science sti submitted department electrical engineering computer science partial fulfillment requirements degree doctor philosophy massachusetts institute technology january cfl massachusetts institute technology signature author 
department electrical engineering computer science january certified 
michael jordan professor department brain sciences thesis supervisor accepted 
arthur smith chairman departmental committee graduate students learning mixtures trees marina revised version thesis submitted department electrical engineering computer science january partial fulfillment requirements degree doctor philosophy challenges density estimation machine learning usually data multivariate dimensionality large 
operating joint distributions multidimensional domains raises specific problems encountered univariate case 
graphical models representations joint densities specifically tailored address problems 
take advantage conditional independencies subsets variables domain represent means graph 
graph sparse graphical models provide excellent support human intuition allow efficient inference algorithms 
learning underlying dependence graph data generally np hard 
purpose thesis propose study class models admits tractable inference learning algorithms rich practical applications 
class class mixtures trees models 
mixtures trees inherit excellent computational properties tree distributions subset graphical models combine trees order augment modeling power going standard graphical model framework 
thesis demonstrates performance mixture trees density estimation classification tasks 
time understanding properties tree distribution multivariate density model 
shows tree classifier implements implicit variable selection mechanism 
learning mixtures trees data central subject thesis 
learning algorithm introduced em minimum weight spanning tree algorithms quadratic dimension domain 
algorithm serve tool discovering hidden variables special important class models conditioned hidden variable dependencies observed variables sparse 
shown case sparse discrete data original learning algorithm transformed algorithm jointly subquadratic simulations achieves speedups factors 
thesis supervisor michael jordan title professor department brain sciences acknowledgments years mit fantastic experience joy remember making influence contributed thesis 
michael jordan thesis advisor fostered interest reinforcement learning statistics graphical models differential geometry 
knows 
soul jordan lab equally group meetings 
supported materially morally years giving complete freedom pursuing 
spite freedom thesis owes enormously inspiring influence 
deeply appreciated enthusiasm learning deepening understanding high standards set genuinely positive attitude enriched sense beauty approach science life 
alan willsky paul viola members committee interest support gave project especially asking just right questions thesis better 
hope discussions continue 
paul suggested direction resulted accelerated tree learning algorithms 
algorithms benefited greatly series stimulating meetings david karger 
eric grimson alvin drake gave advice fist days tomaso poggio group adopted semester studentship peter dayan talked time listen insightful comment subject brought 
morris shared enthusiasm trees coded mixtures trees program digits random trees experiments 
david heckerman microsoft group hosted seattle intense months forcing take distance proved entirely benefit 
david max chickering provided code ran part alarm experiments 
carl de marcken insightful critic ideas writing answered questions thought uncountably things needed know dozens problems share joy success 
countless opportunities appreciate quality education received alma polytechnic university sti take br paul dan anil petre stoica thought chose thesis topic chance chose 
couldn greg tommi jaakkola thomas hofmann listened half baked ideas answered patiently thousands questions engaged exciting discussions topic sun 
best friends reasons find enriched years mit 
circle love constantly invisibly surrounds strength 
gail welcomed family 

carl 
mother teacher friend partner adventures 
grandfather judge memory dedicate thesis 
research supported center biological computational learning mit funded part national science foundation contract 
asc 
contents density estimation multidimensional domains 
example 
graphical models conditional independence 
examples established belief network classes 
advantages graphical models 
structure learning belief networks 
inference decomposable models 

goal contributions road map thesis 
contributions 
road map reader 
trees properties tree distributions 
inference sampling marginalization tree distribution 
inference 
marginalization 
sampling 
learning trees maximum likelihood framework 
problem formulation 
fitting tree distribution 
solving ml learning problem 
representation capabilities 
appendix junction tree algorithm trees 
mixtures trees representation power mixtures trees 
basic operations mixtures trees 
learning mixtures trees ml framework 
basic algorithm 
running time storage requirements 
learning mixtures trees shared structure 
remarks learning algorithms 
summary related 
learning mixtures trees bayesian framework map estimation em algorithm 
decomposable priors tree distributions 
decomposable priors tree structures 
priors tree parameters dirichlet prior 
accelerating tree learning algorithm 
assumptions 
accelerated cl algorithms 
idea comparing mutual informations binary variables second idea computing cooccurrences bipartite graph data representation 
putting acl algorithm data structures time storage requirements 
acl ii algorithm 
time memory requirements acl ii 
generalization discrete variables arbitrary arity 
computing cooccurrences 
mutual informations 
acl algorithms em 
decomposable priors acl algorithm 
experiments 
concluding remarks 
appendix bounding number lists nl 
approach hidden variable discovery structure learning paradigms 
problem variable partitioning 
tree model 
variable partitioning general case 
outline procedure 
defining structure simple explanation 
experiments 
experimental procedure 
experiments tree models 
general models 
approximating description length model 
encoding multinomial distribution 
model validation independence testing 
alternate independence test 
threshold mixtures 
validating graphical models hidden variables 
discussion 
experimental results recovering structure 
random trees large data set 
random bars small data set 
density estimation experiments 
digits digit pairs images 
alarm network data set 
classification mixtures trees 
mixture trees classifier 
australian data set 
mushroom data set 
splice data set 
classification structure discovery 
single tree classifier automatic feature selector 
list figures dna splice junction domain 
example bayes net markov net variables 
structure thesis 
mixture trees 
note imply general mixture 
mixture trees shared structure represented graphical model 
bipartite graph representation sparse data set 
edge iv means variable data point 
mean full line standard deviation maximum dotted line kruskal algorithm steps nk runs plotted log ranges 
edge weights sampled uniform distribution 
acl ii algorithm data structure supplies candidate edge 
vertically left variables sorted decreasing nu 
lists cu list variables sorted decreasing order iuv virtual list sorted decreasing nv 
maximum elements lists inserted heap maximum iuv extracted maximum heap 
real running time accelerated full line traditional dotted line algorithm versus number vertices different values sparsity 
number steps kruskal algorithm nk versus domain size measured acl ii algorithm different values 
model graphical model distribution marginalized 
tree model training 
models training 
description lengths accuracies models fixed variable learned data generated tree model 
structures correct 
general detail upper left corner 
model structures scores accuracies obtained learning tree models data sets generated structure 
circles represent structures bad structures 
marks lowest dl models 
axes measure empirical description length bits example vertical axes measure accuracy hidden variable retrieval bottom line 
model structures scores accuracies obtained learning tree models variable data set generated structure 
circles represent structures bad structures 
sizes symbols proportional axis measures empirical description length bits example vertical axis measures accuracy hidden variable retrieval bottom line 
notice decrease dl increasing 
training examples bars learning task 
true structure probabilistic generative model bars data 
mixture trees approximate generative model bars problem 
interconnection variables bar arbitrary 
test set log likelihood bars learning task different values smoothing ff different averages standard deviations trials 
example digit pair 
average log likelihoods bits digit single digit double digit datasets 
mt mixture spanning trees mf mixture factorial distributions br base rate model helmholtz machine trained wake sleep algorithm helmholtz machine trained mean field approximation fv fully visible fully connected sigmoidal bayes net 
notice difference scale figures 
classification performance different mixture trees models australian credit dataset 
results represent percentage correct classifications averaged runs algorithm 
comparison classification performance mixture trees models splice data set 
models tested delve left right nearest neighbor cart hme hierarchical mixture experts ensemble learning hme early stopping hme grown nearest neighbors linear squares linear squares ensemble learning mixture experts ensemble learning stopping 
tree augmented naive bayes classifier nb naive bayes classifier tree represents mixture trees mt mixture trees 
knowledge neural net nn neural net 
cumulative adjacency matrix trees fit examples splice data set smoothing 
size square coordinates ij represents number trees edge variables square means number 
lower half matrix shown 
class variable 
group squares bottom shows variables connected directly class 
variable relevant classification 
surprisingly located vicinity splice junction 
subdiagonal chain shows rest variables connected immediate neighbors 
lower left edge upper right edge 
encoding ei splice junctions discovered tree learning algorithm compared ones watson molecular biology gene 
positions sequence consistent variable numbering splice junction situated positions 
symbols boldface indicate bases probability symbols indicate bases groups bases high probability indicates position occupied base non negligible probability 
cumulated adjacency matrix trees original set variables augmented noisy variables independent original ones 
matrix shows tree structure original variables preserved 
list tables results bars learning task 
average log likelihood bits digit single digit digit double digit pairs datasets 
boldface marks best performance dataset 
results averaged runs 
density estimation results mixtures trees models alarm data set 
training set size 
average standard deviation trials 
density estimation results mixtures trees models data set size generated alarm network 
average standard deviation trials 
performance comparison mixture trees model classification methods australian dataset 
results mixtures factorial distribution reported 
results 
performance mixture trees models mushroom dataset 
models 
index algorithms absorb acl acl outline acl ii learn outline outline chapter desc imi ste num ar vertebra ei de 
ion astral city naked rigor displays iron spine number 
density estimation multidimensional domains probability theory powerful general formalism successfully applied variety scientific technical fields 
field machine learning especially known unsupervised learning probabilistic approach proven particularly fruitful 
task unsupervised learning set observations data producing model description data 
case data assumed generated stationary process building model represents building description process 
definition learning implicit assumption redundancy assumption description data compact data model constructed data predict properties observations source 
opposed supervised learning learning performed view specified task data learner labeled consequently inputs outputs task unsupervised learning output variables envisioned usage model known time learning 
example clustering data set may interested number clusters shapes clusters analysis purposes may want classify observations belonging discovered clusters document classification may model lossy data compression vector quantization 
density estimation general form unsupervised learning provides fully probabilistic approach unsupervised learning 
expressing domain knowledge probability distribution allows formulate learning problem principled way data compression problem equivalently maximum likelihood estimation problem 
prior knowledge exists specified prior distribution class models task bayesian model selection bayesian model averaging 
parametric density estimation probabilistic framework enables separate consider essential description data consider inessential random 
challenges density estimation machine learning usually data multivariate dimensionality large 
examples domains typically high data dimensionality pattern recognition image processing text classification diagnosis systems computational biology genetics 
dealing joint distributions multivariate domains raises specific problems encountered univariate case 
distributions domains dimensions hard visualize represent intuitively 
variables discrete size statespace grows exponentially number dimensions 
continuous bounded domains number data points necessary achieve certain density points unit volume increases exponentially number dimensions 
way seeing radius neighborhood data point kept fixed number dimensions forthcoming denoted increasing relative volume neighborhood exponentially decreasing 
constitutes problem nonparametric models 
possibilities gathering data usually limited physical constraints increase number variables leads case parametric model class increase number parameters model consequently phenomenon overfitting 
increased dimensionality parameter space may lead exponential increase computational demands finding optimal set parameters 
ensemble difficulties related modeling multivariate data known curse dimensionality 
graphical models models joint densities attempting eliminate curse dimensionality limit effects strictly necessary 
advantage independences existing subsets variables domain 
cases dependencies sparse way formalized pattern known graphical models allow efficient inference algorithms 
cases side result graphical representation intuitive easy visualize manage humans 
example discussing graphical models illustrate task density estimation example 
domain represented dna splice junction domain briefly splice encountered experiments section 
consists discrete variables called site 
site represent consecutive sites dna sequence 
take values denoted symbols representing bases nucleic acid 
variable called junction denoted fact middle sequence represents splice junction 
splice junction place section non coding dna called intron meets section coding dna exon 
junction variable takes values ei exon intron variables belong exon intron intron exon reverse happens dna section contains junction 
observation data point observed instantiation variables domain statement reveals ill definedness task free purely unsupervised learning essential task may superfluous 
probability theory overcome difficulty provide better understanding assumptions underlying models constructing 
data junction appears middle position 
site site site site 
site site dna splice junction domain 
case dna sequence corresponding value junction variable 
density estimation assumption observed data generated underlying probabilistic process goal learning reconstruct process observed data 
reconstruction approximation thereof termed model 
thesis model joint probability distribution variables domain 
distribution incorporates knowledge domain 
example model splice domain expected assign relatively high probability sequences biologically plausible sequences observed data set probability close implausible sequences 
plausible sequence coupled correct value junction variable high probability sequence coupled values junction variables receive zero probability 
state space set possible configurations variables size ss 
impossible explicitly assign probability configuration construct compact storage point view tractable computation point view representations probability distributions 
avoid curse dimensionality require models number parameters small slowly increasing dimension learning models data done efficiently 
section shall see graphical probability models property fully satisfy second requirement 
trees subclass graphical models enjoy properties need combined mixture increase modeling power 
example natural ordering variables sequence 
examples digits bars variables arranged dimensional grid 
general necessary spatial relationship variables domain 
forthcoming discuss arranging variables graph 
case graph may required match spatial arrangement variables 
graphical models conditional independence examples established belief network classes introduce graphical models conditional independence short graphical models 
describes graphical models computation minded interpretation probability theory interpretation exposes qualitative nature centuries old formalism compatibility human intuition importantly amenability network representation parallel distributed computation 
graphical models known belief networks forthcoming terms shall interchangeably 
define probabilistic conditional independence follows disjoint sets ca ed example bayes net markov net variables 
variables say independent relationship holds pab pa pb configurations graphical models language say separates sets write equivalently variable sets conditionally independent knowing affect knowledge 
graphical models independences account avoid processing irrelevant information possible 
means graphical model advantage measure variables domain grouped conditionally independent subsets 
domain independences graphical model reduces way probability table generic function variables 
belief network encodes independences means graph way variable associated vertex graph 
graph topology represent dependencies 
rigorously absence edge indicates independence relationship 
graph called structure belief network 
common classes belief networks markov nets bayes nets decomposable models chain graphs markov net better known markov random field defined structure undirected graph arbitrary topology 
variables connected edge neighbors 
independences markov net expresses summarized global markov property variable independent neighbors 
shows markov net illustrates independence relationships encodes 
define clique undirected graph maximal subset variables neighbors 
probability distribution product functions called clique potentials defined clique graph 
example cliques independences represented net 
markov net specified stages graph describes structure model 
structure implicitly defines cliques 
second probability distribution expressed product functions variables clique parameters 
say distribution factors graph 
example distribution factors graph represented number variables clique essential efficiency computations carried model 
fewer variables clique efficient model 
totality parameters corresponding factor functions forms parameter set associated graph structure 
bayes nets belief nets structure directed graph directed cycles structure called directed acyclic graph shortly dag 
denote uv edge directed case call parent descendent variable reached directed path starting children children descendents depicts example dag 
independences encoded bayes net summarized directed markov property states bayes net variable independent non descendents parents 
probability distribution factorized form corresponds independence relationships expressed graph 
factor function depends variable parents call set variables family 
example families 
list independences encoded net includes distribution structure represented cpd short classes belief nets model specified specifying structure 
structure determines subsets closely connected variables cliques case families define factor functions clique potentials conditional probabilities represent model parametrization 
probability distribution factored graph dag undirected possesses independences encoded called conformal graph called map map independences represented useful computational point view belief net structure able represent exactly 
distribution dependencies represented said perfect map dag undirected graph probability distribution perfect graph 
converse true distributions set independences perfect map 
decomposable models 
bayes nets markov nets represent distinct intersecting classes distributions 
probability distribution mapped perfectly bayes net markov net called decomposable model 
decomposable model cliques undirected graph representation play special role 
arranged tree acyclic graph structure called junction tree 
vertices junction tree cliques 
intersection cliques neighbors tree non void called separator 
probability distribution factorized decomposable model junction tree clique potentials oec separator potentials oes way called consistent ensures important property junction tree consistency property junction tree consistent clique ae marginal probability equal clique potential oec 
factorization form qc oes junction tree implicitly assume tree consistent 
property endows decomposable models certain computational simplicity exploited belief network algorithms 
discuss inference junction trees section 
chain graphs general category graphical models 
underlying graph comprises directed undirected edges 
bayes nets markov nets subclasses chain graphs 
advantages graphical models advantages model factorized independences variables categories flexibility modeling power 
flexible dependence topology complemented freedom choice factor functions belief network rich powerful class probabilistic models 
particular belief networks encompass provide unifying view model classes including supervised learning hidden markov models boltzmann machines stochastic neural networks helmholtz machines decision trees mixture models naive bayes models 
important flexibility topology flexibility usages graphical models admit 
belief network represents probability distribution query expressed function probabilities subsets variables acceptable 
means particular belief network classifier variable domain converted take subsets variables inputs outputs respectively compute probabilities outputs inputs diagnostic purposes computing configuration set variables set 
flexibility theoretically property probability density model density representations endowed powerful inference machine allows compute arbitrary conditional probabilities take advantage 
models easier understand interpret 
wide body practical experience shows graphical representations dependencies appealing non technical users statistical models data 
bayes nets allow interpretation directed edge uv causal effect particularly appreciated means knowledge elicitation human domain experts 
bayes net parameters represent conditional probabilities specifying operating conditional probabilities easier human operating representations example specifying joint probability 
outputs model probabilities clear meaning 
give probabilities interpretation degrees belief advocated belief network tool reasoning uncertainty 
advantages learning parameters data structure 
independences mean fewer free parameters compared full probability model model independences set variables 
parameter appears function subset variables depends fewer variables 
certain parametrizations possible bayes net markov net structure allows independent estimation parameters different factors 
general finite amounts data smaller number independent parameters implies increased accuracy estimation parameter lower model variance 
hidden variables missing data 
hidden variable variable value observed 
variables may occasions observed 
happens say current observation domain missing value variable 
observation data point variable missing say observation complete 
graphical models framework allows variables specified observed unobserved data point integrating supervised learning naturally handling missing data hidden variables 
structure learning belief networks learning structure graphical model data easy task 
formulates problem structure learning bayes networks bayesian model selection problem :10.1.1.156.9918
show reasonable assumptions prior parameters bayes network discrete variable domain form dirichlet distribution 
set observations posterior probability network structure computed closed form 
similar results derived continuous variable models jointly gaussian distributions 
finding structure highest posterior probability intractable task 
general dag structures known algorithms finding optimal structure asymptotically efficient exhaustive search 
majority applications structure model assessed domain expert learned examining structures close elicited prior knowledge 
case usually simple structure chosen 
ask include model variable observed 
reason physical model domain postulates variable conditions observe directly state patients liver assessed indirectly certain blood tests 
reason computational nature may introduce hidden variable resulting model explains observed data fewer parameters models include hidden variable 
case hidden causes second example medical domain disease hidden variable allows describe simple way interplay multitude observation facts called symptoms 
chapter discuss issue length 
inference decomposable models task belief network conjunction actual data termed inference 
particular inference means answering generic query model form probability variable having value xv values variables subset ae known 
variables observed values referred categorical evidence denoted general setting define evidence probability distribution called likelihood case considered 
inference restricted sense formally defined computing probability xv current model 
query important reasons answers wide range common queries formulated function queries type obtained modified versions inference algorithm solves query serves benchmark query efficiency inference algorithms class belief networks 
inference bayes nets 
pearl introduced algorithm performs exact inference singly connected bayes networks called polytrees 
singly connected bayes net network underlying undirected graph cycles 
net undirected path variables pearl algorithm came named assumes node receive send messages neighbors parents children performs computations information locally node 
local algorithm 
pearl proves asynchronous exact terminates finite time 
minimum running time bounded diameter undirected graph turn equal number variables singly connectedness graph essential finite termination correctness algorithm output 
general multiply connected bayes nets inference provably np hard 
standard way performing inference bayes net general topology transform decomposable model possible series edge additions combined removing edges directionality 
adding edges graphical model change probability distribution represents hide computationally unusable independences 
decomposable counterpart bayes net constructed inference performed standard inference algorithm decomposable models junction tree algorithm described 
inference junction trees 
defined decomposable model junction tree belief network cliques form tree 
tree distributions introduced section examples decomposable models 
inference graphical model stages 
stages described case junction tree represent known junction tree algorithm entering evidence 
step combines joint distribution variables thought prior evidence acquired different source information produce posterior distribution variables evidence 
worth mentioning impressively successful applications pearl algorithm bayes networks loops published 
turbo codes gallager neal mackay codes belief propagation multiply connected networks 
pearl algorithm performs cases topic intense current research 
expression posterior factorized form original distribution stage satisfy consistency conditions implicit graphical model junction tree definition 
propagating evidence 
stage processing final result posterior expressed consistent normalized junction tree 
phase referred junction tree algorithm proper 
junction tree algorithm requires local computations involving variables clique 
similarly pearl algorithm polytrees information propagated edges tree means separators 
algorithm exact finite time requires number basic clique operations proportional number cliques tree 
variables domain take values finite sets time required basic inference operations clique proportional size total state space clique exponential cardinality clique 
total time required inference junction tree sum state space sizes 
extracting probabilities variables interest marginalization joint posterior distribution obtained previous stage 
consistent junction tree marginal variable computed marginalization oec clique sizes small relative represents important saving time computing marginal 
reason locality operations necessary previous step inference procedure 
returning bayes nets follows time required inference junction tree method exponential size largest clique resulting junction tree 
desirable minimize quantity alternatively sum state space sizes process constructing decomposable model 
objective provably np hard equivalent solving max clique problem 
expected known alternative objective np hard problem 
shown exact inference algorithm local computations hard junction tree algorithm np hard 
inference markov nets 
showed inference markov random fields arbitrary topology intractable 
introduced markov chain monte carlo sampling technique computing approximate values marginal conditional probabilities network known simulated annealing 
similar approaches approximate inference monte carlo techniques devised bayes nets small size 
approximate inference bayes nets topic current research 
existent approaches include pruning model performing exact inference reduced model cutting loops bounding incurred error variational methods bound node probabilities sigmoidal belief networks 

goal contributions road map thesis previous sections challenge density estimation multidimensional domains introduced models tools specifically designed purpose 
shown graphical models focus expressing dependencies variables domain constraining topology dependencies functional form 
property separating dependency structure detailed functional form excellent support human intuition compromising modeling power model class 
probabilistic semantics graphical model possible separate model learning usage belief network constructed data way consistent laws probability 
seen inference graphical models performed efficiently models simple general case np hard problem 
holds learning belief network structure data learning hard general shall see classes structures learning structure parameters done efficiently 
purpose thesis propose describe class models rich useful practical applications admits tractable inference learning algorithms 
class class mixtures trees 
contributions mixture trees models tree defined bayes net node parent 
trees represent acyclic pairwise dependencies limited modeling power 
combining tree distributions mixture represent distribution discrete variables 
number trees mixture components parameter controlling model complexity 
mixtures trees represent different set dependencies graphical models 
algorithmic perspective thesis shows properties tree distributions ability perform basic operations computing likelihoods marginalization sampling linear time directly extend mixtures 
efficient learning algorithm 
thesis introduces efficient algorithm estimating mixtures trees data 
algorithm builds fundamental property trees fact class graphical models tree structure parameters learned efficiently 
embedding tree learning algorithm expectation maximization search procedure produces algorithm quadratic domain dimension linear number trees size data set algorithm finds maximum likelihood estimates serve bayesian estimation 
preserve algorithm efficiency case needs restricted class priors 
thesis characterizes class class decomposable priors shows restrictions class imposes stronger assumptions underlying mixture trees learning algorithm 
shown widely priors belong class 
accelerated learning algorithm sparse binary data 
mind application high dimensional domains document categorization retrieval preference data image compression quadratic algorithm may sufficiently fast 
introduce algorithm exploits property data sparsity construct family algorithms jointly subquadratic 
controlled experiments artificial data trees trees bayesian discovery experiments accelerated structure thesis new algorithms achieve speedup factors performance practically independent long sparsity size data set remain constant 
top approach hidden variable discovery learning structure graphical model data hard important problem 
vast majority algorithms field pursue bottom strategy basic unit graph edge 
fundamental reason graph edge play special role fact edges families cliques building parts graphical model 
thesis pose question structure search performed top manner 
show hidden variable model called model approach natural produces partition variable set clusters structure search performed independently 
show ideas underlying mixture trees learning algorithm serve basis variable partitioning 
result heuristic algorithm discovering hidden variables models 
motivated need validate models obtained hidden variable discovery algorithm investigated large deviation theory testing probabilistic independence discrete variables obtained alternate distribution free independence test sanov theorem 
optimality test study 
road map reader structure thesis seen diagram 
chapter lay foundation defining fundamental concepts reviewing tree distributions inference learning algorithms 
chapter builds define mixture trees variants introduce algorithm learning mixtures trees data maximum likelihood framework show mixtures trees approximate arbitrarily closely distribution discrete domain 
chapters develop material different direction read independently order 
chapter discusses learning mixtures trees bayesian framework 
basic learning algorithm extended take account class priors called decomposable priors 
shown class rich contain important priors dirichlet prior minimum description length type priors 
process assumptions decomposable priors learning algorithm explicit discussed 
chapter aims improve computational properties tree learning algorithm 
introduces data representation new algorithms exploit properties domains construct exact maximum likelihood trees subquadratic time 
compatibility new algorithms em framework decomposable priors discussed 
chapter introduces top method learning structures hidden variable methods scoring obtained models discussion independence test approach validating 
comes chapter devoted experimental assessments 
chapter demonstrates performance mixtures trees various tasks 
results density estimation data generated mixture trees 
part chapter discusses classification mixtures trees 
model density estimator performance classifier experiments excellent competition classifiers trained supervised mode 
behavior single tree classifier demonstrate acts implicit feature selector 
chapter contains concluding remarks 
chapter trees properties think shall see poem lovely tree 
joyce section introduce tree distributions subclass decomposable models demonstrate properties attractive computational point view 
shown fundamental operations distributions inference sampling marginalizing carry directly junction tree counterparts order applied trees 
learning tree distributions formulated maximum likelihood ml estimation problem 
solve algorithm finds tree distribution best approximates target distribution sense kullback leibler divergence 
algorithm optimizes structure parameters time memory proportional size data set quadratic number variables final considerations modeling power ease visualization prepare mixtures trees section 
tree distributions section introduce tree model notation 
denote set variables interest 
stated cardinality variable rv denote number values omega represent domain xv omega particular value similarly subset omega omega domain xa assignment variables particular omega state space variables simplify notation shall need maximum rv shall denote value rmax graphical model paradigm variable viewed vertex undirected graph 
edge connecting variables denoted uv significance clear 
graph called tree cycles 
note definition tree number connected components 
number edges relationship definition differs slightly definition tree graph theory literature 
tree required cycles connected meaning vertices exist path definition tree allows disconnected graphs trees corresponds graph theory called forest 
means adding edge tree reduces number connected components 
tree edges 
define probability distribution conformal tree 
denote tuv tv marginals uv tuv xu xv xu xv tv xv xv 
satisfy consistency condition tv xv xu xv xu uv deg degree vertex number edges incident distribution conformal tree factored tuv xu xv tv xv deg distribution called tree confusion possible 
graph represents structure distribution noting trees domain edge set uniquely defines tree structure confusion possible identify structure 
tree connected spans nodes called spanning tree 
tree triangulated graph easy see tree distribution decomposable model 
fact representation identical junction tree representation cliques identify graph edges cliques size separators nodes degree larger 
junction tree identical tree clique separator potentials marginals tuv tv deg respectively 
shows remarkable property tree distributions distribution conformal tree completely determined edge marginals tuv uv 
tree decomposable model represented terms conditional probabilities tv pa xv shall call representations undirected directed tree representations respectively 
form obtained choosing arbitrary root connected component directing edge away root 
words uv closer root parent denoted pa 
note directed tree obtained vertex parent 
consequently families tree distribution size root roots size families vertices 
having transformed structure directed tree computes conditional probabilities corresponding directed edge recursively substituting tv pa starting root 
pa represents parent directed tree empty set root connected component 
directed tree representation advantage having independent parameters 
total number free parameters representation deg rv ru rv rv shows newly added edge uv increases number parameters ru rv 
shall characterize set independences represented tree 
tree path vertices 
node path say separates correspondingly set root probability distribution decomposed product factors containing variables independent conclude tree variables separated set intersects path 
subsets ae independent ae intersects path verify tree undirected markov property holds 
set variables connected edges variable neighbors separates variables inference sampling marginalization tree distribution section discusses basic operations inference marginalization sampling tree distributions 
demonstrates algorithms performing operations direct adaptations counterparts junction trees heavily relying generic junction tree algorithm 
shown basic operations linear number variables fact important analog operations mixtures trees algorithms trees building blocks 
details algorithms sake completeness skipped prejudice understanding rest thesis 
inference 
shown tree distributions decomposable models generic inference algorithm trees instance inference algorithm decomposable models called junction tree algorithm 
shall define procedure takes inputs tree having structure undirected factored representation categorical evidence set ae procedure performs steps inference process defined section 
enters categorical evidence multiplying original distribution values function representing categorical evidence 
resulting tree local propagation information cliques edges tree 
outputs tree factored conformally original represents posterior distribution tv algorithm described detail section chapter 
demonstrated takes running time order max 
step extracting probabilities variables interest mentioned representation done marginalization discussed section 
particular obtaining posterior probability single variable involves marginalization variable takes rmax operations 
marginalization junction tree computing marginal distribution group variables contained clique performed marginalization respective clique 
tree cliques size marginal variable obtained directly tv separator separator potentials stored explicitly marginalizing potential tuv edge incident leaf node 
marginal single variable obtained rmax additions 
pairwise marginals variables neighbors directly available clique potentials tuv 
enumeration exhausts cases marginals directly available tree distribution 
describe algorithm efficiently compute marginal distributions arbitrary pairs variables 
algorithm generalized marginal distributions arbitrary subsets shown marginal tuv depends potentials edges path path 
wd vertices path tree length 
marginal tuv expressed tuv xv xv tu dy twi wi path tw pa xv tu dy twi wi di twi twi form obtained directed tree representation root summation xw path done recursively starting leaves tree results factor 
form obtained 
third form rewriting previous equation undirected representation 
intermediate variables wi appears factors consequently summation values intermediate variables done variable time indicated sum tv wd 
tw xw tu xw tw tw tw computation value tuv takes pd rwi additions multiplications compute marginal probability table need perform rwi max operations 
equation shows intermediate sums involved computation tuv marginal distributions 
suggests intermediate sums stored pairs uv enumerated judiciously chosen order compute pairwise marginal tables summing variable words pairwise marginals computed max operations 
marginalization presence evidence represents third step inference previous subsection 
problem approached various ways polytree algorithm 
approach follows directly procedures entering evidence calibration marginalization introduced 
find uv tuv ea ae 
enter propagate evidence ea 
compute marginal uv sampling sampling tree best performed directed tree representation 
value root node sampled marginal distribution 
value nodes sampled conditional distribution parent pa recursively starting root 
simple algorithm specialization algorithm sampling junction tree 
algorithm sampling replacement junction tree immediately specialized trees 
sampling tree presence evidence done just marginalization presence evidence steps 
incorporates evidence algorithm second sampling procedure described performed resulting conditional distribution 
learning trees maximum likelihood framework problem formulation shall formulate learning problem maximum likelihood ml estimation task 
assume domain set observations called dataset 
xn 
assume data generated sampling independently unknown tree distribution learning problem consists finding generative model 
maximum likelihood principle estimate model maximizes probability likelihood observed data 
equivalently search optimize logarithm likelihood called log likelihood leads formulate ml learning problem trees follows domain set complete observations find tree distribution argmax xx log 
fitting tree distribution solution ml learning problem published broader context finding tree best fits distribution dataset goodness fit evaluated kullback leibler kl divergence kl log chow liu algorithm constitute building block algorithms developed thesis shall derivation 
impatient reader skip subsection 
start examining 
known distributions kl equality attained kl divergence rewritten kl log log log log notice term depend minimizing kl divergence equivalent maximizing second term called cross entropy know achieved return problem fitting tree fixed distribution finding tree distribution requires finding structure represented edge set corresponding parameters values tuv xu xv edges uv values xu xv 
assume structure fixed expand right hand side kl log log log log tv pa xv xx log tv pa xv pv pa xv log tv pa xv ppa xv pv pa xv log tv pa xv denotes entropy distribution quantity depend puv pv represent respectively marginals inner sums lines taken domains pa respectively 
root node pa void set corresponding range convention value probability ppa 
note terms depend form xv pv pa xv log tv pa xv differs constant independent kl divergence kl pv pa tv pa know minimized tv pa 
pv pa 

fixed structure best tree parameters sense minimum kl divergence obtained copying corresponding values conditional distributions pv pa 
remarks identity achieved distributions tv pa parameterized set parameters 
second identity follows tuv puv subsequently resulting distribution independently choice roots 
structure denote tree edge set parameters satisfy equation 
achieves optimum tree distributions conformal 
previous results mind shall proceed minimization kl tree structures 
notice task equivalent maximizing objective log 
structures expanding formula obtain successively nx xi log xi nx xi log deg log ev xiv nx xi log puv deg log pv xiv nx xi log puv log pu log pv xiv nx xi log pv xiv nx xi log puv pu pv xiv xv nx xi log pv xiv puv xu xv log puv xu xv xu pv xv xv pv xv log pv xv iuv pv equation follows undirected tree representation obtained account equation follows performing summation xu xv definitions puv pv equation terms iuv sum sign represent mutual information variables distribution iuv puv xu xv log puv xu xv xu pv xv mutual information variables quantity non negative equals variables independent 
important facts equation second sum depend structure second importantly dependence additive elements set words edge contributes certain positive amount amount iuv independently presence absence edges size contributions 
situation maximization structures performed efficiently maximum weight spanning tree mwst algorithm weights iuv mwst problem formulated follows graph set real numbers called weights corresponding edge graph find tree sum weights corresponding edges maximized 
problem solved greedy algorithm constructs tree adding edge time decreasing order weights 
variants algorithm simplest called kruskal algorithm runs log time 
note weights strictly positive tree maximum number edges connected components result 
weights zero possible obtain trees connected component 
sophisticated mwst algorithms exist see example improve kruskal algorithm running time memory requirements 
running time published algorithms proportional number candidate edges 
case number equal pairs variables considered 
best running time achievable mwst algorithm 
henceforth assume mwst algorithm runs time specify implementation 
summarized algorithm algorithm probability distribution domain procedure mwst weights fits maximum weight spanning tree 
compute marginals pv puv 
compute mutual informations iuv 
call mwst iuv outputs edge set tree distribution 
set tuv puv uv output algorithm takes input probability distribution domain outputs tree distribution minimizes kl divergence kl 
running times algorithm steps follows steps 
steps 

computing marginals mutual informations pairs variables running time dependent representation generally expected mutual information values computed 
step 
mwst algorithm takes operations log kruskal variant 
step 
step comprises nr max assignments remember 
total running time nr max consider rmax constant 
solving ml learning problem previous subsection algorithm finds tree distribution closest kl divergence distribution kl 
solve initial maximum likelihood estimation problem sufficient call uniform distribution data see note kl nx log xi log 
minimizing expression equivalent maximizing equation 
case step algorithm computing marginals max time 
time dominates times required steps max log usually case running time ml tree estimation algorithm max 
representation capabilities graphical representations easy grasp means human intuition subclass tree graphical models intuitive 
sparse graphs having fewer edges 
importantly precisely variables path words separation relationships subsets variables easy read general bayes net topology obvious tree 
tree edge corresponds simplest common sense notion direct dependency natural representation 
simplicity tree models intuitively appealing limits modeling power 
words class dependency structures representable trees relatively small 
instance domain dimension nn distinct undirected spanning trees total undirected graphs 
mixture trees models way circumventing problem 
thesis show mixtures trees arbitrarily increase representation power tree distributions sacrificing little computational advantages certain extent preserving appeal human intuition 
appendix junction tree algorithm trees section mechanisms entering evidence tree density model maintaining consistent representations presence evidence 
tree distribution junction tree inference done junction tree algorithm undirected tree representation described previously 
entering evidence general sense evidence subset variables defined possibly unnormalized probability distribution ea omega 
finding special type evidence fv takes values set 
finding represents statement take certain values 
finding fv expressed sum ffi functions non zero terms number non zero values fv 
fv xv xv omega fv xv ffi xv xv ea consists collection findings variable ea fv evidence said categorical 
rest thesis categorical evidence considered 
handling non categorical evidence contained clique reader referred 
discusses special case non categorical evidence compatible polytree algorithm 
see ff nn fi note ff grows nn fi grows 
dirac symbol ffi prior distribution pv variables categorical evidence ea ffi xa ae xv xa pv xv xa ea xa represents probability xv xa observing evidence ea 
probability course xv 
conditional probability remaining variables evidence proportional product ea xv pv ea xa convention xa ea ea xa express posterior conditional distribution variables evidence ea ea pv ea xa 
equations discussion easily generalizes case ea product sums ffi functions case categorical evidence 
categorical evidence ea define operation entering evidence equation 
normalization constant equation represents prior probability categorical evidence ea 
conditional probability categorical evidence ea ea pv ea xa tree distribution equation rewrites ea tv ea xa prior ea replaced efficiently computed shown 
note posterior probability obtained unnormalized evidence unnormalized function 
propagating evidence 
categorical evidence entered tree decomposable model resulting distribution ea factored graph factors clique potentials consistent junction tree normalized 
step called tree calibration purpose normalized consistent distribution 
calibration represents junction tree algorithm proper applicable general decomposable model specialization tree distributions 
general case proofs consistency see 
important keep mind calibration add information model merely reorganizes information order represent form convenient subsequent 
basic operation evidence propagation called absorption 
say edge vw absorbs neighboring edge uv procedure called algorithm absorb absorb input edge potentials uv uv vw 
uv 
tv tuv 
vw output vw note absorption asymmetric operation absorb absorb produce different results 
absorption potentials vw uv consistent 
uv normalized vw 
potentials consistent absorb operation direction changes 
facts easily proved xw vw xv xw xw xv xw xv xv xv xv xv xw xv xv tv xv xv xu uv xu xv derivation proves consistency absorption 
potentials consistent absorption fraction xv tv xv equals xv absorption direction leaves potentials unchanged 
assume uv normalized 
xv xw vw xv xw xv vw xv xw xv xv xv uv xu xv necessary absorption multiple edges 
defined algorithm absorb absorb 
um input edge potentials uiv uiv vw 


iv uiv 
tv tuv 
iv vw say edge vw absorbs adjacent edges 

previous case absorption multiple edges potentials involved operation necessarily consistent 
consistent sufficient edges uiv absorb vw 
shall see shortly idea junction tree propagation algorithm tree consistent series periphery root edge root clique followed second series opposite direction 
mechanism implemented recursive procedures described 
called edge edge calls edges adjacent absorbs 
algorithm input factored representation tree nodes vw 
um 

ui 
absorb 
um output fulfills task recursively absorbing subtree rooted node edge vw 
reverse effect called edge vw edges adjacent absorb vw call algorithm input factored representation tree nodes vw 
um 


absorb ui 
ui output procedures introduce junction tree algorithm specialization trees junction trees called 
algorithm takes input tree distribution space omega represented set consistent normalized marginal distributions tuv categorical ae outputs distribution ea represented consistent normalized structure distribution marginals corresponding variables observed variables ffi functions 
algorithm ea input tree distribution factored representation conformal categorical evidence ea subset ae 
enter evidence ea 
choose edge vw root edge 

normalize vw vw 
output proofs correctness algorithm 
normalization constant computed step algorithm represents probability evidence ea 
see notice potential vw root edge change normalization 
tree edges absorb directly indirectly vw 
algorithm edge potentials normalized 
tree consistent simple function identification conclude represents propagation evidence tree distribution takes order max time 
chapter mixtures trees previous section shown framework graphical probability models tree distributions enjoy properties attractive modeling tools flexible topology intuitively appealing sampling computing likelihoods linear time simple efficient algorithms marginalizing conditioning exist 
fitting best tree distribution done exactly efficiently 
trees capture simple pairwise interactions variables prove insufficient complex distributions 
chapter introduces powerful model mixture trees 
thesis show mixtures trees enjoy computational advantages trees addition universal approximators space distributions 
define mixture trees distribution form mx kt 
mx 
tree distributions mixture components called mixture coefficients 
graphical models perspective mixture trees viewed containing unobserved choice variable takes value 
probability conditioned value distribution visible variables tree 
trees may different structures different parameters 
note variable structure component trees mixture trees bayesian network markov random field 
adopt notation independent distribution 

ae imply hand mixture trees capable representing dependency structures conditioned value variable choice variable usual bayesian network markov net 
situations model potentially useful abound real life consider example bitmaps handwritten digits 
ed ca ed ca ed ca mixture trees 
note imply general mixture 
mixture trees shared structure represented graphical model 
obviously contain dependencies pixels pattern dependencies vary digits 
imagine medical database recording body weight data patient 
body weight function age height healthy person depend conditions patient suffered disease athlete 
situation ones mentioned conditioning variable produces dependency structure characterized sparse acyclic pairwise dependencies mixture trees model able uncover exploit kind dependencies 
impose trees mixture structure obtain mixture trees shared structure 
note case resulting mixture preserve independence relationships component trees 
represented bayes net choosing orientation trees directly chain graph 
chain graphs introduced represent superclass bayes nets markov random fields 
chain graph contains directed undirected edges 
representation graphical model 
simple modification mixture trees mixture trees shared structure choice variable observed 
models called mixtures visible choice variable forthcoming 
stated assumed choice variable hidden 
referred hidden variable 
representation power mixtures trees shown discrete variable domains class mixtures trees represent distribution 
theorem 
vn set variables finite ranges probability distribution omega 
represented mixture trees 
proof 
denote ffix distribution omega defined ffix ffix identified tree having connected components factored distribution vj vj 

distribution represented mixture ffi distributions ffix 
sum taken range represent mixing coefficients 
completes proof 
basic operations mixtures trees marginalization marginal distribution variables subset ae qa xa xv xv mx kt mx xv mx kt ka xa marginal mixture marginals component trees 
inference restricted sense marginalization conditioning evidence performed analog way 
eb evidence ae set variables evidence 
qa xa eb xv eb mx kt ka xa eb 
infer value hidden variable evidence eb applying bayes rule eb eb eb kt kb eb eb particular evidence consists observing visible variables 
vn xn 
xn posterior probability distribution kt sampling sampling mixture trees presence evidence straightforward extension sampling tree discussed chapter 
procedure sampling mixture trees uses procedure samples tree distribution evidence possibly void algorithm input mixture trees evidence 
sample value 

sample component mixture returns value output subsection shown basic operations mixtures trees marginalization conditioning sampling direct extensions corresponding operations tree distributions 
complexity scales accordingly marginalization subset mixture trees takes times computation marginalization subset single tree 
instance computing marginal single variable separator takes max 
inference mixture trees takes junction tree propagation operations mnr max 
inferring value hidden variable takes mn multiplications 
sampling mixture ns ns number operations required sample value vertex clique conditioned value vertex 
directed tree representation ns rmax choice tree sample way choice requiring operations 
learning mixtures trees ml framework basic algorithm section show mixture trees fit observed dataset maximum likelihood paradigm em algorithm 
learning problem similar formulated trees chapter 
set observations 
xn required find mixture trees satisfies nx log xi 
rest thesis assume missing values variables assume hidden 
denote hidden values choice variable zi 

learning ml mixture trees model done means expectationmaximization em algorithm 
iterative algorithm devised fitting maximum likelihood parameters models hidden missing variables 
iteration consists steps expectation maximization described 
em algorithm introduces likelihood function called complete log likelihood log likelihood observed unobserved data current model estimate 
lc nx mx ffik zi log log xi complete log likelihood depends unknown values hidden variable directly computable 
idea underlying em algorithm compute optimize expected value lc 
expectation step consists estimating posterior probability hidden variable observations 
case means estimating probability tree generating data point xi zi xi flk kt xi xi ffik uses posterior probabilities compute expectation lc linear function flk values 
lc nx mx flk log log xi introduce quantities gamma nx flk xi 
xi flk gamma sums gamma interpreted total number data points generated component normalizing posteriors flk gamma obtain probability distribution data set 
shown acts target distribution express expected complete log likelihood terms gamma lc mx gamma log mx gamma nx xi log xi maximization step em algorithm parameters model maximize lc 
proved iteration steps converges local maximum log likelihood visible data model 
inspecting equation see expression lc sum terms depend disjoint subsets model parameters 
maximize separately term sum part model depends 
maximizing term subject constraint mx obtains new values parameters gamma kn 
obtain new distributions maximize expression negative cross entropy nx xi log xi problem equivalent problem fitting tree distribution solved exactly shown section 
give brief reminder procedure 
compute mutual information pair variables target distribution kuv xu xv log kuv xu xv ku xu kv xv 
second optimal tree structure maximum weight spanning tree algorithm weight edge tree marginals kuv ku uv exactly equal corresponding marginals kuv target distribution computed intermediate step computation mutual informations 
algorithm summarized algorithm outline input dataset 
xn initial model 
procedure iterate convergence step compute xi 

step 
ml 
gamma mt output model 
show steps algorithm detail 
step displayed running time 
time memory requirements discussed subsection 
algorithm input dataset 
xn initial model 
procedure mwst weights fits maximum weight spanning tree convergence step compute xi 

mnn step 

gamma 
compute marginals kv kuv mn 
compute mutual information mnr max 
call mwst generate mn 
kuv kuv kv kv mnr max running time storage requirements running time 
computing likelihood data point tree distribution directed tree representation parameters represent conditional distributions takes multiplications 
step require mnn floating point multiplications divisions 
step situation divisions computing values step mn marginals step mn max mutual informations step mn running mwst algorithm times step mnr max computing tree parameters directed representation step algorithm 
values right algorithm 
total running time em iteration mn mnr max dominating terms cost correspond computation marginals kuv mn mutual informations distributions storage requirements storing data takes mn integer numbers additionally represent model needs mnr max real parameters 
storage necessary independently learning algorithm 
intermediate values needed em algorithm flk values require mn storage locations 
values overwrite flk values additional storage needed 
single variable pairwise marginals require max storage 
overwritten successive values mutual informations pairs variables require storage locations 
overwritten additional temporary storage required mwst algorithm 
kruskal algorithm amount proportional number candidate edges 
algorithms require linear space 
total storage mn mnr max max dominant cost corresponds computation pairwise marginals distributions learning mixtures trees shared structure possible constrain trees share structure constructing truly bayesian network 
steps em algorithm learn mixtures trees shared structure called 
reasoning calculations parallel ones describing fitting tree distribution section 
step identical step algorithm 
expression expected complete log likelihood lc mx gamma log mx gamma nx xi log xi easily see reestimation 
decoupled estimation rest model performed way gamma kn 
difference tree distributions constrained structure 
maximization tree distributions decoupled separate tree estimations performed simultaneously trees 
reasoning similar section shows structure optimal parameters tree edge kuv equal parameters corresponding marginal distribution kuv equation reproduced kuv kuv remains find optimal structure 
expression optimized second sum equation mx gamma nx xi log xi making substitution expression series steps similar obtain equal mx gamma kv mx constant independent structure iuv constant optimizing structure done mwst algorithm edge weights represented iuv conditional mutual information variables adjacent algorithm essentially learning algorithm :10.1.1.30.9978
expressions right column represent running time step algorithm 
algorithm input dataset 
xn initial model 
procedure mwst weights fits maximum weight spanning tree convergence step compute xi 

mnn step 

compute marginals kv kuv mn 
compute conditional mutual information iuv mnr max 
call mwst iuv generate 

gamma 

kuv kuv kv kv mnr max remarks learning algorithms described procedures learning mixtures trees data important assumption explicit 
parameter independence assumption value pa distribution kv pa multinomial rv free parameters independent tree structure parameters mixture parameter independence assumption essential computational efficiency algorithms 
independence parameters kv pa parameters allows simple form step em algorithm 
independence tree structures allows mwst algorithm globally maximize tree structures iteration 
practical note equation calls copying marginals target distributions algorithms implement directed tree representation copy parameters conditional probabilities kv pa 
note algorithm attains global solution learning mixture trees em algorithm converge local maximum likelihood 
tree structures obtained basic algorithm connected 
chapters show reasons ways obtain disconnected tree structures 
missing variables handled elegantly trees 
number nonadjacent missing variables marginalized rmax time bound grows exponentially size largest connected subset missing variables 
observed unknown choice variable interesting special case situation choice variable fact observed variables small subset thereof don know 
discover build mixtures conditioning observables compare posteriors build standard mixture model compare mutual information structure variable identify candidate 
summary related chapter introduced mixture trees model variants shown basic operations mixtures trees direct extensions operations tree distributions inherit excellent computational properties 
tractable algorithm learn model data maximum likelihood framework 
trees noticed flexibility computational efficiency early classification task called mixture trees visible choice variable fitting different tree class 
builds chow liu algorithm exact algorithm learning polytree true distribution known polytree 
fitting best polytree arbitrary distribution np hard 
considered model explicit observed choice variable special case bayesian multinet 
appeared tree augmented naive bayes classifier developed 
considers continuous variables suggests ingenious heuristic deciding continuous variables discretized density approximated mixture gaussians 
mixture trees encompasses aforementioned models special cases assumption stated distinct assumptions called parameter modularity global parameter independence local parameter independence :10.1.1.156.9918
introduced 
classification results mixture trees chapter published 
mixture models second parent mixtures trees come longer way 
cite mixture models closely related 
mixture factorial distribution form naive bayes model choice variable class variable classification task known auto class name attentive study heavily excellent cost performance ratio 
successfully mixture factorial distributions hidden variable classification line followed combined 
idea learning tractable simple belief networks superimposing mixture account remaining dependencies developed independently mixtures gaussian belief networks 
interleaves em parameter search bayesian model search heuristic general algorithm 
shall go examine learning model bayesian framework show algorithms introduced extended case 
quadratic time storage complexity learning algorithm satisfactory medium scale problems may prohibitive problems high dimensionality 
shall show requirements reduced certain conditions practical importance 
shall examine mixture trees classification 
study mixture trees learning algorithm heuristic solving challenging problem hidden variable discovery 
section shows naive bayes classifier little sensitivity irrelevant attributes just single tree classifier partly explains success 
chapter learning mixtures trees bayesian framework ml paradigm assumes model estimated data excluding sources knowledge parameters model structure 
prior knowledge exists represented probability distribution space mixtures trees models bayesian formulation learning combine sources information 
bayesian framework main object interest posterior distribution models case mixtures trees observed data 
bayes formula posterior proportional represents prior distribution class mixture trees models second factor likelihood data model probability observation obtained model averaging dq special cases posterior representable closed form 
common approach approximate posterior distribution mode example laplace approximation 
approach replace integration equation finite sum set models high posterior probability 
approximation equivalent setting mixtures consequently normalization constant formula computed 
choose model summarize posterior distribution natural choice mean distribution 
shown mean expressed map estimate certain parameterization 
finding modes posterior distribution necessary step approaches maximum posteriori map estimation seen purpose se chapter concerned maximizing posterior distribution 
map estimation em algorithm equation previous section maximizing posterior log likelihood model equivalent maximizing log log differs log additive constant 
em algorithm previously find maximum likelihood estimates adapted maximize expression represents expression log likelihood plus term log 
quantity iteratively maximized em log log lc easy see added term influence step em algorithm proceed exactly 
step able successfully maximize 
seen previous chapter exact maximization enabled fact obtained separate set equations mixture components look priors mixtures trees models amenable decomposition 
prior space mixtures trees mixture components comprises prior distribution hidden variable priors trees structures parameters 
require prior expressed product form ek ek 
remember key efficient maximization equation fact decomposed sum independent terms corresponding edge similar property desirable prior 
prior probability tree components prior tree structure represented edge set ek prior tree parameters structure 
shall require components decompose way matches factoring likelihood equation ek ek kuv ek kuv prior class mixtures trees having properties called decomposable prior 
context decomposable priors step em algorithm new model qnew obtained solving set decoupled maximization problems parameters new argmax sigma mx gamma log log 
tree equation previous chapter replaced new nx xi log xi log requiring prior decomposable equivalent making strong independence assumptions example means prior probability tree mixture independent probability distribution mixture variable 
sections implications independence assumptions explicit 
shown independence assumptions strong ones restrictive 
class decomposable priors rich contain members interesting practical importance 
decomposable priors tree distributions decomposable priors tree structures general form decomposable prior tree structure edge contributes constant factor independent presence absence edges exp prior expression maximized step em algorithm equation lc mx gamma log mx gamma nx xi log xi ek gamma consequently edge weight tree penalized corresponding fi divided total number points tree responsible kuv fi kuv gamma negative increases probability uv final solution 
contrary positive acts penalty presence edge uv tree 
sufficiently large weight negative edge added tree 
introducing edge penalties obtain trees having fewer edges disconnected 
notice strength prior decreases gamma total number points assigned mixture component equal priors trees trees accounting fewer data points penalized stronger fewer edges 
chooses edge penalties proportional increase number parameters caused addition edge uv tree fi ru rv minimum description length mdl type prior implemented 
note effective penalty parameter inversely proportional number data points gamma tree parameter belongs responsible 
context learning bayesian networks prior suggested delta delta distance metric prior network structure :10.1.1.156.9918
prior penalizes deviations prior network 
trees distance metric symmetric difference delta prior factorable entailing ln uv ln uv happens case mixtures trees shared structure 
recall case edge uv weight proportional mutual information conditioned iuv 
effect decomposable prior penalize weight priors tree parameters dirichlet prior introduce important subclass decomposable priors parameters called dirichlet priors 
dirichlet prior conjugate prior multinomial distribution 
distribution mixture variable multinomial facts shown section cover case 
discrete random variable values pz 
probability distribution sample size pz ry nj 
represent number times value observed called sufficient statistics data 
sample said obey multinomial distribution 
dirichlet distribution defined domain depends real parameters gamma pj gamma yj gamma represents gamma function defined gamma 
nonnegative integer gamma 
importance dirichlet distribution connection distributed variable resides fact parameters multinomial distribution prior dirichlet distribution parameters 
observing sample sufficient statistics nj 
posterior distribution dirichlet distribution parameters nj 
justifies denoting distribution parameters 
popular alternative parameterization dirichlet distribution rx jn 
note parameterization means parameters equal 
say dirichlet distribution conjugate prior class multinomial distributions 
property having conjugate priors characteristic exponential family distributions 
dirichlet prior natural coordinates multinomial distribution represented defined parameters 
infinitely ways parametrize distribution 
set parameters ry corresponding representation dirichlet prior results known change variable formula ry ry representing absolute value determinant jacobian 
note presence factor maximum different value different position parametrization 
dependence parametrization fundamental drawback map estimation justifies bayesian approximate bayesian approaches mentioned 
contrast mean measurable function measurable set independent parametrization 
particular mean dirichlet distribution independent parametrization equal jpr 
special interest called natural parametrization multinomial defined unconstrained parameters oe oei log 

parameters oe take values 
reverse transformation oe coordinates coordinates defined pr oej pr 
natural parametrization dirichlet distribution expressed oe gamma rj rj gamma pr pr remarkable property natural parametrization mode coincides position mean 
see suffices equate partial derivatives dirichlet distribution oe parameters 
calculations obtains pr ip rj 
equivalently 
dirichlet priors trees mixtures show assumptions likelihood equivalence says data help discriminate structures represent probability distribution parameter modularity says parameters corresponding edge tree prior time edge tree parameter independence says directed tree parametrization parameters edge independent :10.1.1.156.9918
combined weak technical assumptions imply parameter prior dirichlet 
case trees likelihood equivalence automatically assured definition tree parametrizations directed represent distribution 
parameter modularity parameter independence implicitly assumed basis tree fitting procedure 
requiring decomposable prior tree structures preserves parameter modularity 
requiring decomposable priors fits naturally framework adopted introduces additional constraints 
show likelihood equivalence constrains dirichlet priors parameter sets share common equivalent sample size :10.1.1.156.9918
results hold trees mixtures described framework 
easy see learning tree distributions dirichlet parameters collapse fictitious sufficient statistics represented pairwise counts times 
case trees prior parameters possible edges tree described set fictitious marginal counts 
ru 
rv alternatively normalize counts express dirichlet prior trees table fictitious marginal probabilities uv pair variables plus equivalent sample size gives strength prior 
important note fictitious marginal counts alternatively pairwise marginals uv set arbitrarily 
values uv satisfy uv omega xu xv xu xv technical assumptions assumptions amount positivity joint prior 
take example set binary variables assume pab pab pbc pbc 
implies constrains pac pac 
joint distribution omega 
represent system linear equality inequality constraints 
verifying directly theoretically straightforward computationally intractable account exponential number columns system matrix number order omega 
notice uninformative prior uv urv valid represents set pairwise marginals uniform distribution omega 
dirichlet prior represented natural parameters empirical distribution sample size fact dirichlet prior conjugate prior follows finding map tree equivalent finding ml tree 
consequently parameters optimal tree tu puv previous section equation represent mean posterior distribution 
parameter independence assumption conclude optimal tree distribution mean posterior distribution structure 
mixtures maximizing posterior translates replacing gamma equation 
step em algorithm fits optimal tree distribution gamma gamma kp 
want distinguish trees different dirichlet priors tree 
previous arguments step resulting trees values represent mean respective posterior distribution 
imply mixture kt mean posterior 
presence hidden variable cancels independence assumption allows conclude single tree 
prior called bdeu prior :10.1.1.156.9918
note passing uninformative prior denoted metric valid prior point view equations 
chapter accelerating tree learning algorithm 
fast said fish 
dr cat hat chow liu cl algorithm chapter algorithm algorithm builds quadratic dimension domain due fact minimize kl divergence kl log algorithm needs mutual information iuv pair variables domain 
empirical distribution obtained set data computation mutual information values requires time memory quadratic number variables linear size dataset shown section computationally expensive step fitting tree data 
time memory requirements step acceptable certain problems may prohibitive dimensionality domain large 
example domain information retrieval 
information retrieval data points documents data base variables words vocabulary 
common representation document binary vector dimension equal vocabulary size 
vector component corresponding word appears document 
number documents data base order 
vocabulary sizes thousands tens thousands 
means fitting tree data necessitates mutual information computations counting operations 
problem particularity document contains relatively small number words order component values corresponding binary vector null 
call property data sparsity 
sparsity improve time memory requirements algorithm 
rest chapter shows answer question 
shows improvement carried learning mixtures trees models 
call algorithms obtained accelerated chow liu algorithms acl 
table offers preview results achieved 
symbol represents measure data sparsity nk number steps kruskal algorithm 
acl step mnn msn step mn sn nk obvious general proportionality size data set improved needs examine data point 
focus improving dependence situation follows mwst algorithms efficient algorithms know run time proportional number candidate edges ne 
tree learning problem ne results algorithms quadratic number variables additional information weights completely arbitrary edge uv weight equal mutual information iuv variables fact better 
way mwst algorithm uses edge weights comparisons 
idea compare mutual informations pairs variables computing possible 
way partially sorting edges running mwst algorithm significant savings running time achieved 
second important idea exploit sparsity data computing pairwise marginals puv 
combining result algorithms fitting tree distribution certain assumptions jointly subquadratic running time memory 
start stating assumption underlying 
assumptions binary variables 
variables take values set 
variables takes value say say 
loss generality assume variable times dataset 
binary variables assumption eliminated section 
integer counts 
target distribution derived set observations size pv pv nv represents number times variable dataset 
assumption pv nv exclude non informative variables ensuring strict positivity pv nv 
denote nuv number times variables simultaneously 
call events cooccurrence marginal puv puv nuv puv nu nuv puv nv nuv puv nv nu nuv information necessary fitting tree summarized counts nv nuv 
assumed non negative integers 
consider represented counts 
section assumption considerably relaxed 
sparse data 
denote number variables observation define sparsity data max example data documents variables represent words vocabulary represents maximum number distinct words document 
time memory requirements accelerated cl algorithm going introduce depend sparsity lower sparsity efficient algorithm 
assumed constant 
shall see assumptions introduced sparsity assumption assumption dispense program accelerating tree learning algorithm 
data dimension ratio bounded ratio number data points vs dimension domain bounded bounded away 
rmin nn rmax technical assumption useful 
plausible assumption large accelerated cl algorithms idea comparing mutual informations binary variables mutual information variables expressed notations iuv hu hv nu log nu nu log nu log nv log nv nv log nv log nuv log nuv nu nuv log nu nuv nv nuv log nv nuv nu nv nuv log nu nv nuv log knowing fixed dataset follows iuv function variables nv nu nuv 
fix nuv nu analyze variation mutual information nv iuv nv log nv log nv log nv nuv log nu nv nuv log nv nu nv nuv nuv nv equating derivative obtains extremum easy verify minimum corresponds mutual independence note uv necessarily integer 
importantly calculation practical implication assume fixed nu fixed 
assume variables sorted decreasing nv interested variables ordering 
denote fact express intuitive way assume edge uv directed lower ranking variable higher ranking 
case mutual informations variables correspond weights edges outgoing partition set variables edges sets vc 
number nuv cooccurrences partition sets vc nuv nv nuv nv subsets iuv varies monotonically increasing decreasing nv 
means sorted increasing respectively decreasing nv variables set sorted decreasing order mutual information way achieve partial sorting mutual informations iuv computing mutual informations involved 
obtain iuv need calculate compare elements lists 
procedure save substantial amounts computation provided total number values nuv small data sparse pairs variables cooccur 
consequently list fewer elements lists 
pool significant cost computation time 
accelerated algorithms introduced uses lists second uses 
second idea computing cooccurrences bipartite graph data representation bipartite graph data representation 
xn set observations binary variables probability 
efficient represent observation list variables respective observation 
data point xi 
represented list list xiv 
name representation comes depiction bipartite graph edge vi corresponds variable observation space required representation sn nn smaller space required binary vector representation data 
data variables bipartite graph representation sparse data set 
edge iv means variable data point 
computing cooccurrences bipartite graph representation note total number nc cooccurrences dataset nc vo nuv previously defined sparsity data 
data point contains variables contributing cooccurrences sum 
result 
shown shortly computing cooccurrence counts takes amount time logarithmic factor 
algorithm computes cooccurrence numbers nuv constructs lists 
data sparse expect lists average larger lists 
representing explicitly construct complement representing sorted union lists words list variables cooccur 
assume variables sorted decreasing nv ties broken arbitrarily 
algorithm serve building block accelerated chow liu acl algorithms subsections introduce 
acl uses lists shown 
second acl ii simplifies process creating lists pooling contents rest lists variables cooccur list cu sorted mutual information iuv 
variable shall initialize temporary storage cooccurrences denoted organized fibonacci heap heap 
lists computed follows algorithm input list variables sorted decreasing nu dataset 

initialize 

insert 
construct lists construct list output lists vc 
construct set lists vc proceed follows compute empty vnew extract max vnew new set cooccurrences insert corresponding lists vnew insert corresponding lists insert set cooccurrences insert corresponding lists step nv insert insert insert algorithm works follows contains entry cooccurrence extract elements sorted order cooccurrences come sequence 
store keep increasing count new value comes heap 
time store previous variable corresponding vc 
different come decreasing order know insert insert obtain desired sorting lists 
list variables cooccur sorted decreasing nv 
assumed variables cooccur efficient store complement 
insertion vc assumes implicitly test list created create necessary 
handle insertion efficiently store current maximum value variable cmax 
maintain vector pointers vc lists dimension cmax 
list empty corresponding pointer null pointer 
way insertion list takes constant time 
time includes checking list initialized 
insertion lists followed insertion variable list constant time operation 
amount additional storage incurred method discussed 
running time 
shown insertion extremity list takes constant time 
extracting maximum heap takes logarithmic time size heap 
extracting elements heap size lu takes log log lu 
lu log lu 
bounding time empty heaps 
extracting elements heaps takes pu lu log lu 
knowing pu lu nc easy prove maximum attained lu equal 
performing calculations obtain log nn 
total number list insertions proportional remains compute time needed create 
know insertion heap takes constant time nc cooccurrences insert 
algorithm runs log nn time 
memory requirements memory requirements temporary heaps equal nc space required final lists proportional total number cooccurrences 
remains bound space taken vectors pointers lists 
space larger oe cmax 
total number cooccurrences nc length length nc constraint oe maximized length lists cmax equal 
cmax 
follows oe total space required algorithm 
putting acl algorithm data structures far efficient method partially sorting mutual informations candidate edges 
aim create mechanism output edges uv decreasing order mutual information 
shall set mechanism form heap called contains element represented edge highest mutual information edges outgoing maximum set obviously maximum mutual informations possible edges eliminated 
record form iuv iuv key sorting 
maximum extracted edge replaced edge lists 
place kruskal algorithm construct desired spanning tree 
outline algorithms algorithm acl outline ne number tree edges phi ne uv extract max uv create cycle edges add uv ne get new uv insert output remains show efficiently construct suitable 
hard lists vlist sorted list variables 
suffices take element list called compute respective mutual information iuv insert triple iuv heap 
extract element implicitly represented discussed 
extracting maximum provide desired maximum edges originating quadruple iuv inserted replace eliminated edge list 
remains show handle variables cooccur maintain pointer pu vlist 
initially pu points successor vlist 
compare element equal increment delete head recursively find vlist cooccur compute mutual information insert triple iuv ilist 
get variable cooccurring increment repeat recursively comparison procedure head 
summarize algorithm algorithm acl input variable set size dataset 

compute nv create vlist list variables sorted decreasing nv 
partial sort mutual informations 
create vlist create iuv extract max replacement insert iuv 
storing nuv values edges added 
uv compute probability table tuv nu nv nuv output time storage requirements running time step compute variables frequencies nv 
done scanning trough data points increasing corresponding nv time 
procedure take sn operations 
adding time initialize nv time sort nv values log gives upper bound running time step acl algorithm log sn running time second step estimated log nn step takes mutual information computation head list plus log cmax operations extract maximum insert reader reminded insertion heap constant time 
cmax denote maximum number non empty lists associated variable total number lists denoted nl 
notations total time step nl log cmax step kruskal algorithm 
edge extracted inserted 
extraction takes log size new insertion involves extraction done log cmax computation mutual information constant time 
checking current edge uv create cycle adding done constant time 
denote nk total number edges examined kruskal algorithm step takes number operations order nk log step computes probability tables constant time 
running time adding terms obtain upper bound running time acl algorithm log log nn nl log cmax nk log refine result bounding nl cmax 
appendix computes upper bound number lists nl 
third assumption holds bounded away bound reduces sn 
cmax shall obvious upper bound cmax nc notice cmax influences running time logarithm bound polynomial 
expression running time acl algorithm log log nn sn log nk log nn simplifies log log nn sn nk log nn sn nk bound ignoring logarithmic factors polynomial degree variables nk 
know nk total number edges inspected kruskal algorithm range nk 
worst case algorithm quadratic reasons believe practice dependence nk subquadratic 
random graph theory suggests distribution weight values edges kruskal algorithm take number steps proportional log 
result sustained experiments conducted ran kruskal algorithm sets random weights domains dimension 
runs performed 
shows average maximum nk plotted versus log curves display close linear dependence 
memory requirements store data results need sn dataset bipartite graph representation store variables store resulting tree structure parametrization 
additional storage required algorithm includes vlist lists created step algorithm 
step created memory 
space occupied proportional number elements nl 
seen number sn 
steps auxiliary storage total space algorithm nl bound appendix sn acl ii algorithm data sparse simplify acl algorithm 
lists replaced single list cu variables cooccurring difference cu list introduced previously records cu contain additional field mutual information iuv sorted 
list steps mean full line standard deviation maximum dotted line kruskal algorithm steps nk runs plotted log ranges 
edge weights sampled uniform distribution 
heap size nu ee ee pi pi pi pi acl ii algorithm data structure supplies candidate edge 
vertically left variables sorted decreasing nu 
lists cu list variables sorted decreasing order iuv virtual list sorted decreasing nv 
maximum elements lists inserted heap maximum iuv extracted maximum heap 
preserves function supplying vlist variables cooccurring decreasing order mutual information implies computing iuv cu ahead time nc maximum weight edge outgoing obtained comparing mutual informations corresponding current heads lists cu 
data structure shown schematically algorithm described 
algorithm acl ii input variable set size dataset 

compute nv create vlist list variables sorted decreasing nv 
construct lists compute mutual informations iuv create sort cu 
create vlist argmax iuv 
storing nuv values edges added 
uv compute probability table tuv nu nv nuv output time memory requirements acl ii steps algorithm copy corresponding steps acl 
analyze steps 
constructing discussed previously take log nn time memory 
need compute mutual informations sort resulting lists 
worst case lists equal 
total time bounded log log result encountered 
constructing takes units time memory 
extraction log 
extractions virtually represented take nk nc time steps nc elements skipped 
nk number steps taken kruskal algorithm 
running time acl ii algorithm log sn log nn nk log additional memory requirement bounded 
comparing appears possible determine algorithm asymptotically faster bounds 
algorithms large parts identical factor log nn loose overestimate formulas may running times algorithms close reality 
important fact acl ii algorithm managed relax assumptions previous section algorithm rely integer counts 
difference essential context em algorithm 
generalization discrete variables arbitrary arity section show algorithm accelerated discrete domains variables take values 
algorithm going develop simple extension acl ii algorithm 
shall basic ideas extension modifications acl ii algorithm imply 
acl algorithms introduced previously exploiting data sparsity 
generalized necessary extend notion sparsity 
forthcoming shall assume variable exists special value appears higher frequency values 
value denoted loss generality 
example medical domain value variable represent normal value abnormal values variable designated non zero values 
similarly diagnostic system indicate normal correct value non zero values assigned different failure modes associated respective variable 
occurence variable event cooccurrence means non zero data point 
define number non zero values observation sparsity maximum data set 
anticipated high frequency values help accelerate tree learning algorithm 
shall represent occurrences explicitly creating compact efficient data structure 
shall demonstrate way mutual informations non cooccurring variables similar acl ii algorithm 
computing cooccurrences previously introduced idea representing explicitly values data point replaced list xlist variables occur 
non zero value list store value variable index 
xlist list xv xv 
similarly cooccurrence represented quadruple xu xv xu xv 
counting storing cooccurrences done time proportionally larger amount memory required additional need store nonzero variable values 
cooccurrence count nuv shall way contingency table 
represents number data points 
contingency table marginal counts jv defined number data points completely determine joint distribution consequently mutual information iuv 
constructing cooccurrence contingency tables multiplies storage requirements step algorithm max change running time 
mutual informations subsection goal mutual informations iuv cooccur shall show done exactly 
derivations clearer terms probabilities shall notations pv iv pv pv pv quantities represent empirical probabilities value respectively 
entropies denoted chain rule expression entropy discrete variable 
entropy hv multivalued discrete variable decomposed way hv pv log pv pv log pv pv log pv pv pv pv log pv pv log pv pv log pv pv log pv hv pv pv pv log pv pv hv hv pv hv decomposition represents sampling model choose zero outcome non zero choose remaining values sampling distribution pv pv pv hv uncertainty associated choice hv hv entropy outcome second 
advantage decomposition purpose separates outcome encapsulates uncertainty number hv 
mutual information non cooccurring variables shall fact find expression mutual information iuv non cooccurring variables terms pu pv hu 
iuv hu hu second term conditional entropy hu pv hu pv hu term equation non zero value condition nuv implies 
develop hu decomposition equation 
hu hu pu hu non zero time non zero values paired zero values knowing brings additional information know 
probabilistic terms hu hu term hu entropy binary variable probability 
probability equals pu pu pv pu note order obtain non negative probability equation needs pu pv condition satisfied cooccur 
replacing previous equations formula mutual information get iuv pu log pu pv log pv pu pv log pu pv expression remarkably depends pu pv 
partial derivative respect pv yields iuv pv log pv pu pv value negative independently pv 
shows mutual information increases monotonically occurrence frequency pv 
note expression derivative rewrite result obtained binary variables case nuv 
shown acl ii algorithm extended variables values making minor modification replacement scalar counts nv nuv vectors jv respectively contingency tables 
acl algorithms em far shown accelerate cl algorithm assumption target probability distribution defined terms integer counts nv nuv true fitting tree distribution observed data set case classification models data points partitioned observed class variable 
important application cl algorithm mixtures trees case learning mixtures em algorithm counts defining component trees integer 
recall step em algorithm computes posterior probability mixture component having generated data point xi 
flk defined equation 
values fl effect weighting points dataset values different trees mixture 
counts kv kuv corresponding tree defined terms fl values gamma flk kv xiv flk kuv xiv flk 
counts general integer numbers 
learning mixtures trees acl ii algorithm recommended 
shall examine steps acl ii algorithm show modify order handle weighted data 
step sort variables times producing different components 
computing kv values done similarly previous section modification occurrence data point adds flk kv incrementing counter 
operations done pairs variables cooccur original data set preserving algorithm guarantee efficiency 
step similar approach taken 
time inserting store flk 
heap emptied current count sums flk values corresponding note fact data sparse accelerate step 
precompute frequent likelihood delta 
point xi multiplies ratio kv pa pa kv pa pa precomputed 
way step run msn time previously computed mnn 
decomposable priors acl algorithm assumes tree mixture fit data maximum likelihood framework 
section study possibility priors conjunction acl algorithm 
classes priors shall concerned priors discussed chapter 
shall examine priors tree structure having form exp uv 
shown section prior translates penalty weight edge uv seen mwst algorithm iuv easily seen general values modification handled acl algorithms 
affect ordering edges outgoing way counts nv nuv 
constant pairs ordering edges affected 
need acl algorithm constant penalty fi compare iuv edge moment extracted kruskal algorithm quantity fin algorithm stops soon edge mutual information smaller penalty fin proceeds 
course context em algorithm replaced gamma fi different component mixture 
variables binary number values mdl type edge penalty translates constant 
regarding dirichlet priors tree parameters shown represented set fictitious counts uv maximizing posterior probability tree equivalent minimizing kl divergence kl mixture empirical distribution fictitious distribution defined uv 
challenges assumptions accelerated algorithms 
counts nv nuv cease integers 
affects acl algorithm 
second algorithms rely fact nuv values 
counts uv violate assumption acl algorithms inefficient 
particular acl ii algorithm degrades standard algorithm 
having uv rare case 
particular characteristic non informative priors aim smoothing model parameters 
means smoothing priors acl algorithms general compatible 
uniform prior constitutes exception prior case binary variables fictitious cooccurrence counts equal 
fact prove small large values order mutual informations preserved respectively reversed 
fact allows run acl ii algorithm efficiently slight modification 
experiments experiments compare hypothesized gain speed accelerated algorithm traditional version chapter controlled conditions artificial data 
binary domain dimensionality varying 
data point fixed number variables 
sparsity took values time real running time accelerated full line traditional dotted line algorithm versus number vertices different values sparsity kruskal number steps kruskal algorithm nk versus domain size measured acl ii algorithm different values 
small values chosen gauge advantage accelerated algorithm extremely favorable conditions 
larger value help see performance degrades realistic circumstances 
data point representing list variables generated follows variable picked randomly range 
subsequent points sampled random walk random step size 
pair set points generated 
data set acl ii fit tree distribution running times recorded plotted 
improvements traditional version sparse data spectacular learning tree variables data points takes hours traditional algorithm seconds accelerated version data sparse 
acl ii algorithm takes minutes complete improving traditional algorithm factor 
noticeable running time accelerated algorithm independent dimension domain 
side number steps nk grows observation implies bulk computation lies steps preceding kruskal algorithm proper 
computing cooccurrences organizing data time spent 
observation deserve investigation large real world applications 
confirms running time algorithm grows quadratically independent concluding remarks chapter way advantages sparsity data accelerate tree learning algorithm 
ideas introduced developed algorithms exhausting number possible variants 
methods achieve performance advantage characteristics data sparsity problem weights represent mutual informations external maximum weight spanning tree algorithm proper 
algorithms evaluated considering data sparsity constant 
case actual complexity algorithms computed easily bounds provided dependence explicit 
shown empirically significant part algorithms running time spent computing cooccurrences 
prompts applying trees mixtures trees high dimensional tasks focus methods structuring data computing approximating marginal distributions mutual informations specific domains 
acl ii algorithm relies heavily fact cooccurrences variables zero 
fact smoothly handle real values non zero cooccurrence counts classes priors 
acl algorithm restrictive sense requires integer counts perspective clear disadvantage acl ii simplicity versatility 
appendix bounding number lists nl denote total number nonempty lists nl 
obvious nl maximized list lengths constant determined 
case number lists expressed successively nl cx nc solving obtain amounts nl nc ns assumption nn bounded equation simplifies nl sn chapter approach hidden variable discovery cl de fire st 
asi vor gest le ce fr virgin 
ion heaps tangled straw 
find clean gesture comprise straighten golden beam divine triangle eye 
chapter presents way mixtures trees learning algorithm discovering structure particular class graphical models hidden variables 
process structure discovery model selection model validation important components 
goal mind introduce novel method validating independencies graphical models hidden variables scope broader class models investigated 
structure learning paradigms structure discovery challenging fascinating problems graphical models field 
surprise task identifying dependencies independencies observations core scientific discovery intellectual process 
field loosely categorized areas research 
category structure learning methods oldest conditional independence tests 
introduce algorithm constructs bayesian network consistent list independence statements :10.1.1.38.1913
similar result exists decomposable models described method learn decomposable models decomposable approximations markov network models oracle conditional independence queries 
algorithm exponential size largest separator 
works impressive success inverting relationship graphical model structure list independencies 
hand strength results reflects strength underlying assumptions 
words methods assume input information hard obtain graph 
testing conditional independence exponential 
method assumes know independencies domain implicitly pairs independent 
give list independencies 
assuming undertake intractable task performing required independence tests faced uncertain contradictory information methods prepared handle 
presents algorithm finding polytree structure mutual information measurements underlying distribution known polytree 
chapter presents method learning star decomposable graphs case binary variables ideal correlation measurements 
methods drawback requiring ideal information obtained data 
introduces algorithm learning minimal map distribution data 
algorithm evaluation pairwise mutual informations variables independence tests require computational effort proportional size largest separator shown efficiently sparse graphs 
implicit assumption complete independence information obtained pairwise independence tests assumption examined chapter 
assumed sufficient data ensure correct answer independence tests assumption brings group ones requiring ideal information 
second category methods explicitly takes account existence finite sample data points structure learned 
influential bayesian approach pioneered developed heckerman geiger chickering classical :10.1.1.156.9918
state problem bayesian learning bayesian belief networks data explicit assumptions underlying approach show calculate network structure score 
score model comparison 
prove optimizing bayesian score networks structures np hard node allowed parent 
local search search algorithms typically generate proposed additions deletions edge graph proposed moves evaluated computing approximation marginal likelihood updated graph exact calculations performed cases graphs hidden nodes :10.1.1.156.9918
local search common approach structure learning far 
methods scores proposed 
cited works assume data complete hidden variables 
develop method scoring equivalence classes dags hidden variables special case dependence relationships jointly gaussian augmented heuristic search algorithm 
third category methods includes methods consider learning data explicitly aimed finding dependencies 
methods attempt construct complete graphical model find dependencies independencies possible 
directly find dependencies discrete databases counting coincidences subsamples 
constructs minimal markov nets heuristic way 
methods aiming discovering structure high dimensional domains applying sophisticated algorithms computationally expensive 
little explored area research area approach belongs 
star decomposable graph tree visible domain augmented hidden variables hidden variable degree 
method developed forthcoming fundamentally method learning data 
belongs third category taxonomy fact mainly concerned finding certain class dependency structures fully determined graphical model 
relates second category fact generates multiple models compares score defined 
problem variable partitioning approach structure learning thesis differs methods mentioned 
adopting local search heuristic single edge addition deletion methods second category looking dependencies methods third category attempts find separators 
separator variable group variables observed separates independent subsets 
separator exists learning structure domain reduces learning structures number possible structures grows partitioning domain search space reduced manner 
separators proceed recursively effectively implementing conquer structure learning algorithm 
call approach variable partitioning 
separator included called visible 
visible separator small size practically variables brute force manner trying possibilities 
number trials trial implies testing conditional independence large sets exponential task 
shown chapter section certain assumptions aforementioned independence tests reduced tests pairs variables 
trial quadratic focuses different situation case separator hidden variable 
case brute force approach sufficient different method needs devised 
machinery developed case applicable visible separators sense visible separator simple case hidden variable separator 
hidden variable separator shortly hidden variable assumed partition clusters variables mutually independent conditioned maintain computational tractability assume restrictive architecture cluster particular assume cluster tree 
propose algorithm goal find set trees find choice tree structure cluster 
examine happens true distribution data violates restrictive assumption 
denote usual set variables interest 
remember xv particular value xa assignment variables subset shorthand xv variables referred visible 
introduce additional variable hidden unobservable variable denoted variable takes values ranging joint space denoted assumption joint distribution essential assumption 
hidden variable partitions mutually disjoint subsets term originates junction tree literature 
model graphical model distribution marginalized 

ap sj aj conditionally independent write aj aj 
consider graphical model corresponding 
assumption implies graph topology 
edge set union contains directed edge hidden variable visible variables contains undirected edges pairs visible variables 
induced graph connected components 
graphical model language conditional independence translates separating respective variable clusters view model mixture factorial models hyper variables call model satisfies assumption hyper mixture factorials short model 
models chain graphs 
note aj mutually independent imply aj independent marginalizing values graphically aj mutually separated general separated graph gm em corresponding marginal distribution mx compute number parameters models represented simplifying assumption represented probability tables 
pars pars sizes close introducing hidden variable result exponential savings terms number parameters 
formulate variable partitioning task follows domain sj aj aj aj set observations 
xn sj aj find sets aj number values distribution ph distribution fli xi values hidden variable observed instances xi course final goal construct model domain variable partitioning completed choices model precisely graph consistent assumption equivalent structure type algorithm learns construct method including recursive call variable partitioning algorithm probabilistic model domains aj sizes domains smaller task considerably easier constructing model directly 
aware important difficulties quest face 
searching space possible partitions size 
forbidding task 
shown learning structure belief networks data absence hidden variables intractable problem :10.1.1.156.9918
second model mixture model know learning mixtures certain instances np hard 
reasons show provide heuristic procedures coping large scale important search problem 
theoretical difficulties shown graphical models hidden variables belong curved exponential family bic mdl criterion consistent estimate marginal likelihood model structure exponential family models 
computing bayesian posterior marginal probabilities model structure represents equally intractable computation 
generally known models hidden variables subject non identifiability problems 
simple example illustrate suppose observed distribution binary variables uniform know distribution marginal mixture distribution ph pab underdetermined system equations unknowns ph pab possible solutions ab ab ab ab distributions uniform marginal convex combination 
infinity mixture models marginal observed variables 
non identifiability cause severe convergence problems learning model 
task identifiability important absence find hidden variable solutions potentially infinity 
explain data equally 
identifiability hidden variable models area current research 
existent results preliminaries general theory 
case models binary variables gives sufficient condition highly relevant problem 
theorem model domain consisting binary variables identifiable hold pa pa fully connected graphical models 
tree model algorithm variable partitioning derived additional assumptions model structure 
shall assume known 
define tree model ajj model trees structure fixed parameters may vary denote structure connected component ej 
union edge set sj ej 
distinct connected components conditional distribution pv viewed product independent trees ajs tree having connected components 
enables avoid explicitly specifying notation view special case mixture trees shared structure 
allows efficient algorithms devised mixtures trees fitting tree models 
tree model obviously mixture trees shared structure learning done variant learning algorithm 
derivation algorithm closely parallels section final description thereof included 
algorithm input dataset 
xn procedure kruskal weights fits maximum weight tree edges minimum number connected components initial model initialize em 
iterate convergence step compute xi 

step 
gamma 

compute marginals kv kuv 

compute mutual information 

call kruskal iuv generate 
kuv kuv kv kv output see change standard algorithm step need construct maximum weight tree prescribed maximum number edges 
kruskal algorithm fits purpose easily adds edges order reader aware distinction mixture components indexed values connected components subtrees tree mixture component indexed 
decreasing weight 
proved stopping kruskal algorithm adds edges results maximum weight spanning tree number edges 
variable partitioning general case outline procedure tractable algorithm learning special case models tree models 
step apply algorithm data known generated tree model 
need proceed caution model selection validation criteria evaluate output algorithm 
procedure outlined stage process stage generates models different values different structures parameters repeatedly calling algorithm 
second stage validates models produced stage testing independencies imply contradicted data computes score valid model 
different models compared score 
method summarized follows algorithm learn outline input dataset 
xn confidence level ffi parameters mmax pmax trials procedure 
mmax 
pmax 
trials initialize randomly validate confidence score mm output tvalid score mm models best score rest chapter studies develops approach 
subtasks involved evaluating description length model testing independence detailed descriptions sections respectively 
defining structure simple explanation set goal learning real domain inherently unobserved variables having little prior knowledge know discovered right interactions variables shall define structure case structure simple view data simplicity measured description length model 
reader may realized proof similar proof theorem 

stated version known minimum description length mdl principle 
dl dl dl term log negative base log likelihood 
term decomposed dl bits bits structure mx bits parameters structure note tree structure requires number bits independently variable specifying parent oe 
term constant models 
computing description length amounts computing description length parameters distributions straightforward task 
approach ways 
approximation model description length derived rissanen known bayes information criterion bic 
assumes log description length parameters 
bic edge uv contributes delta ru rv log bits addition description tree edges take bottom line 
minimization dl tree structures automatically incorporated em algorithm assigning iuv delta 
method advantage indirectly number connected components drawback equation penalizes parameters component equally 
see problem imagine component responsible data parameters component influence data likelihood model penalized 
sensible replace formula representing effective number parameters 
heuristic remedy tried problem take account fact parameters estimated different amounts data considered edge uv contributes total penalty delta ru rv mx max log gamma gives criterion 
second approach directly approximate dl model 
achieve represent probability values number bits smaller machine precision obtaining approximate model mb 
chosen range dl mb easily nonuniform prior tree structures 
small implicitly assumed prior term vary computed 
approximate true dl argmin dl mb full description procedure included section 
method purely method models learned penalty equations 
number connected components consequence weight modification weights negative 
kruskal algorithm adding edges encounters weight implicitly controls penalty increase favoring sparser structures 
alternatively vary previously fixed range compare resulting models description lengths 
selecting description length dl provides consistent criterion comparing model structures different numbers trees enabling select local optima deal problem usual way restarting em algorithm times time different random points model space 
consequently new run em algorithm may different model structure parameters help inasmuch problem role em algorithm diverse candidate structures 
experiments experimental procedure experiments described aim assess ability algorithm discover correct structure hidden variable cases data generated mixture trees 
cases evaluate description length alternative scores discussed model selection criteria task 
experiments run artificial data generated steps 
model structure created figures show structures 
domain visible variables hidden variable values 
second structure sets random parameters sampled resulting model generate data set examples training testing 
values hidden variable recorded served testing purposes 
structure tree model general models 
case pilot experiments shown discovering variable partitioning relatively easy true partitioning visible marginal distribution data data sets checked property accepted true structure recovered fitting single tree 
experiments tree models group experiments demonstrates algorithm capable recovering structure generative model tree model 
model structure shown 
random parametrizations structure generated training set test set 
training set models learned mdl edge penalty choose number connected components comparison case tree model training 
models training 
fi tried 
set parameters algorithm run times different random initializations 
results data sets displayed 
see correct structure cases corresponding mdl edge penalty 
high accuracy means values hidden variable correctly estimated 
seen description length higher true model 
number values hidden variable correctly selected 
results second data set similar correct structure trials 
example successful structure learning hidden variable discovery bars learning task section 
general models experiments run data sets generated structure 
parameter setting learning algorithm run times different initial points 
initial points chosen randomly including knowledge desired structure 
stated empirical description length dl model selection criterion 
diverge little algorithm outlined previously leaving model validation examining properties resulting models 
series experiments run models fixed true values 
shows trials correct structure prevailed terms number times appeared terms description length 
cases dl criterion best model correct structure description length bits accuracy accuracy description length bits description lengths accuracies models fixed variable learned data generated tree model 
structures correct 
general detail upper left corner 
cases best models correct 
accuracy hidden variable retrieval relatively high baseline accuracy high 
remaining data sets correct structure score structures best models correct structure 
run series similar experiments applying mdl edge penalty set situation learning algorithm consistently overestimated structure compatible correct 
searched space values manner similar search purpose models trained 
results encouraging produced significant number models shorter description length 
suggests presence residual dependences components taken additional link especially case investigated model validation phase 
far estimating appears hard task 
set experiments studies effect varying fixed true value 
models trained data sets 
summarizes results set sees models cluster separately region higher description lengths 
things happen proportion structures increased data sets description length models decreases remains case settles hidden variable accuracy remains stationary cases 
cases minimum dl model best structure data set best models correct 
explanation behavior true mixture components tree distributions adding components mixture trees helps model true distribution closely original mixture components allocated tree 
explanation sustained examining confusion matrices trees data represents comes component original mixture 
confusion matrix data set 
columns trees rows original components matrix element represents proportion data description length bits accuracy description length bits accuracy description length bits accuracy description length bits accuracy description length bits accuracy description length bits accuracy description length bits accuracy description length bits accuracy description length bits accuracy description length bits accuracy model structures scores accuracies obtained learning tree models data sets generated structure 
circles represent structures bad structures 
marks lowest dl models 
axes measure empirical description length bits example vertical axes measure accuracy hidden variable retrieval bottom line description length bits accuracy model structures scores accuracies obtained learning tree models variable data set generated structure 
circles represent structures bad structures 
sizes symbols proportional axis measures empirical description length bits example vertical axis measures accuracy hidden variable retrieval bottom line 
notice decrease dl increasing component modeled tree cluster accuracy experiments show choosing solely minimizing description length lead overestimation number components 
gap dl dimension models rest models data sets study possibility criterion selecting comparison different model section criteria results plotted shown replacing turn bic modified bic test set log likelihood horizontal axis order see differences selecting correct structure 
test log likelihood fails select best model cases dl think sufficient evidence loglikelihood description length 
differences favor empirical description length significant way 
results obtained second structure qualitatively similar 
approximating description length model described method approximate dl mixture trees model 
general method probability model graphical 
idea encode model best fixed number bits compute description length data approximated model called mb 
smaller length original representation local maximum likelihood term dl mb larger original dl model dl equal smaller original dl 
varying reasonable range minimizing total description length obtain dl mb upper bound local minimum desired description length 
better encoding sense preserving likelihood observed data point closer minimization true dl 
practice realize tradeoff encoding quality computational cost 
want cheap encoding schemes preserve represented distribution reasonably 
evaluate dl mixture trees idea approximate dl tree parameters mixture dl 
assume structure description takes number bits tree pointers possibly null parent node ignored dl 
dl parameters approximated log parameters tree distributions encoded number bits number sum precisions bm bm defined 
number parameters equation 
total number bits excluding constant structure dl bm bm ru rv rv ru log rv show encode parameters trees 
directed tree implementation parameters represent values conditional probabilities tv pa xv course pa empty set 
fixed value pa distribution multinomial 
describe way encoding multinomial distribution 
encoding multinomial distribution encoding set real numbers finite number bits involves sort quantization 
encode parameters distribution needs quantize 
quantization multinomial distribution parameters done natural parametrization introduced section 
distribution values 
jmax index largest 
natural parameters reproduced slightly modified form oei log ilog 
jmax 
adding minus sign front logarithm function ensures oe parameters positive 
parameters allowed range 
going represent oe values finite number bits 
done stages large oe values truncated constant bm exceed 
corresponds bounding smallest values away 
resulting values represented bm bit precision binary numbers equivalently multiplied bm stored integers 
note bm controls representation precision small probabilities bm control precision large probabilities 
precisions relative largest value 
full encoding represented encoding precision bm position maximum jmax values bm min oei bm 
jmax decode values representation apply formula bm replace maximum position jmax note value bm necessary decoding 
value separate table remember encoding distributions domain bm encoded separately total number bits encode bm bm ss log bm bm adding right hand sides values tv pa obtains terms 
see free parameter encoded bm bm bits ru log rv rv log ru positive numbers ru rv 
search tractable values bm bm common distributions bm recorded 
model validation independence testing alternate independence test assume independent discrete variables ru rv values respectively sample size drawn joint distribution 
variables independent mutual information obviously zero estimate sample iuv generally exactly equal zero 
section want bound probability sample mutual information iuv larger threshold 
course iuv exceed max log ru log rv assumed value 
shall large deviation theory particular sanov theorem obtain bound 
denote puv pu pv respectively sample joint distribution marginals 
shall consider sets probability distributions domain omega uv set represents factorized distributions omega uv 
notation problem formulated probability sample distribution belong true distribution sanov theorem gives result distribution closed set probability distributions probability sample distribution sample size drawn denoted qn satisfies qn kl size domain kl distribution closest kl divergence 
know distribution omega uv kl kl 
minp kl minp result sanov theorem conclude qn nm words independent variables probability sample mutual information iuv exceeds threshold bounded nm ffi 
conversely probability ffi derive corresponding log ffi log bound distribution free requires assumption distributions expected value fixed ffi decreases sample size contrast test assumptions underlying distribution values analogues independent sample size 
closure interior sanov theorem reasoning similar enables establish lower bound qn qn nm interesting compare test devised call sanov test known test independence 
easy show statistic approximating kl divergence 
compute quantity kl pu pv puv function counts nij times ni times nj times nij 
ru 
rv 
somewhat tighter bound equation kl 
kl pu pv puv ij nij log nij nij ln ij nij ln ij ln ij nij ln nij ij ss ln ij nij nij ij nij nij nij ij 
ln ij nij nij nij ln sanov test kl puv pu pv independence test approximation reverse divergence kl pu pv puv 
course motivation test divergence central limit theorem arguments 
threshold mixtures undertake estimate approximate confidence interval sample mutual information variables independent third variable case true joint distribution mx usual values represent probabilities variable state 

assume sample size sample group data points values groups size nk respectively 
mx nk denote sample mutual information group denote ae probability sample mutual information size sample exceed 
ae qn course ae function ru rv true distribution simplify notation drop variables 
acceptable ru rv fixed dependence masked bounds hold goal fix confidence level ffi find thresholds mk 
depending ffi mk probability ffi 
start expressing probability event mk function values mk arrive convenient expression equate ffi 
introduce notation sample size representing values 
nm tuple nk values defined 


nm 
denote choose 
nm multinomial coefficient indexed 
nm 
mk yk nk ae nk mk 
yk nk ae nk mk factors ae vary slowly show happens sum dominated term corresponding largest nn 
qk value attained ss kn 

stirling approximation ss ne 
yk qk kn 
kn kn kn 
ss pk allows write ss ae mk choose mk mk kn log ffi log kn ffi ae kn mk ffi kn ss ffi ffi ffi corresponding ffi ffi ffi ss conclude prove assumption functions ae nk mk vary slowly argument 
precisely interested upper bounding 
ae nk mk ae mk ae mk ffi small values ffi quantity close unity validating previous derivation 
validating graphical models hidden variables section apply previous test models interest graphical models hidden variable separates observed ones conditionally independent subsets 
assume set variables 
xn observations model mx kt ka xa kb xb partition assumes hidden variable values 
probabilities want test data supports independence implied referring sanov test developed section arguments similar hold independence test 
issues need addressed case variable observed 
monte carlo sampling posterior flk obtain sequences 
zn 
compute 
compare respective thresholds 
second establishing independence sets variables implies summation omega omega omega 
configuration space usually larger size available sample 
omega threshold giant comparison meaningless 
comparable omega calculations intractable 
test independence performing independence tests pairs variables general true 
true broad class distributions 
particular true distribution data chain graph holds 
test independence pairs variables computationally intensive tractable 
performing simultaneous tests take account 
bonferroni inequality states hypothesis true confidence conjunction true confidence uv 
case represent hypotheses denote events mk 
special reason treat differently ffl ffl test procedure summarized follows algorithm model data set confidence ffl 
ffl ffl fix ffi compute ffi 
compute fl flk 

posterior probability xi generated component mixture 
compute thresholds mk repeat sample fl 

compute compare mk ffi confidence ffl means reject means accept output accept independence hypothesis tests step 
accept reject crux method step need estimate low probabilities high confidence ffl pair course freedom choose ffi closer order require fewer iterations compare ffi required confidence level 
larger ffi means lower threshold mk turn means loosening approximation sanov theorem increasing probability type error false acceptance 
choice ffl reflects tradeoff type type ii errors 
small ffl probability rejecting model rejecting independence hypothesis correct low accept independence true 
probability inflated case dividing ffl tests 
better accept somewhat larger false rejection risk choosing larger ffl lest test permissive 
examine assumptions tests true procedure 
individual independence tests dependent share observed values 
xn bonferroni inequality accounts 
dataset dataset model trained 
independence test depart assumptions 
data observed quantity estimated repeating 
expressed mk yi mk mk yi correct test predict bound probability 
involves intractable summation mk 
general fact independence tests utility 
independence test reject independence provide estimate risk doing 
seen examining proof sanov theorem 
accept independence estimating risk error associated decision usually difficult 
discussion chapter exploration domain learning structure graphical models hidden variables 
proposed mixture trees learning algorithm order uncover presence hidden variable partitions visible variables set mutually independent subsets 
model useful number values hidden variable small offers possibility drastically reduce complexity resulting density 
hidden variable corresponding separation discovered model structure refined subsequent structure learning separately variable clusters 
knowledge top approach problem structure learning 
stressed task attempt solve known difficult 
searching space possible structures proven nphard 
transformed continuous domain optimization problem em algorithm relying multiple initializations obtain candidate structures 
empirically showed unreasonable em algorithm capable uncovering hidden structure converge structure detectable marginal distribution visible variables 
problem model selection domains hidden variable topic current research 
introduced method empirically evaluate description length distribution 
empirical description length number criteria bic holdout set likelihood modified bic compared task selecting best model structure 
examined criteria relatively similar accuracy slight advantage empirical description length 
criteria proved completely reliable selecting structure 
reason task scoring function matter impossibility distinguish failure discover independency structure information interest side inaccuracy representation conditional distributions structure information irrelevant task 
context comparing models score experiment determining correct number connected components easy task 
particular data experiments mdl edge penalty systematically strong biasing models false independencies 
alternate approach test obtained models independencies assume 
testing independence models hidden variables unsolved problem 
propose heuristic monte carlo sampling novel approach independence testing large deviation theory 
independence tests allow eliminate structures hypothesised variable clusters independent hidden variables help model fewer connected components correct cases considered minor structure error 
risk discarding structure correct residual dependencies trees simple represent distributions variable cluster induce inaccuracy values hidden variable 
contrast case working assumption data generated tree model correct proved easier 
rate structure recovery em algorithm structure parameters correct high 
likewise accuracy values hidden variable 
model selection fixed search values model selection criteria empirical dl reliable 
chapter experimental results section describes experiments run order assess capabilities usefulness mixture trees model 
experiments examine ability algorithm recover original distribution data generated mixture trees 
models artificial data 
group experiments studies performance mixture trees density estimator data experiments generated mixtures trees 
perform classification experiments 
experiments study mixture trees classifier consisting single tree trained density estimator 
comparisons classifiers trained supervised unsupervised mode 
section ends discussion single tree classifier feature selection properties 
experiments stated training algorithm initialized random independently data knowledge desired solution 
expressed bits example called compression rates 
lower value compression rate better fit data 
experiments involve small data sets smoothing procedure 
technique applied called smoothing marginal described 
computes pairwise marginal distributions data set replaces marginals kuv kuv ff kuv ffp intuitively effect operation give small probability weight unseen instances trees similar reducing effective model complexity 
method effective practice ff function gamma kuv 
particular experiments thesis global smoothing parameter ff mixture components mixture component tree edges 
ff equation global smoothing parameter may larger 
cases fitting single tree smoothing uniform distribution smoothing marginal 
recovering structure random trees large data set experiment generated mixture trees variables vertices 
distribution choice variable tree structure bit negative base logarithm likelihood 
training examples bars learning task 
parameters sampled random 
mixture generate data points training set algorithm 
initial model components random 
compared structure learned model generative model computed likelihoods learned original model test dataset consisting points 
results retrieving original trees excellent trials algorithm failed retrieve correctly tree trial 
result accounted sampling noise tree wasn recovered 
recovering missing tree algorithm fit identical trees generating tree highest 
difference log likelihood samples generating model approximating model bits example 
correctly recovered trees approximating mixture higher log likelihood sample set generating distribution 
shows structure distribution recovered correctly 
random bars small data set bars problem benchmark structure learning problem neural network unsupervised learning algorithms 
variants describe experiments 
domain square binary variables depicted 
data generated manner flips fair coin decide generate horizontal vertical bars represents hidden variable model 
bars turned independently black probability pb 
noise added flipping bit image independently probability pn 
learner shown data generated process task learner discover data generating mechanism 
mean discover data generating mechanism mixture trees 
mixture trees model approximates true structure low levels noise shown 
note tree variables forming bar equally approximation 
consider structure discovered model learns mixture having connected components bar 
additionally shall test classification accuracy learned model vert pb pn pn pn pn pb true structure probabilistic generative model bars data 
hh mixture trees approximate generative model bars problem 
interconnection variables bar arbitrary 
smoothing value test likelihood bits case test set log likelihood bars learning task different values smoothing ff different averages standard deviations trials 
comparing true value hidden variable horizontal vertical value estimated model data point test set 
compare results directly neural network results mainly published results qualitative focusing model able learn structure data 
closely replicate experiment described way experiment slightly difficult theirs 
training set size data set contains ambiguous examples examples bars consequently ambiguous point view classification noise level small zero zero 
authors studying problem assume incomplete knowledge structure case number hidden units neural net similar train models 
choose final model likelihood holdout set 
typical values literature problem harder increasing choose 
probabilities pb pn respectively 
test set size 
obtain trees connected components small edge penalty fi 
data set small smoothing different values ff 
value run learning algorithm times training set different random initial points average results 
selection average test set log likelihoods bits standard deviations 
best model 
structure recovery 
examined resulting structures trials structure recovery perfect sense explained remaining trial learned model missed correct structure altogether converging different local minimum apparently closer initial point 
comparison examined training methods structure recovered respectively cases 
table results bars learning task 
test set ambiguous unambiguous bits class accuracy example digit pair 
result held range smoothing parameter ff 
classification 
classification performance shown table 
result reported obtained value ff chosen test set log likelihood 
note data generated previously described mechanism model achieve perfect classification performance 
due ambiguous examples shown upper row third left bars bars 
probability ambiguous example current value pb plb pb ambiguous examples appear class equal probability error rate caused 
comparing theoretical upper bound value corresponding column table shows model classification performance trained ambiguous examples 
support second test set size generated time including non ambiguous examples 
classification performance shown corresponding section table rose 
table shows likelihood test data learned model 
ambiguous test set bits away true model entropy bits data point 
non ambiguous test set compression rate significantly worse surprise distribution test set different distribution model trained 
density estimation experiments digits digit pairs images tested mixture trees density estimator running subset binary vector representations handwritten digits measuring average log likelihood 
datasets consist normalized quantized binary images handwritten digits available postal service office advanced technology 
data set call digits contained images single digits dimensions second called pairs hitherto contained dimensional vectors representing randomly paired digit images 
displays example digit pair 
training validation test table average log likelihood bits digit single digit digit double digit pairs datasets 
boldface marks best performance dataset 
results averaged runs 
digits pairs set contained exemplars respectively 
data sets training conditions algorithms compared described 
model trained training set likelihood validation set stops increasing 
tried mixtures trees fitted basic algorithm 
digits pairs datasets chose mixture model highest log likelihood validation set calculated average log likelihood test set bits example 
averages runs shown table 
notice small difference test likelihood models due early stopping training algorithm 
compare result results published algorithms plotted mixture factorial distributions mf completely factored model assumes variable independent called base rate br helmholtz machine trained wake sleep algorithm helmholtz machine mean field approximation training fully visible fully connected sigmoid belief network fv 
table displays performances mixture trees models tested 
results mixture trees absolute winner compressing simple digits comes second model pairs digits 
comparison particular interest comparison performance mixture trees mixture factored distribution 
spite structural similarities mixture trees performs significantly better mixture factorial distribution indicating exists structure exploited mixture spanning trees captured mixture independent variable models 
comparing values average likelihood mixture trees model digits pairs see second twice 
suggests model just mixture factored distributions able perform compression digit data unable discover independence double digit set 
alarm network data set second set density estimation experiments features alarm network 
bayes net model benchmark model structure learning experiments 
medical diagnostic alarm message system patient monitoring constructed expert knowledge 
domain discrete variables values connected directed arcs 
alarm net constructed complete data set experiments data generated model 
artificial data interesting experiment reasons data generating br mf gzip fv mt likelihood bits digit br mf gzip fv mt likelihood bits digit average log likelihoods bits digit single digit double digit datasets 
mt mixture spanning trees mf mixture factorial distributions br base rate model helmholtz machine trained wake sleep algorithm helmholtz machine trained mean field approximation fv fully visible fully connected sigmoidal bayes net 
notice difference scale figures 
distribution tree mixture trees nodes parent topology dag sparse suggesting dependence approximated mixture trees small number components generative model captures features real domain practical importance 
generated training set having data points separate test set data points 
sets train compare models mixtures trees mixtures factorial distributions true model training gzip training 
mixtures trees factorial distributions comparison way separate validation set data points training set models different values trained remaining data evaluated training process converges validation set 
value run trials 
averaged value log likelihood validation set choose optimal models evaluated test set likelihood achieved compared classes models 
results table 
result reported gzip obtained writing data file binary format additional information removed presenting gzip program 
see data set mixture trees approximate true distribution perfectly 
note difference likelihood values true model training test set suggesting data set thousands large data set domain 
models approximate true distribution mixture trees clear winner 
ahead gzip base rate model expected mixture factorials 
superiority mixture trees model clear 
examine sensitivity algorithms size data set ran experiment training set size 
results table 
mixture trees closest true model mixture table density estimation results mixtures trees models alarm data set 
training set size 
average standard deviation trials 
model train likelihood test likelihood bits data point bits data point alarm net mixture trees mixture factorials base rate gzip table density estimation results mixtures trees models data set size generated alarm network 
average standard deviation trials 
model train likelihood test likelihood bits data point bits data point alarm net mixture trees ff mixture factorials ff base rate gzip factorials larger margin large data set 
notice degradation performance mixture trees relatively mild bit model complexity reduced drastically mixture components context additional parameter smoothing 
indicates important role played tree structures fitting data motivates advantage mixture trees mixture factorials data set 
classification mixtures trees mixture trees classifier classification common important task probabilistic models 
density models trained data sole purpose classification 
procedure asymptotically optimal empirical evidence offer performance practice 
section devoted experimentally assessing performance mixture trees model classification tasks 
density estimator turned classifier ways essentially likelihood ratio methods 
denote class variable set input variables method adopted classification experiments name mixture trees classifier mixture trees model trained domain treating class variable variable pooling training data 
testing phase new instance omega classified picking value class variable variables settings 
xc second method calls partitioning training set values class variable training density estimator partition 
equivalent training mixture trees visible choice variable choice variable class method trees forced structure obtain tree augmented naive bayes classifier 
classify new instance turns bayes formula australian data set investigated performance mixtures trees classification tasks uci repository 
experiment data set australian database 
examples consisting attributes binary class variable 
attributes numbers real valued discretized attribute respectively attributes roughly equally sized bins 
experiments order compare test training set sizes respectively ratio roughly 
value tested ran algorithm fixed number epochs training set recorded performance test set 
repeated times time random start random split test training set 
series runs training parameters fixed second series runs fi table performance comparison mixture trees model classification methods australian dataset 
results mixtures factorial distribution reported 
results 
method correct method correct mixture trees fi backprop mixture factorial distributions side cal smart bayes trees logistic discrimination nearest neighbor linear discrimination ac newid radial basis functions lvq cart alloc castle cn naive bayes quadratic discrimination flexible bayes accuracy classification performance different mixture trees models australian credit dataset 
results represent percentage correct classifications averaged runs algorithm 
table performance mixture trees models mushroom dataset 
models 
algorithm correctly soft class test 
train 
classified bits datapoint bits data point smoothing smooth marginal ffm ffp smooth uniform ffm ffp edge pruning parameter change way keep mfi approximatively constant 
results roughly fraction pruned edges independently number components 
results slightly better previous experiment 
common series experiments relatively large set values performance stays top range 
hypothesize caused multiple ways models smoothed edge pruning smoothing marginals early stopping 
best performance mixtures trees second case compared published results dataset table 
comparison correct classification rates obtained cited training test sets size mixtures factorial distributions best model decision tree called cal 
full description methods 
seen dataset mixture trees achieves best performance followed mixture factorial distributions 
mushroom data set second data set mushroom database 
data set instances discrete attributes including class variable treated attribute purpose model learning 
training set comprised randomly chosen examples test set formed remaining 
smoothing methods penalty ffp entropy mixture variable smoothing marginal uniform distribution 
smoothing coefficient ffm divided mixture components proportionally gamma dataset smoothing effective reducing overfitting improving classification performance 
results shown table 
soft classification expresses integrated measure confidence classifier 
note classification correct classifier achieved high confidence 
splice data set 
classification structure discovery task classification dna splice junctions 
domain consists variables representing sequence dna bases additional class variable 
error rate error rate ff ff nn tree mt delve tree nb train test train test error rate error rate ff ff delve tree nb delve tree nb train test train test comparison classification performance mixture trees splice data set 
models tested delve left right nearest neighbor cart hme hierarchical mixture experts ensemble learning hme early stopping hme grown nearest neighbors linear squares linear squares ensemble learning mixture experts ensemble learning early stopping 
tree augmented naive bayes nb naive bayes classifier tree represents mixture trees mt trees 
knowledge neural net nn neural net 
task determine middle sequence splice junction type 
class variable take values ei junction variables take values corresponding possible dna bases coded symbols 
data set consists labeled examples 
compared mixture trees model categories classifiers performed series experiments 
third experiment involving splice data set described 
series compared model performance reported results multilayer neural networks knowledge neural networks task 
sizes training set test set reproduced cited papers examples respectively 
constructed trees mixtures trees different smoothing values ff 
fitting single tree done step 
fit mixture separated examples training set learned model em algorithm remaining 
training stopped likelihood validation set stopped decreasing 
regarded additional smoothing model 
results averaged trials seen tree mixture trees model perform similarly single tree showing better classification accuracy 
single tree times fewer parameters mixture strongly prefer model task 
notice situation smoothing improve performance unexpected data set relatively large 
exception mixture trees model ff trees mixture trees models significantly outperform models tested problem 
note mixture trees contains prior knowledge domain models neural network model trained supervised mode optimizing class accuracy includes detailed domain knowledge training begins 
second set experiments pursued comparison benchmark experiments splice data set part delve repository 
delve benchmark uses subsets splice database examples training 
testing done examples cases 
algorithms tested delve performances shown fitted single trees different degrees smoothing 
learned naive bayes nb tree augmented naive bayes models 
nb model fit step just tree 
delve protocol ran algorithms times different random initializations training testing sets 
results figures striking plots dramatic difference methods delve classification obtained simple tree cases error rates tree models half performance best model tested delve 
average error single tree trained examples away average error trees trained examples dataset 
explanation remarkable accuracy preservation decrease number examples discussed section 
naive bayes model exhibits behavior similar tree model slightly accurate 
augmenting naive bayes model significantly hurts dna composed sections useful coding proteins called exons inserted sections non coding material called introns 
splice junctions junctions exon intron types exon intron ei represents exon intron intron exon places intron ends exon coding section begins 
cumulative adjacency matrix trees fit examples splice data set smoothing 
size square coordinates ij represents number trees edge variables square means number 
lower half matrix shown 
class variable 
group squares bottom shows variables connected directly class 
variable relevant classification 
surprisingly located vicinity splice junction 
subdiagonal chain shows rest variables connected immediate neighbors 
lower left edge upper right edge 
ei junction exon intron tree ca ag true ca ag junction intron exon 
tree ct ct ct ct true ct ct ct ct ct encoding ei splice junctions discovered tree learning algorithm compared ones watson molecular biology gene 
positions sequence consistent variable numbering splice junction situated positions 
symbols boldface indicate bases probability symbols indicate bases groups bases high probability indicates position occupied base non negligible probability 
classification performance 
plots allow observe effect degree smoothing varies large 
contrast previous experiment splice data smoothing beneficial effect classification accuracy values certain threshold larger values accuracy strongly degraded smoothing 
accuracy profiles observed tree models naive bayes models similar bias variance tradeoff curves commonly encountered machine learning applications 
surprisingly effect diminishes increasing size data set invisible training set size 
discovering structure 
presents summary tree structures learned example set form cumulated adjacency matrix 
adjacency matrices graph structures obtained experiment summed 
size black square coordinates proportional value th element cumulated adjacency matrix 
square means respective element 
adjacency matrix symmetric half matrix shown 
see tree structure stable trials 
variable represents class variable splice junction situated variables 
shows splice junction variable depends dna sites vicinity 
sites remote splice junction dependent immediate neighbors 
examining tree parameters edges adjacent class variable observe variables build certain patterns splice junction random uniformly distributed absence splice junction 
patterns extracted learned trees shown 
displays true encodings ei junctions reader reminded adjacency matrix graph position ij graph edge connecting vertices 
cumulated adjacency matrix trees original set variables augmented noisy variables independent original ones 
matrix shows tree structure original variables preserved 

match encodings perfect 
conclude domain tree model provides classifier discovers true underlying model physical reality generates data 
algorithm arrives result prior knowledge know variable class doesn know variables sequence 
single tree classifier automatic feature selector examine single tree classifier splice data set closely 
separation properties tree distribution probability class variables depends neighbors variables class variable connected tree edges 
tree acts implicit variable selector classification variables adjacent queried variable set variables called markov blanket relevant determining probability distribution 
property explains observed preservation accuracy tree classifier size training set decreases variables relevant class dependence parametrized independent pairwise probability tables parameters accurately fit relatively examples 
long training set contains data establish correct dependency structure classification accuracy degrade slowly decrease size data set 
explain superiority tree classifier models delve previous example variables relevant class 
tree finds correctly 
classifier able perform reasonable feature selection hindered remaining irrelevant variables especially training set small 
exactly shows absolute difference classification error models delve tree largest smallest training set 
markov blanket tree classifies way naive bayes model markov blanket variables inputs 
naive bayes model built feature selector input variables relevant class distributions pv roughly values consequently posterior pc serves classification factors corresponding simplify influence classification 
explains naive bayes model performs splice classification task 
notice variable selection mechanisms implemented tree classifier naive bayes classifier 
sensitivity irrelevant features 
verify single tree classifier acts feature selector performed experiment splice data 
augmented variable set variables values randomly independently assigned probabilities 
rest experimental conditions training set test set number random restarts identical splice experiment 
fitted set models small fi smoothing compared structure performance corresponding model set experiment series 
structure new models form cumulative adjacency matrix shown 
see structure original variables unchanged stable noise variables connect random uniform patterns original variables 
expected examining structure classification performance new trees affected newly introduced variables fact average accuracy trees variables higher accuracy original trees 
standard deviation accuracy making difference insignificant 
chapter subject thesis tree tool statistical multivariate modeling 
tree graphical model ability express dependencies variables means graph separately detailed form dependencies contained parameters property excellent support human intuition allows design general inference learning algorithms 
trees simple models especially evident examines algorithm fits tree distribution 
information target distribution tree capture contained small number pairwise marginals 
simplicity leads computational efficiency 
efficient inference direct consequence fact trees decomposable models small clique size existence efficient algorithm learning tree structure property trees graphical models family 
thesis shows limitation trees modeling power overcome combining mixtures 
number trees mixture smoothing parameter allows control complexity resulting model 
point view ability graphically represent independencies mixtures strictly speaking graphical models independence relationships visible variables determined graph topology 
words dependency structure separated parametrization classes belief networks 
computational point view mixtures trees belief network perspective methods 
inherit computational properties trees terms inference learning data extent fact mixtures trees algorithms thesis combinations modifications corresponding algorithms trees 
learning algorithm mixtures trees introduced version known em algorithm 
converges local optimum 
property characteristic mixtures general model 
illustration fact search best model structure free lunch just learning optimal graphical model structure requires brute force search learning mixtures suffers local optima problem 
sensible ask class models chosen 
rule suggested discussion structure search graphical models problems significant knowledge dependency structure data structure search minimal necessary little prior knowledge structure dependencies take mixtures trees preferred candidate 
structure unknown simple splice junction example single tree may suffice task 
reader keep mind final answer depend properties actual data 
thesis examined mixture trees learning algorithm heuristic method hidden variable discovery 
research motivated part cases science everyday life identifying hidden variable enables providing computationally simple model change way thinking diseases hidden variables core medical knowledge 
second motivation seeing problem instance general divide conquer approach problem structure learning 
structure search requires model selection model validation thesis explored topics 
devised tested model selection criterion empirical measure model description length 
model validation side introduced new distribution free independence test 
making learning scale better dimensionality domain advantage certain properties data research topics concerned 
property built data sparsity loosely defined low entropy marginal distributions individual variables 
algorithms introduced reduced running time orders magnitude mainly intelligently reorganizing data manner exploits sparsity 
underlines importance combining graphical models perspective data mining techniques enabling conquer high dimensional domains exist 
devoting thesis trees may want ask unique properties advantages demonstrated 
classes graphical models learned efficiently data 
class 
class bayes nets variables topological ordering fixed number parents node bounded class optimal model structure target distribution nl time greedy algorithm 
models trees instances matroids 
matroid unique algebraic structure maximum weight problem corresponding maximum weight spanning tree problem solved optimally greedy algorithm 
class graphical models matroids structure learning algorithm better brute force 
finding classes open problem 
introducing studying mixture trees thesis extended scope graphical modeling unsupervised learning 
time aimed demonstrate potential exciting field research far exhausted 
bibliography irvine machine learning repository 
ftp ftp ics uci edu pub machine 
ann becker dan geiger 
sufficiently fast algorithm finding close optimal junction trees 
uai proceedings 
berrou glavieux 
near optimum error correcting coding decoding turbo codes 
ieee transactions communications 
brucker 
complexity clustering algorithms 
editors optimierung und operations research lecture notes economics mathematical systems pages 
springer verlag 
peter cheeseman john stutz 
bayesian classification autoclass theory results pages 
aaai press 
jie cheng david bell liu 
learning belief networks data information theory approach 
proceedings sixth acm international conference information knowledge management 
chow liu 
approximating discrete probability distributions dependence trees 
ieee transactions information theory may 
gregory cooper 
computational complexity probabilistic inference bayesian belief networks 
artificial intelligence 
gregory cooper edward herskovits 
bayesian method induction probabilistic networks data 
machine learning 
thomas cormen charles leiserson ronald rivest 
algorithms 
mit press 
thomas cover joy thomas 
elements information theory 
wiley 
robert cowell 
sampling replacement junction trees 
statistical research city university london 
sanjoy dasgupta 
learning polytrees 

dawid 
applications general propagation algorithm probabilistic expert systems 
statistics computing 
peter dayan richard zemel 
competition multiple cause models 
neural computation 
luis de campos juan 
algorithms learning decomposable models chordal graphs 
dan geiger prakash editors proceedings th conference uncertainty ai pages 
morgan kaufmann 
carl de marcken 
dependency detection subquadratic time 

dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
denise draper steve hanks 
localized partial evaluation belief networks 
proceedings th conference uncertainty ai 
morgan kaufmann publishers 
michael fredman robert endre tarjan 
fibonacci heaps uses improved network optimization algorithms 
association computing machinery july 
brendan frey geoffrey hinton peter dayan 
wake sleep algorithm produce density estimators 
mozer hasselmo editors neural information processing systems number pages 
mit press 
nir friedman dan geiger moises goldszmidt 
bayesian network classifiers 
machine learning 
nir friedman moises goldszmidt tom lee 
bayesian network classification continous attributes getting best discretization parametric fitting 
proceedings international conference machine learning icml 
nir friedman moses goldszmidt 
building classifiers bayesian networks 
proceedings national conference artificial intelligence aaai pages menlo park ca 
aaai press 
gabow galil spencer robert endre tarjan 
efficient algorithms finding minimum spanning trees undirected directed graphs 
combinatorica 
robert gallager 
low density parity check codes 
mit press 
dan geiger 
entropy learning algorithm bayesian conditional trees 
proceedings th conference uncertainty ai pages 
morgan kaufmann publishers 
dan geiger 
knowledge representation inference similarity networks bayesian multinets 
artificial intelligence 
dan geiger christopher meek 
graphical models exponential families 
proceedings th conference uncertainty ai pages 
morgan kaufmann publishers 
stuart geman donald geman 
stochastic relaxation gibbs distributions bayesian restoration images 
ieee transactions pattern analysis machine intelligence pami 
gilks richardson spiegelhalter editors 
markov chain monte carlo practice 
chapman hall london 
david heckerman 
tutorial learning bayesian networks 
technical report msr tr microsoft research 
david heckerman dan geiger david chickering :10.1.1.156.9918
learning bayesian networks combination knowledge statistical data 
machine learning 
hofmann volker tresp 
nonlinear markov networks continuous variables 
michael jordan michael kearns sara solla editors neural information processing systems number pages 
mit press 
tommi jaakkola 
variational methods inference estimation graphical models 
phd thesis massachusetts institute technology 
finn jensen 
bayesian networks 
springer 
finn jensen frank jensen 
optimal junction trees 
proceedings th conference uncertainty ai 
morgan kaufmann publishers 
finn jensen steffen lauritzen olesen 
bayesian updating causal probabilistic networks local computations 
computational statistics quarterly 
michael jordan zoubin ghahramani tommi jaakkola lawrence saul 
learning inference graphical models chapter variational methods graphical models pages 

kjaerulff 
approximation bayesian networks edge removals 
technical report report university department mathematics computer science 
petri kontkanen petri myllymaki henry tirri 
constructing bayesian finite mixture models em algorithm 
technical report univeristy department computer science 
kotz johnson eds 
encyclopedia statistical sciences 
wiley 
kullback leibler 
information sufficiency 
ann 
math 
stat march 
steffen lauritzen 
graphical models 
clarendon press oxford 
steffen lauritzen wermuth 
mixed interaction models 
research report institute electronic systems university denmark 
david mackay radford neal 
near shannon limit performance low density parity check codes 
electronics letters 
david madigan adrian 
model selection accounting model uncertainty graphical models occam window 
journal american statistical association 
marina michael jordan 
estimating dependency structure hidden variable 
michael jordan michael kearns sara solla editors neural information processing systems number pages 
mit press 
marina michael jordan morris 
estimating dependency structure hidden variable 
technical report aim cbcl massachusetts institute technology artificial intelligence laboratory 
revised version aim 
michie spiegelhalter taylor 
machine learning neural statistical classification 
ellis horwood publishers 
stefano monti gregory cooper 
bayesian network combines finite mixture model naive bayes model 
technical report university pittsburgh march 
radford neal 
connectionist learning belief networks 
artificial intelligence 
hermann ney ute essen reinhard kneser 
structuring probabilistic dependences stochastic language modelling 
computer speech language 
michiel noordewier geoffrey towell jude 
training knowledgebased neural networks recognize genes dna sequences 
richard lippmann john moody david editors advances neural information processing systems volume pages 
morgan kaufmann publishers 
judea pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufman publishers san mateo ca 
carl rasmussen radford neal geoffrey hinton drew van camp michael revow zoubin ghahramani robert 
delve manual 
www cs utoronto ca delve 
rissanen 
stochastic complexity statistical inquiry 
world scientific publishing new jersey 
jeff schlimmer 
mushroom database 
irvine machine learning repository 
jim smith 
geometry bayesian graphical models hidden variables 
proceedings th conference uncertainty ai pages 
morgan kaufmann publishers 
sibson 
optimally efficient algorithm single link cluster method 
computer journal 
spiegelhalter thomas best gilks 
bugs bayesian inference gibbs sampling version 
medical research council biostatistics unit cambridge 
peter spirtes thomas richardson 
polynomial time algorithm determining dag equivalence presence latent variables selection bias 
proceedings workshop pages 
elena barbara 
identification graphical log linear models unobserved variable 
technical report universit di 
evan 
automated motif discovery protein structure prediction 
phd thesis university toronto 
robert endre tarjan 
data structures network algorithms 
society industrial applied mathematics 
bo thiesson christopher meek maxwell chickering david heckerman 
learning mixtures bayes networks 
technical report msr por microsoft research 
geoffrey towell jude 
interpretation artificial neural networks mapping knowledge neural networks rules 
john moody steve hanson richard lippmann editors advances neural information processing systems volume pages 
morgan kaufmann publishers 
thomas verma judea pearl :10.1.1.38.1913
algorithm deciding set observed independencies causal explanation 
didier dubois michael wellman bruce ambrosio philippe smets editors proceedings th conference uncertainty ai pages 
morgan kaufmann 
james watson nancy hopkins jeffrey roberts joan alan weiner 
molecular biology gene volume benjamin cummings publishing edition 
yair weiss 
belief propagation revision networks loops 
technical report aim cbcl massachusetts institute technology artificial intelligence laboratory 
douglas west 
graph theory 
prentice hall 
joe whittaker 
graphical models applied multivariate statistics 
john wiley sons 

