erasure coding vs replication quantitative comparison hakim weatherspoon john kubiatowicz computer science division university california berkeley cs berkeley edu 
peer peer systems positioned take advantage gains network bandwidth storage capacity computational resources provide longterm durable storage infrastructures 
quantitatively compare building distributed storage infrastructure self repairing resilient faults replicated system erasure resilient system 
show systems employing erasure codes mean time failures orders magnitude higher replicated systems similar storage bandwidth requirements 
importantly erasure resilient systems order magnitude bandwidth storage provide similar system durability replicated systems 
today exponential growth network bandwidth storage capacity computational resources inspired new class distributed peer peer storage infrastructures 
systems farsite freenet intermemory oceanstore cfs past seek capitalize rapid growth resources provide inexpensive highly available storage centralized servers 
designers systems propose achieve high availability long term durability face individual component failures replication coding techniques 
wide scale replication potential increase availability durability introduces important challenges system architects 
system architects increase number replicas achieve high durability large systems 
second increase number replicas increases bandwidth storage requirements system 
contributions briefly quantify availability gained erasure codes 
second show erasure resilient codes order magnitude bandwidth storage replication systems similar mean time failure mttf 
third show employing erasure resilient codes increase mttf system orders magnitude simple replication storage overhead repair times 
contributions addition bandwidth comparison 
background common methods achieve high durability data complete replication parity schemes raid 
imposes extremely high bandwidth storage overhead provide robustness necessary survive high rate failures expected wide area 
erasure code provides redundancy overhead strict replication 
erasure codes divide object fragments recode fragments 
call rate encoding 
rate code increases storage cost factor key property erasure codes original object reconstructed fragments 
example encoding block divides block fragments encodes original fragments fragments increasing storage cost factor 
erasure codes superset replicated raid systems 
example system creates replicas block described erasure code 
raid level described erasure code 
data integrity erasure coding malicious environment requires precise identification failed corrupted fragments 
ability identify corrupted fragments potentially factorial combination fragments try reconstruct block combinations 
result system needs detect fragment corrupted discard 
secure verification hashing scheme serve dual purpose identifying verifying fragment 
necessarily case correctly verified fragments reconstruct block 
scheme increase bandwidth storage requirements shown times replication 
assumptions assume replicated erasure encoded systems consist collection independently identically distributed failing disks assumption data periodically repaired replace lost redundancy replicated erasure encoded systems 
case raid disk manufacturers failed disks immediately replaced new blank ones dissemination replica fragment block placed unique randomly selected disk 
postulate global sweep repair process scans system attempting restore redundancy reconstructing block redistributing lost replicas fragments new set disks repair raid triggered disk fails fundamentally different sweep repair 
type repair required data lost couple years regardless redundancy 
denote time period sweeps block epoch 
availability availability gained erasure codes result exploiting statistical stability large number components 
availability block computed follows probability block available total number fragments number fragments needed reconstruction total number machines world number currently unavailable machines probability block available equal number ways arrange unavailable fragments unreachable servers multiplied number ways arrange available fragments reachable servers divided total number ways arrange fragments servers 
machines percent currently simply storing complete replicas provides nines availability 
rate erasure coding document fragments gives document nines availability consumes amount storage bandwidth supporting assertion fragmentation increases availability 
ignoring types failures software errors operational errors configuration problems simple analysis 
system comparison system size total blocks write rate compare systems replication erasure codes 
section comparisons 
fix mean time failure mttf system repair epoch 
second fix storage overhead repair epoch 
fix mttf system storage overhead 
compare replicated erasure encoded systems denoted terms total storage total bandwidth leaving source entering destination total number disk seeks required sustain rate repair write read 
compare reads considering storage bandwidth amount data required read block systems fragments equivalent replica storage bandwidth requirements 
fix mttf repair epoch subsection compare replicated systems erasure encoded systems fixed system mttf repair epoch 
assuming store delete data total system size terms total number fixed size blocks reached systems lifetime computed total number users follows total seconds generally system size defined number users focus answering question resources required store data system long term 
define durability system expected mttf losing block sufficiently larger expected lifetime system number users 
total seconds derive compute storage bandwidth disk seeks required solving equations 
total bytes stored system total storage capacity required system function bandwidth required support writes repair total storage repair epoch number disk seeks required support repair writes reads 
repair bandwidth computed dividing total bytes stored repair epoch 
compute storage systems number replicas rate encoding block size 
compute bandwidth terms storage follows repair epoch system 
bandwidth due original data written repaired expressed compute number disk seeks required support writes repair reads 
size disk block 
equation states number disk seeks required dependent number replicas total number fragments throughput system size repair epoch number replicas fragments needed reconstruct block number replicas fragments fit disk block 
replicated system compared similar erasure encoded system bandwidth storage disk seek ratios numbers concrete parameters appropriate 
bolosky measured average workstation produces data 
associate workstation user 
set kb blocks kb disk blocks users months years 
consequence parameters calculate total blocks years 
analysis described reprinted appendix solve number replicas rate compute constraints respectively 
satisfy applying parameters equations produce result results show replicated system requires order magnitude bandwidth storage disk seeks erasure encoded system size 
fix storage overhead repair epoch formulas subsection verify durability system calculations 
example simple failure model section parameters section set repair time months replicas rate replicated erasure encoded systems apparent storage overhead factor 
appendix compute block replicated servers years block rate code servers years 
difference highlights advantage erasure coding 
fix mttf storage overhead final comparison fix mttf storage overhead replicated erasure encoded system 
implies storage bandwidth writes equivalent systems 
case erasure encoded systems repaired frequently require repair bandwidth 
section attain durability example devise systems years factor storage overhead blocks years 
replicated system meets requirements replicas repair epoch month 
erasure encoded system meets requirements code repair epoch months 
replicated system uses times bandwidth erasure encoded system repair 
years blocks described subsection factor storage overhead erasure encoded system meets requirements code repair epoch months replicated system replicas repair blocks instantly continuously 
discussion previous section advantages erasure codes caveats 
issues tob highlight need intelligent buffering data need caching 
client erasure resilient system sends messages larger number distinct servers replicated system 
erasure resilient system sends smaller logical blocks servers replicated system 
issues considered liability outweigh results section 
view way 
assume storage servers utilized number clients means additional servers simply spread larger client base 
second assume intelligent buffering message aggregation 
outgoing fragments smaller simply aggregate larger messages larger disk blocks consequences fragment size 
assumptions implicit exploration metrics total bandwidth number disk blocks previous section 
concern erasure resilient systems time server overhead perform read increased multiple servers contacted read single block 
simplest answer concern mechanisms durability separated mechanisms latency reduction 
consequently assume erasure resilient coding utilized durability replicas caching utilized latency reduction 
nice thing organization replicas utilized caching soft state constructed destroyed necessary meet needs temporal locality 
prefetching reconstruct replicas fragments fragments replicas replicas replicas fragments fragments updates fig 

hybrid update architecture updates sent central produces archival fragments time updates live replicas 
clients achieve low latency read access utilizing replicas directly 
fragments advance 
hybrid architecture illustrated 
similar provided oceanstore 
open research issues affect replicated erasure encoded systems alike 
failure independence troubling assumption previous sections failures independent identically distributed 
true sets storage servers 
list possible techniques address independence 
routing overlay networks chord pastry tapestry provide location routing infrastructure permits fragments distributed geographically diverse locations eliminating large class correlations caused natural disasters denial service attacks administrative boundaries 
second sophisticated measurement modeling techniques choose set nodes maximally independent fragment dissemination 
efficient repair action taken maintain replicas fragments despite failure replicas fragments lost 
sweep repair simplistic assumes data world reconstructed periodic basis 
entirely implausible object independent repaired parallel consume resources 
related idea global scale distributed persistent storage infrastructure motivated ross anderson proposal eternity service 
knowledge tradeoffs bandwidth storage disk seeks comparing replicated system erasure coded system discussed literature 
discussion durability gained building system erasure codes appeared intermemory 
authors describe technique increases object resilience node failure system incorporate repair mechanism increase objects durability 
appeared large body subject distributed storage 
system anonymous publishing uses information dispersal algorithm manner analogous erasure codes 
discussion section motivated need hybrid system 
oceanstore distributed storage system uses notion promiscuous caching replicas soft state read benefit erasure codes durability 
systems similar goals include past farsite 
past large scale peer peer storage utility 
farsite seeks provide distributed file system comprised cooperating trusting machines 
rely replication durability availability 
described availability durability gains provided erasure resilient system 
quantitatively compared systems replication systems erasure codes 
showed mean time failure mttf erasure encoded system shown orders magnitude higher replicated system storage overhead repair period 
novel result analysis showed codes order magnitude bandwidth storage replication systems similar mttf 
care taken take advantage temporal spatial locality erasure encoded systems order magnitude disk seeks replicated systems 

anderson eternity service 
proceedings 

bolosky douceur ely theimer feasibility serverless distributed file system deployed existing set desktop pcs 
proc 
sigmetrics june 

chen goldberg gottlieb yianilos prototype implementation archival intermemory 
proc 
ieee icde feb pp 


clark sandberg wiley hong freenet distributed anonymous information storage retrieval system 
proc 
workshop design issues anonymity unobservability berkeley ca july pp 


dabek kaashoek karger morris stoica wide area cooperative storage cfs 
proc 
acm sosp october 

dingledine freedman molnar project distributed anonymous storage service 
proc 
workshop design issues anonymity unobservability july 

druschel rowstron storage management caching past largescale persistent peer peer storage utility 
proc 
acm sosp 

kubiatowicz oceanstore architecture global scale persistent storage 
proc 
asplos nov acm 

patterson gibson katz case raid redundant arrays inexpensive disks may 

patterson hennessy computer architecture quantitative approach 
forthcoming edition 

ratnasamy francis handley karp schenker scalable content addressable network 
proceedings sigcomm august acm 

rhea wells eaton geels zhao weatherspoon kubiatowicz maintenance free global storage oceanstore 
proc 
ieee internet computing ieee 

stoica morris karger kaashoek balakrishnan chord scalable peer peer lookup service internet applications 
proceedings sigcomm august acm 

zhao joseph kubiatowicz tapestry infrastructure faulttolerant wide area location routing 
tech 
rep ucb csd university california berkeley computer science division april 
appendix durability derivation appendix describe mathematics involved computing mean time failure mttf particular erasure encoded block 
considering server failure model repair process described section calculate mttf block follows 
calculate probability fragment placed randomly selected disk survive epoch length epoch average life disk probability distribution disk lives 
equation derived similarly equation residual average lifetime randomly selected disk 
term reflects probability disk lifetime new fragment land disk early lifetime survive epoch 
probability distribution obtained disk failure distributions augmented assumption disks service years discarded data 
compute probability block reconstructed epoch number fragments block rate encoding 
formula computes probability fragments available epoch 
mttf block epoch size computed mttf equation computes average number epochs block expected survive times length epoch 
