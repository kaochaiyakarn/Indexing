statist 
soc 
part pp 
regularization variable selection elastic net hui zou trevor hastie stanford university usa received december 
final revision september summary 
propose elastic net new regularization variable selection method 
real world data simulation study show elastic net outperforms lasso enjoying similar sparsity representation 
addition elastic net encourages grouping effect strongly correlated predictors tend model elastic net particularly useful number predictors bigger number observations 
contrast lasso satisfactory variable selection method case 
algorithm called lars en proposed computing elastic net regularization paths efficiently algorithm lars lasso 
keywords grouping effect lars algorithm lasso penalization problem variable selection 
motivation consider usual linear regression model predictors xp response predicted 
xp model fitting procedure produces vector coefficients 
example ordinary squares ols estimates obtained minimizing residual sum squares 
criteria evaluating quality model differ circumstances 
typically aspects important accuracy prediction data difficult defend model predicts poorly interpretation model scientists prefer simpler model puts light relationship response covariates 
parsimony especially important issue number predictors large 
known ols poorly prediction interpretation 
penalization techniques proposed improve ols 
example ridge regression minimizes residual sum squares subject bound norm coefficients 
continuous shrinkage method ridge regression achieves better prediction performance bias variance trade 
ridge regression produce parsimonious model keeps predictors model 
best subset selection address correspondence trevor hastie department statistics stanford university stanford ca usa 
mail hastie stanford edu royal statistical society zou hastie contrast produces sparse model extremely variable inherent discreteness addressed breiman 
promising technique called lasso proposed tibshirani 
lasso penalized squares method imposing penalty regression coefficients 
owing nature penalty lasso continuous shrinkage automatic variable selection simultaneously 
tibshirani fu compared prediction performance lasso ridge bridge regression frank friedman uniformly dominates 
variable selection increasingly important modern data analysis lasso appealing owing sparse representation 
lasso shown success situations limitations 
consider scenarios 
case lasso selects variables saturates nature convex optimization problem 
limiting feature variable selection method 
lasso defined bound norm coefficients smaller certain value 
group variables pairwise correlations high lasso tends select variable group care selected 
see section 
usual situations high correlations predictors empirically observed prediction performance lasso dominated ridge regression tibshirani 
scenarios lasso inappropriate variable selection method situations 
illustrate points considering gene selection problem microarray data analysis 
typical microarray data set thousands predictors genes fewer samples 
genes sharing biological pathway correlations high segal conklin 
think genes forming group 
ideal gene selection method able things eliminate trivial genes automatically include groups model gene selected grouped selection 
kind grouped variables situation lasso ideal method select variables candidates efron lacks ability reveal grouping information 
prediction performance scenario rare regression problems 
possible strengthen prediction power lasso 
goal find new method works lasso lasso best fix problems highlighted mimic ideal variable selection method scenarios especially microarray data deliver better prediction performance lasso scenario 
propose new regularization technique call elastic net 
similar lasso elastic net simultaneously automatic variable selection continuous shrinkage select groups correlated variables 
fishing net retains big fish 
simulation studies real data examples show elastic net outperforms lasso terms prediction accuracy 
section define na elastic net penalized squares method novel elastic net penalty 
discuss grouping effect caused elastic net penalty 
section show na procedure tends regression problems 
introduce elastic net corrects problem 
efficient algorithm lars en elastic net proposed computing entire elastic net regularization paths computational effort single ols fit 
prostate cancer data illustrate methodology section simulation results comparing lasso elastic net section 
section shows application elastic net classification gene selection leukaemia microarray problem 

na elastic net 
definition suppose data set observations predictors 
yn response xp model matrix xj predictors 
location scale transformation assume response centred predictors standardized yi xij ij fixed non negative define na elastic net criterion na elastic net estimator minimizer equation arg min procedure viewed penalized squares method 
solving equation equivalent optimization problem arg min subject call function elastic net penalty convex combination lasso ridge penalty 
na elastic net simple ridge regression 
consider 
elastic net penalty function singular derivative strictly convex having characteristics lasso ridge regression 
note lasso penalty convex strictly convex 
arguments seen clearly fig 


solution develop method solve na elastic net problem efficiently 
turns minimizing equation equivalent lasso type optimization problem 
fact implies na elastic net enjoys computational advantage lasso 
zou hastie fig 

dimensional contour plots level shape ridge penalty contour lasso penalty contour elastic net penalty see singularities vertices edges strictly convex strength convexity varies lemma 
data set define artificial data set 
na elastic net criterion written arg min proof just simple algebra omit 
lemma says transform na elastic net problem equivalent lasso problem augmented data 
note sample size augmented problem rank means na elastic net potentially select predictors situations 
important property overcomes limitations lasso described scenario 
lemma shows na elastic net perform automatic variable selection fashion similar lasso 
section show na elastic net ability selecting grouped variables property shared lasso 
elastic net fig 

exact solutions lasso ridge regression na elastic net orthogonal design 
ols shrinkage parameters case orthogonal design straightforward show parameters na elastic net solution na elastic net ols sgn ols ols denotes positive part 
solution ridge regression parameter ridge ols lasso solution parameter lasso ols sgn ols fig 
shows operational characteristics penalization methods orthogonal design na elastic net viewed stage procedure ridge type direct shrinkage followed lasso type thresholding 

grouping effect large small problem west grouped variables situation particularly important concern addressed times literature 
example principal component analysis construct methods finding set highly correlated genes hastie 
az 
tree harvesting hastie uses supervised learning methods select groups predictive genes hierarchical clustering 
algorithmic approach hlmann performed clustering supervised learning 
careful study segal conklin strongly motivates regularized regression procedure find grouped genes 
consider generic penalization method zou hastie arg min positive valued 
qualitatively speaking regression method exhibits grouping effect regression coefficients group highly correlated variables tend equal change sign negatively correlated 
particular extreme situation variables exactly identical regression method assign identical coefficients identical variables 
lemma 
assume xi xj 
strictly convex 
minimizer equation 
lemma shows clear distinction strictly convex penalty functions lasso penalty 
strict convexity guarantees grouping effect extreme situation identical predictors 
contrast lasso unique solution 
elastic net penalty strictly convex enjoying property assertion 
theorem 
data parameters response centred predictors standardized 
na elastic net estimate 
suppose 
define xt xj sample correlation 
quantity describes difference coefficient paths predictors xj highly correlated consider xj theorem says difference coefficient paths predictor predictor 
upper bound inequality provides quantitative description grouping effect na elastic net 
lasso grouping effect 
scenario section occurs frequently practice 
theoretical explanation efron 

simpler illustration consider linear model 
tibshirani gave explicit expression easily obtain cos angle 
easy construct examples corr cos vanish 

bayesian connections lq penalty bridge regression frank friedman fu equation generalization lasso ridge regression 
bridge estimator viewed bayes posterior mode prior elastic net exp ridge regression corresponds gaussian prior lasso laplacian double exponential prior 
elastic net penalty corresponds new prior exp compromise gaussian laplacian priors 
bridge regression similarities elastic net fundamental difference 
elastic net produces sparse solutions bridge regression 
fan li proved lq penalty family lasso penalty produce sparse solution 
bridge regression keeps predictors model ridge regression 
automatic variable selection penalization primary objective lq penalization candidate 

elastic net 
deficiency na elastic net automatic variable selection method na elastic net overcomes limitations lasso scenarios 
empirical evidence see sections shows na elastic net perform satisfactorily close ridge regression lasso 
call na 
regression prediction setting accurate penalization method achieves prediction performance bias variance trade 
na elastic net estimator stage procedure fixed find ridge regression coefficients lasso type shrinkage lasso coefficient solution paths 
appears incur double amount shrinkage 
double shrinkage help reduce variances introduces unnecessary extra bias compared pure lasso ridge shrinkage 
section improve prediction performance na elastic net correcting double shrinkage 

elastic net estimate follow notation section 
data penalty parameter augmented data na elastic net solves lasso type problem arg min elastic net corrected estimates defined elastic net recall na elastic net elastic net na elastic net elastic net coefficient rescaled na elastic net coefficient 
scaling transformation preserves variable selection property na elastic net simplest way undo shrinkage 
properties na elastic zou hastie net described section hold elastic net 
empirically elastic net performs compared lasso ridge regression 
justification choosing scaling factor 
consider exact solution na elastic net predictors orthogonal 
lasso known minimax optimal donoho case implies na elastic net optimal 
scaling elastic net automatically achieves minimax optimality 
strong motivation rescaling comes decomposition ridge operator 
predictors standardized sample correlation 
ridge estimates parameter ridge ry rewrite usual ols operator correlations shrunk factor call decorrelation 
equation interpret ridge operator decorrelation followed direct scaling shrinkage 
decomposition suggests grouping effect ridge regression caused decorrelation step 
combine grouping effect ridge regression lasso direct shrinkage step needed removed rescaling 
ridge regression requires shrinkage control estimation variance effectively new method rely lasso shrinkage control variance obtain sparsity 
stand elastic net 
theorem gives presentation elastic net decorrelation argument explicit 
theorem 
data elastic net estimates xt easy see arg min lasso arg min theorem interprets elastic net stabilized version lasso 
note sample version correlation matrix elastic net shrinks identity matrix 
equations say rescaling elastic net penalization mathematically equivalent replacing shrunken version lasso 
linear discriminant analysis prediction accuracy improved replacing shrunken estimate friedman hastie 
likewise improve lasso regularizing equation 

connections univariate soft thresholding lasso special case elastic net 
interesting special case elastic net emerges 
theorem simple closed form arg min xi sgn xi observe xi univariate regression coefficient ith predictor estimates applying soft thresholding univariate regression coefficients equation called univariate soft thresholding ust 
ust totally ignores dependence predictors treats independent variables 
may considered illegitimate ust variants methods significance analysis microarrays nearest shrunken centroids classifier tibshirani shown empirical performance 
elastic net naturally bridges lasso ust 

computation algorithm lars en propose efficient algorithm called lars en solve elastic net efficiently proposed algorithm lars efron 

proved starting zero lasso solution paths grow piecewise linearly predictable way 
proposed new algorithm called lars solve entire lasso solution path efficiently order computations single ols fit 
lemma fixed elastic net problem equivalent lasso problem augmented data set 
algorithm lars directly create entire elastic net solution path efficiently computational efforts single ols fit 
note augmented data set observations variables slow computation considerably 
facilitate computation advantage sparse structure crucial case 
detail outlined efron 
kth step need invert matrix ak ak ak active variable set 
done efficiently updating cholesky factorization previous step 
note ga index set amounts updating cholesky factorization xt ak 
turns simple formula update cholesky factorization xt ak similar formula updating cholesky factorization xt golub van loan 
exact ak zou hastie function cholesky factorization xt ak 
addition calculating vector inner products nonactive predictors current residuals save computations simple fact zero elements 
word explicitly compute quantities algorithm lars 
economical record non zero coefficients active variables set lars en step 
algorithm lars en sequentially updates elastic net fits 
case microarray data necessary run algorithm early stopping 
real data simulated computational experiments show optimal results achieved early stage algorithm lars en 
algorithm steps requires pm operations 

choice tuning parameters discuss choose type value tuning parameter elastic net 
defined elastic net choice tuning parameter 
lasso conventional tuning parameter norm coefficients fraction norm 
proportional relationship parameterize elastic net 
advantage valued 
algorithm lars lasso described forward stagewise additive fitting procedure shown identical boosting efron 
new view adopts number steps algorithm lars tuning parameter lasso 
fixed elastic net solved algorithm lars en similarly number lars en steps second tuning parameter 
types tuning parameter correspond ways interpret piecewise elastic net lasso solution paths shown fig 

established methods choosing tuning parameters hastie 
chapter 
training data available tenfold cross validation cv popular method estimating prediction error comparing different models 
note tuning parameters elastic net need cross validate dimensional surface 
typically pick relatively small grid values say 
algorithm lars en produces entire solution path elastic net 
tuning parameter selected tenfold cv 
chosen giving smallest cv error 
computational cost tenfold cv ols fits 
twodimensional cv computationally usual setting 
case cost grows linearly manageable 
practically early stopping ease computational burden 
example suppose want variables final model may algorithm lars en steps consider best 
drop subscript parameter 

prostate cancer example data example come study prostate cancer 
predictors clinical measures log cancer volume log prostate weight age logarithm amount benign seminal invasion standardized coefficients svi pgg gleason age lcp standardized coefficients elastic net beta max beta beta max beta fig 

lasso estimates function elastic net estimates function estimates piecewise linear key property efficient algorithm solution paths show elastic net identical univariate soft thresholding example final model selected svi log penetration lcp gleason score gleason percentage gleason score pgg 
response logarithm prostate specific antigen 
ols ridge regression lasso na elastic net elastic net applied data 
prostate cancer data divided parts training set observations test set observations 
model fitting tuning parameter selection tenfold cv carried training data 
compared performance methods computing prediction mean squared error test data 
table clearly shows elastic net winner competitors terms prediction accuracy sparsity 
ols worst method 
na elastic net performs identically ridge regression example fails variable selection 
lasso includes svi pgg final model elastic net selects table 
prostate cancer data comparing different methods method parameter test mean squared error variables selected ols ridge regression lasso na elastic net elastic net svi lcp pgg gleason age zou hastie svi lcp pgg 
prediction error elastic net lower lasso 
see case elastic net ust selected big 
considered piece empirical evidence supporting ust 
fig 
displays lasso elastic net solution paths 
check correlation matrix predictors see medium correlations highest pgg gleason 
seen elastic net dominates lasso margin 
words lasso hurt high correlation 
conjecture ridge regression improves ols elastic net improve lasso 
demonstrate point simulations section 

simulation study purpose simulation show elastic net dominates lasso terms prediction accuracy better variable selection procedure lasso 
simulate data true model examples 
examples original lasso tibshirani compare prediction performance lasso ridge regression systematically 
fourth example creates grouped variable situation 
example simulated data consist training set independent validation set independent test set 
models fitted training data validation data select tuning parameters 
computed test error mean squared error test data set 
notation describe number observations training validation test set respectively 
details scenarios 
example simulated data sets consisting observations predictors 

pairwise correlation xi xj set corr 
example example 
example simulated data sets consisting observations predictors 
set corr 
example simulated data sets consisting observations predictors 
chose 
predictors generated follows xi xi xi xi xi independent identically distributed table 
median mean squared errors simulated examples methods replications method results examples example example example example lasso elastic net ridge regression na elastic net numbers parentheses corresponding standard errors medians estimated bootstrap mean squared errors 
elastic net independent identically distributed model equally important groups group members 
pure noise features 
ideal method select true features set coefficients noise features 
table fig 
box plots summarize prediction results 
see na elastic net poor performance example behaves identically ridge regression examples lasso example 
examples elastic net significantly accurate lasso lasso doing better ridge regression 
reductions prediction error examples respectively 
simulation results indicate elastic net dominates lasso collinearity 
table shows elastic net produces sparse solutions 
elastic net tends select variables lasso owing grouping effect 
example grouped selection required elastic net behaves oracle 
additional grouped selection ability elastic net better variable selection method lasso 
idealized example showing important differences elastic net lasso 
independent variables 
response generated 
suppose observe independent identically distributed 
observations generated model 
variables form group underlying factor form second group underlying factor 
group correlations group correlations 
oracle identify group important variates 
fig 
compares solution paths lasso elastic net 

microarray classification gene selection typical microarray data set thousands genes fewer samples 
unique structure microarray data feel classification method properties 
zou hastie mse mse mse lasso ridge lasso ridge lasso ridge mse lasso ridge fig 

comparing accuracy prediction lasso net ridge regression na elastic net elastic net outperforms lasso examples example example example example gene selection built procedure 
limited fact 
genes sharing biological pathway able include groups model automatically gene selected 
published results domain appears classifiers achieve similar low classification error rates 
methods select genes satisfactory way 
popular classifiers fail respect properties 
lasso fails 
support vector machine guyon penalized logistic regression zhu hastie successful classifiers gene selection automatically univariate ranking golub standardized coefficients table 
median number non zero coefficients method results examples example example example example lasso elastic net beta max beta standardized coefficients beta max beta elastic net fig 

lasso elastic net solution paths lasso paths unstable reveal correction information contrast elastic net smoother solution paths clearly showing grouped selection significant group trivial group decorrelation yields grouping effect stabilizes lasso solution recursive feature elimination guyon reduce number genes final model 
automatic variable selection method elastic net naturally overcomes difficulty ability grouped selection 
leukaemia data illustrate elastic net classifier 
leukaemia data consist genes samples golub 
training data set samples type leukaemia acute leukaemia type leukaemia acute leukaemia 
goal construct diagnostic rule expression level genes predict type leukaemia 
remaining samples test prediction accuracy diagnostic rule 
apply elastic net coded type leukaemia response classification zou hastie function fitted value indicator function 
tenfold cv select tuning parameters 
computation manageable 
time model fitted select significant genes predictors statistic scores tibshirani 
note screening done separately training fold cv 
practice screening affect results misclassification error misclassification error steps steps fig 

leukaemia classification gene selection elastic net early stopping strategy steps finds optimal classifier computational cost elastic net paths early stopping number steps convenient fraction norm computing depends fit step algorithm lars en actual values available tenfold cv algorithm stopped early training set steps equivalent table 
summary leukaemia classification results method tenfold cv test error number error genes golub support vector recursive feature elimination penalized logistic regression recursive feature elimination nearest shrunken centroids elastic net standardized coefficients steps elastic net fig 

leukaemia data elastic net coefficients paths numbers top indicate number non zero coefficients selected genes step optimal elastic net model fit step selected genes note size training set lasso select genes contrast elastic net selected genes limited sample size chosen tenfold cv bigger grouping effect stronger elastic net path relatively early stage screened variables model 
fitting tuning done training set classification error evaluated test data 
stopped algorithm lars en steps 
seen fig 
number steps algorithm tuning parameter elastic net classifier gives tenfold cv error test error genes selected 
fig 
displays elastic net solution paths gene selection results 
table compares elastic net competitors including golub method support vector machine penalized logistic regression nearest shrunken centroid tibshirani 
elastic net gives best classification internal gene selection facility 

discussion proposed elastic net novel shrinkage selection method 
elastic net produces sparse model prediction accuracy encouraging grouping effect 
empirical results simulations demonstrate performance elastic net superiority lasso 
class classification method elastic net appears perform microarray data terms misclassification error automatic gene selection 
zou hastie methodology motivated regression problems elastic net penalty classification problems consistent zhang loss functions including loss considered binomial deviance 
nice properties elastic net better understood classification paradigm 
example fig 
familiar picture boosting test error keeps decreasing reaches long flat region slightly increases hastie 
coincidence 
fact discovered elastic net penalty close connection maximum margin explanation rosset success support vector machine boosting 
fig 
nice margin explanation 
progress elastic net penalty classification reported 
view elastic net generalization lasso shown valuable tool model fitting feature extraction 
lasso explain success boosting boosting performs high dimensional lasso explicitly lasso penalty hastie friedman 
results offer insights lasso ways improve 
rob tibshirani ji zhu helpful comments associate editor referee useful comments 
trevor hastie partially supported dms national science foundation ro eb national institutes health 
hui zou supported dms national science foundation 
appendix proofs 
proof lemma 
part fix 
consider follows xi xj obvious strictly convex 

minimizer equation contradiction 

part consider 
see lasso solution 
rest directly verified definition lasso omitted 

proof theorem non zero sgn sgn 
equation satisfies sgn elastic net sgn subtracting equation equation gives xt equivalent xt residual vector 
standardized xi xj xt xj equation 
equation implies xi xj 
proof theorem elastic net estimates 
definition equation arg min arg min tx ty substituting identities equation arg min arg min breiman 
heuristics instability stabilization model selection 
ann 
statist 
hlmann 
finding predictive gene groups microarray data 

anal 
az 
simple method finding molecular signatures gene expression data 
technical report 
spanish national cancer center 
available www arxiv org abs bio 
qm 
donoho johnstone kerkyacharian picard 
wavelet shrinkage discussion 
statist 
soc 

efron hastie johnstone tibshirani 
angle regression 
ann 
statist 
zou hastie fan li 
variable selection penalized likelihood oracle properties 
am 
statist 
ass 
frank friedman 
statistical view chemometrics regression tools 
technometrics 
friedman 
regularized discriminant analysis 
am 
statist 
ass 
friedman hastie rosset tibshirani zhu 
discussion boosting papers 
ann 
statist 
fu 
penalized regression bridge versus lasso 
graph 
statist 
golub van loan 
matrix computations 
baltimore johns hopkins university press 
golub slonim tamayo mesirov loh downing 
molecular classification cancer class discovery class prediction gene expression monitoring 
science 
guyon weston vapnik 
gene selection cancer classification support vector machines 
mach 
learn 
hastie tibshirani botstein brown 
supervised harvesting expression trees 
genome biol 
hastie tibshirani eisen brown ross weinstein alizadeh staudt botstein 
gene shaving method identifying distinct sets genes similar expression patterns 
genome biol 
hastie tibshirani friedman 
elements statistical learning data mining inference prediction 
new york springer 

ridge regression 
encyclopedia statistical sciences vol 
pp 

new york wiley 
rosset zhu hastie 
boosting regularized path maximum margin classifier 
mach 
learn 
res 
segal conklin 
regression approach microarray data analysis 
biol 
johnstone yang 
prostate specific antigen diagnosis treatment prostate ii radical treated patients 

tibshirani 
regression shrinkage selection lasso 
statist 
soc 

tibshirani hastie narasimhan chu 
diagnosis multiple cancer types shrunken centroids gene expression 
proc 

acad 
sci 
usa 
tibshirani chu 
significance analysis microarrays applied transcriptional responses radiation 
proc 

acad 
sci 
usa 
west huang ishida marks 
predicting clinical status human breast cancer gene expression profiles 
proc 

acad 
sci 
usa 
zhang 
statistical behavior consistency classification methods convex risk minimization 
ann 
statist 
zhu hastie 
classification gene microarrays penalized logistic regression 
biostatistics 
