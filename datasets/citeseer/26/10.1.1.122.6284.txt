learning bayesian network structure massive datasets sparse candidate algorithm nir friedman institute computer science hebrew university jerusalem israel nir cs huji ac il learning bayesian networks cast optimization problem computational task find structure maximizes statistically motivated score 
large existing learning tools address optimization problem standard heuristic search techniques 
search space extremely large search procedures spend time examining candidates extremely unreasonable 
problem critical deal data sets large number instances number attributes 
introduce algorithm achieves faster learning restricting search space 
iterative algorithm restricts parents variable belong small subset candidates 
search network satisfies constraints 
learned network selecting better candidates iteration 
evaluate algorithm synthetic real life data 
results show significantly faster alternative search procedures loss quality learned structures 
years growing interest learning structure bayesian networks data :10.1.1.112.8434:10.1.1.51.7221:10.1.1.156.9918
somewhat generalizing approaches finding structure 
approach poses learning constraint satisfaction problem 
approach try estimate properties conditional independence attributes data 
usually done statistical hypothesis test test 
build network exhibits observed dependencies independencies 
examples approach include :10.1.1.51.7221
second approach poses learning optimization problem 
start defining statistically motivated score describes fitness possible structure observed data 
scores include bayesian scores nachman center neural computations hebrew university jerusalem israel cs huji ac il dana pe institute computer science hebrew university jerusalem israel cs huji ac il mdl scores :10.1.1.156.9918:10.1.1.156.9918
learner task find structure maximizes score 
general np hard problem need resort heuristic methods 
constraint satisfaction approach efficient sensitive failures independence tests 
common opinion optimization approach better tool learning structure data 
existing learning tools apply standard heuristic search techniques greedy hill climbing simulated annealing find high scoring structures 
see example :10.1.1.112.8434:10.1.1.156.9918
generic search procedures apply knowledge expected structure network learned 
example greedy hill climbing search procedures examine possible local changes step apply leads biggest improvement score 
usual choice local changes edge addition edge deletion edge reversal 
approximately possible changes variables number cost evaluations acute learn massive data sets 
evaluation new candidates requires collecting various statistics data expensive number instances grows 
collect statistics usually need perform pass data 
techniques reduce cost collection activity expect non trivial computation time new set statistics need 
consider domains large number attributes number possible candidates grows quickly 
candidates considered search eliminated advance statistical understanding domain 
example greedy hill climbing possible edge additions removed consideration independent data decide consider parent course heuristic argument marginally independent strong dependence presence variable xor 
domains changes introduce cycles evaluated 
number feasible operations usually quite close reasonable assume pattern dependencies appear 
idea measure dependence mutual information variables guide network construction new 
example chow liu algorithm uses mutual information construct network maximizes likelihood score 
consider networks larger degree authors mutual information greedily select parents 
authors attempt maximize statistically motivated score 
fact easy show situations methods learn erroneous networks 
mutual information simple example statistical cue 
incorporate similar considerations procedure explicitly attempts maximize score 
provide algorithm empirically performs massive data sets 
general idea quite straightforward 
statistical cues data restrict set networks willing consider 
choose restrict possible parents variable 
having potential parents variable consider possible parents 
reasonable domains expect learn families parents 
attempt maximize score respect restrictions 
search techniques case perform faster search space significantly restricted 
show cases find best scoring network satisfying constraints 
cases constraints improve heuristics 
course procedure fail find network misguided choice candidate parents phase lead low scoring network second phase manage maximize score respect constraints 
key idea algorithm network second stage find better candidate parents 
find better network respect new restrictions 
iterate manner convergence 
rest organized follows 
section review necessary background learning bayesian network structure 
section outline structure sparse candidate algorithm show orthogonal issues need resolved select candidates iteration search constraints possible parents 
examine issues sections respectively 
section evaluate performance algorithm synthetic real life datasets 
conclude discussion related directions section background learning structure consider finite set discrete random variables variable may take values finite set denoted val capital letters variable names lowercase letters denote specific values taken vari ables 
sets variables denoted boldface capital letters assignments values variables sets denoted boldface lowercase letters bayesian network annotated directed acyclic graph encodes joint probability distribution formally bayesian network pair component directed acyclic graph vertices correspond random variables graph encodes set conditional independence assumptions variable independent non descendants par ents second component pair represents set parameters quantifies network 
pa pa pa pa pa pa pa contains parameter possible value denotes set parents particular instantiation parents 
graph discussed specify parents graph bayesian network speci fies unique joint probability distribution pa problem learning bayesian network stated follows 
training set instances find network best matches common approach problem introduce scoring function evaluates network respect training data search best network score 
scoring functions commonly learn bayesian networks bayesian scoring metric principle minimal description length mdl 
full description see :10.1.1.156.9918:10.1.1.156.9918
important characteristic mdl score bayesian score certain class factorized priors bde priors decomposability presence full data :10.1.1.156.9918
instances complete assign values variables scoring functions decomposed way score score pa pa pa statistics variables number instances match pa possible instantiation pa decomposition scores crucial learning structure 
local search procedure changes arc move efficiently evaluate gains change 
procedure reuse computations previous stages evaluate changes parents variables changed move 
example procedure greedy hill climbing procedure step performs local change results maximal gain reaches local maximum 
procedure necessarily find global maximum perform practice see :10.1.1.156.9918
example search procedures advance arc changes include beam search stochastic hill climbing simulated annealing 
input data set initial network decomposable score score score parameter output network loop convergence restrict pa select variable set candidate parents 
defines directed graph note usually cyclic 
maximize find network maximizing networks satisfy score pa return outline sparse candidate algorithm implementation search methods involves caching computed counts avoid unnecessary passes data 
cache allows marginalize counts 
cache compute summing values usually faster making new pass data 
dominating factors computational cost learning complete data number passes training data 
particularly true learning large training sets 
sparse candidate algorithm section outline framework sparse candidate algorithm underlying principle algorithm fairly intuitive 
calls variables strong dependency located near network 
strength dependency variables measured mutual information correlation 
fact restricting network graph tree chow liu algorithm exactly 
measures mutual information formally defined pairs variables selects maximal spanning tree required network 
aim similar argument finding networks necessarily trees 
general problem np hard 
seemingly reasonable heuristic select pairs create network edges 
approach take complex interactions account 
example true structure includes substructure form expect observe strong dependency consider parents 
high dependency recognize help predicting take account 
approach basic intuition mutual information refined manner 
measures dependency pairs variables focus attention search 
variable promising candidate parents restrict search networks variables parents gives smaller search space hope find structure quickly 
main drawback procedure choose candidate parents variable committed 
mistake initial stage lead find inferior scoring network 
iterate basic procedure constructed network reconsider candidate parents choose better candidates iteration 
example chosen candidate allowing variable weaker dependency replace 
resulting procedure general form shown 
framework defines class algorithms depending choose candidates restrict step perform search maximize step 
choice methods steps independent 
examine detail sections 
go discuss issues address convergence properties iterations 
clearly level say performance algorithm 
easily ensure monotonic improvement 
require restrict step selected candidates parents include current parents selection pa satisfy find set variables requirement implies winning network legal structure iteration 
search procedure maximize step examines structure return structure scores immediately get score score issue stopping criteria algorithm 
types stopping criteria score criterion terminates score score candidate criterion terminates score monotonically increas ing bounded function score criterion guaranteed 
candidate criterion able continue improve iteration improvement score 
enter non terminating cycle need limit number iterations improvement score 
choosing candidate sets section discuss possible measures choosing candidate set 
choose candidate parents assign measure relevance candidate set choose variables highest measure 
general outline shown 
clear cases xor relations pairwise scoring functions capture dependency variables 
computational efficiency limit type functions 
considering candidate essentially assume spurious independencies data 
precisely parent independent independent subset parents 
simple natural measure dependence mutual information denotes observed frequencies dataset 
mutual information non negative 
equal independent 
higher mutual information stronger dependence researchers tried construct networks add edges variables high mu tual information 
cases mutual information approximation candidate parents simple cases measure fails 
example consider network variables easily select parameters network select parents mutual information select redundant know adds new information choice take account effect example shows general problem pairwise selection iterative algorithm overcomes 
select candidates learning procedure hopefully sets parent reestimate relevance done mutual information 
outline possible approaches approach alternative definition mutual information 
define mutual information distance distribution distribution assumes independent kullback leibler divergence de fined mutual information measures error introduce assume independent 
input data set network score parameter output variable set candidate parents size loop calculate pa choose highest ranking pa pa set return outline restrict step estimate network similar test measure discrepancy estimate empirical estimate define notice empty network parameters estimated data get initial iteration case uses mu tual information select candidates 
iterations discrepancy find variables modeling joint empirical distribution poor 
example expect network parent quite different measure highly relevant approximation weak parents opportunity candidates point 
issues measure requires compute pairs variables 
learning networks large number variables computationally expensive 
easily approximate probabilities simple sampling approach 
computation posterior probabilities evidence approximation prior probabilities hard 
simply sample instances network estimate pair wise interactions 
experiments pa second approach extend mutual information score semantics bayesian networks 
recall bayesian network parents shield non descendants 
suggests measure conditional independence statement independent holds 
holds current parents separate parent hand hold parent descendant testing conditional independence statement holds estimate strongly vio lated 
natural extension mutual information task notion conditional mutual information measures error introduce assuming independent different values define pa shield empty network measure equivalent shield ing remove ancestors candidate set shield descendants 
deficiency measures take account cardinality various variables 
example possible candidate parents values bit information values bits information expect informative hand estimate robustly involves fewer parameters 
considerations lead scores penalize structures parameters searching structure space complex model easier misled empirical distribution 
considerations design score restrict step 
see define measure form start reexamining shielding property 
chain rule mutual information pa pa pa pa pa pa conditional mutual information additional information get predicting compared prediction term depend don need compute compare information different provide equivalent comparative measure shield pa consider score maximize step cautious approximation mutual information penalty number parameters get score measure score score pa simply measures score adding current parents calculating shield score expensive calculating disc disc needs joint statistics pairs require pass data computation cached iterations 
measures require joint statistics pa general pa changes iterations usually requires new pass data set iteration 
cost calculating new statistics reduced limiting attention variables large mutual information note mutual information computed previously collected statistics learning small candidate sets section examine problem finding constrained bayesian network attaining maximal score 
show candidate sets improves efficiency standard heuristic techniques greedy hill climbing 
suggest alternative heuristic divide conquer paradigm exploits sparse structure constrained graph 
formally attempt solve problem maximal restricted bayesian network input set digraph bounded degree decomposable score output network instances maximizes respect expected problem hard combinatorial aspect 
proposition np hard 
follows slight modification np hardness finding optimal unconstrained bayesian network 
standard heuristics np hard standard heuristics computationally efficient give better approximation compared unconstrained problem 
due fact search space complexity iteration number counts needed 
search space possible bayesian networks extremely large 
searching smaller space hope better chance finding high scoring network 
search space size remains exponential tiny comparison space bayesian networks domain 
see note restrict search bayesian networks possible parent sets parents variable 
hand possible parent sets variable 
course acyclicity constraints disallow networks change order magnitude size sets 
examining time complexity iteration heuristic searches points favor 
greedy hill climbing score initial changes calculated iteration requires new calculations 
initial calcu lations iteration requires calculation 
large fraction learning time involves collecting sufficient statistics data 
restricting candidate sets saves time 
reasonably small compute statistics pass input 
statistics need evaluating subsets parents computed marginalization counts 
dramatically reduce number statistics collected data 
divide conquer heuristics section describe algorithms utilize combinatorial properties candidate graph order efficiently find maximal scoring network constraints 
simplify discussion details bayesian network learning problem focus underlying combinatorial problem 
problem specified follows input digraph set weights output acyclic subgraph maximizes pa effective paradigms designing algorithms divide conquer 
particular problem global constraint need satisfy acyclicity 
selected variable parents attain maximal weight 
want decompose problem components efficiently combine maximal solutions 
standard graph decomposition methods decompose decomposition find acyclic solutions component combine global solution 
strongly connected components scc simplest decomposition form disallows cycles components strongly connected components 
subset vertices strongly connected contains directed path directed path set maximal strongly connected superset clear maximal strongly connected components disjoint cycle involves vertices union strongly connected component 
partition vertices maximal strongly connected components 
cycle contained single component 
ensure acyclicity locally component get acyclic solution variables 
means search maximum component independently 
formalize idea definitions 
partition define subgraphs pa proposition strongly connected components graph maximizes graph acyclic maximizes decomposing strongly connected components takes linear time see apply decomposition search maxima component separately 
graph contains large acyclic 
connected components face hard combinatorial problem finding graphs remainder section focus decomposition components 
separator decomposition decompose strongly connected graphs consider cycles components 
goal find small bottlenecks cycles go 
consider possible ways breaking cycles bottlenecks 
definition separator set vertices 
components edges 

vertex search maximal choice parents component 
disjoint partition vertices sets second property separator ensures partition exists 
property holds separates moralized graph ponents 
appear clique com scc decomposition decomposition allow maximize independently 
suppose find acyclic graphs maximize respectively 
combined graph acyclic maximize unfortunately cyclic 
property separators ensures source potential conflicts separator involve vertices path addition path combined graph cyclic 
conversely easy verify cycle involve vertices suggests way ensuring combined graph acyclic 
force order vertices require respect order separator algorithm possible order find maxi graphs respect return outline separator efficiently solve disallow cycles 
formally partial order say graph directed path respects proposition separator complete order acyclic graphs respect acyclic 
small separator suggests simple algorithm described 
approach considers pairs independent sub problems 
cost find ing solution sub problems smaller problem relatively small procedure efficient 
proposition notation separator algorithm maximizes graphs respect maximizes graphs respect proposition implies algorithm returns optimal solution 
cluster tree decomposition maximizes section cluster trees representations candidate graphs implying recursive separator decomposition clusters 
idea similar standard clique tree algorithms bayesian network inference 
representation discuss class graphs polynomial time 
definition cluster tree pair tree family clusters subsets vertex exists path called running intersection property 
introduce notation edge separator breaking subtrees define set vertices assigned parents define contrast define set vertices appearing necessarily parents 
small efficiently algorithm 
devise dynamic programming algorithm computing optimal graph cluster tree separators 
root cluster tree arbitrary inducing order tree vertices 
cluster root subtree spanning away tree separator separating rest 
sub vertices neighbors define cluster total order weight maximal partial solution respects acyclic respecting crux algorithm finding weights done recursive manner previously computed maxima 
proposition cluster order sub vertices equal acyclic respecting ranges orders consistent restriction order proposition rapid evaluation tables phase working way leaves inwards traversal computed weight ordering separators adjacent root cluster second phase traverses root outwards order back trace choices phase leading maximum total weight examining complexity algorithm see cluster visited twice expensive visit requiring operations size candidate sets 
get result theorem size largest cluster cluster tree finding maximizes done summary algorithm linear size cluster tree worse exponential size largest cluster tree 
discussion assumed fixed cluster tree 
practice need select cluster tree 
known hard problem scope 
note small cluster tree polynomial time 
method iter time score kl stats greedy disc disc score score table summary results synthetic data alarm domain 
results report quality network measured terms score bde score divided number instances kl divergence generating distribution 
columns measure performance terms execution time seconds number statistics collected data 
methods reported disc discrepancy measure shielding measure score score measure 
cluster tree heuristics algorithm previous section linear number clusters worse exponential size largest cluster 
situations expect hopelessly intractable 
algorithm provides intuition decompose heuristic search problem 
key idea computing cluster tree clusters large mixture exact algorithm small clusters heuristic searches greedy hill climbing larger clusters 
due space constraints briefly outline main ideas approach 
sufficiently small efficiently store tables exact cluster tree algorithm 
clusters large maximization proposition 
perform heuristic search greedy hill climbing space parents vertices find partial network consistent ordering induced current assignment 
proceeding manner approximate exact algorithm 
approximation examines series small search spaces presumably easier deal original search space 
approach easily extended deal cluster trees separators small 
experimental evaluation section illustrate effectiveness sparse candidate algorithm 
examine synthetic example real life dataset 
current experiments designed evaluate effectiveness general scheme show utility various measures selecting candidates restrict phase 
experiments described greedy hill climbing maximize phase 
currently working implementation heuristic algorithms described section hope report results 
statistics strongly connected component sizes reported 
basic heuristic search procedure greedy hill considers local moves form edge addition edge deletion edge reversal 
iteration procedure examines change score possible move applies leads biggest improvement 
iterations repeated convergence 
order escape local maxima procedure augmented simple version tabu search 
keeps list candidates seen applying best local change applies best local change results structure list 
note tabu list best allowed change reduce score current candidate 
terminate procedure fixed number changes failed result improvement best score seen far 
termination procedure returns best scoring structure encountered 
reported experiments greedy hillclimbing procedure maximize phase sparse candidate algorithm search procedure 
case local changes considered allowed current choice candidates 
case procedure considers possible local changes 
case serves point compare results 
expanded version compare search procedures 
compare search procedures need measure performance task hand computational cost 
evaluation quality score assigned network algorithm 
addition synthetic data measure true error respect generating distribution 
allows assess significance differences scores search 
evaluating computational cost complicated 
simplest approach measure running time 
report running times unloaded pentium ii mhz machines running linux 
running times depend various coding issues implementation 
attempted avoid introducing bias code procedure basic library evaluating score candidates computing caching sufficient statistics 
actual search carried code greedy hill climbing procedure 
additional indication computational cost measured number sufficient statistics computed data 
massive datasets computations significant portion running time 
minimize number passes data cache allows previously computed statistics greedy hc disc disc score score time greedy hc disc disc score score collected statistics greedy hc disc disc score score time greedy hc disc disc score score collected statistics disc disc score score time disc disc score score collected statistics text text clc graphs showing performance different algorithms text biological domains 
graphs top row show plots score axis vs running time axis 
graphs bottom row show run measured terms score axis vs number statistics computed axis 
reported methods vary terms candidate selection measure disc discrepancy measure shielding measure score score measure size candidate set 
points curve sparse candidate algorithm result iteration 
marginalize statistics get statistics subsets 
report number actual statistics computed data 
experiments bde score uniform prior equivalent sample size :10.1.1.156.9918
choice fairly prior code initial bias correct network 
strength equivalent sample size set prior experiments tuned 
set experiments sample instances alarm network 
network studies structure learning various papers treated common benchmark field 
network contains variables values values values 
note consider data set particularly massive allow estimate behavior search procedure 
plan synthetic data larger networks 
results small data set reported table 
table measure score networks error respect generating distributions 
results toy domain show algorithm particular selection heuristic finds networks comparable score greedy hill climbing 
timing results small scale experiments significant see sparse candidate algorithm usually requires fewer statistics records 
note iteration gorithm finds reasonably high scoring networks 
subsequent iterations improve score 
reestimation candidate sets score lead important improvements 
test learning algorithms challenging domains examined data text 
data set contains messages newsgroups approximately 
represent message vector containing attribute newsgroup attributes word vocabulary 
constructed data sets different numbers attributes focusing subsets vocabulary 
removing common words sorting words frequency data set 
data sets included group designator text set text set common words 
trained messages randomly selected total data set 
results experiments reported 
see case attributes learn networks reasonably close greedy hill half running time half number sufficient statistics 
attributes speedup larger 
expect consider data sets larger number attributes speedup ratio grow 
test devised synthetic dataset originates real biological data 
gene expression data 
data describes expression level selection method candidate sets sizes cell cycle regulated genes experiments 
learned network dataset sampled instances learned network 
synthetic dataset 
see details 
results reported 
experiments greedy hill climbing search stopped lack memory store collected statistics 
stage far range scores shown 
try assess time take reach score networks methods times slower conservative extrapolation 
note discrepancy measure slower learning curve score measure 
note iteration initial tion adds modest number new statistics calculate measure pairs variables initially significant mutual information 
statistics collected itera contributions fold 
propose simple heuristic improving search efficiency 
restricting search examine small number candidate parents variable find high scoring networks efficiently 
furthermore showed improve choice candidates account network learned getting higher scoring networks demonstrated effects experiments 
results show procedure lead dramatic reduction learning time 
comes small loss quality worse lead higher scoring networks 
second showed restricting variable small group candidate parents get theoretical guarantees complexity learning algorithm 
result theoretical interest best knowledge non trivial case find polynomial time learning algorithm networks degree greater 
theoretical argument practical ramifications 
showed exact polynomial algorithm expensive guide finding approximate solutions 
process implementing new heuristic strategy evaluating 
addition experimental results describe algorithm applied ongoing works 
sparse candidate method combined structural em procedure learning structure incomplete data 
setup cost finding statistics higher counting number instances perform inference instances 
consequence reduction number requested statistics shown results leads significant saving run time 
similar cost issues occur variant algorithm learning probabilistic models relational databases 
procedure crucial component ongoing analysis real life gene expression data contains thousands attributes 
directions research 
ultimate aim type algorithm learning domains thousands attributes 
domains cost restrict step algorithm prohibitive quadratic number variables 
currently examining heuristic methods finding candidates 
learn network candidates help focus variables examined restrict step 
direction interest combination methods ideas efficient learning large datasets 
acknowledgments supported generosity michael trust 
beinlich suermondt chavez cooper 
alarm monitoring system 
proc 
nd euro 
conf 
ai med 

bodlaender 
linear time algorithm finding tree decompositions small treewidth 
siam computing 
bouckaert 
bayesian belief networks construction inference 
phd thesis utrecht university netherlands 
boyen friedman koller 
discovering structure complex dynamic systems 
uai 

chickering 
transformational characterization equivalent bayesian network structures 
uai pp 


chickering 
learning bayesian networks np complete 
ai stat 
chickering 
learning equivalence classes bayesian network structures 
uai pp 


chow liu 
approximating discrete probability distributions dependence trees 
ieee trans 
info 
theory 
cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
cormen leiserson rivest 
algorithms 

cover thomas 
elements information theory 


fraud debt detection bayesian network learning system 
uai pp 


friedman nachman pe bayesian networks analyze genome expression data 
tr cs hebrew university 
progress 
see www cs huji ac il labs 
getoor friedman koller pfeffer 
learning probabilistic relational models 
ijcai 

heckerman 
tutorial learning bayesian networks 
jordan editor learning graphical models 
heckerman geiger chickering :10.1.1.156.9918
learning bayesian networks combination knowledge statistical data 
machine learning 
jensen 
bayesian networks 

probabilistic analysis rocchio algorithm tfidf text categorization 
icml 
lam bacchus 
learning bayesian belief networks approach mdl principle 
comp 
int 
moore lee 
cached sufficient statistics efficient machine learning large datasets 
ai 
res 
pearl verma 
theory inferred causation 
kr pp 


sahami 
learning limited dependence bayesian classifiers 
pp 

spellman sherlock zhang iyer anders eisen brown botstein 
comprehensive identification cell cycle regulated genes yeast cerevisiae microarray hybridization 
mol 
biol 
cell 
spirtes glymour scheines 
causation prediction search 
