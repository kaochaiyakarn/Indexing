partitioning clustering web document categorization daniel boley maria gini robert gross hong sam han kyle hastings george karypis vipin kumar mobasher jerome moore department computer science engineering university minnesota minneapolis mn clustering techniques software agents order retrieve lter categorize documents available world wide web 
clustering useful extracting salient features related web documents automatically formulate queries search similar documents web 
traditional clustering algorithms priori knowledge document structures de ne distance similarity documents probabilistic techniques bayesian classi cation 
traditional algorithms dimensionality feature space high relative size document space 
introduce new clustering algorithms ectively cluster documents presence high dimensional feature space 
clustering techniques generalizations graph partitioning require pre speci ed ad hoc distance functions capable automatically discovering document similarities associations 
conduct experiments real web data various feature selection heuristics compare clustering schemes standard distance techniques agglomeration clustering bayesian classi cation methods autoclass 
keywords clustering categorization world wide web documents graph partitioning association rules principal component analysis 
supported part army research da daag army highperformance computing research center cooperative agreement number daah contract number daah con tent necessarily re ect position policy government cial endorsement inferred 
additional support provided ibm partnership award ibm sur equipment 
access computing facilities provided minnesota supercomputer institute 
world wide web vast resource information services continues grow rapidly 
powerful search engines developed aid locating unfamiliar documents category contents subject 
unfortunately queries return inconsistent results document referrals meet search criteria interest user 
may currently feasible extract full meaning html document intelligent software agents developed extract features words structure html doc ument employ classify categorize documents 
clustering ers advantage priori knowledge categories needed categorization process unsupervised 
results clustering automatically formulate queries search similar documents web organize bookmark les construct user pro le 
new clustering algorithms graph partitioning compare performance traditional clustering algorithms information retrieval 
traditional clustering algorithms de ne distance similarity documents prob techniques classi cation 
algorithms break size document space dimensionality corresponding feature space increases 
high dimensionality characteristic information retrieval applications lter categorize documents world wide web 
contrast partitioning algorithms perform presence high dimensional space 
section describe clustering algorithms section results iments di erent methods select features documents compare results di erent clustering algorithms 
show partitioning clustering methods perform better tradi tional distance clustering 
section compare similar systems ideas research 
clustering methods existing methods document clustering probabilistic methods dis tance similarity measures see 
distance methods means analysis hierarchical clustering nearest neighbor clustering selected set words appearing di erent doc uments features 
document represented feature vector viewed point ina multi dimensional space 
number problems clustering multi dimensional space traditional distance probability methods 
trivial de ne distance measure space 
feature vectors scaled avoid skewing result di erent document lengths possibly common word documents 
techniques tfidf proposed precisely deal problems experiments tfidf scaling help 
second number di erent words documents large 
distance schemes generally require calculation mean document clusters chosen initially random 
high dimensional space cluster means randomly chosen clusters poor job separating documents 
similarly probabilistic methods classi cation autoclass perform size feature space larger size sample set may depend independence underlying features 
web documents su er high dimensionality high correlation feature values 
hierarchical agglomeration clustering hac computationally expensive autoclass performed poorly examples 
proposed clustering algorithms described designed ciently handle high dimen sional spaces large data sets shown experimental results describe 
association rule hypergraph partitioning association rule hypergraph partitioning clustering method associ ation rule discovery technique data mining 
technique discover items transactional database example nd sales relationships items sold supermarket customer transactions 
database perspective transactions viewed relational table item represents attribute domain attribute binary domain indicating item bought particular transaction non negative integer indicating frequency purchase transaction 
depicts portion typical supermarket transaction database 
portion typical supermarket transaction database association rule discovery methods rst nd groups items occurring frequently transactions 
groups items referred frequent item sets 
method discovered frequent item sets form hypergraph vertices items hyperedge represents frequent item set 
hypergraph partitioning algorithm nd item clusters 
similarity items captured implicitly frequent item sets 
document retrieval domain possible view set documents transactional form 
case document corresponds item possible feature corresponds transaction 
entries table domain document attribute represents frequency occurrence speci ed feature word document 
frequent item sets association rule discovery algorithm corresponds set documents su ciently large number features words common 
frequent item sets mapped hyperedges hypergraph 
atypical document feature dataset represented transactional database depicted 
view typical document feature set consists set vertices set hyperedges 
extension graph sense hyperedge connect vertices 
model set vertices corresponds set documents clustered hyperedge corresponds set related documents 
akey problem modeling data items hypergraph determining related items grouped hyperedges determining weights hyperedge 
case hyperedges represent frequent item sets association rule discovery algorithm 
association rules capture relationships items transaction 
set transactions transaction subset item set subset de ne support count respect number transactions contain association rule expression form support rule de ned jt con dence de ned 
task discovering association rule nd rules greater minimum support threshold greater minimum con dence threshold 
association rule discovery composed steps 
rst step discover frequent item sets candidate sets support greater minimum support threshold speci ed 
second step generate association rules frequent item sets 
frequent item sets computed association rule algorithm apriori excellent candidates nd related items 
note algorithms nd frequent item sets support greater speci ed threshold 
value threshold may determined domain speci manner 
frequent relationships items size greater equal 
note distance relationships capture relationships pairs data points frequent items sets capture relationship larger sets data points 
added modeling power nicely captured hypergraph model 
hypergraph representation cluster relatively large groups related items partitioning highly connected partitions 
way hypergraph parti algorithm partitions hypergraph parts weight cut partitioning minimized 
note minimizing hyperedge cut essentially minimize relations violated splitting items groups 
parts bisected recursively partition highly connected 
task hmetis multi level hypergraph partitioning algorithm partition large hypergraphs size nodes minutes personal computers 
hypergraph partitioned parts eliminate bad clusters cluster tness criterion 
set vertices representing hyperedge set vertices representing partition 
tness function measures goodness partition de ned follow weight fitness je cj tness function measures ratio weights edges partition weights edges involving vertex partition 
examined lter vertices highly connected rest vertices partition 
connectivity de ned follow connectivity connectivity measures percentage edges vertex associated 
high connectivity value suggests vertex edges connecting proportion vertices partition 
vertices connectivity measure greater give threshold value considered belong partition remaining vertices dropped partition 
ltering non relevant documents achieved support criteria association rule discovery components algorithm 
depending support threshold 
documents meet support documents share large subsets words documents pruned 
feature particularly useful clustering large document sets returned standard search engines keyword queries 
principal direction divisive partitioning pddp algorithm principal direction algorithm document represented feature vector word frequencies scaled unit length 
algorithm divisive method sense begins documents single large cluster proceeds splitting subclusters recursive fashion 
stage process method selects unsplit cluster split splits cluster subclusters 
part scatter value measuring average distance documents cluster mean just cluster size desired keep resulting clusters approximately size 
part construct linear discriminant function principal direction direction maximal variance 
speci cally compute mean documents cluster principal direction respect mean 
de nes hyperplane normal principal direction passing mean 
hyperplane split cluster parts children clusters cluster 
entire cycle repeated times desired resulting binary tree hierarchy clusters root entire document set interior node split children 
leaf nodes constitute partitioning entire document set 
de nition hyperplane principal component analysis similar hotelling karhunen loeve transformation 
compute principal direction leading eigenvector sample covariance matrix 
expensive part fast lanczos singular value solver 
advantage high degree sparsity term frequency matrix lanczos solver cient cost proportional number term frequency matrix 
discussed detail 
method di ers latent semantic indexing lsi ways 
lsi originally formulated di erent purpose method reduce dimensionality search space purpose handling queries retrieving documents set search terms 
secondly operates unscaled vectors scale document vectors unit length 
thirdly lsi singular value decomposition matrix document vectors computed shift documents mean origin order compute covariance matrix 
fourthly lsi method compute singular vectors entire matrix document vectors order singular vectors processing 
method compute single leading singular vector vector considerably easier obtain 
course repeat computation cluster course algorithm clusters smaller initial root cluster computations faster 
experiments norm scaling represented feature vector word counts scaled unit length usual euclidean norm 
leaves sparsity pattern untouched 
alternate scaling tfidf scaling scaling lls zero entries drastically increasing cost algorithm factor 
spite increased costs tfidf scaling lead noticeable improvement pddp results experiments 
hierarchical agglomeration clustering classical algorithm implemented bottom hierarchical agglomeration clustering hac method distance function 
start trivial clusters containing document 
cycle loop closest clusters merged cluster 
loop cycle reduces number clusters repeated desired number clusters reached 
experiments chose distance function cosine measure essentially cosine angle documents space cluster represented mean 
cluster means scaled corresponding cluster sizes discourage large clusters 
autoclass algorithm 
autoclass probabilistic mixture modeling 
set data autoclass nds maximum parameter values speci probability distribution functions clusters 
vc vc kg class mixture probability autoclass calculates probability data point xi belongs class cj bayes rule xi cj jp xij vj pk lp xij vl advantages autoclass theoretical foundation bayesian statistics 
clustering results provide full description cluster terms probability distribution attributes 
works discrete continuous attributes 
results clustering fuzzy gives probabilities data point belonging di erent clusters 
analysts determine cluster membership data point probabilities 
weaknesses autoclass underlying probability model assumes independence attributes 
domains assumption restrictive 
problem basic model provide satisfactory distribution function ordered discrete attributes 
furthermore attributes respect clustering hidden biases may dominate clustering process 
experimental results experimental setup experiments selected web pages broad categories action aa business capital bc electronic commerce ec employee rights er intellectual property ip industrial partnership ipt information systems materials processing mp manufacturing systems integration msi personnel management pm 
pages obtained search standard search engine 
pages downloaded labeled archived 
labeling facilitates entropy calculation subsequent page directed archive 
ensures stable data sample pages fairly dynamic content 
results obtained similar experiments smaller set documents previously reported 
documents obtained part network excellence manufacturing website line web org index html originally experiments described 
experiments describe grew initial set experiments validate larger dataset results obtained original experiments 
conducted series experiments larger dataset documents obtained yahoo online news service support scalability analysis described section 
word selection dataset set criteria size comments words select non words stemmed 
quantile ltering quantile ltering selects frequently occurring words accumulated frequencies exceed threshold including words partition contributes word exceeds threshold 
top words select frequently occurring words include words partition contributes top plus emphasized words th word 
select top words augmented word emphasized html document words appearing title big strong emphasize tags 
frequent item sets select words document word lists appear priori word clusters 
object measure identify important groups words 
words prune words selected exclude text frequency occurring 
top text prune words selected exclude frequency occurring 
top text frequency top text frequency top text frequency table setup experiments 
word lists documents ltered list stemmed porter su stripping algorithm implemented 
derived experiments clustered documents algorithms described earlier analyzed results 
objective reduce dimensionality clustering problem retaining important features documents 
experiments conducted distinguished selection rules shown table 
evaluation clustering results validating clustering algorithms comparing performance di erent algorithms complex di cult nd objective measure quality clusters 
decided entropy measure goodness clusters 
cluster contains documents class value cluster cluster contains documents di erent classes entropy cluster higher 
total entropy calculated weighted sum entropies clusters 
compare results various experiments comparing entropy algorithms feature selection methods fig 

note hypergraph partitioning method cluster documents entropy computed documents clustered 
experiments suggest clustering methods partitioning best type information retrieval applications 
require calculation mean randomly chosen clusters issue having cluster means close space apply 
linearly scalable respect cardinalities document feature spaces contrast hac autoclass quadratic 
quality clusters ected dimensionality data sets 
general methods similar behavior experiments ltering word frequencies frequent item set experiment discussed 
pddp methods performed better traditional methods hac norm scaling regardless feature selection criteria 
algorithms autoclass hac tfidf scaling computationally prohibitive dimensionality 
example feature selection criteria dataset size pddp took minutes hac took hour minutes autoclass took minutes 
tried pddp algorithm larger dataset documents experiments show algorithm scales linearly number non zero entries term frequency matrix 
document uses small fraction entire dictionary words term frequency matrix sparse 
larger dataset entries nonzero 
algorithm able take advantage sparsity yielding scalable performance illustrated 
entropy cluster entropies series experiment 
autoclass hac tfidf pddp tfidf pddp norm hac norm entropy comparison di erent algorithms clusters 
results shown slightly di erent numbers clusters experiment precisely clusters forj forj forj forj 
note lower entropy indicates better cohesiveness clusters 
aside performance quality clusters experiments point notable 
expected general clustering algorithms yield better quality clusters full set feature experiment 
course discussion shows large datasets computational costs may prohibitive 
important select smaller set representative features improve performance clustering algorithms losing quality 
experi ments various feature selection methods represented show restricting feature set appearing frequent item sets discovered part association rule algorithm succeeded identifying small set features relevant clustering task 
fact algorithms experiment produced results better obtained full set 
noted drawn discussion con rmed earlier experiment totally independent set documents 
particular experiment better judge quality clustering looking distribution class labels clusters 
shows class distribution experiment seconds sgi challenge time obtain clusters pddp algorithm series docs words words words series docs words words words words words number nonzero entries term frequency matrix time obtain clusters various data sets pddp algorithm 
time seconds sgi challenge vertical axis plotted number term frequency matrix horizontal axis 
pddp norm scaling aa bc ec er ip ipt mp msi pm 
















entropy pddp tfidf scaling aa bc ec er ip ipt mp msi pm 

















entropy distribution documents clusters pddp algorithm tfidf scaling clusters 
column shows documents label appear clusters 
pddp algorithm tfidf scaling 
full results including data sets available web www cs umn edu wap html 
similarly shows class distribution experiment hac algorithm tfidf scaling shows results shows results autoclass 
hac norm scaling aa bc ec er ip ipt mp msi pm 
















entropy hac tfidf scaling aa bc ec er ip ipt mp msi pm 







entropy distribution documents clusters hierarchical agglomeration clustering algo rithm tfidf scaling clusters 
aa bc ec er ip ipt mp msi pm 

















entropy distribution documents clusters algorithm clusters 
cluster contains documents clustered yielded clusters 
autoclass aa bc ec er ip ipt mp msi pm 





entropy distribution documents clusters autoclass algorithm clusters 
documents clustered 
related anumber web agents various information retrieval techniques open hyper text web documents automatically retrieve lter categorize documents :10.1.1.44.3114
example information embedded link structures document content classify group documents terms contain hyperlink structures 
system requires information maintained routers 
bo bookmark organizer combines hierarchical agglomerative clustering techniques user inter action organize collection web documents listed personal bookmark le 
pattern recognition methods word clustering hartigan means partitional clustering algorithm discover salient html document features words nding similar html documents web 
clustering algorithm scale large numbers documents 
broder calculates sketch document clusters similar documents sketches exceed threshold resemblance 
document url similar documents easily identi ed index www needs maintained 
maarek agglomerative clustering method form clusters documents listed personal bookmark le 
individual documents characterized pro le vectors consisting pairs lexically ne words document similarity function indices share 
method may scale large document searches 
syskill webert system represents html page boolean feature vector uses naive bayesian classi cation nd web pages similar single user pro le 
balabanovic presents system uses single de ned pro le nd similar web documents user 
candidate web pages located best rst search comparing word vectors user pro le vector returning highest scoring pages 
tfidf scheme calculate word weights normalized document length 
system needs keep large dictionary limited user 
awell known widely technique dimensionality reduction principal component analysis pca 
consider data items variables 
pca computes covariance matrix size calculate leading eigenvectors covariance matrix 
leading eigenvectors matrix principal features data 
original data mapped new principal directions 
projected data lower dimensions clustered traditional clustering algorithms means hierarchical clustering autoclass 
pca provides guidelines determine right number dimension data proportion variance explained characteristic roots covariance matrix 
noted di erent methods provide widely di erent guidelines data di cult nd right number dimension 
choice small lose important features data 
hand choice large capture important features dimensionality large traditional clustering algorithms ectively 
indexing lsi dimensionality reduction technique extensively information retrieval domain similar nature pca 
nding singular value decomposition covariance matrix nds singular value decomposition original data 
pca lsi preprocessing methods produce lower dimensional representation dataset subsequent processing algorithm 
context query systems lsi successful reducing noise data leading higher precision results user queries 
may considered possible preprocessing modules context unsupervised clustering preliminary experiments direction carried lsi followed means pddp yielding respective entropies 
obtain results lsi extract dimension approximation term frequency matrix basis subsequent means pddp method 
main di culty lsi pca methods necessity compute leading singular values vectors term frequency matrix desired dimension 
naive dense matrix solver takes operations compute prohibitively expensive 
method takes advantage sparsity speed substantially 
example lanczos method great success pddp algorithm 
considerably di cult compute leading singular values vectors lsi just leading singular vector pddp 
time available resulting low dimension approximation typically dense 
substantially increases processing cost subsequent clustering method potentially occupying space original data depending choice kohonen self organizing feature map neural network scheme projects high dimensional input data feature map smaller dimension proximity relationships input data preserved 
data sets large dimensionality discussed convergence slow depending initialization 
new methods clustering association rule hypergraph partitioning principal direction divisive partitioning particularly suitable type information retrieval applications discussed 
methods depend distance measures perform high dimensional spaces 
experiments suggest methods perform better traditional clustering algorithms regardless techniques feature selection 
particular perform features document clustering 
addition experiments suggest features selected restricted frequent item sets derived apriori algorithm traditional methods tend perform better 
evident hypergraph partitioning method may perform better features selected include words emphasized document authors html tags 
research plans include developing methods incremental clustering classi cation doc uments discovering initial set clusters 
furthermore clustering techniques proposed word clustering 
word clusters classify new documents search related documents web 
mark ackerman learning probabilistic user pro les 
ai magazine 
agrawal mannila srikant toivonen verkamo 
fast discovery association rules 
fayyad piatetsky shapiro smith uthurusamy editors advances knowledge discovery data mining pages 
aaai mit press 
agrawal mannila srikant toivonen verkamo 
fast discovery association rules 
fayyad piatetsky shapiro smith uthurusamy editors advances knowledge discovery data mining pages 
aaai mit press 
marko balabanovic yoav shoham yun 
adaptive agent automated web browsing 
journal visual communication image representation 
www stanford edu cgi bin wp get wp 
berge 
graphs hypergraphs 
american elsevier 
berry dumais gavin brien 
linear algebra intelligent information retrieval 
siam review 
boley 
principal direction divisive partitioning 
technical report tr department computer science university minnesota minneapolis 
boley 
hierarchical taxonomies divisive partitioning 
technical report tr depart ment computer science university minnesota minneapolis 
andrei broder steven glassman mark manasse 
syntactic clustering web 
proc 
th international world wide web conference april 
proceedings www conf org get html 
chang hsu 
customizable multi engine search tool clustering 
proc 
th interna tional world wide web conference 
cheeseman stutz 
baysian classi cation autoclass theory results 
fayyad piatetsky shapiro smith uthurusamy editors advances knowledge discovery data mining pages 
aaai mit press 
chen sycara 
webmate personal agent browsing searching 
proc 
nd international conference agents 
richard duda peter hart 
pattern classi cation scene analysis 
john wiley sons 
frakes 
stemming algorithms 
frakes baeza yates editors information retrieval data structures algorithms pages 
prentice hall 
frakes baeza yates 
information retrieval data structures algorithms 
prentice hall englewood cli nj 
golub van loan 
matrix computations 
johns hopkins univ press rd edition 
han karypis kumar mobasher 
clustering association rule hypergraphs 
workshop research issues data mining knowledge discovery pages tucson arizona 
han karypis kumar mobasher 
hypergraph clustering high dimensional data sets summary results 
bulletin technical committee 
jackson 
user guide principal components 
john wiley sons 
jain dubes 
algorithms clustering data 
prentice hall 
karypis aggarwal kumar shekhar 
multilevel hypergraph partitioning application vlsi domain 
proceedings acm ieee design automation conference 
kohonen 
self organization associated memory 
springer verlag 
lu fu 
sentence sentence clustering procedure pattern analysis 
ieee transac tions systems man cybernetics 
maarek israel ben shaul 
automatically bookmarks contents 
proc 
th international world wide web conference may 
www conf inria fr ch html papers overview html 
moore han boley gini gross hastings karypis kumar mobasher 
web page categorization feature selection association rule principal component clustering 
th workshop information technologies systems dec 
porter 
algorithm su stripping 
program 
ross quinlan 
programs machine learning 
morgan kaufmann san mateo ca 
gerard salton michael mcgill 
modern information retrieval 
mcgraw hill 
titterington smith makov 
statistical analysis finite mixture distributions 
john wiley sons 
ron weiss velez mark sheldon peter duda david gi ord 
hierarchical network search engine ex content link hypertext clustering 
seventh acm conference hypertext march 
paris lcs mit edu 
marilyn william punch 
finding salient features personal web page categories 
proc 
th international world wide web conference april 
proceedings www conf org get html 

