software testability new veri cation je rey voas keith miller reliable software technologies department computer science suite health sciences building sunset hills road state university reston va spring eld il gmu edu miller eagle edu fax software veri cation encompasses wide range techniques activities geared demonstrating software reliable 
veri cation techniques testing provide way assess likelihood software fail 
introduces di erent cation shows incorrect program fail 
veri cation applies fault injection methods predict actual faults hide 
veri cation combined software testing assess con dence code hiding faults 
code hides faults di cult test 
order minimize problem hidden faults seek methods identifying isolating source code hide faults 
introduce notion information loss characteristic measured early phases design suggest planned software harbor faults di cult uncover testing 
keywords software testing software testability fault failure reliability probability failure software design testability 
supported part national research council nasa langley resident research nasa nag 
software veri cation defense disasters caused faulty software develop ment 
lives depend software software quality cation demand increased attention 
computer software begins replace human decision makers fundamental concern machine able perform tasks level precision skilled person 
catastrophe may caused automated system reliable manual system 
means assessing critical automated sys tems acceptably safe reliable 
concentrate veri cation technique assessing reliability 
ieee standard glossary software engineering terminology de nes software ver cation process evaluating system component determine products development phase satisfy conditions imposed start phase 
restated software veri cation process assesses degree acceptability software acceptability judged speci cation 
software veri cation broadly divided classes dynamic software testing formal veri cation typically involves level static theorem proving 
dynamic software testing process executing software repeatedly con dence gained software correct defects commonly referred probable correctness software high level acceptability 
testing alternatively subdivided main classes white box box 
white box testing bases selection test cases code black box testing bases selection description legal input domain 
static theorem proving mathematical process showing function computed program matches function speci ed 
program executions occur process result binary value function computed program matches speci cation 
problems arise rigorous process questions concerning program termination correctness rigorous process prove proof 
furthermore process completing proof di cult writing program 
describe di erent type veri cation complement dynamic testing static theorem proving 
new type veri cation call software testability focuses probability fault program revealed testing 
de ne software testability probability piece software fail execution testing particular assumed input distribution software includes fault 
veri cation standard ieee de nition way assessing input output pairs correct 
testability examines di erent behavioral characteristic likelihood code fail code incorrect 
computer science researchers developing software reliability models answer question probability code fail 
testability asks di erent question probably code fail faulty 
musa labels similar measurement exposure ratio reliability formulae 
empirical methods estimating testability distinct musa techniques 
research emphasized random testing attractive statistical properties 
full generality software testability de ned di erent types testing data ow testing mutation testing 
ieee standard glossary software engineering terminology de nes testability degree system component facilitates establishment test criteria performance tests determine criteria met degree requirement stated terms permit establishment test criteria performance tests determine criteria met 
note order determine degree test criteria testability simply measure hard satisfy particular testing goal 
examples testing goals include coverage complete fault 
testability requires input distribution commonly called user pro le requirement unique testability statistical prediction semantic behavior software operation include assumption distributions inputs operation 
reader note de nition software testability di ers earlier de nitions testability ieee de nition 
past software testability informally discuss ease input selection criteria satis ed testing 
example tester desired full branch coverage testing di cult select inputs cover branches software classi ed having poor testability 
de nition di ers signi cantly just trying nd sets inputs satisfy coverage goals quantifying probability particular type testing cause failure 
focus de nition testability semantics software behave contains fault 
di erent asking facilitates coverage correct 
software testability analysis related distinct software testing formal veri cation 
software testing testability analysis requires empirical create estimates 
testing testability analysis require oracle 
testing testability complementary testing reveal faults testability testability suggest locations faults hide testing testing 
section explores testability analysis conjunction testing formal methods give clearer view developing software 
pieces puzzle software testability software testing formal veri cation pieces puzzle puzzle software high true reliability 
lucky piece software undergone enormous amounts successful testing undergone formal veri cation high testability pieces suggest puzzle solved high reliability 
software testing formal veri cation software testability er information quality piece software 
technique supplies unique perspective di erent evidence analyst take account 
clues analyst making guesses having clues better having 
hypothetical example analyses consider system modules 
modules tested random tests current versions modules pass tests 
addition system passed random tests 
modules judged intricate critical subjected formal veri cation various points development 
testability analysis reveals modules highly insensitive testing testing nd faults modules faults exist 
modules formally veri ed 
point veri cation resources concentrate modules low testability formally veri ed vulnerable hidden faults 
example consider system built entirely formally veri ed modules 
development approach inspired cleanroom analysts wait system integration random system testing 
testing faults discovered code repaired 
regression testing new random tests reveal failures testability analysis identi es places code testing highly reveal faults 
pieces code subjected formal analysis non random tests devised exercise sections extensively 
system true xed reliability generally unknown try estimate value reliability modeling 
widely accepted threshold software changes reliable highly reliable failures hour ight threshold knowing testing demonstrate degree precision 
examples illustrate testability information replace testing formal veri ca tion suggest testing formal veri cation relied exclusively 
view software development veri cation inclusive 
ective software practitioners take advantage available information order build assess quality software 
rest focus software testability discussion context technique complementary testing formal veri cation 
remainder elaborates topics software testability designing bility sensitivity analysis 
section describe detail testing formal analysis 
look mechanics predicting testability early stage development 
describe concept information loss characteristic predicted initial design description function programmed 
information loss characteristic provides insights improve testability basing software design testability heuristics 
describe sensitivity analysis technique quanti es behavioral information likelihood faults hiding 
sensitivity analysis dynamic empirical relying repeated executions original mutations source code data states 
sensitivity analysis allows assess testability achieved applying design testability schemes 
nal section shows predictions sensitivity analysis included estimates software reliability 
software testability big picture better provide understanding testability consider simple analogies 
software faults gold software testing gold mining 
software testability survey done mining takes place 
job dig gold 
establishes likelihood digging particular spot rewarding 
say may gold gold top feet valley 
location say nd gold rst feet plateau gold 
plateau dig feet sure gold 
software testing begins initial survey obvious advantages testing blind 
testability suggests testing intensity gives digging depth 
testability provides degree di culty incurred testing particular location detect fault 
testing degree speci ed testability observe failures reasonably sure program correct second analogy illustrate fewer tests yield equivalent con dence correct ness sure software hide faults imagine writing program scanning black white satellite photos looking evidence large barge 
sure barge appear black rectangle barge cover pixel area image program techniques barge size established 
example assume original image subsampled pixel new image average square pixels original image 
subsampled image scanned times quickly original barge size guaranteed large barge detectable lower resolution image 
shape suspected barge determined detailed examination original image higher resolution 
barge exist image smaller gle low resolution image hide barge inside averaged pixels 
lower bound barge size lower resolution image su cient locating 
direct relationship minimum barge size amount speed accomplished subsampling 
looking barge image analogous looking faults program examining groups pixels examine output test case executions 
fault cause larger proportion inputs fail testing analogous bigger barge fewer random tests required reveal fault coarser grid locate barge 
guarantee fault program cause program fail su ciently large proportion tests reduce number tests necessary con dent faults exist 
analogies indicate testability information useful strategy complementary testing 
section discuss design testability system 
designing testability prevention goal assess software quality accurately demonstrate high quality 
black box testing assess software intractable amount testing required establish small probability failure 
variable represent true probability failure 
assess probability failure failures test con dence shown approximately successful executions tests input distribution needed 
foolproof automated reasonably sure suggest colloquial con dence statistical con dence 
discuss precise quanti cations section oracle available practical problems testing clear 
furthermore random black box testing software fail software xed random black box testing completely restart ignore previous successful executions redo testing 
statistics shown new code contains new faults 
clearly need seek new methods increase ectiveness testing 
reduce number required tests tractable number techniques available 
select tests greater ability faults 
research test coverages mutation adequate test sets seeks nd ective individual tests sets tests 

design software greater ability fail faults exist 
designing programs fail big faults means create programs larger proportions code exercised input contain program constructs cause state program incorrect constructs incorrect propagate incorrect program states software failure 
software design testability addresses issue developing code fails big topic focus section 
problem code veri cation techniques applied late software life cycle 
code exists point awed incorrect ine cient design decisions little done veri cation undo mistakes enormous additional costs 
problem exists viewing testability code produced 
testability addressed earlier life cycle 
integrated circuit design testability long viewed required characteristic 
inte circuit design engineers notion termed observability notion closely related software testability 
observability ability view value particular node embedded circuit 
software modules contain local variables lose ability see information local variables functional testing 
sense having local variables software analogous lowering observability circuits 
discussing circuits states principal obstacle testing large scale integrated circuits signals 
method increasing observability circuits design increase pin count allowing extra pins carry additional internal signals checked testing 
output pins increase observability increasing range potential bit strings chip 
feel issue functional testing object oriented systems 
apply similar notion increasing pin count software testing increasing amount data state information checked unit testing 
information loss occurs internal information computed program execution communicated program output 
information loss increases potential cancellation data state errors decreases software testability 
divide information loss broad classes implicit information loss explicit information loss 
explicit information loss occurs variables validated execution self test execution termination output 
explicit information loss observed technique static data ow analysis 
explicit information loss frequently occurs result information hiding factors contribute 
information hiding design philosophy allow information leave modules potentially misused modules 
information hiding widely accepted structured programming practice 
hiding internal information testability system level data local variables available revealing faults 
implicit information loss occurs di erent incoming parameters user de ned function built operator produce result 
example operator level consider integer division computation div di erent incoming values result assigned 
consider user de ned function takes integer parameters produces boolean parameter di erent integer tuples possible result 
consider computation implicit information loss 
examples potential implicit information loss occurring observed statically analyzing code 
speci cation states oating point variables input implementation boolean variables contain implementation output know implicit information loss occur implementation speci cation 
written information concerning domain ranges speci cations estimate degree implicit information loss occur speci cation implemented 
domain range ratio clues suggesting degree implicit information loss may occur execution may visible program speci cation speci cation metric termed main range ratio suggesting degree implicit information loss 
recall example able observe implicit information loss code inspection 
speci ca tion domain range ratio suggests portion implicit information loss may occur code inspection give additional information concerning implicit information loss 
domain range ratio drr speci cation ratio cardinality domain speci cation cardinality range speci cation 
denote drr cardinality domain cardinality range 
previously stated ratio discernible speci cation 
drr estimated gives important information possible testability problems code required implement speci cation 
roughly predict degree implicit information loss 
generally drr increases speci cation potential implicit information loss occurring implementation increases 
greater previous research suggested faults remain undetected exist testing implicit information loss occurs run risk lost information may included evidence incorrect data states 
evidence visible output probability observing failure testing somewhat reduced 
degree reduced depends incorrect information isolated bits data state lost eventually released output 
probability observing failure decreases probability undetected faults increases 
research report presents similar concerning relationship faults remaining undetected type function containing fault 
performing mutation testing experiments boolean functions noted faults boolean func tions cardinality range apt undetected faults types functions 
boolean functions typically great degree implicit information loss 
result supports idea testability drr correlated 
additional evidence correlation exists implicit information loss testability currently collected 
correlating implicit information loss drr implicit information loss common built operators modern programming languages 
operators mod high 
table functions generalized degrees implicit information loss 
function classi ed having implicit information loss table receive altered incoming parameter produce identical output original incoming parameter function classi ed having implicit information loss table altered incoming parameter produce altered output 
table suggests data state error cancellation occur suggests data state error cancellation occur 
table assume constant simplicity 
table mathematical entities computer environment represent cardinality xed length number domain results div result potential values variable potential values variable total pairs potential inputs 
notice inputs integer division produces output real division produces unique outputs 
representations nite size 
domain illustrates relationship implicit information loss drr 
input pairs functions performs real division performs integer division 
real division function unique outputs integer division function output 
example shows di erences forms division correlated di erent amounts information loss 
explicit information loss harder find early explicit information loss predicted drr 
recall explicit information loss observed static code inspection potential implicit information loss predicted functional descriptions code inspection 
explicit information loss may observable design document depending level detail 
explicit information loss dependent software designed dependent speci cation input output pairs 
function implicit information loss drr comment integer integer mod testability decreases decreases div testability decreases increases trunc real round real sqr real sqrt real real integer integer sin integer degrees odd integer boolean boolean table implicit information loss various functions cardinality integers cardinality reals 
design heuristics strategies reducing detrimental ects information loss testability 
speci cation decomposition isolating implicit information loss major advantage drr guide development early life cycle 
drr speci cation xed modi ed changing speci cation ways decomposing speci cation reduce potential data state error cancellation occurring modules 
speci cation decomposition hands control drr subfunction 
gain intuitive feeling subfunction implemented degree testing needed particular con dence module propagating data state errors 
rule thumb guides intuitive feeling greater drr testing needed overcome potential data state error cancellation 
design speci cation decomposed manner program modules designed high drr low drr 
isolating modules propagate incoming data state errors program testing low drr testing analysis resources shifted module testing modules propagate incoming data state errors 
minimizing variable reuse reducing implicit information loss method decreasing implicit information loss minimize reuse variables 
instance seen computation asa sqr destroys original value take square root computation retrieve absolute value lost sign 
minimizing variable reuse attempt decrease amount implicit information loss 
minimizing variable reuse requires creating complex expressions declaring variables 
number variables increased memory requirements increased execution 
complex expressions reduce testability single expression represents previously intermediate values 
literature supports programming languages variables programs written languages certainly su er low 
advocate variables making variables available testing 
increasing parameters reducing explicit information loss consider analogy modules integrated circuits local variables internal signals integrated circuits 
analogy allows see explicit information loss caused local variables parallels notion low observability circuits 
explicit information loss suggests lower prefer possible lessen amount explicit informa tion loss occurs testing 
limiting amount explicit information loss possible bene knowing modules greater data state error cancellation potential validation begins 
approach limiting amount explicit information loss insert write statements print internal information 
information checked correctness test 
second approach increasing amount output cations return treating local variables parameters testing 
third approach inserts self tests assertions executed check internal information computation 
research suggested assertions particularly useful testability analysis assertions assure particular variable correct range point execution failed assertion suggests possibility previous computations variable de nition depends erroneous 
approach messages concerning incorrect internal computations subsequently produced terms testability likelihood fault hiding reduced 
approaches produce important results 
people formalizing speci cation forced produce detailed information states internal computations 
increase likelihood code written correctly forces code test 

dimensionality range intended function increased may increase cardinality range reducing information loss 
approaches simulate idea previously mentioned integrated circuits increasing signals 
advocating approaches practice information hiding design 
writing software safety critical software competing imperative enhance testability 
information available testing encourages undetected faults increased output discourages un detected faults 
answer pattern software testing closely hardware testing specify special output variables pins speci ed implemented speci cally exclusively testing 
disadvantage approaches approaches bene cial need additional speci ed information concerning internal computations 
real message research ort better specify occur intermediate computation level assessed reliabilities remain low 
unfortunate design testability research validate internal information hope increase software testability 
validate internal information way additional internal information 
requires information described speci cation requirements phase 
willing specify details point expect substantially improve reliability assessments 
sensitivity analysis previous section described improve software testability 
question remains measure increase testability 
section brie presents model quan software testability described detail 
sensitivity analysis separating software failure phases execution software fault creation incorrect data state propagation incorrect data state discernible output 
part model software failure referred pie propagation infection execution 
separate algorithm part pie model analyzer analyzer analyzer 
rest section give brief outline phases sensitivity analysis 
details see 
simplify explanations describe phase separately production analysis system processing phases overlap 
analysis random testing accuracy sensitivity analysis depends part estimate input distribution drive software 
fault cause failure executed 
faults isolated single location program 
location de ned single high level language statement machine code instruction intermediate amount computation 
experiments far de ned location piece source code change data state including input output les program counter 
assignment statement statement de ne location statement read de nes locations 
probability execution location determined repeated executions code inputs selected random input distribution 
execution analyzer estimates execution probabilities 
location contains fault location executed data state execution may may changed adversely fault 
fault change data state data state incorrect input say data state infected 
estimate probability infection second phase sensitivity analysis performs series syntactic mutations location 
mutation program re executed random inputs time monitored location executed data state immediately compared data state original program point execution 
state di ers infection taken place 
infection analyzer estimates infection probability 
third phase analysis estimates propagation 
location question mon random tests 
location executed resulting data state changed assigning random value data item predetermined distribution 
research ongoing best distribution random selection 
data state changed program continues executing output results 
output results changed data state compared output resulted change 
outputs di er propagation occurred propagation probability estimated 
propagation analyzer estimates propagation probabilities 
sensitivity analysis fault simulation injection method relies assump tions admittedly awed single fault simple fault 
single fault assumption says program contains single fault multiple faults distributed pro acronym pie doesn spell 
authors regret confusion may cause 
gram 
simple fault assumption says fault exists single location distributed program 
assumptions combinatorics simulating classes distributed multiple faults intractable 
fault classes simulate arti cially restricted 
despite theoretical weakness practice empirical techniques yielded impressive experimental results see 
phase sensitivity analysis produces probability estimate number trials divided number events occurred execution infection propagation 
random test reveal fault execution infection propagation occur result failure 
product mean estimates yields estimate probability failure result location fault 
take minimum estimates obtain product bound minimum probability failure result location fault 
sensitivity analysis new empirical technique pilot experiments early done hand coded syntactic mutations semi automated data state mutations 
complexity processing required sensitivity analysis quadratic number code locations requires considerable bookkeeping execution time 
sensitivity analysis require oracle completely automated programs size processing time practical limit large programs analyzed single block 
fully automated commercialized sensitivity analysis tool built applied systems large sloc 
tool operate larger systems knowledge 
value sensitivity analysis generally fold determining system level testing needed gain con dence faults hiding identifying regions code extreme low testability additional unit testing resources applied 
additional bene ts realized slight modi cations analysis algorithms 
example described modify algorithms increase fault tolerance improve software safety assessment 
applications outside scope 
summary contend results experiments sensitivity analysis su cient motivate additional research technique successful experiments shown 
guarantee new technique possible assess reliability precisions required life critical software 
think premature declare assessment impossible 
sections argue software testability produces accurate predictions possible combine random black box testing sensitivity analysis assess reliability precisely possible black box testing 
squeeze play random black box testing sensitivity analysis gather information possible prob ability failure estimates program 
techniques generate information distinct ways random testing treats program single monolithic black box sensitivity analysis examines source code location location random testing requires oracle deter mine correctness sensitivity analysis requires oracle judge correctness random testing includes analysis possibility faults sensitivity analysis focuses assumption fault exists 
techniques give di erent kinds predictions probability failure 
true probability failure particular program conditioned input distri bution single xed value exact value unknown 
treat probability failure random variable black box random testing estimate probability density function pdf conditioned input distribution 
predict pdf ing sensitivity analysis prediction conditioned input distribution testing pdf pdf predicted sensitivity analysis conditioned assumption program contains exactly fault fault equally location program 
assumption single randomly located error variation competent programmer hypothesis 
figures show examples possible approximated pdf 
horizontal location height indicates estimated probability true probability failure program value curve example estimated pdf derived random black box testing assume testing uncovered failures 
test expect probabilities failure near closer 
course test produces correct output pr 
details deriving estimated pdf random tests 
curve example estimated pdf derived sensitivity analysis 
sensitivity analysis estimates location probability failure induced program single fault location 
pdf conditioned assumed input distribution assumption program contains exactly fault assumption location equally fault 
marked interval estimates estimated pdf 
interval marked includes area estimated pdf formalizes method employed nd predicted probability failure sensitivity analysis 
mean estimated pdf curve estimate probability failure 
prediction minimum probability failure sensitivity analysis 
random testing actual probability failure con dence 
similarly interval includes area estimated pdf sensitivity analysis exists fault imply probability failure greater con dence 
exists fault induces near zero probability failure testing nd error 
locations sensitivity estimates close zero troubling ultra reliable application 
fault induces probability failure pof fault failures observed fault 
faults program true probability failure ultra reliability correctness achieved 
suggests fault induce small probability failure suggests tiny impact faults meaning cause program fail probabilities 
wenow attempt quantify meaning estimated pdfs taken 
hamlet derived equation determine calls probable correctness 
tests executed failures occurred pr probable correctness true pof approximation 
likelihood meaning thinking really represent impact caused true pof smallest fault program smallest possible non zero pof program independent faults removed 
faults assumed unknown 
represent prediction sensitivity analysis code testing distribution fault classes sensitivity analysis simulated 
note situations true know 
test cases nd wehave con dence pr obtained subtracting small factor numerical estimate sensitivity analysis test cases 
actual interval con dence just pr pr pr probability failed correctly assess minimum pof induced fault program fault classes simulated 
probability function fault classes simulated sample size test cases sensitivity analysis 
realize fault program fault class simulated smaller impact fault classes consider 
better explain equation assume tested times errors 
assume sensitivity analysis selected smallest pof induced faults simulated 
point establishing con dence software 
trivially pr pr pr 
pr pr pr hamlet calls measure probable correctness called con dence correctness equations cast traditional hypothesis test 
assessing pr scope 
requires additional probabilities probability actual fault causing lower impact actual failure probability probability order magnitude precise number input values determining isa statistical approximation error 
second problems partially formalized 
estimate probability program correct estimating pr sensitivity analysis estimating pr hamlet probable correctness equation goal squeeze play model push xed con dence high 
approaches exceeds increasingly con dent software correct 
example equation suppose program independent faults impacts 
situation 
suppose 
situation 
xed close meaning try large respect likelihood program fail tests close able apply equation failure occur 
suppose remove faults observing failures sensitivity analysis modi ed program 
test code times order close 
pr pr 
know probability true fault causing lower impact actual failure probability knowing faults best con dence absolute correctness successful tests con dence true probability failure pr pr section shown method combining testability analysis testing results sharpen estimate true pof 
testability measurement essentially clean operation method assessing software achieved desired level reliability 
believe testability assessment useful earlier development software 
idea table gives feeling expensive testing di erent levels con dence di erent degrees testability 
research suggests software testability clari es characteristic programs largely ignored 
think testability ers signi cant insights useful design testing reliability assessment 
conjunction existing testing formal veri cation methods testability holds promise quantitative improvement statistically veri ed software quality 
program single fault 
table various particularly interested designing software increase testability 
ideally process begins functional description input distribution pair speci es intended software 
may theoretical upper bound exists testability achieved functional description input distribution pair 
functional description include internal information able increase upper bound 
existence upper bound testability solely conjecture research sensitivity analysis studying software tendency reveal faults testing suggests exists 
challenge software testing researchers consider conjecture 
piece software hide fault testing 
possible examine code characteristic knowing particular fault exists software correctness 
rely correctness characteristic software testability gives new perspective 
brie described dynamic technique software sensitivity analysis predicting software testability 
sensitivity analysis yielded promising results experiments research practical application technique continues 
ective cient testability measurement techniques discovered techniques employed measure testability convinced inherent software char important factor consider software development assessment 
neil 
level sensitive scan design tests chips boards system 
electronics march 
butler 
infeasibility experimental quanti cation life critical software reliability 
proceedings sigsoft software critical systems december new orleans la 
richard hamlet 
probable correctness theory 
information processing letters april 
hamlet voas 
faults sleeve amplifying software reliability assessment 
proc 
acm sigsoft issta pages cambridge ma june 
brian 
experiments software testing 
technical report uiucdcs university illinois urbana champaign department computer science november 
john musa anthony 
software reliability measurement prediction application 
mcgraw hill 
miller park nicol voas 
ing probability testing reveals failures 
ieee trans 
software engineering january 
voas miller payne 
comparison dynamic software testability metric static cyclomatic complexity 
proc 
nd int 
conf 
software quality management edinburgh scotland july 
voas michael miller 
con dently assessing zero probability software failure 
proc 
th international conf 
computer safety reliability security 
october poznan poland 
voas 
pie dynamic failure technique 
ieee trans 
software engineering august 

