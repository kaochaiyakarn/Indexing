lectures statistical modeling theory rissanen helsinki institute information technology technical universities tampere helsinki finland computer learning research center university london royal holloway uk rissanen mdl research org contents part information coding shannon wiener information basics coding random variables 
basic properties entropy related quantities 
channel capacity 
chain rules 
jensen inequality 
theory types 
equipartition property 
coding random processes random processes 
entropy stationary processes 
markov processes 
tree machines 
algorithm 
arithmetic codes 
universal coding lempel ziv ma algorithms 
universal context coding 
part ii statistical modeling kolmogorov complexity universal algorithmic model 
kolmogorov sufficient statistics 
statistical modeling models 
universal models 
mixture model 
normalized maximum likelihood model 
predictive universal model 
strong optimality 
prediction bound loss functions 
stochastic complexity 
mdl principle 
structure function model complexity 
sets typical sequences 
distinguishable distributions hypothesis testing 
applications linear regression 
mdl denoising 
examples 
statistical modeling model building activity aimed learning rules restrictions set observed data called laws go stated maxwell 
traditional approach presumably influenced physics imagine assume data generated sample population originally parametrically defined probability distribution generally called nonparametric distribution 
imagined unknown distribution estimated data various principles squares maximum likelihood principles minimization mean loss function mean taken respect imagined distribution 
process situation similar physics law guessed correctly capable describing data sufficiently able account unavoidable deviations small random instrument noise 
statistical applications law assumed probability distribution sample estimates get data certainly deviate observed data small instrument noise 
worse reliable absolute way decide data set sample typically obtained suggested distribution 
true special cases coin flipping physical data generating mechanism suggests probability model reasonable assurance match 
general knowledge machinery generates data convert probability distribution samples statistically similar observed data impossible task trying estimate exist 
certainly imagine target distribution estimated assessment estimation error meaningless 
put way problem statistics viewed estimating data probability distribution belonging assumed set rational way compare different sets distributions assumed 
general complex distribution fits data better simpler degree fit measured reasonable standard impossible theory rule inevitable best model complex assumed 
usual way avoid resort judgment ad hoc procedures 
practitioners course know intuitively find true data generating distribution regarded unreachable model 
leads awkward position estimate model estimates define actual model live modeling model 
different approach problem step right direction 
starting reasonable idea general model collection parametric models capture regular features data merely look model confines collection selected best 
order formalize akaike assumed true model exist captures properties data collection selected 
looks model collection closest true model sense information theoretic kullback leibler distance 
model maximum number parameters minimize distance certainly wanted 
distance unknown true model ones collection computed anyway objective changed minimize mean distance mean taken estimated models 
estimated asymptotically result akaike aic criterion 
overcome basic difficulty stemming fact density function true false exists captures information data 
impossible interpret akaike criterion criterion derived false premise terms observed data know properties data learned 
admit remedy sorts difficulties stemming true model assumption devise robust estimation method designed matter true distribution large class generate data 
avoid catastrophic results way approach unsatisfactory general principle goes grain intent model building statistical inference learn properties data generating mechanism 
way aimed escaping curse false assumption true data generating mechanism called cross validation data reuse 
technique picks portion available data fit models uses rest compare performance fitted models 
sets models picked randomly 
pure adhoc technique fallacious idea repeated sampling extract information existing data 
summarize traditional dogmatic approach statistical model building prob abilities viewed inherent properties random data restricted resting logical foundations 
undoubtedly reason evolved general theory provide insight including statements limitations statistical inquiry kind done 
lack general theory restricts creation new methods model construction required understand data met field data mining dna modeling image processing complexity far exceeds considered traditional statistics 
different popular bayesian view modeling probabilities worry interpretation assigned just seemingly attractive idea best hypothesis model collection highest probability true light observed data prior probability assigned hypotheses data independent grounds 
course prior determined data longer available 
hypotheses true meaningful talk probability case simple modeling situations 
interpretation offered jeffreys eminent bayesian probability model degree belief place statement degree belief hypothesis meaningful illuminating 
degrees belief satisfy usual axioms probabilities difference names just semantic 
called posterior probability data modifies prior obtained bayes formula normalization joint probability data hypothesis amounts division joint marginal probability data 
define probability distributions virtually set hypotheses see hypothesis highest probability hypotheses true taken model best captures regular features data 
jeffreys offered justification case prior probabilities equal important case desire avoid choice priors shall ready accept hypothesis requires fact observations occurred remarkable coincidence 
statement says probabilities models model assigns highest probability data happens highest posterior probability defined bayes formula 
advantages maximization posterior probability great degree fact dominant degree inherited fundamental principle seek models maximize probability data 
maximum likelihood estimate parameters plays central role posterior model classes peaks maximum likelihood estimate hard see real justification bayesian techniques especially large amounts data simply fact unwittingly approximate quite fundamental global maximum likelihood principle subject matter main interest lectures called 
algorithmic theory information introduced solomonoff discuss briefly provides different foundation statistical inquiry principle free prob lems discussed 
important data need regarded sample distribution idea model simply computer program describes encodes data 
may strange equivalent probability distribution constructed length program number binary digits required describe program 
length shortest program serves measure complexity data string construct known kolmogorov sufficient statistic provides natural measure complexity best model data 
leave wanting statistical inquiry fatal problem shortest program argued provide penultimate ideal model data length algorithmic means 
extensive treatment theory refer 
development coding information theory follows spirit algorithmic theory avoids computability problem see 
permits define stochastic complexity data set relative collection models fewest number bits probabilistic sense data encoded code designed help models 
words set programs universal computer replaced code binary tree leaves define codewords data sequences length observed data 
quite nicely code equivalent probability distribution length path root leaf data sequence decoded essence negative binary logarithm probability assigned data sequence distribution 
true 
universal model defined complexity lets define long sought information data string extracted collection models hand 
fact obtain split code data information bearing part noise having useful information contribute 
numerical data split extends data 
means compare different classes models regardless structure shape size simply amount useful information extract data get foundation authentic theory model building statistics general real valued parameters number treated equal footing 
objective lectures carry program statistical model theory introductory way 
basic relevant notions information coding theories reviewed part part ii main theme statistical modeling developed manner inspired theory kolmogorov complexity universal model reviewed briefly 
part information coding formal notion information sense complexity introduced time late independently norbert wiener claude shannon 
despite fact case notion information influential ideas history science wiener regard worthwhile elaborate 
gave brief sketch application communication problems 
contrast shannon provide elaboration result entirely new discipline information theory growing impact branches science 
clear basic information measure restricted form hartley essence concept measure disorder statistical mechanics longer boltzmann 
initially information theory synonymous communication theory gradually central concept entropy variations kolmogorov sinai entropy chaotic processes started find important applications probability theory parts mathematics 
mid sixties exciting new idea emerged added new branch information theory goals theorems completely different communication problems 
information string formalized length shortest computer program generates string 
new measure usually called kolmogorov complexity introduced clear manner years earlier solomonoff 
idea rediscovered chaitin important contributions theory 
shannon wiener information basics coding random variables coding want transmit store sequences elements finite set 
am terms binary symbols 
set called alphabet elements called symbols kinds numerals 
sequences called messages just data symbols numerals 
defining code function symbol alphabet finite binary string called codeword 
interested codes maps inverse 
code may extended sequences 
xn operation concatenation xn xa denotes string obtained symbol appended string want extended code written invertible codewords symbols write symbols xi separated recognized code string comma 
implies important restriction codes called prefix property states codeword prefix 
requirement making code prefix code implies important kraft inequality ni ni denotes length codeword 
prove consider leaves complete binary tree tree node sons 
tree obtained starting leaf tree splitting successively leaf nodes tree obtained 
leaf tree kraft inequality holds equality 
splitting node length equality holds son nodes just probabilities bernoulli process easy induction implies kraft inequality holds equality complete binary tree 
codewords prefix code leaves tree completed 
claim holds 
easily extended countable alphabets sum limit strictly increasing finite sums bounded unity 
codeword lengths prefix code define probability distribution ni denotes sum left hand side kraft inequality 
converse true essence 
assume conversely set integers 
nm satisfies kraft inequality 
shall describe prefix code lengths codewords integers 
maximum integers 
write term ri ni ni ri ri assumption 
sum leaf probabilities subset leaves 
balanced tree depth get required prefix code partitioning subset segments 
adjacent leaves segment common mother node length ni take codeword 
important practical coding problem results ask design prefix code symbols taken independently probabilities want code short mean length min ai ai ai possible lengths ai required satisfy kraft inequality 
may regarded measure mean complexity alphabet symbols taken sampling distribution intuitively consider object complex takes large number binary digits describe encode 
optimal lengths corresponding prefix code huffman algorithm far important remarkable property theorem code kraft inequality holds mean code length satisfies log 
ai log ai ii ai log ai proof 
give simple proof fundamental theorem usually credited shannon 
letting log stand binary logarithm ln natural ai log ai ai log ai ln ai ai inequality follows ln 
log ai ai ai log ai 
lower bound mean code lengths theorem famous entropy may taken measure ideal mean complexity light large alphabets entropy close 
ideal code length log ai shannon wiener information may taken complexity element ai relative distribution role distribution model elements set describes property turn restricts elements collective sense individually 
entropy measures strength restriction inverse manner 
instance binary strings generated sampling bernoulli distribution source probability symbol greater low entropy meaning strong restriction allowing short mean code length 
conversely coin flipping strings little constrains maximum entropy symbol 
hint theorem intimate connection code length mini mization model selection main theme lectures 
see consider class independent identically distributed iid processes alphabet defined parameters ai ai suppose data sequence length generated unknown process family consider encoding data sequence prefix code 
assign binary string length symbol ai code length string ni ni denotes number times symbol ai occurs string 
code length minimized log ni seen theorem integer 
take log ni satisfy kraft inequality get near optimal code length large alphabets extra length amounts bit symbol 
worry integer length requirement consider log ni ideal code length see optimal ideal code length gives maximum likelihood estimate parameters ai process generated data 
basic properties entropy related quantities notation random variable distribution symbolized write frequently 
clearly sum non negative elements 
important binary case write 
function symmetric concave function reaching maximum value vanishing points 
conditional entropy defined log log 
notice independent 

prove 
relative entropy distributions defined log shannon theorem equality iff 
relative entropy called kullback leibler distance 
note general 
important mutual information defined log marginals denoted respectively 
mutual information symmetric 
see write follows 
log log log log log equality iff independent 
generally conditional mutual information defined satisfies equality holding iff conditionally independent 
log vanishes exactly numerator written denominator 
follows log equality iff uniform distribution denotes range size 
equation implies equality holding iff independent 
channel capacity log log 
channel information theorists mean conditional probability distribution model physical channel denotes random symbols entering channel usually related general distorted symbols exiting channel 
coding chance selecting probability distribution input symbols put channel say 
problem utmost importance select distribution mutual information maximized maxw iw maxw realize pw depends maximization problem difficult 
derive important necessary condition maximizing distribution pw channel capacity iw pw 
derive 
find maximizing probability function wi write wi form lagrangian wx set derivative respect wx zero wx log pw log pw wz wx 

pw second line take sum get maximizing pw algorithm due blahut calculate channel capacity 
hard see capacity results double maximization maxw maxq log fact consider gq pw gq maximizes log gq shannon theorem claim follows 
starting guess maximizing say uniform find maximizing conditional distribution 
conditional distribution find iteration maximization done easily lagrange multipliers 
result cycle repeated 
converges channel capacity follows general result due csiszar 
chain rules theorem 
xn 
xn 

xn xi xi 

proof 
claim follows 
get claim application second term 
simple induction claim follows theorem 
xn xi xi 

proof 
claim follows previous theorem definition conditional mutual information 
instance claim follows third terms second fourth adding 
define conditional relative entropy theorem log proof 
immediate definitions gives claim 
jensen inequality log important tools information theory jensen inequality 
function called convex interval interval lie cord 
lies points function strictly convex 
function concave convex 
theorem convex mean written operation satisfies ef ex 
strictly convex equality implies constant 
proof 
discrete 
range just points pi xi 
convexity 
theorem hold points 
pif xi xk pk pi xi pk xk pk pk pi xi pk pi xi 
pk conclude section demonstrating relative entropy convex function arguments probability measures implies entropy concave function argument single probability measure 
definitions convexity concavity generalized functions reals functions arguments linear combinations may formed 
theorem convex 
proof 
sets numbers ai bi ai log ai bi ai ai log bi seen true application shannon theorem distributions ai aj bi bj 
putting applying summing get claim 
theorem concave 
proof 
uniform distribution log function convex previous theorem claim follows 
theory types give brief account important theory types 
consider iid process alphabet definition type sequence 
xn empirical probability measure px px px denotes number times symbol occurs definition pn set types example binary alphabet pn definition type class px 
px py px example 
theorem 
px px px px 
px 

pn proof px function px 
functions 
theorem xi iid process distribution denote xi probability sequence 
xn 
proof corollary px px px xi example bernoulli process log theorem px pn log px px log log px px nh px nh log ni log ni 
nh px px nh px 
proof upper bound px lower bound px pnt px nh px px nh px max px pn nh px follows 
theorem pn proof implies claim 
corollary equipartition property 
nd nd nh information theoretic inequalities entropy related quantities derived properties mean get strengthened range random variables involved large results hold close approximation essentially values just mean 
intuitively want 
instance may talk complexity individual strings sense nearly strings generated certain probabilistic source mean complexity 
alphabet range random variable large get random variables large range symbols consider sequences 
define set typical sequences iid process distribution follows px type px px px px px px px nd px log goes zero 
grows probability set typical sequences goes near exponential rate matter theorem typical sequences just equal probability 
xn entropy 
called asymptotic equipartition property 
tail probabilities complement events summable probability events zero borel cantelli lemma 
probability set infinite strings px xn finitely times unity positive 
means reaches finite number fluctuation type data generating probability exceed fixed amount depending goes zero shrink zero 
words ni surely ni denotes number occurrences symbol coding random processes hitherto considered data modeled outcomes random variables defined alphabet single probability distribution sampled repeatedly leads iid process 
general way model data sequences consider samples random process 
random processes definition 
countable sequence random variables 
said form random stochastic process measurable function set finite strings exists satisfying axioms 
empty string 
xn 
xn 
xn notice joint probability measure pn 
xn exists random variables 
second axiom requires marginal probability function obtained pn collection 
xn agrees joint probability function pn collection pn 
xn pn 
xn xn course automatically satisfied 
true safely omit axioms notation conforms kolmogorov extension theorem 
states set probability measures pn satisfying equation admit unique extension defined infinite strings 
axiom permits define conditional probabilities xn xn xn linking past clearly needed meaningful predictions 
idea stationarity captured axiom means terms random variables 
xn 
xn 

xn xn 
xn xn just statement shift invariance 
process independent entropy stationary processes ways define entropy random process 
lim 
xn 
lim xn xn 
provided course limits exist 
theorem stationary processes 
proof 
equation subsection 
xn xi 
xn xn 
xn xn 
xn xn 
equality follows stationarity 
limit nonincreasing sequence positive numbers exists 
consider 
xn xi xi 
xi xi 

difference brackets greater implies 
xn xi xi 

second term goes zero increasing term certainly 
take small 
get opposite inequality notice stationarity fact conditioning increase entropy xi xi 

equation 
xn limit 
theorem write 
markov processes clear describe random process constructive way instance purpose fitting data consider special types processes take infinite number conditional probabilities equation specify 
familiar important subclasses processes finite memory definition process finite memory process xn xn xn 
xn xn xn 
xn xn 
write recursion xn 
process called markov process order order markov process satisfying 
xn xn xn 
xn symbols xt called states 
particular conditional probabilities xn xn change function time process called time invariant 
letting xt range 
get equation summing 
xn xn xn xn xn 
due properties matrix pij state transition probabilities probabilities states converge stationary limits written column vector col 

add unity equation satisfy 
example 
binary order markov process 
solving stationary state probabilities additional requirement gives stationary markov processes log 
case order markov process example get denotes binary entropy function evaluated 
tree machines characteristics particular importance markov processes order number free parameters needed define process 
markov process order alphabet size states having probability parameters 
important applications number states large number parameters required specify process small estimation relatively easy 
worthwhile separate characteristics achieved tree machines introduced analyzed developed 
describe binary tree machines 
take complete binary tree characterized property node successors node leaf 
node conditional probability stored root node written 
addition defined permutation indices write 

intent order past symbols xn xn importance influencing symbol xn think importance example markov processes implicitly assume important symbol xn important xn achieved picking permutation reverses order 
modeling image data pick order previous symbol xn symbol xn right symbol xn geometric nearness dimensions symbol xn 
st xn xn 
deepest node tree defined past permuted symbols 
tree defines joint probability 
xn xt st string take 
additional assumption stationarity implies leaf probabilities determine means process gets defined conditional probabilities leaves 
tree need balanced number parameters smaller number states 
notice set leaf nodes may define states 
simple example showing 
take leaf tree consisting leaves 
consider previous symbol xn state symbol xn moves machine state leaf 
order implement tree machine sufficient number past symbols stored states parameters tree needed compute probabilities strings 
algorithm section discussed traditional coding data sequences modeled samples random variable extended sequences amounts iid process 
prefix code minimum mean length huffman algorithm described textbooks coding 
main interest codes models random processes requires generalization traditional codes creation completely different codes 
discussion traditional algorithm due suitable coding random processes huffman algorithm important universal coding modeling algorithm due lempel ziv 
algorithm constructs binary code tree desired number codewords say rule split node maximum probability leaves generated 
segments source strings encoded paths leaves encoded log bits binary ordinal leaf sorted left right 
coding called variable fixed length coding 
analyze codes give convenient formula mean length finite complete tree node probabilities satisfy axioms random process root probability unity internal node probability sum children probabilities 
ti tree leaves si 
si 
mean length defined ti si depth node fuse pair leaves common father father node say denote resulting complete subtree ti see ti ti int runs internal nodes tree ti 
simplify subsequent discussion consider bernoulli sources 
main results hold types dependent sources 
theorem algorithm produces tree leaves maximum mean length 
proof tree leaves maximum mean length 
leaf tree length leaf tree clearly optimal 
arguing induction see ti optimal split leaf completes proof 
maximum probability fast way encode leaf segments tree enumerate left right encode leaf ordinal written binary log bits 
variable fixed coding efficient 
theorem just proved ratio log tm minimized tree trees size 
secondly shannon theorem mean ideal code length leaves leaf entropy tm 
clearly log upper bound entropy excess absolute minimum tm tm depends smaller entropy uniform log tree leaves greater leaf entropy tree 
see consider bernoulli process parameter take smaller entropy defined leaf probabilities complete subtree ti ti log 
si consider subtree ti obtained fusing leaf sons say father node clearly ti ti log log log ti ti int denotes binary entropy function 
result generalizes markov sources alphabet extension tree placed state 
pm pm respectively maximum minimum leaf probabilities tree leaves 
pm 
terms code lengths may easier argue log pm log pm log forms induction base 
show induction inequality holds larger trees 
fact suppose holds size splitting node pn create new nodes probabilities pnp smallest pn leaf tree 
induction assumption log pn log pn log leaves tn need check condition new leaf pnp 
pn completes induction 
clearly log log pn log pn log tn log pn tn log give tn log tn log told see 
arithmetic codes different kind code altogether arithmetic code introduced developed rissanen langdon suited coding kinds random processes 
code ideas 
code cumulative probability strings length sorted alphabetically computed recursively 
practical implementation calculations done fixed size registers 
consider case restriction placed precision sake simplicity consider binary alphabets 
letting denote probability function defines random process take cumulative probability code binary tree left son get immediately recursion 
minor drawback code strings 
main drawback conditional probabilities written finite precision multiplication third equation keep increasing precision soon exceed register pick 
needed satisfy condition random process increasing precision 
ways 
straightforward way replace exact multiplication third equation denotes truncation fractional binary number 
just digits occurrence 
clear addition update code string done register width possible overflow dealt 
shall show presently code strings zero decoding done 
consider strings differ symbol xt 
empty strings symbol 
case nonempty code strings written 

denote strings length respectively 
see regardless xt xt 
obviously true second empty 
nonempty 
right hand side results 
gives largest possible sum 
conclude decoding done rule xt turn decided looking leading bits code string 
problem possible overflow solved bit 
fact register code updated full addition propagate decoded portion string 
order prevent happening additional bit added code string bit position immediately left register 
overflow turn detected decoder taken account 
estimate code length 
see code length essence number leading zeros plus size register needed write probability 
data generated process smallest integer mini taken larger log log log 
get xn log xn large close ideal code length desired large 
estimate excluded effect bit general extends code length amount 
quantity longer defines probability measure random process defines semi measure 
requires multiplication undesirable 
possible remedy defects tolerable increase code length 
consider recursive definition process denote probability binary symbol written binary fraction fractional digits 
take equal low probability symbol course fact general depending past string 
define xn integer satisfies 
words string decreasing probability normalized fixed size register 
calculate recursively 


see digits defines random process 
encoding algorithm initialize register set 
read symbol 
exists shift contents left positions obtain final code string 

symbol low probability symbol say replace contents go 
symbol high probability symbol add register number subtract number 

shift contents registers left positions required bring range 
go 
decoding algorithm initialize register leftmost symbols code string set 
form auxiliary quantity subtracting contents test 
decode symbol load 

decode symbol high probability symbol 
load subtract 
shift contents registers left positions required bring range read number symbols code string positions exist 
go 
case random variables prefix code lengths probability distributions correspondence equate certain codes random processes 
code random process map prefix condition suggests generalization xn denotes length code string 
kraft inequality holds equality code lengths define random process 
time integer length require ment difference prevent representing processes small alphabets accurately codes instance binary bernoulli process smallest positive integer value give maximum value accurate model process say 
arithmetic coding accurately represent process codewords represent cumulative probabilities defining random process 
symbols increase code length kraft inequality fail 
appropriate generalization kraft inequality simply statement quantity arithmetic code constructed recursively defines conditional probability generating random process 
call code regular 
help arithmetic codes state regular code equivalent random process 
universal coding practice data compression problem encode data string probability distributions 
long time problem tackled designing codes intuitive basis realized codes incorporate type model universal entire class pi stationary processes ranges countable set 
know shannon theory design code member class mean length close corresponding entropy reasonable requirement code claims represent entire class universal relative class property mean symbol code length approach symbol entropy limit matter process class generates data eil hi expectation respect pi hi corresponding symbol entropy 
construct define codes property may start asking mean symbol length approach entropy fastest possible rate 
little reflection reveal ask shannon theorem achieve entropy instantly guess correctly data generating process 
countable number index values expect right guess exceptional case interest practice 
conceptual reasons worry remote events need bit careful specifying sense code fastest rate 
subsection turn code designs satisfy requirement 
lempel ziv ma algorithms elegant algorithm universal coding due lempel ziv 
algorithm parses recursively data string nonoverlapping segments rule starting empty segment new added collection symbol longer segment longest match far 
lempel ziv encoded segment pair written binary number gives index longest earlier match list added symbol 
code length string log denotes number parsed segments 
may appear probability model algorithm 
parsing incorporates occurrence counts symbols effect define random process sequence trees variable fixed length coding tree keeps growing 
see consider recursive construction binary tree binary string 
xn 
start node tree root node marked counter initialized son counter initialized 

recursively previous tree parse segment remaining string path root deepest node tree 
climbing tree increment count node visited 

visited node leaf split create new son nodes initialize counts 
denote parsed segments write string terms 
corresponds exactly parses lz algorithm segment may complete 
notice leaves tree count unity father node count sum sons counts 
divide node counts root count node gets marked probability 
leaf highest count gets split just algorithm 
write 
zj denotes prefix full segment 
identifying zj corresponding node far constructed tree define conditional probability symbol xt past string xt zj 
multiply conditional probabilities fully parsed segment get conditional probability 
equality follows fact parsed segment adds root count initialized 
probability string full segments 
gives ideal code length log somewhat better lempel ziv code length fairly crude way coding done code 
lempel ziv code universal large class processes class stationary ergodic processes sense ep matter process generates data 
holds sure sense 
notation ep denotes expectation respect process satisfy minimum optimality requirement stated 
rate convergence shown log case class markov tree machine processes seen 
universal context coding elegant lz algorithm defect doesn know learned constraints searching increase complexity block length 
reflected slow approach ideal entropy data fact generated markov process approximately 
shall describe algorithm learning power 
discuss basic case encoding binary strings generated bernoulli process symbol probability known 
words class processes considered denotes number string coding method called predictive obvious reason encode symbol probability 
knowing symbol occurrences encode symbol probability course 
general put xt xt 
denotes number times symbol occurs string scheme invented laplace asked probability sun rise tomorrow 
easy exercise reader show scheme defines probability sequence 

wrote xn 
code length get log log log 
exactly probability results formula marginal distribution dp dp pick uniform prior probability symbol 
integral dirichlet integral value 
particular strings close zero unity better estimate conditional symbol probability reason explained section 
xt xt 
ready describe algorithm called algorithm context introduced analyzed universal large class markov processes 
particular provides coding string alphabet say binary manner symbol code length approach data generating markov process process approach fastest possible sense precise 
algorithm provides better compression lz algorithm strings markovian type properties 
algorithm stages combined describe separately algorithm growing tree algorithms effect tree pruning accurately choice encoding node 
simplicity reverse ordering past symbols 
algorithm 
node tree marked counts code length 
read symbol xt exists exit 
climb tree reading past string backwards xt xt 
update count ci unity code length log obtained node met conditions satisfied 
node count ci update internal node go 
leaf create new nodes initialize counts 
go 
portion algorithm creates general tree tree grows path traveled frequently 
node represents context symbol occurs ci times entire string 
fact path root node binary string 
ik substring ik 
occurs entire string close times count node notice real occurrence count may little larger substring may occurred node created 
important condition satisfied nodes counts greater 
tree created complete 
exist versions algorithm create incomplete trees 
choice encoding nodes symbol xt occurs node encode ideal code length equation counts node symbol occurs node root occurs shorter node path raises question node pick 
way select encoding node ll describe 
notation substring symbols past string occurred node code length computed predictively conditional probabilities 
rule 
xt pick node si 
notice sides refer code length symbols left hand side symbols occur father node right hand side occur son nodes 
rule finds node son nodes code length longer better father node code length 
clearly algorithm search entire path leaf strategy may find best node 
rule 
possible find optimal subtree complete tree node marked code length symbols occur symbols occur node occur son nodes 
final tree say obtained algorithm insured equation 
write code lengths simply 
algorithm prune 
initialize tree put set leaves 
calculate leaves 

recursively starting leaves compute father node min sj 
element smaller equal second replace sons set father leave unchanged 

continue root reached 
easy convert algorithm context universal code 
need arithmetic coding unit receives input symbol xt predictive probability encoding node say rewritten xt xt xt 
notice coding unit needed encoding node 
arithmetic code works set conditional probabilities defining random process clearly conditional probabilities algorithm context define process xt root node 
code length universal code 
answer question assume data generated stationary markov source order parameters state irreducible 
show mean ideal code length resulting universal code defined equation satisfies log log entropy source 
see universal code exists mean symbol code length approaches entropy faster 
see lz code optimal stronger sense 
part ii statistical modeling statistical modeling finding general laws observed data amounts extracting information data 
despite creation information theory half century ago formal measures information entropy kullback leibler distance relative entropy serious difficulties applying exact idea information extraction model building 
main problem measures refer information probability distributions data 
indication strength dogma statistical thinking relatively information extraction process formalized akaike searching model proposed collection closest true model probability distribution terms kullback leibler distance 
true model assumed lie outside collection known observed data distance estimated 
turns difficult akaike criterion aic amounts asymptotic estimate mean kullback leibler distance mean taken estimated models class having number parameters 
akaike represents important deviation thinking tradition avoiding need add artificial terms criterion penalize model complexity difficulties criterion 
course need assume existence true underlying model 
matter fact paradigm kullback leibler distance gets reduced number parameters fitted models increased problem trying avoid 
gets overcome estimated mean distance great explain paradox ideal criterion known fails estimation process expect things worse produces criterion works better ideal 
known assume true model set fitted models aic criterion find matter large data set suggests lack self consistency 
quite different way formalize problem extracting information data appears natural 
idea inspired theory kolmogorov complexity reviewed complexity data set measured fewest number bits encoded advantage taken proposed class models 
complexity measure relative class models act language allowing express properties data shall see information data 
intuitive sense language poor expect able learn gross properties 
language rich express large number properties including spurious random quirks 
raises thorny issue deciding properties data want learn 
solution idea portion data compressed class models available defined uninteresting noise rest want learn useful learnable information 
may state achieve decomposition data purpose modeling 
formalize fundamental notions complexity information relative class models need assume metaphysical true model represent information data set data sets generated sampling true model 
formalization provide solid foundation theory modeling course solve model selection problems selection class models remains 
fact lot prior information gathered data generated similar conditions 
formalize way saying information suggest promising classes models 
kolmogorov complexity see implies process selecting optimal model model class done informal means human intelligence intuition play dominant role 
currently methodology model selection revealed similar limits done 
kolmogorov complexity string generated sampling probability distribution suggested ideal code length log serve complexity shannon complexity justification mean large alphabets tight lower bound mean prefix code length 
problem course arises measure complexity depends strongly distribution cases interest 
feel intuitively measure complexity ought linked ease description 
instance consider types data strings length length ought taken large point 


generate string flipping coin times definitely regard string simple easy rule permitting short description 
describe string telling length takes log bits giving rule 
length greater description length rule alternate symbols starting encoded computer language binary string ignored comparison log amount information extract rule small information extracted 
second string appears complex quite rule 
zeros ones may taken advantage 
fact just 
strings quite bit number strings length 
just sort strings alphabetically encode string ordinal written binary encode string log bits 
suggests modeling string class bernoulli models 
information extracted way bit previous case amount 
third string maximally complex rule helps shorten code length results write symbol symbol 
information small learned string looks random 
preceding discussion language objects described left ified vague 
ingenious way specify language take universal programming language program machine language form binary string 
usual pro gramming languages universal recursive function number arguments programmed 
computer execute programs pu delivering desired binary data string 
xn output 
clearly countable number programs data string add number instructions canceling create string 
program pu fed computer machine prints stops 
may view computer defining map decoding binary string programs 
terminology program pu codeword length pu string notice machine stops start means program generates stops prefix longer program 
place set programs said property binary tree appear leaves satisfy kraft inequality infinite prefix code 
kolmogorov complexity string relative universal computer defined ku min pu pu 
words length shortest program language set programs generates string 
set programs may ordered length programs length alphabetically 
program index list 
importantly set generating grammar programmed universal computer language effect universal computer execute computer programs translation compiler program called 
list programs place shortest program pu capable translating programs universal computer means kv ku pv ku cu cu depend string exchanging roles see dependence complexity long strings particular universal computer diluted 
course claim dependence eliminated constant cu large wish picking bad universal computer 
recognizing reasonably long string shortest program universal computer designed knowledge particular string capture essentially regular features string safely regard fixed kolmogorov complexity ku provides virtually absolute measure string complexity 
universal algorithmic model kolmogorov complexity provides universal model par excellence equate probability measure pk ku ku sum finite kraft inequality 
construct universal process defined recursively follows string followed symbol pk pk 
universal distribution remarkable property able mimic computable probability distribution sense pk aq constant depend holds universal process 
kolmogorov sufficient statistics identified shortest program string ideal model capture regular features string quite correspond intuition 
fact associate model regular features entire string generating machinery 
instance purely random string regular features model empty having complexity 
outline construction due kolmogorov desired idea model captured algorithmic theory 
kolmogorov complexity immediately extended conditional complexity length shortest program generates string string causes computer 
sees readily mean equality say equality constant depending length strings sense denote approximate inequalities 
take program describes summarizing properties string property data may formalized subset data belongs sequences sharing property 
amount properties inverse relation size set smallest singleton set represents conceivable properties set consisting strings length assigns particular properties length 
may think programs consisting parts part describes optimally set number bits kolmogorov complexity second part merely describes log bits denoting number elements sequence gets described log bits 
consider sets log max xn 
suppose string xn ask maximal set holds 
equivalently set smallest complexity 
set defining program called kolmogorov minimal sufficient statistic description xn define kolmogorov algorithmic information string generalization kolmogorov minimal sufficient statistic due 
consider function min log xn 
clearly implies xn xn nonincreasing function maximum value log log minimum value hx log xn 
mina hx hx line xn constant 
equal smallest min hx xn get kolmogorov minimal sufficient statistic decomposition findings interesting conceptually important solve universal coding modeling problems 
kolmogorov complexity noncomputable 
means program data string computer calculate complexity ku binary integer 
usual proof important theorem due kolmogorov appeals undecidability called halting problem 
give proof due 
suppose contrary program qu said property exists 
write program pu qu subroutine finds shortest string pu ku pu essence program pu examines strings sorted alphabetically nondecreasing length computes ku checks qu inequality holds 
clear shortest string exists ku upper bound pu fixed finite length 
definition kolmogorov complexity ku pu contradicts inequality shown 
program qu assumed property exists 
possible estimate complexity increasing accuracy idea close complexity 
despite negative content proved result utmost importance statistical inference clearly sets limit meaningfully asked 
statistical effort trying search true underlying data generating distribution systematic mechanical means hopeless truth mean best model 
human intuition intelligence endeavor indispensable 
clear light physical phenomena simple data admit laws einstein put god find laws inherently difficult 
generally taken discover simplest laws physics 
algorithmic theory strings maximally complex prove single raises intriguing question data strings arising physical world maximally complex nearly obeying laws 
statistical modeling discussed problem main interest obtain measure complexity useful information data set 
algorithmic theory complexity primary notion allows define intricate notion information 
plan define complexity terms shortest code length data encoded class models codes 
previous section saw leads problem class models include set computer programs model identified computer program code generates data 
select smaller class problem avoided overcome difficulty define shortest code length 
order fall back kolmogorov complexity spell exactly distributions models restrict coding operations 
universal coding just doing coding predictive way lempel ziv code index leaf tree segments determined past data context coding applying arithmetic code symbol conditioned context defined algorithm function past data 
adopt different strategy define idea shortest code length probabilistic sense turns satisfy practical requirements 
formal models 
models data sets type 
yn xn yt xt data kind consider decomposition yt xt parametric function defining part model 
remainder viewed noise need measure size 
done error function extended sequences yt yt 
important error function conditional probability density function yt yt log yt yt seen define parametric class probability models independence 
definite sense virtually types loss functions represented ideal code length probability model 
fact distance function define density function dy 
existence integral clearly puts constraint distance function severe 
instance gives finite integral 
extending independence get ln ln yt yt 
consider unconditional probability models data variable written important classes probability models exponential families 
denotes parameter vector 
functions ln denote vector derivatives da derivatives respect equation dx get holds 
maximum likelihood estimate seen satisfy giving consider entropy ln 
ef ln 
evaluate get remarkable equality ln 
shows things maximized likelihood depends 
factorization 
important related class probability models induced loss functions ln ln yt yt normalizing coefficient depend 
loss functions called simple 
see minimize ideal code length respect get nd ln yt yt ln dy holding 
minimizing value minimizes mean ln yt yt yt yt 
having minimized sum prediction errors respect know mean performance mean taken distribution defined minimizing parameter value 
example consider quadratic error function 
normalizing integral dy depend 
see get normal distribution mean variance 
extend sequences independence minimize negative logarithm respect variance see minimizing variance minimized quadratic loss 
quadratic error function special case loss functions type call class 
normalizing coefficient dy gamma function 
loss functions seen simple 
show distributions maximum entropy distributions 
consider problem max ln maximization ep 
restriction density functions ln ln denotes entropy 
shannon inequality entropy satisfies ln right hand side upper bounded 
equality reached result generalizes familiar fact normal distribution variance maximum entropy distributions variance exceed consider models defined probability density functions single data sequences denote need double parameter structure index 
vector real valued parameters ranging subset dimensional euclidean space 
consider parametric distributions defined discrete probability functions density functions data items xt range real numbers 
get bigger class models union 
example 
class tree machines complete tree defining set contexts determined set leaves 

described parameters define probability assignment string satisfies axioms random process 
example 
take class arma processes xn xn 
en 
bq en pair plays role collection coefficients forms 
equations define required probability distribution model process en iid process random variable having normal distribution zero mean unit variance 
universal models fundamental idea universal model model class assign large probability density data strings get help model class 
equivalently ideal code length data strings minimized 
particular data string universal model depend string depend model class 
way obtain universal models construct universal coding system intuitively appealing coding method fact way universal coding systems constructed extend 
particularly informative useful construct optimization problems solutions define universal models 
mixture model construct universal models solutions certain optimization problems consider mixture universal model 
prototype optimization problems codes shannon theorem expressed min log xn xn solved interested codes class models just may look distributions ideal code length shortest mean worst case opponent model generates data 
minmax problem obtained follows 
define redundancy excess mean code length obtained distribution entropy obtained distribution rn log xn 
may ask distribution solution minimax problem min max rn 
tackled easily embedding wider problem considering mean redundancy rn rn density function parameters space 
really conflict notations limit distributions wi place probability mass shrinking neighborhood call rn rn 
minimizing distribution rn shannon theorem mixture distribution pw gives minimized value rn min rn xn log xn pw iw 
seen mutual information random variables defined kullback leibler distance joint distribution product marginals pw 
may ask worst case follows sup rn sup iw kn seen capacity channel subsection distance maximizing prior means kn model family lies surface special mixture center 
minmax problem generalized minmax relative redundancy problem defined follows define unique parameter value min log log xn 
words picks model class nearest data generating model kullback leibler distance need lie model class 
consider min max log xn 
time solution easy find asymptotically modification mixture solving 
normalized maximum likelihood model consider minimization problem min log 
minimizing parameter ml maximum likelihood estimator 
curiously previous case best model computed known best code length obtained computed model integrate unity 
shannon noiseless coding theorem data generating distribution fixed gives best ideal code length log 
suppose fix best ideal code length equivalently defining probability mass function discrete alphabet consider family ideal prefix code lengths log equivalently set probability mass functions simplicity consider discrete data results hold density functions 
theorem probability mass function solution maxmin problem maxmin value reached unique 
take get shannon problem 
max min log max min log max min min inequality zero take sets second term maxmin value zero solve minmax problem min max 
fact minmax value lower bounded maxmin value renders bracketed term value zero maximizing done 
maxmin minmax problems related shtarkov minmax problem min max log xn replace take nml normalized maximum likelihood universal model write density function form log log log dy 
dy denotes density function statistic induced model 
required integral finite space chosen open bounded set 
keep index fixed formulas drop 
continue derivation quite accurate formula negative logarithm nml model 
main condition required ml estimates satisfy central limit theorem clt converges distribution normal distribution mean zero covariance lim ln slight generalization called fisher information matrix 
cn call canonical prior limit known jeffreys prior 
desired asymptotic formulas log log log log 
term goes zero takes care rate convergence central limit theorem details 
important optimal definable terms model class selected computed computation integral square root fisher information may pose problem 
number ways approximate ll discuss 
minmax problem maxmin problem related solutions close 
illustrated theorem essence due gr white theorem data generated iid process 
probability unity qn xn family density functions 
ln tn nm qn tn ln xn qn probability nm denotes ml estimate evaluated nm sketch proof ln argument indicates point derivative evaluated 
similarly ln xt 
fixed value sum converges strong law large numbers mean log probability sum zero light means limiting mean zero 
proves 
apply strong law large numbers view conclude second claim 
predictive universal model nml model depends ml estimates turn depend define random process 
consider conditionals xt du define universal model negative logarithm follows log log xt 
small values ml estimates may define singular probabilities avoid modify estimates appropriately done context algorithm universal coding 
clearly initialization problem estimate parameters need data points 
predictive process important way obtain universal models particular cases data ordered data set small 
summation parts log log log xt xt log xt xt ratios sum exceed unity happens equal sum positive 
prove section generalization shannon theorem implies log 
equality shown cases 
strong optimality log xt universal models considered solutions minmax maxmin problems cer optimal worst case opponent leave open question ideal code length shorter opponents 
give theorem may viewed extension shannon noiseless coding theorem shows worst case performance rule exception 
consider class parametric probability density functions parameter vector 
ranges open bounded subset dimensional euclidian space 
parameters taken free sense distinct values specify distinct probability measures 
theorem assume exist estimates ml estimates satisfy central limit theorem interior point converges probability normally distributed random variable 
density function defined observations positive numbers set volume goes zero mean taken relative 
log xn log give original proof generalized proof stronger version theorem 
exists elegant proof extension theorem 
proof consider partition set dimensional hypercubes edge length constant 
say mn centers hypercubes form set write cn cube center 
need construct corresponding partition subset xn set sequences length estimator define xn cn 
probability set distribution pn 
assumed consistency estimator probability set xn pn satisfies inequality pn greater number say nc depends small please selecting large 
consider density function qn denote probability mass assigns set xn 
ratio pn defines distribution xn course xn qn 
shannon theorem applied distributions get log xn xn dx pn log pn qn xn positive number set left hand side denoted tn satisfies inequality get tn log 
log qn pn log pn log log nk holds 
replace pn lower bound reduce right hand side 
pick large greater number 
term bracket strictly unity second term bounded log log converges zero growing expression brackets sufficiently large say larger 
qn larger 
smallest set centers hypercubes cover number elements 
lebesgue volume vn bounded total volume hypercubes fact sets xn disjoint get vn nc 
qn nn gives upper bound get desired inequality shows vn grows infinity 
inequality ln xn get vn xn ln dx qn pn denotes complement finish proof 
opposite inequality holds 
adding left hand sides get concludes proof 
ln ln add sever smoothness conditions models prove dawid similar theorem sure sense define random process 
set measure log log log finitely probability 
denotes unique measure infinite sequences defined 
results show right hand side bound beaten code necessary loophole lebesgue measure zero reachability results immediately 
clearly exception provided data generating model gives shorter code length course wild guess 
exists stronger version theorem minmax bound log cn effect states worst case bound rare event beaten essence assume data generated benevolent opponents 
prediction bound loss functions consider loss function yt ft predictor 
normalizing coefficient dy 
xt induced parametric models yt xt write short 
denote class models predictor ft put get induced model pf consider minmax problem min max lf yt yt ep lf 
theorem yt ft predictor 
interval positive inequality ep lf ln holds large set volume goes zero grows infinity expectation respect 
proof consider ep ln pf ep lf 
show ml estimates satisfy central limit theorem conditions theorem satisfied 
right hand side exceeds claim follows 
ep ln quantifications 
stochastic complexity light maxmin theorem strong optimality theorem previous subsection justified regard log log log log cn log cn log log 
stochastic complexity model class 
regard form universal sufficient statistics decomposition model class 
result balasubramanian cn maximal number distinguishable models implies may regard second term learnable information term code length noise 
study issues detail specify models involved carefully find slightly different decomposition number optimally distinguishable models 
mdl principle shown preceding section universal nml models respectively permit extract useful information data extracted model classes 
consider model classes xn xn universal sufficient statistics decompositions 
suppose stochastic complexity smaller 
code length optimal model information proportional log noise proportional dominant part stochastic complexity 
part looks noise model class xn explained useful information model class 
words model class able extract properties data rendering meaningless noise 
take simple example consider alternating binary string 
light bernoulli class probability symbol half string noise extracted useful information zero 
hand order markov model conditional probability symbol unity state symbol unity state render entire string useful information leaving unexplained noise 
different model classes may extract entirely different properties data impose different constraints learning purpose modeling 
considerations suggest best model class tentatively suggested collection gives smallest stochastic complexity data code length index 
min log call mdl minimum description length principle 
code length depends collection 
small finite collection taken logarithm number classes case ignored 
complex collections may selected carefully reflect knowledge situation 
give universal answer 
determination mdl principle leads regression need consider best code length code equivalently prior 

simply cut process point final added code length ignored 
mdl principle represents drastically different foundation model selection fact statistical inference general 
number distinctive features need assume existing data generated 
particular traditional statistics data need assumed form sample population probability law 
admitted traditional statistics models false useful 
view statement meaningless reflects idea true description data generating probability machinery try estimate false models 
view objective modeling estimate assumed unknown distribution inside outside proposed class models find models data 
importantly principle permits comparison models model classes regardless type 
provides vastly general criterion aic bic depend number parameters 
compress data advantage data restricting features 
means small random changes data alter best fitting models mdl models naturally robust 
application principle requires course calculation stochastic complexity complex model classes difficult task practice may settle upper bound approximation 
reasonable way proceed decompose complex model class smaller ones formula derived valid 
code length required links needed put estimated usually visualizing concrete coding scheme 
alternative replace stochastic complexity formula ideal code length obtained predictive universal model despite problems initialization difficulty applicable 
important realize mdl principle say select suggested family model classes 
fact problem adequately formalized 
practice selection human judgement prior knowledge kinds models past researchers 
mention important issue related restriction models instance prediction 
issue apparently discussed time 
mdl view model general captures relevant regular features data expect able predict reliably just properties data wish 
predict properties captured model 
notice model regarded approximation imagined true underlying distribution case traditional statistics restrictions properties predict 
concrete suppose find mdl model class markov models turns bernoulli model data generated order markov process 
case sufficient amount data justify correct model 
prediction probability symbol denotes number zeros data string reliable 
fact fitted markov model order get estimate probability state equals 
contrast prediction probability optimal bernoulli model completely unreliable differ estimate relative number occurrences consecutive zeros data typical correct order markov process 
say mdl models provide reliable estimates properties captured model models imperfect compare theorem 
having determined mdl structure index problem finding optimal model necessarily involves quantized parameter uncountable code length ml parameter infinite 
just quantization done want code length model properties data exceed length data 
deeper theory needed find study 
structure function extend kolmogorov structure function algebraic theory complexity statistical models order avoid problem 
construct analogue kolmogorov complexity generalize model finite set statistical model 
kolmogorov complexity replaced stochastic complexity model class required analogs discussed 
model complexity consider hyper ellipsoid dn bounded parameter space centered defined parameter 
bd maximal rectangle dn 
volume bd nk 
bd denote partition rectangles 
rectangles curvilinear grow difference disappear 
reason norm defined euclidian norm kullback leibler distance models centers adjacent cells pair adjacent cells 
words partition corresponds asymptotically equi distance partition class models 
seen applying taylor expansion kl distance adjacent models written point fi fj ln xn dx 
drop index held constant 
grows difference shrinks zero see kullback leibler distance pairs adjacent models 
cn qd qd bd defined 
obtain discrete prior rectangles wd qd assuming continuity cn qd bd wd bd jeffreys prior 
right hand side depend asymptotically uniform prior model encoded code length log cn qi log cn log 
clear code length asymptotically optimal sense worst case model 
sets typical sequences replace set formalizing properties string algorithmic theory 
intent strings equal sharing properties specified typical set 
suggests replace set typical strings model set strings bd complexity set code length needed describe just log code length worst case sequence need code length worst case sequence bd obtained taylor series expansion follows denotes sequence ln ln empirical fisher information matrix ln point 
assume continuity converges 
replacing ln ln results insignificant error define analog kolmogorov structure function follows min ln xn 
minimizing inequality satisfied equality asymptotic approximations get get recall equation rewritten ln ln cn 
hx ln xn 
hx ln ln ln 
min hx xn gives crucial optimal way separate complexity data complexity model complexity noise 
want ignore constants means approximate equality want write equality 
poses bit problem value reach stochastic complexity log 
resolve problem asking smallest value part complexity left hand side reaches minimum value 
case ask smallest fact value minimizes part code length min hx ln minimum reached point ln ln cn 
ln 
large logarithm number optimal models isnot larger number obtained balasubramanian ln cn 
similarly universal sufficient statistics sition differs term large 
ln depend small consider line analogous sufficiency line algorithmic theory ln ln 
curve hx lies line max max ln ln point line tangent 
universal sufficient statistics decomposition immediately resolves puzzling anomaly traditional statistics maximum likelihood estimate values parameters acceptable estimate number 
just parameters principle estimation applicable 
explanation maximum likelihood principle rejected cases distinguishing noise learnable part information 
case damage minor large amounts data convergence properties estimates 
case damage devastating 
light decomposition see case separate noise part information fit parameters capture information words precision quantization defined cells bd example 
derive universal sufficient statistics decomposition bernoulli class parameter 
nml distribution xn xn denoting number xn write form xn xn 
order evaluate resulting ideal code length ln important tous stirling approximation formula form refined robbins ln 
ln ln permits evaluate terms sum equation sufficient accuracy ln nh ln ln ln binary entropy gives recognizing sum denominator equation step length approximation riemann integral get approximately dp integral square root fisher information dirichlet integral value 
normalizing coefficient log nh log log cn log 
added term goes zero rate takes care errors application stirling formula 
accurate approximations normalizing coefficient exist 
length intervals bn giving optimal quantization bn approximations close zero unity 
example continued 
discuss structure function bernoulli class 
width equivalence class parameter bd 
canonical probability distribution centers equivalence classes structure function cn min nh log denotes number string binary entropy function evaluated point minimizing value distinguishable distributions balasubramanian showed interesting result cn fixed parameters gives maximal number distinguishable models obtained data size idea distinguishability differential geometry somewhat intricate mainly reason defined terms covers parameter space partitions 
fact models fi fj matter far apart intersecting supports sense models distinguished direct interpretation 
wish refine balasubramanian idea obtaining measure separation models intuitively appealing easily interpreted 
importantly show value parameter number equivalence classes bd measure optimized 
considering partition rectangles previous section construct special model defined center rectangle bd having rectangle support qd bd qd bd 
see defined model note 
integrating bd get bd dy bd 
nd denote number rectangles bd partition parameter space 
nd cn qd 
qd value cn nd gives number rectangles particular number rectangles close cn 
natural prior universal model written wd view member models mixture perfectly separated supports partition parameter space 
objective study models support set se quences undoubtedly natural models defined parameters model data yt yt 
consider kl distance 
clearly smaller distance larger probability mass falls support bd perfectly separable model 
minimize distance tell adjacent models overlap get measure adjacent models distinguished 
meaningful minimum reduce approaches peak gets divided decreasing volume kl distance increases bounds 
theorem minimizing 
proof 
taylor expansion min ln bd ln yn dyn 
ln ln qd data generated converges surely 
ln bd ln bd ln qd dy bd 
second convergence follows previous convergence 
need evaluate integral 
rotation coordinates set eigenvalues 
bd 
bd th side length rectangle 
putting right hand side minimized ln min ln 
development refines ideas balasubramanian talk optimally dis models type 
number differs balasubramanian number models cn cn 
structure function optimal number difference due fact considered worst case typical sequence sets bd mean kl distance 
regard models defined parameters cell indistinguishable corresponding sequences equivalent defines optimal model 
relevant hypothesis testing need test difference models indistinguishable 
estimate probability set bd induced model fi fundamental implication hypothesis testing 
change variables ni integral bd nk bd bd cumulative density function normal density function mean zero unit variance 
conclude section discussion second mixture implementation 
mixtures universal models integrate unity 
little difference densities assign sequence 
difference define conditionals range yt restricted bd yn wd yn applied iteration calculated define initial state 
terms sum direct calculation takes lot computations 
sum approximated integral close earlier discussed mixture xn jeffreys prior 
fact 
clearly recursive implementation xn evaluate integrals yt conclude actual calculation density assignment universal models simple matter 
brings final point great advantage idea distinguishable distri butions permits isolation optimal model center size having components 
exact calculation center problem estimate quantizing components size determined eigenvalues 
optimal model provides alternative clean solution traditional ideas robust estimates confidence intervals 
cn hypothesis testing basic assumption neyman pearson hypothesis testing data generated hypotheses considered assumption verified means 
result need estimate hypotheses models important source uncertainty testing procedure considered 
drawback testing optimal way measure separation null composite hypotheses 
neyman pearson testing done roundabout way follows value test selected amounts selecting arbitrarily probability making mistake null hypothesis assumed generate data 
power function opposing hypotheses say parametrically defined calculated gives probability rejecting null hypothesis fact false 
power represents confidence decision accepting rejecting null hypothesis arbitrarily selected level 
case composite hypothesis versus null hypothesis power evidently increases opposing hypothesis gets farther farther away parameter value defines null hypothesis 
know opposing hypotheses generate data case null hypothesis false impossible talk confidence result test 
recommended value test level 
means null hypothesis accepted data fall far tail distribution taken null hypothesis despite possibility opposing hypothesis give greater probability data case null hypothesis rejected 
suppose test find drug common cold 
data fall far tail matter accept useless drug 
suppose drug intended cure serious illness serious side effects 
matters accept reject null hypothesis 
summary find optimal decision boundary null hypothesis opposing ones see data fall 
wish tamper decision boundary rationally account effects resulting errors 
subsection discuss different hypothesis procedure hypotheses models fitted data 
real issue able measure models fitted data separated 
case just models problem amounts calculating error probabilities determine decision boundary sum error probabilities minimized 
difficult case parametric class models taken null hypothesis 
central problem partition parameter space countably equivalence classes adjacent models optimally distinguished measure intuitively acceptable 
testing null hypothesis opposing ones model testing procedure pick null hypothesis center equivalence classes optimal say opposing composite hypothesis consists models defined centers data fall accept null hypothesis 
fall reject null hypothesis accept hypothesis fj 
confidence testing measured ratio bd index see confidence increases rapidly increasing distance null hypothesis appears reasonable adjacent models hardest distinguish 
example bernoulli class 
suppose null hypothesis width equivalence class null hypothesis probability 

data fall confidence falls say adjacent intervals confidence twice great 
applications linear regression linear squares regression problem polynomial fitting fundamental modeling problem give exact formula stochastic complexity applicable small data sets 
due importance problem discuss detail full treatment see 
consider basic linear regression problem data type yt 

wish learn values xit 
regressor variables xi influence corresponding values yt regression variable may large number regressor variables problem interest find subset may regarded important 
clearly difficult problem able compare performance subsets different sizes 
fit linear model type yt xt 
ik denotes subset indices regressor variables prime denotes transposition computation required code lengths deviations modeled samples iid gaussian process zero mean variance parameter 
model response data 
yn normally distributed density function yt xt xit matrix defined values regressor variables indices 
write taken positive definite 
development fixed drop matrices parameters 
maximum likelihood solution parameters yt xt 
density function admits important factorization follows nk 
see admits factorization joint density function course product marginal density conditional density 
factor normal mean covariance distribution degrees freedom 
independent statistic said sufficient 
depends data 
consider nml density function restricted set include zn dz numerator equation simple form problem evaluate integral denominator 
integrating conditional equals fixed value yields unity 
get expression density function br ellipsoid volume dy kvk br nml density function log ln ln ln 
ln ln ln 
wish get rid parameters clearly affect criterion essential manner replace parameters influence relevant criterion 
set parameters values minimize 
resulting density function 
course correct multiplying prior result density function triplet quite right 
normalization process dz range defined presently 
subsequent equations factorization 
integrate conditional keeping constant gives unity 
setting integrate resulting function range volume hyper ellipsoids bounded 
told get dz expressed volume element vk dv ln dv dr ln negative logarithm ln ln ln ln ln ln ln ln ln 
term depends sense ml estimates included range parameters 
means term ignored values vary function 
interesting closely related formula obtained fitting hyperparameters mixture density data 
applying stirling approximation functions get nml criterion min ln ln ln ln denotes number elements 
mdl denoising important special case linear regression problem called denoising problem remove noise data sequence 
xn taken row vector remaining smooth signal 
xn represents real information bearing data xt xt 

ordinarily viewed problem estimating true signal xt expressed orthonormal basis wi 
win 
independent normally distributed noise mean variance added 
estimation done minimizing risk criterion xt xt expectation respect assumed normal distribution estimated data 
trouble estimation 
fact regression matrix wij defined rows transpose inverse defines transformation denote column vectors strings data 
xn coefficients 
cn respectively 
orthonormality parseval equality holds 
accordingly gives natural estimate meaningless minimum 
estimation done arbitrary scheme portion extracted smooth signal 
contrast mdl solution regression problem poses difficulty universal sufficient statistics decomposition provides natural definition noise part data compressed selected normal model class rest defined optimal model gives desired information bearing signal 
decomposition xt xt 
defined optimal model 
fact orthonormal permits dramatic simplification take advantage large want regression matrix defined basis vectors 
denote defined matrix 
transform equations get replaced denotes ml solution obtained available basis vectors entire matrix corresponding orthogonal projection space spanned basis vectors 

ik denote set indices denote row vector components ici zero 
words non zero components indices corresponding components define ml estimates notations previous section 
ml estimate xt xt 
criterion finding best subset including number elements equivalent min min ln sk ln sk ln sk 
parseval equality sum squared deviations minimized largest coefficients absolute value 
utilized earlier solutions denoising problem terms threshold coefficients coefficients absolute value threshold set zero 
threshold certain minmax property orthonormal transformations defined wavelets derived donoho johnstone just ln said variance estimated 
second term criterion maximized largest squared coefficients clear threshold exits search minimizing index set possible index sets avoided 
theorem theorem orthonormal regression matrices index set minimizes criterion indices 
largest coefficients absolute value indices 
smallest ones proof arbitrary collection fixed number indices sk corresponding sum squared coefficients 
ui term sk 
derivative respect ui dc dui sk sk nonpositive sk tk tk sk positive 
second derivative negative means function ui concave 
sk tk reduce replacing say smallest square larger square outside get sk larger tk smaller 
process possible 
consists indices largest squared coefficients 
similarly sk tk reduce consists indices smallest squared coefficients 
sk tk squared coefficients equal claim holds trivially 
remarks may weird threshold defined require setting coefficients exceed threshold zero 
assumptions data criterion applied happen signal recovered defined model complex noise relative classes distributions considered normal noise uniform models 
case may pay reverse roles information bearing signal noise 
denoising problem data typically smooth denoised curve simple simply optimize criterion largest coefficients absolute value min min ln ln ln 
denoting column vector defined coefficients 
ci 
zero signal recovered examples calculate examples wavelets defined daubechies scaling function 
example case mdl threshold close obtained traditional techniques threshold obtained complicated cross validation technique 
second example real speech data 
example mean signal xi consists equally spaced samples function defined piecewise polynomials data points xi added pseudorandom normal mean noise standard deviation defined data sequence xi 
threshold obtained nml criterion 
thresholds called visushrink type derived donoho johnstone 
close threshold obtained complex cross validation procedure nason 
second example data sequence consists samples voiced portion speech 
nml criterion retains coefficients exceeding threshold absolute value 
gives value noise variance 
shows original signal information bearing signal extracted nml criterion 
see nml criterion removed sharp peaks large pulses despite fact locally high frequency content 
simply compressed retained coefficients general principle criterion regarded noise 
original signal dotted line nml signal solid line 
speech signal smoothed daubechies wavelet akaike 
information theory extension maximum likelihood principle pages petrov eds second international symposium information theory 
budapest balasubramanian 
statistical inference occam razor statistical mechanics space probability distributions neural computation arxiv org list boltzmann 
uber er barth 
barron rissanen yu 
mdl principle modeling coding special issue ieee trans 
information theory years information theory vol 
october pp chaitin 
length programs computing finite binary sequences sta tistical considerations jacm cover thomas 
elements information theory john wiley sons new york pages davisson 
universal noiseless coding ieee trans 
information theory vol 
nr november table integrals series products academic press new york pages gr 
minimum description length principle reasoning uncer tainty phd thesis institute logic language computation universiteit van amsterdam pages hansen yu 
model selection principle minimum description length jasa appear hartley 
transmission information bell system technical journal jeffreys 
theory probability clarendon press oxford pages third edition kolmogorov 
approaches quantitative definition information problems information transmission lempel ziv compression individual sequences variable rate coding ieee trans 
information theory vol 
september li vitanyi 
kolmogorov complexity applications springer verlag pages merhav feder 
strong version redundancy capacity theorem universal coding ieee trans 
information theory vol 
pp may 

topics descriptive complexity phd thesis linkoping university linkoping sweden 
source coding algorithms fast data compression phd thesis stanford university rissanen langdon jr universal modeling coding ieee trans 
information theory vol 
nr 
rissanen 
generalized kraft inequality arithmetic coding ibm res 
dev 
nr rissanen 
universal data compression system ieee trans 
information theory vol 
nr 
rissanen 
universal coding information prediction estimation ieee trans 
information theory vol 
nr 
rissanen 
stochastic complexity modeling annals statistics vol rissanen 
stochastic complexity statistical inquiry world scientific publ 
suite main street river edge new jersey pages rissanen 
fisher information stochastic complexity ieee trans 
information theory vol 
pp rissanen 
mdl denoising ieee trans 
information theory vol 
nr 
november 
rissanen 
strong optimality normalized ml models universal codes information data ieee trans 
information theory vol 
nr 
july 
shannon 
mathematical theory communication bell system technical journal shtarkov yu 

universal sequential coding single messages translated problems information transmission vol 
july september 
solomonoff 
formal theory inductive inference part information control part ii information control takeuchi jun ichi barron andrew 
robustly minimax codes universal data compression st symposium information theory applications japan december 
vereshchagin vitanyi 
kolmogorov structure functions application foundation model selection personal communication vovk 
aggregating strategies proceedings rd annual workshop compu tational learning theory morgan kauffman pp 
weinberger rissanen feder 
universal finite memory source ieee trans 
information theory vol 
pp may white 
estimation inference specification analysis cambridge university press cambridge uk pages wiener 
cybernetics edition second edition revisions addi tional chapters mit press wiley new york 
yamanishi kenji decision theoretic extension stochastic complexity application learning ieee trans 
information theory vol 
july 

