machine learning kluwer academic publishers boston manufactured netherlands learning quickly irrelevant attributes abound new linear threshold algorithm nick littlestone saturn ucsc edu department computer information sciences university california santa cruz ca 
received september revised december keywords learning examples prediction incremental learning mistake bounds learning boolean functions linear threshold algorithms 
valiant studied problem learning various classes boolean functions examples 
discuss incremental learning functions 
consider setting learner responds example current hypothesis 
learner updates hypothesis necessary correct classification example 
natural measure quality learning setting number mistakes learner 
suitable classes functions learning algorithms available bounded number mistakes bound independent number examples seen learner 
algorithm learns disjunctive boolean functions variants learning classes boolean functions 
basic method expressed linear threshold algorithm 
primary advantage algorithm number mistakes grows logarithmically number irrelevant attributes examples 
time algorithm computationally efficient time space 

consider learning examples situation goal learner simply mistakes 
task induce concept described boolean function information received example list boolean attributes correct response boolean function attributes 
interested cases correct response function depends small proportion attributes example 
example case may occur pattern recognition tasks feature detectors may extract large number features learner consideration knowing prove useful 
example consider environment learner littlestone builds new concepts boolean functions old concepts banerji valiant 
learner may need sift large library available concepts find suitable ones expressing new concept 
special case situation may design library concepts specifically ease learning certain class complex functions 
case chooses concepts library allow representation function class simple function library concepts 
context concepts library just boolean functions 
example consider dnf class boolean functions represented disjunctive normal form literals term valiant 
available intermediate concepts include conjunctions literals dnf function represented simple disjunction concepts 
return idea presenting algorithm learning dnf 
main result algorithm deals efficiently large numbers irrelevant attributes 
desired implemented neural net framework rumelhart mcclelland simple algorithm 
method learns certain classes functions computed layer linear threshold network include functions disjunctions conjunctions threshold functions kearns li pitt valiant 
functions true designated variables true 
preprocessing techniques extend algorithm classes boolean functions linearly separable dnf fixed 
algorithm applied dnf formulas terms significantly fewer mistakes algorithm valiant 
algorithm similar classical perceptron algorithms uses multiplicative weight update scheme permits better classical perceptron training algorithms attributes irrelevant 
study learning line setting 
mean separate set training examples 
learner attempts predict appropriate response example starting example received 
making prediction learner told prediction correct uses information improve hypothesis 
learner continues learn long receives examples continues examine information receives effort improve hypothesis 
setting advantageous algorithm computes successive hypotheses incrementally saving required calculate hypothesis scratch stored input examples 
algorithm incremental sense 
learning quickly evaluate algorithm learning behavior counting worstcase number mistakes learning function specified class functions 
consider computational complexity 
prove mistake bound algorithm constant factor optimal algorithm applied certain classes functions 
method computationally time space efficient 
algorithm discuss properties mistake bounds concept classes including general lower bounds 
demonstrate close relationship exact identification equivalence queries angluin learning bounded number mistakes 
mistake bounds strong sense depend assumption examples learner sees order sees selection ordering done adversary 
due freedom adversary say early learner mistakes 
example single instance repeated arbitrarily times sequence trials followed instances learner know respond 
adapt mistake bounded algorithms criteria useful settings 
example consider setting learning process separated phases training phase subsequent working phase 
learning occurs training phase mistakes counted working phase 
important hypothesis formed learner training phase 
useful model context probabilistic model introduced valiant discussed blumer ehrenfeucht haussler warmuth angluin 
starting mistake bounded algorithm derive algorithm criteria probabilistic model 
mention indirect way angluin results 
kearns li pitt valiant mentioned related technique 
change learning model involves keeping line setting analyzing probabilistic worst case assumptions 
probabilistic model mentioned 
haussler littlestone warmuth discuss related model developed particularly setting 
interesting compare main algorithm similar classical methods perceptron training 
empirical evidence classical perceptron algorithm number mistakes grows linearly number irrelevant attributes 
keeping theoretical bounds perceptron convergence theorem littlestone duda hart nilsson 
know evidence standard perceptron algorithm better 
contrast prove number mistakes algorithm grows logarithmically number irrelevant attributes 
looked problem dealing efficiently irrelevant attributes context learning boolean functions 
haussler mentions algorithms learning disjunctive functions context valiant learning model 
designed learn rapidly presence irrelevant attributes 
algorithm naturally incremental significantly time space efficient line setting 
valiant introduces mechanism friendly knowledgeable teacher help learner indicating attributes relevant 
addition study classical perceptron algorithms experimented new algorithms conditional probabilities effort reduce cost irrelevant attributes 
theoretical bounds algorithms 
mistake counting model essentially model discussed barzdin 
see angluin smith survey compares number learning models 

setting section describe detail learning environment consider classes functions algorithm learn 
assume learning takes place sequence trials 
order events trial follows learner receives information world corresponding single example 
information consists values boolean attributes remains fixed 
think information received point call point instance call instance space 
learner response 
learner choice responses labeled 
call response learner prediction correct value 
learner told response correct 
information called reinforcement 
trial begins previous trial ended 
assume entire sequence trials single function maps instance correct response instance 
call function target function target concept 
learning quickly call algorithm learning setting algorithm line learning examples 
speak learning algorithms qualification refer algorithms line learning examples 
restrict attention deterministic algorithms 
mistake bounds worst case bounds class possible target functions call target class 

nature absolute mistake bounds section give general results mistake bounds online learning examples 
upper lower bounds number mistakes case ignores issues computational efficiency 
instance space finite space target class assumed collection functions domain range 
results apply infinite provided target class remains finite 
computability issues may arise case consider 
learning algorithm target function ma maximum possible sequences instances number mistakes algorithm target function 
learning algorithm non empty target class ma define ma empty 
number greater equal ma called mistake bound algorithm applied class definition optimal mistake bound target class denoted opt minimum learning algorithms ma 
minimum taken algorithms regardless computational efficiency 
algorithm called optimal class ma opt 
opt represents best possible worst case mistake bound algorithm learning computational resources issue straightforward learning algorithm excellent mistake bounds classes functions 
algorithm uses idea repeated halving set plausible hypotheses 
idea appears various forms barzdin mitchell angluin 
restate current context gives upper limit mistake bound suggests strategies explore searching computationally efficient algorithms 
algorithms describe general algorithms functioning depends knowledge particular target class 
algorithm ma denote ma told target class littlestone algorithm halving algorithm halving algorithm applied finite class functions values 
maintains list functions class agree target function past instances 
call functions list consistent functions 
terminology mitchell consistent functions form current version space algorithm 
initially list contains functions class 
respond new instance algorithm computes values consistent functions new instance prediction agrees majority possibility case tie 
trial algorithm updates list consistent functions 
give second description halving algorithm introduce notation 
target class point associated instance space denote subset containing functions denote functions halving algorithm maintains variable consist value set containing functions consistent past instances 
initially consist halving algorithm receives instance determines sets consist consist 
consist consist algorithm predicts predicts 
algorithm receives reinforcement sets consist accordingly correct response sets con consist sets consist consist 
denote maximum number mistakes algorithm run target class initial list functions consists target function fact comes theorem non empty target class 
proof possible predictions learner able choose prediction agreed half current list consistent functions 
mistake occurs functions agree prediction learner eliminated list consistent functions functions constitute half list 
mistake size list divided 
assumed target function initial class functions consistent function 
method log mistakes 
theorem holds modified version halving algorithm consist changed trials mistakes occur 
proof applies case 
halving algorithm immediately gives theorem learning quickly theorem finite target class opt 
example note classes functions bound tight 
example gx function 
easily verify halving algorithm applied class functions gx xe mistake 
study opt closely 
need definitions 
definition mistake tree target class instance space binary tree nodes non empty subset internal nodes labeled point satisfies root tree 
internal node labeled left child right child 
example shows mistake tree consists functions fi xi 
complete mistake tree mistake tree complete binary tree height define height tree length edges longest path root 
tree complete mistake tree 
trees provide way characterize number mistakes optimal learning algorithm 
optimal algorithm discuss number mistakes 
non empty finite target class equal largest integer exists complete mistake tree definition mistake trees guarantees finite upper bound 
algorithm standard optimal algorithm standard optimal algorithm similar halving algorithm 
maintains variable consist manner halving algorithm examines consist consist determine prediction 
difference halving algorithm lies rule uses choose prediction 
predicting sets functions larger compares consist consist 
consist consist algorithm responds responds 
mistake occurs remaining consistent functions smaller maximal complete mistake tree 
littlestone 
complete mistake tree 
theorem instance space 
soa denote standard optimal algorithm defined finite class functions domain range 
prove theorem lemmas lemma target class proof follows trivially definition empty 
assume non empty 
saying opt equivalent saying deterministic learning algorithm exists function sequence instances mistakes sequence instances 
algorithm show adversary choose function sequence instances mistakes 
adversary keeps track current mistake tree 
initially complete mistake tree lemma follows trivially 
instance chosen adversary label root tree 
algorithm predicts adversary tells algorithm prediction wrong 
response adversary eliminates functions possible target functions 
remaining candidate functions learning quickly class class depending algorithm prediction adversary response 
subtrees root adversary current mistake tree complete mistake tree remaining candidate functions 
adversary sets current mistake tree subtree 
chooses instance label root new current tree 
adversary continues manner forcing algorithm wrong instance 
mistakes adversary current tree complete mistake tree remaining candidate target functions 
long root current tree children corresponding non empty subclasses adversary choose point label root force algorithm mistake 
mistakes desired 
target function chosen adversary candidate remaining mistake 
lemma finite non empty target class 
suppose soa run learn function sequence instances receives xt 
consider variable consist maintained soa 
denote value consist start trial soa mistakes trials proof prove induction base case 
construction soa target function 
contain target function 
functions instance differ label root complete mistake tree 
definition ensures soa respond correctly contains target function 
proves base case induction 
prove lemma arbitrary assuming holds 
soa mistakes trials done 
number trial trials soa mistake 
complete mistake trees xj xj combine complete mistake tree add root node labeled xj 
easy transform complete mistake tree 
assumed exist complete mistake tree 
xj xj response soa corresponded larger values soa wrong property induction hypothesis soa littlestone table 
values functions example 
mistakes trials gives desired result 
proof theorem set lemma get 
lemma states opt 
definition opt opt 
theorem follows 
consequences theorem opt description soa obtain algorithm 
note example shows arbitrarily large target classes opt 
construct target class point target class point instance standard optimal algorithm halving algorithm different predictions consider example target class halving algorithm optimal 
example instance space element set 
target class consist functions values shown table 
instances received halving algorithm order target function halving algorithm mistakes 
version halving algorithm chooses case tie halving algorithm mistakes target function 
hand sequence points target function soa mistakes 
see considering point instance space turn 
opt learning quickly opt 
matter instance soa mistake prediction chosen remaining consistent functions optimal mistake bound 
halving algorithm optimal target class 
give lower bound opt terms vapnik chervonenkis vapnik chervonenkis dimension combinatorial parameter proven useful studies learning vapnik blumer haussler littlestone warmuth define vapnik chervonenkis dimension notion shattered set 
definition set shattered target class exists function definition vapnik chervonenkis dimension non empty target class cardinality largest set shattered denote dim 
define dim 
theorem target class dim opt 
proof dim 
choose set vk shattered construct complete mistake tree internal nodes depth labeled vj 
nodes chosen subclasses required definition mistake tree 
subclasses non empty required definition virtue fact vk shattered vapnik chervonenkis dimension prove useful lower bound opt concept classes consider sections 
concept classes vapnik chervonenkis dimension weak lower bound 
fact example shows opt arbitrarily large classes dim 
example take 
function fj fj 
dim opt see note 
set size shattered dim 
theorem opt log see opt construct complete mistake tree 
label root point 

subclasses similar original class half large 
vapnik vapnik chervonenkis dimension called capacity 
littlestone easy see points labels children root split subclasses exactly 
line reasoning formalized yield inductive construction mistake tree 

general transformations close relationship learning algorithms type considering exactly identify target function bounded number equivalence queries described angluin 
equivalence query request algorithm asks target function matches function described query 
algorithm receives negative answer equivalence query receives counterexample point target function proposed function disagree 
equivalence query algorithms consider receive examples input counterexamples queries 
section term query algorithm refer algorithm learns equivalence queries terms line learning algorithm mistake bounded algorithm algorithm learning examples refer algorithms type discussed 
describe relationship equivalence query algorithms model define notion current hypothesis algorithm line learning examples 
current hypothesis defined initially trials function instance space 
value instance defined response algorithm give trial instance received trial defined deterministic algorithm 
copy state algorithm trial copy state determine simulating algorithm prediction algorithm new instance sending instance running version algorithm 
state considered representation current hypothesis algorithm 
portion state suffice 
representation represent functions appearing queries algorithm learns examples transformed query algorithm 
show number queries needed number mistakes learning examples algorithm note angluin results queries restricted functions target class question 
conversion functions queries allowed come class functions original algorithm uses hypotheses 
note transformation functions learning quickly inverse transformation possible query algorithm transformed algorithm learns examples making bounded number mistakes 
efficiency transformed algorithm depend difficulty evaluating functions queries 
number mistakes transformed algorithm bounded number queries query algorithm 
give details transformations 
algorithm transformation mistake bounded learning algorithm transformation yields query algorithm target class 
query derived algorithm initial hypothesis algorithm algorithm waits response query repeats response response subsequent query response indicates query specified correct target function algorithm halts reports correct target function response latest query includes counterexample 
derived algorithm gives instance algorithm receiving prediction tells prediction incorrect 
algorithm knows wrong query just current hypothesis definition current hypothesis tells respond instance 
algorithm takes new hypothesis algorithm uses query continuing fashion determines correct target function 
query results mistake theorem theorem number queries needed derived algorithm exactly identify target function bounded ma 
algorithm transformation suppose query algorithm achieves exact identification function target class bounded number queries 
transformation yields mistake bounded learning algorithm target class 
initial hypothesis algorithm hypothesis output query algorithm initial query 
algorithm uses hypothesis respond instances received told mistake 
mistake algorithm receives response query 
time mistake algorithm gives algorithm response query tells hypothesis wrong reports queries necessarily compact symbolic representation 
query algorithm derived computationally efficient algorithm line learning examples query functions represented form efficiently evaluated 
littlestone instance mistake counterexample 
algorithm waits predictions query halts reports correct target function 
achieves exact identification events occur 
hypothesis pnew query reported target function new current hypothesis algorithm derived algorithm proceeds manner indefinitely 
theorem follows immediately 
theorem target function number mistakes derived algorithm learning bounded number queries needed algorithm exactly identify convert mistake bounded algorithm algorithm learns effectively probabilistic model introduced valiant described blumer 

angluin refers model pac learning pac stands probably approximately correct 
way perform conversion essentially follows method discussed kearns li pitt valiant failure bounds derive probabilistic learning results 
alternatively indirect route convert mistake bounded algorithm algorithm exact identification equivalence queries conversion described angluin obtain algorithm probabilistic setting 
general algorithm transformations possible 
useful algorithm changes hypothesis mistake occurs haussler referred methods conservative 
transform mistake bounded algorithm conservative algorithm mistake bound 
haussler referred methods failure bounded 
way convert mistake bounded algorithm conservative algorithm transformations convert equivalence query algorithm back mistake bounded algorithm 
mistake bound increases theorems transformations applied stand 
careful analysis double conversion increase disappears 
conversion conservative algorithm straightforward perform directly 

linear threshold algorithm describe main algorithm describing classes target functions 
consider linearly separable boolean functions functions computed layer linear threshold network perceptron 
function said learning quickly linearly separable hyperplane rn separating points function points 
monotone disjunctions constitute class linearly separable functions 
definition monotone disjunction disjunction literal appears negated function form hyperplane xi xik separating hyperplane xn xi xik 
variants algorithm 
variant specialized learning monotone disjunctions 
describe simple transformation remove monotone restriction 
algorithm call algorithm winnow designed efficiency separating relevant irrelevant attributes 
algorithm linear threshold algorithm 
instance space algorithm maintains non negative real valued weights wi wn having initial value 
algorithm real number call threshold 
learner receives instance xn learner responds follows predicts predicts choice prediction critical results 
weights changed learner mistake weights corresponding non zero xi changed 
amount weights changed depends fixed parameter 
bounds obtained set set 
say values 
table describes changes weights response different combinations prediction reinforcement 
threshold left fixed 
note table type update action name mistake corresponds single promotion step single elimination step 
space needed counting bits weight sequential time needed trial clearly linear note non zero weights powers prove weights 
logarithms base weights stored littlestone learner prediction table 
response mistakes 
correct response update action wi xi wi unchanged xi wl wi xi wi unchanged xi update name elimination step promotion step log loga bits weight needed 
running time needed calculate predictions changes weights reduced greatly parallel implementation appropriately constructed neural net 
mistake bound give theorem 
theorem suppose target function literal monotone disjunction xn xi xik 
run sequence instances total number mistakes bounded ak loga example mistake bound log 
set bound simplifies ak gives bound 
dominating term minimized bound log ek log log prove theorem finding bounds number promotion elimination steps occur 
give lemmas proof 
number promotion steps occurred sequence trials number elimination steps occurred sequence trials 
lemma proof consider sum en wi changes time 
initially sum promotion elimination steps cause change 
promotion steps increases sum promotion step occurs ei xi wi elimination step decreases en wi 
sum negative giving desired result 
learning quickly lemma wi 
proof weights initially equal 
value wj increased trial xj 
conditions occur wi immediately prior promotion 
wi promotion 
lemma promotion steps arbitrary number elimination steps exists loga wi proof ik 
look product wi changed elimination promotion steps 
note xi xn xi elimination steps occur xi xn promotion steps occur xi xn 
iie wi unchanged elimination steps increased factor promotion step 
initially wi 
promotion steps wi au giving sa wi logq wi desired 
note lemmas depends form target function appears 
proof theorem total number mistakes run algorithm equal number promotion steps plus number elimination steps bound lemmas lemma bound combining lemmas see lemma gives adding bounds leads desired bound total number mistakes 
note algorithm depend algorithm learn entire target class monotone disjunctions modification 
mistake bound depends number literals actual target concept 
ck denote class literal monotone disjunctions ck denote class monotone disjunctions literals 
suppose wants specialize littlestone algorithm learn target class ck efficiently particular ko 
chooses mistake bound target function literal monotone disjunction 
gives bound ko log 
obtain bound og 
give lower bound number mistakes needed learn ck ck 
theorem lower bound opt ck opt ck log opt ck log 
second form gives formula directly comparable upper bound 
algorithm specialized particular ck algorithm constant factor optimal 
proof ck ck clear opt ck opt ck 
theorem algorithm learn concept class mistake bound equal vapnik chervonenkis dimension concept class 
lemma show vapnik chervonenkis dimension ck bounded log 
gives part theorem 
split derivation second formula cases depending 
desired 
assumption equal giving desired result 
prove general lemma needed give results useful 
note literal monotone conjunctions just term monotone dnf formulas 
literal monotone disjunctions term monotone dnf formulas 
learning quickly lemma class functions expressible term monotone dnf formulas integer 
dim kl log note particular take take proof log theorem follows trivially non empty 
assume 

note ms construct set containing klr points shattered describe construction need refer enumeration ways choose distinct integers set 
kji kjk runs enumeration 
values kji chosen integers 
construct union sets sji set sji contains points point coordinates 
split coordinates groups disjoint groups coordinates 
may coordinates left part group 
number groups fix attention coordinates point sji coordinates groups numbered kji kjk 
coordinates groups kji kjk group kji 
coordinates group kji distinguish points set sji 
set coordinates group manner subset sji corresponding coordinate group kji points points sji possible subsets sji coordinates group 
example suppose 
take get 
picking enumeration ways choose integers take sets sji shown table 
show construct term dnf formula exactly arbitrary subset terms formula length exactly literal negated 
sji 
express formula terms variables variable corresponding coordinate 
gives groups variables corresponding groups coordinates 
jth term contain variable groups kji kjk 
choose variable jth term group kji points points sji 
possible due way sets sji constructed 
see formula consider point point sji coordinates point groups kj kjk 
term littlestone table 
example sets 
sn jth contain variable formula jth term 
coordinates groups kji kjk possibly group ky 
variables jth term possibly ith variable 
value formula match value ith variable jth term 
variable uij sji desired 
algorithm modified larger classes boolean functions 
instance space satisfying class functions property exist 
un xn words inverse images linearly separable minimum separation depends 
second variant winnow handle target classes form 
learning quickly learner prediction table 
response mistakes 
correct response update act wi wi xi wi unchanged xi wi wi xi wi unchanged xi update name demotion step promotion step mistake bound derive practical functions sufficiently large 
example include boolean threshold functions 
threshold function xn defined selecting set significant variables 
value variables 
selected variables xi xik exactly xi xik equivalently value selected variables 
case threshold functions contained 
exist classes linearly separable boolean functions grows exponentially instance space 
example set functions exponentially small consists varies 
functions mistake bound derive grows exponentially give description second variant winnow 
algorithm winnow change winnow involves amount weights changed mistake 
promotion step multiply weights fixed 
setting weights zero elimination step divide 
call demotion step 
careful choice littlestone mistake bound derive learning target function 
table describes responses different types mistakes 
space time requirements winnow similar win 
bits needed store weight logarithm mistake bound 
theorem gives mistake bound winnow 
theorem target function class ui un chosen target function satisfies inequalities algorithm run algorithm receives instances number mistakes bounded proving theorem state prove lemmas analogous lemmas prove theorem 
define manner lemmas 
current lemmas depend particular choice theorem 
lemma proof examine weights changed promotion demotion steps 
wi bef denote weights trial promotion demotion occurs wi aft denote weights resulting promotion demotion 
promotion step write update rule promotion step occurs promotion step 
demotion step learning quickly demotion step occurs wi 
initially sum weights promotions weights negative 
giving au desired 
lemma wi ao 
proof weights initially equal ao 
value wj increased trial xj 
conditions occur wj immediately prior promotion 
wj promotion 
lemma promotion steps elimination steps exists proof symbols wi bef wi aft proof lemma 
time look happens en ui log wi 
write promotion update rule logarithm multiplying ui get promotion step occurs en 
promotion step littlestone demotion step demotion step occur demotion step initially en ui log wi 
promotion steps demotion steps ui non negative get dividing ui gives desired result 
proof theorem lemmas get ui non negative rewrite inequality second inequality involving results lemma eliminate expression 
gives value theorem get learning quickly combining inequalities involving get taylor formula remainder get get assumed lemma bound total number mistakes simplify upper bound get desired 
littlestone earlier example involving threshold functions hx get mistake bound threshold functions kr know lower bounds concept class comparable upper bound 
note threshold functions just literal monotone disjunctions 
winnow learn monotone disjunctions 
mistake bound similar bound larger constants 

transformations target classes various transformations possible apply algorithms classes functions 
think transformations letting derive new learning algorithm existing 
transformations describe take form mappings applied instances predictions 
instance space derived algorithm original algorithm transformations take form functions ti tp 
take tp identity function interchanges negation tp invertible 
derived algorithm receives instance sends instance ti original algorithm generates prediction derived algorithm generates prediction tp 
conclude trial reinforcement received derived algorithm sends original algorithm 
reinforcement passed transformation view message saying right wrong message containing value correct response 
suppose start original algorithm want derive algorithm learn target class seek target class learned mappings ti tp exists tp ti theorem 
theorem suppose transformation ti invertible transformation tp original algorithm accept instances derived algorithm constructed described 
suppose wish algorithm learn target function 
function learned bounded number mistakes tp ti algorithm learn making ma mistakes 
proof prediction derived algorithm response instance algorithm prediction algorithm prediction tp response learning quickly instance ti 
tp ti 
algorithm told mistake derived algorithm mistake 
see happens exactly response instance ti equal ti 
happen ma times 
transformations similar effect substitutions described kearns li pitts valiant 
consider examples ways transformations extend classes functions learnable winnow 
example show transformation satisfies condition theorem desired target function exists function target class learned winnow winnow tp ti note case winnow 
example learning arbitrary disjunctions 
example way learn disjunctions necessarily monotone 
arbitrary disjunctions special cases classes discussed examples 
learner send instances 
learner just responds mistake 
extra mistake counted bound 
learner starts transformations defined follows 
suppose zn instance mistake 
ti function addition modulo 
tp identity 
construct function theorem write target function zn zi 
desired 
mistake bound learning non monotone disjunctions method corresponding mistake bound monotone disjunctions 
littlestone example learning literal monotone conjunctions 
ti xn xn tp thinks false true transformations ti tp just negate arguments 
target function xn xi xik conjunction variables xn xi xik tp ti de morgan law 
win number mistakes bounded log 
example learning linearly separable boolean functions weights vary sign 
class functions exist vn depending xn give transformation learn demonstrate linearly separable boolean function domain 
learn functions winnow transformation ti tp identity 
function find function ti tp ti satisfying condition theorem 
define vn vn 
hi vi mi vi 
function fj ixi desired function 
mistake bound theorem applies replaced sum pi vi vi show linearly separable boolean function 
see observe function identically take vi vi take linearly separable boolean function identically 
find un xn learning quickly allow ui vary sign 
choose ui mi ii 
divide inequalities ui note identically obtain inequalities xn example learning dnf fixed transformation demonstrates learn functions linearly separable 
class dnf consists functions expressed littlestone disjunctive normal form literals term 
valiant kearns 
studied class 
learn dnf 
ci xn range conjunctions form valid terms dnf formula conjunctions literals 
tp identity 
dnf target function terms exist il xn disjunction ci xn cil xn 
defined 
yn 
ti desired 
show 
appear function learned literal monotone disjunction 
target concept terms log nk kl log mistakes 
contrast algorithm learning dnf similar classes valiant forced mistakes roughly nk mistakes small 
lemma gives lower bound vapnik chervonenkis dimension class term dnf formulas 
lower bound mistake bound 
lower bound take required 
lower bound mistake bound case know run number mistakes derived algorithm bounded fixed similar form lower bound 

divides parts 
part contains general results mistakes effective learner computational complexity issue 
second portion describes efficient algorithm learning specific target classes 
general results learning quickly part lead inequalities vcdim opt log non empty target class examples section demonstrate vcdim classes opt large classes log large 
example shows opt differ 
exist classes vcdim log making inequalities equalities 
true class contains exactly concepts required shatter particular finite subset domain 
turn efficient algorithms find win transformations certain concept classes 
include disjunctions conjunctions threshold functions classes linearly separable functions sufficiently large separation classes non linearly separable functions dnf fixed small results contrast kearns 
demonstrated np learner required choose hypotheses target class threshold functions polynomially learnable valiant learning model 
methods mentioned section winnow learns threshold functions converted polynomial learning algorithm valiant model 
algorithm succeeds efficiently learning threshold functions choosing hypotheses larger class boolean functions 
winnow winnow natural algorithms parallel implementation implemented winnow connection machine 
key advantage winnow performance attributes relevant 
define number relevant variables needed express function class number strictly positive weights needed describe separating hyperplane bounds winnow tell target class learned number mistakes bounded constant times fcl target function expressed relevant variables 
follows bound theorem observation inequalities definition fa larger set changing function 
note target class winnow achieve bound necessarily producing hypothesis expressed significant weights 
example attributes match instance weights match hypothesis making significant 
littlestone theme recurs transformation algorithm 
discussed transformations kinds including transformations algorithms target class algorithms target class transformations mistake bounded algorithms query algorithms transformations mistake bounded algorithms algorithms provably perform probabilistic learning model transformations arbitrary mistake bounded algorithms normalized mistake bounded algorithms transformation conservative algorithm 
transformations hope improving behavior algorithm target class capable learning 
regard notice monotone conjunctions learned winnow transformation described example 
literal monotone conjunction just threshold function 
winnow learn threshold function mistake bound applies transformation example winnow learn derived threshold function 
case mistake bound better factor bound transformation 
clear extent difference artifact analysis 
note express threshold function set weights long weight relevant variable threshold sum weights threshold 
tighter constraints weights represent threshold function 
sum weights omitting single relevant variable threshold weights relevant variables sum threshold 
suggests oi threshold function harder winnow learn threshold function 
shown winnow constant factor optimal classes functions ratio mistake bound optimum grows shrinks 
number linearly separable boolean functions attributes blumer 
halving algorithm mistakes learning function 
bound winnow grows proportion exist classes grows exponentially efficient algorithms close gap 
learning quickly advantage winnow perform functions relevant attributes needing know number relevant attributes advance 
true respect separation parameter affects choice multiplier winnow 
practical problems useful version winnow function needing know 
mentioned mistake bounded algorithm transformed algorithm provably performs probabilistic learning model 
run mistake bounded algorithm transformation assume instances chosen randomly examine behavior probabilistic terms 
interesting understand behavior winnow setting 
input data learner contains errors robust weight mistakenly set zero mistake undone 
winnow learn concept classes learnable robust 
currently studying performance errors input data 
research supported contract office naval research 
acknowledge inspiration provided valuable discussions david haussler manfred warmuth key questions asked 
benefited discussions sally floyd ron rivest dana angluin eli upfal 
dick karp raised question lower bounds learning monotone disjunctions larry stockmeyer manfred warmuth subsequently developed ideas relating lower bounds 
mike paterson manfred warmuth suggested improvements upper bounds 
ideas leading complete mistake trees worked conjunction david haussler 
angluin 

queries concept learning 
machine learning 
angluin smith 

inductive inference theory methods 
computing surveys 
banerji 

logic learning basis pattern recognition improvement performance 
advances computers 
barzdin 

prediction general recursive functions 
soviet mathematics doklady 
littlestone blumer ehrenfeucht haussler warmuth 

learnability vapnik chervonenkis dimension technical report crl 
santa cruz university california computer research laboratory 
blumer ehrenfeucht haussler warmuth 

occam razor 
information processing letters 
duda hart 

pattern classification scene analysis 
new york john wiley 


linear function neurons structure training 
biological cybernetics 
haussler 

space efficient learning algorithms 
unpublished manuscript university california department computer information sciences santa cruz 
haussler 

quantifying inductive bias concept learning 
proceedings fifth national conference artificial intelligence pp 

philadelphia pa morgan kaufmann 
haussler littlestone warmuth 

predicting functions randomly drawn points 
unpublished manuscript university california department computer information sciences santa cruz 
kearns li pitt valiant 

learnability boolean formulae 
proceedings nineteenth annual acm symposium theory computing pp 

new york association computing machinery 
kearns li pitt valiant 

results boolean concept learning 
proceedings fourth international workshop machine learning pp 

irvine ca morgan kaufmann 
mitchell 

generalization search 
artificial intelligence 


threshold logic applications 
new york john wiley 
nilsson 

learning machines 
new york mcgraw hill 
rumelhart mcclelland 

parallel distributed processing explorations microstructure cognition 
cambridge ma mit press 


programmer guide connection machine technical report 
new haven ct yale university department computer science 
valiant 

theory learnable 
communications acm 
valiant 

learning disjunctions conjunctions 
proceedings ninth international joint conference artificial intelligence pp 

los angeles ca morgan kaufmann 
vapnik 

estimation dependencies empirical data 
new york springer verlag 
vapnik chervonenkis 

uniform convergence relative frequencies events probabilities 
theory probability applications 
