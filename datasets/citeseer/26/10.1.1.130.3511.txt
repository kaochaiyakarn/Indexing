ieee transactions pattern analysis machine intelligence vol 
january pairwise data clustering deterministic annealing thomas hofmann student member joachim buhmann member partitioning data set extracting hidden structure data arises different application areas pattern recognition speech image processing 
pairwise data clustering combinatorial optimization method data grouping extracts hidden structure proximity data 
describe deterministic annealing approach pairwise clustering shares robustness properties maximum entropy inference 
resulting gibbs probability distributions estimated mean field approximation 
new structure preserving algorithm cluster dissimilarity data simultaneously embed data euclidian vector space discussed dimensionality reduction data visualization 
suggested embedding algorithm outperforms conventional approaches implemented analyze dissimilarity data protein analysis linguistics 
algorithm pairwise data clustering segment textured images 
index terms please leave spaces dash delete text 
information communication technology confronts massive amounts data 
primary goal pattern recognition extract hidden structure data order generate compact data representation enable symbolic data processing concepts 
basic problems pattern recognition concerned detection clusters data sets 
potential applications clustering algorithms cover wide range data compression video audio signals structure detection automatic inference engines machine learning artificial intelligence 
describe stochastic optimization approach data clustering relies known robustness maximum entropy inference 
problem optimally partitioning data set arises different forms dependent data representation vectorial proximity data 
partitioning approach known central clustering derives set prototype vectors quantize set vectorial data minimal quantization error :10.1.1.116.3824
data compression achieved transmission storage indices vectors original data vectors 
second approach data clustering referred pairwise data clustering partitions set data clusters data indirectly characterized pairwise comparisons explicit coordinates 
character authors friedrich wilhelms universitat institut fir iii bonn germany 
mail th jb uni de 
manuscript received june revised sept 
recommended tance 
information obtaining reprints article please send mail computer org ieeecs log number 

data clustering viewed partitioning problem density estimation problem mixture model sense statistics 
ieee data set hidden pairwise relations proximity values frequently violate require ments distance measure triangular inequality necessarily hold self dissimilarity may vanish proximity values negative 
grouping proximity data mathematically formulated combinatorial optimization problem solve minimization heuristic called deterministic annealing 
sets relational data abundant applications molecular biology psychology linguistics econom ics image processing 
data clustering problem pattern recognition statistics belongs class unsupervised learning problems 
large body literature available topic reader referred text books duda hart jain dubes view 
method deterministic annealing described various papers literature neural networks lo computer vision 
deterministic annealing applied central clustering discussed rose series papers 
solution deterministic annealing proce dure vector quantization different rate constraints suggested zo 
chou emphasized design question large code book chosen dependent prespecified costs code vector 
remainder structured follow ing way discuss advantage maximum entropy search heuristic section 
discussion cost functions central pairwise data clustering pre sented section 
approximation techniques calculate expectation values data assignments discussed section 
widely estimation technique called mean field approximation derived variational tech niques alternatively expansion small extension pairwise data clustering data ieee transactions pattern analysis machine intelligence vol 
january visualization described section 
results central clustering section employed simultaneously group embed proximity data low dimensional euclidian space 
simulation results clustering problems molecular biology linguistics performance com parison deterministic annealing conven tional gradient descend technique clustering application pairwise data clustering image seg mentation summarized section 
stochastic optimization entropy inference simulated annealing seminal papers kirkpatrick proposed stochastic optimization strategy simulated annealing :10.1.1.123.7607
analogy experimental annealing procedure stability metal glass improved heating cooling solutions zation problem heated cooled simulations find low costs 
search solutions implemented markov process stochastically samples solution space optimization problem 
optimization problem characterized cost func tion sz denoting admissible solution optimization problem 
new solution accepted rejected metropolis algorithm new solutions decreased costs accepted solutions increased costs accepted expo weighted probability ah 
parameter called computational temperature 
philosophy simu lated annealing gradually reduce temperature search process forcing system solutions low costs 
mathematically stochastic search process optimal solution random walk solution space 
cost differences neighboring states act force field 
effect temperature interpreted random force amplitude pro valleys peaks cost difference smeared vanish stochastic search 
markov process transition matrix converges equilibrium probability distribution exp known gibbs distribution 
quantity denotes gibbs free energy 
temperature formally plays role la parameter enforce constraint expected costs wsa 
gibbs free energy related expected costs entropy 
wta deterministic annealing stochastic search markov process transition matrix allows estimate expectation values system parameters computing time averages monte carlo simulation variables optimiza gb tion problem drawn 
random sequential sampling solution space slow compared deterministic optimization techniques due diffusive nature search process 
deterministic variant simulated annealing deterministic annealing analytically estimates relevant expectation values system parameters variables optimization problem 
introduce generalized free energy fir ts logp wta minimized tractable subspace probability distributions 
inequality holds gibbs distribution maximizes entropy kept fixed 
search space probability densities defined order analytically approximate expectation values optimization parameters 
temperature family generalized free energies increasing complexity high temperature smoothes cost function low temperature reveals full complexity original optimization problem recovering 
deterministic annealing algorithms track solutions high low temperature similar way cooling simulated annealing 
discuss technique detail section 
consider stochastic deterministic search strategy principles statistical physics 
fundamental relationship statistical physics robust statistics established jaynes postulated principle maximum entropy inference 
maximizing entropy yields biased inference method maximally respect missing data 
context data clustering missing information assignments data clusters 
important argument favor maximum entropy method stresses robustness inference technique 
proven maximum en tropy probability distribution maximally stable terms norm expected cost lowered raised changes temperature 
family gibbs distributions cost function possesses optimality property induce variations reduced 
concepts differential geometry family gibbs distributions parameterized temperature hofmann buhmann pairwise data clustering deterministic annealing forms trajectory space probability distributions minimal length 
conclude facts stochastic search heuristic starts large noise level gradually reduces stochasticity zero family gibbs distributions decreasing temperature 
strategy guarantees maximal robustness respect noise 
cost functions data clustering central clustering widely nonparametric technique finding data prototypes central clustering vector quantization 
set dimensional data vectors nj central clustering poses problem deter mining optimal set dimensional vectors prototypes yv kij 
specify data partition introduce boolean assignment variables configuration space mi data point assigned vector yv mi 
solution space defined set admissible configurations constraints cv lm vi assuring data point repre sented unique vector quality set vectors assessed objective function central clustering sums average distortion error data vector corre sponding vector yv appropriate distortion measure yv depends application domain common choice squared euclidian distance yv iix data vector vector 
applications topological ordering vectors coding demand distortion measure considers topological organization vectors xl yv specifies probability index confused index due transmission noise 
distortions low dimensional topological arrangement defining chain dimensional grid popular self organizing topological maps area neural computing 
number clusters limited additional rate dis constraints shannon entropy penalties small clusters postulating fixed number 
stochastic optimization cost function requires determine probability distribution assignments maximum entropy principle originally suggested rose central clustering states distributed gibbs distribution pointed section free energy interpreted smoothed version original cost function factor exp normalizes exponential weights exp fl 
inverse rewritten mcm sum assignments constrained 
cost function fl linear yields factorized gibbs distribution predefined vectors yj 
gibbs distri bution interpreted complete data hood mixture models parameters rose optimal vectors derived maximizing entropy gibbs distribution keeping average costs fl fixed gibbs distribution assignments set fixed vectors 
determine closed equations optimal vectors differentiate argument expected costs kept constant 
resulting equation ieee transactions analysis machine intelligence vol 
january noted 
clus stresses cluster coherency 
alternative clustering costs proposed wide spread acceptance pattern recognition applications 
constant term cf ell subtracted emphasize independence clustering cost function absolute dissimilarity scale 
uni form shift dissimilarity values offset change clustering costs consequently influence statistics assignments 
cost function offset invariance displays tendency extremely heterogeneous partitionings large small clusters 
second important property proposed cost function concerns non symmetric dissimilarities 
changed dissimilarities replaced arithmetic mean dk 
reasons simplicity henceforth assume symmetric furthermore invariant arbitrary permutation cluster indices dv important ignored consideration stochastic optimization problems scaling values number data 
correct scaling yield constant costs data point achieve independence annealing schedules stochastic search heuristics instance size case completely consistent dissimilarities data different clusters large data clusters small constant costs datum require scaling 
opposite case random dissimilarity values averaging effects necessitate 
thorough discussion point statistical phys ics literature optimization problems 
mean field approximation pairwise clustering strategy stochastic optimization dis cussed section central clustering estimate expectation values assignment data clusters specified uncertainty level parametrized computa tional temperature assignments data clusters randomly drawn set admissible configurations gibbs distribution exp hpc exp hpc fl costs pairwise clustering solution 
contrary gibbs distribution central clustering data assignments pairwise clustering statistically dependent gibbs distribution exactly rewritten factorized form 
assignment variable interacts assignment variables 
cost contributions converge averages 
permutation symmetry removed adding small rv cost function 
hn perturbations favor indexing clusters sue 
px 
hofmann buhmann pairwise data clustering deterministic annealing limit large data sets reduce influence cor relations individual data assignments 
approximate average interaction variables mean field 
sections variational technique perturbation expansion derive mean field approximation cor assignment correlations 
method restricted clustering problems applied combinatorial optimization problems 
mean field approximation minimization kl divergence mean field approximation gibbs distribution neglects correlations stochastic variables pairwise clustering cost function determines similar factorized distribution parametrized family distributions po 
distribution po represents accurately statistics original problem specified minimum kullback leibler divergence original gibbs distribution arg min po 
pairwise clustering case define approxi mating family distributions introducing potentials ll effective interactions represents partial costs assigning datum cluster summing partial costs arrive fam ily cost functions correlations ho mi 
linearity assignments reflects fact assume statistical independence assignments distributed factorized gibbs distribution po 
equivalent minimization condition free en ergy derived algebraic transformations pcb ho llp fl fo men averaging brackets 
denote average respect 
positive vanishes obtain wellknown upper bound derived iy 
summary optimal mean fields result variational approach minimizing upper bound free energy minimizing kl divergence 
upper bound interpreted generalized free energy defined re space factorized probability distributions minimization upper bound free energy yields optimal potentials el assigning datum cluster iv qv vv resulting optimal respect assignments technical details appendix reader note potentials el depend variables mil mik 
introduce approximation neglects terms order simplify potentials el 
approxi mi correct limit large simplify presentation assume zero self dissimilarities dii vi 
simplified optimal potentials depend distance matrix averaged assignment variables cluster probabilities 
algorithm estimates assignment probabilities mi optimal potentials defined iteratively 
algorithm initialize randomly temperature re peat step estimate mi function el step calculate el ieee transactions pattern analysis machine intelligence vol 
january ttt satisfy refined mean field approach known tap approach models feedback effects strongly dis ordered clustering instances faithfully naive approach 
refined expected assignments qr algorithm decreases temperature exponentially alternates estimation data assignments potentials step estimate potentials assignments 
estimation procedure ap kp ix carried sequentially parallel 
pap step potentials step 
sequential version step step performed randomly selected datum converges local minimum respect efl 
upper bound free energy uniquely determined miv til explicit dependency 
upper bound plays role lyapunov function update dy potentials sl 
sequential update scheme implemented clustering experi ments see section 
outer loop algorithm re duces temperature exponential fashion choose qt 
choices linear annealing schedules lower temperature yield superior optimization results search process extended slower temperature reduction 
equations expected data assignments variational approach family factorized distri butions implicitly assumes correlations neglected 
direct estimate aver age assignments allows check assump tion holds estimation errors introduced underlying independence hypothesis 
detailed deriva tions equations summarized appendix true expected assignments defined 
fraction exp implements partition unity system equations computationally intractable carry averaging partition unity exponential number assign ment configurations 
smoothness transition cell partition neighboring cell controlled inverse temperature naively interchanging averaging brackets nonlinear function yields equations 
experimentally observed oscillation parallel update known parallel update neural networks 
mi sap denotes kronecker delta 
corrections ila called cavity fields 
question range validity subtle studied extensively statistical physics disordered systems 
empirically measure average values assignments monte carlo simula tion 
estimates inserted yields residual errors difference right left side equations 
residual errors determine quality tap approximation compared naive mean field approximation 
monte carlo experiments matrices gaussian dis tributed random dissimilarity values tap equations estimate average assignments mj reduced residual error compared naive mean field approximation 
difference reaches maximum temperatures near phase transition point degenerate clusters split separate clusters 
naive mean field equation superior low temperature range 
furthermore observed improvements achieved tap equations neg lected small problems asymptotics 
pairwise clustering embedding grouping data clusters important concept dis covering structure 
apart partitioning data classes data analyst relies visual inspection data recognize correlations deviations ran 
task embedding dissimilarity data dimensional euclidian space prerequisite visual inspection known multidimensional scaling 
usually multidimensional scaling formulated op timization problem coordinates costs called stress function pds introduced kruskal 
pds constant normalization measures absolute stress penalizes relative stress 
hofmann buhmann pairwise data clustering deterministic annealing mean field approximation pairwise clustering central clustering section establish connection clus tering multidimensional scaling problem 
strat egy combining data clustering data embedding euclidian space variational approach maximum entropy estimation discussed section 
coordinates data points embedding space estimated way statistics resulting cluster structure matches statistics original pair wise clustering solution 
relation new principle structure preserving data embedding standard scaling summarized diagram gpc mi kl multidimensional scaling offers left path dissimilarities coordinates advocate right path 
variational approach mean field approximation involved right path requires specify parametrized family factorized gibbs distributions 
choose factorized gibbs distributions cost function central clustering fl embedding coordinates xj variational parameters 
approach motivated identity yields correct approximation pairwise cluster ing instances dik xk suppose stationary solution mean field equations 
clustering problem suffices consider mean assignments miv parameters auxiliary variables 
identity allows interpret variables squared distance cluster centroid assumption euclidian data 
multidimensional scaling problem coordinates unknown quantities 
restrict potentials form iix centroid definition specified new family approximating distributions defined parameters ni 
effective dimensionality pa rameter space min significant reduction especially case low dimensional embeddings 
criterion determining embedding coordinates pc hi approximately yields coordinates details derivation summarized appendix coordinates xi yv determined iteratively solving algorithm algorithm structure preserving mds initialize xi yf mi randomly temperature repeat step estimate miv function yt step repeat calculate pi yv update vf fulfill centroid condition convergence ttt convergence understand properties algorithm key idea deriving mean field ap proximation 
statistics approximating system cost function fl optimally adjusted statistics original system 
fact implies able determine variational parameters limit fixed statistics limit zero temperature 
easily seen equations singular asymptotic results require apply rule 
derived system transcendental equations quadratic distortions centroid condition explicitly reflects dependencies clustering procedure euclidian representation 
si solution equations leads efficient algorithm interleaves multidimensional scaling process clustering process avoids arti separation uncorrelated data processing steps 
results demonstrate properties proposed clustering algorithms classes experiments benchmark optimization experiments compare deterministic annealing greedy gradient descent method linkage algorithm pairwise clustering section ii simultaneous pairwise clustering embedding performed artificial real world data section iii pairwise clustering segmentation technique textured images discussed section 
ieee transactions pattern analysis machine intelligence vol 
january fig 

histograms clustering costs protein data set random clustering instance 
gray white bins denote results optimization deterministic annealing gradient de scent gly mean average dissimilarity linkage solution costs 
benchmark experiments deterministic annealing theoretical derivations deterministic annealing algorithms motivated known robustness properties maximum entropy inference 
test claim large number randomly initialized clustering experiments performed dissimilarities taken protein sequences ii dissimilarities randomly drawn uniform distribution 
dissimilarity values pairs protein sequences determined sequence alignment program takes biochemical structural information account 
essence alignment program measures number amino acids exchanged transform sequence second 
sequences belong different protein families hemoglobin globins 
protein dissimilarities sorted clustering solution clusters displayed fig 

cases dissimilarities protein sequence comparisons random dissimilarities span spectrum ordered random clustering instances 
benchmark clustering experiments designed validate claim superior clustering results achieved deterministic annealing compared standard clustering techniques gradient descent 
histograms clustering runs different initializations summarized fig 
protein dissimilarity data random data 
deterministic annealing clearly outperformed conventional gradient descent method random case worst deterministic annealing solution better best gradient descent solution 
case protein data average costs deterministic annealing solution best percent gradient descent solutions average deterministic annealing solution better best gradient descent solutions 
standard mean average dissimilarity linkage algorithms known wards method see sec 
yields clustering result costs wc compared best experimentally achieved result wc 
experiments support claim deterministic annealing yields substantially better solutions comparable computing time 
fig 

embedding dimensional data dimension pro data principle components cluster preserving embedding algorithm ii 
data shown 
fig 

similarity matrix protein sequences globin family dark gray levels correspond high similarity values 
clustering embedding dimensions clustering mds embeddings global local stress minimization 
clustering embedding results properties described algorithm simultaneous euclidian embedding data clustering illustrated different experiments clustering dimension reduction distributed data 
clustering real world proximity data protein sequences 
capacity finding low dimensional representa tions high dimensional data demonstrated data set drawn mixture gaussians dimensions 
centers gaussians randomly distributed unit sphere 
covariance matrices diagonal values randomly drawn set 
best linear projection principal component analysis shown fig 

positions data points denoted letters name respective mix ture component 
linear projection methods pro pursuit yield comparable results direc tion distinguished data generation procedure 
si clustering embedding algorithm distributes data dimension approximately group structure high dimensional space 
cluster preserved separated 
data points assigned wrong cluster 
algo hofmann buhmann pairwise data clustering deterministic annealing rithm selects representation completely unsupervised fashion preserves essential structure grouping formation topology data set 
procedure dimension reduction weakly related idea principal curves principal surfaces 
fig 
summarizes clustering result real world data set protein sequences 
families protein sequences abbreviated capital letters 
gray level visualization dissimilarity matrix dark values similar protein sequences shows formation dis squares main diagonal 
squares cor respond discovered partition clustering resulting clustering costs 
embedding dimensions fig 
shows intercluster distances agreement similarity values data 
best experimentally determined solution fl embedding constraint exceeded quality solution fig 
percent 
results consistent biological classification 
labels hall fig 
individual globin sequences known play outlier role globin family 
corrections cavity fields range percent assignment costs monte carlo simulation 
compared clustering solutions algorithm re sults step procedure embed data kruskal multidimensional scaling criterion cluster embedded data em procedure algorithm depending embedding criterion ab fig 
relative fig 
stress visualizations protein dissimilarities reveal little clus ter structure 
fact reflected high clustering costs embedding guided absolute relative stress respectively 
obvious figs 
simultaneous clustering embedding structure preserving mds algorithm preserves charac original cluster structure better classical mds techniques subsequent central clustering 
application pairwise clustering linguistic data set shown fig 
word fragments compared dynamic programming algorithm 
dissimilarity matrix visualized left side 
dark gray values denote high similarity values 
matrix ordered determined clustering solution eleven clusters 
word fragments similar high likelihood grouped seen labels fig 

cor cavity fields percent range 
fig 

similarity matrix data set word fragments 
calculated clustering solution embedding dimensions 
labels denote groups common word beginnings 
unsupervised texture segmentation pairwise clustering segmenting digital image homogenous regions regions constant slowly varying intensity constant color uniform texture arises fundamental problem image processing 
geman formulate texture segmentation grouping problem constraints valid region shapes 
grouping problem pairwise dissimilarities texture patches correspond pixel blocks image 
major modifications compared introduced dissimilarities calculated gabor wavelet scale space representation 
normalized pairwise clustering cost function objective function image tion 
deterministic annealing algorithm re places monte carlo method proposed 
calculation dissimilarity matrices textured images separated stages 
stage image transformed gabor wavelet representation 
gabor transformation possesses bandpass characteristic known display texture discrimination properties 
orientations different scales separated full octave resulting feature images raw gray scale image io second step empirical feature distribution function calculated separately fea ture image image block blocks centered regular grid overlap 
third stage pairs empirical distri bution functions belonging feature image compared kolmogorov smirnov distance 
pair blocks bi defined stage procedure set inde calculated dissimilarity matrices gener ated combined simple maximum rule 
reminiscent julesz theory texture perception dissimilarity single feature channel sufficient discriminate textures 
procedure generating dissimilarity data images schematically summarized fig 

cumulative histograms ieee transactions pattern analysis machine intelligence vol 
january fig 

texture segmentation pairwise clustering local properties image patches intensity differences local frequencies extracted 
respective empirical feature distribution functions compared kolmogorov smirnov statistics yield dissimilarity values image blocks 
fig 

image size different textures segmented pairwise data clustering 
segmentation result deterministic pairwise clustering shown 
segmentation errors displayed black pixels located segment ries 
applied algorithm randomly composed texture images depicted fig 

resulting segmentation deterministic annealing algorithm pairwise clustering fig 
shows different textures discriminated 
difference image fig 
resulting segmentation ground truth demonstrates incorrect assignments observed border regions statistics belonging different textures mixed 
corrections cavity field terms range percent changes assignments 
segmentation additional penalties thin regions suggested enforce local texture consistency prevent large fragmentation texture regions 
small fraction dissimilarities calculated blocks processed including pairs adjoined blocks small random neighborhood 
details neighborhood selection adapted mean field approach sparse clustering performance statistics large number textured images 
discussion problem grouping data regarded initial fundamental steps information proc data analysis 
concepts artificial intelligence pattern recognition signal processing dependent robust reliable data clustering principles robustness mandatory respect noisy events 
developed maximum entropy framework pairwise data clustering 
known approximation scheme statistical physics mean field approximation derived different ways variational method minimizes kullback leibler divergence original gibbs distribution data assignments parametrized family factorized distributions expectation values data assignments calculated direct fashion 
technique allows correct influence small fluctuations data assignments 
variational approxi mation pairwise clustering central clustering yields structure preserving multidimensional scaling algorithm simultaneously clusters data embeds euclidian space 
algorithm non linear dimension reduction visualization purposes 
re sults pairwise data clustering algorithms analyzing protein linguistic data segmenting textured im ages reported 
benchmark clustering experi ments support claim deterministic annealing yields substantially better results conventional clustering concepts gradient descent minimization 
lined strategy analyzing stochastic algorithms pair wise clustering considered general program deriving robust optimization algorithms maximum entropy principle analogous re sults metric multidimensional scaling problem hierarchical data clustering problem re ported 
acknowledgments pleasure providing protein data linguistic data respectively 
puzicha segmentation experiments 
mds experiments 
supported federal ministry education science bmbf 
hofmann buhmann pairwise data clustering deterministic annealing appendix appendix appendix derive mean field equations pairwise data clustering problem minimizing upper bound free energy respect variational parameters gte derivatives upper appendix derive mean field equations data assignments fluctuation corrections case strongly disordered clustering problems 
dissimilarities bound free energy yields scale alternative derivation necessary identities inserting derivatives gives necessary condition minimum upper bound free energy variational approach section cap ture fluctuations adequately known statis tical physics 
data assignments considered randomly drawn set ble configurations gibbs distribution fl see 
expected assignment datum cluster denotes set assignments mj 
partial summation admissible states mi 

carried analytically 
step separation bution mi costs related clustering costs flc term summation admissible states mt yields mi exp cpc fa equations fulfilled values simultaneously exp ev cl vv ci arbitrary constants 
defined 
summary expected assignments ieee transactions pattern analysis machine intelligence vol 
january term linear capture fluctuation contributions 
comparison coefficients yields equation known markov blanket identity inserting partial derivatives dividing analogous equation ising spins ory magnetic systems see section 
taylor expansion small fluctuations aev qv ev renders closed system equations ex depending averaged assignments 
yields av mk expected assignments wa fa fa fa ev va jr vu fa exp ea exp 
terms ing second order terms expansion receive closed system transcendental equations expected assignments xp tj aa fi 
assumption holds terms vanish 
depth discussion assump tion valid 
assuming validity refined mean field equations derivation tacitly assumes assignment correlation function scales exp ii mm mkp lpt mip fork gap mkp fluctuations data assignments consistent dis dic mlp 
similarities averaged limit view central limit theorem 
case random dissimilarities fluctuations vanish large captured quadratic terms taylor expansion 
introduce effective internal field simulates indirect influence disorder data assignments see anderson palmer 
loss generality dissimilarity values assumed vanishing expected values 
scaling dissimilarities assumed 
shift dissimilarity values random nature allows neglect second term ansatz expected assignments effective internal field hofmann buhmann pairwise data clustering deterministic annealing appendix chain rule yields derivatives upper bound respect variational parameters ka ya derivatives setting equal zero results exact stationary conditions left hand side reduced expression explicit yy covariance ma trix yv 
note exist implicit dependencies depends :10.1.1.123.7607
derivatives ax right hand side exactly calculated solutions linear equation system unknowns 
reduce computational complexity perform approximation assumption hl treating yp independent variable 
equation simplifies vector equation jaynes information theory statistical mechanics physical review vol 
pp 

jaynes information theory statistical mechanics physical review vol 
pp 
jaynes rationale maximum entropy methods proc ieee vol 
pp 

jain dubes algorithms clustering data 
englewood cliffs nj prentice hall 
mclachlan basford mixture models 
new york basel marcel dekker 
gray vector quantization ieee acoustics speech signal processing pp 
apr 
gersho gray vector quantization signal processing 
boston kluwer academic publisher 
duda hart pattern classification scene analysis 
new york wiley 
statistical mechanics underlying theory elastic neural optimizations network vol 
pp 

constrained nets graph matching quadratic assignment problems neural computation vol 
pp 
yuille stolorz statistical physics mixtures distributions em algorithm neural computation vol 
pp 
gold rangarajan graduated assignment algorithm graph matching ieee trans pattern analysis machine intelligence vol 
pp 

geiger girosi parallel deterministic algorithms mrfs surface reconstruction ieee trans pattern analysis machine intelligence vol 
pp 
may 
yuille generalized deformable models statistical physics matching problems neural computation vol 
pp 

bregler omohundro surface learning applications lipreading advances neural information processing systems vol 
cowan tesauro alspector eds 
rose gurewitz fox statistical mechanics phase transitions clustering physical review letters vol 
pp 
rose gurewitz fox deterministic annealing approach clustering pattern recognition letters vol 
pp 

rose gurewitz fox vector quantization deterministic annealing ieee trans information theory vol 
pp 

rose gurewitz fox constrained clustering optimization method ieee trans pattern analysis machine intelligence vol 
pp 

buhmann complexity optimized data clustering competitive neural networks neural computation vol 
pp 

buhmann vector quantization complexity costs ieee trans information theory vol 
pp 
july 
chou gray entropy constrained vector quantization ieee trans acoustics speech signal processing vol 
pp 

kirkpatrick gelatt vecchi optimization simulated annealing science vol 
pp 
approach traveling salesman problem efficient simulation algorithm optimization theory applications vol 
pp 
gardiner handbook stochastic methods 
berlin springer 
tishby levine alternative approach maximum entropy inference physical review vol 
pp 

csisz divergence geometry probability distributions minimization problems annals probability vol 
pp 
kohonen self organization associative memory 
berlin springer 
ieee transactions pattern analysis machine intelligence vol 
january ritter martinetz schulten neural computation self organizing maps 
new york addison wesley 
cover thomas elements theory 
new york john wiley sons 
dempster laird rubin maximum hood incomplete data em algorithm 
royal sta tistical society ser 
methodological vol 
pp 

hofmann puzicha buhmann unsupervised seg mentation textured images pairwise data clustering technical report iai tr friedrich wilhelms universitat bonn institut fur informatik feb 
parisi spin theory 
singapore world scientific 
minimum property free energy cal review vol 

anderson palmer solution solvable model spin glass philosophical magazine vol 
joachim buhmann received phd degree theoretical physics technical univer sity munich 
held postdoctoral positions university southern california lawrence livermore national 
currently works associate professor computer science university bonn germany heads re search group computer vision pattern recognition 
current research interests cover theory neural networks applica tions image understanding signal processing 
special research topics include data clustering data visualization active data selec tion stochastic optimization techniques video compression sensor fusion autonomous robots 

thomas hofmann received degree sammon jr non linear mapping data structure computer science university analysis ieee trans 
computers vol 
pp 

bonn germany 
october kruskal nonmetric multidimensional scaling numerical joined computer vision pattern rec method psychometrika vol 
pp 

group university bonn huber projection pursuit annals statistics vol 
pp 
currently completing phd thesis 
deterministic annealing algorithms hastie stuetzle principal curves american tory data analysis 
research interests cu ass vol 
pp 

clude computer vision neural networks jordan representing words connectionist graphical models machine learning models th ann 
meeting society wash autonomous robots 
dc 
geman geman dong boundary detection constrained optimization ieee trans pattern analysis machine intelligence vol 
pp 
july 
fogel sagi gabor filters texture discriminators biological cybernetics vol 
pp 
daugman uncertainty relation resolution space spatial frequency orientation optimized dimensional visual cortical filters optical society america vol 
pp 

julesz visual pattern discrimination ire transactions information theory pp 
feb 
hofmann puzicha buhmann unsupervised segmentation textured images pairwise data clustering proc 
int conf 
image processing lausanne 
hofmann buhmann hierarchical clustering structures deterministic annealing proc 
knowledge discovery data mining conf portland 
parisi statistical field theory 
redwood city calif addison wesley 
