learning rank maximizing auc linear programming nick street member ieee yi zhang area roc curve auc evaluate ranking performance binary classification problems 
researchers approached auc optimization approximating equivalent mann whitney wmw statistic 
linear programming approach similar norm support vector machines svms instance ranking approximation wmw statistic 
formulation applied nonlinear problems kernel function 
ranking algorithm outperforms svms auc classification performance rbf kernels curiously polynomial kernels 
experiment variations chunking handle quadratic growth number constraints formulation 
motivation real world classification problems may require ordering simple classification 
example direct mail marketing marketer may want know potential customers target sending new catalogs maximize revenue 
number catalogs marketer sending limited know buy product receiving catalog 
case useful distinguish top percent potential customers yield highest likelihood buying product 
doing marketer intelligently target subset customer base increasing expected profit 
slightly different example collaborative book recommendations 
intelligently recommend book algorithm distinguish similarities book preferences people read submitted score book 
combining scores possible return recommendation list user 
ideally list ranked way book top list expected appeal user 
hard see slight difference ranking problems 
marketing problem ranking binary input purchase non purchase book recommendation problem ranking collection partial rankings 
consider problem ranking binary input 
roc curves area roc curve auc ranking popular machine learning problem addressed extensively literature 
department management sciences university iowa iowa city ia usa phone fax email uiowa edu 
nick street department management sciences university iowa iowa city ia usa phone fax email nick street uiowa edu yi zhang department management sciences university iowa iowa city ia usa phone fax email yi zhang uiowa edu commonly performance measure evaluate classifier ability rank instances binary classification problems area roc curve 
roc curves widely visualization comparison performance binary classifiers 
roc curves originally signal detection theory 
introduced machine learning community showed roc curves evaluation comparison classification algorithms 
main reason roc curves mainstream performance evaluation tool fact produced curve independent class distribution underlying misclassification costs 
roc curves plot true positive rate vs false positive rate varying threshold usually probability membership class distance decision surface simply score produced decision function 
roc space upper left corner represents perfect classification diagonal line represents random classification 
point roc space lies upper left point represents better classifier 
classifiers neural networks naive bayes naturally provide probabilities classifiers decision trees 
possible produce roc curves type classifier minor adjustments 
area roc curve auc single scalar value classifier comparison 
statistically speaking auc classifier probability classifier rank randomly chosen positive instance higher randomly chosen negative instance 
auc probability value varies represents positives ranked higher negatives 
larger auc values indicate better classifier performance full range possible thresholds 
possible classifier high auc outperformed lower auc classifier region roc space general high auc classifier better average underlying distribution known 
mainstream classifiers usually designed minimize classification error may perform applied ranking problems 
support vector machines svms exceptions 
svms optimized error minimization nature margin maximization property turn 
shown accuracy best performance measure compare classifier performance especially datasets skewed class cost distributions real world problems 
machine learning community explored question performance measure better general 
cortes mohri investigated relationships auc accuracy 
ling huang showed study auc statistically consistent discriminating value accuracy 
research indicates increase popularity auc performance measure especially cases class distributions misclassification costs unknown 
relations wilcoxon mann whitney statistic assume dataset random variables positive points produced random variables negatives produced 
pairwise comparison positive point compared negative point case auc ratio number correct pairwise orderings vs number possible pairs 
quantity called wilcoxon mann whitney wmw statistic xi yj xi yj pn xi yj number positive points number negative points xi th positive point yj th negative point 
function assigns score point ranking points 
related literature point intuitive algorithm maximizing wmw statistic maximize auc ranking performance 
wmw statistic continuous function making difficult maximize papers suggested approximation approach wmw statistic 
yan continuous function approximation wmw statistic 
doing able gradient methods solve optimization problem 
approximation case rank optimization handled carefully information related ranking may lost easily process approximation 
research instance ranking utilized scenarios kind user feedback available initial rankings 
type ranking usually important typical ranking search engine results ranking products user interfere algorithm obtain valuable feedback user 
feedback form subject evaluating initial rankings reporting mis ranked points simply returning partial ranking subset points ranked 
note binary classification problems address prior intermediate knowledge intra class rankings obtain feedback correctness rankings level algorithm 
problem auc maximization context binary classification similar problem ordinal regression 
literature includes algorithms freund utilize multi stage approach 
algorithm works combining preferences known boosting approach 
variation introduced rudin 
considered state art algorithm sake fairness multi stage algorithm opposed shot optimization algorithm compare results multi stage methods 
believe improve performance lp ranker 
plan compare ranking performance multistage rank optimizing algorithms 
compare performance svms 
algorithm quite similar svms formulation specifically setup maximize ranking performance opposed minimizing error rate 
literature common practice rank new examples output classifier 
classifier methods svms considered believe initially comparing algorithm svms provide valuable feedback improvement get ranking algorithm setting formulation rank optimization problem 
rank optimizing svms came focus 
effort investigate rank optimizing kernels context svms led formulation produced comparatively inferior results regular svm formulations 
led investigations area 
introduced similar kernel formulation led better ranking performance compared previous 
algorithm regular quadratic optimization objective kernel structure improved ranking performance 
discussions conclude rank optimization problem minimize number pairwise 
achieve objective formulation produces scores instance scores rank data points 
algorithm able handle nonlinear problems applied wide range problems 
achieve help kernel functions 
algorithm robust size dataset 
formulation introduces exact speed approaches improve solving time algorithm part progress 
initial results discussed promising 
section introduces lp formulation implements properties mentioned 
results discussions sum 
ii 
algorithm instance training set data vector class label instance 
refer set positive points data set negatives optimize wmw statistic positive points ranked higher negative points xi xj xi xj scoring function 
perfect separation classes impossible real world datasets 
lp constructed minimize error term corresponding number incorrect orderings 
construct linear program soft constraints penalizing negatives ranked positives 
requires number positive points times number negative points constraints total 
formulation avoids combinatorial complexity minimizing error difference scores incorrectly ordered pairs 
select scoring function assigns real valued score data point making optimization problem continuous 
leads set constraints xi xj zij xi xj 
small quantity usually set added right hand side avoid strict inequalities constraints 
scoring function similar svm classification function intercept 
intercept necessary add amount points scoring function influence relative scores ranking 
scoring function defined yi ik xi represents weight point training set 
function 
kernel function induce nonlinearity linear constraints 
objective function minimize error magnitude coefficients 
minimize maximize separation margin similar svms 
proposed objective function lp obtained follows substituting scoring function rearranging left side inequality min ce yi xi xi unit vector tradeoff parameter parts objective represents error term positive negative pair 
experiments rbf polynomial kernels form xi xj xi xj xi xj xixj rbf parameter controls smoothness decision surface polynomial parameter controls degree complexity 
output lp gives optimal set weights instances non zero weights call ranking vectors represent unique points influence ranking 
kernel mapping input space feature space defined kernel function 
possible rank data points mapped space 
tested algorithm rbf kernel polynomial kernels order 
optimal weights point training set scoring function yi xi obtain scores test point 
rank test points scores obtained 
algorithm differs directly solve primal problem nonlinear kernel function 
major advantage primal formulation requires resources solve problem 
needs compute size kernel matrix cover constraints dual formulation needs compute size number positive points times negative points kernel matrix 
number variables primal form dual form 
addition primal form provides flexibility picking objective function 
case norm error terms weights objective function problem linear program 
addition algorithm produces ranking vectors score test points ranking pairs algorithm 
practical applications ranking vectors provide intuitive explanation ranking pairs 
rbf kernel approach similar distance weighted nearest neighbor knn algorithm equal number ranking vectors 
consider scoring function distance weighted knn ranking vectors training points yi distance xi inverse rbf distance function distance xi xj xi xj scoring function different coefficient obtained optimization procedure approach 
linear programming formulation includes pairwise constraints presents challenge number constraints grow quadratic number data points 
implement chunking approach speed solving time optimization problem 
variations experimented explained section 
general overview complete algorithm shown 
create possible constraints divide manageable chunks ci initialize empty constraint set size converged chunk constraints ci solve constraints ci obtaining evaluate ci pairwise violations add violated constraints fig 

algorithm outline iii 
results discussion experiments data sets uci repository 
multi class data sets converted binary classification problems class vs scheme 
details conversions table compared results regular norm svm rbf polynomial kernels 
implemented algorithm matlab cplex optimization matlab environment 
performance comparisons svm sequential minimal optimization smo algorithm implemented weka 
constructed grid search find best rbf parameter settings ranker lp svm 
best results algorithms obtained 
polynomial kernel comparisons degree polynomial 
datasets averaged fold cross validation results 
investigate effects rank optimization classification performance included accuracy results algorithms 
believe nature optimization ranks positive points negatives favorable effect accuracy process enforces separation classes 
obtain accuracy algorithm find threshold score gives best accuracy value training set 
threshold test set classify test points 
table ii shows ranking algorithm performed better general svm ranking accuracy performance rbf kernel 
row table shows statistical comparisons form win loss tie significance set level 
ranker optimized ranking significantly better auc performance datasets worse 
accuracies compared ranker won times lost twice 
got similar results parameter settings include 
experimented polynomial kernels algorithm results obtained far promising 
results polynomial kernel table iii 
case significant win versus significant losses 
currently investigating possible reasons 
intuitively choice kernels performance data dependent case classification 
evaluations rbf kernel performed better datasets polynomial kernel performed poorly cases 
light results believe rbf kernels better suited ranking problems polynomial kernels 
fig 

ranking vs support vectors linearly separable case fig 

ranking vs support vectors linearly non separable case observe ranking vectors quite different support vectors 
created artificial data sets observe differences 
figures show different twodimensional problems problem requires linear decision surface requires nonlinear surface 
table overview datasets modification details datasets points attributes rare class class comments boston rest ecoli pp rest glass rest heart sonar cancer ionosphere haberman liver bupa cancer table ii auc accuracy results rbf kernel fold cross validation 
auc accuracy datasets lp ranker svm lp ranker svm boston ecoli glass heart sonar ionosphere haberman liver table iii auc accuracy results polynomial kernel fold cross validation 
auc accuracy datasets lp ranker svm lp ranker svm boston ecoli glass heart sonar ionosphere haberman liver data figures rbf kernel svm ranker lp 
support vectors expected points appear close boundary ranking vectors usually positioned extremity class dimensional problems 
seen ranking vectors necessary optimally rank points 
ranking vectors usually fewer number influential points ranking data points 
figures illustrate power rbf kernels applicability linear problems nonlinear ones 
remaining challenge quadratic expansion number constraints increasing number data points 
ideally number constraints needed obtain real solution equal number positive points times number negatives 
number grows quickly number data points increases making linear program solving time unreasonably long 
tried speed tricks related chunking reduce number constraints removing potentially redundant ones 
note schemes implement obtain exact solution problem 
chunking divides data manageable bins optimizes separately problem solved reasonable time 
solving chunk points influential points nonzero added bin algorithm iterates dataset 
approach formulations 
reason case non zero represent ranking vectors 
far away decision boundary general necessarily relevant chunks regular chunking 
experimented variations chunking 
scheme chunk evaluated weights remaining data points chunk add set previously violated constraints constraints represent pair wise violations 
case observed majority violated constraints accumulating iterations showing gradual increase iterations expected 
second approach reduce effect sudden increase constraints chunk added top violated constraints lp set 
datasets 
case number passes number iterations convergence increased slowing lp solution time 
overview speed comparisons table iv 
second column shows total number possible constraints dataset 
columns show results approach columns show results second approach 
column labeled pass columns represents number full passes dataset number retained constraints converge 
implemented approaches able significantly reduce number constraints needed solve optimization problem datasets 
speed impressive thought 
investigating similar approaches reduce lp solution times 
researchers tackled problem approximation heuristic 
random sampling approach reducing number constraints 
suggested reducing number constraints optimization problem considering constraints created predefined number nearest positive neighbors negative point 
proposed clustering approach reduction constraints 
heuristics helped speed time obtain solution approximate methods performance heuristic vary greatly nature data 
iv 
introduced lp formulation optimize ranking performance maximizing approximation wmw statistic 
results show algorithm similar norm svms gives superior ranking performance compared regular norm svm rbf kernels 
performance degraded polynomial kernels 
tackled speed issues variations chunking approach able significantly improve lp solving times exact heuristics 
step intend look approximate heuristics speed lp times 
investigate loss performance polynomial kernels 
boser guyon vapnik 
training algorithm optimal margin classifiers 
haussler editor proceedings th annual acm workshop computational learning theory pages 
acm press 
bradley 
area roc curve evaluation machine learning algorithms 
pattern recognition 
scheffer 
auc maximizing support vector learning 
icml workshop roc analysis machine learning 
caruana baluja mitchell 
sort rankprop multitask learning medical risk evaluation 
touretzky mozer hasselmo editors advances neural information processing systems volume pages 
mit press 
cohen schapire singer 
learning order things 
michael jordan michael kearns sara solla editors advances neural information processing systems volume pages 
mit press 
cortes mohri 
auc optimization vs error rate minimization 
thrun saul scholkopf editors advances neural information processing systems 
mit press cambridge ma 
table iv comparison speed heuristics st speed approach nd speed approach datasets constraints pass constraints lp reduction pass constraints lp reduction boston ecoli glass heart sonar cancer ionosphere haberman liver bupa cancer domingos 
metacost general method making classifiers cost sensitive 
knowledge discovery data mining pages 
freund iyer schapire singer 
efficient boosting algorithm combining preferences 
journal machine learning research 
hanley mcneil 
meaning area receiver operating characteristic roc curve 
radiology 
raskutti 
optimising area roc curve gradient descent 
icml international conference machine learning 
acm press 
joachims 
optimizing search engines clickthrough data 
kdd proceedings eighth acm sigkdd international conference knowledge discovery data mining pages 
acm press 
ling huang zhang 
auc statistically consistent discriminating measure accuracy 
proceedings international joint conference artificial intelligence 
mann whitney 
test random variables stochastically larger 
annals mathematical 
john platt 
fast training support vector machines sequential minimal optimization 
burges smola editors advances kernel methods support vector learning 
mit press 
provost fawcett 
analysis visualization classifier performance comparison imprecise class cost distributions 
knowledge discovery data mining pages 
obermayer herbrich graepel 
large margin rank boundaries ordinal regression 
advances large margin classifiers pages 
mit press 
joachims 
query chains learning rank implicit feedback 
kdd proceedings acm conference knowledge discovery data mining 
acm press 

optimizing area roc curve svms 
roc analysis artificial intelligence pages 
rudin cortes mohri schapire 
margin ranking meets boosting middle 
th annual conference computational learning theory 

signal detection theory valuable tools evaluating inductive learning 
proceedings sixth international workshop machine learning pages 
morgan kaufman 
vogt cottrell 
optimize rankings 
technical report cs san diego cse department 
wilcoxon 
individual comparisons ranking methods 
biometrics 
witten frank 
data mining practical machine learning tools techniques java implementations 
morgan kaufmann 
yan mozer 
optimizing classifier performance approximation wilcoxon mann whitney statistic 
international conference machine learning pages 
