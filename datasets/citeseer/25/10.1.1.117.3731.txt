kluwer academic publishers boston 
manufactured netherlands 
tutorial support vector machines pattern recognition christopher burges burges lucent com bell laboratories lucent technologies editor usama fayyad 
tutorial starts overview concepts vc dimension structural risk minimization 
describe linear support vector machines svms separable non separable data working non trivial example detail 
describe mechanical analogy discuss svm solutions unique global 
describe training practically implemented discuss detail kernel mapping technique construct svm solutions nonlinear data 
support vector machines large nite vc dimension computing vc dimension homogeneous polynomial gaussian radial basis function kernels 
high vc dimension normally bode ill generalization performance exists theory shows generalization performance guaranteed svms arguments support observed high accuracy svms review 
results experiments inspired arguments 
examples proofs key theorems 
new material hope reader nd old material cast fresh light 
keywords support vector machines statistical learning theory vc dimension pattern recognition 
purpose provide introductory extensive tutorial basic ideas support vector machines svms 
books vapnik vapnik contain excellent descriptions svms leave room account purpose start teach 
subject said started late seventies vapnik receiving increasing attention time appears suitable introductory review 
tutorial dwells entirely pattern recognition problem 
ideas carry directly cases regression estimation linear operator inversion space constraints precluded exploration topics 
tutorial contains new material 
proofs versions placed strong emphasis clear self contained material accessible possible 
done expense elegance generality generality easily added basic ideas clear 
longer proofs collected appendix 
way motivation alert reader literature summarize applications extensions support vector machines 
pattern recognition case svms isolated handwritten digit recognition cortes vapnik scholkopf burges vapnik scholkopf burges vapnik burges scholkopf object recognition blanz speaker identi cation schmidt detection face detection images osuna freund girosi text categorization joachims :10.1.1.15.9362
regression estimation case svms compared benchmark time series prediction tests muller mukherjee osuna girosi boston housing problem drucker arti cial data pet operator inversion problem vapnik smola 
cases svm generalization performance error rates test sets matches signi cantly better competing methods 
svms density estimation weston anova decomposition stitson studied 
regarding extensions basic svms contain prior knowledge problem example large class svms image recognition problem give results pixels rst permuted randomly image su ering permutation act vandalism leave best performing neural networks severely handicapped done incorporating prior knowledge svms scholkopf burges vapnik scholkopf burges :10.1.1.40.9777
svms generalization performance slow test phase problem addressed burges osuna girosi 
generalized basic ideas smola scholkopf muller smola scholkopf shown connections regularization theory smola scholkopf muller girosi wahba shown svm ideas incorporated wide range algorithms scholkopf smola muller scholkopf :10.1.1.40.9777
reader may nd thesis scholkopf helpful 
problem drove initial development svms occurs guises bias variance tradeo geman bienenstock guyon tting montgomery peck basic idea 
roughly speaking learning task nite amount training data best generalization performance achieved right balance struck accuracy attained particular training set capacity machine ability machine learn training set error 
machine capacity photographic memory new tree concludes tree di erent number leaves seen little capacity lazy brother declares green tree 
generalize 
exploration formalization concepts resulted peaks theory statistical learning vapnik 
bold typeface indicate vector matrix quantities normal typeface vector matrix components scalars 
label components vectors matrices greek indices label vectors matrices roman indices 
familiarity lagrange multipliers solve problems equality inequality constraints assumed 
bound generalization performance pattern recognition learning machine remarkable family bounds governing relation capacity learning machine performance theory grew considerations circumstances quickly mean empirical quantity converges uniformly number data points increases true mean calculated nite amount vapnik 
start bounds 
notation largely follow vapnik 
suppose observations 
observation consists pair xi associated truth yi trusted source 
tree recognition problem xi pixel values image yi image contains tree simplify subsequent formulae 
assumed exists unknown probability distribution data drawn data assumed iid independently drawn identically distributed 
cumulative probability distributions densities 
note assumption general associating xed allows distribution case trusted source assign labels yi xed distribution conditional xi 
section assuming xed suppose machine task learn mapping xi 
yi 
machine de ned set possible mappings 
functions labeled adjustable parameters machine assumed deterministic input give output 
particular choice generates call trained machine 
example neural network xed architecture corresponding weights biases learning machine sense 
expectation test error trained machine jy note density exists dp may written dxdy 
nice way writing true mean error estimate useful 
quantity called expected risk just risk 
call actual risk emphasize quantity ultimately interested 
empirical risk remp de ned just measured mean error rate training set xed nite number observations remp lx xi note probability distribution appears 
remp xed number particular choice particular training set fxi 
quantity xi called loss 
case described take values 
choose 
losses values probability bound holds vapnik log log remp non negative integer called vapnik chervonenkis vc dimension measure notion capacity mentioned 
call right hand side eq 
risk bound 
depart previous nomenclature authors guyon call guaranteed risk misnomer really bound risk risk holds certain probability guaranteed 
second term right hand side called vc con dence 
note key points bound 
remarkably independent ofp 
assumes training data test data drawn independently 
second usually possible compute left hand side 
third know easily compute right hand side 
di erent learning machines recall learning machine just name family functions choosing xed su ciently small machine minimizes right hand side machine gives lowest upper bound actual risk 
gives principled method choosing learning machine task essential idea structural risk minimization see section 
xed family learning machines choose extent bound tight machines able better 
extent bound tight hope right hand side gives useful information learning machine minimizes actual risk 
bound tight chosen family learning machines gives critics justi able target re complaints 
case rely experiment judge 

vc dimension vc dimension property set functions ff generic set parameters speci es particular function de ned various classes function consider functions correspond class pattern recognition case set points labeled possible ways labeling member set ff correctly assigns labels say set points shattered set functions 
vc dimension set functions ff de ned maximum number training points shattered ff note vc dimension exists set points shattered general true set points shattered 

shattering points oriented hyperplanes suppose space data live isr set ff consists oriented straight lines line points side assigned class points side class 
orientation shown specifying side line points assigned label 
possible nd points shattered set functions possible nd 
vc dimension set oriented lines 
points shattered oriented lines 
consider hyperplanes theorem prove useful proof appendix theorem consider set points choose points origin 
points shattered oriented hyperplanes position vectors remaining points linearly independent corollary vc dimension set oriented hyperplanes choose points choose points origin position vectors remaining points linearly independent choose vectors linearly independent 
alternative proof corollary anthony biggs 

vc dimension number parameters vc dimension gives concreteness notion capacity set functions 
intuitively led expect learning machines parameters dimension learning machines parameters dimension 
striking counterexample due levin denker vapnik learning machine just parameter nite vc dimension family classi ers said nite vc dimension shatter points matter large 
de ne step function 
consider parameter family functions de ned sin choose number task nding points shattered 
xi specify yl yi gives labeling choose lx yi vc dimension machine nite 
interestingly shatter arbitrarily large number points nd just points shattered 
simply equally spaced assigned labels shown 
seen follows write phase choice label requires phase mod 
similarly point forces 
implies contrary assigned label 
points analogy set functions eq 
set points lying line oriented hyperplanes set shattered chosen family functions 
points shattered sin despite nite vc dimension 
vc confidence vc con dence monotonic vc dimension sample size 
minimizing bound minimizing shows second term right hand side eq 
varies con dence level assuming training sample size 
vc con dence monotonic increasing function true value selection learning machines empirical risk zero wants choose learning machine associated set functions minimal vc dimension 
lead better upper bound actual error 
general non zero empirical risk wants choose learning machine minimizes right 

note adopting strategy eq 
guide 
eq 
gives chosen probability upper bound actual risk 
prevent particular machine value empirical risk function set higher vc dimension having better performance 
fact example system gives performance despite having nite vc dimension section 
note graph shows vc con dence exceeds unity higher values bound guaranteed tight 

examples consider th nearest neighbour classi er 
set functions nite vc dimension zero empirical risk number points labeled arbitrarily successfully learned algorithm provided points opposite class lie right top 
bound provides information 
fact classi er nite vc dimension bound valid bound valid nearest neighbour classi ers perform 
rst example cautionary tale nite capacity guarantee poor performance 
follow time tradition understanding things trying break see come classi er supposed hold violates bound 
want left hand side eq 
large possible right hand side small possible 
want family classi ers gives worst possible actual risk zero empirical risk number training observations vc dimension easy compute bound non trivial 
example call notebook classi er 
classi er consists notebook room write classes training observations subsequent patterns classi er simply says patterns class 
suppose data positive negative examples samples chosen randomly 
notebook classi er zero empirical risk observations training error subsequent observations actual error vc dimension substituting values eq 
bound ln ln certainly met exp true monotonic increasing 

structural risk minimization summarize principle structural risk minimization srm vapnik 
note vc con dence term eq 
depends chosen class functions empirical risk actual risk depend particular function chosen training procedure 
nd subset chosen set functions risk bound subset minimized 
clearly arrange things vc dimension varies smoothly integer 
introduce structure dividing entire class functions nested subsets 
subset able compute 
srm consists nding subset functions minimizes bound actual risk 
done simply training series machines subset subset goal training simply minimize empirical risk 
takes trained machine series sum empirical risk vc con dence minimal 

nested subsets functions ordered vc dimension 
necessary exploration support vector machines 

linear support vector machines 
separable case start simplest case linear machines trained separable data shall see analysis general case nonlinear machines trained non separable data results similar quadratic programming problem 
label training data fxi yi xi suppose hyperplane separates positive negative examples separating hyperplane 
points lie hyperplane satisfy normal hyperplane jbj kwk perpendicular distance hyperplane origin kwk euclidean norm shortest distance separating hyperplane closest positive negative example 
de ne margin separating hyperplane 
linearly separable case support vector algorithm simply looks separating hyperplane largest margin 
formulated follows suppose training data satisfy constraints xi yi xi yi combined set inequalities yi xi consider points equality ineq 
holds requiring exists point equivalent choosing scale 
points lie hyperplane xi normal perpendicular distance origin bj kwk 
similarly equality ineq 
holds lie hyperplane xi normal perpendicular distance origin bj kwk 
kwk margin simply kwk 
note parallel normal training points fall 
nd pair hyperplanes gives maximum margin minimizing kwk subject constraints 
expect solution atypical dimensional case form shown 
training points equality eq 
holds wind lying hyperplanes removal change solution called support vectors indicated extra circles 
switch lagrangian formulation problem 
reasons doing 
rst constraints replaced constraints lagrange multipliers easier handle 
second reformulation problem training data appear actual training test algorithms form dot products vectors 
crucial property allow generalize procedure nonlinear case section 
introduce positive lagrange multipliers inequality constraints 
recall rule constraints form ci origin margin linear separating hyperplanes separable case 
support vectors circled 
constraint equations multiplied positive lagrange multipliers subtracted objective function form lagrangian 
equality constraints lagrange multipliers unconstrained 
gives lagrangian lp kwk lx iyi xi lx minimize lp respect simultaneously require derivatives lp respect vanish subject constraints call particular set constraints 
convex quadratic programming problem objective function convex points satisfy constraints form convex set linear constraint de nes convex set set simultaneous linear constraints de nes intersection convex sets convex set 
means equivalently solve dual problem maximize lp subject constraints gradient respect vanish subject constraints call particular set constraints 
particular dual formulation problem called wolfe dual fletcher 
property maximum lp subject constraints occurs values minimum lp subject constraints requiring gradient respect vanish give conditions iyi equality constraints dual formulation substitute eq 
give ld xj note lagrangian di erent labels primal dual emphasize formulations di erent lp ld arise objective function di erent constraints solution minimizing lp maximizing ld 
note formulate problem amounts requiring hyperplanes contain origin constraint appear 
mild restriction high dimensional spaces amounts reducing number degrees freedom 
support vector training separable linear case amounts maximizing ld respect subject constraints positivity ofthe solution 
notice lagrange multiplier training point 
solution points called support vectors lie hyperplanes 
training points lie eq 
holds side thatthe strict inequality eq 
holds 
machines support vectors critical elements training set 
lie closest decision boundary training points removed moved cross training repeated separating hyperplane 

karush kuhn tucker conditions karush kuhn tucker kkt conditions play central role theory practice constrained optimization 
primal problem kkt conditions may stated fletcher lp lp iyi yi xi yi xi kkt conditions satis ed solution constrained optimization problem convex kind constraints provided intersection set feasible directions set descent directions coincides intersection set feasible directions linearized constraints set descent directions see fletcher mccormick 
technical regularity assumption holds support vector machines constraints linear 
furthermore problem svms convex convex objective function constraints give convex feasible region convex problems regularity condition holds kkt conditions necessary su cient solution fletcher 
solving svm problem equivalent nding solution kkt conditions 
fact results approaches nding solution example primal dual path method mentioned section 
immediate application note explicitly determined training procedure threshold implicitly determined 
easily kkt complementarity condition eq 
choosing computing note numerically safer take mean value resulting equations 
notice done far cast problem optimization problem constraints manageable eqs 

finding solution real world problems usually require numerical methods 
say 
rst rare case problem nontrivial number dimensions arbitrary solution certainly obvious solution analytically 

optimal hyperplanes example main aim section explore non trivial pattern recognition problem support vector solution analytically derived useful proof 
problem considered training point turn support vector reason nd solution analytically 
consider symmetrically placed points lying sphere radius precisely points form vertices dimensional symmetric simplex 
convenient points away passes origin perpendicular vector formulation points lie span embedded 
explicitly recalling vectors labeled roman indices coordinates greek coordinates xi rn kronecker delta de ned 
example vectors equidistant points unit circle see consequence symmetry angle pair vectors equal arccos xi xj succinctly xi xj assigning class label arbitrarily point wish nd hyperplane separates classes widest margin 
maximize ld eq 
subject subject equality constraint eq 

strategy simply solve problem inequality constraints 
resulting solution fact satisfy general solution actual maximum ld lie feasible region provided equality constraint eq 
met 
order impose equality constraint introduce additional lagrange multiplier seek maximize ld introduced hessian iyi hij xj setting ld gives yi simple structure diagonal elements diagonal elements fact diagonal elements di er factors yi suggests looking solution form yi yi unknowns 
plugging form eq 
gives de ned yip yi yi substituting equality constraint eq 
nd gives gives solution yip yip kwk xj yip note cases lagrange multiplier remain undetermined determining trivial 
solved problem clearly positive fact zero training points class 
note kwk depends number positive negative polarity points class labels assigned points eq 

clearly true isgiven margin kwk yi xi number points minimum margin occurs equal numbers positive negative examples case margin odd minimum margin occurs case 
cases maximum margin mmax example dimensional simplex consisting points lying spanning labeling points polarity maximum minimum margin see 
note results section amount alternative constructive proof vc dimension oriented separating hyperplanes 

test phase trained support vector machine 
simply determine side decision boundary hyperplane lying half way parallel test pattern lies assign corresponding class label take class sgn 

non separable case algorithm separable data applied non separable data nd feasible solution evidenced objective function dual lagrangian growing arbitrarily large 
extend ideas handle non separable data 
relax constraints necessary cost increase primal objective function doing 
done introducing positive slack variables constraints cortes vapnik xi yi xi yi error occur corresponding exceed unity upper bound number training errors :10.1.1.15.9362
natural way assign extra cost errors change objective function minimized kwk parameter chosen user larger corresponding assigning higher penalty errors 
stands convex programming problem positive integer quadratic programming problem choice advantage lagrange multipliers appear wolfe dual problem maximize ld subject xj iyi solution ns ns number support vectors 
di erence optimal hyperplane case upper bound situation summarized schematically 
need karush kuhn tucker conditions primal problem 
primal lagrangian lp kwk xi ig lagrange multipliers introduced enforce positivity kkt conditions primal problem note runs number training points dimension data lp lp iyi lp yi xi xi ig kkt complementarity conditions eqs 
determine threshold note eq 
combined eq 
shows simply take training point eq 

numerically take average training points 
linear separating hyperplanes non separable case 

analogy consider case data suppose th support vector exerts force fi iyi sti sheet lying decision surface decision sheet denotes unit vector direction 
solution satis es conditions mechanical equilibrium forces torques iyi si iyi si support vectors denotes vector product 
data clearly condition sum forces vanish met 
easily show torque vanishes 
mechanical analogy depends form solution holds separable non separable cases 
fact analogy holds general nonlinear case described 
analogy emphasizes interesting point important data points support vectors highest values exert highest forces decision sheet 
non separable case upper bound corresponds upper bound force point allowed exert sheet 
analogy provides reason call particular vectors support vectors 
examples pictures shows examples class pattern recognition problem separable 
classes denoted circles disks respectively 
support vectors identi ed extra circle 
error non separable case identi ed cross 
reader invited lucent svm applet burges experiment create pictures possible try bit color 
linear case separable left right 
background colour shows shape decision surface 

nonlinear support vector machines methods generalized case decision function linear function data 
boser guyon vapnik showed old trick aizerman accomplish straightforward way 
notice way inwhich data appears training problem eqs 
form dot products xi xj 
suppose rst mapped data possibly nite dimensional euclidean space mapping call course training algorithm depend data dot products functions form xi xj 
kernel function xi xj xi xj need training algorithm need explicitly know example xi xj xjk particular example nite dimensional easy explicitly 
replaces xi xj xi xj training algorithm algorithm happily produce support vector machine lives nite dimensional space furthermore roughly amount take train un mapped data 
considerations previous sections hold doing linear separation di erent space 
machine 
need live inh see eq 

test phase svm computing dot products test point speci cally computing sign ns iyi si ns si si support vectors 
computing explicitly si si 
call space data live 
mnemonic low dimensional high dimensional usually case range higher dimension domain 
note addition fact lives general vector maps map eq 
computed step avoiding sum making corresponding svm ns times faster ns number support vectors 
despite ideas lines signi cantly speed test phase svms burges 
note easy nd kernels example kernels functions dot products xi training algorithm solution independent dimension section discuss functions allowable 
section simple example allowed kernel construct mapping suppose data vectors choose xi xj xi xj easy nd space mapping suchthat choose note subscripts refer vector components 
data de ned square typical situation grey level image data entire image shown 
illustrates think mapping image may live space high dimension just possibly surface intrinsic dimension just note mapping space unique kernel 
equally chosen image square mapping literature svms usually refers space hilbert space section notes point 
think hilbert space generalization euclidean space behaves fashion 
speci cally linear space inner product de ned complete respect corresponding norm cauchy sequence points converges point space 
authors 
kolmogorov require separable countable subset closure space don 
generalization mainly inner product inner product just scalar dot product euclidean spaces general 
interesting older mathematical literature kolmogorov required hilbert spaces nite dimensional mathematicians quite happy de ning nite dimensional euclidean spaces 
research hilbert spaces centers operators spaces basic properties long worked 
people understandably atthe mention hilbert spaces decided term euclidean tutorial 

mercer condition kernels exist pair fh properties described 
answer mercer condition vapnik courant hilbert exists mapping expansion suchthat dx nite dxdy note speci cases may easy check mercer condition satis ed 
eq 
hold nite norm satis es eq 

easily prove condition satis ed positive integral powers dot product show dx dxdy typical term multinomial expansion contributes term form 


xr left hand side eq 
factorizes 


xr yr dxdy dx simple consequence expressed cp cp positive real coe cients series uniformly convergent satis es mercer condition fact noted smola scholkopf muller 
happens uses kernel satisfy mercer condition 
general may exist data hessian inde nite quadratic programming problem solution dual objective function arbitrarily large 
kernels satisfy mercer condition nd training set results positive semide nite hessian case training converge perfectly 
case geometrical interpretation described lacking 

notes mercer condition tells prospective kernel dot product space tell construct homogeneous homogeneous dot product quadratic polynomial kernel discussed explicitly construct mapping kernels 
section show 
extended arbitrary homogeneous polynomial kernels corresponding space euclidean space dimension example degree polynomial data consisting images dim 
usually mapping data feature space enormous number dimensions bode ill generalization performance resulting machine 
set hyperplanes fw bg parameterized dim numbers 
pattern recognition systems billions nite number parameters past start gate 
come svms 
argue form solution adjustable parameters number training samples question requirement margin hyperplanes saving day 
shall see strong case claim 
mapped surface intrinsic dimension dim dim dim obvious mapping surjective 
need bijective consider eq 

image need space considering simple quadratic example vector image 
little playing inhomogeneous kernel xi xj xi xj convince corresponding map linearly dependent vectors linearly independent inh 
far considered cases done implicitly 
equally turn things start construct corresponding kernel 
example vapnik expansion data cut terms form nx cos rx sin rx viewed dot product mapped cos cos sin sin 
corresponding dirichlet kernel computed closed form xi xj xi xj sin xi xj sin xi xj easily seen follows letting xi xj xi xj nx cos cos sin sin nx cos ref nx ref ei sin sin ir clear implicit mapping trick algorithm data appear dot products example nearest neighbor algorithm 
fact derive version principal component analysis scholkopf smola muller trick nd uses 

examples nonlinear svms rst kernels investigated pattern recognition problem kx yk tanh eq 
results classi er polynomial degree data eq 
gives gaussian radial basis function classi er eq 
gives particular kind layer sigmoidal neural network 
rbf case number centers ns eq 
centers si weights threshold produced automatically svm training give excellent results compared classical rbfs case gaussian rbfs scholkopf 
neural network case rst layer consists ns sets weights set consisting dl dimension data weights second layer consists ns weights evaluation simply requires weighted sum sigmoids evaluated dot products test data support vectors 
neural network case architecture number weights determined svm training 
note hyperbolic tangent kernel satis es mercer condition certain values parameters data kxk 
rst noticed experimentally vapnik necessary conditions parameters positivity known shows results pattern recognition problem shown kernel chosen cubic polynomial 
notice number degrees freedom higher linearly separable case left panel solution roughly linear indicating capacity controlled linearly non separable case right panel separable 
degree polynomial kernel 
background colour shows shape decision surface 
note svm classi ers described binary classi ers easily combined handle multiclass case 
simple ective combination trains versus rest classi ers say positive rest negative class case takes class test point corresponding largest positive distance boser guyon vapnik 

global solutions uniqueness solution support vector training problem global unique 
global mean exists point feasible region objective function takes lower value 
address kinds ways uniqueness may hold solutions fw bg unique expansion eq 
solutions fw bg di er 
interest pair fw bg unique may equivalent expansions require fewer support vectors trivial example require fewer instructions test phase 
turns local solution global 
property convex programming problem fletcher 
furthermore solution guaranteed unique objective function eq 
strictly convex case means hessian positive de nite note quadratic objective functions hessian positive de nite strictly convex true positive de nite hessian implies strictly convex objective function vice versa consider fletcher 
hessian positive semide nite solution unique consider points real line coordinates andx polarities 
hessian positive semide nite solution eqs 
unique 
easy nd solutions unique sense expansion unique example consider problem separable points square polarities respectively 
solution note solutions satisfy constraints iyi 
occur general 
solution choose null space hessian hij xj require orthogonal vector components 
adding eq 
leave ld unchanged 
satis es eq 
solution solutions fw bg unique 
emphasize happen principle hessian positive de nite solutions necessarily global 
simple theorem shows nonunique solutions occur solution optimal point deformable solution optimal point away points solutions 
theorem variable stand pair variables fw bg 
hessian problem positive semide nite objective function convex 
points objective function attains minimal value 
exists path solution proof minimum value objective function fmin 
assumption fmin 
convexity fmin 
furthermore linearity satisfy constraints eq 
explicitly combining constraints yi xi yi xi xi simple theorem quite instructive 
example think problems depicted di erent optimal solutions case linear support vector machines 
smoothly move proposed solution generating hyperplanes solutions know proposed solutions fact solutions 
fact cases optimal unique solution suitable choice ect assigning label points 
note perfectly acceptable solution classi cation problem proposed hyperplane cause primal objective function take higher value 

problems proposed incorrect non unique solutions 
note fact svm training nds global solution contrast case neural networks local minima usually exist 

methods solution support vector optimization problem solved analytically number training data small separable case known training data support vectors sections 
note happen problem symmetry section happen section 
general analytic case worst case computational complexity order inversion hessian ns number support vectors examples complexity 
real world cases equations dot products replaced kernels solved numerically 
small problems general purpose optimization package solves linearly constrained convex quadratic programs 
survey available solvers get wright 
larger problems range existing techniques brought bear 
full exploration relative merits methods ll tutorial 
just describe general issues concreteness give brief explanation technique currently 
face means set points lying boundary feasible region active constraint constraint holds 
nonlinear programming techniques see fletcher mangasarian mccormick 
basic recipe note optimality kkt conditions solution satisfy de ne strategy approaching optimality uniformly increasing dual objective function subject constraints decide decomposition algorithm portions training data need handled time boser guyon vapnik osuna freund girosi 
give brief description issues involved 
view problem requiring solution sequence equality constrained problems 
equality constrained problem solved step newton method requires storage factorization projected hessian steps conjugate gradient ascent press number data points problem currently solved extra storage required 
algorithms move face new constraint encountered case algorithm restarted new constraint added list equality constraints 
method disadvantage new constraint active time 
projection methods considered point outside feasible region computed line searches projections done actual move remains inside feasible region 
approach add new constraints 
note approaches active constraints inactive step 
algorithms essential part hessian columns corresponding need computed algorithms compute hessian 
newton approach take advantage fact hessian positive semide nite diagonalizing bunch kaufman algorithm bunch kaufman bunch kaufman hessian inde nite easily reduced block diagonal form algorithm 
algorithm new constraint active inactive factorization projected hessian easily updated opposed recomputing factorization scratch 
point methods variables essentially rescaled remain inside feasible region 
example loqo algorithm vanderbei vanderbei primal dual path algorithm 
method useful problems number support vectors fraction training sample size expected large 
brie describe core optimization method currently active set method combining gradient conjugate gradient ascent 
objective function computed gradient little extra cost 
phase search directions gradient 
nearest face search direction 
dot product gradient indicates maximum lies current point nearest face optimal point search direction computed analytically note require line search phase entered 
jump new face repeat phase 
phase polak conjugate gradient ascent press done new face encountered case phase re entered stopping criterion met 
note search directions projected continue satisfy equality constraint eq 

note conjugate gradient algorithm simply searching subspace 
important projection implemented away eq 
met easy angle resulting search direction search direction prior projection minimized quite easy 
sticky faces algorithm face hit search directions adjusted subsequent searches done face 
sticky faces reset non sticky rate increase objective function falls threshold 
algorithm stops fractional rate increase objective function falls tolerance typically double precision 
note stopping criterion condition size projected search direction falls threshold 
criterion handle scaling 
opinion hardest thing get right handling precision problems correctly 
done algorithm may converge may slower needs 
algorithm working check solution satis es karush kuhn tucker conditions primal problem necessary su cient conditions solution optimal 
kkt conditions eqs 
dot products data vectors replaced kernels appear note expanded eq 
rst general mapping point 
check kkt conditions su cient check satisfy equality constraint holds points eq 
points satisfy eq 

su cient conditions kkt conditions hold note doing explicitly compute doing trivial 

complexity scalability support vector machines striking property 
training test functions depend data kernel functions xi xj 
corresponds dot product space dimension dh dh large nite complexity computing far smaller 
example kernels form xi xj dot product require order dl operations computation xi xj requires dl operations recall dl dimension data 
fact allows construct hyperplanes high dimensional spaces left tractable computation 
svms circumvent forms curse dimensionality proliferation parameters causing intractable complexity proliferation parameters causing tting 

training concreteness give results computational training algorithms bunch kaufman kaufman 
results assume di erent strategies di erent situations 
consider problem training just chunk see 
number training points ns svs dl dimension input data 
case svs upper bound ns number operations nsl basically 
ns starting interior feasible region 
case svs upper bound ns 
svs upper bound ns dll 
larger problems decomposition algorithms proposed date 
chunking method boser guyon vapnik starts small arbitrary subset data trains 
rest training data tested resulting classi er list errors constructed sorted far wrong side margin lie kkt conditions violated 
chunk constructed rst combined ns support vectors ns decided heuristically chunk size allowed grow slowly result slow convergence 
note vectors dropped support vectors chunk may appear nal solution 
process continued data points satisfy kkt conditions 
method requires number support vectors ns small hessian size ns ns memory 
alternative decomposition algorithm proposed overcomes limitation osuna freund girosi 
algorithm small portion training data trained time furthermore subset support vectors need working set set points allowed vary 
method shown able easily handle problem training points support vectors 
noted speed approach relies support vectors having corresponding lagrange multipliers upper bound training algorithms parallel processing ways 
elements hessian computed simultaneously 
second element requires computation dot products training data parallelized 
third computation objective function gradient bottleneck parallelized requires matrix multiplication 
envision parallelizing higher level example training di erent chunks simultaneously 
schemes combined decomposition algorithm osuna freund girosi needed large problems support vectors bound tractable 

testing test phase simply evaluate eq 
require mns operations number operations required evaluate kernel 
dot product rbf kernels dl dimension data vectors 
evaluation kernel sum highly parallelizable procedures 
absence parallel hardware speed test phase large factor described section 
vc dimension support vector machines show vc dimension svms large nite 
explore arguments spite svms usually exhibit generalization performance 
emphasized essentially plausibility arguments 
currently exists theory guarantees family svms high accuracy problem 
call kernel satis es mercer condition positive kernel corresponding space embedding space 
call space minimal dimension kernel minimal embedding space 
theorem positive kernel corresponds minimal embedding space vc dimension corresponding support vector machine error penalty eq 
allowed take values dim 
proof minimal embedding space dimension dh points image mapping position vectors linearly independent 
theorem vectors shattered hyperplanes restricting svms separable case section error penalty allowed take values points linearly separable solution separate family support vector machines kernel shatter points vc dimension dh 
look examples 

vc dimension polynomial kernels consider svm homogeneous polynomial kernel acting data dl dl case dl kernel quadratic section explicitly construct map letting zi ix see dimension corresponds term powers zi expansion fact choose label components wecan explicitly write mapping dl rd leads 

rdl 
xr xr rd dl dlx ri ri theorem space data live dimension dl dl dimension minimal embedding space homogeneous polynomial kernels degree dl dl dl proof appendix 
vc dimension svms kernels 
noted gets large quickly 

vc dimension radial basis function kernels theorem consider class mercer kernels kx assume data chosen arbitrarily family classi ers consisting support vector machines kernels error penalty allowed take values nite vc dimension 
proof kernel matrix kij xi xj gram matrix matrix dot products see horn clearly choose training data diagonal elements ki arbitrarily small assumption diagonal elements ki 
matrix full rank set vectors dot products form linearly independent horn theorem points shattered hyperplanes support vector machines su ciently large error penalty 
true nite number points vc dimension classi ers nite 
note assumptions theorem stronger necessary chosen connection radial basis functions clear 
fact necessary training points chosen rank matrix kij increases limit increases 
example gaussian rbf kernels accomplished training data restricted lie bounded subset dl choosing small rbf widths 
general vc dimension svm rbf classi ers certainly nite data restricted lie bounded subset dl choosing restrictions rbf widths way vc dimension 
case gives second opportunity svm solution computed analytically amounts second constructive proof theorem 
concreteness take case gaussian rbf kernels form kx choose training points smallest distance pair points larger width consider decision function evaluated support vector sj sj ksi sjk sum right hand side largely dominated term fact ratio term contribution rest sum arbitrarily large choosing training points arbitrarily far apart 
order nd svm solution assume moment training point support vector svms separable case section argument hold svms non separable case eq 
allowed take large values 
points support vectors equalities eqs 
hold 
positive negative polarity points 
assume positive negative polarity points thesame value lagrange multiplier 
know assumption correct delivers solution satis es kkt conditions constraints 
eqs 
applied training data equality constraint eq 
satis ed resulting positive kkt conditions constraints satis ed global solution zero training errors 
number training points labeling arbitrary separated error vc dimension nite 
situation summarized schematically 

gaussian rbf svms su ciently small width classify arbitrarily large number training points correctly nite vc dimension left striking conundrum 
vc dimension nite data allowed take values dl svm rbfs excellent performance scholkopf 
similar story holds polynomial svms 
come 

generalization performance svms section collect various arguments bounds relating generalization performance svms 
start presenting family svm classi ers structural risk minimization rigorously implemented give maximizing margin important 

vc dimension gap tolerant classi ers consider family classi ers set functions call gap tolerant classi ers 
particular classi er speci ed location diameter ball hyperplanes parallel normals call set points lying hyperplanes margin set 
decision functions de ned follows points lie inside ball margin set assigned class depending side margin set fall 
points simply de ned correct assigned class classi er contribute risk 
situation summarized 
odd family classi ers condition impose trained result systems similar svms structural risk minimization demonstrated 
rigorous discussion appendix 
label diameter ball perpendicular distance hyperplanes vc dimension de ned maximum number points shattered family shattered mean points occur errors possible ways see appendix discussion 
clearly control vc dimension family classi ers controlling minimum margin maximum diameter members family allowed assume 
example consider family gap tolerant classi ers diameter shown 
margin satisfying shatter points shatter shatter 
families classi ers corresponds sets classi ers just nested subsets functions andh 

gap tolerant classi er data ideas show gap tolerant classi ers implement structural risk minimization 
extension example spaces arbitrary dimension encapsulated modi ed theorem vapnik theorem data rd vc dimension gap tolerant classi ers minimum margin maximum diameter dmax bounded max mine dg 
proof assume lemma vapnik held follow symmetry arguments lemma consider points lying ball points gap tolerant classi ers margin order maximized points lie vertices dimensional symmetric simplex lie surface ball 
proof need consider case number points satis es 
points vc dimension oriented hyperplanes distribution points shattered gap tolerant classi er shattered oriented hyperplane shows 
consider points sphere diameter sphere dimension 
need results section nd distribution points vertices dimensional symmetric simplex shattered gap tolerant classi ers max min odd points shattered nd distribution points shattered max min max min maximum number points shattered may 
bd max min odd points shattered max quantity hand side satis es integer 
odd largest number points shattered certainly bounded dd max mine satis ed 
general vc dimension gap tolerant classi ers satisfy max min result concludes proof 

gap tolerant classi ers structural risk minimization svms see structural risk minimization gap tolerant classi ers 
need consider subset call training succeeds success mean training data assigned label note labels coincide actual labels training errors allowed 
nd subset gives fewest training errors call number errors nmin 
subset nd function gives maximum margin lowest bound vc dimension 
note value resulting risk bound right hand side eq 
bound vc dimension place vc dimension 
nd subset gives nmin training errors 
subset nd gives maximum margin note corresponding risk bound 
iterate take classi er gives minimum risk bound 
divide functions nested subsets follows fd mg satisfying dd family functions vc dimension bounded 
note 
srm proceeds training succeeds subset empirical risk minimized subset choosing gives lowest risk bound 
note essential arguments bound holds chosen decision function just minimizes empirical risk eliminating solutions training point satis es invalidate argument 
resulting gap tolerant classi er fact special kind support vector machine simply count data falling outside sphere containing training data inside separating margin error 
reasonable conclude support vector machines trained similar objectives gain similar kind capacity control training 
gap tolerant classi er svm argument constitute rigorous demonstration structural risk minimization svms 
original argument structural risk minimization svms known awed structure determined data see vapnik section 
believe subtle problem original argument 
structure de ned training points members margin set 
specify fall margin labeled 
simply assigns xed class say vc dimension higher bound derived theorem 
true labels errors see appendix 
labels correct arrives gap tolerant classi ers 
hand known structural risk minimization systems structure depend data shawe taylor shawe taylor :10.1.1.33.8995
unfortunately resulting bounds looser vc bounds loose examine typical case bound factor higher measured test error 
moment structural risk minimization provide rigorous explanation svms generalization performance 
arguments strongly suggest algorithms minimize expected give better generalization performance 
evidence theorem vapnik whichwe quote proof theorem optimal hyperplanes passing origin error error probability error test set expectation left training sets size expectation right training sets size order observations useful real problems compute diameter minimal enclosing sphere described number training points kernel mapping 

compute minimal enclosing sphere mapping embedding space wish compute radius smallest sphere encloses mapped training data wish minimize subject xi ck unknown center sphere 
introducing positive lagrange multipliers primal lagrangian lp xi ck convex quadratic programming problem maximize wolfe dual ld ik xi xi jk xi xj replaced xi xj xi xj subject solution xi problem similar support vector training fact code easily modi ed solve problem 
note sense lucky analysis shows exists expansion center priori reason expect center sphere expressible terms mapped training data way 
said solution support vector problem eq 

chosen geometrical construction fortunate 
consider smallest area equilateral triangle containing points points position vectors linearly dependent center triangle expressed terms 

bound leave vapnik gives alternative bound actual risk support vector machines error number support vectors number training samples error actual risk machine trained examples error expectation actual risk choices training set size number support vectors expectation number support vectors choices training sets size easy see bound arises consider typical situation training training set shown 
support vectors circles errors cross removal re training dotted line denotes new decision surface 
get estimate test error removing training points re training testing removed point repeating training points 
support vector solution know removing training points support vectors include errors ect hyperplane 
worst happen support vector error 
expectation training sets gives upper bound actual risk training sets size 
elegant nd bound 
situations actual error increases number support vectors decreases intuitive systems give fewer support vectors give better performance fail 
furthermore bound tighter estimate vc dimension combined eq 
time predictive shall see section 

vc sv bounds actual risk put observations 
mentioned training svm rbf classi er automatically give values rbf weights number centers center positions threshold 
gaussian rbfs parameter left rbf width eq 
assume rbf width problem 
nd optimal value choosing minimizes shows series experiments done nist digit data training points test points 
top curve left hand panel shows vc bound bound resulting approximating vc dimension eq 
eq 
middle curve shows bound leave eq 
bottom curve shows measured test error 
clearly case bounds loose 
right hand panel shows just vc bound top curve test error scaled factor note curves cross 
striking curves minima place case vc bound loose predictive 
experiments digits showed vc bound gave minimum factor minimized test error digit inconclusive 
interestingly cases vc bound consistently gave prediction minimized test error 
hand leave bound tighter predictive minimum values tested 
actual risk sv bound vc bound sigma squared 
vc bound predictive loose 
vc bound actual risk sigma squared 
limitations biggest limitation support vector approach lies choice kernel 
kernel xed svm classi ers user chosen parameter error penalty kernel big rug parameters 
done limiting kernels prior knowledge scholkopf burges best choice kernel problem research issue :10.1.1.40.9777
second limitation speed size training testing 
speed problem test phase largely solved burges requires training passes 
training large datasets millions support vectors unsolved problem 
discrete data presents problem suitable rescaling excellent results obtained joachims 
done training multiclass svm step optimal design multiclass svm classi ers area research 

extensions brie describe simplest ective methods improving performance svms 
virtual support vector method scholkopf burges vapnik burges scholkopf attempts incorporate known invariances problem example translation invariance image recognition problem rst training system creating new data distorting resulting support vectors translating case mentioned nally training new system distorted undistorted data 
idea easy implement better methods incorporating invariances proposed far 
reduced set method burges burges scholkopf introduced address speed support vector machines test phase starts trained svm 
idea replace sum eq 
similar sum support vectors computed vectors elements training set di erent set weights computed 
number parameters chosen give speedup desired 
resulting vector vector parameters minimizing euclidean norm di erence original vector approximation 
technique svm regression nd cient function representations example data compression 
combining methods gave factor speedup error rate increased nist digits burges scholkopf 

svms provide new approach problem pattern recognition regression estimation linear operator inversion clear connections underlying statistical learning theory 
di er radically comparable approaches neural networks svm training nds global minimum simple geometric interpretation provides fertile ground investigation 
svm largely characterized kernel svms link problems designed large body existing kernel methods 
hope tutorial encourage explore svms 
acknowledgments grateful osuna scholkopf singer smola vapnik comments manuscript 
reviewers editor fayyad extensive useful comments 
special due vapnik patient guidance learned ropes smola scholkopf interesting fruitful discussions shawe taylor schuurmans valuable discussions structural risk minimization 
appendix 
proofs theorems collect theorems stated text proofs 
lemma shorter proof theorem alternative mangasarian wished keep proofs self contained possible 
lemma sets points may separated hyperplane intersection convex hulls empty 
proof allow notions points position vectors points interchangeably proof 
ca cb convex hulls sets points rn denote set points position vectors note contain origin ca cb corresponding meaning convex hulls 
showing linearly separable separable equivalent set linearly separable origin suppose pick denote set points clearly sets linearly separable 
repeating process shows linearly separable origin linearly separable 
cb cb linearly separable origin 
clearly ca cb contain origin 
furthermore ca cb convex ca cb wehave ca cb 
su cient convex set contain linearly separable xmin point euclidean distance minimal 
note point chord joining lies contain points closer 
show xmin 
suppose xmin 
line segment joining xmin convexity implies assumption points xmin form obtuse right triangle obtuse right angle occurring point de ne xmin kx 
distance closest point xmin xmin ands linearly separable ca cb linearly separable anda fortiori linearly separable linearly separable remains show sets points linearly separable intersection convex hulls empty 
assumption exists pair ai ai bi bi 
consider general point ca 
may written iai 
ifw ai bg 
similarly points cb 
ca cb able nd point simultaneously satis es inequalities 
theorem consider set points choose points origin 
points shattered oriented hyperplanes position vectors remaining points linearly independent 
proof label origin assume position vectors remaining points linearly independent 
consider partition points subsets respectively subset containing convex hull set points position vectors satisfy position vectors points including null position vector origin 
similarly convex hull set points position vectors satisfy position vectors points 
suppose intersect 
exists simultaneously satis es eq 
eq 

subtracting equations gives linear combination non null position vectors vanishes contradicts assumption linear independence 
lemma intersect exists hyperplane separating 
true choice partition points shattered 
remains show non null position vectors linearly independent points shattered oriented hyperplanes 
position vectors linearly independent exist numbers suchthat isi sign scale 
eq 
states origin lies convex hull remaining points lemma origin separated remaining points points shattered 
sign place terms negative right indices corresponding partition sno set origin removed 
scale equation jj kj jj kj 
suppose loss generality thatthe holds 
left hand side eq 
position vector point lying convex hull points equality holds points right hand side position vector point lying convex hull points sk convex hulls overlap lemma sets points separated 
points shattered 
theorem data dimensional dimension minimal embedding space homogeneous polynomial kernels degree proof number components label components eq 

component uniquely identi ed choice integers ri pd ri consider objects distributed partitions numbered objects allowed left partitions right partitions 
suppose objects fall partitions 
correspond term xm product eq 

similarly objects falling left partitions corresponds term xm falling right partitions corresponds term xm number distinct terms form xr rd pd ri ri number way distributing objects partitions modulo permutations partitions permutations objects show set vectors components rd span space follows fact components linearly independent functions 
suppose image acting lis subspace exists xed nonzero vector dim vi labeling introduced consider particular component rd dx ri eq 
holds mapping eq 
certainly derivatives de ned apply operator xd rd eq 
pick term corresponding powers xi eq 
giving vr rd true choices rd ri component vanish 
image acting 
gap tolerant classi ers vc bounds point central argument 
normally thinks collection points shattered set functions labels points function set assigns labels points 
vc dimension set functions de ned maximum number points shattered 
consider slightly di erent de nition 
set points shattered set functions choice labels points function set assigns incorrect labels points 
vc dimension set functions de ned maximum number points shattered 
fact second de nition adopt enters vc bound proofs vapnik devroye lugosi 
course functions range data assigned positive negative class de nitions 
falling region simply deemed errors correct de nitions di erent 
concrete example suppose de ne gap intolerant classi ers gap tolerant classi ers label points lying margin outside sphere errors 
consider situation assign positive class points 
gap intolerant classi er margin width greater ball diameter shatter points rst de nition shatter shatter points second correct de nition 
caveat mind outline bounds apply functions range label means point correct 
bounds apply functions de ned mean error corresponding vc dimension higher weakening bound case making useless 
follow notation devroye lugosi 
consider points denote density onr function range set functions 
associated label yx 
fx xng nite number points require property exists xi xi 
de ne set points fx yx fx yx require sets measurable 
denote set de nition xi points 
de ne empirical risk set fxi fxi nx ixi indicator function 
note empirical risk zero xi xi 
de nition de ne actual risk function note points contribute actual risk 
de nition xed xn number di erent ffx xng ag sets de ned 
th shatter coe cient ofa de ned max xn fr na xn de ne vc dimension class maximum integer theorem adapted devroye lugosi theorem fxi de ned points xn denote subset satisfy xi xi 
restriction may viewed part training algorithm 
fxi exp proof exactly devroye lugosi sections theorems 
dropped sup emphasize holds functions particular holds minimize empirical error training data take values 
note proof holds second de nition shattering 
note usual form vc eq 
en vc dimension vapnik setting exp solving clearly results apply gap tolerant classi ers section 
particular classi er speci ed set parameters fb mg ball rd diameter dimensional oriented hyperplane rd scalar called margin 
speci ed normal direction speci es points labeled positive negative function minimal distance origin 
margin set sm de ned sett consisting points minimal distance 
de ne sm andz 
function de ned follows corresponding sets eq 

notes 
muller private communication 
reader elicits sinking feeling urged study strang fletcher bishop 
simple geometrical interpretation lagrange multipliers boundary corresponding single constraint gradient function parallel gradient function contours specify boundary 
boundary corresponding intersection constraints gradient parallel linear combination non negative case inequality constraints gradients functions contours specify boundary 

phrase learning machine function estimation algorithm training parameter estimation procedure testing computation function value performance generalization accuracy error rate test set size tends nity stated 

name test set train set got rst 

term oriented hyperplane emphasize mathematical object considered pair fh ng set points lie hyperplane particular choice unit normal 
fh ng fh ng di erent oriented hyperplanes 

set points dimensional subspace linear space said general position kolmogorov 
convex hull set points general position de nes dimensional simplex vertices points 

derivation bound assumes empirical risk converges uniformly actual risk number training observations increases vapnik 
necessary su cient condition lim number training samples set decision functions vapnik vapnik 
set functions nite vc dimension vc entropy isl log classi ers required uniform convergence hold bound 

nice geometric interpretation dual problem basically nding closest points convex hulls sets 
see bennett bredensteiner 

de ne torque repeated indices summed right hand side totally antisymmetric tensor 
recall greek indices denote tensor components 
sum torques decision sheet iy 
original formulation vapnik called extreme vectors 

decision function mean function sign represents class assigned data point 
intrinsic dimension mean number parameters required specify point manifold 

alternatively argue form solution possible lie subspace dimension 
preparation 

smola pointing 

reviewers pointing 

core quadratic optimizer lines 
higher level code handle caching dot products chunking io quite complex considerably larger 

kaufman providing results 

recall ceiling sign de means smallest integer greater equal 
typo actual formula vapnik corrected 

note example distance pair vertices symmetric simplex see eq 

rigorous proof needed far know 

shawe taylor pointing 

vapnik private communication 

alternative bound corresponding set totally bounded non negative functions equation vapnik 
loss functions value zero empirical risk zero bound looser log log eq 
case 

blanz private communication aizerman braverman 
theoretical foundations potential function method pattern recognition learning 
automation remote control 
anthony biggs 
pac learning neural networks 
handbook brain theory neural networks pages 
bennett bredensteiner 
geometry learning 
geometry page appear washington 
mathematical association america 
bishop 
neural networks pattern recognition 
clarendon press oxford 
blanz scholkopf burges vapnik vetter 
comparison view object recognition algorithms realistic models 
von der malsburg von seelen editors arti cial neural networks icann pages berlin 
springer lecture notes computer science vol 

boser guyon vapnik 
training algorithm optimal margin classi ers 
fifth annual workshop computational learning theory pittsburgh 
acm 
james bunch linda kaufman 
stable methods calculating inertia solving symmetric linear systems 
mathematics computation 
james bunch linda kaufman 
computational method inde nite quadratic programming problem 
linear algebra applications 
burges scholkopf 
improving accuracy speed support vector learning machines 
mozer jordan petsche editors advances neural information processing systems pages cambridge ma 
mit press 
burges 
simpli ed support vector decision rules 
saitta editor proceedings thirteenth international conference machine learning pages bari italy 
morgan kaufman 
burges 
building locally invariant kernels 
proceedings nips workshop support vector machines appear 
burges 
support vector web page svm research bell labs com 
technical report lucent technologies 
cortes vapnik 
support vector networks 
machine learning 
courant hilbert 
methods mathematical physics 
interscience 
luc devroye laszlo gabor lugosi 
theory pattern recognition 
springer verlag applications mathematics vol 

drucker burges kaufman smola vapnik 
support vector regression machines 
advances neural information processing systems 
fletcher 
practical methods optimization 
john wiley sons nd edition 
stuart geman elie bienenstock 
neural networks bias variance dilemma 
neural computation 
girosi 
equivalence sparse approximation support vector machines 
neural computation appear cbcl ai memo mit 
guyon vapnik boser bottou solla 
structural risk minimization character recognition 
advances neural information processing systems 

space problem book 
van nostrand 
roger horn charles johnson 
matrix analysis 
cambridge university press 
joachims 
text categorization support vector machines 
technical report ls viii number university 
ftp ftp ai informatik uni dortmund de pub reports report ps kaufman 
solving qp problem support vector training 
proceedings nips workshop support vector machines appear 
kolmogorov 
introductory real analysis 
prentice hall 

nonlinear programming 
mcgraw hill new york 
garth mccormick 
non linear programming theory algorithms applications 
john wiley sons 
montgomery peck 
linear regression analysis 
john wiley sons nd edition 
wright 
optimization guide 
siam 
jorge 
solution large quadratic programming problems bound constraints 
siam optimization 
mukherjee osuna girosi 
nonlinear prediction chaotic time series support vector machine 
proceedings ieee workshop neural networks signal processing pages amelia island fl 

muller smola ratsch scholkopf kohlmorgen vapnik 
predicting time series support vector machines 
proceedings international conference onarti cial neural networks page 
springer lecture notes computer science 
edgar osuna robert freund federico girosi 
improved training algorithm support vector machines 
proceedings ieee workshop neural networks signal processing eds 
principe giles morgan wilson pages amelia island fl 
edgar osuna robert freund federico girosi 
training support vector machines application face detection 
ieee conference computer vision pattern recognition pages 
edgar osuna federico girosi 
reducing run time complexity support vector machines 
international conference pattern recognition submitted 
william press brain flannery teukolsky william 
numerical recipes art scienti computing 
cambridge university press nd edition 
schmidt 
identifying speaker support vector networks 
interface proceedings sydney 
scholkopf 
support vector learning 
oldenbourg verlag munich 
scholkopf burges vapnik 
extracting support data task 
fayyad uthurusamy editors proceedings international conference knowledge discovery data mining 
aaai press menlo park ca 
scholkopf burges vapnik 
incorporating invariances support vector learning machines 
von der malsburg von seelen editors arti cial neural networks icann pages berlin 
springer lecture notes computer science vol 

scholkopf simard smola vapnik 
prior knowledge support vector kernels 
jordan kearns solla editors advances neural information processing systems cambridge ma 
mit press 
press 
scholkopf smola 
muller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
press 
scholkopf smola 
muller burges vapnik 
support vector methods learning feature extraction 
ninth australian congress neural networks appear 
scholkopf sung burges girosi niyogi poggio vapnik 
comparing support vector machines gaussian kernels radial basis function classi ers 
ieee trans 
sign 
processing 
john shawe taylor peter bartlett robert williamson martin anthony 
framework structural risk minimization 
proceedings th annual conference computational learning theory pages 
john shawe taylor peter bartlett robert williamson martin anthony 
structural risk minimization data dependent hierarchies 
technical report neurocolt technical report nc tr 
smola scholkopf 
method pattern recognition regression approximation operator inversion 
algorithmica appear 
smola scholkopf 
muller 
general cost functions support vector regression 
ninth australian congress neural networks appear 
alex smola bernhard scholkopf klaus robert muller 
connection regularization operators support vector kernels 
neural networks appear 
stitson gammerman vapnik vovk watkins weston 
support vector anova decomposition 
technical report royal holloway college report number csd tr 
gilbert strang 
applied mathematics 
wellesley cambridge press 
vanderbei 
interior point methods algorithms formulations 
orsa computing 
vanderbei 
loqo interior point code quadratic programming 
technical report program statistics operations research princeton university 
vapnik 
estimation dependences empirical data russian 
nauka moscow 
english translation springer verlag new york 
vapnik 
nature statistical learning theory 
springer verlag new york 
vapnik 
statistical learning theory 
john wiley sons new york preparation 
vapnik smola 
support vector method function approximation regression estimation signal processing 
advances neural information processing systems 
grace wahba 
support vector machines reproducing kernel hilbert spaces 
proceedings nips workshop support vector machines appear 
weston gammerman stitson vapnik vovk watkins 
density estimation support vector machines 
technical report royal holloway college report number csd tr 

