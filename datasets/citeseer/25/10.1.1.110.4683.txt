modelling gene expression data dynamic bayesian networks kevin murphy mian computer science division university california life sciences division lawrence berkeley national laboratory berkeley ca tel fax cs berkeley edu lbl gov interest reverse engineering genetic networks time series data 
show proposed discrete time models including boolean network model kau ss linear model haeseleer nonlinear model weaver special cases general class models called dynamic bayesian networks dbns 
advantages dbns include ability model stochasticity incorporate prior knowledge handle hidden variables missing data principled way 
provides review techniques learning dbns 
keywords genetic networks boolean networks bayesian networks neural networks reverse engineering machine learning 
possible experimentally measure expression levels genes simultaneously change time react external stimuli see wfm dlb 
amount experimental data expected increase dramatically 
increases need automated ways discovering patterns data 
ultimately automatically discover structure underlying causal network assumed generate observed data 
consider learning stochastic discrete time models discrete continuous state hidden variables 
generalizes linear model haeseleer nonlinear model weaver popular boolean network model kau ss deterministic fully observable 
fact models stochastic important known gene expression inherently stochastic phenomenon ma 
addition underlying system deterministic appear stochastic due inability perfectly measure variables 
crucial learning algorithms capable handling noisy data 
example suppose underlying system really boolean network noisy observations variables 
data set contain inconsistencies boolean network model 
giving look probable model data course requires model defined probabilistic semantics 
ability models handle hidden variables important 
typically measured usually mrna levels factors care ones include cdna levels protein levels model relationship factors measure values 
prior knowledge constrain set possible models learn 
models called bayesian belief networks bns pea method choice representing stochastic models uai uncertainty artificial intelligence community 
section explain bns show generalize boolean network model kau ss hidden markov models models widely computational biology community 
sections review various techniques learning bns data show reveal lfs special case algorithm 
section consider bns continuous opposed discrete state discuss relationship linear model haeseleer nonlinear model weaver techniques neural network literature bis 
bayesian networks bns special case general class called graphical models nodes represent random variables lack arcs represent conditional independence assumptions 
undirected graphical models called markov random fields mrfs see wms application biology simple definition independence sets conditionally independent nodes separated graph 
contrast directed graphical models bns complicated notion independence bayes ball algorithm 
sets conditionally independent separated pea way ball get graph 
hidden nodes nodes values known depicted unshaded observed nodes shaded 
dotted arcs indicate direction flow ball 
ball pass hidden nodes convergent arrows top left observed nodes outgoing arrows 
see sha details 
takes account directionality arcs see 
graphical models directed undirected arcs called chain graphs 
bn intuitively regard arc indicating fact thata causes 
formal treatment causality context bns see hs :10.1.1.51.3370
evidence assigned subset nodes subset nodes observed bns causal reasoning known causes unknown effects diagnostic reasoning known effects unknown causes combination 
inference algorithms needed briefly discussed section 
note nodes observed need inference want learning 
addition causal diagnostic reasoning bns support powerful notion explaining away node observed parents dependent rival causes explaining child value see bottom left case 
contrast undirected graphical model parents independent child separates separate 
important advantages directed graphical models undirected ones include fact bns encode deterministic relationships easier learn bns see section separable models sense fri 
shall focus exclusively bns 
careful study relationship directed undirected graphical models see pea whi lau 
interesting note theory underlying graphical models involves concepts chordal triangulated graphs gol arise areas computational biology evolutionary tree construction perfect physical mapping interval graphs 
markov chain represented dynamic bayesian net dbn 
hidden markov model hmm represented dbn 
shaded nodes observed nodes hidden 
relationship hmms example bn consider 
call dynamic bayesian net dbn represents random time time tt slices shown 
graph see independent path bayes ball 
course order markov property states independent past 
consider hidden 
observe time step random variable distribution depends xt 
graph captures conditional independence assumptions hidden markov model hmm rab 
addition graph structure bn requires specify conditional probability distribution cpd node parents 
hmm assume hidden state discrete distribution pr xt tt 
transition matrix time 
observed discrete specify distribution pr yt ot 
observation matrix time 
hmm possible observed variables gaussian case specify mean covariance value hidden state variable value oft pr see section 
hopefully 
familiar example hmms consider way aligning protein sequences 
case hidden state variable take possible values xt fd mg represent delete insert match respectively 
protein alignment refer time position static sequence 
important difference gene expression represent time see section 
observable variable take possible values represent possible amino acids plus gap alignment character 
probability distribution values depends current current state system xt 
distribution pr profile time invariant background distribution pr yt allt 
dbn represented boolean network 
hmm learning means finding estimates see section structure model fixed 
relationship boolean networks consider dbn 
show time slices dbn structure repeats 
just hmm thought single stochastic automaton thought network interacting stochastic automata state automaton timet example state determined previous state previous state ofb 
interconnectivity automata represented 
presence directed cycles means bn arc node ito inter slice adjacency matrix 
addition connections time slices allow connections time slice intra slice connections 
model occurrence effects turn turned 
relationship causal natural model undirected arc resulting model chain graph don consider 
transition matrix automaton pr bt called cpt conditional probability table represents cpd tabular form 
row cpt contains single non zero value automaton fact deterministic automaton 
nodes automata deterministic states system equivalent boolean network kau ss 
boolean network learning means finding best structure inter slice connectivity matrix see section 
know correct structure easy logical rule node exhaustively enumerating finding fits data 
stochastic boolean networks define stochastic boolean network model modifying cpds 
example popular cpd uai community noisy model pea 
just gate certain th input flipped 
example pr noisy gate modelled bn 
noisy version noisy version deterministic indicated double ring 
shaded nodes observed non shaded nodes hidden 
represent cpd tabular form follows note cpt entries number parents constraints entries 
start requirement pr ja pr ja values columns resulting entries 
entries remaining columns functions parameters qk 
called implicit parametric distribution advantage data needed learn parameter values see section 
contrast full multinomial distribution unconstrained cpt model arbitrary interactions parents 
noisy gate input independent cause may inhibited 
modelled bn shown 
common imagine parents permanently child turn spontaneously 
leak node catch unmodelled causes 
generalize noisy gate nonbinary variables functions hen sri hb 
possible loosen assumption causes independent mh 
popular compact representation cpds uai community decision tree 
stochastic generalization concept function kau popular boolean networks field 
function inputs property takes specific value output function independent remaining inputs 
comparison dbns hmms note convert model defining new variable state space cross product original variables xt pr pr bt pr ct pr ct bt ct collapsing original model chain 
conditional independence relationships buried inside transition matrix 
particular entries transition matrix products smaller number parameters transition matrix correspond absence arcs model 
ift just means automaton get says connections underlying state variables 
markov chains hmms representation sparse discrete models kind considering 
higher order models assumed models order markov property connections adjacent time slices 
assumption loss generality easy convert higher order markov chain order markov chain simply adding new variables called lag variables contain old state clamping transition matrices new variables identity matrix 
note doing hmm opposed dbn blow state space exponentially 
relationship probabilistic models emphasis section models relevant gene expression bayesian nets evolutionary tree construction genetic pedigree analysis 
fact peeling stewart algorithm invented decade rediscovered uai community 
bayesian networks augmented utility decision nodes create influence diagram compute optimal action sequence actions optimal decision theoretic sense maximizing expected utility useful designing gene knockout experiments don pursue issue 
learning bayesian networks section discuss learn bns data 
problem summarized follows 
structure observability method known full sample statistics known partial em gradient ascent unknown full search model space unknown partial structural em full observability means values variables known partial observability means know values variables 
principle measured case usually called hidden variables just happen unmeasured training data case called missing variables 
note variable observed intermittently 
unknown structure means know complete topology graph 
typically know parts know properties graph common assume bound maximum number parents fan node take may know nodes certain type connect nodes type 
prior knowledge hard constraint soft prior 
exploit soft prior knowledge bayesian methods additional advantage return distribution possible models single best model 
handling priors model structures quite complicated discuss issue see hec review 
assume goal find single model maximizes scoring function discussed section 
consider priors parameters simpler 
section consider dbns variables continuous see numerical priors proxy structural priors 
interesting approximation full blown bayesian approach learn mixture models different structure depending value hidden discrete mixture node 
done gaussian networks discrete undirected trees unfortunately severe computational difficulties generalizing technique arbitrary discrete networks 
restrict attention learning structure inter slice connectivity matrix dbn 
advantage need worry introducing directed cycles arrow time disambiguate direction causal relations know arc go vice versa network models temporal evolution physical system 
course completely overcome problem correlation causation possible correlation due hidden common causes observed common effects explaining away phenomenon 
models learned data subject experimental verification 
particularly relevant example applies algorithm problem classifying dna splice sites exon intron 
examined tree mixture find nodes commonly connected class variable nodes vicinity splice junction cpts encoded known ag pattern 
hmc discusses bayesian techniques learning causal static networks sgs discusses constraint techniques see fre critique approach 
techniques social sciences 
major advantage molecular biology usually feasible structure model known learning task parameter estimation 
focus learning structure calls parameter learning subroutine start discussing case 
mention learning bns huge subject touch aspects consider relevant learning genetic networks time series data 
details see review papers bun bun hec 
known structure full observability assume goal learning case find values parameters cpd maximizes likelihood training data assumed independent observed values slice oft slices 
notational simplicity assume sequence length 
imagine unrolling slice dbn produce static bn 
assume parameter values nodes constant tied time contrast assumption hmm protein alignment time series length get data point sample cpd initial slice xm andt data points cpds 
reliably estimate parameters nodes yip xi yip xi slice usually assume fixed apriori 
leaves samples remaining cpds 
cases small compared number parameters require fitting numerical prior regularize problem see section 
case call estimates maximum map estimates opposed maximum likelihood ml estimates 
bayes ball algorithm see easy see node conditionally independent parents taken definition bn chain rule probability find joint probabilityof nodes graph wherem number nodes unrolled network excluding slice logp xi dl pa xi parents nodes numbered topological order parents children 
normalized loglikelihood training setl nlog pr djg fd dsg sum terms node verify hypotheses experimentally 
trying estimate parameters nodes initial slice omitted contribution probability simplicity 
see log function decomposes structure graph maximize contribution log likelihood node independently assuming parameters node independent nodes 
parameter estimation discrete nodes cpts compute ml estimates simple counting 
known hmm literature ml estimates cpts prone sparse data problems solved mixtures dirichlet priors pseudo counts 
estimating parameters noisy distributions relatives harder essentially presence hidden variables see techniques discussed section 
choosing form cpds discussion assumed knew form cpd node 
uncertain approach mixture distribution introduces hidden variable things complicated see section 
alternatively decision tree table parent values associated non zero probabilities fg represent cpd 
increase number free parameters gradually number parents 
known structure partial observability variables observed likelihood surface multimodal iterative methods em lau gradient ascent find local maximum ml map function 
algorithms need inference algorithm compute expected sufficient statistics related quantity node 
inference bns inference bayesian networks huge subject go 
see jen commonly algorithm junction tree algorithm 
hd gives cookbook junction tree algorithm 
shj explains forwards backwards algorithm special case junction tree algorithm place start familiar hmms 
dec place start familiar peeling algorithm junction tree approach efficient learning 
mur discusses inference bns discrete continuous variables efficient techniques handling networks observed nodes 
exact inference densely connected discrete bns computationally approximate methods 
approaches including sampling meth ods mcmc mac variational methods mean field 
dbns computationally sense connections slices sparse correlations arise time steps rendering unrolled network effectively dense 
simple approximate inference algorithm specific case sparse dbns described bk bk 
unknown structure full observability start discussing scoring function select models discuss algorithms attempt optimize function space models examine computational sample complexity 
objective function model selection commonly pr gjd pr djg pr assumed goal find model maximum likelihood 
pr re algorithm lfs analysis ra general systems theory community kli kri stated finding model sum mutual information mi pr ct node parents maximal appendix prove objective functions equivalent sense rank models order 
trouble ml model complete graph largest number parameters fit data best 
principled way avoid kind fitting put prior models specifying prefer sparse models 
bayes rule map model maximizes logs find log pr gjd log pr djg log wherec log pr constant independent ofg 
effect prior equivalent penalizing overly complex models minimum description length mdl approach 
exact bayesian approach model selection usually unfeasible involves computing marginal pgp sum exponential number models see section 
asymptotic approximation posterior called bic bayesian information criterion defined follows log logn pr gjd log pr djg number samples gis dimension model gis ml estimate parameters 
fully observable case dimension model subsets lattice 
th row represents subsets 
term computed equation 
second term penalty model complexity 
number parameters model sum number parameters node see bic score decomposes just log likelihood equation 
techniques invented maximizing mi bic case decomposable scoring function 
algorithms finding best model score decomposable learn parent set node independently 
nk sets arranged lattice shown 
problem find highest scoring point lattice 
approach taken reveal lfs start bottom lattice evaluate score points successive level point score 
scoring function pa pa mutual information andh entropy ofx see appendix definitions highest possible score corresponds pa perfect predictor ofx 
normalized mi achievable deterministic relationship 
stochastic relationships decide gains mi produced larger parent set worth 
standard approach ra community uses fact mil number samples 
test decide increase mi statistically significant 
gives kind confidence measure connections learn 
alternatively complexity penalty term bic score 
course know achieved maximum possible score know searching evaluate points lattice obviously branch bound 
large common approach number free parameters 
model hidden variables 
nodes compact representations cpds lower penalty allow connections form rejected fg 
search assume bound maximum number parents node takes nk time 
unfortunately real genetic networks known genes high fan fanout restricting bound tok impossible discover important master genes 
obvious way avoid exponential cost need bound heuristics avoid examining possible subsets 
fact heuristics kind problem learning optimal structure np hard chg 
approach ra framework called extended dependency analysis eda con follows 
start evaluating subsets size keep ones significant sense mi target node take union resulting set set parents 
disadvantage greedy technique fail find set parents subset size significant mi target variable 
monte carlo simulation con shows random relations property 
addition highly interdependent sets parents fail pairwise mi test violate causal independence assumption necessary justify noisy similar cpds 
alternative technique popular uai community start initial guess model structure specific point lattice perform local search evaluate score neighboring points lattice move best point reach local optimum 
multiple restarts try find global optimum learn ensemble models 
note partially observable case need initial guess model structure order estimate values hidden nodes expected score model see section starting fully disconnected model bottom lattice bad idea lead poor estimate 
sample complexity addition computational cost important consideration amount data required reliably learn structure 
deterministic boolean networks issue addressed statistical physics perspective computational learning theory combinatorial perspective 
particular prove fan bounded number samples needed identify boolean network nnodes lower bounded upper bounded 
unfortunately constant factor exponential ink quite large certain genes remarked 
analyses assume tabula rasa approach practice strong prior knowledge structure parts reduce data requirements considerably 
bn hidden 
simplest network capture hidden variable created arc reversal node elimination 
binary nodes assume full cpts network independent parameters second 
scaling large networks genes yeast simultaneously measured dlb clear preprocessing exclude irrelevant genes just try learn connections rest 
change expression level experiment 
dlb dataset example consists times steps genes showed significant change rest completely uninformative 
course big techniques discussed pre process clustering genes similar timeseries 
section discuss techniques learning structure dbns continuous state scale better methods discussed discrete models 
unknown structure partial observability come hardest case structure unknown hidden variables missing data 
difficulty score decompose 
observed fri fri fmr expected score decompose iterative method alternates evaluating expected score model inference engine changing model structure local maximum reached 
called structural em sem algorithm 
sem succesfully learn structure discrete dbns data fmr 
inventing new hidden nodes far structure learning meant finding right connectivity pre existing nodes 
interesting problem inventing hidden nodes demand 
hidden nodes model compact see 
standard approach keep adding hidden nodes time part network see performing structure learning step score drops 
problem choosing cardinality num ber possible values hidden node type cpd 
problem choosing add new hidden node 
point making child hidden children marginalized away need find existing node needs new parent current set possible parents adequate 
rm heuristic finding nodes need new parents consider noisy node nearly non leak parents indicator missing parent 
generalizing technique noisy ors interesting open problem 
approach high means current set parents inadequate explain residual entropy pa best bic sense set parents able find current model suggests need create new node add pa 
simple heuristic inventing hidden nodes case dbns check markov property violated particular node 
suggests need connections slices back time 
equivalently add new lag variables see section connect 
note reasoning multiple time slices forms basis model free correlational analysis approach ar asr 
heuristic dbns perform clustering timeseries observed variables see associate hidden nodes cluster 
result markov model treestructured hidden backbone 
possible approach problem learning hierarchically structured dbns 
building hierarchical object oriented bns dbns hand straightforward algorithms exploit modular structure certain extent speed inference kp 
course interpreting meaning hidden nodes tricky especially switch interpretation true false states assuming simplicity hidden node binary provided permute parameters appropriately 
symmetries cause multiple maxima likelihood surface 
opinion fully automated structure discovery techniques useful hypothesis generators tested experiment 
dbns continuous state assumed simplicity random variables discrete valued 
reality quantities care genetic networks continuous valued 
coarsely quantize continuous variables convert small number discrete values lose lot information finely quantize resulting discrete model parameters 
better continuous variables directly 
atx create bn continuous variables need specify graph structure conditional probability distributions node 
common distribution gaussian analytically tractable 
consider example dbn normal gaussian distribution evaluated normalizing constant ensure density integrates weight regression matrix 
denotes transpose ofx 
way writing independent gaussian noise terms corresponding unmodelled influences 
called order auto regressive ar time series model ham 
vector containing expression levels genes timet corresponds precisely model 
explicitely mention gaussian noise implicit decision squares fit see section 
despite simplicity model particular linear nature find models experimental data 
xt xt xt wheren exp define nonlinear models 
example de sigmoid function obtain nonlinear model 
simple extend linear nonlinear models allow external yt xt authors point 
example inject agonist causes seizures localized cell death severely disrupts normal gene expression patterns rat cns measure expression levels 
model bkt injection exp hours injection vectors representing response offset bias term respectively 
observed 
consider observed gaussian cpds 
hidden discrete variables yt hmm gaussian outputs 
hidden continuous variables cx 
called linear dynamical system dynamics observations linear 
called state space model 
perform inference lds wellknown kalman filter algorithm bsl just special case junction tree algorithm 
dynamics non linear inference harder standard technique local linear approximation called extended kalman filter ekf similar techniques developed recurrent neural networks literature pea 
reason widespread success hmms gaussian outputs discrete hidden variables approximate arbitrary non linear dynamics training data 
ifw rest stick case fully observable possibly non linear models simplicity inference necessary 
consider bayesian methods parameter learning thought inferring probable values nodes represent parameters 
learning assumed vector random variables 
represent component vector scalar node inter slice connections correspond non zero values weight matrix undirected connections slice correspond non zero entries lau 
example diagonal model equivalent 
case continuous state models convert back forth representing structure graphically numerically 
means structure learning reduces param zd exp xl jj eter fitting avoid expensive search discrete space models 
bra 
consequently discuss techniques learning parameters continuous state dbns 
consider ar model 
notational simplicity assume normalizing constant inverse variance precision 
equation find likelihood data independent ofw value zd txt 
product individual normalizing constants number samples error cost function data 
having multiple sequences essentially concatenating single long sequence modulo boundary conditions shall henceforth just single index differentiating respect setting yields txt rewrite familiar form 
define djw ed zd exp matrices ca think th row th input vector linear neural network hidden layers perceptron th row corresponding output 
notation rewrite expression follows called normal equation pseudo inverse ofx square 
see squares solution maximum likelihood ml solution assuming gaussian noise 
technique haeseleer compute squares value ofw interpret jj unspecified threshold indicating absence arc 
bishop writes bis technique little theoretical motivation performs poorly practice 
sophisticated techniques developed neural network literature involve examining change error function due small changes values weights 
requires computing hessian hof error surface 
technique optimal brain damage cds diagonal sophisticated technique optimal brain surgeon hs remove assumption :10.1.1.32.7223
believe hope 
true model sparse entries encode knowledge assumption gaussian prior zw exp ij zw exp ew weight exp ij zs exp ed ew wij yij error cost function weights 
bayes rule equation equations posterior distribution weights wjd denominator independent ofw maximum posterior map value minimizing ad hoc iterative pruning method achieve similar effect 
xi jw cost function ed ew ed second term called regularizer encourages learning weight matrix small values 
unfortunately prior favours small weights large ones fixed hp 
regularization technique called weight decay 
note regularizer overcomes problem underdetermined fewer samples parameters nn values called hyperparameters control prior weights system noise respectively 
values unknown correct bayesian approach integrate approximation works better practice approximate posterior gaussian mixture gaussians find map values plug equation see bis sec 
mac details :10.1.1.51.7418
associate separate find map values soft means entries keep called automatic relevance determination ard mac better weight deletion techniques discussed especially small data sets :10.1.1.51.7418
explained dbns discussed learn 
hope apply techniques particular ones discussed section real biological data publicly available 
logp xi xi andi xi pa xi xjy xx yp xjy logp xjy mutual information mi entropy ofx conditional entropy 
pa xi 
define mi score model mis pii xi pag xi probabilities set empirical values pag xi mean parents 
theorem 
bns node full cpt fully observable data set 
xjy equivalence mutual information maximum likelihood methods start introducing standard notation information theory ct 
carried provisional experiments subset data described wfm available rsb info nih gov mol physiol pnas html hasn ands 
applying ard technique genes involved subsystem described significant indicating fully connected graph 
got sparse graph larger unpublished data set unspecified pruning threshold mis mis pr xi kji pr xi proof see note normalized log likelihood equation rewritten pii xi pag xi xii xi xi xi pr xi kji pa xi brevity number times event xi occurs training set 
ifi define kji pr xi number times 
assume node full cpt set probabilities empirical estimates npr xi xi kpr xi log xi independent structure graph xi pag xi 
note theorem similar result cl show optimal thenf tree structured mrf maximal weight spanning tree mwst graph weight arc nodes xi xj 
extends result mixtures trees 
trees significant advantage iff models order 
models receive score ambiguous say arg arg 
sense minimizing kl divergence true estimated distribution 
xx yp logp compute mwst ino time number variables 
clear useful model gene expression data 
node full cpt parametric cpd right form capable representing true conditional probability distribution node asn nijk npr xi kjj proof goes 
leaves additional problem choosing form cpd see section 
miyano 
identification genetic networks small number gene expression patterns boolean network model 
proc 
pacific symp 
biocomputing 
ar arkin ross 
statistical construction chemical reaction mechanisms measured timeseries 
phys 
chem 
asr arkin shen ross 
test case correlation metric construction reaction pathway measurements 
science 
boutilier friedman goldszmidt koller 
context specific independence bayesian networks 
proc 
conf 
uncertainty ai 
bis bishop 
neural networks pattern recognition 
clarendon press 
bk boyen koller 
approximate learning dynamic models 
neural info 
proc 
systems 
bk boyen koller 
tractable inference complex stochastic processes 
proc 
conf 
uncertainty ai 
binder koller russell kanazawa 
adaptive probabilistic networks hidden variables 
machine learning 
bra brand 
entropic estimator structure discovery 
neural info 
proc 
systems 
bsl bar shalom li 
estimation tracking principles techniques software 
artech house 
bun buntine 
operations learning graphical models 
ai research pages 
bun buntine 
guide literature learning probabilistic networks data 
ieee trans 
knowledge data engineering 
cds le cun denker solla :10.1.1.32.7223
optimal brain damage 
neural info 
proc 
systems 
chg chickering heckerman geiger 
learning bayesian search methods experimental results 
ai stats 
cl chow liu 
approximating discrete probability distributions 
ieee trans 
info 
theory 
con 
large systems 
intl 
general systems 
ct cover thomas 
elements information theory 
john wiley 
dec dechter 
bucket elimination unifying framework probabilistic inference 
jordan editor learning graphical models 
kluwer 
durbin eddy krogh mitchison 
biological sequence analysis probabilistic models proteins nucleic acids 
cambridge university press cambridge 
dlb derisi brown 
exploring metabolic genetic control gene expression genomic scale 
science 
haeseleer wen fuhrman somogyi 
linear modeling mrna expression levels cns development injury 
proc 
pacific symp 
biocomputing 
michael eisen paul spellman patrick brown david botstein 
cluster analysis display genome wide expression patterns 
proc 
national academy science usa 
fg friedman goldszmidt 
learning bayesian networks local structure 
proc 
conf 
uncertainty ai 
friedman koller pfeffer 
structured representation complex stochastic systems 
proc 
conf 
am 
assoc 
ai 
fmr friedman murphy russell 
learning structure dynamic probabilistic networks 
proc 
conf 
uncertainty ai 
fre freeman 
association causation regression 
turner editors causality crisis 
statistical methods search causal knowledge social sciences 
notre dame press 
fri friedman 
learning bayesian networks presence missing values hidden variables 
proc 
conf 
uncertainty ai 
fri friedman 
bayesian structural em algorithm 
proc 
conf 
uncertainty ai 
geiger heckerman meek 
asymptotic model selection directed networks hidden variables 
proc 
conf 
uncertainty ai 
gol golumbic 
algorithmic graph theory perfect graphs 
academic press 
ham hamilton 
time series analysis 
wiley 
hb heckerman breese 
causal independence probability inference bayesian networks 
ieee trans 
systems man cybernetics 
hd huang darwiche 
inference belief networks procedural guide 
intl 
approx 
reasoning 
hec heckerman 
tutorial learning bayesian networks 
technical report msr tr microsoft research march 
hen henrion 
practical issues constructing belief networks 
proc 
conf 
uncertainty ai 
hertz 
statistical issues reverse engineering genetic networks 
proc 
pacific symp 
biocomputing 
hmc heckerman meek cooper 
bayesian approach causal discovery 
technical report msr tr microsoft research february 
hp hanson pratt 
comparing biases minimal network construction back propogation 
neural info 
proc 
systems 
hs hassibi stork 
second order derivatives network pruning optimal brain surgeon 
neural info 
proc 
systems 
hs heckerman shachter :10.1.1.51.3370
decision theoretic foundations causal reasoning 
ai research 
jen jensen 
bayesian networks 
ucl press london england 
jordan ghahramani jaakkola saul 
variational methods graphical models 
jordan editor learning graphical models 
kluwer 
jordan ghahramani saul 
hidden markov decision trees neural info 
proc 
systems 
kau kauffman 
origins order 
self organization selection evolution 
oxford univ press 
kli klir 
architecture systems problem solving 
plenum press 
kp koller pfeffer 
object oriented bayesian networks 
proc 
conf 
uncertainty ai 
kri krippendorff 
information theory structural models qualitative data quantitative applications social sciences 
page publishers 
lau lauritzen 
em algorithm graphical association models missing data 
computational statistics data analysis 
lau lauritzen 
graphical models 
oup 
lfs liang fuhrman somogyi 
reveal general reverse engineering algorithm inference genetic network architectures 
pacific symposium biocomputing volume pages 
ma mcadams arkin 
stochastic mechanisms gene expression 
proc 
national academy science usa 
mac mackay :10.1.1.51.7418
probable networks plausible predictions review practical bayesian methods supervised neural networks 
network 
mac mackay 
monte carlo methods 
jordan editor learning graphical models 
kluwer 
mh meek heckerman 
structure parameter learning causal independence causal interaction models 
proc 
conf 
uncertainty ai pages 
mil miller 
note bias information estimates 
editor information theory psychology 
free press 
meila jordan morris 
estimating dependency structure hidden variable 
technical report mit ai lab 
mur murphy 
variational approximation bayesian networks discrete continuous latent variables 
proc 
conf 
uncertainty ai 
submitted 
pea pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
pea pearlmutter 
gradient calculations dynamic recurrent neural networks survey 
ieee trans 
neural networks 
rab rabiner 
tutorial hidden markov models selected applications speech recognition 
proc 
ieee 
rm ramachandran mooney 
theory refinement bayesian networks hidden variables 
machine learning proceedingsof international conference 
sgs spirtes glymour scheines 
causation prediction search 
springer verlag 
print 
sha shachter 
bayes ball rational determining irrelevance requisite information belief networks influence diagrams 
proc 
conf 
uncertainty ai 
shj smyth heckerman jordan 
probabilistic independence networks hidden markov probability models 
neural computation 
sri srinivas 
generalization noisy model 
proc 
conf 
uncertainty ai 
ss somogyi 
modeling complexity genetic networks understanding pleiotropic regulation 
complexity 
thiesson meek chickering heckerman 
learning mixtures dag models 
proc 
conf 
uncertainty ai 
wfm wen fuhrman carr smith barker somogyi 
large scale temporal gene expression mapping central nervous system development 
proc 
national academy science usa 
whi whittaker 
graphical models applied multivariate statistics 
wiley 
wms white muchnik smith 
modelling protein cores markov random fields 
mathematical biosciences 
weaver stormo 
modeling regulatory networks weight matrices 
proc 
pacific symp 
biocomputing 
