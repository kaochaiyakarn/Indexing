bayesian hierarchical model learning natural scene categories li fei fei california institute technology electrical engineering dept pasadena ca usa vision caltech edu propose novel approach learn recognize natural scene categories 
previous require experts annotate training set 
represent image scene collection local regions denoted codewords obtained unsupervised learning 
region represented part theme 
previous themes learnt hand annotations experts method learns theme distributions codewords distribution themes supervision 
report satisfactory categorization performances large set categories complex scenes 

ability analyze classify accurately rapidly scene find highly useful everyday life 
thorpe colleagues humans able categorize complex natural scenes containing animals vehicles quickly 
li colleagues showed little attention needed rapid natural scene categorization 
studies posed serious challenge conventional view understand context complex scene needs recognize objects turn recognize category scene 
recognize context scene having recognized objects 
number studies approaches classify indoor versus outdoor city versus landscape sunset versus mountain versus forest global cues power spectrum color histogram information 
oliva torralba incorporated idea global frequency local spatial constraints 
key idea intermediate representations classifying scenes scenes labelled respect local global properties human observers 
similarly vogel schiele pietro perona california institute technology electrical engineering dept pasadena ca usa perona vision caltech edu intermediate representation obtained human observers learning semantic context scene 
country coast suburb tall bldg highway bedroom kitchen office ins 
city streets forest mountain 
dataset consists categories largest natural scene category dataset date 
detailed description dataset section 
main requirement approaches manual annotation intermediate properties 
human subjects instructed rank hundreds training scenes different properties ruggedness roughness 
human subjects asked classify local patches training images different semantic concepts water foliage sky 
cases involve tens hours manual labor 
works clearly point usefulness intermediate representations motivate think methods learning representations directly data hand annotating images tedious expensive expert defined labels somewhat arbitrary possibly sub optimal 
learnt studies classifying different textures materials 
traditional texture models identify large dictionary useful textons codewords 
category texture model learnt capture signature distribution textons 
loosely think texture particular intermediate representation complex scene 
methods yield model representation manually segmented training examples 
limitation traditional texture model hard assignment distribution class 
fine underlying images genuinely created single mixture textons 
hardly case complex scenes 
example critical trees occupy suburb scene houses 
fact recognize suburb scene trees just 
key insights previous appear intermediate representations improves performance intermediate representations thought textures turn composed mixtures textons codewords 
goal take advantage insights avoid manually labeled segmented images train system possible 
adapt problems image analysis blei colleagues designed represent learn document models 
framework local regions clustered different intermediate themes categories 
probability distributions local regions intermediate themes learnt automatic way bypassing human annotation 
supervision needed apart single category label training image 
summarize contribution follows 
algorithm provides principled approach learning relevant intermediate representations scenes automatically supervision 
algorithm principled probabilistic framework learning models textures codewords textons 
approaches histogram models textons special case algorithm 
flexibility hierarchy model approaches easily generalized extended framework 
model able group categories images sensible hierarchy similar humans 
introduce generative bayesian hierarchical model scene categories section 
section describes dataset different categories scenes experimental setup 
section illustrates experimental results 
discuss section results directions 
feature detection representation learning input image local feature extraction form codebook represent image bag codewords learn bayesian hierarchical models class 
approach training testing class class class class model model unknown image decide best model 
flow chart algorithm 
feature detection representation recognition fig summary algorithm learning recognition 
model image collection local patches 
patch represented codeword large vocabulary codewords fig 
goal learning achieve model best represents distribution codewords category scenes 
recognition identify codewords unknown image 
find category model fits best distribution codewords particular image 
algorithm modified latent dirichlet allocation lda model proposed blei 
differ model explicitly introducing category variable classification 
furthermore propose variants hierarchical model fig 
model structure easier understand model fig going generative process creating scene specific category 
put process plain english choosing category label say mountain scene 
mountain class draw probability vector determine intermediate theme select generating patch scene 
creating patch image determine particular theme mixture possible themes 
example rock theme selected turn privilege codewords occur frequently rocks slanted lines 
theme favoring horizontal edges chosen draw codeword horizontal line segment 
repeat process drawing theme codeword times eventually forming entire bag patches construct scene mountains 
fig graphical illustration generative model 
call model theme model 

theme model scene categorization shares intermediate level themes feature level codewords 
theme model scene categorization shares feature level codewords traditional texton model 
fig slight variation model fig 
call theme model 
specified rest focus theme model 
ready show mathematical details formulation model learn parameters 
theme models notations definitions theme model fig 
contrast explicitly terminology texture studies 
patch basic unit image defined patch membership dictionary codewords indexed 
th codeword dictionary represented vector fig shaded common convention indicate observed variable 
nodes graph unobserved shading 
equivalent image document 
codeword patch model word 
texture material literature codeword referred texton 
image sequence patches denoted xn th patch image 
category collection images denoted xi 
equivalent corpus 
write process generates image formally model 

choose category label image 
total number categories 
dimensional vector multinomial distribution 
particular image category draw parameter determines distribution intermediate themes foliage water sky distributed scene 
done choosing image 
parameter multinomial distribution choosing themes 
matrix size dimensional dirichlet parameter conditioned category total 

patches xn image choose theme zn mult 
zn dim unit vector 
indicates th theme selected rock theme 
choose patch xn xn zn matrix size number themes total number codewords codebook 
kt 
dimensional dirichlet random variable property 
conjugate distribution multinomial distribution 
themes best described discrete variable multinomial distribution dirichlet distribution natural choice describe distribution 
probability density dir ci ci ci ck parameters write full generative equation model 
joint probability theme mixture set themes patches category zn xn zn mult dir zn mult zn xn zn xn zk fig shows theme model hierarchical representation scene category model 
dirichlet parameter category category level parameters sampled process generating category scenes 
multinomial variables scene level variables sampled image 
discrete theme variable patch patch level variables sampled time patch generated 
wish model intermediate themes category sharing categories introduce link class node patch xn xn xn zn different copies size kt xt zk 
generative equations eq changed dependency due space limitation shall omit deriving learning inference rules second theme model 
release technical note online completeness 
bayesian decision show proceed learn model parameters look decisions unknown scene 
unknown image represented collection patches codewords 
keep notation image patches 
compute probability scene class parameters learnt training set 
convenience distribution assumed fixed uniform distribution omit estimate 
decision category comparing likelihood category arg maxc 
general obtained integrating hidden variables eq 
zn xn zn zn unfortunately eq tractable due coupling 
wide range approximate inference algorithms considered including laplace approximation variational approximation mcmc method 
section briefly outline variational method variational message passing vmp convenient framework carry variational inferences 
learning variational inference learning goal maximize log likelihood term log estimating optimal 
jensen inequality bound log likelihood way 
log logp logq eq log eq log arbitrary variational distribution 
letting denote rhs equation log kl second term rhs equation stands kullback leibler distance probability densities 
maximizing lower bound respect minimizing kl distance variational posterior probability true posterior probability 
eq estimate variational parameters 
substituting variational lower bound surrogate intractable marginal likelihood turn estimate model parameters 
iterative algorithm alternates steps till convergence 
soon publish technical note detailed derivations website 

step class images optimize values variational parameters 
update rules ni exp ni image index patch index digamma function 

step maximize lower bound log likelihood respect model parameters 
finding maximum likelihood estimates expected sufficient statistics computed step 
brief comparison compare hierarchical model traditional texton model texture recognition instance 
fig graphical representation traditional texton model 
see class textures materials single multinomial parameter associated class 
words generate image patches drawn single theme 
fine training data pure textures segmented manually 
themes single mixture learnt codewords suffice 
shown may extended training different models category textures different lighting view point conditions 
requires manual separations data labelling segmented textures 
section show empirically explicitly modelling intermediate themes complex scenes model achieve better recognition performances traditional texton model fig 
features codebook formulation theme model represent image collection detected patches assigned membership large dictionary codewords 
show patches obtained memberships assigned 

codebook obtained training examples categories images category 
image patches detected sliding grid random sampling scales 
codewords sorted descending order size membership 
interestingly codewords appear represent simple orientations illumination patterns similar ones early human visual system responds 
local region detection representation previous studies natural scene categorization focused global features frequency distribution edge orientations color histogram shown local regions powerful cues 
compared global features local regions robust occlusions spatial variations 
tested different ways extracting local regions 

evenly sampled grid 
evenly sampled grid spaced pixels image 
size patch randomly sampled scale pixels 

random sampling 
randomly sampled patches image 
size patch randomly sampled scale pixels 

kadir brady saliency detector 
roughly regions salient location scale extracted saliency detector 
scales interest point pixels 

lowe dog detector 
roughly regions stable rotationally invariant different scales extracted dog detector 
scales interest point vary pixels 
different representations describing patch normalized pixel gray values dim sift vector 
table compares contrasts experimental results model different local region detectors representations 
codebook formation collection detected patches training images categories learn codebook performing means algorithm 
clusters small number members pruned 
defined centers learnt clusters 
fig shows codewords learnt gray value pixel intensities 

dataset experimental setup dataset contains categories natural scenes fig highway images inside cities images tall buildings images streets images suburb residence images forest images coast images mountain images open country images bedroom images kitchen images images office images 
average size image approximately pixels 
categories provided oliva torralba collected mixture corel images personal photographs 
rest categories obtained google image search engine personal photographs 
worth noting coast forest open country mountain categories similar categories reported 
grayscale images learning recognition 
believe complete scene category dataset literature far 
categories scenes split randomly separate sets images training rest testing 
codebook codewords learnt patches drawn random half entire training set 
model category scenes obtained training images 
asked categorize test image decision category label gives highest likelihood probability 
confusion table illustrate performance models 
confusion table axis represents models category scenes 
axis represents ground truth categories scenes 
orders scene categories axes 
ideal case expect completely white diagonal line show perfect discrimination power category models categories scenes 
specified performances section quoted average value diagonal entries confusion table 
category recognition task random chance 
excluding preprocessing time feature detection codebook formation takes minutes obtain categories models training images category ghz machine 
highway inside city tall buildings street suburb forest coast mountain open country bedroom kitchen office themes codewords 
internal structure models learnt category 
row represents category 
left panel shows distribution intermediate themes 
right panel shows distribution codewords appearance codewords selected top codewords category model 
highway inside city tall building street suburb forest coast mountain open country bedroom kitchen office correct incorrect 
examples testing images category 
row category 
columns left show examples correctly recognized images column right shows example incorrectly recognized image 
superimposed image show samples patches belong significant set codewords category model 
note incorrectly categorized images number significant codewords model tends occur 
best viewed color 

results distance measure models highway inside city tall building street suburb forest coast mountain open country bedroom kitchen office hw ic tb st sb fr ct mt oc br kt lv number categories average perf 
rank statistics test 
left panel 
confusion table theme model training test examples category grid detector patch representation 
average performance 
right panel 
rank statistics confusion table shows probability test scene correctly belong top probable categories 
ranges 
probability themes forest themes top textons theme 
example themes forest category 
left panel distribution themes 
right panel codewords dominant themes category 
notice codewords theme visibly consistent 
foliage tree branch themes appear emerge automatically data 
office bedroom kitchen inside city suburb streets forest mountain open country tall building highway 
dendrogram relationship category models theme distribution 
axis pseudo euclidean distance measure models 
fig overview performance theme model 
total number themes 
closer look confusion table fig reveals highest block errors occurs indoor categories bedroom kitchen office 
way evaluate performance rank statistics categorization results fig 
best second best choices mean categorization result increases 
fig demonstrate internal structure models learnt category 
take highway category example fig 
left panel shows coast number training examples performance performance number themes performance total number codewords 
number training examples vs performance 
number themes vs performance 
number codewords vs performance 
performances quotes mean confusion table 
number significant codewords maximum number codewords available number categories learnt performance 
number significant codewords function number categories learnt 
significance defined probabilistic weight 
performance comparison theme model theme model traditional texton model 
average distribution intermediate themes generating highway images 
right panel show large number samplings average distribution codewords generating highway images 
clearly distribution codewords fig influenced distribution themes 
show right panel top codewords occur highway images 
note horizontal lines dominate top choices 
instance codewords tall building category 
see top choice codewords vertical edges case tall buildings 
indoor categories tend sharp horizontal vertical edges 
quite revealing scene statistics indoor structures 
distribution themes codewords indoor categories indicates confusion categories 
fig shows testing image examples 
establish relationship categories looking model distances see dendrogram fig 
distribution themes close categories close dendrogram 
example closest categories indoor environments 
fig illustrates different aspects algorithm descriptor grid random saliency dog pixel dim sift table 
performance comparison different feature detectors representations 
performance quoted mean confusion table similar fig 
sift representation general robust pixel representation 
sliding grid yields number patches performs detectors 
performances versus number training examples themes codewords codebook 
fig shows sharing resources codewords intermediate themes number significant codewords learning new models tend level quickly 
table shows different feature detection representation influences performance 

summary discussion proposed bayesian hierarchical model learn recognize natural scene categories 
model adaptation vision ideas proposed context document analysis previous schemes require detailed manual annotation images training database model learn characteristic intermediate themes scenes supervision human intervention achieves comparable performance see table details 
categ 
training categ 
training requirements perf 
theme model unsupervised human annotation semantic concepts patches human annotation proper ties thousands scenes table 
comparison algorithm methods 
average confusion table performances comparable categories forest mountain open country coast methods 
roughly number training examples human supervision 
fig indicates training examples model potential achieve higher performances 
model principled probabilistic framework learning automatically distribution codewords intermediate level themes thought akin texture descriptions 
fig shows model outperforms traditional texton models fixed codeword mixing pattern estimated category scenes 
way think model generalization texton models textures require samples pure texture trained 
contrast model may trained complete scenes infer intermediate themes data 
important explore relationship themes meaningful textures semantic concepts suggested 
addition provide framework share basic level codewords intermediate level themes different scene categories 
similarly number features learnt increases sub linearly number new categories increases 
tested algorithm diverse set scene types introducing number new categories op posed 
performances indoor scenes suggest model complete 
minimum need richer set features different cues hierarchy codewords able form powerful models difficult categories 
acknowledgment 
chris bishop tom minka silvio savarese max welling helpful discussions 
oliva michael fink providing parts dataset 
blei ng jordan 
latent dirichlet allocation 
journal machine learning research 
gelman carlin stern rubin bayesian data analysis 
chapman hall crc 
gorkani picard 
texture orientation sorting photos glance 
int 
conf 
pattern recognition 
kadir brady 
scale saliency image description 
international journal computer vision 
leung malik 
representing recognizing visual appearance materials dimensional textons 
ijcv june 
li koch perona 
natural scene categorization near absence attention 
proc natl acad sci usa 
lowe 
object recognition local scale invariant features 
proc 
international conference computer vision pages 
minka lafferty 
expectation propagation generative aspect model 
proc 
th conference uncertainty artificial intelligence pages 
oliva torralba 
modeling shape scene holistic representation spatial envelope 
int 
journal computer vision 
portilla simoncelli 
parametric texture model joint statistics complex wavelet coefficients 
ijcv october 
szummer picard 
indoor outdoor image classification 
int 
workshop content access image databases bombay india 
thorpe 
speed processing human visual system 
nature 
torralba murphy freeman 
sharing features efficient boosting procedures multiclass object detection 
proc 
ieee cvpr 
treisman gelade 
feature integration theory attention 
cognitive psychology 
vailaya figueiredo jain zhang 
image classification content indexing 
ieee trans 
image processing 
varma zisserman 
texture classification filter banks necessary 
cvpr pages ii 
vogel schiele 
semantic typicality measure natural scene categorization 
dagm annual pattern recognition symposium tuebingen germany 

variational message passing applications 
phd thesis university cambridge 
