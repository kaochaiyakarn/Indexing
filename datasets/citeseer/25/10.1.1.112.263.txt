database architecture optimized new bottleneck memory access peter stefan martin kersten data cwi amsterdam netherlands amsterdam netherlands nl kersten cwi nl past decade advances speed commodity cpus far paced advances memory latency 
main memory access increasingly performance bottleneck computer applications including database systems 
article simple scan test show severe impact bottleneck 
insights gained translated guidelines database architecture terms data structures algorithms 
discuss vertically fragmented data structures optimize cache performance sequential data access 
focus equi join typically random access operation introduce radix algorithms partitioned hash join 
performance algorithms quantified detailed analytical model incorporates memory access cost 
experiments validate model performed monet database system 
obtained exact statistics events tlb misses cache misses hardware performance counters modern cpus 
cost model show carefully tuned memory access pattern radix algorithms perform confirmed experimental results 
carried author university amsterdam supported sion permission copy fee part material granted provided copies distributed direct commercial advantage vldb copyright notice title publication date appear notice copying permission large data base endowment 
copy republish requires fee special permission endowment 
proceedings th vldb conference edinburgh scotland 
custom hardware workstations pcs experiencing tremendous improvements past decades 
unfortunately growth equally distributed aspects hardware performance capacity 
shows speed commercial microprocessors increasing roughly year speed commodity dram improved little past decade mow 
part reason direct tradeoff capacity speed dram chips highest priority increasing capacity 
result perspective processor memory getting slower dramatic rate 
affects computer systems making increasingly difficult achieve high processor efficiencies 
aspects memory performance interest bandwidth latency address translation 
way reduce effective memory latency appli speed mhz processors dram year hardware trends dram cpu speed cations incorporate cache memories memory subsystem 
fast expensive sram memory chips way computer boards caches 
due rising cpu clock speeds time bridge physical distance chips cpu problem modern cpus come chip cache see 
physical distance major complication designs trying reduce main memory latency 
new dram standards rambus ram sld concentrate fixing memory bandwidth bottleneck mcc latency problem 
cache memories reduce memory latency requested data cache 
mainly depends memory access pattern application 
special care taken memory latency increasing performance bottleneck preventing applications including database systems fully exploiting power modern hardware 
memory latency memory bandwidth translation logical virtual memory addresses physical page addresses severe impact memory access performance 
memory management unit mmu modern cpus translation lookaside buffer tlb kind cache holds translation typically pages 
logical address tlb translation additional cost 
tlb occurs 
tlb handled trapping routine operating system kernel translates address places tlb 
depending implementation hardware architecture tlb misses costly main memory access 
overview article investigate effect memory access cost database performance looking detail main memory cost typical database applications 
research group studied large mainmemory database systems past years 
research started prisma project focusing massive parallelism centered monet high performance system targeted query intensive application areas olap data mining 
research monet experimentation platform 
rest organized follows section analyze impact memory access costs basic database operations 
show special care taken database server running simple sequential scan table spend cycles waiting memory accessed 
memory access bottleneck difficult avoid cache line cache line memory page cpu die cpu registers cache swap file disk cache main memory bus virtual memory hierarchical memory system complex database operations sorting aggregation join exhibit random access pattern 
section discuss consequences bottleneck data structures algorithms database systems 
identify vertical fragmentation solution database data structures leads optimal memory cache usage 
concerning query processing algorithms focus equi join introduce new radix algorithms partitioned hash join 
analyze properties algorithms detailed analytical model quantifies query cost terms cpu cycles tlb misses cache misses 
model enables show algorithms achieve better performance having carefully tuned memory access pattern 
evaluate findings conclude hard data obtained experiments justify basic architectural choices monet system back intuition 
initial experiment section demonstrate severe impact memory access cost performance elementary database operations 
shows results simple scan test number popular workstations past decade 
test sequentially scan inmemory buffer iteratively reading byte varying stride offset subsequently accessed memory addresses 
experiment mimics happens database server performs read scan byte column memory table certain record width stride happen selection column zero selectivity simple aggregation max sum 
axis shows cost iterations elapsed time axis shows stride 
sure buffer memory memory caches 
elapsed time msec origin cpu mhz bytes bytes year sun cpu ultrasparc mhz bytes bytes year ultra cpu ultrasparc mhz bytes bytes year cpu sparc mhz bytes year record width bytes stride reality check simple memory scan tuples stride small successive iterations scan read bytes near memory hitting cache line 
number cache misses low 
rate reaches maximum iteration soon stride reaches size cache line bytes 
rate increases stride exceeds size cache line bytes 
certain memory read cache performance worse stays constant 
model describes depending stride execution costs iteration experiment terms pure cpu costs including data accesses chip cache additional costs due cache accesses main memory accesses tcp tl tl ml ll ml min lsl ml ml min lsl mx lx denoting number cache misses cache line sizes cache memory access latencies level respectively 
machines exhibit pattern performance degradation decreasing data locality clearly shows penalty poor memory cache usage dramatically increased years 
cpu speed improved order magnitude memory access latencies hardly changed 
fact draw sad attention paid query processing data locality advances cpu power due memory access bottleneck 
considerable growth memory bandwidth reflected growing cache line sizes solve problem data locality low 
trend improvement bandwidth latency ram sld expected continue real solutions sight 
mow proposed hide memory latency cpu issuing prefetch instructions data going accessed 
effectiveness technique database applications limited due fact amount cpu memory access tends small database operations cpu select experiment requires cycles origin 
proposal caching system computer configurable allowing programmer give cache hint specifying memory access stride going region 
specified data fetched optimizing bandwidth usage 
proposal considered custom hardware os compiler tools need provide possibility incorporate hints user programs 
memory fetch origin gets bytes sun lx gets improvement factor 
architectural consequences previous sections shown appropriate think main memory computer system random access memory 
section analyze consequences data structures algorithms database systems 
data structures default physical tuple representation consecutive byte sequence accessed bottom operators query evaluation tree typically selections projections 
case sequential scan seen performance strongly determined record width position axis 
width quickly large performance decreases item tuple shown occupies bytes relational systems 
achieve better performance smaller stride needed purpose recommend vertically decomposed data structures 
monet uses decomposed storage model ck storing column relational table separate binary table called binary association table bat 
bat represented memory array fixed size field records oid value binary units bun 
width typically bytes 
case origin machine deduce scan selection table stride takes cpu cycles iteration stride takes cycles 
words simple range select little cpu tuple cycles memory access cost stride weighs quite heavily cycles 
useful monet apply space optimizations reduce tuple memory requirements bats virtual oids generally decomposing relational table get identical system generated column oids decomposition bats dense ascending 

bats monet computes fly accessed positional lookup bun avoids allocating byte oid field 
called virtual oid void column 
apart reducing memory requirements half optimization beneficial joins semi joins performed oid columns 
join columns void monet uses positional lookup hash lookup effectively eliminating join cost 
projection phase query processing typically leads monet additional tuple reconstruction joins oid columns caused fact tuples decomposed multiple bats 
item table part price tax flag date comment air mail truck air ship air ship mail int int int float float int float char int varchar date date date char bytes width relational tuple bytes vertical fragmentation monet oid int oid float oid float oid str air mail truck air ship air ship void chr encoding bat chr str reg air truck air mail rail fob ship mail logical appearance physical data structures bytes bytes optimized bat storage byte column vertically decomposed storage bats byte encodings database columns low domain cardinality 
columns monet uses fixed size encodings byte integer values 
simple technique chosen require decoding effort values selection string mail re mapped selection byte value 
complex scheme bit compression yield memory savings decoding step required values accessed quickly counterproductive due extra cpu effort 
decoding just cost handful cycles tuple double amount cpu effort simple database operations range select experiment 
shows applying techniques storage needed bun column reduced bytes just 
query processing algorithms shortly discuss effect memory access bottleneck design algorithms common query processing operators 
selections selectivity low data needs visited best done scan select optimal data locality 
higher selectivities lehman carey lc concluded tree bucket chained hash table best data structures accelerating selections main memory databases 
ron reports tree block size equal cache line size optimal 
findings increased impact cache misses support claim lookup hash table tree cause random memory access entire relation non cache friendly access pattern 
grouping aggregation algorithms sort merge hash grouping 
sort merge table sorted group attribute followed scanning 
hash grouping scans relation keeping temporary hash table group values key give access aggregate totals 
number groups limited hash table fits cache probably cache 
hash grouping superior sort merge concerning main memory access sort step random access behavior done entire relation grouped probably fit cache 
equi joins hash join long preferred main memory join algorithm 
builds hash table smaller relation inner relation 
outer relation scanned tuple hash lookup done find matching tuples 
inner relation plus hash table fit memory cache performance problem occurs due random access pattern 
merge join viable alternative requires sorting relations cause random access larger memory region 
consequently identify join problematic operator investigate possible alternatives get optimal performance hierarchical memory system 
pass cluster different cluster buffers straightforward clustering algorithm pass bits pass bit pass bit radix cluster lower bits indicated parentheses clustered hash join showed main memory variant grace join relations partitioned hash number separate clusters fit memory cache performs better normal bucket chained hash join 
employs straightforward clustering algorithm simply scans relation clustered inserting scanned tuple clusters depicted 
constitutes random access pattern writes separate locations 
exceeds number available cache lines cache occurs exceeds number tlb entries number tlb misses explode 
factors severely degrade join performance 
improvement straightforward algorithm propose clustering algorithm cache friendly memory access pattern high values radix algorithms radix cluster algorithm splits relation clusters multiple passes see fig 

lower bits integer hash value column done sequential passes pass clusters tuples bp bits starting leftmost bits bp 
number clusters created radix cluster hp pass subdivides cluster hp bp new ones 
algorithm starts entire relation considered cluster subdivided clusters 
pass takes clusters subdivides new ones yielding clusters total 
note behaves straightforward algorithm 
joining bit radix clustered inputs black tuples hit interesting property radix cluster number randomly accessed regions hx kept low high number clusters achieved multiple passes 
specifically keep hx bx smaller number cache lines avoid cache altogether 
radix clustering column bits tuples lowest bits column hash value appear consecutively relation typically forming chunks tuples 
strictly necessary store cluster boundaries additional data structure algorithm scanning radix clustered relation determine cluster boundaries looking lower 
allows fine clusterings introducing overhead large boundary structures 
interesting note radix clustered relation fact ordered radix bits 
algorithm partitioned hash join exploit property performing merge step radix bits radix clustered relations get pairs clusters hash joined 
alternative radix join algorithm proposed fine clustering ca partitioned radix cluster radix cluster foreach cluster hash join radix join radix cluster radix cluster foreach cluster nested loop join algorithms employed radix cluster 
number clusters high radix clustering brought potentially matching tuples near 
chunk sizes small simple nested loop sufficient filter matching tuples 
radix join similar hash join sense number tuned relation cardinality divided small constant just length hash table 
constant gets radix join degenerates sort merge join knu employed sorting phase 
quantitative assessment radix cluster algorithm previous section provides tuning parameters 
number bits clustering implying number clusters 
number passes clustering 
number bits clustering pass bp 
exhaustive series experiments analyze performance impact different settings parameters 
establishing parameters settings optimal relation bits turn attention performance join algorithms varying values experiments combined gain insight join performance 
experimental setup experiments binary relations bats bytes wide tuples varying cardinalities consisting uniformly distributed unique random numbers 
join experiments join hit rate result join bat contains oid oid combinations matching tuples join index val 
subsequent tuple reconstruction cheap monet equal algorithms just include comparison 
experiments carried origin machine mhz mips processor 
system kb cache consisting lines bytes mb cache consisting lines bytes sufficient main memory hold data structures 
system uses page size kb tlb entries 
hardware event counters mips cpu sil get exact data number cycles tlb misses misses misses experiments 
data experiments formulate analytical main memory cost model quantifies query cost terms hardware events 
intel pentium family sun ultrasparc dec alpha provide similar counters 
radix cluster analyze impact parameters bp radix clustering conduct series experiments keeping parameter fixed varying remaining 
conduct experiments various numbers bits passes distributing bits evenly passes 
points depict results bat tuples remaining cardinalities behave way 
bits just pass yields best performance cf 

number clusters filled concurrently exceeds number tlb entries number tlb misses increases tremendously cf 
tlb misses decreasing performance 
bits passes perform better 
costs additional pass compensated having significantly tlb misses pass half number bits 
analogously passes bits passes bits 
number clusters pass limited number tlb entries 
second moderate increase tlb misses occurs number clusters exceeds number cache lines behavior really explain 
similarly number cache misses cache misses significantly increases num misses misses tlb misses tlb tlb tlb passes passes passes pass number bits performance model radix cluster tlb ber clusters pass exceeds number cache lines cache lines respectively 
impact additional misses total performance obvious pass doesn occur pass bits pass 
impact additional misses total performance nearly completely vanishes due heavier penalty tlb misses misses 
notice best case execution time increases number bits 
model calculates total execution costs radix cluster depending number passes number bits cardinality tc wc ml ll ml mt lb lt lb mli bp re li hp hp li li li li hp log hp li li li li mt lb bp hp re hp lb lb re lb hp lb hp re li cl li denote number cache lines relation cluster respectively re number pages relation li li total number cache lines caches lb number tlb entries 
term mli equals minimal number li misses pass fetching input storing output 
second term counts number additional li misses number clusters approaches number available li exceeds 
mt lb analogously 
due space limits omit term models additional tlb misses number clusters exceeds number available lines 
detailed description formulae mbk 
lines represent model bat tuples 
model shows accurate question remaining distribute number bits passes 
experimental results due space limits cf 
mbk showed performance strongly depend distribution bits 
origin mhz calibrated lt lb ns ll ns ns wc ns 
isolated join performance analyze impact number pure join performance including clustering cost 
points depict experimental results radix join cache misses tlb misses elapsed time different cardinalities 
lower graph shows performance radix join improves increasing number 
upper graph misses confirms cluster sizes significantly smaller size reasonable 
number cache misses explodes due cache 
limited execution time single run minutes cluster sizes significantly smaller size tlb size number tlb entries page size 
number cache misses stay constant 
performance improvement continues mean cluster size tuple 
point radix join degenerated sort merge join 
high cost radix join large cluster size explained fact performs nested loop join pair matching clusters 
clusters need kept small results indicate cluster size tuples optimal 
model calculates total execution costs radix join depending number bits cardinality tr wr ml ll ml mt lb lt lb mli re li cl li li li cl li cl li li li cl li li li mt lb re cl lb re re li cl li li li cl denotes cluster size byte lb lb denotes memory range covered lb pages 
term tr calculates costs evaluating join predicate tuple outer relation checked tuple respective cluster cost check wr 
second term represents costs creating result denoting costs tuple 
left term mli simplicity presentation assume cardinalities operands result 
misses misses tlb misses size size number bits size tuples performance model radix join misses misses tlb misses size size size tuples size number bits performance model partitioned hash join equals minimal number li misses fetching operands storing result 
right term counts number additional li misses inner loop number li lines cluster approaches number available li lines exceeds 
mt lb analogously 
lines prove accuracy model different cardinalities wr ns ns 
partitioned hash join exhibits increased performance increasing number radix bits 
shows performance increase flattens point entire inner cluster including hash table consists pages tlb entries 
fits cache comfortably 
performance decreases slightly point inner cluster fits cache 
performance reaches minimum 
fixed overhead allocation hash table structure causes performance decrease cluster sizes get small tuples clusters get numerous 
radix join provide cost model partitioned hash join th wh ml ll ml mt lb lt lb mli re li cl cl li li li cl li cl mt lb re cl cl lb lb li cl lb lb cl li lb denote byte cluster size sizes caches memory range covered lb pages respectively 
wh represents pure calculation costs tuple building hash table doing hash lookup creating result 
represents additional costs cluster creating destroying hash table 
left term mli equals minimal number li misses fetching operands storing result 
right term counts number additional li misses cluster size approaches li size exceeds 
soon clusters get significantly larger li memory access yields cache due cache length memory accesses tuple necessary building hash table doing hash lookup access actual tuple 
simplicity presentation omit formulae additional overhead allocating hash table structure cluster sizes get small 
interested reader referred mbk 
number tlb misses modeled analogously 
lines represent model different cardinalities wh ns ns 
predictions accurate 
join performance having analyzed impact tuning parameters clustering phase joining phase separately turn attention combined cluster join cost partitioned hash join radix join 
radix cluster gets cheaper radix bits radix join partitioned get expensive 
putting experimental data obtained cluster determine optimum number relation cardinality join algorithm 
turns possible strategies correspond diagonals figures partitioned hash join log clustered bits inner relation plus hash table fits cache 
strategy partitioned hash join experiments 
tlb partitioned hash join log lb clustered bits inner relation plus hash table spans lb pages 
experiments show significant improvement pure join performance tlb 
partitioned hash join log clustered bits inner relation plus hash table fits cache 
algorithm uses clustered bits previous ones really needs multi pass radix cluster algorithm straightforward pass cluster cause cache clusters 
radix radix join log clustered bits 
radix join stable performance higher startup cost needs significantly bits options 
winner large cardinalities 
compares radix join thin lines partitioned hash join thick lines bit range corresponding optimal number passes radix cluster see section 
diagonal lines mark setting belong strategies 
optimal setting join algorithm strategies partitioned hash join performs best cluster size approximately tuples min radix just tuples cluster radix min slightly better radix 
compares radix cluster strategies non partitioned hash join simple hash sort merge join 
clearly demonstrates cache conscious join algorithms perform significantly better random access algorithms 
cache conscious refer cache cache especially tlb 
shows radix algorithms improve strategy cardinalities larger require clustering passes radix join 
evaluation research brought light severe impact memory access performance elementary database operations 
hardware trends indicate bottleneck remains quite time expectation impact eventually deeper bottleneck 
database algorithms data structures designed optimized memory access outset 
sloppy implementation key algorithms features innermost level operator tree pointer swizzling object table lookup performance disaster faster cpus come rescue 
conversely careful design lead order magnitude performance advancement 
monet system development decreased memory access stride vertical decomposition choice back intuition 
provides hard backing feature fact basis performance 
simple scan experiment demonstrates decreasing stride crucial optimizing usage memory bandwidth 
concerning query processing algorithms formulated radix algorithms demonstrated experimentation algorithms form addition improvement 
modeling done show algorithms improve cache behavior join processing represents important improvement previous main memory cost models ln wk 
characterizing main memory performance coarse level procedure call magical costs factors obtained profiling methodology mimics memory access pattern algorithm modeled quantifies cost counting cache events cpu cycles 
tlb passes passes passes passes min radix min best number bits radix performance radix join thin lines vs partitioned hash join thick lines helped formulating models usage hardware event counters modern cpus 
think findings relevant mainmemory databases engineers 
vertical fragmentation memory access cost strong impact performance database systems macro level including manage disk resident data 
nyberg nbc stated techniques software assisted disk striping reduced bottleneck queries analyze large relations olap data mining read data faster processed 
observed effect drill benchmark brk commercial database product managing disk resident data run large buffer pool 
executing exclusively memory bound product measured factor slower benchmark monet system 
inclusion cache optimization techniques described able improve results benchmark extra order magnitude 
clearly shows importance main memory access optimization techniques 
monet manipulating virtual memory mappings treat management data memory large granularity 
line consideration disk resident data bottom level memory hierarchy goes virtual memory main memory cardinality sort merge simple hash tlb min radix radix min algorithm comparison cache memories cpu registers 
algorithms tuned run level memory exhibit performance lower levels radix join pure sequential access consequently runs virtual memory 
major performance bottleneck shifting memory access think mainmemory optimization data structures algorithms described increasingly decisive order efficiently exploit power custom hardware 
shown memory access cost increasingly bottleneck database performance 
subsequently discussed consequences finding data structures algorithms employed database systems 
recommend vertical fragmentation order better scarce memory bandwidth 
introduced new radix algorithms join processing formulated detailed analytical cost models explain algorithms optimal hierarchical memory systems modern computer hardware 
placed results broader context database architecture recommendations systems 
apers van den berg grefen kersten wilschut 
prisma db parallel main memory relational dbms 
ieee trans 
knowledge data eng december 
kersten 
monet geographical extensions novel approach high performance gis processing 
proc 
intl 
conf 
extending database technology pages avignon france june 
brk hl 
drill benchmark 
proc 
int conf 
large data bases pages new york ny usa june 
wilschut kersten 
flattening object algebra provide performance 
proc 
ieee int conf 
data engineering pages orlando fl usa february 
ck copeland khoshafian 
decomposition storage model 
proc 
acm sigmod int conf 
management data pages austin tx usa may 
knu knuth 
art computer programming volume 
addison wesley reading ma usa 
lc lehman carey 
study index structures main memory database management systems 
proc 
int conf 
large data bases pages kyoto japan august 
ln 
neimat 
modelling costs mm dbms 
proc 
int workshop real time databases issues applications pages newport beach ca usa march 
mbk kersten 
optimizing main memory join modern hardware 
technical report ins cwi amsterdam netherlands october 
mcc 
memory bandwidth machine balance current high performance computers 
ieee technical committee computer architecture newsletter december 
mckee wright wulf 
smarter memory improving bandwidth streamed 
ieee computer july 
mow mowry 
tolerating latency software controlled data prefetching 
phd thesis stanford university computer science department 
nbc nyberg barclay gray lomet 
risc machine sort 
proc 
acm sigmod int conf 
management data pages minneapolis mn usa may 
ram rambus technologies direct rambus technology disclosure 
www rambus com docs pdf 
ron design modeling parallel data server telecom applications 
phd thesis link ping university 
sil silicon graphics mountain view ca 
performance tuning optimization origin onyx january 
kant naughton 
cache conscious algorithms relational query processing 
proc 
int conf 
large data bases pages santiago chile september 
sld dram whitepaper 
www com documents pdf 
val valduriez 
join indices 
acm trans 
database systems june 
wk 
whang krishnamurthy 
query optimization memory resident domain relational calculus database system 
acm trans 
database systems march 
