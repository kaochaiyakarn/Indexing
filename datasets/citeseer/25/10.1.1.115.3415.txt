novelty redundancy detection adaptive filtering yi zhang jamie callan school computer science carnegie mellon university pittsburgh pa usa callan cs cmu edu addresses problem extending adaptive information filtering system decisions novelty redundancy relevant documents 
argues relevance modelled explicitly separately 
set redundancy measures proposed evaluated experiments redundancy thresholds 
experimental results demonstrate cosine similarity metric redundancy measure mixture language models effective identifying redundant documents 
categories subject descriptors information systems miscellaneous software software engineering general terms algorithm design experimentation 
information filtering systems monitor document streams find documents match information needs specified user profiles 
research information filtering focuses learning accurate identifying relevant documents example long term observations document stream periodic feedback user 
research area called adaptive information filtering system performance typically evaluated relevancy recall precision utility metrics 
common complaint information filtering systems distinguish documents contain new relevant information documents contain information relevant known 
information filtering system provide better service users identified categories documents user profile relevant ii relevant contains new information iii relevant contains new information 
permission digital hard copies part personal classroom granted fee provided copies distributed pro commercial advantage copies bear notice full citation rst page 
copy republish post servers redistribute lists requires prior speci permission fee 
sigir tampere finland copyright acm 
thomas minka department statistics carnegie mellon university pittsburgh pa usa minka cs cmu edu users decide treat relevant documents contain new information 
decision document contains new information depends relevant information document covered information documents delivered previously 
complicates filtering problem 
relevance document traditionally stateless boolean value 
document relevant regard document appears stream documents 
decisions redundancy novelty depend stream document appears 
study defined task created evaluation dataset contains known redundant documents 
model relevance redundancy separately different similarity measures relevancy redundancy 
developed tested variety redundancy measures 
sections describe efforts evaluating developing algorithms redundancy novelty detection filtering 
description problem review related 
section describes algorithms measuring redundancy 
section introduces simple thresholding algorithm deciding redundancy 
sections describe experimental methodology results 
section concludes 

redundancy novelty detection want filtering system distinguish relevant documents contain new novel relevant information relevant documents don document arrives system determine topic relevancy detection topic redundant redundancy detection 
define redundant mean relevant information document covered relevant documents delivered previously 
task identifying novel redundant documents addressed prior lack clear definition redundancy lack evaluation data 
research reported novelty redundancy defined set relevant documents ii respect previously seen documents iii opposite endpoints scale 
point particularly important 
treat novelty redundancy boolean values imply thresholding process maps value continuous redundancy novelty scale boolean value 
tested approach novelty redundancy creating definition redundancy includes duplicate near duplicate documents documents redundant content different presentation 
stream documents relevance filtering redundancy filtering novel redundant novel redundant system includes traditional document filtering relevance second stage novelty redundancy detection 
evaluation dataset judged undergraduate assessors system delivers documents novel relevant identify documents similar previously delivered relevant documents sense having topic dissimilar previously delivered documents sense containing new information 
goals contradictory may unrealistic expect single component satisfy 
observation suggests stage approach problem shown 
traditional adaptive information filtering solutions relevancy filtering 
clear type algorithm redundancy filtering defer discussion solutions section simply observe stage architecture simplify problem 
notation 
notation defined respect particular user profile 
sets documents dt document arrives time evaluated redundancy 
set documents delivered profile time dt arrives including dt 
dr set relevant documents delivered profile 
dr 
dt measure redundancy document dt di usually refers relevant document delivered dt arrived 
formulate task assumptions basis research acquiring redundancy judgements developing algorithms 
assumption redundancy new document dt depends documents user saw dt arrived 
dt dt measure 
assumption dt depends relevant documents dr user seen dt arrives dt dt dr 
assumption documents set dt redundant dt redundant 
softer assumption dt dt information filtering systems statistical retrieval models usually compute score indicating document matches profile documents scores profile specific dissemination threshold delivered 
similarly task identifying redundant documents divided subtasks calculate score measure redundant document profile identify documents scores profile specific redundancy threshold 
architecture second stage redundancy filter consists elements redundancy score calculation ii redundancy threshold learning 
adaptive filtering system architectural components defines different research agenda 
scoring mechanism requires profile specific anytime updating redundancy measures 
threshold mechanism requires threshold updating module 
focus research described 
thresholding focus implement simple threshold setting algorithm system complete enable evaluation redundancy measures context operational filtering system 

related research closely related novelty redundancy detection adaptive information filtering story detection task associated topic detection tracking tdt research 
tdt system monitors stream chronologically ordered documents usually news stories 
story detection fsd task defined detecting story discusses event 
event defined happens specific time place 
online clustering approaches common solution fsd task :10.1.1.45.9162
new stories compared clusters stories previously known events 
new story matches existing cluster describes known event describes new event 
argue concepts event novelty related solutions defined detect events novelty 
think 
fsd event task tdt researchers developed distinct set methods topic tracking 
events certain structures occurrence patterns media 
information filtering profiles contrast tend intended follow subject area relatively long period time 
distinct events occur stream relevant documents 
expect tasks sensitive different vocabulary patterns 
user profile stream relevant documents define far smaller universe documents encountered tdt task expect novelty redundancy detection filtering environment easier task fsd tdt 
fsd difficult problem far solved 
predicted actual error rates unacceptably high applications 
similarities tasks worth exploring redundancy measures investigated motivated fsd 

redundancy measures assume traditional information filtering techniques identify relevant documents recognize filtering system mistakes deliver documents relevant discard documents relevant 
simplicity assume novelty redundancy detection performed stream documents presumed relevant 
frame problem finding measure dt dr assumption section 
approach novelty redundancy detection cluster previously delivered documents measure redundancy current document distance cluster 
approach similar solutions tdt story detection problem 
concerns approach sensitive clustering accuracy strong assumptions nature redundancy think user dependant 
approach measure redundancy distance new document previously delivered document document document distance 
approach may robust clustering may better match users view redundancy 
asked assessors annotate evaluation dataset easiest identify new document redundant specific previously seen document harder identify redundant set previously seen documents 
observation allows simplify calculation dt dr setting equal value maximally similar value dt di 
dt dr dr dt di duplicate detection goal instructive case simple 
dt di exact duplicates dt di dt di high value duplicate document maximally redundant 
natural way measure dt di measures similarity distance difference dt di 
document timestamps important source evidence documents redundant delivered documents 
redundancy decisions truncating delivery history documents delivered profile reduces number documents considered reduces computational costs 
set experiments reported 
redundancy symmetric metric 
dj may cause dk viewed redundant presentation order reversed dk dj may viewed containing novel information 
simple example document dk subset paragraph longer document dj 
problem characteristic motivates exploration asymmetric forms traditional similarity distance difference measures 
different approaches redundancy detection 
simple set distance measure designed boolean set oriented document representation 
geometric distance cosine similarity measure simple metric designed bag words document representations 
variations kl divergence related smoothing algorithms complex metrics designed measure differences word distributions 
set difference set difference measure represents document set words 
novelty new document dt measured number new words smoothed set representation dt 
word wi occurred frequently document dt frequently old document di new information covered di covered dt 
words expected frequent new document tend frequent corpus tend frequent relevant documents 
may topic related stopwords words behave stopwords relevant documents stopwords corpus 
effective measure compensate types words 
set difference measure compensates corpus stopwords smoothing new document word frequencies word counts previously seen documents 
compensates topic stopwords smoothing new document word frequencies word counts delivered presumed relevant documents 
measure redundancy current document dt respect old document di 
dt di set dt set di wj set iff count wj count wj frequency word wj document number filtered documents contain wj number delivered relevant documents contain word wj set experiments learned training data 
true difference sets set dt set di set dt set di words set dt set di shouldn contribute novelty dt optimal novelty measure asymmetric 
geometric distance different geometric distance measure manhattan distance cosine distance 
manhattan distance sensitive document length cosine distance appropriate task 
prior research showed cosine distance measure useful tdt fsd task 
cosine distance symmetric measure related angle vectors 
represent document vector wn dt di cos dt di wk dt wk dt dt di study unique word dimension set tf idf score weight dimension 
distributional similarity probabilistic language models shown promise identifying relevant documents ad hoc ir tasks 
language model approach document represented unigram word distribution kullback leibler divergence distributional similarity measure way measure redundancy document 
dt di kl dt di wi dt log wi wi dt language model document multinomial distribution 
maximum likelihood estimation mle wi tf wi tf wj problem mle word occurs document get zero probability wi 
word dt di kl dt 
smoothing techniques necessary adjust maximum likelihood estimation kl measure appropriate 
prior research shows retrieval performance highly sensitive smoothing parameters 
smoothing methods applied ad hoc information retrieval text classification :10.1.1.14.5443:10.1.1.136.8113
prior research selected methods bayesian smoothing dirichlet priors shrinkage 
bayesian smoothing dirichlet priors approach smoothing uses conjugate prior multinomial distribution dirichlet distribution 
dirichlet distribution parameters wn posterior distribution bayesian analysis tf wi wi wi tf wj wj experiments wj dt set wj wj 
smoothing shrinkage approach smooths shrinking parameter estimates sparse data estimates rich data :10.1.1.14.5443:10.1.1.14.5443
special case general jelinek mercer smoothing method involves deleted interpolation estimation linearly interpolated gram models 
estimating language model document shrink mle estimator mle mle estimator language model general english mle mle estimator language model topic mle mle mle mle 
estimated documents filtering system processed estimated documents filtering system delivered presumed relevant documents 
derive empirical optimal values leave cross validation described :10.1.1.14.5443
experiment leave 
mixture model section introduce new algorithm generative model document creation 
approach uses qe general english le mt qt topic lt mi qd core new information ld core mixture model generating relevant documents 
probabilistic language models kl distance described section 
new mixture model measure novel view relevant documents generated 
view language model smoothing algorithm designed specifically task 
shown assume relevant document generated mixture language models general english language model user specific topic model document specific information model core 
word wi document generated language models probability core respectively wi core core ep wi tp wi wi core core 
example information need star wars relevant document words probably come general english model words star wars probably come topic model document title martin contract star wars site words martin generated new information model core 
core information topic core core information particular relevant document 
dt di kl dt core core fix core exists unique optimal value document core model core maximizes likelihood document 
core argmax core core core points worth noting mixture model 
equations look similar computations performed final model acquired calculate kl divergence quite different 
equation uses shrinkage increase probability words occur frequently topic general english occur frequently document equation uses mixture model decrease probability words 
shrinking mle mle shrinkage smoothing reduces distance documents due words reducing effect redundancy measure 
mixture model directly decrease probability words reduce effect 
fix values core 
train core get core core mle unsmoothed language model document benefit smoothing lost 
model intentionally focus new document avoids contradiction identifying relevance novelty 
task deliver relevant documents learning algorithm try recognize documents similar delivered relevant documents training data 
task deliver documents containing novel information learning algorithm avoid documents similar delivered 
model introduces core means measure relevancy redundancy focused different parts document 
relevancy system focus redundancy focus core 
tasks longer contradictory train score em algorithm find language model similar problems :10.1.1.17.4542

redundancy thresholds observed human assessors making redundancy decisions annotators working topics disagreed 
disagreement due differences assessors internal definition redundancy 
assessor feel document dt considered redundant previously seen document di covered dt assessor consider redundant coverage exceeded 
person tolerance redundancy modeled user dependent threshold converts redundancy score redundancy decision 
user feedback documents redundant serves training data 
time system learn estimate probability new document redundancy score considered redundant user thinks dt redundant dt dr 
step process maps document dt dimensional space dt dr redundancy measure learns training data probability redundancy value dimension 
approach similar spirit adaptive filtering systems identify relevant documents 
ideally optimization goal set deciding kind threshold setting algorithm 
step simple algorithm setting thresholds 
solution intentionally simple part lack adequate test collection labelled redundant information profile part problem research focus 
algorithm learning redundancy thresholds initialize redundancy threshold value high redundant documents near duplicates considered redundant document dt delivered means dt dt arrives ask user document redundant 
document redundant dt di di dr dt dt clearly weak algorithm decreases threshold 
threshold low method increasing 
effectiveness algorithm explored section 
experimental methodology ap news wall street journal dataset created gigabyte dataset combining ap news wall street journal data trec cds 
chose corpora widely available information needs relevance judgements available nist newswire corpora cover time period topics guaranteeing redundancy document stream 
documents ordered chronologically 
trec topics simulated user profiles 
decision collect redundancy assessments depends part view task 
viewed redundancy relationship document dt set documents example subset documents delivered particular profile impossible collect redundancy assessments 
need enumerate possible subsets documents delivered time ask assessors judge dt redundant respect set 
number possible subsets impractical small values know real world redundancy set documents delivered previously model relationship pairs documents 
approach adopted developing algorithms decision part intended collect redundancy judgements 
hired undergraduate students research read relevant documents profile chronological order provide redundancy judgments 
decision restrict attention relevant documents assumption section consistent filtering system component decisions relevance 
assessors judged topic time 
instructed decision document information contained redundant document seen previously topic identify prior document 
topic judged assessors differences resolved assessors 
believe operational environments different people different definitions redundancy different redundancy thresholds 
modelled environment giving assessors precise definition 
provided degrees redundancy absolutely redundant somewhat redundant assessors apply expectations system behave 
assessor thought person definitely want read dt absolutely contained new information dt marked absolutely redundant 
asses precision pt avg 
recall precision dataset set difference cosine distance lm shrinkage lm dirichlet prior lm mixture model recall comparing redundancy measures ap news wall street journal data 
documents considered redundant assessor marked absolutely redundant somewhat redundant 
sor thought new document new information person want read document redundant prior document document marked somewhat redundant 
documents completely redundant somewhat redundant marked novel 
example redundancy assessments shown 
field profile id second field document id redundant document 
subsequent document ids documents preceded stream redundant 
indicates document partially redundant 
ap ap user read document ap ap somewhat redundant 
ap ap user read document ap ap absolutely redundant 
ap ap ap user read ap ap ap absolutely redundant 
average records trec topic note single record may relate document prior documents 
records profile absolutely redundant rest represent partial 
students reported choice corpus old topics dull task unable collect assessments topics 
results set assessments profiles 
trec interactive dataset combined dataset trec trec trec interactive tracks create test dataset containing documents financial times london 
trec topics defines user profile 
topic trec assessors identified instances 
different instances different aspects topic 
document topic precision pt avg 
recall precision dataset set difference cosine distance lm shrinkage lm dirichlet prior lm mixture model recall comparing redundancy measures ap news wall street journal data 
documents considered redundant assessor marked absolutely redundant 
language model lm measures shrinkage dirichlet prior smoothing perform equally overlap 
mapped multiple instances topic 
mapping relevant documents instances provided nist 
evaluation treated instance aspect topic assumed user wants see document aspect 
document redundant user seen document instance new document belongs 
dataset created explicitly redundancy detection matched task ap news wall street journal dataset described 
felt second dataset isn perfect useful source information 
evaluation methodology believe important evaluate particular component system metric affected strengths weaknesses parts system 
case factor filtering system identifies relevant documents sets redundancy thresholds 
experiments assume filtering system identifies relevant documents precision recall evaluating redundancy filtering stream documents marked relevant nist assessors 
tests evaluate effectiveness algorithms factor effect redundancy threshold algorithm reporting average precision recall figures redundant documents 
precision recall known metrics ir community 
adapt redundancy detection task shown 
redundancy precision redundancy mistake redundancy recall correspond number documents measure recall precision mistake set distance cosine distance lm shrinkage lm dirichlet prior lm mixture model table average performance different redundancy measures simple thresholding algorithm measured topics ap news wall street journal dataset 
absolutely redundant somewhat redundant documents treated redundant 
fall categories redundant non redundant delivered delivered simplicity precision recall refer redundancy precision redundancy recall rest 

experimental results redundancy measures described section compared datasets described sections 
redundancy score calculated relevant document dt relevant documents di preceded document stream 
results shown figures form average recall precision graphs set redundant documents 
datasets set difference measure accurate 
representing document set boolean word features smoothing add delete additional words ineffective 
traditional cosine similarity metric geometric distance measure effective 
result small surprise cosine similarity justified theoretically language modelling approaches 
cosine similarity metric symmetric expected asymmetric measures better model task 
cosine similarity demonstrated times tasks robust similarity metric 
results add redundancy detection long list tasks effective 
results language modelling algorithms confirm prior research showing importance selecting smoothing algorithm 
mixture model approach consistently accurate smoothing algorithms corpora 
effective cosine similarity measure trec interactive track dataset 
approach mixing information corpus topic document language models provides new point view model documents deliver improved effectiveness compared language modelling approaches 
result completely surprising algorithm explicitly models new document 
results suggest robust cosine similarity measure 
implemented simple threshold setting algorithm section 
threshold setting algorithm simple weak part set optimization goal specifying relative rewards penalties delivering precision pt avg 
recall precision trec interactive dataset set difference cosine distance lm shrinkage lm dirichlet prior lm mixture model recall comparing redundancy measure trec interactive track data 
document aspect covered previously delivered documents considered redundant 
measure recall precision mistake set distance cosine distance lm shrinkage lm dirichlet prior lm mixture model table average performance different redundancy measures simple thresholding algorithm measured topics ap news wall street journal dataset 
absolutely redundant documents treated redundant 
novel redundant documents 
simple algorithm analyze different redundancy measures 
particular provides accurate indication user see operational environment 
tables summarize effectiveness redundancy measures simple redundancy threshold algorithm 
results reported datasets 
metrics described section 
evaluate redundancy measures percentage mistakes cosine similarity mixture model redundancy measures better rest 
measures yields reasonably low percentage mistakes strict definition redundancy table satisfying percentage somewhat redundant documents treated redundant table 
result implies simple redundancy threshold algorithm models user treating somewhat redundant novel 
know threshold setting algorithm important system accuracy hypothesize threshold setting depends user 
table shows reasonably accuracy possible thresholding algorithm matched task 

research reported step adaptive information filtering systems learn identify documents novel redundant addition relevant nonrelevant 
defines task evaluation methodol measure recall precision mistake set distance cosine distance lm shrinkage lm dirichlet prior lm mixture model table average performance different redundancy measures simple thresholding algorithm measured topics trec interactive dataset 
ogy set novelty redundancy measures 
reusable corpus created generally available documents set redundancy judgements created existing corpus adapted task 
experimental results demonstrate possible identify redundant documents reasonable accuracy 
demonstrate importance suitable redundancy threshold algorithm analogous algorithm information filtering systems 
results suggest algorithm depend user model redundancy 
extremely small amount training data available relevance adaptive filtering challenging problem 
proposed measures assessing new document respect previously seen stream documents 
experimental results demonstrate known cosine similarity metric effective new task 
demonstrate new metric mixture language models effective cosine similarity metric cases 
believe metric mixture language models important contribution effective algorithm task 
believe viewing documents mix information covered corpus topic new information models appropriate model information filtering task 
results reported attempt apply approach realistic task expect see attempts 
research attempt redundancy novelty detection adaptive filtering environment open problems research 
research profile specific anytime updating redundancy measures just scratches surface 
cosine similarity worked experiments believe underlying redundancy relationship asymmetric asymmetric measures eventually accurate 
features timestamp document source phrases proper names important sources evidence novelty decisions 
research measured redundancy pairs easy assess easy model underlying task probably better modelled comparing clusters delivered documents new document 
best choice may problem specific depending corpus profile characteristics 

acknowledgments john lafferty zhai yiming yang chun jin wei xu helpful discussions recommendations 
material supported air force research laboratory contract 
opinions findings recommendations expressed authors necessarily reflect sponsors 

allan carbonell doddington yamron yang 
topic detection tracking pilot study 
topic detection tracking workshop report 

allan lavrenko jin 
story tdt hard 
proc 
th international conference information knowledge management 
allan papka lavrenko 
line new event detection tracking 
proc 
st annual international acm sigir conference research development information retrieval 
carbonell yang brown jin zhang 
cmu tdt report nov 
topic detection tracking workshop report 

franz ward 
story detection combining similarity novelty approaches 
topic detection tracking workshop report 
jones furnas 
pictures relevance 
journal american society information science 
kraaij hiemstra 
trec language technology information retrieval 
proceedings eighth text retrieval conference trec 
lee 
measures distributional similarity 
proceedings th acl 
mccallum rosenfeld mitchell ng :10.1.1.14.5443
improving text classification shrinkage hierarchy classes 
proceedings eighteenth international conference machine learning 
miller leek schwartz 
hidden markov model information retrieval system 
proceedings th annual international acm sigir eon research development information retrieval pages 
robertson 
threshold setting adaptive filtering 
journal documentation 
robertson hull 
trec filtering track report 
ninth text retrieval conference trec 
kraaij 
tno tdt language model topic detection 
topic detection tracking workshop report 

stokes 
combining semantic syntactic document classifiers improve story detection 
proceedings th annual international acm sigir eon research development information retrieval 
yamron van 
dragon tracking detection systems tdt evaluation 
proceedings broadcast news transcription understanding workshop 
zhai lafferty :10.1.1.17.4542
model feedback language modeling approach information retrieval 
proceedings tenth international conference information knowledge management 
zhai lafferty 
study smoothing methods language models applied ad hoc information retrieval 
proc 
th annual int acm sigir eon research development information retrieval pages 
zhang callan 
maximum likelihood estimation thresholds 
proc 
th annual int acm sigir eon research development information retrieval 
