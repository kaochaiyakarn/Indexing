computing optimal policies partially observable decision processes compact representations craig boutilier david poole department computer science university british columbia vancouver bc canada cs ubc ca poole cs ubc ca phone fax partially observable markov decision processes provide general model decision theoretic planning problems allowing trade offs various courses actions determined conditions uncertainty incorporating partial observations agent 
dynamic programming algorithms information belief state agent construct optimal policies explicit consideration past history high computational cost 
discuss structured representations system dynamics incorporated classic pomdp solution algorithms 
bayesian networks structured conditional probability matrices represent pomdps representation structure belief space pomdp algorithms 
allows irrelevant distinctions ignored 
apart speeding optimal policy construction suggest representations exploited great extent development useful approximation methods 
briefly discuss difference perspective adopted influence diagram solution methods vis vis pomdp techniques 
keywords decision theoretic planning partially observable mdps influence diagrams structured representation crucial aspects intelligent agenthood planning constructing course action appropriate circumstances 
planning models ai attempted deal conditions incomplete knowledge true state world possible action failures multiple goals planning begun quantify uncertainties account competing objectives 
increased interest decision theoretic planning lead planning algorithms essentially solve multistage decision problems indefinite infinite horizon 
result representations commonly uncertain reasoning community bayes nets bns influence diagrams ids equivalents adopted planning purposes see brief survey 
useful underlying semantic model dtp problems partially observable markov decision processes pomdps 
model operations research stochastic control accounts tradeoffs competing objectives action costs uncertainty action effects observations provide incomplete information world 
general problems typically specified terms state transitions observations associated individual states specifying problem terms problematic state space system grows exponentially number variables needed describe problem 
influence diagrams bayesian networks provide natural way specifying dynamics system including effects actions observation probabilities exploiting problem structure independencies random variables 
problems specified compactly naturally 
addition algorithms solving influence diagrams exploit independencies computational gain 
unfortunately difficulties associated id solution methods association policy observed history agent stage best decision conditioned previous observations actions taken 
exponential blow state space partially alleviated size single policy grows exponentially horizon 
pomdp algorithms community adopt different perspective problem 
agent observable history explicitly recorded belief state probability distribution state space constructed summarizes history 
stage agent belief state sufficient determine expected value subsequent action choice :10.1.1.50.603
converting partially observable process completely observable process belief space history independent dynamic programming algorithms 
belief space continuous sondik shown properties space exploited give rise finite algorithms solving pomdps partitioning belief space appropriately 
algorithms computationally intensive relying representations parts value functions belief states require entries state system 
addition number partitions needed may grow exponentially horizon 
current algorithms feasible problems tens states :10.1.1.53.7233
propose method policy construction usual pomdp solution techniques exploit representations reduce effective state space problem 
assume structured state space contrast usual pomdp formulations generated set random variables actions relatively local effects variables observations rewards costs structured 
allows bns ids represent large problems compactly 
addition tree structured conditional probability utility matrices equivalently rules represent additional structure independence variable assignments 
representation identify relevant distinctions state space point pomdp algorithm 
result values associated collection states may represented 
example suppose identified fact certain stage process variable effect subsequent expected value 
furthermore assume action variables influence probability action addition suppose true effect independent expected value action previous stage states satisfying states satisfying words value depends probability propositions 
values need computed 
structure identified readily bayesian network representation action structured conditional probability matrices 
structure problem identify relevant distinctions stage problem perform computations entire collection states distinctions irrelevant utility calculations 
note algorithm exploits partitioning ideas structured policy iteration algorithm ignore possible observations motivating example 

essentially extends ideas account partial observations state applies piecewise fashion components value function describe 
algorithm solves question compactly representing belief states value functions specifying computing separate values state state space exponential number variables 
address question potentially exponentially growing horizon partitioning state space 
general growth may necessary ensure optimality 
optimal policy may fine grained 
algorithm sacrifice optimality merely exploits problem structure optimal policy construction 
avoiding exponential blow partitioning belief space necessarily involves approximation 
address question structured representations identify quantify relevant distinctions provides tremendous leverage approximation techniques 
instance certain distinctions ignored structured value function representation allows quickly identify variables conditionally impact value 
section briefly contrasts perspectives adopted pomdp id formulation decision problems 
section describe pomdps stage belief network representation pomdp 
section describe particular pomdp algorithm due monahan sondik 
choose algorithm conceptually simple easy restricted space available 
section describe incorporate structure captured representations reduce effective state space monahan algorithm point computation 
provide concluding remarks section 
pomdps influence diagrams section clarify distinctions underlying pomdp id approaches representation partially observable decision problems 
history vs belief state fully observable markov decision problem decision maker agent accurately observe state system controlled 
markov assumption observation information required optimal action choice 
partially observable setting agent gets noisy incomplete observations true nature state 
optimal action choice basis current observations agent past action observations relevant decision influence current state estimate 
distinct approaches incorporating past history decision process 

agent condition action choice explicitly past history 
approach decision conditioned previous actions current past observations 
model adopted solution influence diagrams called forgetting principle 
suppose set possible observations stage set actions stage decisions taken policy maps possible history action choice dynamic programming principle optimize action history element independently 
finite stage finite 
unfortunately results exponential growth policy specification computation 

agent maintain belief state distribution possible current system states summarizing history elaborate 
approach action choice conditioned agent current belief state 
system states set belief states assigning probability states probability state derived policy function belief states actions 
optimal decision belief state independently 
domain policy function grow stage dimensional infinite space 
approach policy formulation adopted pomdp solution algorithms community requires method dealing uncountable belief space 
state 
distinction pomdp influence diagram specifications partially observable problem lies notion state 
pomdp formulation states taken states core process system controlled 
actions taken observations viewed external system part system state 
system state sufficient render effects actions independent past history 
action choices observations modeled meta level 
action choice observation bayes rule update belief state observations just change probability distribution states 
bn id tradition observation random variable part state addition variables define core process observation added extra variable 
state space joint probability space models observation simply states impossible conditioning observations involves setting inconsistent expanded states probability zero renormalizing 
representation adopted actions action choice stage variable constitutes part expanded state space 
value variable chosen agent independent variables 
pomdps structured representations section review classic presentation pomdps adopted operations research community 
refer details surveys :10.1.1.53.7233:10.1.1.53.7233
describe structured representations adopted ai community represent factored state spaces 
explicit state space presentation assume dynamics system controlled modeled pomdp stationary stage independent dynamics 
assume finite set states capture relevant aspects system set actions available agent controller 
simplicity assume actions taken attempted states 
action takes agent state effects actions predicted certainty slightly abusing notation write pr js denote probability reached action performed state 
transition probabilities encoded jsj jsj matrix action 
formulation assumes markov property system question 
system partially observable planning agent may able observe exact state introducing source uncertainty action selection 
assume set possible observations agent 
observations provide evidence true nature various aspects state 
general observation stage depend stochastically state action performed outcome action stage 
assume family distributions observations pr jjs 
pr ljs denote probability observing action executed state results state special case fully observable system modeled assuming pr ljs iff assume simplicity observation probability depends action taken starting state resulting state pr ljs pr ljs assume real valued reward function associates rewards penalties various states denotes relative goodness state assume cost function denoting cost action state 
purposes suffices consider rewards penalties separately action costs 
plan policy function determines choice action stage system evolution 
policy optimal specified horizon number stages maximizes expected value system trajectory induces maximizes expected rewards accumulated reward incorporates action costs state rewards penalties 
appropriate action choice requires agent predict expected effects possible actions expected value states visited sequence action choice 
utility possible actions depend current state history relevant beliefs current state 
mentioned adopted id algorithms policy represented mapping initial state estimate sequence actions observations previous stages action stage 
especially elegant way treat problem maintain current belief state described 
intuitively belief state tuple ni denotes probability system state markov assumption belief state sufficient predict expected effects action state system 
state belief characterizing estimate system state stage decision process update belief state action taken observation stage form new belief state characterizing state system stage 
hand fact gave rise forgotten 
denote transformation belief state action performed observation defined sj pr ojs pr ijs sj sk pr ojs pr probability system state action taken observation prior belief state easy see summarizes necessary information subsequent decisions 
essential assumption classical pomdp techniques stage decision process assuming accurately summarizes past actions observations optimal decision solely intuitively think converting partially observable mdp original state space fully observable mdp belief space set belief states 
review policy construction techniques section 
stage belief network formulate problem explicit pomdp unreasonable expect problem specified manner state spaces grow exponentially number variables relevant problem hand 
explicit formulation requires specify jsj jsj probability matrix action describing transition probabilities jsj joj probability matrix action describing observation probabilities jsj action cost reward vectors 
regularities action effects reward structure usually permit natural concise representations 
consider simple example robot check user wants coffee get going shop street 
robot rewarded user wants coffee wc coffee hc penalized hc false wc true 
robot get wet raining goes get coffee umbrella imagine number tasks 
robot check weather grab umbrella focus actions getting coffee getc checking user wants coffee means quick inspection 
represent dynamics state slice temporal bayes net set nodes representing state world prior hc wc hc wc obs null hc hc matrix hc tree representation hc wc action networks getc action node state variable set representing world action performed directed arcs representing causal influences sets :10.1.1.53.7233
addition distinguished variable representing possible observations performing action 
variables influence observation indicated directed arcs 
obs denote set observations possible illustrates network representations actions getc 
require graph acyclic ease exposition assume arcs directed pre action variables post action variables 
post action nodes usual matrices describing probability values values parents action assume conditional probability matrices represented rules done :10.1.1.50.603:10.1.1.50.603
representation exploited great effect solution completely observable mdps 
adopt ideas 
tree representation matrix variable illustrated illustrating asymmetries exploited 
tree associated proposition network action denoted tree ja 
observation variable action single value getc provides information true state world 
action causal influences post action variables viewed ramifications complicate algorithm slightly minor detail conceptual understanding 
hc wc obs wc wc hc reward hc wc reward function network effect core state variables provide information user coffee preference observation indicates wc indicates wc specified errors 
easy see set action networks put single network decision node corresponding influence diagram 
add decision node denoting choice action arcs post action node 
conditional probability tree node action choice root tree ja attached branch root 
set values observation variable union possible observations individual action networks tree branches action choice contains individual observation trees 
see brief survey representations 
state reward function represented similar structured fashion single value node tree structured matrix see allows features influence reward ignored 
leaves tree denote reward associated states determined branch 
similar representation action cost 
assume action costs constant assume cost getc 
monahan pomdp algorithm belief vector summarize current state estimate explicitly incorporating history allows policies expressed independent history note network representations network strictly necessary conditional probability trees allow determination parents :10.1.1.50.603
decisions contingent current state belief 
fact potentially specification policy difficult 
space possible belief states uncountably infinite 
computing associating action choice belief state impossible explicit formulation 
observed sondik optimal policies may computed specified finitely performing computations associating actions various regions belief space 
crucial component pomdp algorithm discovery appropriate set regions optimal policy 
pomdp algorithms fact explicitly compute regions optimal decision may vary 
keep track finite set vectors stage decision process 
vector vector size jsj specifies value state 
vector associated action intuitively vector defines value performing action state fixed value function 
expected value vector dot product crucial observation sondik fact stage value function policy piecewise linear convex linear component represented vector 
true value function piecewise linear convex represent linear components set vectors ig true value maximum products see vector action associated optimal action choice stage action associated maximizing vector 
computation optimal policy requires generate set vectors stage set vectors stage 
words dynamic programming style computation determine representation stage value function policy stage value function 
method adopted pomdp algorithms precise technique enumerating set required vectors varies 
set vectors associated stage 
monahan set vectors necessary stage generated follows action associate observation obs 
generate new vector follows denotes ith component 
judicious application bayes rule allows expression quantities read directly single set vectors stage stationary policy sufficient 
problem specification sj pr ljs pr jjs intuitively think new vector specifying value belief state stage assuming action chosen observed value stage determined stage vector action generate set new vectors size jobs set union sets generated action 
action associated vector action generate 
optimal action choice stage determined computing maximizes adopting action associated emphasize policy explicitly represented algorithm constructs set vectors stage finite horizon problem optimal action choice belief state easily determined infinite horizon problems single set vectors obtained 
course may generated vectors irrelevant sense dominated vectors stage 
eliminating dominated vectors proceeding compute vectors earlier stages number subsequent vectors generated computation time drastically reduced 
dominated vectors easily identified set linear programs variables state program vector 
note finite horizon stage problem process setting contain immediate state reward vector 
infinite horizon discounted problem certain assumptions run process convergence reached initial vector choice crucial see 
focus finite stage problems sequel infinite horizon problems offer special difficulties 
exploiting structure solving pomdp crucial step monahan algorithm generation set vectors stage set vectors 
note vector size jsj new vectors generated pointwise 
representation construction single vector computationally prohibitive number variables grows approach quickly infeasible 
problem represented compactly network representation expect set vectors compactly represented 
values associated various states vector exploit fact clustering states value 
simple example initial vector stage simply immediate state reward function 
problem vector may compactly expressed tree form values 
propose method optimal policy construction general eliminates need represent vectors pointwise need construct new vectors standard pointwise fashion 
method monahan algorithm exploits fact stage process vectors representing value function stage may structured making relevant distinctions state space problem representation allows preserve structure generation new vectors 
linear component piecewise linear value functions classic pomdp representations represented vector consisting values associated linear segment state 
represent segments trees 
trees interior nodes state variables edges descending variable correspond possible assignments variable leaves real values 
precisely representation immediate reward function 
tree partitions state space obvious way branch denotes set states satisfying variable assignment determined branch 
represents unique vector value state simply value associated partition containing state 
generation single tree algorithm allows construct set trees required stage set trees associated stage 
presenting algorithm detail describe main intuitions underlying method considering construction single tree 
consider subproblem generating value tree corresponding performing single action stage assuming expected value fixed 
consider combine trees form stage vector corresponding particular strategy associates element obs 
suppose wish determine value performing action stage expected value stage determined fixed tree 
method generating new tree exploits abductive repartitioning algorithm described closely related poole probabilistic horn abduction refer details :10.1.1.50.603
roughly structured value function conditions states different expected value stage action easily determined appeal action representation particular action may different effects states differences effects related variables variable assignments relevant value function states identical expected value need distinguished function construct tree way relevant distinctions 
construction proceeds tree want generate conditions prior performance action cause outcome probabilities respect partitions induced vary 
proceed stepwise fashion explaining interior nodes turn root node proceeding recursively children 
tree initially empty 
explain root node say generating conditions values variable different probabilities action explanation 
note explanation essentially lifted problem description probability tree tree ja action network tree initial partial tree 
partial tree leaves labeled probabilities variables explained far 
explanations continue variable tree explanation added leaf nodes current partial tree 
note explanation variable added branch current partial tree 
tree describes conditions relevant value 
explanation relevant circumstances allow conditions met positive probability tree xja need added branches conditions possible 
explanation ancestor generated relevant probabilities label leaves current partial tree 
variables explained expected reward branch easily computed probability labels branch determine probability partition tree note redundant inconsistent branches explanations added partial tree deleted 
example variable labeling node hc wc initial value tree wc wc step wc wc hc hc wc wc hc wc wc hc wc hc hc wc wc wc hc wc wc wc hc wc hc wc hc wc hc wc hc step generating explanation value tree xja occurs earlier partial tree node tree xja deleted assignments node tree xja redundant inconsistent 
shrinkage possible 
illustrate process consider example illustrated 
take immediate reward function tree initial value tree wish generate expected value tree stage assuming action getc taken assuming determines value stage 
explaining conditions influence probability wc getc step 
causes tree inserted new tree 
explain hc step 
initial value tree asserts hc relevant wc true new tree added left branch existing tree wc probability zero right 
note probabilities describe probability variable question action branches relate conditions affect probabilities action 
clear step consider conditions prior getc affect occurrence wet getc 
note final tree information needed compute expected value leaf probabilities leaf uniquely determine probability landing partition initial value tree getc 
note get true expected value just value add trees current state value action cost 
general may cause explanation trees grow fact keeps trees compact possible tends set trees stabilize quickly 
refer details see 
step wc hc wc tree getc tree hc wc new trees stage distinctions influence action cost operation trivial common assumptions 
example tree initial tree stage expected distinctions preserved subsequent value trees 
furthermore action cost constant requires simple addition constant leaves 
shows expected total value tree action getc obtained adding value tree obtained 
shows expected value tree action assumptions 
value trees generally vectors suitable stage correspond fixed observational strategy strategy assigns vector observation 
account observations note element corresponds action choice observation strategy assigns vector obs 
consider problem generating actual tree corresponding action strategy assigning observation conditions influence probability observation affect expected value affect subsequent choice vector stage new tree contain distinctions 
partially specified tree observation tree corresponding action note branches tree correspond conditions relevant observation probability leaves labeled probability making observation leaves tree add weighted sum explanation trees 
specifically leaf tree set possible nonzero wc hc wc wc wc wc hc step step step new tree stage probability observations exposition assume leaf conditions corresponding leaf expect observe probabilities pr pr respectively 
expect receive value associated explanation tree probability pr probability pr 
take weighted sum trees add resulting merged tree appropriate leaf node tree 
concrete consider example illustrated 
assume trees trees getc elements consider generating new tree placed corresponds action invokes strategy associates observation observation 
observation tree observation probability depends wc see step 
consider weighted combination trees leaf leaf wc add tree wc add 
gives redundant tree middle 
course prune away inconsistent branches collapse redundant nodes obtain final tree shown right 
computing weighted sum trees relatively straightforward 
multiply value leaf node tree corresponding probability 
add weighted trees involves constructing smallest single tree forms partition state space subsumes explanation trees 
implemented simple tree merging operation example see similar tree merging different purpose 
exploring heuristic strategies determining variable orderings merged trees 
fact particular problem assuming stages elements 
example shows generation element 
hc generation process described involves overhead construction explanation trees piecing observation probabilities note putting trees really involves patching partial trees exist action networks computationally intensive may appear 
substantially follow monahan algorithm directly repeat process action strategy combination 
note generating explanation trees fixed action tree independent actual strategy adopted strategy tells piece explanation trees 
leads informal algorithm generating ha construct value tree 
action strategy associating elements obs elements construct new tree proceeds adding weighted sum determined expected value trees determined previous step leaf nodes observation tree deleting redundant nodes inconsistent branches 
add tree 
prune set eliminating unnecessary trees 
fixed action explanation tree element computed 
savings possible piecing certain strategies associates vector observation value tree part directly 
final stage algorithm involves pruning set possible trees eliminate dominated accurately dominate vector 
words belief state greater vector need chosen bearing policy 
elimination vectors greatly reduce number vectors generated earlier stages 
monahan suggests simple set linear programs eliminating useless vectors 
tree representation vectors allows elimination phase tractable 
lps eliminate vectors variables corresponding state 
representation reduce number variables required formulate lps need distinctions state space tree collection corresponds finding smallest subsuming tree set constructing lp variable leaf tree see full 
note pomdp algorithms clever generating set possible vectors 
example sondik algorithm subsequent approaches attempt enumerate possible combinations observational strategies eliminate useless vectors 
focus monahan approach conceptually simple allows illustrate exact nature structured vector representations exploited computationally 
currently investigating algorithms direct vector generation adapted representation 
concluding remarks sketched algorithm constructing optimal policies pomdps exploits problem structure exhibited rules decision trees reduce effective state space various points computation 
crucial aspect approach ability construct conditions relevant certain stage process relevant distinctions stage 
coalescence ai planning optimization techniques related approaches provide significant improvements policy construction algorithms 
space limitations preclude complete presentation algorithm example refer full details 
directions current research include applying ideas algorithms enumerate vectors directly exhaustive enumeration elimination empirical evaluation problem characteristics dictate history belief state policy construction effective 
believe ideas offers tremendous leverage approximation methods 
structured representation allow branches collapsed way minimizes lost value allows vectors compared easily allowing elimination similar vectors 
alleviate great extent combinatorial explosion required vectors horizon increases see 
richard dearden goldszmidt helpful discussions topic 
research supported nserc research ogp 
boutilier dean hanks 
planning uncertainty structural assumptions computational leverage 
manuscript 
boutilier dearden goldszmidt 
exploiting structure policy construction 
aaai spr 
symp 
extending theories action stanford 

linear stochastic systems 
wiley new york 
cassandra 
optimal policies partially observable markov decision processes 
report cs brown univ 
cassandra kaelbling littman :10.1.1.53.7233
acting optimally partially observable stochastic domains 
aaai pp 
dean kanazawa :10.1.1.53.7233
model reasoning persistence causation 
comp 
intel 
dean wellman 
planning control 
morgan kaufmann 
howard matheson 
influence diagrams 
howard matheson editors principles applications decision analysis pages 
strategic decisions group ca 
lovejoy 
computationally feasible bounds partially observed markov decision processes 
operations research 
monahan 
survey partially observable markov decision processes theory models algorithms 
mgmt 
sci 
parr russell 
approximating optimal policies partially observable stochastic domains 
unpublished manuscript 
pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
poole :10.1.1.50.603
probabilistic horn abduction bayesian networks 
art 
intel 
poole kanazawa 
decision theoretic abductive basis planning 
aaai spr 
symp 
decision theoretic planning stanford 
smallwood sondik :10.1.1.50.603
optimal control partially observable markov processes finite horizon 
op 
res 
smith matheson 
structuring conditional relationships influence diagrams 
op 
res 
sondik 
optimal control partially observable markov processes infinite horizon discounted costs 
op 
res 

