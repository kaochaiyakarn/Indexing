learning mixtures trees marina department statistics university washington seattle wa mmp stat washington edu michael jordan division computer science department statistics university california berkeley ca jordan cs berkeley edu october describes mixtures trees model probabilistic model discrete multidimensional domains 
mixtures trees generalize probabilistic trees chow liu different complementary direction bayesian networks 
efficient algorithms learning mixtures trees models maximum likelihood bayesian frameworks 
discuss additional efficiencies obtained data sparse data structures algorithms exploit sparseness 
experimental results demonstrate performance model density estimation classification 
discuss sense tree classifiers perform implicit form feature selection demonstrate resulting insensitivity irrelevant attributes 
press journal machine learning research running head mixtures trees corresponding author marina meila university washington department statistics box seattle wa phone email mmp cs cmu edu probabilistic inference core technology ai largely due developments graph theoretic methods representation manipulation complex probability distributions 
guise directed graphs bayesian networks undirected graphs markov random fields probabilistic graphical models number virtues representations uncertainty inference engines 
graphical models allow separation qualitative structural aspects uncertain knowledge quantitative parametric aspects uncertainty represented patterns edges graph represented numerical values associated subsets nodes graph 
separation natural domain experts taming problems associated structuring interpreting troubleshooting model 
importantly graph theoretic framework allowed development general inference algorithms cases provide orders magnitude speedups brute force methods 
virtues gone unnoticed researchers interested machine learning graphical models widely explored underlying architectures systems classification prediction density estimation :10.1.1.156.9918:10.1.1.101.3165:10.1.1.30.9978
possible view wide variety classical machine learning architectures instances graphical models graphical model framework provides natural design procedure exploring architectural variations classical themes :10.1.1.150.82
machine learning problems problem learning graphical model data divided problem parameter learning problem structure learning 
progress problem cast framework expectation maximization em algorithm 
em algorithm essentially runs probabilistic inference algorithm subroutine compute expected sufficient statistics data reducing parameter learning problem decoupled set local statistical estimation problems node graph 
link probabilistic inference parameter learning important allowing developments efficient inference immediate impact research learning algorithms 
problem learning structure graph data significantly harder problem 
practice structure learning methods heuristic methods perform local search starting graph improving adding deleting edge time :10.1.1.156.9918
important special case parameter learning structure learning tractable case graphical models form tree distribution 
shown tree distribution maximizes likelihood set observations nodes parameters tree time quadratic number variables domain 
algorithm known chow liu algorithm 
trees virtue probabilistic inference guaranteed efficient historically earliest research ai efficient inference focused trees 
research extended early considering general singly connected graphs considering graphs arbitrary patterns connectivity 
line mixture trees domain consisting random variables hidden choice variable 
conditional value dependency structure tree 
detailed presentation mixture trees model provided section 
research provided useful upgrade path tree distributions complex bayesian markov networks currently studied 
consider alternative upgrade path 
inspired success mixture models providing simple effective generalizations classical methods simpler density estimation settings consider generalization tree distributions known mixtures trees mt model 
suggested mt model involves probabilistic mixture set graphical components tree 
describe likelihood algorithms learning parameters structure models 
consider probabilistic mixtures general graphical models general case bayesian multinet introduced geiger heckerman 
bayesian multinet mixture model mixture component arbitrary graphical model 
advantage bayesian multinets traditional graphical models ability represent context specific independencies situations subsets variables exhibit certain conditional independencies values conditioning variable 
see context specific independence :10.1.1.16.1782
making context specific independencies explicit multiple collections edges obtain parsimonious representations joint probabilities efficient inference algorithms 
machine learning setting advantages general bayesian multinet formalism apparent 
allowing mixture component general graphical model forces face difficulties learning general graphical structure 
greedy edge addition edge deletion algorithms particularly ill suited bayesian multinet focus collections edges single edges underlies intuitive appeal architecture 
view mixture trees providing reasonable compromise simplicity tree distributions expressive power bayesian multinet doing restricted setting leads efficient machine learning algorithms 
particular show simple generalization chow liu algorithm possible find local maxima likelihoods penalized likelihoods efficiently general mt models 
algorithm iterative expectation maximization em algorithm inner loop step involves invoking chow liu algorithm determine structure parameters individual mixture components 
concrete sense algorithm searches space collections edges 
summary mt model multiple network representation shares basic features bayesian markov network representations brings new features fore 
believe features expand scope graph theoretic probabilistic representations useful ways may particularly appropriate machine learning problems 
related mt model classification setting density estimation setting contact different strands previous literature guises 
classification setting mt model builds seminal tree classifiers chow liu extensions due friedman geiger goldszmidt :10.1.1.30.9978
chow liu proposed solve way classification problems fitting separate tree observed variables classes classifying new data point choosing class having maximum class conditional probability corresponding tree model 
friedman geiger goldszmidt took point departure naive bayes model viewed graphical model explicit class node directed edges disconnected set nodes representing input variables attributes 
introducing additional edges input variables yields tree augmented naive bayes classifier :10.1.1.30.9978
authors considered constrained model different patterns edges allowed value class node formally identical chow liu proposal 
choice variable mt model identified class label mt model identical chow liu approach classification setting 
necessarily wish identify choice variable class label experiments classification treat class label simply input variable 
yields discriminative approach classification training data pooled purposes training model see section 
choice variable remains hidden yielding mixture model class 
similar spirit mixture discriminant analysis model hastie tibshirani mixture gaussians class multiway classification problem 
setting density estimation clustering compression problems mt model contact large active literature mixture modeling 
briefly review salient connections 
auto class model mixture factorial distributions mf excellent cost performance ratio motivates mt model way naive bayes model motivates model classification setting 
factorial distribution product factors depends exactly variable 
kontkanen study mf hidden variable classification approach extended monti cooper 
idea learning tractable simple belief networks superimposing mixture account remaining dependencies developed independently thiesson studied mixtures gaussian belief networks 
interleaves em parameter search bayesian model search heuristic general algorithm 
tree distributions section introduce tree model notation 
denote set discrete random variables interest 
random variable represent range xv particular value rv finite cardinality 
subset xa denote assignment variables simplify notation xv denoted denoted simply 
need refer maximum rv denote value rmax 
undirected markov random field representations tree distributions 
identifying vertex set graph set random variables consider graph set undirected edges 
allow tree multiple connected components trees generally called forests 
definition number edges number connected components related follows implying adding edge tree reduces number connected components 
tree edges 
case refer tree spanning tree 
parameterize tree way 
tuv denote joint probability distribution require distributions consistent respect marginalization denoting tu xu marginal tuv xu xv xv xu respect xv assign distribution graph follows tuv xu xv tv xv deg deg degree vertex number edges incident verified fact probability distribution pairwise probabilities tuv marginals tree distribution defined distribution admits factorization form 
tree distributions represented directed bayesian network graphical models 
denote directed tree possibly forest set tree undirected directed representations 
directed edges node parent denoted pa 
parameterize graph follows tv pa xv pa xv pa arbitrary conditional distribution 
verified defines probability distribution marginal conditional conditionals pa 
shall call representations undirected directed tree representations distribution respectively 
readily convert representations example convert directed representation choose arbitrary root connected component direct edge away root 
closer root pa compute conditional probabilities corresponding directed edge recursively substituting pa pa starting root 
illustrates process tree vertices 
directed tree representation advantage having independent parameters 
total number free parameters representation deg rv ru rv rv right hand side shows edge increases number parameters ru rv 
set conditional independencies associated tree distribution readily characterized 
particular subsets independent intersects path ignoring direction edges directed case marginalization inference sampling tree distributions basic operations computing likelihoods conditioning marginalization sampling inference performed efficiently tree distributions particular operations time complexity 
direct consequence factorized representation tree distributions equations 
representational capabilities graphical representations natural human intuition subclass tree models particularly intuitive 
trees sparse graphs having fewer edges 
path pair variables independence relationships subsets variables easy read general bayesian network topologies obvious tree 
tree edge corresponds simple common sense notion direct dependency natural representation 
simplicity tree models appealing limits modeling power 
note number free parameters tree grows linearly size state space exponential function class dependency structures representable trees relatively small 
learning tree distributions learning problem formulated follows set observations 
required find tree maximizes log likelihood data argmax log 
note maximum taken respect tree structure choice edges include respect numerical values parameters 
rest assume simplicity missing values variables words observations complete 
letting denote proportion observations xi training set equal alternatively express maximum likelihood problem summing configurations argmax log 
form see log likelihood criterion function negative cross entropy 
fact solve problem general letting arbitrary probability distribution 
generality prove useful section consider mixtures trees 
solution learning problem algorithm due quadratic complexity see 
steps algorithm 
compute pairwise marginals puv xu xv xu xv 
empirical distribution case computing marginals requires operations 
algorithm input distribution domain procedure mwst weights outputs maximum weight spanning tree 
compute marginal distributions pv puv 
compute mutual informations iuv 
mwst iuv 
set tuv puv uv output chow liu algorithm maximum likelihood estimation tree structure parameters 
second marginals compute mutual information pair variables distribution iuv puv xu xv log puv xu xv pu xu pv xv operation requires max operations 
third run maximum weight spanning tree mwst algorithm see iuv weight edge algorithms run time return spanning tree maximizes total mutual information edges included tree 
chow liu showed maximum weight spanning tree maximizes likelihood tree distributions optimizing parameters uv equal corresponding marginals puv distribution tuv puv algorithm attains global optimum structure parameters 
mixtures trees define mixture trees mt model distribution form kt 

tree distributions mixture components called mixture coefficients 
mixture trees viewed containing unobserved choice variable takes value 
probability conditioned value distribution observed variables tree 
trees may different structures different parameters 
example mixture trees 
note varying structure component trees mixture trees bayesian network markov random field 
adopt notation independent distribution 

imply 
hand mixture trees capable representing dependency structures conditioned value variable choice variable usual bayesian network markov random field 
situations model potentially useful abound real life consider example bitmaps handwritten digits 
images obviously contain dependencies pixels pattern dependencies vary digits 
imagine medical database recording body weight data patient 
body weight function age height healthy person depend conditions patient suffered disease athlete 
situation ones mentioned conditioning variable produces dependency structure characterized sparse acyclic pairwise dependencies mixture trees may provide model domain 
constrain trees mixture structure obtain mixture trees shared structure see 
case 
addition represented bayesian network markov random field chain graph 
chain graphs introduced represent superclass bayesian networks markov random fields 
chain graph contains directed undirected edges 
generally consider problems choice variable hidden unobserved possible utilize mt frameworks choice variable observed 
models discuss section studied previously referred generically mixtures observed choice variable :10.1.1.30.9978
stated assumed choice variable hidden 
mixture trees shared structure represented bayes net markov random field 
represented chain graph 
double boxes enclose undirected blocks chain graph 
marginalization inference sampling mt models kt mt model 
consider basic operations marginalization inference sampling recalling operations time complexity component tree distributions marginalization 
marginal distribution subset follows qa xa kt xa marginal mixture marginals component trees 
inference 
xv evidence 
probability hidden variable evidence xv results achieved applying bayes rule follows xv kt xv tv xv 
particular observe choice variable xv obtain posterior probability distribution kt probability distribution subset evidence qa xa xv kt xa xv kt xv xv xa xv result mixture results inference procedures run component trees 
sampling 
procedure sampling mixture trees stage process samples value choice variable distribution 
value sampled procedure sampling tree distribution 
summary basic operations mixtures trees marginalization conditioning sampling achieved performing corresponding operation component mixture combining results 
complexity operations scales linearly number trees mixture 
learning mt models expectation maximization em algorithm provides effective approach solving learning problems employed particular success setting mixture models general latent variable models :10.1.1.136.9119
section show em provides natural approach learning problem mt model 
important feature solution provides estimates parametric structural aspects model 
particular assume current section number trees fixed algorithm derive provides estimates pattern edges individual trees parameters 
estimates maximum likelihood estimates subject concerns overfitting constrained nature tree distributions helps ameliorate overfitting problems 
possible control number edges indirectly priors discuss methods doing section 
set observations 
required find mixture trees satisfies argmax log 
framework em algorithm likelihood function referred incomplete log likelihood contrasted complete log likelihood function lc log kt zi log log equal equal kth value choice variable zero 
complete log likelihood log likelihood data unobserved data 
observed 
idea em algorithm utilize complete log likelihood generally easy maximize surrogate incomplete log likelihood generally somewhat easy maximize directly 
particular algorithm goes uphill expected value complete log likelihood expectation taken respect unobserved data 
algorithm form interacting pair steps step expectation computed current value parameters step parameters adjusted maximize expected complete log likelihood 
steps iterate proved converge local maximum incomplete log likelihood 
expectation see step mt model reduces expectation delta function conditioned data quantity recognizable posterior probability hidden variable ith observation cf 
equation 
define kt xi xi posterior probability 
substituting expected value complete log likelihood obtain lc log log introduce quantities 
sums interpreted total number data points generated component definitions obtain lc log log quantity maximize respect parameters 
see lc separates terms depend disjoint subsets model parameters step decouples separate various parameters 
maximizing term respect parameters subject constraint obtain update equation 
order update see maximize negative cross entropy log problem solved algorithm section 
see step learning mixture components mt models reduces separate runs algorithm target distribution normalized posterior probability obtained step 
summarize results derivation em algorithm mixtures trees 
algorithm input dataset 
initial model 
procedure iterate convergence step compute 

step 
output model 
running time algorithm learning mt models 
computing likelihood data point tree distribution directed tree representation takes multiplications 
step requires mnn floating point operations 
step computationally expensive phase computation marginals uv kth tree 
step time complexity mn 
mn max mn required iteration mutual informations runs mwst algorithm 
need mnr max computing tree parameters directed representation 
total running time em iteration mn mn max 
algorithm polynomial iteration dimension domain number components size data set 
space complexity polynomial dominated max space needed store pairwise marginal tables uv tables overwritten successive values 
learning mixtures trees shared structure possible modify algorithm constrain trees share structure estimate models 
step remains unchanged 
novelty reestimation tree distributions step constrained structure 
maximization decoupled separate tree estimations remarkably performed efficiently 
algorithm input dataset 
initial model 
procedure mwst weights outputs maximum weight spanning tree iterate convergence step compute 

step compute marginals uv compute mutual informations uv mwst uv set set uv uv uv 
output model 
algorithm learning models 
readily verified structure optimal parameters tree edge uv equal parameters corresponding marginal distribution uv remains find optimal structure 
expression optimized second denoting sum right hand side equation 
replacing uv uv mutual information ik uv sum follows log uv iuv new quantity uv appearing represents mutual information conditioned hidden variable general definition discrete variables distributed iuv pz xz puv xz log xz uv xz xu xz xv xz second term represents sum conditional entropies variables independent tree structure 
optimization structure achieved running mwst algorithm edge weights represented iuv summarize algorithm 
decomposable priors map estimation mixtures trees bayesian learning framework combines information obtained direct observations prior knowledge model represented probability distribution 
object interest bayesian analysis posterior distribution models observed data quantity rarely calculated explicitly 
practical methods approximating posterior include choosing single maximum posteriori map estimate replacing continuous space models finite set high posterior probability expanding posterior mode :10.1.1.156.9918
finding local maxima modes distribution necessary step methods primary concern section 
demonstrate maximum posteriori modes efficiently maximum likelihood modes particular choice prior 
consequences approximate bayesian averaging possible 
second uses non informative prior map estimation equivalent bayesian smoothing represents form regularization 
regularization particularly useful case small data sets order prevent overfitting 
map estimation em algorithm model dataset logarithm posterior equals log log plus additive constant 
em algorithm adapted maximize log posterior fixed :10.1.1.33.2557
comparing equation see quantity maximized log log lc 
prior term influence step em algorithm proceeds exactly cf 
equation 
able successfully maximize right hand side step require log decomposes sum independent terms matching decomposition lc 
prior mixtures trees amenable decomposition called decomposable prior 
product form ek parameters ek uv pa parenthesized factor equation represents prior tree structure ek second factor prior parameters 
requiring prior decomposable equivalent making independence assumptions particular means parameters tree mixture independent parameters trees probability mixture variable 
section show assumptions overly restrictive constructing decomposable priors tree structures parameters showing class rich contain members practical importance 
decomposable priors tree structures general form decomposable prior tree structure edge contributes constant factor independent presence absence edges exp uv prior expression maximized step em algorithm log uv log consequently edge weight uv tree tk adjusted corresponding value uv divided total number points tree responsible uv uv uv negative uv increases probability final solution positive value uv acts penalty presence edge tree 
mwst procedure modified add negative weight edges obtain disconnected trees having fewer edges 
note strength prior inversely proportional total number data points assigned mixture component equal priors trees trees accounting fewer data points penalized strongly fewer edges 
chooses edge penalties proportional increase number parameters caused addition edge uv tree uv ru rv log minimum description length mdl type prior implemented 
context learning bayesian networks prior suggested distance metric bayes net structures prior network structure :10.1.1.156.9918
prior penalizes deviations prior network 
prior decomposable entailing uv ln ln decomposable priors structure structure common trees 
case effect prior penalize weight uv uv decomposable prior remarkable property normalization constant computed exactly closed form 
possible completely define prior compute averages prior compute model evidence 
number undirected tree structures variables result quite surprising 
decomposable priors tree parameters decomposable prior parameters introduce dirichlet prior :10.1.1.156.9918
dirichlet distribution defined domain form 
numbers parametrize interpreted sufficient statistics fictitious data set size called fictitious counts 
represents strength prior 
specify prior tree parameters specify dirichlet distribution probability tables tv xu uv ed possible tree structure ed achieved means set parameters satisfying xu xv 
settings prior parameters xv xu xv 
rv tree contains directed edge uv defined xv 
rv 
representation prior compact order max parameters consistent different directed parametrizations tree distribution receive prior 
assumptions allowing define prior explicated parallel reasoning general bayes nets :10.1.1.156.9918
denote empirical distribution obtained data set size uv distribution defined fictitious counts 
property dirichlet distribution follows learning map tree equivalent learning ml tree weighted combination datasets np :10.1.1.156.9918
consequently parameters optimal tree tuv puv 
mixture trees maximizing posterior translates replacing equation 
implies step em algorithm step exact tractable case map estimation decomposable priors 
note posteriors models different defined constant depends compare posteriors mts different numbers mixture components experiments chose performance criteria validation set likelihood density estimation experiments validation set classification accuracy classification tasks 
experiments section describes experiments run order assess promise mt model 
experiments structure identification experiments examine ability algorithm recover original distribution data generated mixture trees 
group experiments studies performance mt model density estimator data experiments generated mixtures trees 
perform classification experiments studying mt model single tree model 
comparisons classifiers trained supervised unsupervised mode 
section ends discussion single tree classifier feature selection properties 
experiments training algorithm initialized random independently data 
stated learning algorithm run convergence 
expressed bits example called compression rates 
lower value compression rate better fit data 
experiments involve small data sets bayesian methods discussed section impose penalty complex models 
order regularize model structure decomposable prior tree edges uv 
regularize model parameters dirichlet prior derived pairwise marginal distributions data set 
approach known smoothing marginal :10.1.1.30.9978
particular set parameter characterizing dirichlet prior tree fixed smoothing coefficient equally variables amount inversely proportional mixture components 
intuitively effect operation trees similar reducing effective model complexity 
structure identification random trees large data set structure identification experiment generated mixture trees variables vertex having values 
distribution choice variable tree structure parameters sampled random 
training examples bars learning task 
mixture generate data points training set algorithm 
initial model components random 
compared structure learned model generative model computed likelihoods learned original model test dataset consisting points 
algorithm quite successful identifying original trees trials algorithm failed identify correctly tree trial 
result accounted sampling noise tree wasn identified mixture coefficient 
difference log likelihood samples generating model approximating model bits example 
random bars small data set bars problem benchmark structure learning problem unsupervised learning algorithms neural network literature 
domain square binary variables depicted 
data generated manner flips fair coin decide generate horizontal vertical bars represents hidden variable model 
bars turned independently black probability pb 
noise added flipping bit image independently probability pn 
learner shown data generated process task learner discover data generating mechanism 
mixture trees model approximates true structure low noise levels shown 
note tree variables forming bar equally approximation 
consider structure discovered model learns mixture having connected components bar 
additionally shall test classification accuracy learned model comparing true value hidden variable horizontal vertical value estimated model data point test set 
seen row third column training set examples ambiguous 
retained ambiguous examples training set 
total training set size 
trained models 
evaluated models hidden variables visible variables pn pn pb pn pn vert true structure probabilistic generative model bars data 
mixture trees approximates generative model bars problem 
interconnection variables bar arbitrary 
pb validation set likelihood bits case smoothing value test set log likelihood bars learning task different values smoothing parameter different presents averages standard deviations trials 
validation set size choose final values smoothing parameter 
typical values literature choose 
parameter values pb pn 
obtain trees connected components small edge penalty 
validation set log likelihoods bits 
clearly best model 
examined resulting structures trials structure recovery perfect 
interestingly result held range smoothing parameter simply cross validated value 
way comparison examined training methods structure recovered respectively cases 
ability learned representation categorize new examples coming group referred classification performance shown table 
result reported obtained separate test set final cross validated value 
note due presence ambiguous examples model achieve perfect classification 
probability ambiguous example pb yields error rate 
comparing lower bound value corresponding column table shows model performs quite trained ambiguous examples 
support second test set size generated time including non ambiguous examples 
classification performance shown corresponding section table rose 
table shows likelihood test data evaluated learned model 
ambiguous test set table results bars learning task 
test set ambiguous unambiguous bits class accuracy example digit pair 
bits away true model likelihood bits data point 
test set compression rate significantly worse surprising distribution test set different distribution model trained 
density estimation experiments digits digit pairs images density estimation experiment involved subset binary vector representations handwritten digits 
datasets consist normalized quantized binary images handwritten digits available postal service office advanced technology 
dataset refer digits dataset contains images single digits dimensions 
dataset pairs contains dimensional vectors representing randomly paired digit images 
datasets training conditions employed described see example digit pair 
training validation test sets contained exemplars respectively 
model trained training set likelihood validation set stopped increasing 
tried mixtures trees fit algorithm 
digits pairs datasets chose mixture model highest log likelihood validation set calculated average log likelihood test set bits example 
averages runs shown table 
compare results results published 
algorithms plotted completely factored base rate br model assumes variable independent mixture factorial distributions mf unix gzip compression program helmholtz machine trained wake sleep algorithm helmholtz machine mean field approximation training fully observed table average log likelihood bits digit single digit digit double digit pairs datasets 
results averaged runs 
likelihood bits digit br mf gzip fv mt digits pairs likelihood bits digit br mf gzip fv mt average log likelihoods bits digit single digit double digit datasets 
notice difference scale figures 
fully connected sigmoid belief network fv mixture trees mt model 
shown mt model yields best density model simple digits second best model pairs digits 
comparison particular interest mt model mixture factorial mf model 
spite structural similarities models mt model performs significantly better mf model indicating structure data exploited mixture spanning trees captured mixture independent variable models 
comparing values average likelihood mt model digits pairs see second twice 
suggests model mf model able perform compression digit data unable discover independence double digit set 
alarm network second set density estimation experiments features alarm network data generating mechanism 
bayesian network constructed expert table density estimation results mixtures trees models alarm data set 
training set size 
average standard deviation trials 
model train likelihood test likelihood bits data point bits data point alarm net mixture trees mixture factorials base rate gzip table density estimation results mixtures trees models data set size generated alarm network 
average standard deviation trials 
model train likelihood test likelihood bits data point bits data point alarm net mixture trees mixture factorials base rate gzip knowledge medical diagnostic alarm message system patient monitoring 
domain discrete variables values connected directed arcs 
note network tree mixture trees topology graph sparse suggesting possibility approximating dependency structure mixture trees small number components generated training set having data points separate test set data points 
sets trained compared methods mixtures trees mt mixtures factorial mf distributions true model gzip mt mf model order degree smoothing selected cross validation randomly selected subsets training set 
results table see mt model outperforms mf model gzip base rate model 
examine sensitivity algorithms size data set ran experiment training set size 
results table 
mt model closest true model 
notice degradation performance table density estimation results mixtures trees models faces data set 
average standard deviation trials 
model train likelihood test likelihood bits data point bits data point mixture trees mixture factorials base rate gzip mixture trees relatively mild bit model complexity reduced significantly 
indicates important role played tree structures fitting data motivates advantage mixture trees mixture factorials data set 
faces dataset third density estimation experiment subset images normalized face images dataset 
images downsampled variables pixels gray levels 
divided data randomly examples training examples left validation set select mt mf models 
results table show mixture trees clear winner 
mt achieves performance times fewer parameters second best model mixture factorial distributions 
note essential ingredient success mt digits experiments data normalized pixel variable corresponds approximately location underlying digit face 
expect mts perform randomly chosen image patches 
classification mixtures trees mixture trees classifier density estimator turned classifier ways essentially likelihood ratio methods 
denote class variable set input variables method adopted classification experiments name mt classifier mt model trained domain treating class variable variable pooling training data 
testing phase new instance classified picking value class variable settings variables argmax xc xc table performance comparison mt model classification methods australian dataset 
results mixtures factorial distribution reported 
results 
method correct method correct mixture trees backprop mixture factorial distributions side cal decision tree smart bayes trees logistic discrimination nearest neighbor linear discrimination ac newid radial basis functions lvq cart alloc castle cn naive bayes quadratic discrimination flexible bayes similarly mf classifier side mf trained second method calls partitioning training set values class variable training tree density estimator partition 
equivalent training mixture trees observed choice variable choice variable class see 
particular trees forced structure obtain tree augmented naive bayes classifier 
case turns bayes formula argmax classify new instance analog mf classifier setting naive bayes classifier 
australian data set dataset examples consisting attributes binary class variable 
replicated experimental procedure closely possible 
test training set sizes respectively 
value ran algorithm fixed number epochs training set recorded performance test set 
repeated times time random start random split test training set 
small data set size edge pruning best performance mixtures trees compared published results dataset table 
error rate mt mf nb error rate mt mf nb classification results mixtures trees models data set mt mf 
nursery data set mt mf 
nb tree augmented naive bayes naive bayes classifiers respectively 
plots show average standard deviation test set error rate trials 
dataset data comprises examples specifying discrete attributes species mushroom families classifying edible poisonous 
arities variables range 
created test set examples training set examples 
examples kept aside select rest training 
smoothing 
classification results test set 
suggests relatively easy classification problem seeing examples guarantees perfect performance achieved 
mt achieves nearly optimal performance making mistake trials 
mf naive bayes models follow 
nursery dataset data set contains entries consisting discrete attributes class variable values data randomly separated training set size test set size 
case mts mfs data set partitioned examples training candidate models examples select optimal naive bayes models trained examples 
smoothing training set large 
classification results shown 
original data set contains data points correspond fifth class value eliminated data 
error rate nn nb tree mt comparison classification performance mt models splice data set train test 
tree represents mixture trees mt mixture trees 
knowledge neural net nn neural net 
splice dataset classification studied classification performance mt model domain dna splice junctions 
domain consists variables representing sequence dna bases additional class variable 
task determine middle sequence splice junction type 
splice junctions types ei represents exon intron intron exon place intron ends exon coding section begins 
class variable take values ei junction variables take values corresponding possible dna bases 
dataset consists labeled examples ran series experiments comparing mt model competing models 
series experiments compared results multilayer neural networks knowledge neural networks task 
replicated authors choice training set size test set size sampled new training test sets trial 
constructed trees mixtures trees 
fitting mixture early stopping procedure examples separated training set training stopped likelihood examples stopped increasing 
results averaged trials variety values 
seen single tree mt eliminated examples original data set ambiguous inputs 
see details error rate delve tree nb delve tree nb train test train test comparison classification performance mixture trees models trained small subsets splice data set 
models tested delve left right nearest neighbor cart hme hierarchical mixture experts ensemble learning stopping hme grown nearest neighbors lls linear squares lls ensemble learning mixture experts ensemble learning early stopping 
tree augmented naive bayes classifier nb naive bayes classifier tree single tree classifier 
error rate model perform similarly single tree showing better classification accuracy 
note situation smoothing improve performance unexpected data set relatively large 
exception mt model single tree mt models outperform models tested problem 
note tree models contain prior knowledge domain models neural network model trained supervised mode optimizing class accuracy includes detailed domain knowledge 
strong showing single tree model splice task pursued second series experiments compare tree model larger collection methods delve repository 
delve benchmark uses subsets splice database examples training 
testing done examples cases 
presents results algorithms tested delve single trees different degrees smoothing 
show results naive bayes nb tree augmented naive bayes models 
results delve represent averages runs different random initializations training testing sets trees nb outputs initialization dependent averaged performance models learned different splits union training testing set 
early stopping cross validation case 
results show single tree quite successful domain yielding error rate half error rate best model tested delve 
average error single tree trained examples greater average error tree trained examples 
attempt explain striking preservation accuracy small training sets discussion feature selection section 
naive bayes model exhibits behavior similar tree model slightly accurate 
augmenting naive bayes model significantly hurts classification performance 
splice dataset structure identification presents summary tree structures learned dataset form cumulated adjacency matrix 
adjacency matrices graph structures obtained experiment summed 
size black square coordinates proportional value th element cumulated adjacency matrix 
square means respective element 
adjacency matrix symmetric half matrix shown 
see tree structure stable trials 
variable represents class variable hypothetical splice junction situated variables 
shows splice junction variable depends dna sites vicinity 
sites remote splice junction dependent immediate neighbors 
examining tree parameters edges adjacent class variable observe variables build certain patterns splice junction random uniformly distributed absence splice junction 
patterns extracted learned trees shown 
displays cumulative adjacency matrix trees fit examples splice data set smoothing 
size square coordinates ij represents number trees edge variables square means number 
lower half matrix shown 
class variable 
group squares bottom shows variables connected directly class 
variable relevant classification 
surprisingly located vicinity splice junction 
subdiagonal chain shows rest variables connected immediate neighbors 
lower left edge upper right edge 
ei junction exon intron tree ca ag true ca ag junction intron exon 
tree ct ct ct ct true ct ct ct ct ct encoding ei splice junctions discovered tree learning algorithm compared ones watson molecular biology gene 
positions sequence consistent variable numbering splice junction situated positions 
symbols boldface indicate bases probability symbols indicate bases groups bases high probability indicates position occupied base non negligible probability 
true encodings ei junctions 
match encodings perfect 
conclude domain tree model provides classifier discovers model physical reality underlying data 
note algorithm arrives result absence prior knowledge know variable class variable know variables sequence result obtained indices variables scrambled 
splice dataset feature selection examine single tree classifier splice data set closely 
markov properties tree distribution probability class variables depends neighbors variables class variable connected tree edges 
tree acts implicit variable selector classification variables adjacent queried variable set variables called markov blanket relevant determining probability distribution 
property explains observed preservation accuracy tree classifier size training set decreases variables relevant class dependence parametrized independent pairwise probability tables parameters fit accurately relatively examples 
long training set contains data establish correct dependency structure classification accuracy degrade slowly decrease size data set 
cumulated adjacency matrix trees original set variables augmented noisy variables independent original ones 
matrix shows tree structure original variables preserved 
explanation helps understand superiority tree classifier models delve small subset variables relevant classification 
tree finds correctly 
classifier able perform feature selection reasonably hindered remaining irrelevant variables especially training set small 
markov blanket tree classifies way naive bayes model markov blanket variables inputs 
naive bayes model built feature selector input variables relevant class distributions roughly values consequently posterior serves classification factors corresponding simplify little influence classification 
may explain naive bayes model performs splice classification task 
notice variable selection mechanisms implemented tree classifier naive bayes classifier 
verify single tree classifier acts feature selector performed experiment splice data 
augmented variable set variables values randomly independently assigned probabilities 
rest experimental conditions training set test set number random restarts identical splice experiment 
fit set models small smoothing 
structure new models form cumulative adjacency matrix shown 
see structure original variables unchanged stable noise variables connect random uniform patterns original variables 
expected examining structure classification performance new trees affected newly introduced variables fact average accuracy trees variables higher accuracy original trees 
accelerated tree learning algorithm argued mixture trees approach significant advantages general bayesian networks terms algorithmic complexity 
particular step em algorithm mixtures trees algorithm scales quadratically number variables linearly size dataset step linear mixtures trees situation pass em algorithm quadratic linear time complexity recommends mt approach large scale problems quadratic scaling problematic particularly large problems 
section propose method reducing time complexity mt learning algorithm demonstrate empirically large performance gains able obtain method 
concrete example kind problems mind consider problem clustering classification documents information retrieval 
variables words vocabulary data points documents 
document represented binary vector component equal word document equal word 
typical application number documents order vocabulary size numbers fitting single tree data requires counting operations 
note domain characterized certain sparseness particular document contains relatively small number words components binary vector 
section show take advantage data sparseness accelerate algorithm 
show sparse regime rank order mutual information values computing values 
show speed computation sufficient statistics exploiting sparseness 
combining ideas yields algorithm accl accelerated chow liu algorithm provides significant performance gains running time memory 
accl algorithm accl algorithm case binary variables presenting extension general discrete variables section 
binary variables say variable takes value say 
loss generality assume variable times dataset 
target distribution assumed derived set observations size denote nv number times variable dataset nuv number times variables simultaneously 
call events cooccurrence marginal puv puv nuv puv nu nuv puv nv nuv puv nv nu nuv information necessary fitting tree summarized counts nv nuv 
consider represented counts 
easy extension handle non integer data data points weighted real numbers 
define notion sparseness motivates accl algorithm 
denote number variables observation define data sparseness max 
example data documents variables represent words vocabulary represents maximum number distinct words document 
time memory requirements algorithm describe depend sparseness lower sparseness efficient algorithm 
algorithm realize largest performance gains recall algorithm greedily adds edges graph choosing edge currently maximal value mutual information 
algorithm describe involves efficient way rank order mutual information 
key aspects algorithm compare mutual information non cooccurring variables compute cooccurrences list representation 
comparing mutual informations non cooccurring variables focus pairs cooccur nuv 
pair mutual information iuv function nu nv analyze variation mutual information respect nv corresponding partial derivative iuv nv nv log nu nv result implies variable variables nuv nuv nv nv implies iuv iuv 
observation allows partially sort mutual informations iuv non cooccurring pairs computing 
sort variables number occurrences nv 
store result list gives total ordering variables list nv nu 
define list variables ordering cooccurring nuv 
list sorted decreasing nv implicitly decreasing iuv 
data sparse pairs variables cooccur 
creating lists large number values mutual information partially sorted 
showing construction examine efficient way computing nuv counts data sparse 
computing cooccurrences list representation 
set observations binary variables 
efficient represent observation list variables respective observation 
data point 
represented list xlist list 
space required lists sn smaller space required binary vector representation data nn 
note total number nc cooccurrences dataset nc nuv 
variables cooccur set cooccurrence lists created 
contains records iuv nuv sorted decreasing iuv 
represent nv vn list nuv sorted iuv list nuv sorted nv virtual heap edge data structure supplies candidate edge 
vertically left variables sorted decreasing nu 
lists sorted decreasing iuv virtual list sorted decreasing nv 
maximum elements lists inserted fibonacci heap 
maximum iuv extracted maximum fibonacci heap 
lists contain elements complements nuv 
shown computation cooccurrence counts construction lists takes amount time proportional number cooccurrences nc logarithmic factor log 
comparing value time compute puv algorithm see method replaces dimension domain memory requirements lists proportional nc 
algorithm data structures 
described efficient methods computing cooccurrences partially sorting mutual informations 
aim create mechanism output edges decreasing order mutual information 
shall set mechanism form fibonacci heap called contains element represented edge highest mutual uv algorithm accl input variable set size dataset xlist 
procedure kruskal 
compute nv create list variables sorted decreasing nv 
compute cooccurrences nuv create lists 
create vlist argmax iuv insert nuv iuv 
kruskal store nuv edges uv added 
compute probability table tuv nu nv nuv output accl algorithm 
information edges eliminated 
record form nuv iuv iuv key sorting 
maximum extracted edge replaced largest terms iuv edge lists 
perform task data structures shown 
kruskal algorithm construct desired spanning tree 
summarizes resulting algorithm 
running time algorithm requires sn computing nv log sorting variables log step step nk log nc kruskal algorithm nk number edges examined algorithm log time extraction nc upper bound number elements lists elements need skipped occasionally extract variables virtual lists creating probability tables step 
summing terms obtain upper bound running time accl algorithm log sn log nk log 
ignore logarithmic factors simplifies sn nk 
steps mean full line standard deviation maximum dotted line number steps nk kruskal algorithm runs plotted log ranges 
edge weights sampled uniform distribution 
constant bound polynomial degree variables nk 
nk range nk worst case complexity accl algorithm quadratic empirically show find dependence nk generally subquadratic 
random graph theory implies distribution weight values edges kruskal algorithm take time proportional log 
verify result conducted set monte carlo experiments ran kruskal algorithm sets random weights domains dimension 
runs performed 
plots average maximum nk versus log experiments 
curve average displays essentially linear dependence 
memory requirements storage data results need nc space store cooccurrences lists kruskal algorithm 
additional space accl algorithm 
discrete variables arbitrary arity briefly describe extension accl algorithm case discrete domains variables take values 
extend definition data sparseness assume variable exists special value appears higher frequency values 
value denoted loss generality 
example medical domain value variable represent normal value abnormal values variable designated non zero values 
occurrence variable event cooccurrence means non zero data point 
define number non zero values observation sparseness maximum data set 
exploit high frequency zero values represent occurrences explicitly creating compact efficient data structure 
obtain performance gains mutual informations non cooccurring variables 
computing cooccurrences avoid representing zero values explicitly replacing data point list xlist xlist list xv xv 
cooccurrence represented quadruple xu xv xu xv 
cooccurrence count nuv way contingency table ij uv 
ij uv represents number data points 
counting storing cooccurrences done time max larger amount memory necessitated additional need store non zero variable values 
mutual informations goal mutual informations iuv cooccur theorem shows done exactly 
theorem discrete variables cooccur dataset nv nw number datapoints respectively iuv respective empirical mutual informations sample nv nw iuv equality identically 
proof theorem appendix 
implication theorem accl algorithm extended variables values making minor modification replacement scalar counts nv nuv vectors respectively contingency tables ij uv 
experiments section report results experiments compare speed accl algorithm standard method artificial data 
experiments binary domain dimension varies 
data point fixed number variables value 
sparseness takes values 
data generated artificially constructed non uniform non factored distribution 
pair set points created 
data set algorithm accl algorithm fit single tree distribution 
running times plotted 
improvements accl standard version spectacular learning tree variables data points takes hours standard algorithm seconds time running time accl full line dotted line algorithms versus number vertices different values sparseness accelerated version data sparse 
sparse regime accl algorithm takes minutes complete improving traditional algorithm factor 
note running time accelerated algorithm nearly independent dimension domain 
recall hand number steps nk grows implies bulk computation lies steps preceding kruskal algorithm proper 
computing cooccurrences organizing data time spent 
confirms running time traditional algorithm grows quadratically independent concludes presentation accl algorithm 
method achieves performance gains exploiting characteristics data sparseness problem weights represent mutual information external maximum weight spanning tree algorithm proper 
algorithm obtained worst case sn typically sn represents significant asymptotic improvement traditional chow liu algorithm 
large accl algorithm gracefully degrades standard algorithm 
algorithm extends non integer counts directly applicable mixtures trees 
seen empirically significant part running time spent computing cooccurrences 
prompts learning statistical models large domains focus efficient computation usage relevant sufficient statistics 
direction includes structural em algorithm trees :10.1.1.24.1555:10.1.1.27.7911
closely related representation pairwise marginals puv counts 
fact representation viewed reduced tree stores pairwise statistics 
consequently tree representation computed exploited steps accl algorithm 
versions accl algorithm see 
kruskal number steps kruskal algorithm nk versus domain size measured accl algorithm different values mixture trees mt probabilistic model joint probability distributions represented finite mixtures tree distributions 
tree distributions number virtues representational computational statistical limited expressive power 
bayesian markov networks achieve significantly greater expressive power retaining representational virtues trees incur significantly higher costs computational statistical fronts 
mixture approach provides alternative upgrade path 
bayesian markov networks distinguished relationships edges statistical model selection procedures networks generally involve additions deletions single edges mt model groups overlapping sets edges mixture components edges added removed maximum likelihood algorithm constrained fit tree models mixture component 
seen straightforward develop bayesian methods allow finer control choice edges smooth numerical parameterization component models 
chow liu basic maximum likelihood algorithm fitting tree distributions provides step em algorithm showed ensembles trees solve classification problems tree models class conditional density classes 
approach pursued friedman geiger goldszmidt emphasized connection naive bayes model empirical results demonstrated performance gains obtained enhancing naive bayes allow connectivity attributes :10.1.1.30.9978
contribution general line research treat ensemble trees mixture distribution 
mixture approach provides additional flexibility classification domain choice variable need class label allows architecture applied unsupervised learning problems 
algorithms learning inference relatively benign scaling inference linear dimensionality step em learning algorithm quadratic favorable time complexity important virtue tree approach 
particularly large problems arise information retrieval applications quadratic complexity onerous 
allow mt model cases developed accl algorithm exploiting data sparseness paying attention data structure issues significantly reduced run time examples speed obtained accl algorithm orders magnitude 
classes graphical models structure learned efficiently data 
consider class bayesian networks topological ordering variables fixed number parents node bounded fixed constant class optimal model structure target distribution time greedy algorithm 
models share trees property matroids 
matroid unique algebraic structure maximum weight problem particular maximum weight spanning tree problem solved optimally greedy algorithm 
graphical models matroids efficient structure learning algorithms interesting open problem find additional examples models 
appendix appendix prove theorem section theorem discrete variables cooccur dataset nv nw number data points respectively iuv respective empirical mutual informations sample equality identically 
proof 
notation pv nv nw iuv pv pv pv represent empirical probabilities value respectively 
entropies denoted aim show iuv 
pv note chain rule expression entropy discrete variable 
particular entropy hv multivalued discrete variable decomposed way hv pv log pv pv log pv hv pv pv pv log pv pv hv hv pv hv note mutual information non cooccurring variables iuv hu second term conditional entropy pv expand decomposition 
pv non zero time non zero values paired zero values consequently hu 
term denoted entropy binary variable probability 
probability equals pu pv note order obtain non negative probability equation needs pu pv condition satisfied cooccur 
replacing previous equations formula mutual information get iuv pu log pu pv log pv pu pv log pu pv expression remarkably depends pu pv 
partial derivative respect pv yields iuv pv log pv pu pv value negative independently pv 
shows mutual information increases monotonically occurrence frequency pv 
note expression derivative result obtained binary variables 
acknowledge support project national science foundation nsf iis multidisciplinary research program department defense muri 
bishop latent variable models 
learning graphical models jordan ed 
mit press cambridge ma 
boutilier friedman goldszmidt koller context specific independence bayesian networks :10.1.1.16.1782
proceedings th conference uncertainty ai morgan kaufmann pp 

buntine guide literature learning graphical models 
ieee transactions knowledge data engineering 
cheeseman stutz bayesian classification autoclass theory results 
advances knowledge discovery data mining fayyad shapiro smyth uthurusamy eds 
aaai press pp 

cheng bell liu learning belief networks data information theory approach 
proceedings sixth acm international conference information knowledge management 
chow liu approximating discrete probability distributions dependence trees 
ieee transactions information theory 
cooper herskovits bayesian method induction probabilistic networks data 
machine learning 
cormen leiserson rivest algorithms 
mit press cambridge ma 
cowell dawid lauritzen spiegelhalter probabilistic networks expert systems 
springer new york ny 
dayan zemel competition multiple cause models 
neural computation 
dempster laird rubin maximum likelihood incomplete data em algorithm 
journal royal statistical society 
fredman tarjan fibonacci heaps uses improved network optimization algorithms 
journal association computing machinery 
frey hinton dayan wake sleep algorithm produce density estimators 
neural information processing systems cambridge ma touretzky mozer hasselmo eds mit press pp 

friedman bayesian structural em algorithm 
proceedings th conference uncertainty ai san francisco ca morgan kaufmann pp 

friedman geiger goldszmidt bayesian network classifiers 
machine learning 
friedman getoor efficient learning constrained sufficient statistics 
proceedings th international workshop artificial intelligence statistics 
friedman getoor koller pfeffer learning probabilistic relational models 
proceedings th international joint conference artificial intelligence ijcai pp 

friedman goldszmidt lee bayesian network classification continous attributes getting best discretization parametric fitting 
proceedings international conference machine learning icml 
geiger entropy learning algorithm bayesian conditional trees 
proceedings th conference uncertainty ai morgan kaufmann publishers pp 

geiger heckerman knowledge representation inference similarity networks bayesian multinets 
artificial intelligence 
hastie tibshirani discriminant analysis mixture modeling 
journal royal statistical society 
heckerman geiger chickering learning bayesian networks combination knowledge statistical data :10.1.1.156.9918
machine learning 
hinton dayan frey neal wake sleep algorithm unsupervised neural networks 
science 
jelinek statistical methods speech recognition 
mit press cambridge ma 
jordan jacobs hierarchical mixtures experts em algorithm 
neural computation 
kontkanen myllymaki tirri constructing bayesian finite mixture models em algorithm 
tech 
rep university helsinki department computer science 
lauritzen em algorithm graphical association models missing data 
computational statistics data analysis 
lauritzen graphical models 
clarendon press oxford 
lauritzen dawid larsen 
independence properties directed markov fields 
networks 
mixture models inference applications clustering 
marcel dekker ny 
jaakkola bayesian learning tree distributions 
preparation 
jaakkola tractable bayesian learning tree distributions 
proceedings th conference uncertainty ai san francisco ca boutilier goldszmidt eds morgan kaufmann pp 

jordan estimating dependency structure hidden variable 
neural information processing systems jordan kearns solla eds mit press pp 

learning mixtures trees 
phd thesis massachusetts institute technology 
michie spiegelhalter taylor machine learning neural statistical classification 
ellis horwood new york 
monti cooper bayesian network combines finite mixture model naive bayes model 
tech 
rep university pittsburgh march 
moore lee cached sufficient statistics efficient machine learning large datasets :10.1.1.27.7911
journal artificial intelligence research 
murphy aha irvine machine learning repository 
ftp ftp ics uci edu pub machine learning databases 
neal hinton view em algorithm justifies incremental sparse variants :10.1.1.33.2557
learning graphical models jordan ed nato science series 
kluwer academic publishers pp 

ney essen kneser structuring probabilistic dependences stochastic language modelling 
computer speech language 
noordewier towell shavlik training knowledge neural networks recognize genes dna sequences 
advances neural information processing systems lippmann moody touretzky eds morgan kaufmann publishers pp 

pearl probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufman publishers san mateo ca 
philips moon rauss rizvi feret evaluation methodology face recognition algorithms 
proceedings conference computer vision pattern recognition san juan puerto rico 
rasmussen neal hinton van camp revow ghahramani delve manual 
www cs utoronto ca delve 
rissanen stochastic complexity statistical inquiry 
world scientific publishing new jersey 
rubin thayer em algorithms ml factor analysis 
psychometrika 
saul jordan mean field learning algorithm unsupervised neural networks 
learning graphical models jordan ed 
mit press cambridge ma pp 

shafer shenoy probability propagation 
annals mathematics artificial intelligence 
smyth heckerman jordan probabilistic independence networks hidden markov probability models 
neural computation 
thiesson meek chickering heckerman learning mixtures bayes networks 
tech 
rep msr por microsoft research 
watson hopkins roberts weiner molecular biology gene ed vol 
benjamin cummings publishing menlo park ca 
west graph theory 
prentice hall upper saddle river nj 

