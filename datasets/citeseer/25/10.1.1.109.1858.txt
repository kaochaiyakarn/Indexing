double chunking solving svms large datasets fernando rez cruz bal vidal antonio art rodr guez gatsby computational neuroscience unit university college london queen square 
london wc ar united kingdom 
fernando gatsby ucl ac uk signal theory communications department university carlos iii madrid 
universidad madrid spain novel approach solve svm large training sets 
propose substitute shrinking technique double chunk able exploit cache memory efficiently obtain solution considerably runtime complexity 
support vector machines svms state art tools solving input output knowledge discovery problems sch lkopf smola rez cruz bousquet 
svm binary classification defines margin exclusion region classes maximized ensure obtained classifier able generalise previously unseen patterns 
labelled training dataset xi yi xi rd yi svm computes maximum margin solution constraint optimization problem min subject yi xi define linear classifier feature space nonlinear input space penalty applied margin errors samples fulfil margin requirement 
solve problem lagrange multipliers construct wolfe dual fletcher leading quadratic programming qp problem define quadratic upper bound solved iteratively rez cruz known iterative re weighted square procedure 
cases svm solution representer theorem kimeldorf wahba fairly general conditions allows represent linear combination samples training set iyi xi 
representer theorem allows optimization functional defined terms dot products samples feature space 
need kernel transformation nonlinear mapping 
sample potentially part solution optimization procedures need kernel matrix contains pairs samples 
directly applied samples 
osuna authors proposed chunking mechanism solve svms large datasets 
chunking mechanism relies solution svm sparse zero samples known bounded support vectors 
samples hold equality unbounded support vectors value needs determined learning procedure 
chunking mechanism allows solve svm training samples chunk 
samples left training samples fulfil constraints solving chunk 
procedure repeated svm solution reached 
convergence procedure shown keerthi 
procedure efficiently implemented svm light joachims including cache memory widely kernels shrinking technique reduces training sample active set change value 
chunking procedure solved samples iteration qp formulation solved analytically 
procedure known sequential minimal optimization smo platt current powerful svm solver libsvm chang 
libsvm samples violate constraints iteration chunk svm solution reached 
incorporates cache memory shrinking mechanism described joachims 
limitations chunking algorithms presents large datasets samples times algorithm wander approximation solution built converge fast solution cache shrinking 
wandering effect due samples presenting similar output values clear picked obtain largest improvement svm solution 
iterations wasted little improvement undoing previous changes 
training set large cache memory emptied iterations gaining 
shrinking reduce active set samples look potentially candidates entering chunk 
proposed double chunking technique solving svms large datasets prevent wandering happening 
know approximation solution able chunking procedure efficiently get final svm solution 
working samples compute solution thousands 
reduced dataset chunking www csie ntu edu tw cjlin libsvm allow find solution fast 
partial result give rough idea svm solution compute error remaining samples choose samples enter large chunk 
solve large chunk chunking technique giving name algorithm repeat process svm solution reached 
double chunking allows able get faster svm solution standard chunking technique making cache memory available avoiding wandering nature large datasets 
double chunking solve svm need compute kernel matrix 
problems small number samples problem addressed directly 
larger ones need different procedure store kernel matrix memory 
osuna authors proposed chunking scheme solved sequence qp problems similar svm lead svm solution samples 
chunking procedure works follows chooses samples number training examples usually solved qp tools obtains svm solution 
computes error ei yi xi samples 
samples fulfil restrictions choose subset size solve modified svm modification previous solution 
repeat procedure samples fulfil restrictions svm solution 
procedure computational burden solving qp problems computing error sample need compute kernel samples support vectors iteration 
important choose samples enter svm functional iteration minimise number times need compute kernels 
procedure works quite number training samples relatively small thousands samples number samples gets larger heuristics decide samples qp problem accurately loses iterations trying locate solution samples classified svm solved 
number rows kernel matrix stored samples row 
constantly emptying gain 
illustrate point shown running time solve hand written digit recognition task information experimental section increasing number samples seen solve problem samples need times computational time solving tenth training samples 
behaviour standard datasets solving svms chunks 
suppose solution samples starting point svm solution solution final svm solution easily reached 
solution initialized chunking procedure obtain svm solution iterations 
ideas propose solve svm double chunking procedure 
construct large chunk samples solve svm available samples 
problems size dealt directly solved efficiently chunking procedures ones previously described small chunk samples giving name algorithm 
obtain solution large chunk classify remaining samples find ones fulfil restrictions 
solution large chunk approximation easily pick samples provides largest improvements form large chunk 
show algorithmic procedure 
functions selection running time minutes training samples thousands form error selection selection show runtime solve svm increasing number samples 
pseudocode double procedure represented 
selection choose respectively samples form small large chunk iteration 
form construct kernel matrix going optimization step 
error computes error samples large chunk maintains cache memory 
drop loop left standard chunking procedure shrinking 
double chunking procedure readily improved computing error deciding samples enter large chunk selection computational costly step show experimental section need compute kernel matrix support vectors large chunk training samples obtain error sample 
propose computing error samples decide ones form new large chunk compute error sample sample samples fulfil restrictions introduced large chunk fill 
procedure avoids computing error samples iterations significantly reducing runtime complexity 
possible drawback selecting best set samples iteration force take iterations need 
clear getting samples violates restrictions improve solution perform extra iteration considerably cheap terms computational time compare computing error samples large chunk solved gaining speed convergence 
way computing error second advantage 
know processed training samples computed error solving large chunk 
case resolution large chunk need obtained exactly know extra iterations come 
solve svm evaluated samples enforce support vector fulfils restrictions obtaining approximate partial solution significantly fewer iterations 
increase robustness procedure include samples fulfil restrictions chunk include samples unbounded support vectors change weight resolution large chunk samples previous chunk ei change value 
samples takes consideration previous solution avoids new samples reverse previous solution 
shrinking versus double chunk shrinking techniques standard chunking programs reduce runtime complexity effective large datasets 
technique tries locate samples clearly fulfil restrictions eliminate learning procedure working fewer training samples 
allows cache memory samples ei get faster svm solution 
svm solution reached samples verify fulfil restrictions 
seen pruning mechanism training examples 
double chunking previous section regarded growing technique start samples move evaluate samples training set 
double chunk seeks purpose shrinking want evaluate error sample iteration small chunk 
believe double chunking effective shrinking large dataset shrinking process takes long time reduced number training samples 
dataset large chunking procedure wander shrinking take place emptying cache memory constantly 
double chunking procedure concentrate subset samples iteration able exploit cache memory better 
experiments compared double chunk implementation libsvm best svm code nowadays 
mnist handwritten digit recognition standards datasets machine learning community hardest problems solve samples 
presents thousands samples support vectors ones slow svm software need specified exactly 
implemented double chunking procedure procedure proposed rez cruz fastest chunking scheme time compared libsvm procedure 
mnist dataset training samples dimensional space 
selected gaussian rbf kernel xi xj exp xi xj input space dimension set parameters give optimal results cross validation techniques 
mbytes cache memory samples double chunk constraint breaking samples 
samples small chunk 
stopping criteria maximum constraint deviation reported training time increase number samples task compute binary classifier zero labelling rest 
number samples libsvm double chunk number error table runtime complexity minutes libsvm double chunking procedure increasing number training samples 
column show test error examples see large datasets samples procedures takes similar running time converge svm solution 
large datasets double chunk better libsvm 
explained double chunking avoids wandering behaviour iterations solving svm samples 
libsvm hard time guessing best samples optimised iteration 
solving samples example needed iterations large chunk samples took 
remaining time spent selection computing error samples outside large chunk iteration 
seen time compute error samples actual optimization commented section 
positive label libsvm double chunk svs table runtime complexity minutes libsvm double chunking procedure different labelling positive class 
shows number support vectors svs 
concentrate problem samples 
run experiments possible labelling versus versus parameters zero versus 
plotted training time procedure table 
table seen results consistently better wide range different solutions number support vectors varies significantly labelling 
change parameters show double chunking faster conditions 
modified cache memory mb 
change parameter 
results plotted table 
cache mb libsvm double chunk libsvm double chunk libsvm double chunk table runtime complexity minutes libsvm double chunking procedure function cache memory svm parameters 
computed svm solution double chunking modifying parameters show runtime complexity robust setting table 
large chunk size double chunk small chunk size double chunk fraction double chunk table runtime complexity minutes double chunking procedure different number samples large small chunk fraction samples violates constrain enter large chunk iteration 
novel approach dealing large problems get svm solution reduced runtime compare best chunking scheme 
exploit fact large datasets cache store rows kernel matrix concentrate samples thousands get approximate solution obtain exact svm solution runtime complexity 
method improve advisable large chunk rigid incorporate samples improve runtime complexity avoid extra iterations 
done monitoring number support vectors open issue done 
chang hsu lin 

analysis decomposition methods support vector machines 
ieee transaction neural networks 
fletcher 

practical methods optimization 
john wiley sons chichester second edition 
joachims 

making large scale svm learning practical 
sch lkopf burges smola editors advances kernel methods support vector learning pages 
press cambridge ma 
keerthi bhattacharyya murthy 

improvements platt smo algorithm svm classifier design 
technical report dept csa bangalore india 
guppy mpe nus edu sg smo mod ps gz 
kimeldorf wahba 

results spline functions 
journal mathematical analysis applications 
osuna freund girosi 

improved training algorithm support vector machines 
principe morgan wilson editors neural networks signal processing vii proceedings ieee workshop pages amelia island new york 
ieee press 
rez cruz diana zquez art rodr guez 

fast training support vector classifiers 
leen dietterich tresp editors advances neural information processing systems cambridge ma 
press 
rez cruz guez 

convergence procedure support vector machine solution 
neural computation 
accepted 
rez cruz bousquet 

kernel methods potential signal processing 
signal processing magazine 
platt 

sequential minimal optimization fast algorithm training vector machines 
sch lkopf burges smola editors advances kernel methods support vector learning pages 
press cambridge ma 
sch lkopf smola 

learning kernels 
press 
