reinforcement learning factored markov decision processes brian sallans thesis submitted conformity requirements degree doctor philosophy graduate department computer science university toronto copyright brian sallans reinforcement learning factored markov decision processes brian sallans doctor philosophy graduate department computer science university toronto learning act optimally complex dynamic noisy environment hard prob lem 
various threads research reinforcement learning animal conditioning oper ations research machine learning statistics optimal control come offer solutions problem 
thesis novel algorithms learning dynamics learning value function selecting actions markov decision processes 
problems considered high dimensional factored state action spaces fully partially observable 
ap proach take recognize similarities problems solved rein learning graphical models literature combine techniques fields novel ways 
particular new algorithms 
dbn algorithm learns compact representation core process partially observable mdp 
inference dbn intractable approximate inference maintain belief state 
belief state action value function learned reinforcement learning 
show dbn algorithm solve pomdps large state spaces useful hidden state 
second poe algorithm learns approximation value functions large fac state action spaces 
algorithm approximates values negative free energies product experts model 
model parameters learned efficiently inference tractable product experts 
show actions large factored action spaces brief gibbs sampling 
new algorithms take techniques machine learning community apply new ways reinforcement learning problems 
simulation results show new methods solve large problems 
dbn method solve pomdp hidden state space observation space size greater dbn model core process states represented binary variables 
poe method find actions action spaces size ii am grateful people contributed thesis making time computer science graduate program great learning experience 
thesis advisor geoffrey hinton patience guidance seemingly boundless enthusiasm 
input direction invaluable creation thesis research contains 
members committee input interest radford neal zoubin ghahramani craig boutilier 
committee members extensive excellent comments early drafts thesis am grateful 
am grateful brendan frey mike mozer comments questions helped clarify thesis 
completed visiting gatsby computational neuro science unit university college london 
particularly owe great deal peter dayan patience comments 
alberto mendelzon hadzilacos encouraged supported time toronto 
current members neural networks research group university toronto gatsby computational neuroscience unit greatly contributed thesis making fun stimulating attias andrew brown brendan frey morris alberto mike revow sam roweis yee teh emo toronto gatsby 
particular andy alberto yee took time valuable comments initial drafts thesis 
research benefitted constant flow visitors neural networks lab gatsby 
love go parents brother tim agement support 
love helped give motivation see thesis completion 
financially supported natural sciences engineering research council canada post graduate scholarship gatsby charitable foundation 
financially supported natural sciences engineering research council institute robotics intelligent systems gatsby charitable foundation 
iii contents ii iii list tables vii list figures viii nomenclature xiv motivation 
putting context 
contributions thesis 
organization thesis 
background markov decision processes 
dynamic programming 
temporal difference learning 
partially observable markov decision processes 
exact algorithms 
approximation algorithms 
generative models control 
influence diagrams 
dynamic bayesian networks 
factored markov decision processes 
learning inference 
iv expectation maximization algorithm 
exact inference 
variational methods 
monte carlo methods 
projective filtering 
factored states compact representations approximations 
dynamic sigmoid belief network 
approximate inference 
learning process parameters 
automatic relevance determination 
approximating value function 
polynomial approximators 
smooth partially observable value approximation 
multilayer perceptron 
eligibility traces 
simulation results 
new york driving 
restricted ny driving 
visual new york driving 
eligibility traces 
factored actions stochastic policies 
products experts 
tractable models bayesian networks 
discrete states actions 
non factored policies 
continuous states actions 
rate coded rbm 
product 
learning model parameters 
sarsa 
learning 
sampling actions 
sequential action sampling 
alternating block sampling 
local minima 
exploration 
simulation results 
stochastic policy task 
blockers task 
large action task 
rocket task 
discussion states 
model features 
models reward 
convergence 
actions 
macro actions 
multiagent language learning 
conditional completion 
factored states actions 

direct policy methods 
hierarchical learning 
task specific distinctions 
summary 
task functions new york driving 
restricted ny driving 
visual ny driving 
stochastic policy 
blockers 
large action 
rocket 
bibliography vi list tables sensory input new york driving task 
algorithms tested new york driving task 
optimal stochastic policy factored representation 
poe blocker experimental results 
vii list figures markov decision process 
squares indicate visible variables dia indicate actions 
state dependent previous state action reward depends current state action 
partially observable markov decision process 
circles indicate hidden variables squares indicate visible variables diamonds indicate actions 
hidden state dependent previous state action reward depends current state action 
observation just dependent current state 
influence diagram 
squares indicate visible variables diamonds indicate actions 
unobserved chance nodes indicated circles 
value node ellipse 
dynamic bayesian network 
squares indicate visible variables di indicate actions 
hidden state variables time dependent action time hidden variable values time observation depends current state variables 
re ward depends current state action 
schematic projection filtering 
original distribution density projected space simpler distributions 
simpler distribution updated bayes rule result re projected space simpler distributions 
architecture dynamic sigmoid belief network 
circles indicate ran dom variables filled circle observed empty circle unobserved 
squares action nodes diamonds rewards 
iteration iteration squared difference mean field parameters 
graph shows median differences randomly generated ds bns 
vertical lines contain data 
new york driving simulator 
viii results algorithms new york driving 
random driver flat learning ll localist linear lm localist mlp dl distributed linear dla distributed linear ard dm distributed mlp dma distributed mlp ard ut tree mccallum ga genetic algorithm stochastic glickman sycara ga genetic algorithm sigmoid glickman sycara human driver 
results algorithms restricted ny driving 
random driver flat learning qs flat learning stochastic policy ll localist linear lm localist mlp dla distributed linear ard dma distributed mlp ard human driver 
examples new york driving state associated image visual new york driving 
results algorithms visual ny driving 
random driver table learning reduced input ll ll ll localist lin ear states lm lm lm localist mlp states lm localist mlp reduced input states lma localist mlp ard states dma distributed mlp ard hidden variables dma dbn mlp ard reduced input hid den variables human driver 
agent green car clear road fast car yellow blowing horn 
agent looks right sees looming gray car 
agent gaze direction symbolized white cone 
agent passes gray car sees tan car lane 
agent changes lanes right moving tan car 
lets fast yellow car speed away top display 
tan car looming ahead 
agent looks right sees nearby red car 
looks left sees far tan car 
agent changes lanes left moving tan car 
slow cars left drop bottom display 
tan car ahead looms closer 
agent looks right sees free lane 
shifts right passing looming tan car 
ix emission features learned visual new york driving 
feature corresponds binary hidden unit active 
binary units relevant output omitted 
white indicates higher black indicates lower probability activity associated binary unit 
example feature progressively active car gets closer small car outlines dark bigger car outlines get lighter 
feature active cars maximum distance small white dot inactive shoulder images small gray dots horizon 
feature interested cars active horn agent 
performance naive traces function restricted ny driving 
center line plot shows mean reward trials 
dashed lines contain reward values trials 
straight lines show reported final performance mccallum tree algorithm lower glickman sycara genetic algorithm upper 
performance watkins traces function restricted ny driving 
center line plot shows mean reward trials 
dashed lines contain reward values trials 
straight lines show reported final performance mccallum tree algorithm lower glickman sycara genetic algorithm upper 
boltzmann product experts 
estimated action value setting state action units holding units fixed computing free energy network 
actions selected holding state variables fixed sampling action variables 
multinomial state action variable represented set binary units exactly 
illustration restricted boltzmann machine represent simple non factored stochastic policy 
rate coded restricted boltzmann machine 
real valued activities units thought total activity ensemble duplicate units 
equivalently think single unit emit number spikes time interval 
real valued activity rate unit fires 
example network troublesome block sampling 
conditional distributions block sampler problematic example 
top plots show conditional distribution hidden configuration action configuration 
bottom plots show conditional distribution action configuration hidden configuration 
temperature get stuck suboptimal action 
sequential sampling avoid local minima 
frequency versus action number types gibbs sampling 
free energy configuration shown bar 
case block sampling method frequently gets stuck local minimum 
results learning implicit stochastic policy 
hinton diagram shows sign magnitude biases units weights hidden action units 
white positive black negative 
area square indicates magnitude weight bias 
example blocker task 
agents get past blockers zone 
blockers pre programmed strategy agents operate blockers simultaneously 
example agent strategy learning blocker task 
agents initialized random locations bottom field 
agents run top playing field 
agents split run sides 
third agent moves middle zone 
features learned value function approximator 
features correspond stages shown 
feature corresponds hidden unit rbm 
hinton diagram shows agents order activate feature 
vector diagram indicates actions recommended feature 
histogram plot frequency activation feature versus time trial 
shows run feature tends active 
learned features localized state space action space 
feature activity localized time 
xi large action task 
space state bit vectors divided clusters nearest key state 
key state associated key action 
reward received learner number bits shared selected action key action current state 
results large action task 
optimal policy gives average reward 
random shows average return random action selection policy 
exploratory shows reward returned current value function current exploration temperature 
current shows reward returned current value function minimum ration temperature 
curve shows greedy policy respect current value function 
rocket task 
rocket green circle reach target blue ring 
state consists position velocity target posi tion 
action thrust red lines 
results rocket task 
random policy qp learning poe optimal 
example paths rbmrate training 
asterisk indicates rocket starting position 
mark indicates target position 
shows target hit trial finished 
indicates dots trajectory indicate equally spaced points time 
typically control variables saturate leading bang bang control 
correlation state attributes hidden unit activity hidden units 
axis indicates proportion time hidden unit active state attribute 
unit acting detector attribute value proportion near value near 
learning curve agent blockers problem pre learned macro actions 
plot shows average reward time step current policy averaged runs versus training time time steps 
xii connections language interpretation hidden variables conditionally bipartite boltzmann machine 
example shows connections agent system 
state variable particular agent connect directly corresponding action variable hidden word variable 
word variables connected action vari ables 
current state inference boltzmann machine tractable 
action state conditional completion 
actions optimistic pessimistic states sampled conditioned subset states actions 
dynamic poe 
energy hidden states time depend hidden states time observation action time rbm poe network trained bit stochastic policy task 
network trained simple direct policy method average actions probable penalized average actions 
xiii nomenclature general bold font indicates multivariate quantity 
upper case bold letters matrices lower case bold letters vectors 
single elements vector indexed subscript 
script capital letters indicate sets fields 
sets denoted curly braces 
sets denoted listing members xn subscripts xi 
vectors written elements square brackets denotes row vector denotes column vector 
time index single state variable multidimensional state variable si th element multidimensional state variable single action variable multidimensional action variable aj th element multidimensional action variable single observation variable multidimensional observation variable ok th element multidimensional observation variable multidimensional hidden variable hi belief state th element multidimensional hidden variable factored approximate belief state th element factored belief state transition parameter matrix dbn emission parameter matrix dbn generic parameter learning rate parameter immediate reward time total reward time task policy value function policy xiv action value function policy optimal value function optimal action value function eb es bellman error td error td error sarsa td error learning eligibility trace state action time discount factor td parameter delta function cross sum operator set union element wise multiplication matrix transpose xv chapter motivation learning agent placed new environment time hand knowledge environment 
just innate preconceptions 
explores environment modifies preconceptions new experience 
understands world learner start beneficial decisions 
agent moves environment bases actions information number sources 
include sensory input efferent copies motor commands memories previous inputs 
input noisy high dimensional 
actions agent takes time attributes common sensory input high dimensional uncertain effect world 
possible actions include motor commands information gathering inactivity 
agent able deal high dimensional uncertain states ac tions order operate complex environment 
making inferences state world noisy observations popular subject study artificial intelligence machine learning communities 
formalism graphical model 
chapter 
purpose graphical model capture distribution observed data probabilistic model 
graphical representation model indicates variables assumed conditionally independent 
observations variables inferring distribution unobserved hidden variables paramount impor tance learning parameters models 
exact approximate inference algorithms intensely studied graphical models literature 
acting certain uncertain information studied different body literature 
reinforcement learning involves learning act maximize reward signal samples sequences state action pairs environment 
closely linked markov decision processes stochastic dynamic programming 
reinforcement learning high dimensional states state uncertainty partial observability 
particular exact dynamic programming methods solving fully partially observable markov decision processes 
approximate methods dealing real valued state action variables 
surprisingly relatively little interaction communities 
history combining exact inference graphical models dynamic programming 
models called influence diagrams 
combining ap proximate inference methods approximate dynamic programming completely neglected 
influence diagrams solution techniques proposed problem selecting actions reduced problem doing inference bayesian network 
combination transformation approximate inference received attention 
general level purpose thesis combine methods pop ular distinct bodies literature doing derive novel algorithms solve previously unsolved classes problems 
particular study combina tions approximate inference approximate dynamic programming model learning chapter 
previously considered 
includes combining algorithms novel ways algorithms domain solve problems 
thesis shows techniques graphical models literature fruitfully applied problem learning act fully partially observable markov environments 
demonstrate approximate inference learning methods learn dynamics decision processes select actions transforming decision problems density estimation problems 
specifically focus high dimensional fully partially observable factored markov decision processes factored po mdps 
factored po mdps interesting properties common large interesting real world problems 
particular state world described factored mdp set variables 
natural intuitive way describe complex environment 
example naturally describe weather talking temperature precipitation wind visibility 
lower level information conveyed retinas available set neural activities indicate light hitting 
variable simple straightforward combination detailed description world relationship variables get complicated 
similarly actions take world described set values 
low level describe actions large set muscle activations set individual limb motions 
opposite extreme describe actions set intended results want get morning post office grab lunch 
thesis show techniques graphical models literature fruitfully applied reinforcement learning factored mdps 
density modeling techniques designed apply exactly kind data 
density models try find low dimensional data subspace manifold high dimensional chapter 
input space 
models discuss learn environment experience extract compact factored representations world allow inferences world experience 
thesis show factored models states actions learned experience select actions 
subsequent chapters discuss new ways inference learning tech niques applied factored po mdps 
new methods briefly outline problem solved 
indicate methods solve discuss simulation results 
final chapter discuss bringing techniques ways methods extended 
putting context conjunction approximate inference model learning reinforcement learning received little attention 
mapping problem solving influence diagrams inference bayesian networks investigated shachter peot zhang 
suggest approximate inference pursue possibility 
approximate inference compute expectations combined traditional action selection techniques solve influence diagrams charnes shenoy 
combination approximate inference solution methods pomdps investigated rodriguez parr koller thrun poupart ortiz boutilier 
poupart boutilier investigate tuning approximate inference algorithm explicitly maximize expected return 
methods assume knowledge dynamics process 
words methods take input transition emission distribution 
tadepalli ok learn conditional probability tables dynamic bayesian network represents factored mdp 
learn factored approximate value function 
consider par chapter 
tial observability 
chapter consider case compact model pomdp learned experience 
approximate inference maintain belief state approximate belief state value function learned select actions 
value function representations continuous action spaces investigated baird klopf doya sutton ram 
offer different way finding optimal action continuous space actions 
baird klopf probably closest flavour chapter new value function approximation architecture selection actions particularly easy 
jordan suggest mapping values actions energies similar chapter 
confine immediate reward problems 
solution factored po mdps investigated boutilier poole dearden boutilier schneider wong moore riedmiller boutilier dearden goldszmidt koller parr peshkin kim meuleau kaelbling dearden guestrin koller parr 
include factored representations states actions values 
boutilier poole dearden boutilier boutilier dearden goldszmidt consider case actions explicitly enumerated give algorithm finding exact value function compact representation mdp 
dean givan kim schneider wong moore riedmiller koller parr peshkin kim meuleau kaelbling guestrin koller parr algorithms finding approximate value functions approximate policies factored states actions 
assume value function policy expressed compact factored form component value function policy depends small pre specified subset state action variables 
direct policy actor critic methods factored continuous action spaces sutton mcallester singh mansour ng parr chapter 
koller ng jordan tsitsiklis 
methods interesting context chapter 
specific parameterized stochastic policies mentioned previous explicitly normalized 
re family available policies factored actions 
assume independence actions normalize small subsets action variables peshkin kim meuleau kaelbling guestrin koller parr 
contrast method chapter policy implicitly specified parameterized energy function 
see chapter possible parameterize policy implicitly normalizing sample actions policy distribution markov chain monte carlo methods 
expands class policies available direct policy methods 
direct policy methods discussed section 
am aware previous combines factored model learning approximate inference reinforcement learning pomdps 
am aware previous representation stochastic policies large discrete action spaces explicit normalized compact parameterization policy available 
preliminary originally appeared sallans sallans hinton 
contributions thesis thesis contributes 
researchers interested conjunction machine learning bayesian inference reinforcement learning 

new approach modeling factored pomdps factored value functions combines approximate inference model estimation factored value function ap proximation 
chapter 

new technique approximating value functions large factored discrete continuous mdps pomdps 

novel method selecting actions function approximator large action spaces 

new representation stochastic policies large action spaces 

matlab implementations decision problems studied thesis researchers 
organization thesis chapter covers background material necessary understand contributions subsequent chapters 
readers comfortable background topics feel free skip directly chapter 
section contains formal definition markov deci sion processes 
give overview methods solving fully observable mdps including dynamic programming temporal difference learning 
section extends discussion partially observable mdps details number exact algorithms solving pomdps 
discuss approximate algorithms solving pomdps 
section give description graphical models applied mdps pomdps including influence diagrams dynamic bayesian networks 
discuss advantages challenges factored representation po mdp 
con clude section short overview inference learning methods graphical models 
chapter details method learning compact representations core pro cess pomdp simultaneously approximating belief state action value func tion 
section discusses factored state representation outlines challenges need overcome learning represent act complex high dimensional chapter 
pomdp 
sections go detail discussing main challenges learning inference value function estimation 
subsequent sections clude example architecture dynamic sigmoid belief network 
section give results running selection algorithms set successively harder tasks 
chapter method allows compact representation action value functions high dimensional factored mdps efficient selection actions high dimensional action space estimated value 
sections discuss problems unique high dimensional action spaces suggest general method approximating values selecting actions 
section section show specific examples applying method case discrete continuous action spaces respectively 
section detail different update rules learn parameters approximate action value function discuss merits different updates 
section gives details selecting actions high dimensional action spaces 
section simulation results number discrete continuous tasks 
final chapter discuss methods relationship 
summarize contributions thesis 
conclude discussion challenges overcome speculation trends solving large po mdps 
chapter background learning act optimally complex dynamic noisy environment hard prob lem 
various threads research animal conditioning operations research machine learning bayesian statistics optimal control come offer solutions problem 
chapter discuss techniques related 
specifically survey reinforcement learning fully observable partially observ able markov decision problems section section including definitions problems methods finding exactly approximately optimal solutions 
discuss ways representing markov decision problems efficient compact ways section 
outline inference parameter learning compact pa models section 
markov decision processes agent moves environment bases actions information received number sources including sensory input efferent copies motor commands memories previous inputs 
information tells learner state world 
chapter 
background agent state time executes action receives reward environment 
called decision process 
task learning action perform reward called reinforcement learning sutton barto 
state dependent current state action decision process said obey markov property 
called markov decision process mdp bellman see 
sets states actions finite problem called finite mdp 
theoretical done reinforcement learning focused finite case discuss finite mdps review 
distinguish finite indefinite horizon problems task natural endpoint infinite horizon problems task continues forever 
action state reward markov decision process 
squares indicate visible variables diamonds indicate actions 
state dependent previous state action reward depends current state action 
formally mdp consists set states actions transition distribution reward distribution chapter 
background indexes time step ranges discrete set points time 
denote transition probability pij 
denote expected immediate reward received executing action state ri ri goal solving mdp find policy maximizes total expected reward received course task 
policy tells learning agent action take possible state 
possibly stochastic mapping states actions 
policy non stationary case different mapping states actions point time 
alternatively stationary case mapping point time 
finite horizon case policy may non stationary 
infinite horizon discounted case exist optimal stationary policy howard 
focus infinite horizon discounted case stationary policies 
expected return policy defined total reward expected policy 
current time 
note discounting required ensure sum infinite discounted rewards finite quantity optimized defined discount factor 
notice expectation taken respect policy 
assuming problem markov know optimal policy need function current state information required act optimally howard 
class mdps restricted important class problems 
assuming chapter 
background problem markov ignore history process prevent exponential increase size domain policy 
markov assumption underlies large proportion control theory machine learning signal processing including kalman filters kalman hidden markov models hmms rabiner juang time slice dynamic bayesian networks dean kanazawa dy namic programming dp bellman temporal difference learning td sutton 
dynamic programming dynamic programming technique finding solutions optimization problems 
similar divide conquer techniques problem broken sub problems solved 
amenable dynamic programming optimization problem exhibit optimal substructure optimal solution contain optimal solutions subproblems 
case mdps find policy produces greatest expected return 
knowledge transition probabilities pij expected immediate rewards ri stochastic policy calculate expected discounted return current state psj rs 
denotes current time 
function called value function policy 
value function tells agent expected return achieved starting state policy 
chapter 
background eq called bellman equations set linear equa tions state 
set coupled equations solved unknown values 
particular arbitrary initialization iterative update psj rs 
iteration converges unique fixedpoint 
technique called iterative policy evaluation 
iteration called full backup backup backs step dynamics system defining value previous state terms state full value state updated 
policy optimal assigns non zero probability actions maxi value function 
value function associated optimal policy called optimal value function max max psj rs 
eq nonlinear equations state un values optimal value function 
eq see problem computing optimal value function optimal substructure 
iterative dp algorithm called policy iteration howard guaranteed converge optimal value function 
arbitrary initial policy iterates steps chapter 
background 
compute value function current policy 
improve policy greedily choosing actions optimal new policy argmax psj rs 
second step called policy improvement guaranteed improve policy optimal steps iterated converge optimal policy optimal value function 
note policy evaluation step completed policy improved 
particular iterative policy evaluation algorithm iterated convergence 
turns unnecessary allow policy evaluation step complete 
value iteration algorithm similar policy iteration single full backup performed step 
shown algorithm converges optimal policy 
algorithms result optimal policy algorithm better 
policy iteration requires complete policy evaluation steps policy improvement usually done backups value function typically changes little policy slightly improved 
time expect policy iteration require fewer steps total value iteration making policy improvements accurate information 
fact result shown theoretically 
algorithm performs better problem empirical question 
finding value function action value function psj rs max 
chapter 
background similarly optimal action value function denoted context dynamic programming action value functions sense 
storing parameters store ones 
action value functions important temporal difference learning assume complete prior knowledge dynamics system 
temporal difference learning temporal difference td learning sutton dynamic programming addresses reinforcement learning problem act maximize reward signal pro vided environment 
view td performing approximate dynamic pro gramming solve mdp 
nature approximation speed td algorithms converge subjects study 
major difficulties applying dynamic programming real world problems 
dynamic programming centered full backups 
value state updated iteration 
clearly technique scale problems states 
second dp requires perfect model environment available 
obviously going case problems 
full backups td techniques sample backups update value estimate states visited 
rationale states reachable small subset visited practice 
example number possible chess board configurations huge number seen actual expert games smaller 
point expending resources update value states visited 
assumes state reasonable likelihood visited value function learned visited estimation value function 
distinction dp td td algorithms learn interacting environment dp requires perfect model environment begins 
chapter 
background possibility learn model environment observing results performing actions performing full sample backups 
algorithms approach sutton barto bradtke singh 
backing states visited gives rise number td update rules 
known dynamics model compute expected returns samples dynamics interacting environment 
example td update learning rate uses samples environment construct monte carlo estimate expectations eq 
update delta rule designed move estimated value function closer bootstrapped monte carlo estimate 
reduced time appropriate manner states visited infinite number times algorithm certainly converge value function current policy dayan 
method estimates value function current policy incorporated policy iteration algorithm 
update rule called policy update finds value function policy currently executed 
equivalent policy update rule action value function known sarsa rummery niranjan sutton 
learning rate 
name derives fact update depends set values 
case dp difference updates small difference significant 
state value function learned order extract policy model environmental dynamics learned 
sarsa chapter 
background model implicit action value function policy simply maximizing possible actions state max argmax see difference important partially observable mdps model system dynamics learned case maintain belief state policy td update rule action value functions called learning watkins watkins dayan max st 
case optimal value function estimated directly 
policy sample states rewards irrelevant provided states actions continue visited 
version update rule referred 
general updates called td information farther back time just previous time step sutton 
extreme case td averages rewards entire trials task updating value estimates 
technique called pure monte carlo technique uses sampling entire trials update values expected returns see sutton barto discussion monte carlo decision process algorithms 
chapter 
background partially observable markov decision processes interesting decision problems markov observations 
way solve problems mdp framework construct extended states explicitly recording history process appropriate number steps words consider th order markov process 
difficulty determining order steps included history transform problem mdp 
way determine testing statistics reward predictions current states expanding histories reward distributions markov extended states 
approach taken tree algorithm mccallum 
tree algorithm quite flexible allowing process different order different parts state space advantage factored input representation provided 
learn factored representation input 
approach assume process markov state variables observed 
observed variables rewards time assumed conditionally dependent hidden state time hidden state independent hidden visible variables see 
unobserved markov process called core process 
second approach formalized partially observable markov decision process pomdp 
pomdp consists set core states set actions set observables initial distribution core states generally assume observations rewards conditioned current state current action state 
equivalent conditioning just current state action expanded core state space 
chapter 
background action state reward observation partially observable markov decision process 
circles indicate hidden variables squares indicate visible variables diamonds indicate actions 
hidden state dependent previous state action reward depends current state action 
observation just dependent current state 
transition distribution reward distribution output distribution denotes simplex dimensionality 
pomdp assume problem markov respect unobserved random variable 
hidden variable summarizes relevant infor mation history observations 
state core variable time denoted dependent state previous time step action performed 
currently observed evidence denoted assumed independent states observations current state 
state hidden variable known certainty 
distribution states called belief state maintained 
turns process markov belief state 
belief state transforms discrete hidden mdp fully observable mdp states lie simplex 
chapter 
background time step beliefs updated bayes theorem combine belief state previous time step newly observed evidence 
evidence action written oj ji diagonal matrix ii normalizing constant 
case discrete time finite discrete states actions observables called finite pomdp pomdp corresponds hidden markov model hmm distinct transition matrix action 
exact belief updates sequence actions observations computed tractably model forward pass forward backward algorithm rabiner juang 
way find optimal policy learn value function 
process markov belief state domain value function 
sondik showed finite horizon pomdps value function piecewise linear convex belief state 
worst case number linear pieces grows exponentially distance horizon making exact computation optimal value function intractable cases 
infinite horizon limit function may longer piecewise linear 
subclass infinite horizon pomdps called finitely transient value function piecewise linear sondik 
fact pomdp finitely transient optimal value function approximated arbitrary degree accuracy piecewise linear convex function 
computation exact approximate piecewise linear value functions played central role study pomdps 
pomdp formalism suffers similar difficulty explicit th order markov representation know hidden states chapter 
background core process order markov 
problem addressed context hmms stolcke omohundro pomdps represented hmms chrisman mccallum 
notice correct number hidden states localist repre sentation pomdp exponentially inefficient encoding bits information history process requires states 
bode abilities localist models scale problems high dimensional inputs complex non markov structure 
representing pomdps hmms represented compactly dynamic bayesian networks see example dean kanazawa russell boutilier poole 
discuss section 
exact algorithms number algorithms proposed compute optimal value function pomdp lovejoy 
exact algorithms computing optimal value function finite horizon pomdp dp step compute current value func tion previous value function eq 
value iteration algorithms monahan exhaustive enumeration algorithm monahan incremental pruning zhang liu cassandra littman zhang sondik pass pass algorithms sondik smallwood sondik witness algorithm littman cassandra kaelbling 
algorithms finding optimal value function pomdp horizon single action performed th step optimal value function horizon problem compute optimal value function length pomdp 
sondik showed optimal value function length finite horizon pomdp piecewise linear convex 
represented unique minimal set chapter 
background vectors max ak 
stage builds computing unique minimal representation stage important reducing computation 
algorithms try compute unique minimal representation 
algorithms broken general categories cate gory computes stage value function computing large set vectors pruning away excess leaving minimal set 
exhaustive incremental ing algorithms operate way 
second category sondik algorithms witness algorithm build minimal set vectors starting single vector considering nearby vectors 
exhaustive enumeration monahan introduced exhaustive enumeration algorithm finite horizon pomdps name sondik pass algorithm monahan 
represent value function finite horizon pomdp length finite set vectors algorithm computes optimal value function step process works backwards 
th step optimal value function computed step pomdp 
set vectors just immediate rewards action 
action specific vector giving expected immediate reward core state 
sondik algorithms guaranteed worst case 
chapter 
background th set candidate vectors defined previously see eq 
notice new value function composed parts immediate reward weighted sum values stage value function 
weighted sum computed possible assignments vectors previous set possible observations 
new set candidates computed pruned dominated vectors solving linear programming problem vector max ak solution belief state dominates vectors included cassandra littman zhang denote pruning step prune 
simple principle clearly expensive procedure 
number vectors new set prior pruning scales exponentially number possible values observation produce require linear program vector 
practice procedure expensive smallest problems cassandra littman zhang 
incremental pruning incremental pruning algorithm zhang liu cassandra littman zhang conceptually simple exhaustive enumeration 
cassandra littman zhang denote cross sum operation operator defined ai ai 
sum possible combinations elements sets ai 
chapter 
background 
notation previous algorithm written simply prune prune incremental pruning algorithm hinges fact prune prune prune incremental pruning algorithm stated simply prune prune prune prune aa aa 
cassandra littman zhang showed number linear programs required algorithm scales linearly number observations worst case contrast exponential scaling previous algorithm 
practice incremental pruning currently fastest exact pomdp solution algorithms cassandra 
sondik algorithms sondik publish exact algorithms finite infinite horizon pomdps sondik smallwood sondik sondik 
algorithms pre vious ones involve setting solving series linear programming problems chapter 
background candidate vectors set difference candidate vectors chosen 
pass algorithm consists steps incremental pruning eq eq pass sets computed second pass sets merged final set difference sets 
definition witness region vector states dominates vectors defined set belief definition recall exhaustive enumeration algorithm section set candidate vectors generated considering choices vector neighbours denoted vectors differ choices 
choice vectors replacement denotes aa pass algorithm facts sondik refers pass algorithm applied single action problem follow terminology cassandra calling pass algorithm 
chapter 
background fact difficult compute set vectors required represent optimal value function easy compute single vector set active particular belief state action observation 
belief state action observation active vector oj argmax active vector irrespective observation 
fact neighbour theorem cassandra exists belief state exists words candidate vectors dominates belief state neighbour dominates belief state 
basic steps pass algorithm 
vector 
find witness regions border witness region see step vectors active bordering regions 

add vectors 
iterate regions explored vectors chapter 
background 

sets compute prune step fact tells step algorithm feasible arbitrary belief state 
step step performed finding neighbours ak determining witness regions neighbours border witness region 
done solving lp problem max 
linear program solution borders 
know searching neighbours sufficient find bordering regions neighbour theorem 
step step simply performed adding bordering witness regions 
neighbours step set neighbours tested repeat steps 
unfortunately candidate neighbour may active boundary region 
true neighbours pass algorithm return set vectors represents give minimal set 
fact may yield set exponentially larger number observations minimal set 
worst case running time exponential best case running time polynomial 
witness algorithm littman cassandra kaelbling designed overcome problem 
chapter 
background practice pass algorithm perform better witness algo rithm incremental pruning large range problems cassandra 
problem returning large numbers unnecessary vectors theoretical concern 
pass algorithm sondik smallwood sondik follows procedure pass algorithm building set vectors exploring witness regions 
tries construct set directly constructing sets avoids second merging step 
algorithm take account active vectors actions constraints pass algorithm lp problems complicated pass case 
regions explored turn subsets actual witness regions active vectors 
number issues involved implementation algorithm sake brevity discuss 
pass algorithm complicated implement worse performance worst case algorithms deserves credit exact algorithm solving pomdps witness algorithm witness algorithm builds sets exploring regions belief space combining find proceeds looking neighbours vectors known trying determine new vector witness region borders region explored witness algorithm just finds single belief state new vector dominates vectors known belief state called witness new vector 
neighbour theorem strategy eventually find belief state neighbour complete 
witness exists solving lp eq 
steps witness algorithm chapter 
background 
compute dominating vector arbitrary belief state 
add initialize set neighbours investigated 

find neighbours add 
remove neighbour try find witness 

witness exists compute dominating vector add add back 
repeat steps empty 
witness algorithm avoids problems pass algorithm wit looking bordering regions conceptually quite simple 
worst case running time polynomial 
terms empirical formance worse pass algorithm incremental pruning cassandra littman zhang 
approximation algorithms approximation algorithms long history applied rl tasks mdps 
examples include feed forward multilayer perceptrons bertsekas tsitsiklis radial basis functions sato ishii 
continuous state action domains aggregate groups discrete states case number states large table lookup 
attempts function approximators learn value functions pomdps pomdp equivalent mdp states lie simplex previous real valued mdps rele vant 
focus basic techniques early termination exact algorithm parameterized function approximation 
techniques grid interpolation lovejoy operate maintaining value function estimates chapter 
background grid points estimating values points interpolation 
discuss methods detail 
early termination obvious approximation technique infinite horizon pomdp simply ter exact algorithm finishes 
sondik provided bounds error expected procedure terms discount factor number stages sondik maxi ri mini ri 
performance approximations similar compared num ber approximations 
general early termination compare favorably particular problems poorly cassandra 
function approximation approach function approximator learn approximate value func tion 
approximation differentiable parameters learned applying delta rule bellman error eb td error max chapter 
background terms parameter value function update rule learning rate 
eb approach taken bertsekas tsitsiklis bertsekas tsitsiklis littman littman cassandra kaelbling parr russell parr russell 
littman linear approximation action value function qa parr russell reason actual value function represented maximizing set dot products max reasonable differentiable approximation softmax function set dot products km km number vectors chosen arbitrarily 
typically parameter km initially set function quite smooth 
reduced learning approximated value function approaches piecewise linear function 
bertsekas tsitsiklis standard feedforward multilayer perceptron trained eq derivative network computed backpropagation 
apply technique real valued mdps 
approach applied belief state mdps rodriguez parr koller 
results approaches mixed 
parr russell report initial results small tasks parr russell 
technique applied chapter 
background larger problems rodriguez parr koller 
results comparable neural network function approximator backpropagation 
littman report mixed results linear approximator 
tasks simple linear approximation surprisingly tasks quite poorly littman cassandra kaelbling 
note techniques guaranteed converge 
best linear approximation shown converge bounded region sarsa update rule gordon 
chapter compare results methods belief state mdps show results versions function approximators suitable factored state representa tions 
generative models control simply put generative model statistical model trained partic ular data set generate new data points 
goal duplicate probability distribution original data drawn newly generated points statistically similar model trained 
hidden markov model example generative model 
section discuss genera tive models applied control problems influence diagrams dynamic bayesian networks hmm exponentially inefficient state representation 
influence diagrams influence diagrams bayesian networks types nodes chance nodes action nodes single value node see howard matheson 
nodes represent random variables actions value utility function 
edges represent dependencies variables 
chance nodes unobserved chapter 
background hidden nodes observed evidence nodes 
agent sets values action nodes influence distributions value nodes 
action value chance evidence influence diagram 
squares indicate visible variables diamonds indicate actions 
unobserved chance nodes indicated circles 
value node ellipse 
localist representation pomdp influence diagrams allow com pact factored representation dynamics process terms set random variables 
task influence diagram mdp pomdp find policy optimizes expected value value node 
expectation taken respect posterior distribution hidden chance nodes respect policy action nodes 
posterior computed exactly gaussian influence diagrams shachter intractable number chance action nodes increases 
typically single value node descendants 
influence diagrams directed acyclic backward value node assign time action chance node value node occurring sequence 
convention information available action node time available subsequent action nodes called forgetting rule 
chapter 
background forgetting rule allows influence diagrams represent arbitrarily non markov processes 
unfortunately results policies domains increase exponentially problem horizon 
dynamics process represented compactly policy 
influence diagrams quickly infeasible long running processes 
number algorithms solving influence diagrams howard math shachter cooper shachter peot 
interest brevity detail 
general categories algorithms ones operate directly influence diagram shachter ones transform intermediate representation howard matheson cooper shachter peot 
results dp type approach optimal policy working backwards value node eliminating action nodes integrating chance nodes 
involves performing inference series bayesian networks cooper shachter peot operating decision tree howard matheson 
dynamic bayesian networks dynamic bayesian network influence diagram obeys markov property 
time slice dynamic bayesian network dbn represents system time steps dean kanazawa 
conditional dependencies random variables time time time step represented edges directed acyclic graph 
conditional probabilities stored explicitly parameterized weights edges graph see 
dbn allows compactly represent pomdp set variables con ditional dependencies variables 
network densely connected ference network intractable cooper 
approximate inference methods clude markov chain monte carlo neal variational methods jordan ghahramani chapter 
background action state time reward observation dynamic bayesian network 
squares indicate visible variables diamonds indicate actions 
hidden state variables time dependent action time hidden variable values time observation depends current state variables 
reward depends current state action 
jaakkola saul projective filtering boyen koller 
think pomdp dbn just nodes representing random variable consecutive time steps take number values 
thinking way clear general problems inherent representing pomdp value functions carry dbns 
structure dbn allows compact representations transition emission probabilities structure representation value function 
number researchers proposed methods computing exact value functions boutilier poole approximate value functions mcallester singh sallans factored pomdps take advantage inherent structure 
applying dbn large problem distinct issues disentangle parameterized dbn capture underlying pomdp dbn hurt approximate inference approximation value function achieve reasonable performance 
characterization value chapter 
background function piecewise linear convex belief state assumes model known exactly belief state updated exactly 
words belief state really characterize correct probability particular state underlying core markov process 
interesting questions approximation arise assumptions met 
inaccuracies introduced 
model known exactly belief updates exact 

belief updates expensive compute exactly 

value function complex store exactly linear pieces approximation 
chapter discuss system true 
true general large complex problem 
proposed methods tackle issues 
mcallester singh approximate inference method conjunction complementary value function approximation 
approxi mations projection operator hidden state distribution value function previous time step updated exactly projected back space tractable distributions value functions 
assume model known exactly find bounds error value function approximation caused projection operations 
rodriguez parr koller similar projection operator approximate inference parameterized value function approxima tions 
reinforcement learning need supplied transition emission probabilities 
feedforward mlps smooth value function approximation parr russell 
poupart boutilier assume inference approximate resultant error value function select best approximate inference technique 
assumed transition emission probabilities known advance 
chapter 
background sections discuss methods exact approximate inference methods estimating parameter values parameterized bayesian networks dbns 
factored markov decision processes subsequent chapters focus factored markov decision processes factored mdps factored partially observable mdps state action represented set variables 
formally factored po mdp consists set pr set field possible values state variable set possible values action variable initial value state variable transition distribution pr reward distribution 
state tuple action tuple 
addi pomdp includes set output symbols output distributions 
output tuple 
factored mdp pomdp represented compactly set dynamic bayesian networks transition emission probabilities parameterized tak ing advantage conditional independencies state action variables 
pomdps expressed way solved exactly dynamic programming advantage compact representation boutilier dearden gold 
computing value function exactly value functions learned reinforcement learning techniques advantage struc tured representation boutilier dearden koller parr st aubin hoey boutilier 
learning inference far focused task learning value functions fully partially observable markov decision processes 
dynamic programming algorithms chapter 
background assumed learner access knowledge dynamics process 
section discuss learning models transition generation dynamics data making inferences state core process 
expectation maximization algorithm expectation maximization algorithm iterative method learning max imum likelihood parameters generative model random variables observed hidden dempster laird rubin 
hidden random variables represent quantities think underlying causes observables 
example model designed explain data consisting shoe size reading ability age hidden variable 
hidden variables continuous example discrete case class labels 
settings visible variables settings hidden variables parameters model 
name implies steps algorithm expectation step calculate distribution hidden variables visible variables current value parameters 
maximization step compute values parameters maximize expected log likelihood distribution step set argmax log argmax log step involves inferring distribution hidden variables step involves learning new parameters 
shown cases steps chapter 
background repeated true log likelihood increase stay local maximum reached 
notice step require solving difficult non linear optimization prob lem 
natural implement partial step just find set parameters improve expected log likelihood fully maximizing 
example gradient ascent partial steps quite common 
algorithms partial step called generalized algorithms gem guaranteed improve true likelihood dempster laird rubin 
step difficult depending form posterior 
times resort approximate inference finding true find approximation true posterior 
approximation compute expectation required step 
inference central problem learning ml parameters algorithm 
performing exact approximate inference important problem solved learn parameters generative model model ml parameters 
exact inference cases need approximate inference efficiently com pute correct posterior distribution 
examples include case linear gaussian models singly connected graphical models variant probability propa gation compute posterior correctly pearl 
factor analysis example hidden markov model baum petrie ex ample 
kalman filters singly connected gaussian kalman 
unfortunately inference provably hard directed acyclic graphical models 
problem inference general belief networks np hard cooper 
apparently simpler problems shown np hard 
finding single set hidden states maximum posteriori map states chapter 
background problem approximate inference accuracy known np hard shimony dagum luby 
new class undirected graphical models proposed mem bers powerful listed allow exact inference 
products experts poe models hinton 
tradeoff poe models explicitly define density distribution data space 
define unnormalized energy function 
difficult maximize log probability data model typically done models linear dynamical systems hidden markov models 
hinton suggests new learning algorithm called minimizing contrastive divergence gradient method minimizing energy data maximizing energy step reconstruc tions data 
see hinton details products experts unsupervised setting 
discuss products experts detail chapter 
advantages exact inference obvious speed precision 
proper ties account wide adoption models exact inference possible 
un fortunately exact inference typically restricted range architectures expressive capture complicated phenomena 
products experts models allow exact inference expressive capture interesting non linear structure data harder train singly connected gaussian models 
variational methods consider parameterized distribution hidden variables 
metric mea sures difference distribution true posterior optimize parameters approximate approximation called variational ap proximation parameters variational parameters 
suitable distance chapter 
background metric called kullback leibler divergence kl log dy vector hidden unit activities observation 
evaluate eq directly need evaluate trying approximate place 
get difficulty evaluating variational free energy function joint probability hidden visible units 
variational free energy comparable variational free energy statistical physics 
joint probability readily available directed belief networks 
free energy upper bound negative log probability data 
negative log probability data plus kl divergence log kl optimizing parameters minimizing eq minimize eq effect log independent 
turns train model minimizing eq respect model parameters 
variational methods advantages 
allow calculate upper lower bound log probability data current generative model jaakkola fast 
deterministic advan tage situations gradients shallow sampling noise hinder learning 
variational methods yield approximations low variance biased 
side variational methods easy come reader note free energy defined opposite sign log kl 
sign consistent free energy statistical physics 
chapter 
background approximating distribution 
typically variational approximation architecture specific 
care taken approximating distribution simplistic capture important features posterior complicated subsequent calculations 
finding approximating distributions art science 
commonly variational approximations mean field approximation maximum posteriori map approximation 
mean field approximation assume hidden units independent 
having distribution representation requires space exponential number hidden units needs linear space 
mean field approximations provide lower bound log likelihood data 
mean field parameters just updated self consistent 
simplifies mean field equations provides approximation likelihood lower bound 
self consistency equations solved iteratively 
map approximation assume single spike infinite density point ym 
entire mass distribution replaced single spike 
case kl divergence infinite optimize parameters respect expected energy ep energy log log dy log ym dy log ym quantity proportional posterior optimization move spike point maximum density posterior name maximum posteriori 
reasonable approximation true posterior unimodal chapter 
background sharply peaked 
approximations common simple large class problems 
posterior multimodal broad may insufficient 
appear mean field map approximations simple give results learning model parameters 
mitigating factor allows simple approximations reasonable job coupled model learning update model parameters performing gradient descent model parameters adjusted reduce kl increase log 
model parameters change true posterior brought closer approximation 
monte carlo methods monte carlo approximations estimate expectations eval posterior explicitly sample 
samples xn distribution approximate expectation function follows dx xi number different schemes generating set samples de pend form distribution see neal survey monte carlo sampling techniques 
general monte carlo methods dependent archi tecture variational methods 
single monte carlo method applied broad range architectures 
disadvantage monte carlo methods speed take long time approximate expectation high degree accuracy 
approximations involve random sample get noisy chapter 
background estimate quantities interest 
subset monte carlo methods called markov chain monte carlo mcmc 
markov chain monte carlo sampling techniques appropriate sample directly distribution interest 
sample ergodic markov chain unique equilibrium distribution distribution want sample 
markov chain reached equilibrium draw samples original distribution interest 
drawback mcmc techniques general wait chain reach equilibrium 
take long time difficult tell happens 
done learning inference brief sampling hinton ghahramani 
involves samples allow markov chain reach equilibrium 
variational method pressure model parameters take values sample posterior distribution taken iterations sampling 
variational method brief sampling yields biased estimate posterior distribution 
variational method brief sampling easily applied large range graphical models 
brief sampling mcmc methods general give access bound log likelihood 
gibbs sampling gibbs sampling example markov chain monte carlo sampling technique 
con sider distribution random variables xk 
gibbs sampling sufficient sample variables conditioned xj xi 
gibbs sampling proceeds follows 
initialize xk sampling initial distribution 

sample xj xi 
chapter 
background sampling subsequent variables newly sampled values ing variables 

iterate convergence stationary distribution ps distribution interest 
course method elaborate 
need sample single variables sample subsets blocks variables 
required sample distribution block xj xi conditioned variables 
practice biggest problem knowing markov chain converged stationary distribution ps 
projective filtering approximate inference method markov processes received attention machine learning community boyen koller 
requires projection op erator transforms arbitrary probability distribution density real valued domains distribution subset simpler distributions pa approxima tion involves steps 
time distribution pt pa 
bayes rule update distribution pt called belief update 
includes applying dynamics markov process incorporating newly observed evidence 
notice pt may fall outside pa 
projection operator get pt pa subset pa property calculations want perform tractable member subset 
example set distributions question distributions binary variables just storing distribution require space 
typical subset distributions chapter 
background binary variables represented completely factored form requiring space see 
projection operator case marginalization 
original density projection operator simplified density belief update updated density projection operator simplified density schematic projection filtering 
original distribution density projected space simpler distributions 
simpler distribution updated bayes rule result re projected space simpler distributions 
intuitively method relies fact updated distribution fall outside set simple distributions far away eas ily projected back space simple distributions 
easily means projection operation handled worst case space time re 
example belief update operation introduce correlations fully factored approximating distribution 
shown theoretically error approximating distribution remains bounded tive number projection operations performed boyen koller planning pomdp simplified belief states results bounded deviation optimal rewards mcallester singh 
inference method tested empirically pomdps rodriguez parr koller poupart boutilier 
chapter factored states chapter method learning compact representations core process partially observable mdp approximating belief state action value function 
preliminary results sallans 
compact representations approximations decision problems want solve grow larger need efficient representations model world 
early research pomdps described state world single discrete number 
sufficient solving gridworld style problems tens states observations 
interested problems involving thousands billions states observations 
solution methods larger problems properties 
representation core process compact 

efficient way map observation space internal states belief states internal states finite automata 

learning mapping experience 
chapter 
factored states solution method take advantage fact tiny subset possible observations seen system 
typically lie low dimensional manifold embedded high dimensional observation space example think visual scenes occur versus possible pixel settings 
unsupervised learning bayesian network meets criteria 
bayesian network compactly represent transition emission probabilities factored pomdp core state represented set random variables 
inference network allow update belief state new evidence 
gradient methods update parameters network experience 
learned model give high probability subset possible observations seen ignore rest 
time slice dynamic bayesian network dbn represents system consecutive time steps dean kanazawa 
conditional dependencies random variables time time time step represented edges directed acyclic graph 
conditional probabilities stored explicitly parameterized weights edges graph 
number challenges overcome bayesian network model dynamics 
transition emission reward probabilities known priori learned observing interactions agent world 
similarly learning belief state value function requires observing statistics rewards 
interact updating value function depends believe dynamics process 
network densely connected inference intractable cooper 
approximate inference methods affect accuracy belief updating turn affect estimate belief state value function process parameters 
top function approximation belief state value function 
chapter 
factored states method chapter designed show challenges insurmountable 
method consists learning compact bayesian network rep resentation system dynamics learning belief state value function representation approximate inference update belief state new evi dence observed 
steps carried simultaneously learning agent interacts world 
approximations coupled making theoretical analysis difficult 
re cent research investigated subsets issues theoretically empirically mcallester singh poupart boutilier thrun rodriguez parr koller sallans 
approaches necessitates making choices approximate inference model learning value function approximation 
discuss detail set choices involving mean field approximate inference gradient parameter learning 
ideally factored representation learned quickly model rep resent transition emission process parameters 
factored representation allow core mdp parameterized simply large localist representation 
factorization assumption carried value function approximation eliminating parameters simplifying interpretation hidden variables 
section chapter discuss issues approximate inference learning model parameters approximating value function 
describe general problem detail specific example dynamic sigmoid belief network 
final sections discuss simulation results 
final simulation shows dbn method solving time sensitive driving problem hidden state input space size small subset possible inputs observed 
model solve problem core state space size represented binary variables 
chapter 
factored states dynamic sigmoid belief network chapter illustrate dbn approach running example fully connected dynamic extension sigmoid belief network neal units time slice 
refer network dynamic sigmoid belief network see 
random variables si binary conditional probabilities time architecture dynamic sigmoid belief network 
circles indicate random variables filled circle observed empty circle unobserved 
squares action nodes diamonds rewards 
relating variables adjacent time steps encoded action specific weights st ik st ik weight ith unit time step th unit time step assuming action taken time nonlinearity usual logistic function exp 
note bias incorporated weights clamping binary units 
observed variables assumed discrete conditional distribution output hidden state multinomial parameterized output weights 
probability observing output value exp exp chapter 
factored states denotes output weight hidden unit output value parameterization expected log likelihood new time slice log log log exp log exp ust st st st matrix action specific weights matrix hidden observation weights vector zeros tth place denotes th element vector 
expectation taken hidden variables current previous time steps 
architecture accommodate factored observations ing probability variable observation independent hidden variables log log log log exp exp st st st indexes visible multinomial variables 
seperate emission matrix visible variable 
chosen architecture discrete output hidden variables appropriate model tasks consider 
binary hidden variables relatively easy interpret 
extensive literature approximate inference schemes sigmoid belief networks neal saul jaakkola jordan jaakkola 
discrete outputs easily replaced continuous outputs chapter 
factored states allowing binary hidden unit gate real valued gaussian output unit sallans 
approximate inference inference fully connected bayesian network intractable 
chapter dis cussed number approximate inference techniques 
perform belief update factored pomdps thrun rodriguez parr koller sal lans poupart boutilier 
mean field method fully factored approximating distribution mean field parameters 
st st form approximating distribution write approxi mate likelihood new time slice hidden variable values replaced approximate means log log exp log exp uo mean field energy negative approximate log likelihood approximating distribution set value mean field parameter compute probability variable si rest mean field parameters held fixed 
just related difference energies si takes value zero chapter 
factored states exp si exp esi exp esi exp esi esi si si si si denote energy si takes value zero respec tively 
equate mean field parameter probability si si results coupled non linear equations mean field parameter 
solve self consistent set mean field parameters iterating equations 
esi si parameters converge self consistent values iterations 
demonstrate ran approximate inference algorithm randomly generated 
transition emission parameters chosen 
binary hidden state variables observable variable take values 
random network mean field parameters initialized randomly uniform distribution range 
eq iterated times 
squared difference parameter values iteration recorded 
median iteration iteration difference trials shown 
parameters converge iterations 
complex equations introduce additional parameters give lower bounds true log likelihood jaakkola 
assume distribution fully factored 
structured variational approximations take advantage specific connectivity graph result efficient chapter 
factored states log squared change log squared change vs iteration iteration iteration iteration squared difference mean field parameters 
graph shows median differences randomly generated 
vertical lines contain data 
accurate inference algorithms ghahramani jordan 
values mean field parameters time held fixed computing values step calculations done line learner inter acting world go back revise belief state estimates evidence 
analogous running forward portion forward backward algorithm hidden markov models 
note values belief states taken account estimating values current belief states see section 
chapter 
factored states learning process parameters general maximum likelihood parameters dynamic bayesian network learned algorithm 
common examples includes system identification linear dy systems shumway stoffer baum welsh algorithm hidden markov models baum petrie soules weiss 
methods involve seeing entire data sequence computing posterior distribution hidden variables times sequence updating parameters statistics posterior distribution 
want identify parameters infinite horizon pomdp completed data sequence available 
windowed approach apply full updates entire window 
suggested line algorithms keep running average updates parameters batch case sato ishii 
online gradient method 
parameters model optimized online stochastic gradient ascent approximate expected log likelihood model parameters 
transition parameter update differentiating single time slice approximate likelihood log log exp log exp chapter 
factored states emission parameter update similarly log log log exp log log exp exp exp exp exp transition parameter update emission parameter update learning rates 
exp exp exp gradient approach advantage process stationary slowly varying parameters model adapt track changing dy 
quickly changing dynamics switching model appropriate ghahramani hinton 
parameter update rules simple easy chapter 
factored states compute 
course disadvantage small steps taken update model parameters learning rate chosen priori 
automatic relevance determination difficulty applying latent variable models decide dimensionality latent space 
useful model adapt structure process modeled 
ways model structure altered additional hidden units added accommodate complexity hidden units pruned required 
methods learn dynamical models time series data stolcke omohundro pomdps chrisman mccallum 
approach form automatic relevance determination ard mackay neal 
informally hope discover hidden variables relevant explaining dynamics emissions process 
variables relevant pruned away 
technique implemented form bayesian structure learning prior gaussian distribution placed weights favouring small magnitudes 
key point ard input unit prior variance parameter 
small variance suggests weights leaving unit small unit little influence subsequent values 
large variance indicates unit important explaining data 
implement full ard approximating posterior model parameters 
update scheme inspired ard neal 
derivative approximate likelihood wa ij modified update ij ij ri ij ij chapter 
factored states learning rate parameterizes strength decay zero ri ri maxk ij intuitively weight leading hidden unit important modeling process early learning gradient respect weight large 
weights leading unit higher learning rate 
conversely weights leading unit important weights get high learning rate 
case term eq move weights zero 
role hidden unit learned gradient decrease 
point equilibrium reached size gradient strength weight decay 
typically goal regularization prevent overfitting 
essentially unlimited data training subject training time limitations really issue 
important context avoid bad local minima likelihood 
see experimental results section quality learned model vary widely 
allowing model start large state space pruning ard avoid local minima get robust consistent results 
practice training ard resulted better performance large problems 
approximating value function computing optimal value function intractable 
factored state space rep resentation appropriate natural bit extreme assume action value function decomposed way ps qk qf neal method conjunction early stopping keep magnitude irrelevant weights small 
early stopping appropriate infinite horizon pomdps global weight decay 
chapter 
factored states main reason making assumption simple approxi mation easily implemented 
may perform practice 
fact simulation results follow see section find best performing value function approximator factorization assumption 
simplifying assumption finding optimal value function tractable 
states completely independent qk piecewise linear convex number pieces scaling exponentially horizon 
test approximate value functions 
function approximator parameter update equations 
polynomial approximators simplest approximator linear qf similar quadratic approximation qf qk mk tk qk bat parameters approximations 
symbol denotes element wise vector multiplication 
cases update term approximation modified learning rule watkins dayan corresponds delta rule target input maxa qf 
linear approximator update chapter 
factored states differentiating value function qk max qf qf qf qk max qf qf ke learning td error rt max qf qf quadratic update rule similarly mk mk qk qk bat bat eq learning rate temporal discount factor 
smooth partially observable value approximation tried factored version smooth partially observable value approximation parr russell qf km km multiple linear segments indexed approximate value function 
linear segments combined differentiable softmax operation 
line chapter 
factored states segments optimized learning 
parameter km adjusted course learning 
parameter km increased approximation closer max operation 
notice sum inside root positive 
parr russell suggest adding additional parameter inside softmax force values positive qf km km constrain positive optimization add action specific offset ba qf km update rule derived parr russell qj eq tk qj km qs km update rule global offset linear case 
multilayer perceptron km bat feedforward multilayer perceptron factorization assump tion 
mlp approximator learned backpropagation rumelhart hinton williams 
input network estimated belief state output vector values possible actions 
feedforward network hid den layer 
hidden units sigmoid activation functions output units words function decomposed terms hidden random variable 
function approximator takes fully factored approximate belief state input 
chapter 
factored states linear qp denote input hidden hidden output weights respectively 
activity jth hidden unit denoted hidden unit activity written way emphasize function belief state 
update rule simply delta rule td error error function uj hj eq hj eq see bertsekas tsitsiklis examples mlps reinforcement learning real valued states 
eligibility traces far discussed temporal difference algorithms current reward signal estimate current td error 
algorithms reward signal time steps estimate error current time 
example full monte carlo algorithms estimate value current state summing rewards state task 
consider intermediate algorithm 
step backup sutton barto estimates value summing reward signal steps followed value function 
chapter 
factored states st averaging step backups different values get different td error signals 
family td errors parameterized average errors particular way eb learner gets td error signal combining immediate reward estimate value 
td error discussed previously intuitively controls bias variance trade learning value function 
moves estimate value function moves biased value estimate states unbiased monte carlo estimates total return 
eligibility traces way implementing td reinforcement learning methods 
name comes fact eligibility traces keep track parameters eligible updated 
eligibility traces sarsa straight forward 
policy times policy action value function learned keep decaying count number times state visited 
types traces suggested accumulating traces replacing traces 
consider accumulating traces 
notation sutton barto chapter 
factored states accumulating trace 
state action pair revisited trace decayed zero trace greater result 
update table lookup case td sarsa error issues applying eligibility traces learning 
eligibility traces store frequency state action pairs visited policy executed 
learning policy 
directly learns values greedy policy regardless policy followed 
unfortunately learning algorithms follow greedy policy respect current action value function learning 
execute exploratory actions 
eligibility trace valid non greedy action executed 
implementation eligibility traces proposed watkins traces maintained non greedy action executed 
point traces reset zero 
trace current state action pair incremented 
st 
chapter 
factored states problem early learning actions exploratory traces reset frequently 
precisely early learning action value function inaccurate traces useful 
implementation suggested peng williams 
complex advantage traces maintained regardless action executed 
appears practice 
unfortunately implementation approximate 
convergence guarantee watkins implementation enjoys 
naive method suggested sutton barto traces maintained watkins method simply reset exploratory action taken 
treat pomdp belief state mdp correct way implement eligibility trace store trace value belief state separately 
value function updated belief states simultaneously trace 
belief states lie simplex belief states take approach 
maintain trace parameter exact approximate value function representation 
general function approximator represent approximate belief state action value function parameterized eligibility trace maintained parameter 
update accumulating sarsa trace updates similar 
example function approximator linear belief state derivative respect parameter ai chapter 
factored states watkins learning trace update ai ai indexes possible values hidden state variable 
factored pomdp linear approximator mean field inference derivative watkins learning trace update ai ai ai indexes hidden state variables denotes marginal approximate prob ability variable 
experiment sutton barto naive accumulating learning trace watkins accumulating learning trace 
simulation results new york driving new york driving task involves navigating slower faster way traffic multi lane highway mccallum 
speed agent fixed 
relative agent slower cars approach front faster cars approach 
chapter 
factored states agent change lanes obstacle cars 
goal progress passing slower cars staying way faster cars see 
gaze right perception gaze colour tan gaze refined dist far half gaze distance far gaze speed looming gaze direction forward gaze side right gaze object road hear horn new york driving simulator 
road divided lanes 
agent see meters front 
distances discretized meter road sections front agent agent occupied agent 
slow cars move meters second fast cars move meters second agent moves meters second 
time discretized second intervals 
time steps slow cars move back section fast cars move forward section relative agent car 
fast car enters road section slower vehicle slows match speed blocking car begins horn 
fast cars slow obstacle cars effect slow obstacle cars 
fast cars agent car slow meters second horn agent changes lanes 
point speed meters second continue chapter 
factored states way 
agent access information gathering actions look left look center look right look back 
proactive action change lanes current gaze lane 
looking backward looks backward current gaze size left center right 
current gaze side part visible state 
going looking forward left backward right step operation agent look right look back back right 
changing lanes requires actions agent looks lane executes change gaze lane 
changing lanes looking forward center effect 
lane changes forward backward gazing 
lane change ends agent looking forward current lane 
lane changes agent normal forward progress meters second 
looking lane agent gaze follows lane agent forward backward hits obstacle car horizon 
multiple obstacle cars lane agent see closest current gaze lane 
time step agent access visible world state 
visible state consists discrete variables 
variables possible values summarized table mccallum 
table sensory input new york driving task 
dimension size values hear horn gaze object car shoulder road gaze side right center left gaze direction front back gaze speed looming receding gaze distance far near nose gaze refined distance far half near half gaze colour red blue yellow white gray tan chapter 
factored states hear horn fast car horn agent 
gaze object reports agent type object currently view 
agent looking side highway sees shoulder 
looks car sees car 
sees road 
gaze side gaze direction agent restricted field view see current lane lane side 
side currently looking reported gaze side variable 
look forward backward chosen lane 
reported gaze direction 
gaze speed agent tell relative speed object gaze speed 
road shoulder cars moving agent loom 
road shoulder cars moving away agent 
gaze distance gaze refined distance distance object reported variables 
road sections front agent broken groups nose meters near meters far meters 
refined distance variable breaks discretized distances sections 
refined distance take values far half near half 
example state near far half car meters agent 
intent agent get rough fine idea distance paying attention just gaze distance distance refined distance 
road shoulder objects appear horizon gaze distance far refined distance far half 
gaze colour object assigned colour random 
road shoulder white gray tan yellow 
cars colours 
note chapter 
factored states colour informative 
serves increase size state space add noise observations 
agent approaches slower car slow collide slow car 
manages past slow car lane 
mccallum calls nyc squeeze 
reward delivered agent time step 
agent squeezing past slow car receives reward 
faster car receives reward 
receives positive reward 
goal agent change lanes pass slower cars stay way faster cars 
new cars appear randomly time step 
fast cars appear agent slow cars appear front agent 
time step probability new car appear 
fast slow cars equal probability appearing 
cars distributed lanes speed right left slow cars appear lanes probability 
fast cars follow distribution left right 
simulator equipped simple character output modeled shown mccallum 
shows simulator action 
capital letters represent slow vehicles small letters represent fast vehicles 
letter indicates vehicle colour 
letter indicates agent 
symbol points current gaze position row column forward right example 
new york driving task test combinations inference tech niques value function approximators 
table learner human driver trained visible state comparison purposes 
human driver author trained character simulator similar 
different algorithms summarized table described paragraphs 
course summary state variables right omitted 
hear horn value shown 
car shown current gaze position 
state information removed display 
chapter 
factored states performance number algorithms approximations measured task random policy learning sensory inputs model localist repre sentation hidden state consisted single multinomial random variable various approximate value functions mean field inference approxi mate value functions human driver 
localist representation non factored versions value function approximators 
localist method factored multinomial evidence representation described eq 
non human algorithms trained times 
training run lasted iterations 
human driver author trained iterations character graphical display iteration lasting seconds 
results mccallum tree algorithm included comparison lum 
tree algorithm combines linear value function approximator decision tree representation core process state 
decision tree learned experience distinctions subsets observed variables various points past 
value function learned dynamic programming model dynamics learned state representation 
glickman sycara reported results new york driving task 
direct policy method policy represented recurrent neural network 
holding network structure fixed optimize parameters network genetic algorithm 
report results network architectures 
network stochastic binary units 
second network deterministic sigmoid units 
stochastic policies rl algorithms actions chosen results reported follows glickman sycara stochastic network performance runs runs 
sigmoid network performance runs runs 
intervals confidence intervals 
results shown samples scores generated mixtures gaussians appropriate means variances mixing proportions chapter 
factored states boltzmann distribution temperature decreasing time exp qf zb cases learning rate reduced exploration temperature reduced exponentially course task 
hidden units time slice localist model states 
words models matched number parameters 
learner table representation entries states actions 
training performance final time steps examined algorithm 
human driver tested time steps results comparison methods 
results shown 
note results negative smaller magnitudes indicating better perfor mance 
box indicates lower upper quartile median values 
whiskers show extent rest data 
outliers labeled blue stars 
single lines datum available tree human 
show results quadratic approximators 
preliminary tests showed cases quadratic model failed converge approx imation 
unable fit approximator due numerical instabilities 
cases parameters approximation grew infinity 
unable determine happening 
approximator successfully factored setting rodriguez parr koller 
mlp approximator localist distributed representation performed better linear approximation 
ard improved dbn performance sig 
see localist distributed models performed comparably better tree 
noted tree run time steps methods genetic algorithm allowed testing time shorter results multiplied 
chapter 
factored states reward reward vs algorithm ll lm dl dla dm dma ut ga ga algorithm results algorithms new york driving 
random driver flat learning ll localist linear lm localist mlp dl distributed linear dla distributed linear ard dm distributed mlp dma distributed mlp ard ut tree mccallum ga genetic algorithm stochastic glickman sycara ga genetic algorithm sigmoid glickman sycara human driver train time steps 
variance dbn models higher localist methods dbn models occasionally converge poorly performing solution 
models flat learning perform worse stochastic ga gorithm ga results extremely poor solutions 
noted ga algorithm required orders magnitude training time algorithms steps total 
striking result flat learning 
equals outperforms sophisticated algorithms equals human performance 
consistently converges solution 
indicates amount hidden state original task great 
memoryless deterministic policy task 
ask constitutes performance task 
unfortunately chapter 
factored states know optimal policy compare directly optimal perfor mance 
access underlying core states 
solve underlying mdp directly get upper bound optimal performance 
un fortunately core state space extremely large 
tried approximately solve underlying mdp learning different approximation methods 
simple state aggregation 
distinguishing different distances driver just distinguished front greater meters away 
ignored colours 
second method set hand picked binary features fi learn approximate linear action value func tion learning weights features 
approximate value state action fi fi feature satisfied current state zero 
features 
horn blowing 
feature 
slow car front lane distance 
features 
slow car front lane distance 
features 
fast car lane distance 
features 
fast car lane 
features 
am currently lane 
features 
am looking ahead left center right 
features 
am looking left center right 
features bob price suggesting approach 
chapter 
factored states total features 
different algorithms allowed train iterations 
algorithm performed flat learning visible observations 
solution underlying mdp human performance task estimate performance 
information performance maximum attainable reward occurs collisions horns 
achievable learner get reward steps 
unfortunately clear reward achievable policy 
restricted ny driving order increase amount hidden state problem removed sensory inputs 
performance flat learner significantly degraded single input gaze side removed 
shows perfor mance subset algorithms restricted version driving task 
parameters learning rates temperatures training time number states unchanged 
results measured steps simulation 
flat learner doing better random near performance level sophisticated algorithms 
performance flat learner improve uses stochastic policy implemented halting learning temperature 
localist algorithms mixed performance 
reward achieved dbn mlp algorithm roughly previous task 
human performance shown performance original task 
just included comparison 
reason believe human performance degrade removing single state variable 
chapter 
factored states reward reward vs algorithm qs ll lm dla dma algorithm results algorithms restricted ny driving 
random driver flat learning qs flat learning stochastic policy ll localist linear lm localist mlp dla distributed linear ard dma distributed mlp ard human driver visual new york driving new york driving task uses relatively high level sensory input 
attributes looming receding provide lot information specific solving task 
nice aspect dbn approach need high level attributes 
demonstrate created visual new york driving task 
underlying task reward structure original new york driving task 
sensory input shown table learner access image bit hear horn sensor 
image designed convey instantaneous information gaze distance gaze object colour 
pixels objects coloured original task 
background colour black pixel take values meaning approximately possible sensory inputs 
course tiny fraction possible inputs observed 
example images corresponding states shown chapter 
factored states 
notice lot information looming gaze direction absent instantaneous input inferred remembering previous states actions 
idea force learner memory solve task 
bit core state space trained visual new york driving task 
mlp approximate value function ard model learning 
model trained time steps 
localist models states trained comparison 
localist models allowed train time steps 
learning rate temperature schedule unchanged 
tried run localist model ard comparison purposes 
images seen learner approximately wonder dimensionality reduction performed pre processing step followed memoryless learning 
enumerated possible sensory inputs states table learner 
result running reduced input shown 
tested best localist algorithm dbn algorithm reduced input 
algorithm run times 
total reward steps averaged runs 
performance shown 
localist model increasingly state space grows improvement statistically significant 
table learner trained reduced input table learner restricted ny driving task better localist models 
inferior dbn method 
dbn method bad run performance comparable flat learning 
runs significantly better table learner 
dbn performance similar previous versions task original restricted new york driving 
note human performance measured original task 
included comparison purposes 
human driver necessarily visual version task 
chapter 
factored states gaze colour gray gaze refined dist near half gaze distance near gaze speed looming gaze direction forward gaze side center gaze object car hear horn gaze colour tan gaze refined dist near half gaze distance near gaze speed looming gaze direction forward gaze side left gaze object shoulder hear horn gaze colour red gaze refined dist near half gaze distance far gaze speed looming gaze direction forward gaze side center gaze object car hear horn gaze colour tan gaze refined dist far half gaze distance far gaze speed looming gaze direction forward gaze side center gaze object road hear horn gaze colour red gaze refined dist near half gaze distance nose gaze speed looming gaze direction forward gaze side right gaze object car hear horn examples new york driving state associated image visual new york driving 

chapter 
factored states reward reward vs algorithm ll lm lma ll lm lm ll lm dma dma algorithm results algorithms visual ny driving 
random driver table learning reduced input ll ll ll localist linear states lm lm lm localist mlp states lm localist mlp reduced input states lma localist mlp ard states dma distributed mlp ard hidden variables dma dbn mlp ard reduced input hidden variables human driver interestingly localist dbn model poorly reduced input 
suggest structure images important success algorithm 
fact strong correlations input variables time step time steps helps models tremendously 
strong correlations harder models learn structure time series data noisy gradient signals 
figures show typical sequence involving horn collision avoidance 
image left bird eye view road 
sequence images right shows actual state perceived agent 
note forward top image 
slow cars appear top image disappear bottom 
shows emission features learned model 
generated turning binary unit generating image 
colour ignored images 
binary units significant weights image omitted 
learner models cars various distances 
uses bit core state chapter 
factored states indicate horn driver 
watching learner action clear models aspects internal state gaze side 
allows quickly look right sees red car left previous gaze side immediately available state 
see section interpreting behaviour hidden units model 
eligibility traces far discussed final performance algorithm ignored rate methods learn task 
experiment eligibility traces investigate effect varying rate convergence solution 
unit mlp function approximator ard 
task restricted ny driving 
figures show effect varying speed convergence types eligibility traces sutton barto naive trace watkins trace 
traces general eligibility trace form eq 
recall moves zero method comes closer full monte carlo method 
lower line value reported performance tree algorithm 
upper line value reported performance genetic algorithm stochastic recurrent network 
performance current greedy policy holding parameters fixed simulating iterations 
iteration comprised updating state selecting action modifying parameters 
figures show performance averaged learning trials value 
convergence solution fastest intermediate values 
particular methods best 
watkins method appears best fastest convergence solution evaluation trial fixed random seed initialization sequence obstacle cars 
policy evaluated random seed restored value held just prior evaluation trial 
chapter 
factored states lowest variance solutions 
algorithms failed converge solution 
results consistent previous results learning eligibility traces singh sutton singh dayan specific best value varies different problems literature 
note traces reduce original learning algorithm investigated section 
best dbn algorithm learns task number iterations mccallum tree trained actions performance comparable genetic algorithm 
table algorithms tested new york driving task 
algorithm code model regularization inference value random flat learning enumeration state actions table localist linear ll hidden markov model exact linear localist mlp lm hidden markov model exact mlp distributed linear dl mean field linear distributed linear ard dla ard mean field linear mlp dm mean field mlp distributed mlp ard dma ard mean field mlp tree ut decision tree exact linear genetic algorithm stochastic ga stochastic recurrent network genetic algorithm sigmoid ga sigmoid recurrent network human unknown unknown unknown unknown chapter 
factored states chapter 
factored states 


agent green car clear road fast car yellow blowing horn 
agent looks right sees looming gray car 
agent gaze direction symbolized white cone 
chapter 
factored states 


agent passes gray car sees tan car lane 
chapter 
factored states agent changes lanes right moving tan car 
lets fast yellow car speed away top display 
tan car looming ahead 
agent looks right sees nearby red car 
looks left sees far tan car 
chapter 
factored states agent changes lanes left moving tan car 
slow cars left drop bottom display 
tan car ahead looms closer 
chapter 
factored states agent looks right sees free lane 
shifts right passing looming tan car 
chapter 
factored states 
emission features learned visual new york driving 
feature corresponds binary hidden unit active 
binary units relevant output omitted 
white indicates higher black indicates lower probability activity associated binary unit 
example feature progressively active car gets closer small car outlines dark bigger car outlines get lighter 
feature active cars maximum distance small white dot inactive shoulder images small gray dots horizon 
feature interested cars active horn agent 
chapter 
factored states reward reward reward iterations iterations iterations learning curves naive eligibility traces reward reward reward iterations iterations iterations performance naive traces function restricted ny driving 
center line plot shows mean reward trials 
dashed lines contain reward values trials 
straight lines show reported final performance mccallum tree algorithm lower glickman sycara genetic algorithm upper 
chapter 
factored states reward reward reward iterations iterations iterations learning curves watkins eligibility traces reward reward reward iterations iterations iterations performance watkins traces function restricted ny driving 
center line plot shows mean reward trials 
dashed lines contain reward values trials 
straight lines show reported final performance mccallum tree algorithm lower glickman sycara genetic algorithm upper 
chapter factored actions chapter new method compact representation action value functions high dimensional factored mdps 
representation allows efficient selection actions high dimensional action space estimated value 
initial results sallans hinton 
lot research discussion problems dealing large state spaces 
comparatively little dealing large action spaces dean givan kim meuleau hauskrecht kim peshkin kaelbling dean boutilier peshkin meuleau kaelbling guestrin koller parr 
action space large action composed action variables 
infinite action real valued 
practice action spaces tend large 
example consider activation large numbers muscles simultaneous actions players football field 
previous chapters discussed table function approximator reinforcement learning 
seen cases table representation value function appropriate 
particularly true factored mdps size state action space grows exponentially number state action variables 
similarly state action space continuous table chapter 
factored actions representation usually inappropriate 
non linear function approximator model action value function extracting policy involves solving constrained non linear optimization problem action selection 
chapter method value state action pair represented negative free energy state action pair non causal graphical model 
interpretation action value negative free energy possible select actions necessarily optimal actions efficiently 
stochastic policies trying solve pomdp technique solve fully observable mdp 
mdp framework applied problems strictly markov observable state 
policies currently visible state select actions known memoryless policies 
shown stochastic memoryless policies perform better pomdps deterministic memoryless policies jaakkola singh jordan 
result mind learning stochastic policies value function direct policy methods 
direct policy methods extended include memory adding additional memory bits turned part state time step peshkin meuleau kaelbling 
turning memory bits formalized extending action set include actions flip bits 
case action space quickly large 
methods strict assumptions structure stochastic policies represented 
example value function method map chapter 
factored actions state action pairs values boltzmann distribution express policy exp exp denominator sum possible actions 
direct policy method map states probability performing action 
cases prove prob high dimensional factored actions 
summation actions value function method quickly intractable 
case parameterized policies family parameterized policies chosen advance 
possible choice family factored policies peshkin meuleau kaelbling 
probability selecting element action independent elements current state 
choice policies subsets action variables correlated guestrin koller parr 
details distribution action variables correlated modes conditional action distribution pre specified 
illustrate point consider problem bit action optimal policy shown table 
family policies pre selected solving table optimal stochastic policy factored representation action probability problem family factored policies able find policy case 
course prior knowledge problem able select family parameterized policies policy exists 
prior knowledge problem hard come draw sample paths chapter 
factored actions mdp 
return example section 
products experts represent value state action pair free energy pair probabilistic model 
model represent value function product experts hinton 
products experts probabilistic models combine simpler models multiplying distributions 
product experts probability assigned state action pair normalized product probabilities assigned individual experts pk pk pk probability distribution expert parameters experts indexes possible state action pairs 
product experts model defined energy function defines joint probability distribution ek energy equal negative log probability additive constant 
constant equal log partition function exp exp denominator called partition function 
computing partition function typically intractable large state action spaces 
energy easily computed 
additional latent variables model distribution chapter 
factored actions observed variables defined terms free energy ek hk denotes vector hidden variables 
term expected energy second term entropy posterior distribution hidden units 
bayesian networks partition function unity energy equal negative log probability visible hidden variables 
product models undirected equality holds additive constant 
free energy terms expected energy respect posterior distribution hidden variables visible variables entropy posterior 
posterior distribution exp ek hk exp ek exp ek hk exp ek hk posterior distribution product experts factors product posterior distributions individual experts 
long inference individual experts tractable inference tractable product model 
inference tractable product experts useful properties 
state action pair exact free energy easily computed 
second derivative free energy respect parameter network simple 
properties vital products experts reinforcement learning 
examples product models include restricted boltzmann machines smolensky hinton products hidden markov models brown hinton products chapter 
factored actions gaussian mixtures hinton rate coded restricted boltzmann machines teh hinton 
notice case individual experts binary logistic units hidden markov models gaussian mixtures tractable 
model trained minimize temporal difference error sarsa learning update rule see section 
training action state clamping state drawing sample action variables gibbs sampling geman geman 
finding optimal actions difficult large problems selecting action probability approximately proportional exp normally done small number iterations gibbs sampling including simulated annealing 
principle gibbs sampling converge equilibrium distribution draw unbiased samples boltzmann exploration policy temperature 
particular select actions policy may complicated parameterize explicitly 
decide priori modes policy distribution action variables may correlated 
method allows represent access larger class policies previously possible direct policy actor critic methods 
particular allows represent policies compact functional representation 
allows learn stochastic policies know priori correlations represented see sections 
provides way selecting approximately optimal actions large discrete real valued action spaces 
tractable models bayesian networks product experts model model action value function 
probabilistic model suffice 
example completely tractable model mixture gaussians singly connected bayesian network 
chapter 
factored actions cases partition function computed tractably sample actions model exactly resorting gibbs sampling 
alternatively different class models combined approximate inference 
example multiply connected bayesian networks varia tional approximation 
case value state action pair represented negative variational free energy 
action selection involve iterative procedure fitting variational parameters current state access entire policy distribution 
closely related actor critic methods discussed section 
discrete states actions kinds experts retaining useful properties poe 
section focus case expert single binary logistic unit 
case interesting inference learning model intensely studied ackley hinton sejnowski hinton sejnowski smolensky freund haussler hinton binary values relatively easy interpret 
multinomial state action variable represented set binary units constrained exactly 
product experts bipartite restricted boltzmann machine rbm smolensky hinton see 
si denote th state variable aj denote th action variable 
denote binary latent variables experts hk see 
keep mind states clamped actions sampled multinomial restrictions respected 
restrictions ignore fact binary state vector represents values set multinomial state variables 
likewise binary action variable 
representation free energy chapter 
factored actions hidden units state units action units boltzmann product experts 
estimated action value setting state action units holding units fixed computing free energy network 
actions selected holding state variables fixed sampling action variables 
multinomial state action variable represented set binary units exactly 
binary case 
state si action aj free energy expected energy posterior distribution hidden units minus entropy posterior 
posterior distribution hidden units factors posterior distri bution latent binary variable independent state action 
free energy simple compute hidden units independent posterior distribution hk hk ik hk jk bk hk hk log hk hk log hk wik weight th expert binary state variable si weight equivalently just think terms multinomial units potts model 
chapter 
factored actions th expert binary action variable aj bk bi bj biases hk bk expected value expert latent variable data denotes logistic function 
terms eq correspond expected energy third negative entropy distribution hidden units data 
emphasize free energy computed tractably inference tractable product experts 
non factored policies recall simple problem mentioned section 
clear restricted boltzmann machine action sampling optimal policy represented 
shown 
hidden unit half time half time 
action hidden illustration restricted boltzmann machine represent simple non factored stochastic policy 
action selected 
action selected 
important point large number different correlations action variables captured value function approximator manifested policy action sampling 
need know ahead time action variables correlated modes policy distribution 
learned model 
cost actions sampled chapter 
factored actions iterative procedure 
continuous states actions free energies model values continuous valued state action spaces 
section discuss particular real valued task 
describe ways product model model continuous states actions 
rate coded rbm consider restricted boltzmann machine unit duplicated times 
total activity replicated set units integer range 
approximate real valued unit large number duplicated discrete units 
real value able take values 
advantage inference learning rules duplicated units unchanged activities range 
replica unit computation probability computation performed simultaneously replicas 
sampling sum activities probabilities efficient approximation 
sum independent binomial random variables converges normal distribution limit approximate sampling sum independent duplicate units sampling gaussian distribution appropriate mean variance 
replicas hidden unit unit active probability mean np variance np 
selecting activities independent units sample gaussian mean variance round nearest integer 
equivalently think single unit emits spikes time interval 
total activity ensemble units equal rate spiking chapter 
factored actions ik rate coded restricted boltzmann machine 
real valued activities units thought total activity ensemble duplicate units 
equivalently think single unit emit number spikes time interval 
real valued activity rate unit fires 
maximum rate reason type rbm called rate coded restricted boltzmann machine rbmrate teh hinton 
product alternative real valued states actions product simple mixture models 
hinton mixture gaussian uniform distribu tion 
latent variable discrete binary variable indicates gaussian uniform active 
energy terms gaussian active con tributes quadratic energy mean 
uniform active contributes zero energy 
final energy hidden unit activities sum quadratic energies gaussians active 
free energy model hk bk log log vk hk log hk hk log hk chapter 
factored actions indexes element product model dimensionality vector bk bias switching probabilities vk mean standard deviation gaussian hk posterior probability making gaussian active hk exp bk log log vk model isotropic gaussians 
disadvantage rbmrate advantage model real value encoded uncertainty state binary hidden unit 
real value take intermediate values distribution hidden unit high entropy 
model describe case am certain value 
model entropy real value separate 
variance encodes uncertainty real value bias encode information uncertainty particular gaussian 
mean gaussian encodes expected real value 
encode am certain value put mean variance small 
despite apparent benefits model dif fit model practice 
particular binary units trained models tended resulted simple gaussian model 
free energy fit value state action pairs 
state action pair large negative value way represent model placing gaussians far pair 
gaussian near state action pairs turned 
chapter 
factored actions learning model parameters model parameters adjusted goodness state action pair product model approximates action value 
modified temporal difference learning update rules sarsa 
saw chapter temporal difference error quantifies inconsistency value state action pair discounted value state action pair immediate reinforcement account 
values approximated negative free energy poe model update parameters model sarsa temporal difference update rule learning update rule 
sarsa possibility modified sarsa learning rule designed minimize td error rummery niranjan sutton 
consider case rbm sarsa update delta rule update target input 
update wik wik wik restricted boltzmann machine partial derivative respect weight give wik wik hk hk bk hk chapter 
factored actions hk log hk hk log hk si hk weights biases updated similarly bi bj bk aj hk si aj hk real valued rbm rate networks parameter updates identical 
uni gauss models means gaussians updated ik ik hk bk log log vk hk log hk hk log hk hk si ik biases log variances updated similarly bk log hk hk si ik proof convergence general learning rule works practice ignores effect changes parameters 
possible derive update rules actual gradient 
see example baird chapter 
factored actions moore 
learning discussed chapter sarsa policy update 
algorithm chasing moving target trying converge value function policy currently executed 
alternative policy method tries converge value function optimal policy regardless policy currently executed 
update rule policy algorithm 
update rule wik wik partial derivative action value function respect parameter eq 
difference sarsa action denotes action sampled low temperature 
course guaranteed optimal action typically action experiments tried see section 
sarsa guaranteed converge bounded region value function space linear case 
fact examples divergence known bertsekas tsitsiklis 
works simulation cases 
sampling actions trained network current state need generate actions negative free energy 
gibbs sampling 
possible ways implementing gibbs sampling advantages drawbacks 
chapter 
factored actions sequential action sampling iterate action units sampling turn holding fixed integrating hidden units 
discrete case evaluate free energy action variable possible values holding action units fixed 
sample value boltzmann distribution 
words updating action units softmax enforce constraint set binary units represent mutually exclusive actions action variable 
iterative gibbs sampling reaches equilibrium draws unbiased samples action values value 
nice feature method avoid sampling noise explicitly integrating hidden units 
possible inference tractable poe model 
drawback obvious parallelize sampling possible see neal 
serial computers speed differences caused primarily implementation details 
alternating block sampling method alternating block sampling 
method appropriate undirected graphical model poe bipartite graph structure 
start arbitrary initial action represented action units 
holding state units fixed update hidden units parallel get sample posterior distribution hidden units state action 
update action units parallel get sample posterior distribution actions states hidden units 
updating action units softmax enforce constraint set binary units represent mutually exclusive settings action variable 
alternating gibbs sampling reaches equilibrium draws unbiased samples hidden unit action chapter 
factored actions pairs value 
marginalize throwing away hidden activities just keeping action values 
nice feature method sampling hidden units conducted parallel 
similarly poe models consider action variables sampled parallel 
especially advantageous large action spaces 
problem block sampling frequently want find actions sampled boltzmann distribution temperature 
just alternately sample hidden action variables lower temperature select actions correct probabilities 
sample hidden action pairs low joint energy 
hidden action pairs may correspond actions 
account replicating hidden units sampling neal communication 
suppose want sample actions temperature positive integer want sample distribution proportional consider network copies hidden units 
joint distribution actions replicas hidden units marginal hk exp hk hk exp hk exp 
exp hk sampling joint distribution larger network get sample hk chapter 
factored actions correct marginal simply discarding hidden unit values 
method proposed general optimization method doucet godsill robert 
local minima explicitly sampling hidden units block sampling method sensitive local minima joint hidden unit action energy surface 
tested kinds sampling random networks hidden action variables 
weights biases chosen gaussian distribution zero mean standard deviation 
brief sampling lasted iterations 
sampling done temperature 
local minima problems approximately percent networks 
network shown worst example problem 
example network troublesome block sampling 
hidden action case best action 
best action 
conditional distributions hidden states actions shown 
iteratively sampling hidden units actions causes sampler get example frequencies actions sampling methods different sense squared difference 
chapter 
factored actions probability probability action hidden hidden action action hidden hidden action action hidden hidden action action hidden hidden action conditional distributions block sampler problematic example 
top plots show conditional distribution hidden configuration action configuration 
bottom plots show conditional distribution action configuration hidden configuration 
temperature get stuck suboptimal action 
sequential sampling avoid local minima 
stuck suboptimal hidden action configuration 
tion action expect suboptimal action 
expect jump suboptimal minima probability iteration sampling temperature 
time expect sampler find better action 
brief sampling time find action case 
shows frequency energy states types sampling initial action 
test cases find example network block sampling worked better sequential sampling 
course types networks type sampling prone getting stuck local minima 
chapter 
factored actions frequency block sampling action frequency sequential sampling action frequency versus action number types gibbs sampling 
free energy configuration shown bar 
case block sampling method frequently gets stuck local minimum 
general block sampling sensitive local minima involving hidden state configurations sequential sampling sensitive local minima change action variables simultaneously escape 
practice sequential sampling block sampling resulted similar performance various experimental tasks 
experiments implemented matlab block sampling faster case 
able fast matlab linear algebra routines quickly sample hidden units actions units matlab operations 
contrast sequential action sampling involves iterating action variables 
type iteration slow matlab 
language issue 
exploration select actions value decide ex strategy 
common action selection scheme boltzmann exploration 
chapter 
factored actions probability selecting action proportional action value function 
move exploration exploitation adjusting temperature parameter possible selection scheme greedy optimal action selected probability random action selected probability 
exploration probability reduced time move learner exploration exploitation 
sarsa update rule boltzmann exploration samples boltzmann distribution current temperature sufficient 
greedy evaluate maxa 
approximated sampling low temperature 
improve mixing temperature initialized high value lowered course sampling 
simulation results test algorithm introduce tasks stochastic policy task blockers task large action task rocket task 
task highlights different aspect algorithm strengths weaknesses 
stochastic policy task implementation example task mentioned section 
visible state action consists binary variables 
settings action variables randomly selected rewarded 
words actions rewarded rest possible actions receive reward 
visible learner hidden state variable indicates actions currently rewarded 
executing action result reward 
executing correct action results reward chapter 
factored actions toggling binary hidden variable 
task repeating action multiple times result single reward execution 
access hidden state optimal policy alternate actions 
learner access hidden state memory optimal policy stochastic pick actions equal probability 
restricted boltzmann machine hidden unit trained instance task 
learning update rule training lasted trials 
result shown 
hidden bias weights visible bias reward reward vs iteration random optimal learned iterations results learning implicit stochastic policy 
hinton diagram shows sign magnitude biases units weights hidden action units 
white positive black negative 
area square indicates magnitude weight bias 
actions 
activated hidden unit second activated visible biases hidden unit 
hidden unit turns probability 
graph shows plot reward vs iteration learning 
reward executing policy recommended current action value function steps averaging chapter 
factored actions result 
optimal reward expected reward picking randomly rewarding actions equal probability 
action bits alternate time step 
action bits alternate held fixed 
learner discover subset action variables alternate 
example agent learned switch action bit switch fifth 
anti correlated hidden unit 
pre specified family stochastic policies able achieve performance allowed action variables correlated anti correlated 
number action variables grows pre specifying correlations allowed prohibitive terms number parameters required 
blockers task blockers task operative multi agent task offensive players trying reach zone defensive players trying block see 
blockers agents zone example blocker task 
agents get past blockers 
blockers preprogrammed strategy agents cooperate blockers simultaneously 
task operative long agent reaches zone team rewarded 
team receives reward agent reaches zone reward 
blockers pre programmed fixed blocking strategy 
agent occupies square grid blocker occupies horizontally adjacent squares 
agent move square occupied blocker agent 
task non wrap edge conditions bottom left right sides chapter 
factored actions field blockers agents move left right 
agents move squares occupied agents blockers 
agents ordered 
agents want move square agent ordering succeed trying move square unsuccessful 
note agents see moves earlier agents making decisions 
ordering just resolve collisions 
move unsuccessful agent remains current square 
blockers moves ordered subsequent blockers decisions moves earlier blockers 
blockers operate zone defense 
blocker takes responsibility group columns 
example blocker responsible columns blocker responsible columns 
agent moves columns front zone blocker move block 
ordering blockers move agents stopped blockers 
restricted boltzmann machine hidden units trained learning rule blocker task agents blocker 
combined state consisted position variables agents blocker take integer values 
combined action consisted action variables values 
network run twice combined actions combined actions learning rate going linearly ture going exponentially course training 
trial terminated zone reached combined actions taken whichever occurred 
trial initialized blocker placed randomly top row agents placed randomly bottom row 
learning rate temperature schedule train learner table containing learning parameters selected trial error preliminary experiments 
chapter 
factored actions elements learner allowed train combined actions 
training policy run steps rewards 
algorithms compared hand coded policy agents move opposite sides field move zone 
case algorithms performed comparably poe network performs training iterations see table 
variation results hand coded policy poe caused random starting positions agents blockers 
rbm network hidden units trained blockers task agents blockers 
input consisted position variables blocker agent action variables agent 
network trained combined actions learning rate temperature schedule previous task 
trial terminated zone reached steps taken whichever occurred 
training resultant policy run steps rewards received 
table representation elements table learner trained comparison 
hand coded policy moved agents left middle right column moved agents zone 
poe performed slightly worse hand coded policy 
results experiments summarized table 
note reward totals negative smaller magnitude better 
example typical run task shown 
strategy discovered learner force blockers apart agents move middle third 
examples features learned experts shown 
hidden units active specific configuration state space recommends specific set actions 
histograms feature indicate feature tends active trial 
histograms show feature ac tivity localized time 
features thought macro actions short term chapter 
factored actions table poe blocker experimental results algorithm reward random policy agents blocker hand coded agents blocker learning agents blocker steps poe agents blocker steps poe agents blocker steps random policy agents blockers hand coded agents blockers poe agents blockers steps policy segments 
hidden unit active particular phase trial recommends actions appropriate phase ceases active 
large action task solution blockers task shows free energy approximate action value function 
task difficult prolonged temporal delays reward blockers task large action space 
task opposite properties large action space temporal delays reward 
consider version task bit action 
task generated follows small number state action pairs randomly selected 
call key pairs 
execution state chosen random action selected learner 
reward generated finding key state closest current state hamming distance 
reward received learner equal number bits match action key state current action 
agent selects key action receives maximum reward reward action subtracting number incorrect bits task chapter 
factored actions example agent strategy learning blocker task 
agents initialized random locations bottom field 
agents run top playing field 
agents split run sides 
third agent moves middle zone 
illustrated 
poe network hidden units trained instantiation large action task bit state space bit action space 
key states randomly selected 
network run actions learning rate going temperature going exponentially course training 
iteration initialized random state 
action selection consisted iterations gibbs sampling 
optimal action known state compare results learning parameters selected trial error preliminary experiments 
chapter 
factored actions agent agent agent agent agent agent agent agent agent agent agent agent features learned value function approximator 
features correspond stages shown 
feature corresponds hidden unit rbm 
hinton diagram shows agents order activate feature 
vector diagram indicates actions recommended feature 
histogram plot frequency activation feature versus time trial 
shows run feature tends active 
learned features localized state space action space 
feature activity localized time 
optimal policy 
size state action space compare table learner hand crafted policy 
results shown 
chapter 
factored actions key state current state key action current action reward large action task 
space state bit vectors divided clusters nearest key state 
key state associated key action 
reward received learner number bits shared selected action key action current state 
reward reward vs iteration exploratory random optimal current iterations results large action task 
optimal policy gives average reward 
random shows average return random action selection policy 
exploratory shows reward returned current value function current exploration temperature 
current shows reward returned current value function minimum exploration temperature 
curve shows greedy policy respect current value function 
learner overcome difficulties 
find actions receive re wards state 
cluster states share commonly rewarded actions infer underlying key states 
state space contains entries chapter 
factored actions action space contains entries trivial task 
learner achieves perfect performance actions 
rocket task task involves factored real valued state action variables 
rocket guided target 
state consists real valued variables position velocity target position 
position variables range velocity variables range 
action consists real valued variables thrust 
action variables range 
immediate reward current distance target bonus target reached rt tx ty indicator function unity target reached time step zero 
rocket task 
rocket green circle reach target blue ring 
state consists position velocity target xy position 
action thrust red lines 
rate coded restricted boltzmann machine hidden units trained rocket task 
trial task lasted steps target reached whichever came 
network trained combined actions sarsa update rule learning rate going linearly temperature chapter 
factored actions going exponentially course training 
trial initialized rocket random position random velocity target random position 
training policy run trials rewards 
result training compared random policy optimal policy head straight target maximum thrust 
results shown 
see learned policy near optimal policy 
reward qp algorithm results rocket task 
random policy qp learning poe optimal due limitations rbmrate network coding real valued variables 
variables tended saturate leading bang bang type control axes 
seen 
tractable mixture gaussians model trained rocket task 
mixture contained elements 
target position held fixed cen ter performance comparable rbmrate network 
target location allowed vary performance degraded completely 
mixture model unable deal larger state space 
network trained rocket task 
network better representational properties behaved poorly practice 
fact unable successfully fit model task 
result binary variables chapter 
factored actions example paths rbmrate training 
asterisk indicates rocket starting position 
mark indicates target position 
shows target hit trial finished 
indicates dots trajectory indicate equally spaced points time 
typically control variables saturate leading bang bang control 
biased 
possible new models poe family better representing real values frequently approximately satisfied models hinton teh help improve performance tasks chapter 
factored actions real valued states actions 
chapter discussion states dbn algorithm related various streams research machine learning rein learning 
factored dynamic models learned data ghahramani jordan 
structured variational approximation batch ing 
non factored models learned pomdps chrisman mccallum 
split merge model selection techniques determine size required state space 
split merge criteria designed maximize likelihood chrisman maximize ability model predict values mc 
pre specified factored models dynamic programming boutilier poole meuleau hauskrecht kim peshkin kaelbling dean boutilier dean givan kim poupart boutilier boutilier dearden goldszmidt charnes shenoy poupart ortiz boutilier reinforcement learning mccallum rodriguez parr koller thrun investigated 
mcmc approximations thrun poupart ortiz boutilier belief state simplification rodriguez parr koller poupart boutilier approximate inference pomdp 
chapter 
discussion factored models fully observable mdps learned tadepalli ok 
knowledge attempt combine learning factored model approximate inference reinforcement learning 
model features shown learning compact representation core state pomdp feasible 
dbn model performs methods problems smaller state space outperforms localist models complex problems high dimensional inputs substantial hidden state 
units dbn encode useful features input car near nose position 
encode dynamic history current gaze direction previous action 
advantages simple stochastic policy learned learning learner knows oncoming car best randomly select look left right 
systematically looks left right wasting fewer actions 
interesting ask information units encoding visual ny driving task 
access factored visible state original ny driving task table compare hidden unit activities presence original attributes get insight hidden units encoding 
show correlation original state variables hidden unit activity interesting units 
particular hidden units encode world state object type object speed object distance proprioceptive information gaze direction 
graphs unit counted active approximated posterior probability turning unit greater equal 
remember model access original sensory information 
just access image 
learned model attributes useful describing dynamics emissions process 
chapter 
discussion frequency frequency frequency vs gaze side unit left center right gaze side frequency vs speed unit loom speed frequency frequency frequency vs gaze distance unit far near nose gaze distance frequency vs object unit truck shoulder road object correlation state attributes hidden unit activity hidden units 
axis indicates proportion time hidden unit active state attribute 
unit acting detector attribute value proportion near value near 
dbn model learned regard task solved 
localist models trained unsupervised way perform 
suggests dbn model simply able capture dynamical structure localist models 
structure turns useful predicting reward results higher performance 
interesting learn models features useful predicting reward 
discuss possibility learning task specific models section 
chapter 
discussion models reward dbn method requires model order update belief state step 
model known full iterations dynamic programming sample backups learning 
possible full backups efficiently access compact representation process boutilier poole 
speed convergence improve stability algorithm 
similarly consider batch updates dynamical model model held fixed experience captured period time 
model updated reflect batch experience 
full dynamic programming backups require model immediate reward 
described method include model immediate reward generation 
learn simple model assuming core state variable contributes immediate reward independently ks mi drawn gaussian learned mean variance 
parameters gaussian learned stochastic gradient descent squared difference predicted sampled immediate reward 
learning model instantaneous reward necessary dynamic programming 
useful belief updating observed reward evidence current state really update belief state time step 
done current method 
convergence limitation dbn algorithm convergence 
formal guarantees learning rule value function converge best approximation chapter 
discussion parameters remain bounded 
am training dynamic model stochastic gradient descent guaranteed limit dbn converge local optimum approximate log probability 
simulations shown convergence problem dbn model 
runs converge poorly performing solution 
interestingly failures caused value function model dynamics 
model dynamics method observed converge performing solution 
suggested regularization model help get consistent results 
reason introduced ard ard improve convergence solutions 
forcing method learn task specific model help 
formal guarantee convergence value function 
monte carlo methods get unbiased estimates expected return allow learn approximate value function guaranteed convergence subject usual restrictions learning rates convergence stochastic gradient ascent 
full monte carlo method require training time algorithm longer practical 
local minima value function potential problem 
actions action sampling method closely related actor critic methods barto sutton anderson sutton 
normally actor critic method actor network viewed biased scheme selecting actions value assigned critic 
selection biased choice parameterization 
sampling method action selection unbiased markov chain allowed converge 
resultant policy potentially complicated typical parameterized actor network allow 
parameterized policy chapter 
discussion explicitly normalized 
exactly trade explored graphical models literature monte carlo inference neal variational approximations jaakkola 
sampling algorithm related probability matching jordan actions probable model temperature probability computed slowly reduced time order move exploration exploitation avoid local minima 
sampling algorithm probability matching algorithm parameterized distribution maximized gradient descent address temporal credit assignment 
macro actions way interpret individual models product learning macro basis actions 
seen blockers task hidden units come represent sets actions spatially temporally localized 
think hidden units representing basis actions combined form wide array possible actions 
benefit having basis actions reduces number possible actions making exploration efficient 
drawback set basis actions span space possible actions actions impossible execute 
optimizing set basis actions reinforcement learning find set form useful actions excluding action combinations seen useful 
advantage having subset basis actions seen pre learned action basis set comparing learning time model learn state action associations 
re ran agent blockers task see section different conditions see 
condition learning macros original experiment 
state hidden action hidden weights randomly initialized learned si 
second condition optimized action macros fixed set chapter 
discussion average reward learning curve macro actions learning macros optimized action macros optimized state macros random action macros iterations learning curve agent blockers problem pre learned macro actions 
plot shows average reward time step current policy averaged runs versus training time time steps 
pre learned action hidden weights learns state hidden weights 
fixed action hidden weights taken earlier successful run algorithm 
sim third condition optimized state macros fixed set optimized state hidden weights learns action hidden weights 
final condition fixed random set action hidden weights optimizes state hidden weights 
notice increased learning speed simply due reducing number learned parameters 
state hidden associations fixed advance model fails learn 
asymmetry related basic asymmetry mdps learner observes state choose action 
pre selecting combinations states represented hidden units task exploration easier 
learner see combinations states select exponential number actions 
simply fix fact learning harder states represented ones seen early learning 
probably reason optimized state macros condition fails learn 
states represented value function restricted seen successful trials seen learning exploration 
early exploration states seen essentially look value function approximator 
chapter 
discussion random macro actions priori 
example number hidden unit combinations number possible actions 
simply fixing random macro actions speed learning 
action macros learned exclude bad actions 
example learned macro actions agents move 
restriction subset actions speeds learning 
macro actions learned poe confused temporally actions 
learning temporally actions important area current research reinforcement learning parr russell precup sutton singh mcgovern dietterich 
macro actions learned poe features common temporally actions 
particular poe macro actions tend remain active temporally prolonged periods 
temporally actions 
come formal machinery temporally actions termination conditions difficult fit existing frameworks temporal abstraction 
poe basis actions thought finding smaller subset actions large space possible actions 
multiagent language learning interesting area cooperative multiagent research idea allowing individual agents supplement state information communicating 
communication channel agents may limited bandwidth noisy 
challenge learn common language 
additional information agent interpreted useful way 
bandwidth agent just state information turning problem single large mdp assuming agents collectively observe state information 
course larger state space slow learning 
trade supplying additional information improve solution restricting chapter 
discussion information speed learning 
language learning operative multiagent systems explored yanco stein peshkin kim meuleau kaelbling jim giles 
latent variables product model thought just kind addi tional information shared agents 
case information supplement state information replaces 
connection explicit way add lateral connections state action variables agent 
second hidden unit eliminate state hidden connections agents state variables 
state agent influences latent variable expert 
hidden activities consist messages coming individual agents 
agent decide action state hidden unit activities see 
note direct state action connections mean boltzmann machine longer bipartite 
condition known state conditionally bipartite 
case exact inference tractable 
conditional completion action allowed particular time step constrained natural want know best action consistent constraints 
example agents blocker task unable move directions ask actions agents consistent restriction 
gibbs sampling allows easily fixing subset action variables required values sampling rest 
result set values action variables conditioned fixed values see 
similarly fix state variables sample 
doubt useful data completion state variable missing nice fill probable value conditioned 
unfortunately chapter 
discussion connections language interpretation hidden variables conditionally bipartite boltzmann machine 
example shows connections agent system 
state variable particular agent connect directly corresponding action variable hidden word variable 
word variables connected action variables 
current state inference boltzmann machine tractable 
zone zone action state conditional completion 
actions optimistic pessimistic states sampled conditioned subset states actions 
single value function network 
filling values probable dynamics world fill values yield high expected returns 
words values filled state variables desirable probable 
chapter 
discussion optimistic form state completion giving upper bound reward expect see really know values state variables take 
interestingly pessimistic state completion reversing sign free energy sampling values state variables select values give lower expected returns 
way get estimate bad current situation decide act optimistic pessimistic state completion 
alternatively learn separate density model states state completion learned conditional distribution state variables state variables 
factored states actions previous chapters methods reinforcement learning mdps pomdps factored state action spaces 
methods combined number ways 
pomdp method chapter learn dbn model pomdp real valued poe model value function lieu value function approximators 
useful action space factored real valued 
alternative learn dynamical model relating latent variables poe model different time steps see 
view learning extended state consists current observation latent variables previous time step 
update parameters poe transition parameters minimize td error time steps 
similar learning additional memory bits peshkin meuleau kaelbling context value functions 
latent variables treated actions current time step additional state variables step 
hidden action chapter 
discussion ht dynamic poe 
energy hidden states time depend hidden states time observation action time variables drawn complex non parameterized distribution dictated poe model 
interestingly method related learning task specific dbn models see section 
number directions 
outline possibilities 
direct policy methods thesis concentrated value function solution methods 
technique popular direct policy approach williams hansen sutton mcallester singh mansour ng jordan 
idea learning value function deriving policy learn parameters policy directly 
direct policy methods interesting cases optimal value function quite hard represent optimal policy 
function approximators direct policy methods typically com chapter 
discussion pact mappings states distributions actions 
poe function approximation technique place explicit policy parameterizations 
adjusting parameters poe negative free energy approximates value parameters adjusted directly maximize value duced policy 
policy implicitly parameterized extracted model gibbs sampling 
particularly advantageous cases state action spaces augmented additional memory bits 
addi tional memory bits learn better policies pomdps peshkin meuleau kaelbling pass information operating agents peshkin kim meuleau kaelbling 
way encode correlations external memory bits direct policy method allow sequential setting bits 
bit set current visible state bit set current visible state bit 
requires backward propagation reward information potentially time steps 
way pre specify policy correlations allowed 
idea correlations simple policy explicitly normalized 
example specify fully factored policies policies allow correlations small sets action variables 
poe network implicitly parameterize policy 
way need pre specify action variables interact pass reward information backward steps action variable selection 
example trained poe network bit version stochastic policy task 
remember task best stochastic policy involves selecting best actions equal probability 
resultant performance comparable value function method see 
simple version direct policy method 
algorithm maintains chapter 
discussion hidden bias weights visible bias reward reward vs iteration random optimal learned iterations rbm poe network trained bit stochastic policy task 
network trained simple direct policy method average actions probable penalized average actions 
decaying average reward running average reward instantaneous reward 
update rule decreased free energy action average increased average action average probable 
action average probable 
case free energy correspond value just captures relative goodness actions 
simple direct policy method 
methods developed theoretically sound proofs convergence williams hansen sutton mcallester singh mansour ng jordan 
poe methods subject restriction method require chapter 
discussion samples policy policy distribution 
hierarchical learning seen section hidden units interpreted spatio temporal macro actions 
interestingly hidden units tend active singly 
stead hidden units sparse distributed activity pattern 
interesting build hierarchical model second layer see visible states hidden units previous layer 
hidden units interpreted action variables higher level model 
pomdp model treated latent variables extra state information mentioned section organized hierarchically 
task specific distinctions currently dbn model learned trying maximize likelihood observations 
results generally applicable model necessarily best model solving particular task 
maximum size model fixed advance possible statistical features data captured relevant task 
example correlation colour object type driving task 
represented model colour really relevant solving driving task object type attribute available 
interesting tie parameter learning directly learning task 
simple step direction learn model immediate reward see section 
encourage model learn representation immediate rewards markov current core state 
alternatively split merge criteria encourage rewards markov core states 
approach mccallum 
similar vein ard value function approximator tell core chapter 
discussion state variables relevant predicting reward 
particular variable informative reward parameters linking value function fall zero pruned irrespective role transition emission model 
alter learning rule model penalizing size td error 
td error function current belief state 
belief state deterministic function current belief state action observation rt max deterministic belief update function parameterized 
fixed value function penalize update parameter transition emission model factor proportional derivative td error respect parameter log eq encourage model learn representation value function approximation approximator true value function 
interestingly draw link variation dbn model dynamic version poe method mentioned section 
consider adding parameter log eq task independent dbn model 
task specific dynamic poe 
try learn general model meets performance requirements slowly reducing course learning moni performance 
chapter 
discussion summary thesis shown approximate inference model learning value function approximation allow solution large factored markov decision processes 
new techniques allow compact models stochastic policies unanticipated corre lations learned 
shown new algorithms solve problems extremely large state action spaces bit states bit actions 
thesis fills gap intersection model learning approximate inference reinforcement learning 
draws links approximate inference state representation action selection 
demonstrated new connections areas graphical models reinforcement learning hopefully stimulate research 
available new set test problems available matlab functions benchmarks research 
specific contributions thesis 
new method modeling factored pomdps 
dbn method combines ap proximate inference model estimation value function approximation 
demonstrated dbn algorithm solve pomdps large state observation spaces 

new technique approximating value functions large factored mdps pomdps 
shown poe algorithm solve mdps large action spaces discrete continuous mdps 

novel method selecting actions poe function approximator 
demonstrated brief sampling method works large action spaces 

new representation stochastic policies large action spaces 
policy explicitly represented necessary know priori exactly chapter 
discussion correlations included 
far am aware method able represent complex stochastic policies large action spaces policy may compact functional representation 

matlab implementations decision problems studied thesis 
hopefully problems particularly visual ny driving task encourage devel opment algorithms solve new classes large interesting po mdps 
specific contributions outlined areas research cluding multi agent language learning hierarchical action representation task specific model learning 
certainly story solution large factored po mdps 
particular methods practice known convergence results error bounds 
think re search learning task specific models directly learning stochastic policies particularly fruitful 
hopefully thesis encourage cross areas bayesian inference machine learning reinforcement learning 
appendix task functions variable type obvious variables generally described bit vectors multinomial vectors reals structs exceptions 
bit vectors matlab vectors element integer 
multinomial vectors matlab vectors element integer attribute specific set 
reals matlab floating point numbers 
structs task specific matlab structures 
generally need know inside struct initialized manipulated function calls 
matlab code tasks available www gatsby ucl ac uk sallans tasks html 
new york driving new york driving task involves driving lane single direction highway 
driver pass slower cars avoid getting way faster cars 
function initializes data structure maintains state simu vector observables 
initialize core hidden state observation sn input output sn initial core state struct initial observation multinomial vector chapter task functions function takes current state simulator action returns state simulator vector observations reward 
step driving dynamics sn input current core state struct current action output sn new core state struct emitted observation multinomial vector immediate reward real final functions display current state road human readable format 
display state information input current observation multinomial vector output display road state input current core state struct output restricted ny driving task identical new york driving task gaze side feature removed 
testing indicated memoryless policy performed original task 
removing gaze side information task gained hidden chapter task functions state 
belief state policies continued performance memoryless policy fell 
masking done setting gaze side attribute center irrespective true value 
mask gaze side input current observation multinomial vector output new masked observation multinomial vector visual ny driving task state reward structure ny driving task 
order task difficult sensory input replaced colour image single horn bit indicate faster car stuck driver 
addition functions function convert current observable state image 
image conjunction hear horn bit observation vector visual new york driving 
generate image observation input current observation multinomial vector output image matrix elements stochastic policy task demonstrates free energy approach learn access stochastic policies correlations know priori 
initialize core hidden state sn sa input chapter task functions sa size action number bits output sn core state step dynamics sn input current core state struct current action bit vector output sn new core state struct immediate reward real blockers task shows free energy approach learn value functions tasks temporally delayed reward 
function initializes states actions simulator 
blockers randomly placed top row agents randomly placed bottom row 
initialize state sn na nb input na number agents nb number blockers number rows field number columns field output sn initialized state bit vector function agents blocker task 
step blocker dynamics sn chapter task functions input current state bit vector current action bit vector output sn new state bit vector immediate reward real final function agents blockers task 
numbers agents blockers different field dimensions 
number agents blockers computed function state action size field dimensions 
step blocker dynamics sn input current state bit vector current action bit vector number rows field number columns field output sn new state bit vector rew immediate reward real large action large action task shows free energy method represent value functions search efficiently large discrete state spaces 
initialize key states actions ks ss input ss length state vector length action vector output ks key state action structure struct chapter task functions compute reward state action pair ks input current state bit vector current action bit vector ks key state action pairs struct output immediate reward real rocket rocket task show free energy method learn value functions real valued state action spaces 
initialize rocket state sn input output sn rocket target state struct step rocket dynamics sn input current state struct current action real vector output sn new state struct immediate reward real bibliography ackley hinton sejnowski 
learning algorithm boltzmann machines 
cognitive science 
baird klopf 
reinforcement learning high dimensional con actions 
technical report wl tr wright laboratory wright patterson air force base 
baird moore 
gradient descent general reinforcement learning 
see kearns solla cohn 
barto bradtke singh 
learning act real time dynamic programming 
artificial intelligence 
barto sutton anderson 
neuronlike adaptive elements solve difficult learning control problems 
ieee transactions systems man cybernetics 
baum petrie 
statistical inference probabilistic functions finite state markov chains 
annals mathematical statistics 
baum petrie soules weiss 
maximization technique occurring statistical analysis probabilistic functions markov chains 
annals mathematical statistics 
bellman 

dynamic programming 
princeton nj princeton university press 
bellman 

markov decision process 
journal mathematical ics 
bertsekas tsitsiklis 
neuro dynamic programming 
belmont ma athena scientific 
boutilier dearden 
approximating value trees structured dynamic programming 
icml 
bibliography boutilier dearden goldszmidt 
stochastic dynamic program ming factored representations 
unpublished manuscript 
boutilier dearden goldszmidt 
stochastic dynamic program ming factored representations 
articial intelligence 
boutilier poole 
computing optimal policies partially observable decision processes compact representations 
proc 
aaai 
boyen koller 
tractable inference complex stochastic processes 
proc 
uai 
brown hinton 
products hidden markov models 
jaakkola richardson eds proceedings artificial intelligence statis tics pp 

cassandra 

exact approximate algorithms partially observ able markov decision processes 
providence ri department computer science brown university 
ph thesis 
cassandra littman zhang 
incremental pruning simple fast exact method partially observable markov decision processes 
uncertainty artificial intelligence 
charnes shenoy 
forward monte carlo method solving influence diagrams local computation 
school business working school business university kansas 
chrisman 

reinforcement learning perceptual aliasing perceptual distinctions approach 
tenth national conference ai 
cooper 

method belief networks influence diagrams 
proc 
fourth conference uncertainty artificial intelligence pp 

cooper 

computational complexity probabilistic inference bayesian belief networks 
artificial intelligence 
dagum luby 
approximating probabilistic inference bayesian belief networks np hard 
artificial intelligence 
dayan 

convergence td general 
machine learning 
dean givan kim 
solving stochastic planning problems large state action spaces 
proc 
fourth international conference artificial intelligence planning systems 
bibliography dean kanazawa 
model reasoning persistence 
computational intelligence 
dearden 

structured prioritized sweeping 
icml 
dearden boutilier 
abstraction approximate decision theoretic planning 
artificial intelligence 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal statistical society series 
dietterich 

hierarchical reinforcement learning maxq value func tion decomposition 
journal artificial intelligence research 
doucet godsill robert 
marginal maximum posteriori esti mation markov chain monte carlo 
technical report cambridge university department engineering 
doya 

temporal difference learning continuous time space 
see touretzky mozer hasselmo pp 

freund haussler 
unsupervised learning distributions binary vectors layer networks 
moody hanson lippmann eds advances neural information processing systems volume 
morgan kaufmann san mateo 
geman geman 
stochastic relaxation gibbs distributions bayesian restoration images 
ieee transactions pattern analysis ma chine intelligence 
ghahramani hinton 
variational learning switching state space models 
neural computation 
ghahramani jordan 
factorial hidden markov models 
machine learning 
glickman sycara 
evolutionary search stochastic policies mem ory reinforcement learning hidden state 
proceedings eighteenth international conference machine learning 
gordon 

reinforcement learning function approximation converges region 
see leen dietterich tresp 
bibliography guestrin koller parr 
multiagent planning factored mdps 
advances neural information processing systems volume 
mit press cambridge 
hansen 

solving pomdps searching policy space 
proc uai pp 


solving pomdp policy linear approximate learning algorithm 
technical report 
hinton 

products experts 
icann volume pp 

hinton 

training products experts minimizing contrastive diver gence 
technical report tr gatsby computational neuroscience unit ucl 
hinton ghahramani 
generative models discovering sparse distributed representations 
phil 
trans 
roy 
soc 
london 
hinton sejnowski 
parallel distributed processing volume chapter learning relearning boltzman machines pp 

mit press cambridge 
hinton 
teh 
discovering multiple constraints fre quently approximately satisfied 
proc uai 
howard 

dynamic programming markov processes 
cambridge ma mit press 
howard matheson 
principles applications decision analysis volume ii chapter influence diagrams pp 

strategic decision group park ca 
jaakkola 

variational methods inference estimation graphical models 
cambridge ma department brain cognitive sciences mit 
ph thesis 
jaakkola singh jordan 
reinforcement learning algorithm partially observable markov decision problems 
tesauro touretzky leen eds advances neural information processing systems vol ume pp 

mit press cambridge 
jim giles 
communication improve performance multi agent systems 
proc 
agents 
bibliography jordan ghahramani jaakkola saul 
variational methods graphical models 
machine learning 
kalman 
march 
new approach linear filtering prediction prob lems 
trans 
asme series journal basis engineering 
kearns solla cohn eds 

advances neural infor mation processing systems volume 
mit press cambridge 
koller parr 
computing factored value functions policies structured mdps 
proc 
ijcai 
koller parr 
policy iteration factored mdps 
proc 
uncer tainty ai uai 
tsitsiklis 
actor critic algorithms 
submitted siam journal control optimization february 
leen dietterich tresp eds 

advances neural information processing systems volume 
mit press cambridge 
littman cassandra kaelbling 
learning policies partially observable environments scaling 
proc 
international conference machine learning 
littman cassandra kaelbling 
efficient dynamic programming updates partially observable markov decision processes 
technical report cs department computer science brown university 
lovejoy 

survey algorithmic methods partially observable markov decision processes 
annals operations research 
mackay 

bayesian non linear modeling energy prediction com 
ashrae transactions 
mcallester singh 
approximate planning factored pomdps belief state simplification 
proc 
uai 
mccallum 

reinforcement learning selective perception hidden state 
rochester ny dept computer science rochester 
ph thesis 
mccallum 

efficient exploration reinforcement learning hidden state 
aaai fall symposium model directed autonomous systems 
bibliography mcgovern 

acquire macros algorithm automatically learning macro actions 
nips workshop abstraction hierarchy reinforcement learning 
meuleau hauskrecht 
kim peshkin kaelbling dean boutilier 
solving large weakly coupled markov decision processes 
proc 
aaai 
monahan 

survey partially observable markov decision processes theory models algorithms 
management science 
neal 

connectionist learning belief networks 
artificial intelligence 
neal 

probabilistic inference markov chain monte carlo methods 
technical report crg tr department computer science university toronto 
neal 

bayesian learning neural networks 
new york ny springer verlag 
neal 

assessing relevance determination methods delve 
bishop ed neural networks machine learning 
new york ny springer verlag 
neal 

circularly coupled markov chain sampling 
technical report dept statistics university toronto 
ng jordan 
pegasus policy search method large mdps pomdps 
proc uai 
ng parr koller 
policy search density estimation 
see solla leen ller pp 

parr russell 
approximating optimal policies partially observable stochastic domains 
proc 
ijcai 
parr russell 
reinforcement learning hierarchies machines 
jordan kearns solla eds advances neural information processing systems volume 
mit press cambridge 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
san mateo ca morgan kaufmann 
peng williams 
incremental multi step learning 
proc 
th international conference machine learning pp 

bibliography peshkin 
kim meuleau kaelbling 
learning cooperate policy search 
proc 
uai 
peshkin meuleau kaelbling 
learning policies external memory 
proc 
th international conference machine learning pp 

poupart boutilier 
value directed belief state approximation pomdps 
proc 
uai 
poupart boutilier 
vector space analysis belief state approximation pomdps 
proc uai 
poupart ortiz boutilier 
value directed sampling methods monitoring pomdps 
proc uai 
precup sutton singh 
theoretical results reinforcement learn ing temporally options 
proceedings tenth european con ference machine learning 


markov decision processes discrete stochastic dynamic pro gramming 
new york wiley 
rabiner 
juang january 
hidden markov models 
ieee magazine 
rodriguez parr koller 
reinforcement learning approxi mate belief states 
see solla leen ller 
rumelhart hinton williams 
learning internal repre sentations back propagating errors 
nature 
rummery niranjan 
line learning connectionist systems 
technical report cued infeng tr engineering department cambridge university 
russell 

learning agents uncertain environments extended 
proc 
colt 
acm press 
jordan 
reinforcement learning probability matching 
see touretzky mozer hasselmo pp 

sallans 

hierarchical community experts 
toronto canada university toronto 
sc 
thesis 
sallans 

learning factored representations partially observable markov decision processes 
see solla leen ller pp 

bibliography sallans hinton 
free energies represent values multiagent reinforcement learning task 
see leen dietterich tresp 
sutton ram 
experiments reinforcement learn ing problems continuous state action spaces 
adaptive behavior 
sato ishii 
reinforcement learning line em algorithm 
see kearns solla cohn 
saul jaakkola jordan 
mean field theory sigmoid belief networks 
journal artificial intelligence research 
schneider 
wong moore riedmiller 
distributed value functions 
proc 
th international conf 
machine learning pp 

morgan kaufmann san francisco ca 
shachter 

evaluating influence diagrams 
operations research 
shachter 
gaussian influence diagrams 
management science 
shachter peot 
decision making probabilistic inference methods 
proc 
eighth conference uncertainty artificial intelligence pp 

shimony 

finding maps belief networks np hard 
artificial intelli gence 
shumway stoffer 
approach time series smoothing forecasting em algorithm 
journal time series analysis 
singh dayan 
analytical mean squared error curves temporal difference learning 
machine learning 
singh sutton 
reinforcement learning replacing eligibility traces 
machine learning 
smallwood sondik 
optimal control partially observable processes finite horizon 
operations research 
smolensky 

information processing dynamical systems foundations harmony theory 
rumelhart mcclelland eds parallel dis tributed processing explorations microstructure cognition 
volume foundations 
cambridge ma mit press 
bibliography solla leen 
ller eds 

advances neural informa tion processing systems volume 
mit press cambridge 
sondik 

optimal control partially observable markov processes infinite horizon discounted costs 
operations research 
sondik 

optimal control partially observable markov processes 
stanford calf department engineering economic systems stanford univer sity 
ph thesis 
st aubin hoey boutilier 
approximate policy con struction decision diagrams 
see solla leen ller 
stolcke omohundro 
hidden markov model induction bayesian model merging 
hanson cowan giles eds advances neural information processing systems volume pp 

morgan kaufmann san mateo 
sutton 

learning predict method temporal differences 
ma chine learning 
sutton 

integrated architectures learning planning reacting approximating dynamic programming 
proc 
international conference machine learning 
sutton 

generalization reinforcement learning successful examples sparse coarse coding 
see touretzky mozer hasselmo pp 

sutton barto 
reinforcement learning 
cam bridge ma mit press 
sutton mcallester singh mansour 
policy gradient methods reinforcement learning function approximation 
see solla leen ller pp 

tadepalli ok 
scaling average reward reinforcement learning approximating domain models value function 
proc 
th interna tional conference machine learning 
teh 
hinton 
rate coded restricted boltzmann machines face recognition 
see leen dietterich tresp 
thrun 

monte carlo pomdps 
see solla leen ller 
bibliography touretzky mozer hasselmo eds 

advances neural information processing systems volume 
mit press cambridge 
watkins 

learning delayed rewards 
cambridge uk cam bridge university 
ph thesis 
watkins dayan 
learning 
machine learning 
williams 

simple statistical gradient algorithms connection ist reinforcement learning 
machine learning 
yanco stein 
adaptive communication protocol cooperating mobile robots 

meyer roitblat wilson eds animals animats international conference simulation adaptive behavior pp 

mit press 
zhang 

probabilistic inference influence diagrams 
proc 
uai pp 

zhang liu 
planning stochastic domains problem char approximation 
technical report cs department computer science hong kong univeristy science technology 
