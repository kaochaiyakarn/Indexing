templates solution linear systems building blocks iterative methods richard barrett michael berry tony chan james demmel june donato jack dongarra victor charles henk van der vorst document electronic version nd edition templates book available purchase society industrial applied mathematics www siam org books 
supported part darpa aro contract number daal national science foundation science technology center cooperative agreement 
ccr applied mathematical sciences subprogram office energy research department energy contract de ac stichting nationale computer crg 
computer science mathematics division oak ridge national laboratory oak ridge tn 
department computer science university tennessee knoxville tn 
applied mathematics department university california los angeles ca 
computer science division mathematics department university california berkeley ca 
science applications international oak ridge tn texas advanced computing center university texas austin austin tx national institute standards technology gaithersburg md office science technology policy executive office president department mathematics utrecht university utrecht netherlands 
ii book divided book main chapters 
chapter gives motivation book templates 
chapter describes stationary nonstationary iterative methods 
chapter historical development state art methods solving challenging computational problems facing researchers 
chapter focuses preconditioners 
iterative methods depend part preconditioners improve performance ensure fast convergence 
chapter provides glimpse issues related iterative methods 
chapter preceding especially recommended experienced user wishes guidelines tailoring specific code particular machine 
includes information complex systems stopping criteria data storage formats parallelism 
chapter includes overviews related topics close connection lanczos algorithm conjugate gradient algorithm block iterative methods red black orderings domain decomposition methods multigrid methods row projection schemes 
appendices contain information templates blas software obtained 
glossary important terms book provided 
field iterative methods solving systems linear equations constant flux new methods approaches continually created modified tuned eventually discarded 
expect material book undergo changes time time new approaches mature state art 
plan update material included book periodically editions 
welcome comments criticisms help updating process 
please send comments questions email templates cs utk edu 
iii list symbols 
matrices 
vectors 
scalars matrix transpose ah conjugate transpose hermitian matrix inverse inverse ai matrix element jth matrix column ai matrix subblock ai vector element ux second derivative respect xt vector dot product inner product diag jth component vector ith iteration diagonal matrix diag 
diagonal matrix constructed scalars 
span 
spanning space vectors 
set real numbers rn real space norm norm norm defined ax max min eigenvalues maximum resp 
minimum modulus max min largest smallest singular values spectral condition number matrix linear operator complex conjugate scalar max maximum value set min minimum value set summation big oh asymptotic bound conventions book diagonal matrix lower triangular matrix upper triangular matrix orthogonal matrix preconditioner identity matrix typically exact solution ax discretization mesh width iv authors gratefully acknowledge valuable assistance people commented preliminary drafts book 
particular adams bill matthew peter forsyth roland freund gene golub eric grosse mark jones david kincaid steve lee nachtigal jim ortega david young insightful comments 
geoffrey fox initial discussions concept templates karin designing front cover 
supported part darpa aro contract number daal national science foundation science technology center cooperative agreement 
ccr applied mathematical sciences subprogram office energy research department energy contract de ac stichting nationale computer crg 
vi contents list symbols iv list figures ix templates 
methods covered 
iterative methods overview methods 
stationary iterative methods 
jacobi method 
gauss seidel method 
successive overrelaxation method 
symmetric successive overrelaxation method 
notes 
nonstationary iterative methods 
conjugate gradient method cg 

cg normal equations 
generalized minimal residual gmres 
biconjugate gradient bicg 
quasi minimal residual qmr 
conjugate gradient squared method cgs 
biconjugate gradient stabilized bi cgstab 
chebyshev iteration 
computational aspects methods 
short history krylov methods 
survey krylov methods 
preconditioners 
cost trade 
left right preconditioning 
jacobi preconditioning 
block jacobi methods 
discussion 
preconditioning 
incomplete factorization preconditioners 
creating incomplete factorization 
vii viii contents point incomplete factorizations 
block factorization methods 
blocking systems partial differential equations 
incomplete lq factorizations 
polynomial preconditioners 
preconditioners 
preconditioning symmetric part 
fast solvers 
alternating direction implicit methods 
related issues complex systems 
stopping criteria 
details stopping criteria 
readily available 
estimating 
stopping progress longer 
accounting floating point errors 
data structures 
survey sparse matrix storage formats 
matrix vector products 
sparse incomplete factorizations 
parallelism 
inner products 
vector updates 
matrix vector products 
preconditioning 
wavefronts gauss seidel conjugate gradient methods 
blocked operations gmres method 
remaining topics lanczos connection 
block step iterative methods 
reduced system preconditioning 
domain decomposition methods 
overlapping subdomain methods 
non overlapping subdomain methods 
remarks 
multigrid methods 
row projection methods 
obtaining software overview blas glossary notation 
flowchart iterative methods list figures jacobi method 
gauss seidel method 
sor method 
method 
preconditioned conjugate gradient method 
preconditioned gmres method 
preconditioned biconjugate gradient method 
preconditioned quasi minimal residual method look ahead 
preconditioned conjugate gradient squared method 
preconditioned biconjugate gradient stabilized method 
preconditioned chebyshev method 
preconditioner solve system mx lu 
preconditioner solve system mx 
construction ilu incomplete factorization preconditioner storing inverses pivots 
wavefront solution central difference problem domain points 
preconditioning step algorithm neumann expansion incomplete factorization 
block version ilu factorization 
algorithm approximating inverse banded matrix 
incomplete block factorization block tridiagonal matrix 
profile nonsymmetric skyline variable band matrix 
rearrangement conjugate gradient parallelism 
ix list figures chapter statements true 
users want black box software complete confidence general problem classes having understand fine algorithmic details 
users want able tune data structures particular application software reliable provided general methods 
turns true different groups users 
traditionally users asked provided black box software form mathematical libraries lapack linpack nag 
highperformance community discovered write custom software problem 
reasons include inadequate functionality existing software libraries data structures natural convenient particular problem overly general software sacrifices performance applied special case interest 
meet needs groups users 
believe 
accordingly book introduce templates 
template description general algorithm executable object code source code commonly conventional software library 
templates general descriptions key algorithms offer degree customization user may desire 
example configured specific data structure problem specific computing system problem run 
focus iterative methods solving large sparse systems linear equations 
methods exist solving problems 
trick find effective method problem hand 
unfortunately method works problem type may 
may 
providing templates suggest choose implement effective method specialize method specific matrix types 
restrict iterative methods repeatedly improving approximate solution accurate 
methods access coefficient matrix linear system product 
user need supply subroutine computing permits full exploitation sparsity special structure believe reading book applications developers able templates get program running parallel machine quickly 
know choose implement approach solve particular problem 
specialists able assemble modify codes having huge investment required tune large scale applications particular machine 
hope users gain better understanding algorithms employed 
education chapter 
traditional goals mathematical software believe approach go long way providing valuable service 
templates 
templates offer significant advantages 
templates general reusable 
simplify ports diverse machines 
feature important diversity parallel architectures 
second templates exploit expertise distinct groups 
expert numerical analyst creates template reflecting depth knowledge specific numerical technique 
computational scientist provides value added capability general template description customizing specific contexts applications needs 
third templates language specific 
displayed algol structure readily translatable target language fortran basic linear algebra subprograms blas possible familiar styles believe users trust algorithms 
hope users gain better understanding numerical techniques parallel programming 
template provide mathematical description flow iteration discussion convergence stopping criteria suggestions applying method special matrix types banded systems advice tuning example preconditioners applicable tips parallel implementations hints method 
templates obtained electronic mail 
matlab implementation dense matrices fortran program calls blas see appendix details 
methods covered 
iterative methods developed impossible cover 
chose methods illustrate historical development iterative methods represent current state art solving large sparse linear systems 
methods discuss 
jacobi 
gauss seidel 
successive relaxation sor 
symmetric successive relaxation discussion blas building blocks see lapack routines 
see appendix 
methods covered 

conjugate gradient cg 
minimal residual symmetric lq 
conjugate gradients normal equations 
generalized minimal residual gmres 
biconjugate gradient bicg 
quasi minimal residual qmr 
conjugate gradient squared cgs 
biconjugate gradient stabilized bi cgstab 
chebyshev iteration method general description including discussion history method numerous literature 
give mathematical conditions selecting method 
intend write cookbook deliberately avoided words numerical recipes phrases imply algorithms blindly knowledge system equations 
state art iterative methods permit knowledge linear system needed guarantee convergence algorithms generally known algorithm tuned 
chosen algorithmic outline guidelines choosing method implementing particular kinds high performance machines 
discuss preconditioners relevant data storage issues 
chapter 
chapter iterative methods term iterative method refers wide range techniques successive approximations obtain accurate solutions linear system step 
book cover types iterative methods 
stationary methods older simpler understand implement usually effective 
nonstationary methods relatively development analysis usually harder understand highly effective 
nonstationary methods idea sequences orthogonal vectors 
exception chebyshev iteration method orthogonal polynomials 
rate iterative method converges depends greatly spectrum coefficient matrix 
iterative methods usually involve second matrix transforms coefficient matrix favorable spectrum 
transformation matrix called preconditioner 
preconditioner improves convergence iterative method sufficiently overcome extra cost constructing applying preconditioner 
preconditioner iterative method may fail converge 
overview methods short descriptions methods discussed brief notes classification methods terms class matrices appropriate 
sections chapter detailed descriptions methods 
stationary methods jacobi 
jacobi method solving variable locally respect variables iteration method corresponds solving variable 
resulting method easy understand implement convergence slow 
gauss seidel 
gauss seidel method jacobi method uses updated values soon available 
general jacobi method converges gauss seidel method converge faster jacobi method relatively slowly 
sor 
successive overrelaxation sor derived gauss seidel method introducing extrapolation parameter 
optimal choice sor may converge faster gauss seidel order magnitude 
chapter 
iterative methods 
symmetric successive overrelaxation advantage sor standalone iterative method useful preconditioner nonstationary methods 
nonstationary methods conjugate gradient cg 
conjugate gradient method derives name fact generates sequence conjugate orthogonal vectors 
vectors residuals iterates 
gradients quadratic functional minimization equivalent solving linear system 
cg extremely effective method coefficient matrix symmetric positive definite storage limited number vectors required 
minimum residual symmetric lq 
methods computational alternatives cg coefficient matrices symmetric possibly indefinite 
generate solution iterates cg coefficient matrix symmetric positive definite 
conjugate gradient normal equations 
methods application cg method forms normal equations ax solves system aa computes solution solves solution vector coefficient matrix nonsymmetric nonsingular normal equations matrices aa symmetric positive definite cg applied 
convergence may slow spectrum normal equations matrices favorable spectrum generalized minimal residual gmres 
generalized minimal residual method computes sequence orthogonal vectors combines squares solve update 
cg requires storing sequence large amount storage needed 
reason restarted versions method 
restarted versions computation storage costs limited specifying fixed number vectors generated 
method useful general nonsymmetric matrices 
biconjugate gradient bicg 
biconjugate gradient method generates cg sequences vectors system original coefficient matrix sequence mutually orthogonal bi orthogonal 
method cg uses limited storage 
useful matrix nonsymmetric nonsingular convergence may irregular possibility method break 
bicg requires multiplication coefficient matrix transpose iteration 
quasi minimal residual qmr 
quasi minimal residual method applies squares solve update bicg residuals smoothing irregular convergence behavior bicg 
qmr largely avoids breakdown occur bicg 
hand effect true minimization error residual converges smoothly essentially improve bicg 
conjugate gradient squared cgs 

stationary iterative methods conjugate gradient squared method variant bicg applies updating operations sequence sequences vectors 
ideally double convergence rate practice convergence may irregular bicg 
practical advantage method need multiplications transpose coefficient matrix 
biconjugate gradient stabilized bi cgstab 
biconjugate gradient stabilized method variant bicg cgs different updates sequence order obtain smoother convergence cgs 
chebyshev iteration 
chebyshev iteration recursively determines polynomials coefficients chosen minimize norm residual min max sense 
coefficient matrix positive definite knowledge extremal eigenvalues required 
method advantage requiring inner products 
stationary iterative methods iterative methods expressed simple form bx depend iteration count called stationary iterative methods 
section main stationary iterative methods jacobi method gauss seidel method successive overrelaxation sor method symmetric successive overrelaxation method 
case summarize convergence behavior effectiveness discuss 
give historical background notes 
jacobi method jacobi method easily derived examining equations linear system ax isolation 
ith equation ai jxj bi solve value xi assuming entries remain fixed obtain xi bi ai jxj ai 
suggests iterative method defined bi ai jx ai jacobi method 
note order equations examined irrelevant jacobi method treats independently 
reason jacobi method known method simultaneous displacements updates principle done simultaneously 
matrix terms definition jacobi method expressed chapter 
iterative methods choose initial guess solution 

xi 

xi xi ai jx xi bi xi ai check convergence continue necessary jacobi method matrices represent diagonal strictly lower triangular strictly upper triangular parts respectively 
pseudocode jacobi method 
note auxiliary storage vector algorithm 
possible update vector place values needed computation convergence jacobi method iterative methods solving discretized partial differential equations 
context rigorous analysis convergence simple methods jacobi method 
example consider boundary value problem lu discretized lu xi xi xi xi xi xi 

eigenfunctions operator 
function un sin eigenfunction corresponding sin 
eigenvalues jacobi iteration matrix sin 
easy see high frequency modes eigenfunction un large damped quickly damping factor modes small close 
spectral radius jacobi iteration matrix attained eigenfunction sin type analysis applied example generalized higher dimensions stationary iterative methods 
jacobi gauss seidel method spectral radius discretization mesh width number variables number space dimensions 
gauss seidel method consider linear equations 
proceed jacobi method assume equations examined time sequence previously computed 
stationary iterative methods choose initial guess solution 


ai jx 
ai jx bi ai check convergence continue necessary gauss seidel method results soon available obtain gauss seidel method bi ai jx ai jx ai 
important facts gauss seidel method noted 
computations appear serial 
component new iterate depends previously computed components updates done simultaneously jacobi method 
second new iterate depends order equations examined 
gauss seidel method called method successive displacements indicate dependence iterates ordering 
ordering changed components new iterate just order change 
points important sparse dependency component new iterate previous components absolute 
presence zeros matrix may remove influence previous components 
judicious ordering equations may possible reduce dependence restoring ability updates groups components parallel 
reordering equations affect rate gauss seidel method converges 
poor choice ordering degrade rate convergence choice enhance rate convergence 
practical discussion tradeoff parallelism versus convergence rate standard reorderings reader referred chapter 
matrix terms definition gauss seidel method expressed ux 
represent diagonal lower triangular upper triangular parts respectively 
pseudocode gauss seidel algorithm 
successive overrelaxation method successive overrelaxation method sor devised applying extrapolation gauss seidel method 
extrapolation takes form weighted average previous iterate chapter 
iterative methods choose initial guess solution 


ai jx 
ai jx bi ai check convergence continue necessary sor method computed gauss seidel iterate successively component denotes gauss seidel iterate extrapolation factor 
idea choose value accelerate rate convergence iterates solution 
matrix terms sor algorithm written follows 
pseudocode sor algorithm 
choosing value sor method simplifies gauss seidel method 
theorem due kahan shows sor fails converge outside interval 
technically term convenience term overrelaxation value 
general possible compute advance value optimal respect rate convergence sor 
possible compute optimal value expense computation usually prohibitive 
frequently heuristic estimate mesh spacing discretization underlying physical domain 
coefficient matrix symmetric positive definite sor iteration guaranteed converge value choice significantly affect rate sor iteration converges 
sophisticated implementations sor algorithm employ adaptive parameter estimation schemes try home appropriate value estimating rate iteration converging 
coefficient matrices special class called consistently ordered property see young includes certain orderings matrices arising discretization elliptic pdes direct relationship spectra jacobi sor iteration 
stationary iterative methods matrices 
principle spectral radius jacobi iteration matrix determine priori theoretically optimal value sor opt 
seldom done calculating spectral radius jacobi matrix requires impractical amount computation 
relatively inexpensive rough estimates example power method see golub van loan yield reasonable estimates optimal value 
symmetric successive overrelaxation method assume coefficient matrix symmetric symmetric successive overrelaxation method combines sor sweeps way resulting iteration matrix similar symmetric matrix 
specifically sor sweep carried second sweep unknowns updated reverse order 
forward sor sweep followed backward sor sweep 
similarity iteration matrix symmetric matrix permits application preconditioner iterative schemes symmetric matrices 
primary motivation convergence rate optimal value usually slower convergence rate sor optimal see young page 
details preconditioner see chapter 
matrix terms iteration expressed follows 
note simply iteration matrix sor roles reversed 
pseudocode algorithm 
notes modern treatment iterative methods dates back relaxation method 
precursor sor method order approximations unknowns relaxed varied computation 
specifically unknown chosen estimates location largest error current approximation 
relaxation method considered impractical automated computing 
interesting note multiple instruction multiple data stream mimd parallel computers interest called asynchronous chaotic iterative methods see miranker baudet closely related original relaxation method 
chaotic methods order relaxation unconstrained eliminating costly synchronization processors effect convergence difficult predict 
notion accelerating convergence iterative method extrapolation predates development sor 
overrelaxation accelerate convergence original relaxation method 
ad hoc sor method different chapter 
iterative methods choose initial guess solution 


ai jx 
ai jx bi ai 

ai jx 
ai jx check convergence continue necessary method relaxation factor updating variable impressive results classes problems see ehrlich 
main theory stationary iterative methods varga young young 
reader referred books details concerning methods described section 
nonstationary iterative methods nonstationary methods differ stationary methods computations involve information changes iteration 
typically constants computed inner products residuals vectors arising iterative method 
conjugate gradient method cg conjugate gradient method effective method symmetric positive definite systems 
oldest best known nonstationary methods discussed 
method proceeds generating vector sequences iterates approximations solution residuals 
nonstationary iterative methods compute ax initial guess 
solve mz endif ap ip iq check convergence continue necessary preconditioned conjugate gradient method corresponding iterates search directions updating iterates residuals 
length sequences large small number vectors needs kept memory 
iteration method inner products performed order compute update scalars defined sequences satisfy certain orthogonality conditions 
symmetric positive definite linear system conditions imply distance true solution minimized norm 
iterates updated iteration multiple search direction vector ip correspondingly residuals ax updated choice equation 
search directions updated residuals ap 
ap minimizes possible choices choice ensures ap equivalently orthogonal 
fact show choice orthogonal previous ap respectively 
pseudocode preconditioned conjugate gradient method 
uses preconditioner obtains version conjugate gradient algorithm 
case algorithm may simplified skipping solve line replacing 
theory conjugate gradient method constructs ith iterate element span 
minimized exact chapter 
iterative methods solution ax minimum guaranteed exist general symmetric positive definite 
preconditioned version method uses different subspace constructing iterates satisfies minimization property different subspace 
requires addition preconditioner symmetric positive definite 
minimization error equivalent residuals ax orthogonal 
symmetric orthogonal basis krylov subspace span 
ai constructed term recurrences recurrence suffices generating residuals 
conjugate gradient method coupled term recurrences updates residuals search direction vector updating search direction newly computed residual 
conjugate gradient method quite attractive computationally 
close relationship conjugate gradient method lanczos method determining eigensystems construction orthogonal basis krylov subspace similarity transformation coefficient matrix tridiagonal form 
coefficients computed cg iteration arise lu factorization tridiagonal matrix 
cg iteration reconstruct lanczos process vice versa see paige saunders golub van loan 
relationship exploited obtain relevant information eigensystem preconditioned matrix see 
convergence accurate predictions convergence iterative methods difficult useful bounds obtained 
conjugate gradient method error bounded terms spectral condition number matrix 
recall max min largest smallest eigenvalues symmetric positive definite matrix spectral condition number max min 
exact solution linear system ax symmetric positive definite matrix cg symmetric positive definite preconditioner shown see golub van loan ay 
relation see number iterations reach relative reduction error proportional 
cases practical application error bound straightforward 
example elliptic second order partial differential equations typically give rise coefficient matrices discretization mesh width independent order finite elements differences number space dimensions problem see instance axelsson barker 
preconditioning expect number iterations proportional conjugate gradient method 
results concerning behavior conjugate gradient algorithm obtained 
extremal eigenvalues matrix separated observes socalled superlinear convergence see golub leary convergence rate increases iteration 
phenomenon explained fact cg tends eliminate components error direction eigenvectors associated extremal eigenvalues 
eliminated method proceeds eigenvalues exist system convergence rate depends reduced system smaller condition number analysis see van der van der vorst 
effectiveness preconditioner reducing condition number separating extremal eigenvalues deduced studying approximated eigenvalues related lanczos process 

nonstationary iterative methods implementation conjugate gradient method involves matrix vector product vector updates inner products iteration 
slight computational variants exist structure see reid 
variants cluster inner products favorable property parallel machines discussed 
discussion conjugate gradient method vector shared memory computers see dongarra 
discussions method general parallel architectures see demmel heath van der vorst ortega 
formal presentation cg theoretical properties textbook 
shorter presentations axelsson barker golub van loan 
overview papers published years existence method golub leary 
conjugate gradient method viewed special variant lanczos method see positive definite symmetric systems 
methods variants applied symmetric indefinite systems 
vector sequences conjugate gradient method correspond factorization tridiagonal matrix similar coefficient matrix 
breakdown algorithm occur corresponding zero pivot matrix indefinite 
furthermore indefinite matrices minimization property conjugate gradient method longer defined 
methods variants cg method avoid lu factorization suffer breakdown 
minimizes residual norm 
solves projected system minimize keeps residual orthogonal previous ones 
convergence behavior conjugate gradients indefinite systems analyzed paige parlett van der vorst 
theory positive definite symmetric construct orthogonal basis krylov subspace term recurrence relations 
eliminating search directions equations gives recurrence ar ti ti ti written matrix form ari ri ti ti tridiagonal matrix ti 

chapter 
iterative methods case problem longer defines inner product 
try minimize residual norm obtaining ar 
ri minimizes ax ari ri 
exploit fact di diag ri orthonormal transformation respect current krylov subspace ax di final expression simply seen minimum norm squares problem 
element position ti annihilated simple givens rotation resulting upper system subdiagonal elements having removed previous iteration steps simply solved leads method see paige saunders 
possibility solve system cg method ti upper part ti 
cg rely existence decomposition positive definite 
alternative decompose ti lq decomposition 
leads simple recurrences resulting method known see paige saunders 
cg normal equations methods simplest methods nonsymmetric indefinite systems 
methods systems general complicated conjugate gradient method transforming system symmetric definite applying conjugate gradient method attractive coding simplicity 
theory system linear equations ax nonsymmetric possibly indefinite nonsingular coefficient matrix obvious attempt solution apply conjugate gradient related symmetric positive definite system ax approach easy understand code convergence speed conjugate gradient method depends square condition number original coefficient matrix 
rate convergence cg procedure normal equations may slow 
proposals improve numerical stability method 
best known paige saunders applying lanczos method auxiliary system clever execution scheme delivers factors lu decomposition tridiagonal matrix computed carrying lanczos procedure means improving numerical stability normal equations approach suggested bj 
observation matrix construction iteration coefficients inner product ap leads suggestion inner product replaced ap ap 

nonstationary iterative methods generalized minimal residual gmres generalized minimal residual method extension applicable symmetric systems unsymmetric systems 
generates sequence orthogonal vectors absence symmetry longer done short recurrences previously computed vectors orthogonal sequence retained 
reason restarted versions method 
conjugate gradient method residuals form orthogonal basis space span ar 
gmres basis formed explicitly av 
reader may recognize modified gram schmidt orthogonalization 
applied krylov sequence orthogonalization called arnoldi method 
inner product coefficients stored upper hessenberg matrix 
gmres iterates constructed coefficients yk chosen minimize residual norm ax 
gmres algorithm property residual norm computed iterate having formed 
expensive action forming iterate postponed residual norm deemed small 
pseudocode restarted gmres algorithm preconditioner 
theory generalized minimum residual gmres method designed solve nonsymmetric linear systems see saad schultz 
popular form gmres modified gram schmidt procedure uses restarts control storage requirements 
restarts gmres krylov subspace method converge steps assuming exact arithmetic 
course practical value large storage computational requirements absence restarts prohibitive 
crucial element successful application gmres revolves decision restart choice unfortunately exist examples method convergence takes place nth step 
systems choice fails converge 
saad schultz proven useful results 
particular show coefficient matrix real nearly positive definite reasonable value may selected 
implications choice discussed 
implementation common implementation gmres suggested saad schultz relies modified gram schmidt orthogonalization 
householder transformations relatively costly stable proposed 
householder approach results fold increase convergence may better especially ill conditioned systems see walker 
point view parallelism gram schmidt orthogonalization may chapter 
iterative methods initial guess 
solve ax solve mw av hk hk iv hi hi apply ji hi construct ji acting ith st component st component jis small update quit update scheme update replaces computations compute solution hy upper triangular part hi elements squares sense singular represents components 
accurate approximation quit preconditioned gmres method preferred giving stability better parallelization properties see demmel heath van der vorst 
adopt modified gram schmidt approach 
major drawback gmres amount storage required iteration rises linearly iteration count 
fortunate obtain extremely fast convergence cost rapidly prohibitive 
usual way overcome limitation restarting iteration 
chosen number iterations accumulated data cleared intermediate results initial data iterations 
procedure repeated convergence achieved 
difficulty choosing appropriate value small gmres may slow converge fail converge entirely 
value larger necessary involves excessive uses storage 
unfortunately definite rules governing choice choosing restart matter experience 
discussion gmres vector shared memory computers see dongarra 
nonstationary iterative methods general architectures see demmel heath van der vorst 
biconjugate gradient bicg conjugate gradient method suitable nonsymmetric systems residual vectors orthogonal short recurrences proof see faber 
gmres method retains orthogonality residuals long recurrences cost larger storage demand 
biconjugate gradient method takes approach replacing orthogonal sequence residuals mutually orthogonal sequences price longer providing minimization 
update relations residuals conjugate gradient method augmented biconjugate gradient method relations similar update sequences residuals ia sequences search directions choices ap ensure bi orthogonality relations ap pseudocode preconditioned biconjugate gradient method preconditioner 
convergence theoretical results known convergence bicg 
symmetric positive definite systems method delivers results cg twice cost iteration 
nonsymmetric matrices shown phases process significant reduction norm residual method comparable full gmres terms numbers iterations see freund nachtigal 
practice confirmed observed convergence behavior may quite irregular method may break 
breakdown situation due possible event circumvented called look ahead strategies see parlett taylor liu 
leads complicated codes scope book 
breakdown situation occurs lu decomposition fails see theory subsection repaired decomposition 
done qmr see 
breakdown near breakdown situations satisfactorily avoided restart iteration step immediately near breakdown step 
possibility switch robust possibly expensive method gmres 
implementation bicg requires computing matrix vector product ap transpose product applications product may impossible perform instance matrix formed explicitly regular product operation form instance function call evaluation 
chapter 
iterative methods compute ax initial guess choose example 

solve mz solve method fails endif ap ip iq check convergence continue necessary preconditioned biconjugate gradient method parallel environment matrix vector products theoretically performed simultaneously distributed memory environment extra communication costs associated matrix vector products depending storage scheme duplicate copy matrix alleviate problem cost doubling storage requirements matrix 
care exercised choosing preconditioner similar problems arise solves involving preconditioning matrix 
difficult fair comparison gmres bicg 
gmres really minimizes residual cost increasing keeping residuals orthogonal increasing demands memory space 
bicg minimize residual accuracy comparable gmres cost twice amount matrix vector products iteration step 
generation basis vectors relatively cheap memory requirements modest 
variants bicg proposed increase effectiveness class methods certain circumstances 
variants cgs bi cgstab discussed coming subsections 
quasi minimal residual qmr biconjugate gradient method displays irregular convergence behavior 
implicit lu decomposition reduced tridiagonal system may exist resulting breakdown algorithm 
related algorithm quasi minimal residual method freund nachtigal attempts overcome problems 
main idea algorithm 
nonstationary iterative methods solve reduced tridiagonal system squares sense similar approach followed gmres 
constructed basis krylov subspace bi orthogonal orthogonal gmres obtained solution viewed quasi minimal residual solution explains name 
additionally qmr uses look ahead techniques avoid breakdowns underlying lanczos process robust bicg 
convergence convergence behavior qmr typically smoother bicg 
freund nachtigal quite general error bounds show qmr may expected converge fast gmres 
relation residuals bicg qmr freund nachtigal relation may deduce phases iteration process bicg significant progress qmr arrived approximation hand bicg progress qmr may show slow convergence 
look ahead steps qmr method prevent breakdown cases called breakdown number look ahead steps yield iterate 
implementation pseudocode preconditioned quasi minimal residual method preconditioner 
algorithm follows term recurrence version look ahead freund nachtigal algorithm 
version qmr simpler implement full qmr method look ahead susceptible breakdown underlying lanczos process 
implementational variations scale lanczos vectors term recurrences coupled term recurrences 
decisions usually implications stability efficiency algorithm 
professional implementation qmr look ahead freund nachtigal available netlib see appendix modified algorithm include relatively inexpensive recurrence relation computation residual vector 
requires extra vectors storage vector update operations iteration avoids expending matrix vector product residual calculation 
algorithm modified full preconditioning steps required 
computation residual done convergence test 
uses right post preconditioning cheap upper bound computed iteration avoiding recursions details see freund nachtigal proposition 
upper bound may pessimistic factor 
qmr roughly problems respect vector parallel implementation bicg 
scalar overhead iteration slightly bicg 
cases slightly cheaper bicg method converges irregularly fast little reason avoid qmr 
conjugate gradient squared method cgs bicg residual vector regarded product ith degree polynomial pi 
polynomial satisfies pi pi pi 
chapter 
iterative methods compute ax initial guess solve choose example solve 
method fails zt method fails solve solve endif ap method fails method fails iv solve iw solve method fails ip endif check convergence continue necessary preconditioned quasi minimal residual method look ahead 
nonstationary iterative methods compute ax initial guess choose example 
method fails endif solve solve check convergence continue necessary preconditioned conjugate gradient squared method suggests pi reduces smaller vector advantageous apply contraction operator twice compute equation shows iteration coefficients recovered vectors turns easy find corresponding approximations approach leads conjugate gradient squared method see 
convergence observes speed convergence cgs twice fast bicg agreement observation contraction operator applied twice 
reason contraction operator really reduces initial residual reduce reduced vector pk evidenced highly irregular convergence behavior cgs 
aware fact local corrections current solution may large cancellation effects occur 
may lead accurate solution suggested updated residual see van der vorst 
method tends diverge starting guess close solution 
implementation cgs requires number operations iteration bicg involve computations circumstances computation impractical cgs may attractive 
chapter 
iterative methods compute ax initial guess choose example 
method fails endif solve iv check norm small set solve check convergence continue necessary continuation necessary preconditioned biconjugate gradient stabilized method pseudocode preconditioned conjugate gradient squared method preconditioner 
biconjugate gradient stabilized bi cgstab biconjugate gradient stabilized method bi cgstab developed solve nonsymmetric linear systems avoiding irregular convergence patterns conjugate gradient squared method see van der vorst 
computing cgs sequence bi cgstab computes qi pi qi ith degree polynomial describing steepest descent update 
convergence bi cgstab converges fast cgs faster 
cgs viewed method bicg contraction operator applied twice 
bi cgstab interpreted product bicg repeatedly applied gmres 
locally residual vector minimized leads considerably smoother convergence behavior 
hand local gmres step krylov subspace expanded bi cgstab break 
breakdown situation occur addition breakdown underlying bicg algorithm 
type breakdown may avoided combining bicg methods selecting values see 
nonstationary iterative methods algorithm 
alternative bi cgstab see gutknecht general approaches suggested 
note bi cgstab stopping tests method converged test norm subsequent update numerically questionable 
additionally stopping test saves unnecessary operations minor importance 
implementation bi cgstab requires matrix vector products inner products inner products bicg cgs 
pseudocode preconditioned biconjugate gradient stabilized method preconditioner 
chebyshev iteration chebyshev iteration method solving nonsymmetric problems see golub van loan varga chapter 
chebyshev iteration avoids computation inner products necessary nonstationary methods 
distributed memory architectures inner products bottleneck respect efficiency 
price pays avoiding inner products method requires knowledge spectrum coefficient matrix ellipse enveloping spectrum identified difficulty overcome adaptive construction developed implemented ashby 
chebyshev iteration suitable nonsymmetric linear system enveloping ellipse include origin 
comparison methods comparing pseudocode chebyshev iteration pseudocode conjugate gradient method shows high degree similarity inner products computed chebyshev iteration 
scalars selected define family ellipses common center foci contain ellipse encloses spectrum general field values rate convergence minimal length axis ellipse 
provide code assumed known 
code including adaptive iteration parameters reader referred ashby 
chebyshev method advantage gmres short recurrences 
hand gmres guaranteed generate smallest residual current search space 
bicg methods short recurrences minimize residual suitable norm chebyshev iteration require estimation parameters spectrum 
gmres bicg may effective practice superlinear convergence behavior expected chebyshev 
symmetric positive definite systems ellipse enveloping spectrum degenerates interval min max positive axis min max smallest largest eigenvalues circumstances computation inner products bottleneck may advantageous start cg compute estimates extremal eigenvalues cg coefficients sufficient convergence approximations switch chebyshev iteration 
similar strategy may adopted switch gmres bicg type methods chebyshev iteration 
chapter 
iterative methods convergence compute ax initial guess max min max min 

solve mz endif ip ax 
check convergence continue necessary preconditioned chebyshev method symmetric case preconditioner symmetric chebyshev iteration upper bound conjugate gradient method provided computed min max extremal eigenvalues preconditioned matrix 
severe penalty overestimating underestimating field values 
example symmetric case max underestimated method may diverge overestimated result may slow convergence 
similar statements nonsymmetric case 
implies needs fairly accurate bounds spectrum method effective comparison cg gmres 
implementation chebyshev iteration iteration parameters known soon knows ellipse containing eigenvalues field values operator 
computation inner products necessary methods gmres cg avoided 
avoids synchronization points required cg type methods machines hierarchical distributed memory may achieve higher performance suggests strong parallelization properties discussion see saad dongarra 
specifically soon segment computed may computing sequence corresponding segments pseudocode preconditioned chebyshev method preconditioner 
handles case symmetric positive definite coefficient matrix eigenvalues assumed real interval min max include zero 
computational aspects methods efficient solution linear system largely function proper choice iterative method 
obtain performance consideration computational ker 
computational aspects methods matrix inner vector precond method product saxpy product solve jacobi gs sor cg gmres bicg qmr bc cgs bi cgstab chebyshev table summary operations iteration means multiplications matrix transpose 
method performs real matrix vector product preconditioner solve number operations equivalent matrix vector multiply 
saxpy operations vector scalings 
implementations recursively update residual 
nels method efficiently executed target architecture 
point particular importance parallel architectures see 
iterative methods different direct methods respect 
performance direct methods dense sparse systems largely factorization matrix 
operation absent iterative methods preconditioners may require setup phase iterative methods lack dense matrix suboperations 
operations executed high efficiency current computer architectures expect lower flop rate iterative direct methods 
dongarra van der vorst give experimental results provide benchmark code iterative solvers 
furthermore basic operations iterative methods indirect addressing depending data structure 
operations relatively low efficiency execution 
lower efficiency execution imply total solution time system 
furthermore iterative methods usually simpler implement direct methods full factorization stored handle larger sytems direct methods 
section summarize method matrix properties 
method problem type knowledge matrix properties main criterion selecting iterative method 
computational kernels 
different methods involve different kernels depending problem target computer architecture may rule certain methods 
table lists storage required method preconditioning 
note including original system ax ignore scalar storage 

jacobi method extremely easy matrix strongly diagonally dominant method probably best considered iterative methods preconditioner nonstationary method 
trivial parallelize 
chapter 
iterative methods method storage jacobi matrix sor matrix cg matrix gmres matrix bicg matrix cgs matrix bi cgstab matrix qmr matrix chebyshev matrix table storage requirements methods iteration denotes order matrix 
implementations recursively update residual 

gauss seidel method typically faster convergence jacobi general competitive nonstationary methods 
applicable strictly diagonally dominant symmetric positive definite matrices 
parallelization properties depend structure coefficient matrix 
different orderings unknowns different degrees parallelism multi color orderings may give full parallelism 
special case sor method obtained choosing 
successive relaxation sor accelerates convergence gauss seidel relaxation may yield convergence gauss seidel fails relaxation 
speed convergence depends critically optimal value may estimated spectral radius jacobi iteration matrix certain conditions 
parallelization properties gauss seidel method 

conjugate gradient cg applicable symmetric positive definite systems 
speed convergence depends condition number extremal eigenvalues separated superlinear convergence behavior result 
inner products act synchronization points parallel environment 
parallel properties largely independent coefficient matrix depend strongly structure preconditioner 

generalized minimal residual gmres applicable nonsymmetric matrices 
gmres leads smallest residual fixed number iteration steps steps increasingly expensive 
order limit increasing storage iteration step restarting necessary 
depends right hand side requires skill experience 
gmres requires matrix vector products coefficient matrix 

computational aspects methods number inner products grows linearly iteration number restart point 
implementation simple gram schmidt process inner products independent imply synchronization point 
stable implementation modified gram schmidt orthogonalization synchronization point inner product 

biconjugate gradient bicg applicable nonsymmetric matrices 
requires matrix vector products coefficient matrix transpose 
method cases matrix implicitly operator usually corresponding transpose operator available cases 
parallelization properties similar cg matrix vector products preconditioning steps independent done parallel communication stages packaged 

quasi minimal residual qmr applicable nonsymmetric matrices 
designed avoid irregular convergence behavior bicg avoids breakdown situations bicg 
bicg significant progress iteration step qmr delivers result step 
bicg temporarily diverges qmr may reduce residual albeit slowly 
computational costs iteration similar bicg slightly higher 
method requires transpose matrix vector product 
parallelization properties bicg 

conjugate gradient squared cgs applicable nonsymmetric matrices 
converges diverges typically twice fast bicg 
convergence behavior quite irregular may lead loss accuracy updated residual 
tends diverge starting guess close solution 
computational costs iteration similar bicg method doesn require transpose matrix 
bicg matrix vector products independent number synchronization points parallel environment larger 

biconjugate gradient stabilized bi cgstab applicable nonsymmetric matrices 
computational costs iteration similar bicg cgs method doesn require transpose matrix 
alternative cgs avoids irregular convergence patterns cgs maintaining speed convergence result observe loss accuracy updated residual 

chebyshev iteration applicable nonsymmetric matrices book symmetric case 
chapter 
iterative methods method requires explicit knowledge spectrum field values symmetric case iteration parameters easily obtained extremal eigenvalues estimated directly matrix applying iterations conjugate gradient method 
computational structure similar cg synchronization points 
adaptive chebyshev method combination methods cg gmres continue iteration suitable bounds spectrum obtained methods 
selecting best method class problems largely matter trial error 
depends storage available gmres availability bicg qmr expensive matrix vector products solve steps comparison inner products 
matrix vector products relatively expensive sufficient storage available may attractive gmres delay restarting possible 
table shows type operations performed iteration 
particular problem data structure user may observe particular operation performed efficiently 
short history krylov methods methods orthogonalization developed number authors early 
lanczos method mutually orthogonal vector sequences motivation came eigenvalue problems 
context prominent feature method reduces original matrix tridiagonal form 
lanczos applied method solving linear systems particular symmetric ones 
important property proving convergence method solving linear systems iterates related initial residual multiplication polynomial coefficient matrix 
joint stiefel independent discovery method classical description conjugate gradient method solving linear systems 
error reduction properties proved experiments showing premature convergence reported conjugate gradient method direct method iterative method 
stiefel method closely related reduction lanczos method symmetric matrices reducing mutually orthogonal sequences orthogonal sequence important algorithmic difference 
lanczos term recurrences method stiefel uses coupled term recurrences 
combining recurrences eliminating search directions lanczos method obtained 
arnoldi discusses lanczos method presents new method combining features lanczos stiefel methods 
lanczos method applied nonsymmetric systems search directions 
stiefel method generates self orthogonal sequence 
fact combined asymmetry coefficient matrix means method longer effects reduction tridiagonal form upper hessenberg form 
minimized iterations galerkin method algorithm known arnoldi algorithm 
conjugate gradient method received little attention practical method time partly importance finite termination property 
reid pointed detailed account early history cg methods refer reader golub leary 

survey krylov methods important application area lay sparse definite systems renewed interest method 
methods developed years employ implicitly upper hessenberg matrix arnoldi method 
overview characterization orthogonal projection methods nonsymmetric systems see ashby saad schultz young 
fletcher proposed implementation lanczos method similar conjugate gradient method coupled term recurrences named bi conjugate gradient method bicg 
survey krylov methods research design krylov subspace methods solving nonsymmetric linear systems active field research new methods emerging 
book included best known popular methods particular extensive computational experience gathered 
section shall briefly highlight developments methods treated 
survey methods freund golub nachtigal 
reports meier yang tong extensive numerical comparisons various methods including ones discussed detail book 
suggestions reduce increase memory computational costs gmres 
obvious restart included gmres 
approach restrict gmres search suitable subspace higher dimensional krylov subspace 
methods idea viewed preconditioned gmres methods 
simplest ones exploit fixed polynomial preconditioner see johnson micchelli paul saad nachtigal reichel trefethen 
sophisticated approaches polynomial preconditioner adapted iterations saad preconditioner may iterative method choice van der vorst axelsson 
stagnation prevented method van der vorst including steps phases process 
de part optimality gmres maintained hybrid method iterations preconditioning method kept orthogonal iterations underlying gcr method 
approaches advantages problems far clear priori strategy preferable case 
focused endowing bicg method desirable properties avoiding breakdown avoiding transpose efficient matrix vector products smooth convergence exploiting expended forming krylov space reduction residual 
discussed bicg method kinds breakdown lanczos breakdown underlying lanczos process breaks pivot breakdown tridiagonal matrix implicitly generated underlying lanczos process encounters zero pivot gaussian elimination pivoting factor 
exact breakdowns rare practice near breakdowns cause severe numerical stability problems 
pivot breakdown easier overcome approaches proposed literature 
noted symmetric matrices lanczos breakdown occur possible breakdown pivot breakdown 
qmr methods discussed book circumvent pivot breakdown solving squares systems 
methods tackling problem fletcher saad gutknecht bank chan 
lanczos breakdown difficult eliminate 
considerable attention analyzing nature lanczos breakdown see parlett chapter 
iterative methods gutknecht various look ahead techniques see freund nachtigal parlett nachtigal freund gutknecht nachtigal freund golub nachtigal gutknecht 
resulting algorithms usually complicated give template form codes freund nachtigal available netlib 
possible eliminate breakdowns require look ahead steps arbitrary size breakdowns 
far methods received practical form look ahead may prove crucial component methods 
bicg method need matrix vector multiplies inconvenient doubling number matrix vector multiplies compared cg increase degree underlying krylov subspace 
methods proposed overcome drawback 
notable ingenious cgs method discussed earlier computes square bicg polynomial requiring obviating need bicg converges cgs attractive faster converging alternative 
cgs inherits magnifies breakdown conditions irregular convergence bicg see van der vorst 
cgs generated interest possibility product methods generate iterates corresponding product bicg polynomial polynomial degree chosen certain desirable properties computable recourse bi cgstab method van der vorst example auxiliary polynomial defined local minimization chosen smooth convergence behavior 
gutknecht noted bi cgstab viewed product bicg gmres suggested combining bicg gmres numbered iteration steps 
anticipated lead better convergence case eigenvalues complex 
efficient robust variant approach suggested describe easily combine bicg gmres modest basic methods squared 
example squaring lanczos procedure chan de van der vorst obtained transpose free implementations bicg qmr 
squaring qmr method freund derived transpose free qmr squared method quite competitive cgs smoother convergence 
unfortunately methods require extra matrix vector product step efficient 
addition bi cgstab product methods designed smooth convergence cgs 
idea quasi minimal residual qmr principle obtain smoothed iterates krylov subspace generated product methods 
freund proposed qmr version cgs called 
numerical experiments show cases retains desirable convergence features cgs correcting erratic behavior 
transpose free nature low computational cost smooth convergence behavior attractive alternative cgs 
hand bicg polynomial breaks cgs 
possible remedy combine look ahead lanczos technique appears quite complicated methods kind appeared literature 
chan derived similar qmr version van der vorst bi cgstab method called 
methods offer smoother convergence cgs bi cgstab little additional cost 
clear best krylov subspace method time best krylov subspace method 
methods winner specific problem class main problem identify classes construct new methods uncovered classes 
nachtigal reddy trefethen shows group methods cg bicg gmres cgs class problems method winner loser 
shows clearly ultimate method 
best hope expert system guides user choice 
iterative methods reach robustness direct methods beat direct methods 
survey krylov methods problems 
problems iterative schemes attractive direct methods multigrid 
hope find suitable methods preconditioners classes large problems unable solve known method cpu restrictions memory convergence problems ill conditioning cetera 
chapter 
iterative methods chapter preconditioners convergence rate iterative methods depends spectral properties coefficient matrix 
may attempt transform linear system equivalent sense solution favorable spectral properties 
preconditioner matrix effects transformation 
instance matrix approximates coefficient matrix way transformed system ax solution original system ax spectral properties coefficient matrix may favorable 
devising preconditioner faced choice finding matrix approximates solving system easier solving finding matrix approximates multiplication needed 
majority preconditioners falls category notable example second category discussed 
cost trade preconditioner iterative method incurs extra cost initially setup iteration applying trade cost constructing applying preconditioner gain convergence speed 
certain preconditioners need little construction phase instance preconditioner incomplete factorizations substantial involved 
scalar terms may comparable single iteration construction preconditioner may vectorizable parallelizable application preconditioner case initial cost amortized iterations repeated preconditioner multiple linear systems 
preconditioners take application amount proportional number variables 
implies multiply iteration constant factor 
hand number iterations function matrix size usually improved constant 
certain preconditioners able improve situation notably modified incomplete factorizations preconditioners multigrid techniques 
parallel machines trade efficacy preconditioner classical sense parallel efficiency 
traditional preconditioners large sequential component 
chapter 
preconditioners left right preconditioning transformation linear system practice 
instance matrix symmetric conjugate gradients method immediately applicable system 
method described remedies employing inner product orthogonalization residuals 
theory cg method applicable 
cg type methods book exception qmr derived combination preconditioned iteration matrix correspondingly changed inner product 
way deriving preconditioned conjugate gradients method split preconditioner transform system am symmetric obvious method symmetric iteration matrix conjugate gradients method applied 
remarkably splitting practice needed 
rewriting steps method see instance axelsson barker pgs 
golub van loan usually possible reintroduce computational step solve mu step applies preconditioner entirety 
different approach preconditioning easier derive 
consider system 
am matrices called left right preconditioners respectively apply iterative method system 
additional actions iterative process xn xn necessary 
arrive schematic deriving left right preconditioned iterative method symmetrically preconditioned methods book 

take preconditioned iterative method replace occurence 
remove vectors algorithm duplicates previous step 

replace occurrence method am 
calculation initial residual add step 

method add step final calculated solution 
noted methods reduce algorithms section choices 
jacobi preconditioning jacobi preconditioning simplest preconditioner consists just diagonal matrix ai mi 
known point jacobi preconditioner 
possible preconditioner extra storage matrix 
division operations usually quite costly practice storage allocated reciprocals matrix diagonal 
strategy applies preconditioners 
block jacobi methods block versions jacobi preconditioner derived partitioning variables 
index set 
partitioned si sets si mutually disjoint ai index subset mi 
preconditioner block diagonal matrix 
natural choices partitioning suggest problems multiple physical variables node blocks formed grouping equations node 
structured matrices partial differential equations regular grids partitioning physical domain 
examples partitioning lines case planes case 
discussed 
parallel computers natural partitioning coincide division variables processors 
discussion jacobi preconditioners need little storage block case easy implement 
additionally parallel computers don particular problems 
hand sophisticated preconditioners usually yield larger improvement 
preconditioning preconditioner jacobi preconditioner derived coefficient matrix 
original symmetric matrix decomposed certain conditions show point jacobi algorithm optimal close optimal sense reducing condition number preconditioners diagonal form 
shown forsythe strauss matrices property van der general sparse matrices 
extensions block jacobi preconditioners see demmel 
sor gauss seidel matrices preconditioners technical reason 
optimal maps eigenvalues coefficient matrix circle complex plane see young 
case polynomial acceleration possible accelerating polynomial reduces trivial polynomial pn resulting method simply stationary sor method 
research varga shown polynomial acceleration sor suboptimal yield improvement simple sor optimal 
chapter 
preconditioners diagonal lower upper triangular part matrix defined parametrized optimal value parameter parameter sor method reduce number iterations lower order 
specifically second order problems spectral condition number attainable see axelsson barker 
practice spectral information needed calculate optimal prohibitively expensive compute 
matrix factored form preconditioner shares properties factorization methods see 
instance suitability vector processors parallel architectures depends strongly ordering variables 
hand factorization priori possibility breakdown construction phase incomplete factorization methods 
incomplete factorization preconditioners broad class preconditioners incomplete factorizations coefficient matrix 
call factorization incomplete factorization process certain fill elements nonzero elements factorization positions original matrix zero ignored 
preconditioner factored form lu lower upper triangular 
efficacy preconditioner depends approximates creating incomplete factorization incomplete factorizations preconditioners encountered far non trivial creation stage 
incomplete factorizations may break attempted division zero pivot result indefinite matrices negative pivots full factorization matrix guaranteed exist yield positive definite matrix 
incomplete factorization guaranteed exist factorization strategies original matrix matrix 
originally proved van der vorst see van der vorst 
cases pivots zero negative strategies proposed substituting arbitrary positive number see restarting factorization positive value see 
important consideration incomplete factorization preconditioners cost factorization process 
incomplete factorization exists number operations involved creating solving system coefficient matrix cost may equal iterations iterative method 
parallel computers problem aggravated generally poor parallel efficiency factorization 
factorization costs amortized iterative method takes iterations preconditioner linear systems instance successive time steps newton iterations 
solving system incomplete factorization preconditioner incomplete factorizations various forms 
lu nonsingular triangular matrices solving system proceeds usual way incomplete 
incomplete factorization preconditioners lu 

zi ii yi 
xi ii zi preconditioner solve system mx lu 

zi ii yi 
xi zi ii preconditioner solve system mx 
factorizations diagonal strictly triangular matrices determined factorization process 
case equivalent formulations mx ld case diagonal elements twice times formula lead expect divisions performed storing explicitly practical thing 
cost extra storage store ld saving computation 
solving system formulation outlined 
second formulation slightly harder implement 
point incomplete factorizations common type incomplete factorization set matrix positions keeping positions outside set equal zero factorization 
resulting factorization incomplete sense fill 
set usually chosen encompass positions ai 
position zero exact factorization called fill position outside fill said discarded 
chosen coincide set nonzero positions discarding fill 
factorization type called ilu factorization incomplete lu factorization level zero precise incomplete factorization lu refer positions talk positions factorization 
matrix nonzeros combined 
zero refers fact level zero fill permitted nonzero elements original matrix 
fill levels defined calling element level caused elements level fill level caused original matrix elements 
chapter 
preconditioners nonzero set aij 
set dii aii 
set dii dii 
set djj djj construction ilu incomplete factorization preconditioner storing inverses pivots describe incomplete factorization formally ai ai ka ai kak ai 
van der vorst proved matrix factorization exists choice gives symmetric positive definite matrix symmetric positive definite 
guidelines allowing levels fill van der vorst 
fill strategies major strategies accepting discarding fill structural numerical 
structural strategy accepting fill certain level 
pointed zero location filling say step assigned fill level value fill max fill fill 
aij nonzero level value changed 
numerical fill strategy drop tolerances fill ignored small suitable definition small 
definition sense mathematically harder implement practice amount storage needed factorization easy predict 
see discussions preconditioners drop tolerances 
simple cases ilu ilu ilu method incomplete factorization produces nonzero elements sparsity structure original matrix preconditioner worst takes exactly space store original matrix 
simplified version ilu called ilu needed 
prohibit fill elements alter diagonal elements alterations diagonal elements ignored situation 
splitting coefficient matrix diagonal lower triangular upper triangular parts da la ua preconditioner written la ua diagonal matrix containing pivots generated 
generating preconditioner described 
upper lower triangle matrix unchanged storage space needed 
fact order avoid division operations preconditioner solve stage store graph theoretical terms ilu ilu coincide matrix graph contains triangles 

incomplete factorization preconditioners resulting lower upper factors preconditioner nonzero elements set fact general true preconditioner 
fact ilu preconditioner contains diagonal parts original matrix derive efficient implementation preconditioned cg 
new implementation merges application tridiagonal factors matrix preconditioner saving substantial number operations iteration 
special cases central differences consider special case matrix derived central differences cartesian product grid 
case ilu ilu factorizations coincide remarked calculate pivots factorization elements triangular factors equal diagonal elements assume natural line line ordering grid points 
letting coordinates regular grid easy see pivot grid point determined pivots points 
points grid lines get generating relations pivots ai ai di ai ai ai nd nai ai ai ai ai nd ai kn 
conversely describe factorization algorithmically initially di ai nm di di ai id ai kn di di ai id ai nm assumed variables problem ordered called natural ordering sequential numbering grid lines points grid line 
encounter different orderings variables 
modified incomplete factorizations modification basic idea incomplete factorizations follows product ai ka kak nonzero fill allowed position simply discarding fill quantity subtract diagonal element ai factorization scheme usually called modified incomplete factorization 
mathematically corresponds forcing preconditioner original matrix 
reason considering modified incomplete factorizations behavior spectral condition number preconditioned system 
mentioned second order elliptic equations condition number coefficient matrix function discretization mesh width 
order magnitude preserved simple incomplete factorizations usually reduction large constant factor obtained 
modified factorizations interest combination small perturbations spectral condition number preconditioned system lower order 
proved dupont kendall modified incomplete factorization da gives central difference case 
general proofs gustafsson axelsson barker 
chapter 
preconditioners 
parallel max min ny ni nx ni nx wavefront solution central difference problem domain points 
keeping row sums constant keep column sums constant 
computational fluid mechanics idea justified argument material balance stays constant iterates 
equivalently wishes avoid artificial diffusion 
cheshire observed column sums choice guarantees sum elements material balance error zero ri elements summing zero 
modified incomplete factorizations break especially variables numbered natural row row ordering 
noted chan kuo full analysis 
slight variant modified incomplete factorizations consists class relaxed incomplete factorizations 
fill multiplied parameter subtracted diagonal see grimes axelsson chan stone van der vorst 
dangers presence rounding error see van der vorst 
vectorization preconditioner solve may appear sequential time solving factorization order number variables things quite bad 
consider special case central differences regular domain points 
variables diagonal domain locations depend previous diagonal 
possible process operations diagonal wavefront parallel see vector computer pipeline see van der vorst 
way vectorizing solution triangular factors form expansion inverses factors 
consider moment lower triangular matrix normalized form strictly lower triangular 
inverse series series called neumann expansion second euler expansion 
series finite length prohibits practical fact 
parallel vectorizable preconditioners derived incomplete factorization small number terms series 
experiments indicate small number terms giving high execution rates yields full precision recursive triangular solution see axelsson van der vorst 

incomplete factorization preconditioners 

lt 
ux preconditioning step algorithm neumann expansion incomplete factorization 
practical considerations implementing expansion algorithms 
instance normalization equation la preconditioner described section described da la ua la ua write lad ua 
choose store product lad doing doubles storage requirements matrix doing means separate multiplications la performed expansion 
suppose products lad ua stored 
define replace solving system mx computing algorithm 
parallelizing preconditioner solve algorithms vectorization outlined parallel computers 
instance variables wavefront processed parallel dividing wavefront processors 
radical approaches increasing parallelism incomplete factorizations renumbering problem variables 
instance rectangular domains start numbering variables corners simultaneously creating simultaneous wavefronts fold parallelism see dongarra van der vorst 
extreme case red black ordering general matrices multi color ordering gives absolute minimum number sequential steps 
multi coloring attractive method vector computers 
points color uncoupled processed vector see doi melhem poole ortega 
ordering strategies usually trade degree parallelism resulting number iterations 
reason different ordering may give rise different error matrix particular norm error matrix may vary considerably orderings 
see experimental results duff partial explanation 
chapter 
preconditioners 
block factorization methods xi aii 
yi 
aij aji xj xj block version ilu factorization consider block variants preconditioners accelerated methods 
block methods normally feasible problem domain cartesian product grid case natural division lines planes dimensional case blocking incomplete factorizations effective dimensional case see instance 
blocking scheme cartesian product grids size number blocks increases problem size 
idea block factorizations starting point incomplete block factorization partitioning matrix mentioned 
incomplete factorization performed matrix blocks basic entities see axelsson golub basic 
important difference point methods arises inversion pivot blocks 
inverting scalar easily done block case problems arise 
inverting pivot block costly operation 
second initially diagonal blocks matrix sparse maintain type structure factorization 
need approximations inverses arises 
addition fill diagonal blocks discarded altogether 
describes incomplete block factorization analogous ilu factorization section updates diagonal blocks 
case incomplete point factorizations existence incomplete block methods guaranteed coefficient matrix matrix 
general proof see axelsson 
approximate inverses block factorizations pivot block generally forced sparse typically banded form need approximation inverse similar structure 
furthermore approximation easily computable rule option calculating full inverse banded part 
simplest approximation diagonal matrix reciprocals diagonal di ai possibilities considered axelsson axelsson golub 
particular example 
attractive theoretical property original matrix symmetric positive definite factorization positive diagonal approximation inverse symmetric positive definite 

incomplete factorization preconditioners banded matrix factor algorithm approximating inverse banded matrix yi xi ai ai incomplete block factorization block tridiagonal matrix banded approximations inverse banded matrices theoretical justification 
context partial differential equations diagonal blocks coefficient matrix usually strongly diagonally dominant 
matrices elements inverse size exponentially decreasing distance main diagonal 
see moss smith general proof detailed analysis matrix case 
special case block applications block tridiagonal structure coefficient matrix 
examples problems regular grid blocks correspond lines grid points problems regular grid blocks correspond planes grid points 
block tridiagonal structure arise naturally imposed renumbering variables cuthill mckee ordering 
matrix incomplete block factorizations particularly simple nature fill occur outside diagonal blocks ai properties follow treatment pivot blocks 
generating recurrence pivot blocks takes simple form see 
factorization left sequences xi block forming pivots yi blocks approximating inverses 
types incomplete block factorizations reason block methods interest potentially suitable vector computers parallel architectures 
consider block factorization block diagonal matrix pivot blocks block lower upper triangle factorization coincide la uq case block tridiagonal matrix 
turn incomplete factorization replacing block diagonal matrix pivots block diagonal matrix incomplete factorization pivots diag xi giving writing lad ua equally valid practice harder implement 
chapter 
preconditioners factorizations type covers methods golub solving linear system means solving smaller systems xi matrices 
alternatively replace inverse block diagonal matrix approximations inverses pivots diag yi giving 
second type discussed axelsson axelsson solving system entails multiplying yi blocks 
second type higher potential 
unfortunately factorization theoretically troublesome see 
blocking systems partial differential equations physical problem variables grid point coupled partial differential equations possible introduce blocking natural way 
blocking equations gives small number large blocks axelsson gustafsson equations linear elasticity blocking variables node gives small blocks karlsson semiconductor equations 
systematic comparison approaches bank 
incomplete lq factorizations saad proposes construct incomplete lq factorization general sparse matrix 
idea orthogonalize rows matrix gram schmidt process note sparse matrices rows typically orthogonal standard gram schmidt may bad general 
saad suggest dropping strategies fill produced orthogonalization process 
turns resulting incomplete factor viewed incomplete factor matrix aa experiments show cg process normal equations aa effective relevant problems 
polynomial preconditioners far described preconditioners classes approximate coefficient matrix linear systems preconditioner coefficient matrix easier solve original system 
polynomial preconditioners considered members second class preconditioners direct approximations inverse coefficient matrix 
suppose coefficient matrix linear system normalized form spectral radius 
neumann series write inverse bk approximation may derived truncating infinite series 
iterative methods considering idea applying polynomials coefficient matrix initial residual analytic connections basic method polynomially accelerated 
dubois greenbaum investigated relationship basic method splitting polynomially preconditioned method basic result classical methods steps polynomially preconditioned method exactly equivalent kp steps original method accelerated methods specifically 
preconditioners chebyshev method preconditioned iteration improve number iterations factor gain number times coefficient matrix applied polynomial preconditioning eliminate large fraction inner products update operations may increase efficiency 
define polynomial preconditioner abstractly polynomial pn normalized 
choice best polynomial preconditioner choosing best polynomial minimizes 
choice infinity norm obtain chebyshev polynomials require estimates lower upper bound spectrum estimates may derived conjugate gradient iteration see 
accurate lower bound spectrum may hard obtain johnson micchelli paul saad propose squares polynomials weight functions 
functions require upper bound easily computed instance bound maxi ai see 
experiments comparing chebyshev squares polynomials ashby otto 
application polynomial preconditioning symmetric indefinite problems described ashby 
polynomial chosen transforms system definite 
preconditioners properties differential equation number preconditioners exist derive justification properties underlying partial differential equation 
cover see 
preconditioners usually involve types discussed allow specialized faster solution methods 
preconditioning symmetric part pointed conjugate gradient methods non systems require storage previously calculated vectors 
somewhat remarkable preconditioning symmetric part coefficient matrix leads method need extended storage 
method proposed golub 
solving system symmetric part matrix may easier solving system full matrix 
problem may tackled imposing nested iterative method preconditioner symmetric part 
proved efficiency preconditioner symmetric part carries outer method 
fast solvers applications coefficient matrix symmetric positive definite 
reason usually partial differential operator derived self adjoint coercive bounded see axelsson barker 
follows coefficient matrix relation holds matrix similar differential equation xt ax bx depend matrix size 
importance preconditioner gives iterative method number iterations depend matrix size 
chapter 
preconditioners precondition original matrix derived different pde attractive properties preconditioner 
common choice take matrix separable pde system involving matrix solved various called fast solvers fft methods cyclic reduction generalized marching algorithm see dorr bank bank rose 
simplest example elliptic operator preconditioned poisson operator giving iterative method un un lun 
golub transformation method considered speed convergence 
example original matrix arises ux uy preconditioner formed ux uy extension non self adjoint case considered elman schultz 
fast solvers attractive number operations require slightly higher order number variables 
coupled fact number iterations resulting preconditioned iterative methods independent matrix size methods close optimal 
fast solvers usually applicable physical domain rectangle cartesian product structure 
domain consisting number pieces domain decomposition methods see 
alternating direction implicit methods poisson differential operator split natural way sum operators discretized representations 
observation iterative schemes suitable choices proposed 
alternating direction implicit adi method proposed solution method parabolic equations 
approximations subsequent time steps 
steady state solving elliptic equations 
case subsequent iterates see mitchell 
generalization scheme variable coefficients fourth order elliptic problems relatively straightforward 
method implicit requires systems solutions alternates necessary directions 
attractive practical point view tensor product grids solving system instance matrix entails number uncoupled tridiagonal solutions 
need little storage needed matrix executed parallel vectorize 
theoretical reason adi preconditioners interest shown spectrally equivalent original coefficient matrix 
number iterations bounded independent condition number 
problem data distribution 
vector computers system solution involve large strides columns variables grid stored 
preconditioners contiguously solution involve contiguous data 
stride equals number variables column 
parallel machines efficient solution possible processors arranged px py grid 
solve processor row works independently rows 
inside row processors instance schur complement method 
sufficient network bandwidth essentially reduce time solving subdomain systems plus time interface system 
method close optimal 
chapter 
preconditioners chapter related issues complex systems conjugate gradient methods real symmetric systems applied complex hermitian systems straightforward manner 
non hermitian complex systems distinguish cases 
general coefficient matrix method possible conjugate gradients method normal equations ax split system real complex parts method gmres resulting real nonsymmetric system 
certain practical situations complex system non hermitian symmetric 
complex symmetric systems solved classical conjugate gradient lanczos method short recurrences complex inner product replaced biconjugate gradient method method susceptible breakdown happen 
look ahead strategy remedy cases see freund van der vorst 
stopping criteria iterative method produces sequence vectors converging vector satisfying system ax effective method decide 
stopping criterion 
identify error small 
error longer decreasing decreasing slowly 
limit maximum amount time spent iterating 
user wishing read little possible simple stopping criterion adequate 
user supply quantities tol preferably integer maximum number iterations algorithm permitted perform 
real number norm reasonable order magnitude approximation absolute value largest entry matrix 
real number norm reasonable approximation absolute value largest entry 
chapter 
related issues real number tol measures small user wants residual ax ultimate solution 
way choose tol approximate uncertainty entries relative respectively 
example choosing tol means user considers entries errors range respectively 
algorithm compute accurately inherent uncertainty warrants 
user choose tol greater machine precision 
algorithm repeat compute approximate solution compute residual ax compute 
tol 
note change step step occurs near convergence need recomputed 
available stopping criterion may replaced generally stricter criterion tol case final error bound 
estimate available may stopping criterion tol guarantees relative error computed solution bounded tol 
details stopping criteria ideally magnitudes entries error fall user supplied threshold 
hard estimate directly residual ax readily computed 
rest section describes measure sizes vectors bound terms measure errors vector matrix norms 
common vector norms maxj xj xj xj algorithms may norm bx fixed nonsingular matrix 
corresponding vector norms matrix norms maxj aj maxk aj jk aj bab 
may matrix norm max aa max denotes largest eigenvalue 
henceforth refer mutually machine ieee standard floating point arithmetic single precision double precision 

stopping criteria consistent pair 
form mutually consistent pairs 
norms satisfy triangle inequality ax mutually consistent pairs 
details properties norms see golub van loan 
difference norms dependence dimension 
vector length entries uniformly distributed satisfy grow grow stopping criterion may permitted grow proportional order harder satisfy large approaches bounding inaccuracy computed solution ax call forward error hard estimate directly introduce backward error allows bound forward error 
backward error defined smallest possible value max exact solution denotes general matrix times goes 
backward error may easily computed residual ax show 
provided bound inverse bound forward error terms backward error simple equality ax implies 
stopping criterion form yields upper bound forward error 
may prefer stricter harder estimate bound see 
matrix vector absolute values components 
backward error direct interpretation stopping criterion addition supplying bound forward error 
recall backward error smallest change max problem ax exact solution original data errors previous computations measurements usually worth iterating smaller errors 
example machine precision worth making just rounding entries fit machine creates errors large 
discussion consider stopping criteria properties 
mentioned criterion 
tol 
equivalent asking backward error described satisfy tol tol 
criterion yields forward error bound tol second stopping criterion discussed require may stringent criterion criterion 
tol 
equivalent asking backward error satisfy tol 
difficulty method occur ill conditioned nearly lies null space may difficult method satisfy stopping criterion 
see ill conditioned note criterion yields forward error bound tol chapter 
related issues estimate available just upper bound error falls threshold 
yields third stopping criterion criterion 
tol 
stopping criterion guarantees tol permitting user specify desired relative accuracy tol computed solution drawback criteria usually treat backward errors component equally norms measure entry equally 
example sparse dense loss possibly important structure reflected 
contrast stopping criterion gives option scaling component aj bj differently including possibility insisting entries zero 
cost extra matrix vector multiply criterion 
maxj tol 
user defined matrix nonnegative entries user defined vector nonnegative entries denotes vector absolute values entries criterion satisfied means aj tol ej bj tol fj choosing user vary way backward error measured stopping criterion 
example choosing ej fj stopping criterion essentially criterion 
choosing ej aj fj bj stopping criterion measure componentwise relative backward error smallest relative perturbations component necessary exact solution 
tighter stopping criterion requires things sparsity pattern choices reflect structured uncertainties criterion yields forward error bound matrix absolute values entries mention criterion recommend widely 
mention order explain potential drawbacks dubious criterion 
tol 
commonly criterion disadvantage depending strongly initial solution common choice criterion equivalent criterion may difficult satisfy algorithm 
hand large inaccurate large artificially large means iteration may soon 
criterion yields forward error bound 

stopping criteria readily available possible design iterative algorithm ax directly available case algorithms book 
completeness discuss stopping criteria case 
example ones splits get iterative method nx gx natural residual compute gx ax words residual residual preconditioned system ax case hard interpret backward error original system ax may derive forward error bound 
stopping criterion requires estimate 
case methods splitting 
example implementation preconditioned conjugate gradient algorithm computes implementation book computes 
implementation stopping criterion tol criterion 
may get forward error bound stopping criterion 
estimating bounds error inevitably rely bounds large number problem dependent ways estimate mention 
splitting get iteration nx gx matrix inverse norm need know estimate splitting standard jacobi sor matrix special characteristics property may estimate 
symmetric positive definite chebyshev acceleration adaptation parameters step algorithm estimates largest smallest eigenvalues max min anyway 
symmetric positive definite min 
adaptive estimation done lanczos algorithm see section usually provide estimates largest rightmost smallest leftmost eigenvalues symmetric matrix cost matrix vector multiplies 
general nonsymmetric may apply lanczos method aa fact min aat min 
possible estimate provided willing solve systems linear equations coefficient matrices 
done dense linear system solvers extra cost systems small compared cost lu decomposition see hager higham anderson 
case iterative solvers cost solves may times original linear system 
linear systems coefficient matrix differing right hand sides solved viable method 
approach paragraph lets estimate alternate error bound 
may smaller simpler case rows badly scaled consider case diagonal matrix widely varying diagonal entries 
compute denote diagonal matrix diagonal entries equal entries see demmel duff 
estimated technique paragraph multiplying harder multiplying diagonal matrix 
chapter 
related issues stopping progress longer addition limiting total amount limiting maximum number iterations willing natural consider stopping apparent progress 
methods jacobi sor exhibit nearly monotone linear convergence initial transients easy recognize convergence degrades 
methods conjugate gradient method exhibit plateaus convergence residual norm constant value iterations decreasing principle plateaus see greenbaum depending problem 
methods cgs appear wildly large number steps residual begins decrease convergence may continue erratic step step 
words idea criterion stops progress solution longer form criterion method problem dependent 
accounting floating point errors error bounds discussed section subject floating point errors innocuous deserve discussion 
infinity norm maxj xj requires fewest floating point operations compute overflow cause exceptions xj finite hand computing xj straightforward manner easily overflow lose accuracy underflow true result far overflow underflow thresholds 
reason careful implementation computing danger available subroutine blas expensive computing 
consider computing residual ax forming matrix vector product ax subtracting floating point arithmetic relative precision 
standard error analysis shows error computed bounded typically bounded usually closer 
choose tol criterion criterion may satisfied method 
uncertainty value induces uncertainty error 
refined bound error jth component bounded times jth component 
means uncertainty really bounded 
quantity estimated inexpensively provided solving systems coefficient matrices inexpensive see paragraph 
bounds severe overestimates uncertainty examples exist attainable 
data structures efficiency iterative methods considered previous sections determined primarily performance matrix vector product preconditioner solve storage scheme matrix preconditioner 
iterative methods typically sparse matrices review number sparse storage formats 
storage scheme arises naturally specific application problem 
section review popular sparse matrix formats numerical software packages 
surveying various formats demonstrate matrix vector product incomplete factorization solve formulated sparse matrix formats 
ieee standard floating point arithmetic permits computations nan number symbols 

data structures survey sparse matrix storage formats coefficient matrix sparse large scale linear systems form ax efficiently solved zero elements stored 
sparse storage schemes allocate contiguous storage memory nonzero elements matrix limited number zeros 
course requires scheme knowing elements fit full matrix 
methods storing data see instance saad 
discuss compressed row column storage block compressed row storage diagonal storage jagged diagonal storage skyline storage 
compressed row storage crs compressed row column section storage formats general absolutely assumptions sparsity structure matrix don store unnecessary elements 
hand efficient needing indirect addressing step single scalar operation matrix vector product preconditioner solve 
compressed row storage crs format puts subsequent nonzeros matrix rows contiguous memory locations 
assuming nonsymmetric sparse matrix create vectors floating point numbers val integers col ind row ptr 
val vector stores values nonzero elements matrix traversed row wise fashion 
col ind vector stores column indexes elements val vector 
val ai col ind row ptr vector stores locations val vector start row val ai row ptr row ptr 
convention define row ptr nnz nnz number nonzeros matrix storage savings approach significant 
storing elements need nnz storage locations 
example consider nonsymmetric matrix defined 
crs format matrix specified arrays val col ind row ptr val col ind row ptr matrix symmetric need store upper lower triangular portion matrix 
trade complicated algorithm somewhat different pattern data access 
compressed column storage ccs analogous compressed row storage compressed column storage ccs called boeing sparse matrix format 
ccs format identical crs format columns stored traversed rows 
words ccs format crs format ccs format specified arrays val row ind col ptr row ind stores row indices nonzero col ptr stores index elements val start column ccs format matrix chapter 
related issues val row ind block compressed row storage col ptr sparse matrix comprised square dense blocks nonzeros regular pattern modify crs ccs format exploit block patterns 
block matrices typically arise discretization partial differential equations degrees freedom associated grid point 
partition matrix small blocks size equal number degrees freedom treat block dense matrix may zeros 
nb dimension block number nonzero blocks matrix total storage needed nnz block dimension nd defined nd nb 
similar crs format require arrays format rectangular array floating point numbers val nb nb stores nonzero blocks block row wise fashion integer array col ind stores actual column indices original matrix elements nonzero blocks pointer array row blk nd entries point block row val col ind 
savings storage locations reduction indirect addressing crs significant matrices large nb 
compressed diagonal storage cds matrix banded bandwidth fairly constant row row worthwhile take advantage structure storage scheme storing matrix consecutive locations 
eliminate vector identifying column row pack nonzero elements way matrix vector product efficient 
storage scheme particularly useful matrix arises finite element finite difference discretization tensor product grid 
say matrix ai banded nonnegative constants called left right ai case allocate matrix array val 
declaration reversed dimensions corresponds linpack band format cds allow efficiently vectorizable matrix vector multiplication small 
usually band formats involve storing zeros 
cds format may contain array elements correspond matrix elements 
consider nonsymmetric matrix defined 
cds format store matrix array dimension mapping val ai 
rows val array 
data structures val val val notice zeros corresponding non existing matrix elements 
generalization cds format suitable manipulating general sparse matrices vector supercomputers discussed melhem 
variant cds uses stripe data structure store matrix structure efficient storage case varying bandwidth matrix vector product slightly expensive involves gather operation 
defined stripe matrix set positions 
strictly increasing function 
specifically 
computing matrix vector product ax stripes element stripe sk multiplied xi products accumulated yi respectively 
nonsymmetric matrix defined stripes matrix stored rows val array jagged diagonal storage jds val val val val jagged diagonal storage format useful implementation iterative methods parallel vector processors see saad 
compressed diagonal format gives vector length essentially size matrix 
space efficient cds cost gather scatter operation 
simplified form jds called storage purdue storage described follows 
matrix elements shifted left columns stored consecutively 
rows padded zeros right give equal length 
corresponding array matrix elements val array column indices col ind stored val val val val chapter 
related issues col ind col ind col ind col ind clear padding zeros structure may disadvantage especially bandwidth matrix varies strongly 
crs format reorder rows matrix number nonzeros row 
compressed permuted diagonals stored linear array 
new data structure called jagged diagonals 
number jagged diagonals equal number nonzeros row largest number nonzeros row data structure represent matrix consists permutation array perm reorders rows floating point array containing jagged diagonals succession integer array col ind containing corresponding column indices indices pointer array jd ptr elements point jagged diagonal 
advantages jds matrix multiplications discussed saad 
jds format matrix linear arrays perm col ind jd ptr jagged diagonals separated semicolons col ind skyline storage sks perm jd ptr final storage scheme consider skyline matrices called variable band profile matrices see duff reid 
importance direct solution methods handling diagonal blocks block matrix factorization methods 
major advantage solving linear systems having skyline coefficient matrices pivoting necessary skyline structure preserved gaussian elimination 
matrix symmetric store lower triangular part 
straightforward approach storing elements skyline matrix place rows order floating point array val keep integer array row ptr elements point row 
column indices nonzeros stored val easily derived stored 
nonsymmetric skyline matrix illustrated store lower triangular elements sks format store upper triangular elements column oriented sks format transpose stored row wise sks format 
separated substructures linked variety ways 
approach discussed saad store row lower triangular part column upper triangular part contiguously floatingpoint array val 
additional pointer needed determine diagonal elements separate lower triangular elements upper triangular elements located 
matrix vector products iterative methods discussed earlier product matrix transpose times vector needed input vector want compute products ax algorithms storage formats crs cds 

data structures profile nonsymmetric skyline variable band matrix 
crs matrix vector product matrix vector product ax crs format expressed usual way yi ai jxj traverses rows matrix matrix matrix vector multiplication row ptr row ptr val col ind method multiplies nonzero matrix entries operation count times number nonzero elements significant savings dense operation requirement transpose product equation yi jxj aj implies traversing columns matrix extremely inefficient operation matrices stored crs format 
switch indices yi yi aj 
matrix vector multiplication involving chapter 
related issues row ptr row ptr col ind col ind val matrix vector products largely structure indirect addressing 
properties computer 
product ax favorable memory access pattern iteration outer loop reads vectors data row matrix input vector writes scalar 
transpose product hand reads element input vector row matrix reads writes result vector machine methods implemented separate memory paths cray mp memory traffic limit performance 
important consideration risc architectures 
cds matrix vector product matrix stored cds format possible perform matrix vector product ax rows columns take advantage cds format 
idea change coordinates doubly nested loop 
replacing get yi yi ai jxj yi yi ai jxi index inner loop see expression ai accesses jth diagonal matrix main diagonal number 
algorithm doubly nested loop outer loop enumerating diagonals diag nonnegative numbers diagonals left right main diagonal 
bounds inner loop follow requirement algorithm diag diag left diag right loc max diag min diag loc loc val loc diag loc diag transpose matrix vector product minor variation algorithm 
update formula obtain yi yi ai yi ai jxi diag diag right diag left loc max diag min diag loc loc val loc diag diag loc diag 
data structures memory access cds matrix vector product ax vectors inner iteration 
hand indirect addressing algorithm vectorizable vector lengths essentially matrix order regular data access machines perform algorithm efficiently keeping base registers simple offset addressing 
sparse incomplete factorizations efficient preconditioners iterative methods performing incomplete factorization coefficient matrix 
section discuss incomplete factorization matrix stored crs format routines solve system factorization 
consider factorization ilu type simplest type factorization fill allowed matrix nonzero fill position see section 
consider factorizations allow higher levels fill 
factorizations considerably complicated code essential complicated differential equations 
solution routines applicable cases 
iterative methods qmr involve transpose matrix vector product need consider solving system transpose factorization 
generating crs ilu incomplete factorization subsection consider matrix split da la ua diagonal lower upper triangular part incomplete factorization preconditioner form da la da ua 
way need store diagonal matrix containing pivots factorization 
suffices allocate preconditioner pivot array length pivots 
fact store inverses pivots pivots 
implies system solution divisions performed 
additionally assume extra integer array diag ptr allocated contains column row indices diagonal elements row val diag ptr ai factorization begins copying matrix diagonal pivots val diag ptr elimination step starts inverting pivot pivots pivots nonzero elements ai check aj nonzero matrix element element cause fill ai diag ptr row ptr false row ptr col ind diag ptr col ind col ind true element val endif chapter 
related issues update aj true val diag ptr col ind val diag ptr col ind element pivots val crs factorization solve system solved usual manner introducing temporary vector lz uy choice equivalent ways solving system lu la ua lad ua la ua lad ua fourth formulae suitable require multiplication division difference second third ease coding 
section third formula section second transpose system solution 
halves solution largely structure matrix vector multiplication 
sum row ptr diag ptr sum sum val col ind pivots sum step sum diag row ptr sum sum val col ind pivots sum temporary vector eliminated reusing space algorithmically overwrite overwriting input data general recommended 
crs factorization transpose solve solving transpose system lu slightly involved 
usual formulation traverse rows solving factored system access columns matrices prohibitive cost 
key idea distribute newly computed component triangular solve immediately remaining right hand side 
instance write lower triangular matrix 
system ly written 
computing modify 
upper triangular systems treated similar manner 
algorithm access columns triangular systems 
solving transpose system matrix stored crs format essentially means access rows algorithm 
data structures tmp tmp tmp pivots diag ptr row ptr tmp col ind tmp col ind tmp val step pivots row ptr diag ptr col ind col ind val extra temporary tmp clarity overlapped tmp considered equivalent crs preconditioner solve uses short vector lengths indirect addressing essentially memory traffic patterns matrix vector product 
generating crs ilu incomplete factorization incomplete factorizations levels fill allowed accurate ilu factorization described 
hand require storage considerably harder implement section algorithms full factorization sparse matrix duff reid 
preliminary need algorithm adding vectors stored sparse storage 
lx number nonzero components stored integer array xi 
similarly stored ly 
add copying full vector adding total number operations lx ly copy ly add nonzero lx add creating new components zero ly lx lx counting initial zeroing array 
chapter 
related issues lx lx endif order add sequence vectors add vectors executing writes different implementation possible allocated sparse vector sparsity pattern constructed additions 
discuss possibility 
slight refinement algorithm add levels nonzero components assume integer vectors length lx ly respectively full length level vector corresponding addition algorithm copy ly add nonzero don change levels lx add creating new components zero carry levels ly lx lx lx lx lx endif describe ilu factorization 
algorithm starts matrix gradually builds factorization form stored lower triangle diagonal upper triangle array respectively 
particular form factorization chosen minimize number times full vector copied back sparse form 
specifically sparse form factorization scheme row oriented version traditional left looking factorization algorithm 
describe incomplete factorization controls fill levels see equation 
alternatively drop tolerance section attractive point implementation 
fill levels perform factorization symbolically determining storage demands reusing information number linear systems sparsity structure 
preprocessing reuse information possible fill controlled drop tolerance criterium 

parallelism matrix arrays assumed compressed row storage particular ordering elements inside row arrays point locations diagonal elements 
row go elements row col col row copy row row dense vector col row row col row col multiply row col subtract result allowing fill level endif insert row row invert pivot row row normalize row col row row mind col row col col row structure particular sparse matrix apply sequence problems instance different time steps newton iteration 
may pay perform incomplete factorization symbolically determine amount location fill structure numerically different structurally identical matrices 
case array numerical values store levels symbolic factorization phase 
parallelism section discuss aspects parallelism iterative methods discussed book 
iterative methods share computational kernels discuss independent method 
basic time consuming kernels iterative schemes inner products vector updates matrix vector products ap methods preconditioner solves 
examine turn 
conclude section discussing particular issues computational wavefronts sor method block operations gmres method 
inner products computation inner product vectors easily parallelized processor computes inner product corresponding segments vector local inner products lips 
distributed memory machines lips sent processors combined global inner product 
done send chapter 
related issues processor performs summation lips global accumulation processor followed broadcast final result 
clearly step requires communication 
shared memory machines accumulation lips implemented critical section processors add local result turn global result piece serial code processor performs summations 
overlapping communication computation clearly usual formulation conjugate gradient type methods inner products induce synchronization processors progress final result computed updating completing inner product distributed memory machine communication needed inner product overlap communication useful computation 
observation applies updating completing inner product 
shows variant cg communication time may overlapped useful computations 
just reorganized version original cg scheme precisely stable 
advantage approaches see additional operations required 
rearrangement tricks 
updating iterate delayed mask communication stage ap inner product 
second trick relies splitting symmetric preconditioner llt computes inner product computed st computation mask communication stage inner product 
initial guess ax ap iq small ip quit endif rearrangement conjugate gradient parallelism assumptions cg efficiently parallelized follows 
parallelism 
communication required reduction inner product overlapped update fact done previous iteration step 

reduction inner product overlapped remaining part preconditioning operation iteration 

computation segment followed immediately computation segment followed computation part inner product 
saves load operations segments detailed discussion see demmel heath van der vorst 
algorithm extended trivially preconditioners ldl form nonsymmetric preconditioners biconjugate gradient method 
fewer synchronization points authors ways eliminate synchronization points induced inner products methods cg 
strategy replace inner products typically conjugate gradient methods way inner products performed simultaneously 
global communication packaged 
method proposed saad modification improve stability suggested 
related methods proposed gear azevedo 
schemes applied nonsymmetric methods bicg 
stability methods discussed azevedo 
approach generate number successive krylov vectors see orthogonalize block see van gear 
vector updates vector updates trivially parallelizable processor updates segment 
matrix vector products matrix vector products easily parallelized shared memory machines splitting matrix strips corresponding vector segments 
processor computes matrix vector product strip 
distributed memory machines may problem processor segment vector memory 
depending bandwidth matrix may need communication elements vector may lead communication bottlenecks 
sparse matrix problems arise network nearby nodes connected 
example matrices stemming finite difference finite element problems typically involve local connections matrix element ai nonzero variables physically close 
case natural subdivide network grid suitable blocks distribute processors 
computing api processor requires values pi nodes neighboring blocks 
number connections neighboring blocks small compared number internal nodes communication time overlapped computational 
detailed discussions implementation aspects distributed memory systems see de 
preconditioning preconditioning problematic part parallelizing iterative method 
mention number approaches obtaining parallelism preconditioning 
chapter 
related issues discovering parallelism sequential preconditioners 
certain preconditioners developed parallelism mind executed parallel 
examples domain decomposition methods see provide high degree coarse grained parallelism polynomial preconditioners see parallelism matrix vector product 
incomplete factorization preconditioners usually harder parallelize wavefronts independent computations see instance di modest amount parallelism attained implementation complicated 
instance central difference discretization regular grids gives wavefronts hyperplanes see van der vorst 
parallel variants sequential preconditioners 
variants existing sequential incomplete factorization preconditioners higher degree parallelism devised efficient purely scalar terms ancestors 
examples reorderings variables see duff expansion factors truncated neumann series see van der vorst various block factorization methods see axelsson axelsson preconditioners 
preconditioners optimal parallelism incomplete factorization methods minimal number sequential steps equals color number matrix graphs 
theory parallelism see jones 
fully decoupled preconditioners 
processors execute part preconditioner solve communication method technically block jacobi preconditioner see 
parallel execution efficient may effective complicated parallel preconditioners improvement number iterations may modest 
get bigger improvement retaining efficient parallel execution di robert suggest construct incomplete decompositions slightly overlapping domains 
requires communication similar matrix vector products 
wavefronts gauss seidel conjugate gradient methods sight gauss seidel method sor method basic structure fully sequential method 
careful analysis reveals high degree parallelism method applied sparse matrices arising discretized partial differential equations 
start partitioning unknowns wavefronts 
wavefront contains unknowns directed graph predecessor subsequent wavefronts sets definition necessarily unique successors elements previous wavefront successor predecessor relations hold elements set 
clear elements wavefront processed simultaneously sequential time solving system reduced number wavefronts 
observe unknowns wavefront computed soon wavefronts containing predecessors computed 
absence tests convergence components iterations computed simultaneously 
adams jordan observe way natural ordering unknowns gives iterative method mathematically equivalent multi color ordering 
multi color ordering wavefronts color processed simultaneously 
reduces number sequential steps solving gauss seidel matrix number colors smallest number wavefront contains elements predecessor element wavefront 
parallelism demonstrated leary sor theory holds approximate sense multicolored matrices 
observation gauss seidel method natural ordering equivalent extended method wavefront incomplete factorization preconditioners conjugate gradient method 
fact tests duff analysis show incomplete factorization preconditioners general may take considerably larger number iterations converge preconditioners natural ordering 
offset increased parallelism depends application computer architecture 
blocked operations gmres method addition usual matrix vector product inner products vector updates preconditioned gmres method see kernel new vector av orthogonalized previously built orthogonal set 

version done level blas may quite inefficient 
incorporate level blas apply householder orthogonalization classical gram schmidt twice mitigates classical gram schmidt potential instability see saad 
approaches significantly increase computational classical gram schmidt advantage inner products performed simultaneously communication packaged 
may increase efficiency computation significantly 
way obtain parallelism data locality generate basis av krylov subspace orthogonalize set called step gmres see kim 
compare gmres method new vector immediately orthogonalized previous vectors 
approach increase computational contrast cg numerical instability due generating possibly near dependent set necessarily drawback 
chapter 
related issues chapter remaining topics lanczos connection discussed paige saunders golub van loan straightforward derive conjugate gradient method solving symmetric positive definite linear systems lanczos algorithm solving symmetric eigensystems vice versa 
example consider derive lanczos process symmetric eigensystems conjugate gradient method 
suppose define matrix rk 
upper matrix bk bk 

sequences defined standard conjugate gradient algorithm discussed 
equations 
rk pk 

assuming elements sequence conjugate follows tk ark kbk tridiagonal matrix ap ap 
ap chapter 
remaining topics span 
span 
elements mutually orthogonal shown columns matrix qk rk form orthonormal basis subspace span ab 
diagonal matrix ith diagonal element 
columns matrix qk lanczos vectors see parlett associated projection tridiagonal matrix tk kbk 
extremal eigenvalues tk approximate matrix diagonal subdiagonal elements tk readily available iterations conjugate gradient algorithm construct tk cg iterations 
allows obtain approximations extremal eigenvalues condition number matrix generating approximations solution linear system ax nonsymmetric matrix equivalent nonsymmetric lanczos algorithm see lanczos produce nonsymmetric matrix tk extremal eigenvalues may include complex conjugate pairs approximate nonsymmetric lanczos method equivalent bicg method discussed 
block step iterative methods methods discussed far subspace methods iteration extend dimension subspace generated 
fact generate orthogonal basis subspace newly generated vector respect previous basis vectors 
case nonsymmetric coefficient matrices newly generated vector may linearly dependent existing basis 
prevent break severe numerical error instances methods proposed perform look ahead step see freund gutknecht nachtigal parlett taylor liu freund nachtigal 
new basis vectors generated orthogonalized respect subspace generated 
generating basis method generates series low dimensional orthogonal subspaces 
step iterative methods gear strategy generating vectors processing block reduce computational overhead improve processor cache behaviour 
conjugate gradient methods considered generate factorization tridiagonal reduction original matrix look ahead methods generate block factorization block tridiagonal reduction matrix 
block tridiagonal reduction effected block lanczos algorithm block conjugate gradient method see leary 
methods operate multiple linear systems coefficient matrix simultaneously instance multiple right hand sides right hand side different initial guesses 
block methods multiple search directions step convergence behavior better ordinary methods 
fact show spectrum matrix effectively reduced nb smallest eigenvalues nb block size 
reduced system preconditioning seen earlier suitable preconditioner cg matrix system ax requires fewer iterations solve ax systems mz solved efficiently 
property independent machine second highly machine dependent 
choosing best preconditioner involves balancing criteria way 
domain decomposition methods minimizes computation time 
balancing approach matrices arising point finite difference discretization second order elliptic partial differential equations pdes dirichlet boundary conditions involves solving reduced system 
specifically grid point red black ordering nodes get dr xr fr ax db xb fb dr db diagonal structured sparse matrix nonzero diagonals nonzero diagonals odd 
applying step block gaussian elimination computing schur complement see golub van loan dr reduces dr db db dr xr xr xb xb fr fb dr fr fr proper scaling left right multiplication db obtain second block equation reduced system cd xb fb ct fr 
linear system order order odd determined solution easily retrieved values black points obtained red black ordered preconditioner full system expect faster convergence 
number nonzero coefficients reduced coefficient matrix nonzero diagonals 
higher density offers data locality 
meier sameh demonstrate reduced system approach hierarchical memory machines alliant fx times faster cg poisson equation grids 
dimensional elliptic pdes reduced system approach yields block tridiagonal matrix having diagonal blocks structure dimensional case blocks diagonal matrices 
computing reduced system explicitly leads unreasonable increase computational complexity solving ax matrix products required solve performed implicitly significantly decrease performance 
meier sameh show reduced system approach times fast conjugate gradient method jacobi preconditioning dimensional problems 
domain decomposition methods years attention domain decomposition methods linear elliptic problems partitioning domain physical problem 
subdomains handled independently methods attractive coarse grain parallel computers 
hand stressed effective sequential computers 
brief survey shall restrict standard second order self adjoint scalar elliptic problems dimensions form fb chapter 
remaining topics positive function domain boundary value prescribed dirichlet problem 
general problems set reader referred series proceedings 
discretization shall assume simplicity triangulated set th nonoverlapping coarse triangles subdomains nh internal vertices 
turn refined set smaller triangles th internal vertices total 
denote coarse fine mesh size respectively 
standard ritz galerkin method piecewise linear triangular basis elements obtain symmetric positive definite linear system au generally kinds approaches depending subdomains overlap schwarz methods separated interfaces schur complement methods iterative 
shall domain decomposition methods preconditioners linear system au reduced schur complement system gb defined interfaces nonoverlapping formulation 
standard krylov subspace methods discussed book user supply procedure computing av sw algorithms described computes computation av simple sparse matrix vector multiply sw may require subdomain solves described 
overlapping subdomain methods approach substructure extended larger substructure containing internal vertices triangles th distance refers amount overlap 
ah denote discretizations subdomain triangulation coarse triangulation th respectively 
rt denote extension operator extends zero function th ri corresponding pointwise restriction operator 
similarly rt denote interpolation operator maps function coarse grid th fine grid th piecewise linear interpolation rh corresponding weighted restriction operator 
notations additive schwarz preconditioner kas system au compactly described ha riv 
note right hand side computed subdomain solves plus coarse grid solve ah computed parallel 
term riv evaluated steps restriction vi riv subdomain solves wi wi vi interpolation yi wi 
coarse grid solve handled manner 
theory shows condition number bounded independently coarse grid size fine grid size provided sufficient overlap essentially means ratio distance boundary uniformly bounded 
coarse grid solve term left condition number grows reflecting lack global coupling provided coarse grid 
purpose implementations useful interpret definition kas matrix notation 
decomposition corresponds partitioning components vector overlapping groups index sets ii components 
matrix simply principal submatrix corresponding index set ii 
rt matrix defined action vector ui defined rt ui ui ii zero 
similarly action transpose forms vector picking 
domain decomposition methods components corresponding ii 
analogously rt nh matrix entries corresponding piecewise linear interpolation transpose interpreted weighted restriction matrix 
th obtained th nested refinement action rh rt efficiently computed standard multigrid algorithm 
note matrices rt ri rt rh defined actions need stored explicitly 
note algebraic formulation preconditioner kas extended matrix necessarily arising discretization elliptic problem 
defined 
furthermore positive partitioning index sets ii matrices ri definite guaranteed nonsingular 
difficulty defining coarse grid matrices ah rh inherently depends knowledge grid structure 
part preconditioner left expense deteriorating convergence rate increases 
robert experimented algebraic overlapping block jacobi preconditioner 
non overlapping subdomain methods easiest way describe approach matrix notation 
set vertices th divided groups 
set interior vertices set vertices lies boundaries coarse triangles th 
shall re order ui ub fi fb corresponding partition 
ordering equation written follows ai aib ui ib ab ub fi fb 
note subdomains uncoupled boundary vertices ai ai block diagonal block ai stiffness matrix corresponding unknowns belonging interior vertices subdomain block lu factorization system written ai aib ui ib sb ub fi fb sb ab iba aib schur complement ab eliminating ui arrive equation ub gb fb fi 
note properties schur complement system 
sb inherits symmetric positive definiteness 
sb dense general computing explicitly requires solves subdomain points edges 

condition number sb improvement growth 
vector vb defined boundary vertices th matrix vector product computed ib involves independent subdomain solves 
right hand side gb computed independent subdomain solves 
chapter 
remaining topics properties possible apply preconditioned iterative method basic method nonoverlapping approach 
need preconditioners improve convergence schur system 
shall describe bramble schatz preconditioner 
need decompose non overlapping index sets vh vh vk denote set nodes corresponding vertices vk th ei denote set nodes edges ei coarse triangles th excluding vertices belonging vh 
addition coarse grid interpolation restriction operators rh rt defined shall need new set interpolation restriction operators edges ei rei pointwise restriction operator nei matrix nei number vertices edge ei edge ei defined action ub ei zero 
action transpose extends zero function defined ei defined corresponding partition written block form se sev 
sb ev sv block se block partitioned subblocks zero 
diagonal blocks sei se principal submatrices corresponding ei 
sei represents coupling nodes interface ei separating neighboring subdomains 
defining preconditioner action ei needed 
noted general sei dense matrix expensive compute expensive compute action need compute inverse cholesky factorization 
fortunately efficiently invertible approximations sei proposed literature see keyes gropp shall called interface preconditioners sei 
mention specific preconditioner mei eik nei nei dimensional laplacian matrix tridiagonal matrix main diagonal diagonals ei taken average coefficient edge ei 
note eigen decomposition known computable fast sine transform action mei efficiently computed 
notations bramble schatz preconditioner defined action vector vb defined follows bp ha ei 
analogous additive schwarz preconditioner computation term consists steps restriction inversion interpolation independent carried parallel 
bramble schatz prove condition number bp ssb bounded log 
practice slight growth number iterations small fine grid refined large coarse grid coarser 
log growth due coupling unknowns edges incident common vertex vk accounted smith proposed vertex space modification explicitly accounts coupling eliminates dependence idea introduce subsets called vertex spaces xk 
domain decomposition methods xk consisting small set vertices edges incident vertex vk adjacent 
note overlaps vh 
principal submatrix sb corresponding xk rt corresponding restriction pointwise extension zero xk ces 
dense expensive compute factor solve efficiently approximations variants operator defined developed literature see chan shao 
shall preconditioner smith vertex space preconditioner defined vb ha xk ei 
smith proved condition number sb bounded independent provided sufficient overlap xk remarks multiplicative schwarz methods mentioned additive schwarz preconditioner viewed overlapping block jacobi preconditioner 
analogously define multiplicative schwarz preconditioner corresponds symmetric block gauss seidel version 
solves subdomain performed sequentially current iterates boundary conditions neighboring subdomains 
conjugate gradient acceleration multiplicative method take fewer iterations additive version 
multiplicative version parallelizable degree parallelism increased trading convergence rate multi coloring subdomains 
theory bramble 
inexact solves exact solves involving kas kv replaced inexact solves standard elliptic preconditioners multigrid ilu 
schwarz methods modification straightforward inexact solve additive schwarz preconditioner simply riv 
schur complement methods require changes accommodate inexact solves 
replacing definitions kv easily obtain inexact pre kv sb 
main difficulty evaluation product requires exact subdomain solves way get inner iteration preconditioner ai order compute action alternative perform iteration larger system construct preconditioner factorization replacing terms ai sb sb respectively sb kv care taken scale close ah ai possible respectively sufficient condition number ah ai close unity scaling coupling matrix aib may wrong 
chapter 
remaining topics nonsymmetric problems preconditioners extend naturally nonsymmetric convection diffusion problems nonsymmetric part large 
nice theoretical convergence rates retained provided coarse grid size chosen small depending size nonsymmetric part see cai 
practical implementations especially parallelism nonsymmetric domain decomposition methods discussed 
choice coarse grid size observed empirically see gropp keyes exists optimal value minimizes total computational time solving problem 
small provides better expensive coarse grid approximation requires solving smaller subdomain solves 
large opposite effect 
model problems optimal determined sequential parallel implementations see chan shao 
practice may pay determine near optimal value empirically preconditioner re times 
may geometric constraints range values take 
multigrid methods simple iterative methods jacobi method tend damp high frequency components error fastest see 
led people develop methods heuristic 
perform steps basic method order smooth error 

restrict current state problem subset grid points called coarse grid solve resulting projected problem 

interpolate coarse grid solution back original grid perform number steps basic method 
steps called pre smoothing post smoothing respectively applying method recursively step true multigrid method 
usually generation subsequently coarser grids halted point number variables small direct solution linear system feasible 
method outlined said cycle method descends sequence subsequently coarser grids sequence reverse order 
cycle method results visiting coarse grid twice possibly smoothing steps 
analysis multigrid methods relatively straightforward case simple differential operators poisson operator tensor product grids 
case coarse grid taken double grid spacing previous grid 
dimensions coarse grid quarter number points corresponding fine grid 
coarse grid tensor product grid fourier analysis see instance briggs 
general case self adjoint elliptic operators arbitrary domains sophisticated analysis needed see mccormick 
multigrid methods shown optimal number operations involved proportional number variables 
description clear iterative methods play role multigrid theory smoothers see 
conversely multigrid methods preconditioners 
row projection methods iterative methods 
basic idea partition matrix grid structure variables second block row corresponding coarse grid nodes 
matrix grid incomplete version schur complement coarse grid typically formed red black cyclic reduction ordering see instance elman 
multigrid preconditioners try obtain optimality results similar full multigrid method 
merely supply pointers literature axelsson axelsson mccormick thomas 
row projection methods iterative methods depend spectral properties coefficient matrix instance require eigenvalues right half plane 
class methods limitation row projection methods see bj sameh 
block row partitioning coefficient matrix 
am iterative application orthogonal projectors pix ai ai methods parallel properties robust handling nonsymmetric indefinite problems 
row projection methods preconditioners conjugate gradient method 
case theoretical connection conjugate gradient method normal equations see 
chapter 
remaining topics appendix obtaining software large body numerical software freely available hours day electronic service called netlib 
addition template material dozens libraries technical reports various parallel computers software test data facilities automatically translate fortran programs bibliographies names addresses scientists mathematicians 
communicate netlib number ways email easily window interface called anonymous ftp netlib cs utk edu 
get started netlib sends message form send index netlib ornl gov description entire library sent minutes providing intervening networks netlib server 
fortran versions templates method described book available netlib 
user sends request electronic mail follows mail netlib ornl gov subject line body single multiple requests line may follows send index send shar request results return mail message containing index library brief descriptions contents 
second request results return mail message consisting shar file containing single precision fortran routines readme file 
versions templates available listed table shar filename contents shar single precision routines shar double precision routines shar single precision fortran routines shar double precision fortran routines shar matlab routines save mail message file called templates shar 
edit mail header file delete lines including cut 
directory containing shar file type appendix obtaining software sh templates shar subdirectory created 
unpacked files stay current directory 
method written separate subroutine file named method cg gmres 
argument parameters exception required matrix vector products preconditioner solvers require transpose matrix 
amount workspace needed varies 
details documented routine 
note vector vector operations accomplished blas manufacturers assembly coded kernels maximum performance mask file provided link user defined routines 
readme file gives details instructions test routine 
appendix overview blas blas give standardized set basic codes performing operations vectors matrices 
blas take advantage fortran storage structure structure mathematical system possible 
additionally computers blas library optimized system 
routines 
copies vector vector 
saxpy adds vector multiplied scalar vector 
general matrix vector product 
matrix vector product matrix triangular 
solves triangular matrix prefix denotes single precision 
prefix may changed giving routine double complex double complex precision 
course declarations changed 
important note putting double precision single variables works single double cause errors 
define ai xi see code doing alpha computes inner product vectors putting result scalar 
corresponding fortran segment alpha alpha alpha enddo call saxpy alpha multiplies vector length scalar adds result vector putting result corresponding fortran segment alpha enddo appendix overview blas call lda computes matrix vector product plus vector ax putting resulting vector corresponding fortran segment enddo enddo illustrates feature blas requires close attention 
example routine compute residual vector current approximation solution merely change fourth argument 
vector overwritten residual vector need copy temporary storage 
call lda computes product ax putting resulting vector upper triangular matrix corresponding fortran segment temp temp enddo enddo note parameters single quotes descriptions upper tri angular transpose 
feature extensively resulting storage savings advantages 
variable lda critical addressing array correctly 
lda leading dimension dimensional array lda declared allocated number rows dimensional array appendix glossary adaptive methods iterative methods collect information coefficient matrix iteration process speed convergence 
backward error size perturbations coefficient matrix right hand side linear system ax computed iterate solution band matrix matrix nonnegative constants ai constants called left right respectively 
black box piece software knowledge inner workings user supplies input output assumed correct 
blas basic linear algebra subprograms set commonly occurring vector matrix operations dense linear algebra 
optimized usually assembly coded implementations blas exist various computers give higher performance implementation high level programming languages 
block factorization see block matrix operations 
block matrix operations matrix operations expressed terms submatrices 
breakdown occurrence zero divisor iterative method 
decomposition expressing symmetric matrix product lower triangular matrix transpose ll condition number see spectral condition number 
convergence fact rate iterative method approaches solution linear system 
convergence linear measure distance solution decreases constant factor iteration 
superlinear measure error decreases growing factor 
smooth measure error decreases iterations necessarily factor 
irregular measure error decreases iterations increases 
observation unfortunately imply ultimate convergence method 
appendix glossary stalled measure error stays constant number iterations 
imply ultimate convergence method 
dense matrix matrix number zero elements small warrant specialized algorithms exploit zeros 
diagonally dominant matrix see matrix properties direct method algorithm produces solution system linear equations number operations determined priori size system 
exact arithmetic direct method yields true solution system 
see iterative method 
distributed memory see parallel computer 
divergence iterative method said diverge converge reasonable number iterations measure error grows unacceptably 
growth error sign divergence method irregular convergence behavior may ultimately converge error grows iterations 
domain decomposition method solution method linear systems partitioning physical domain differential equation 
domain decomposition methods typically involve repeated independent system solution subdomains way combining data subdomains separator part domain 
field values matrix field values set ax 
symmetric matrices range min max 
fill position zero original matrix exact factorization incomplete factorization fill elements discarded 
forward error difference computed iterate true solution linear system measured vector norm 
see band matrix 
ill conditioned system linear system coefficient matrix large condition number 
applications condition number proportional power number unknowns understood constant proportionality large 
incomplete factorization factorization obtained discarding certain elements factorization process modified relaxed incomplete factorization compensate way discarded elements 
incomplete lu factorization matrix general satisfy lu hopes factorization lu close function preconditioner iterative method 
iterate approximation solution linear system iteration iterative method 
iterative method algorithm produces sequence approximations solution linear system equations length sequence priori size system 
usually longer iterates closer able get true solution 
see direct method 
krylov sequence matrix vector sequence vectors finite initial part sequence 
krylov subspace subspace spanned krylov sequence 
lapack mathematical library linear algebra routine dense systems solution eigenvalue calculations 
lower triangular matrix matrix ai lq factorization way writing matrix product lower triangular matrix unitary matrix lq 
lu factorization lu decomposition expressing matrix product lower triangular matrix upper triangular matrix lu 
matrix see matrix properties 
matrix norms see norms 
matrix properties call square matrix symmetric ai aj positive definite satisfies xt ax nonzero vectors diagonally dominant ai ai excess amount mini ai called diagonal dominance matrix 
ai matrix ai nonsingular message passing see parallel computer 
multigrid method solution method linear systems restricting extrapolating solutions series nested grids 
modified incomplete factorization see incomplete factorization 
mutually consistent norms see norms 
natural ordering see ordering unknowns 
nonstationary iterative method iterative method iteration dependent coefficients 
normal equations nonsymmetric indefinite nonsingular system equations ax related symmetric systems ax aa 
complex replaced expressions 
norms function called vector norm 
properties hold matrix norms 
matrix norm vector norm denoted called mutually consistent pair matrices vectors ax 
ordering unknowns linear systems derived partial differential equation unknown corresponds node discretization mesh 
different orderings unknowns correspond permutations coefficient matrix 
convergence speed iterative methods may depend ordering parallel efficiency method parallel computer strongly dependent ordering 
common orderings rectangular domains appendix glossary natural ordering consecutive numbering rows columns 
red black ordering numbering nodes coordinates odd numbered 
ordering diagonals ordering nodes grouped levels constant 
nodes level numbered nodes level 
matrices problems regular domains common orderings cuthill mckee ordering starts point numbers neighbors continues numbering points neighbors numbered points 
reverse cuthill mckee ordering reverses numbering may reduce amount fill factorization matrix 
minimum degree ordering orders matrix rows increasing numbers nonzeros 
parallel computer computer multiple independent processing units 
processors immediate access memory memory said shared processors private memory immediately visible processors memory said distributed 
case processors communicate message passing 
pipelining see vector computer 
positive definite matrix see matrix properties 
preconditioner auxiliary matrix iterative method approximates sense coefficient matrix inverse 
preconditioner preconditioning matrix applied step iterative method 
red black ordering see ordering unknowns 
reduced system linear system obtained eliminating certain variables linear system 
number variables smaller original system matrix reduced system generally nonzero entries 
original matrix symmetric positive definite reduced system smaller condition number 
relaxed incomplete factorization see incomplete factorization 
residual iterative method employed solve linear system ax residual corresponding vector ay search direction vector update iterate 
shared memory see parallel computer 
simultaneous displacements method jacobi method 
sparse matrix matrix number zero elements large algorithms avoiding operations zero elements pay 
matrices derived partial differential equations typically number nonzero elements proportional matrix size total number matrix elements square matrix size 
spectral condition number product max min 
notation max min denote largest smallest eigenvalues respectively 
linear systems derived partial differential equations condition number proportional number unknowns 
spectral radius spectral radius matrix max 
spectrum set eigenvalues matrix 
stationary iterative method iterative method performs iteration operations current iteration vectors 
stopping criterion iterative method computes successive approximations solution linear system practical test needed determine iteration 
ideally test measure distance iterate true solution possible 
various metrics typically involving residual 
storage scheme way elements matrix stored memory computer 
dense matrices decision store rows columns consecutively 
sparse matrices common storage schemes avoid storing zero elements result involve indices stored integer data indicate stored elements fit global matrix 
successive displacements method gauss seidel method 
symmetric matrix see matrix properties 
template description algorithm abstracting away implementational details 
tune adapt software specific application computing environment order obtain better performance case 
upper triangular matrix matrix ai vector computer computer able process consecutive identical operations typically additions multiplications times faster intermixed operations different types 
processing identical operations way called pipelining operations 
vector norms see norms 
notation section notation book 
tried standard notation current publication subjects covered 
follow conventions matrices denoted capital letters 
vectors denoted lowercase letters 
lowercase greek letters usually denote scalars instance matrix elements denoted doubly indexed lowercase letter matrix subblocks denoted doubly indexed uppercase letters 
appendix glossary define matrix dimension block dimension follows ai am am define vector dimension follows xi xn notation follows am am simply size clear context denotes identity matrix 
ai 
diag ai denotes matrix elements ai diagonal zeros 
denotes ith element vector kth iteration bibliography 
karlsson preconditioned cg type methods solving coupled systems fundamental semiconductor equations bit pp 

adams jordan sor color blind siam sci 
statist 
comput pp 

anderson lapack users guide siam philadelphia 
cheshire nested factorization reservoir simulation symposium spe 

demmel duff solving sparse linear systems sparse backward error siam matrix anal 
appl pp 

arnoldi principle minimized iterations solution matrix eigenvalue problem quart 
appl 
math pp 

ashby fortran implementation adaptive chebyshev algorithm tech 
rep uiucdcs university illinois 
ashby otto comparison adaptive chebyshev squares polynomial preconditioning hermitian positive definite linear systems siam sci 
statist 
comput pp 

ashby adaptive polynomial preconditioning hermitian indefinite linear systems bit pp 

ashby taxonomy conjugate gradient methods siam numer 
anal pp 

grimes vectorizing incomplete factorizations preconditioners siam sci 
statist 
comput pp 

axelsson incomplete block matrix factorization preconditioning methods 
ultimate answer comput 
appl 
math pp 

general incomplete block matrix factorization method linear algebra appl pp 

axelsson barker finite element solution boundary value problems 
theory computation academic press orlando fl 
axelsson vectorizable preconditioners elliptic difference equations space dimensions comput 
appl 
math pp 

bibliography nested recursive level factorization method point difference matrices siam sci 
statist 
comput pp 

axelsson gustafsson iterative solution solution navier equations elasticity comput 
methods appl 
mech 
engrg pp 

axelsson eigenvalue distribution class preconditioning matrices numer 
math pp 

rate convergence preconditioned conjugate gradient method numer 
math pp 

axelsson analysis incomplete factorizations fixed storage allocation preconditioning methods theory applications evans ed gordon breach new york pp 

axelsson approximate factorization methods block matrices suitable vector parallel processors linear algebra appl pp 

axelsson algebraic multilevel preconditioning methods numer 
math pp 

algebraic multilevel preconditioning methods ii siam numer 
anal pp 

axelsson black box generalized conjugate gradient solver inner iterations variable step preconditioning siam matrix anal 
appl pp 

bank marching algorithms elliptic boundary value problems ii variable coefficient case siam numer 
anal pp 

bank chan jr smith alternate block factorization procedure systems partial differential equations bit pp 

bank rose marching algorithms elliptic boundary value problems 
constant coefficient case siam numer 
anal pp 

bank chan composite step bi conjugate gradient algorithm nonsymmetric linear systems tech 
rep cam ucla dept math los angeles ca 
bank chan analysis composite step biconjugate gradient method numerische mathematik pp 

baudet asynchronous iterative methods multiprocessors assoc 
comput 
mach pp 

axelsson perturbations linear algebra appl pp 

approximate factorizations consistently ordered factors bit pp 

existence criteria partial matrix factorizations iterative methods siam numer 
anal pp 

bibliography bj accelerated projection methods computing pseudoinverse solutions systems linear equations bit pp 

contraction number multigrid method solving poisson equation numer 
math pp 

bramble schatz construction preconditioners elliptic problems mathematics computation pp 

bramble wang xu convergence estimates product iterative methods applications domain decompositions multigrid math 
comp appear 
sameh row projection methods large nonsymmetric linear systems siam sci 
statist 
comput pp 

avoiding breakdown cgs algorithm numer 
alg pp 

avoiding breakdown near breakdown lanczos type algorithms numer 
alg pp 

breakdown free lanczos type algorithm solving linear systems numer 
math pp 

briggs multigrid tutorial siam philadelphia 

cai multiplicative schwarz algorithms nonsymmetric indefinite problems siam numer 
anal pp 

chan fourier analysis relaxed incomplete factorization preconditioners siam sci 
statist 
comput pp 

chan de van der vorst transpose free squared lanczos algorithm application solving nonsymmetric linear systems tech 
rep cam ucla dept math los angeles ca 
chan gallopoulos tong quasi minimal residual variant bi cgstab algorithm nonsymmetric systems tech 
rep cam ucla dept math los angeles ca 
siam sci 
comput appear 
chan eds domain decomposition methods philadelphia siam 
proceedings second international symposium domain decomposition methods los angeles ca january 
eds domain decomposition methods philadelphia siam 
proceedings third international symposium domain decomposition methods houston tx 
eds domain decomposition methods siam philadelphia 
proceedings fourth international symposium domain decomposition methods moscow ussr 
chan 
kuo color fourier analysis iterative algorithms elliptic problems red black ordering siam sci 
statist 
comput pp 

bibliography chan shao efficient variants vertex space domain decomposition algorithm tech 
rep cam ucla dept math los angeles ca 
siam sci 
comput appear 
chan shao choice coarse grid size domain decomposition methods tech 
rep ucla dept math los angeles ca 
appear 
miranker chaotic relaxation linear algebra appl pp 

gear step iterative methods symmetric linear systems comput 
appl 
math pp 

golub generalized conjugate gradient method nonsymmetric systems linear equations computer methods applied sciences engineering second international symposium dec lecture notes economics mathematical systems vol 
berlin new york springer verlag 
golub block preconditioning conjugate gradient method siam sci 
statist 
comput pp 

golub leary generalized conjugate gradient method numerical solution elliptic partial differential equations sparse matrix computations bunch rose eds academic press new york pp 

golub fast direct methods efficient numerical solution nonseparable elliptic equations siam numer 
anal pp 

cuthill mckee reducing bandwidth sparse symmetric matrices acm proceedings th national conference 
azevedo lapack working note reducing communication costs conjugate gradient algorithm distributed memory multiprocessor tech 
report computer science department university tennessee knoxville tn 
azevedo reducing communication costs conjugate gradient algorithm distributed memory multiprocessors tech 
rep ornl tm oak ridge national lab oak ridge tn 
de parallel restructured version gmres tech 
rep delft university technology delft netherlands 
de nested krylov methods preserving orthogonality tech 
rep preprint utrecht university utrecht netherlands 
moss smith decay rates inverses band matrices mathematics computation pp 

demmel condition number equivalence transformations block diagonalize matrix pencils siam numer 
anal pp 

demmel heath van der vorst parallel linear algebra acta numerica vol 
cambridge press new york 
doi parallelism convergence incomplete lu factorizations appl 
numer 
math pp 

bibliography dongarra duff hammarling set level basic linear algebra subprograms acm trans 
math 
soft pp 

dongarra hammarling hanson extended set fortran basic linear algebra subprograms acm trans 
math 
soft pp 

dongarra duff sorensen van der vorst solving linear systems vector shared memory computers siam philadelphia pa 
dongarra grosse distribution mathematical software electronic mail comm 
acm pp 

dongarra moler bunch stewart linpack users guide siam philadelphia 
dongarra van der vorst performance various computers standard sparse linear equations solving techniques computer benchmarks dongarra eds elsevier science publishers new york pp 

dorr direct solution discrete poisson equation rectangle siam rev pp 

unified theory domain decomposition algorithms elliptic problems tech 
rep note department computer science courant institute 
dubois greenbaum approximating inverse matrix iterative algorithms vector processors computing pp 

duff grimes lewis sparse matrix test problems acm trans 
math 
soft pp 

duff effect ordering preconditioned conjugate gradients bit pp 

duff reid direct methods sparse matrices oxford university press london 
dupont kendall approximate factorization procedure solving self adjoint elliptic difference equations siam numer 
anal pp 

method variable directions solving systems finite difference equations soviet math 
dokl pp 

tom 
ehrlich ad hoc sor method comput 
phys pp 

varga optimal best sor iteration method linear algebra appl pp 

analysis parallel incomplete point factorizations linear algebra appl pp 

beware unperturbed modified incomplete point factorizations proceedings imacs international symposium iterative methods linear algebra brussels belgium de groen eds 
bibliography lapack working note distributed sparse data structures linear algebra operations tech 
rep cs computer science department university tennessee knoxville tn 
lapack working note qualitative properties conjugate gradient lanczos methods matrix framework tech 
rep cs computer science department university tennessee knoxville tn 
decay rates inverses banded matrices near toeplitz matrices linear algebra appl pp 

positive aspects vectorizable preconditioners parallel computing pp 

efficient implementation class preconditioned conjugate gradient methods siam sci 
statist 
comput pp 

convergence theorems gauss seidel minimization algorithms tech 
rep computer science center university maryland college park md jan 
elman approximate schur complement preconditioners serial parallel computers siam sci 
statist 
comput pp 

elman schultz preconditioning fast direct methods non self adjoint nonseparable elliptic equations siam numer 
anal pp 

note optimal block scaling matrices numer 
math pp 

faber necessary sufficient conditions existence conjugate gradient method siam numer 
anal pp 

mitchell high accuracy difference schemes splitting operator equations parabolic elliptic type numer 
math pp 

fletcher conjugate gradient methods indefinite systems numerical analysis dundee watson ed berlin new york springer verlag pp 

forsythe strauss best conditioned matrices proc 
amer 
math 
soc pp 

freund conjugate gradient type methods linear systems complex symmetric coefficient matrices siam sci 
statist 
comput pp 

freund gutknecht nachtigal implementation look ahead lanczos algorithm non hermitian matrices siam sci 
comput pp 

freund nachtigal qmr quasi minimal residual method non hermitian linear systems numer 
math pp 

implementation qmr method coupled term recurrences tech 
rep riacs nasa ames ames ca 
freund quasi minimal residual squared algorithm non hermitian linear systems tech 
rep riacs nasa ames ames ca 
bibliography freund transpose free quasi minimum residual algorithm non hermitian linear systems siam sci 
comput pp 

freund golub nachtigal iterative solution linear systems acta numerica pp 

golub eds domain decomposition methods partial differential equations siam philadelphia 
proceedings international symposium domain decomposition methods partial differential equations paris france january 
golub leary history conjugate gradient lanczos methods siam rev pp 

golub van loan matrix computations second edition johns hopkins university press baltimore 
greenbaum predicting behavior finite precision lanczos conjugate gradient computations siam mat 
anal 
appl pp 

gropp keyes domain decomposition local mesh refinement siam sci 
statist 
comput pp 

gustafsson class order factorization methods bit pp 

gutknecht completed theory unsymmetric lanczos process related algorithms part ii tech 
rep ips research report eth rich switzerland 
unsymmetric lanczos algorithms relations de approximation continued fractions qd algorithm proceedings copper mountain conference iterative methods 
variants bi cgstab matrices complex spectrum tech 
rep ips eth rich switzerland 
completed theory unsymmetric lanczos process related algorithms part siam matrix anal 
appl pp 

multi grid methods applications springer verlag berlin new york 
iterative sung gro er teubner stuttgart 
high accuracy difference schemes solving elliptic equations numer 
math pp 

young applied iterative methods academic press new york 
hager condition estimators siam sci 
statist 
comput pp 

stiefel methods conjugate gradients solving linear systems res 
nat 
bur 
stand pp 

conjugacy gradients history scientific computing addison wesley reading ma pp 

bibliography higham experience matrix norm estimator siam sci 
statist 
comput pp 

young generalized conjugate gradient acceleration metrizable iterative methods linear algebra appl pp 

johnson micchelli paul polynomial preconditioning conjugate gradient calculation siam numer 
anal pp 

jones parallel solution sparse systems linear equations proceedings sixth siam conference parallel processing scientific computing keyes reed eds siam philadelphia pp 

parallel graph coloring heuristic siam sci 
statist 
comput pp 

lanczos methods solution nonsymmetric systems linear equations siam matrix anal 
appl pp 

kahan gauss seidel methods solving large systems linear equations phd thesis university toronto 
estimates computational techniques linear algebra mathematics computation pp 

incomplete cholesky conjugate gradient method iterative solution systems linear equations comput 
phys pp 

analysis comparison relaxation schemes robust multigrid preconditioned conjugate gradient methods multigrid methods lecture notes mathematics eds springer verlag berlin new york pp 

linear multigrid methods numerical reservoir simulation phd thesis delft university technology delft netherlands 
keyes chan voigt eds domain decomposition methods partial differential equations siam philadelphia 
proceedings fifth international symposium domain decomposition methods va 
keyes gropp comparison domain decomposition techniques elliptic partial differential equations parallel implementation siam sci 
statist 
comput pp 

domain decomposition nonsymmetric systems equations examples computational fluid dynamics domain decomposition methods proceedings second internation symposium los angeles california january chan periaux eds philadelphia siam pp 

domain decomposition techniques parallel solution nonsymmetric systems elliptic boundary value problems applied num 
math pp 

kim class lanczos algorithms implemented parallel computers parallel comput pp 

bibliography kincaid young grimes fortran package solving large sparse linear systems adaptive accelerated iterative methods acm trans 
math 
soft pp 

algorithm 
family level block factorization type sov 
numer 
anal 
math 
modelling pp 

lanczos iteration method solution eigenvalue problem linear differential integral operators res 
nat 
bur 
stand pp 

solution systems linear equations minimized iterations res 
nat 
bur 
stand pp 

lawson hanson kincaid krogh basic linear algebra subprograms fortran usage acm trans 
math 
soft pp 

contraction number class level methods exact evaluation finite element subspaces model problems multigrid methods proceedings ln eds vol 
lecture notes mathematics pp 

iteration nonsymmetric linear systems numer 
math pp 

incomplete factorization technique positive definite linear systems mathematics computation pp 

mccormick multilevel adaptive methods partial differential equations siam philadelphia 
mccormick thomas fast adaptive composite grid fac method elliptic equations mathematics computation pp 

meier sameh behavior conjugate gradient algorithms processor hierarchical memory comput 
appl 
math pp 

meier yang preconditioned conjugate gradient methods nonsymmetric linear systems tech 
rep csrd university illinois urbana il april 
van der vorst iterative solution method linear systems coefficient matrix symmetric matrix mathematics computation pp 

guidelines usage incomplete decompositions solving sets linear equations occur practical problems comput 
phys pp 

melhem efficient implementation preconditioned conjugate gradient methods vector supercomputers internat 

pp 

block preconditioned conjugate gradient method vector computers bit pp 

multitasking conjugate gradient method cray mp parallel comput pp 

solving sparse symmetric sets linear equations preconditioned conjugate gradients acm trans 
math 
software pp 

bibliography nachtigal reddy trefethen fast nonsymmetric matrix iterations siam matrix anal 
appl pp 

nachtigal reichel trefethen hybrid gmres algorithm nonsymmetric matrix iterations tech 
rep mit cambridge ma 
nachtigal look ahead variant lanczos algorithm application quasi minimal residual methods non hermitian linear systems phd thesis mit cambridge ma 
solving positive semi definite linear systems preconditioned iterative methods preconditioned conjugate gradient methods axelsson eds vol 
lecture notes mathematics nijmegen pp 

robustness modified incomplete factorization methods internat 
comput 
math pp 

leary block conjugate gradient algorithm related methods linear algebra appl pp 

ordering schemes parallel processing certain mesh problems siam sci 
statist 
comput pp 

kincaid user guide version package solving large sparse linear systems various iterative methods tech 
rep cna center numerical analysis university texas austin austin tx april 
ortega parallel vector solution linear systems plenum press new york london 
paige parlett van der vorst approximate solutions eigenvalue bounds krylov subspaces numer 
lin 
alg 
appear 
paige saunders solution sparse indefinite systems linear equations siam numer 
anal pp 

paige saunders algorithm sparse linear equations sparse squares acm trans 
math 
soft pp 

di data structures vectorize cg algorithms general sparsity patterns bit pp 

parlett symmetric eigenvalue problem prentice hall london 
parlett taylor liu look ahead lanczos algorithm unsymmetric matrices mathematics computation pp 

numerical solution parabolic elliptic differential equations soc 

appl 
math pp 

solution large unsymmetric systems linear equations vol 
series micro electronics volume hartung verlag konstanz 
solution large unsymmetric systems linear equations phd thesis swiss federal institute technology rich switzerland 
bibliography poole ortega methods vector computers tech 
rep rm department applied mathematics university virginia charlottesville va 
ed domain decomposition methods proceedings sixth international symposium domain decomposition methods como italy providence ri ams 
appear 
di robert vector parallel cg algorithms sparse non symmetric systems tech 
rep imag tim grenoble france 
reid method conjugate gradients solution large sparse systems linear equations large sparse sets linear equations reid ed academic press london pp 

preconditioning incomplete block cyclic reduction mathematics computation pp 

saad lanczos algorithm oblique projection methods solving large unsymmetric systems siam numer 
anal pp 

practical krylov subspace methods solving indefinite nonsymmetric linear systems siam sci 
statist 
comput pp 

practical polynomial conjugate gradient method siam sci 
statist 
comput pp 

preconditioning techniques indefinite nonsymmetric linear systems comput 
appl 
math pp 

krylov subspace methods supercomputers siam sci 
statist 
comput pp 

basic tool kit sparse matrix computation tech 
rep csrd tr csrd university illinois urbana il 
flexible inner outer preconditioned gmres algorithm siam sci 
comput pp 

saad schultz conjugate gradient algorithms solving nonsymmetric linear systems mathematics computation pp 

gmres generalized minimal residual algorithm solving nonsymmetric linear systems siam sci 
statist 
comput pp 

bi cgstab linear equations involving unsymmetric matrices complex spectrum tech 
rep university utrecht department mathematics utrecht netherlands 
smith domain decomposition algorithms partial differential equations linear elasticity tech 
rep department computer science courant institute 
cgs fast lanczos type solver nonsymmetric linear systems siam sci 
statist 
comput pp 

relaxation methods theoretical physics clarendon press oxford 
bibliography stone iterative solution implicit approximations multidimensional partial differential equations siam numer 
anal pp 

methods cyclic reduction fourier analysis algorithm discrete solution poisson equation rectangle siam rev pp 

tong comparative study preconditioned lanczos methods nonsymmetric linear systems tech 
rep sand sandia nat 
lab livermore ca 
van der condition numbers equilibration matrices numer 
math pp 

van der van der vorst rate convergence conjugate gradients numer 
math pp 

van der vorst iterative solution methods certain sparse linear systems non symmetric matrix arising pde problems comput 
phys pp 

vectorizable variant methods siam sci 
statist 
comput pp 

large tridiagonal block tridiagonal linear systems vector parallel computers parallel comput pp 

problems vector computers supercomputing eds north holland 
report data processing center kyoto university kyoto japan december 
high performance preconditioning siam sci 
statist 
comput pp 

related methods problems vector computers computer physics communications pp 

report data processing center kyoto university kyoto japan may 
convergence behavior preconditioned cg cg presence rounding errors preconditioned conjugate gradient methods axelsson eds vol 
lecture notes mathematics berlin new york springer verlag 
bi cgstab fast smoothly converging variant bi cg solution nonsymmetric linear systems siam sci 
statist 
comput pp 

van der vorst petrov galerkin type method solving ax symmetric complex ieee trans 
pp 

van der vorst family nested gmres methods tech 
rep delft university technology faculty tech 
math delft netherlands 
van minimizing inner product data dependencies conjugate gradient iteration tech 
rep icase nasa langley research center 
varga matrix iterative analysis prentice hall englewood cliffs nj 
preconditioning nonsymmetric indefinite finite element matrices numer 
alg 
appl pp 

bibliography problem non self adjoint generalization conjugate gradient method closed comput 
maths 
math 
phys pp 

walker implementation gmres method householder transformations siam sci 
statist 
comput pp 

multigrid methods wiley chichester 
lanczos method class non symmetric systems linear equations siam numer 
anal pp 

young iterative solution large linear systems academic press new york 
multilevel splitting finite element spaces numer 
math pp 

bibliography appendix flowchart iterative methods matrix symmetric 
transpose available 
storage premium 
try gmres long restart matrix definite 
try cg try qmr try cgs bi cgstab outer eigenvalues known 
try cg try chebyshev cg 
