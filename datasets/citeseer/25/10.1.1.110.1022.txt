unscented kalman filter nonlinear estimation extended kalman filter ekf standard technique number nonlinear estimation machine learning applications 
include estimating state nonlinear dynamic system estimating parameters nonlinear system identification learning weights neural network dual estimation expectation maximization em algorithm states parameters estimated simultaneously 
points flaws ekf introduces improvement unscented kalman filter ukf proposed julier 
central vital operation performed kalman filter propagation gaussian random variable system dynamics 
ekf state distribution approximated propagated analytically order linearization nonlinear system 
introduce large errors true posterior mean covariance transformed may lead sub optimal performance divergence filter 
ukf addresses problem deterministic sampling approach 
state distribution approximated represented minimal set carefully chosen sample points 
sample points completely capture true mean covariance propagated true nonlinear system captures posterior mean covariance accurately rd order taylor series expansion nonlinearity 
ekf contrast achieves order accuracy 
remarkably computational complexity ukf order ekf 
julier demonstrated substantial performance gains ukf context state estimation nonlinear control 
machine learning problems considered 
extend ukf broader class nonlinear estimation problems including nonlinear system identification training neural networks dual estimation problems 
preliminary results 
algorithms developed illustrated number additional examples 
eric wan rudolph van der merwe oregon graduate institute science technology nw walker rd beaverton oregon ece ogi edu ece ogi edu sponsored nsf iri 
ekf applied extensively field nonlinear estimation 
general application areas may divided state estimation machine learning 
divide machine learning parameter estimation dual estimation 
framework areas briefly reviewed 
state estimation basic framework ekf involves estimation state discrete time nonlinear dynamic system represent unobserved state system observed signal 
process noise drives dynamic system observation noise note assuming additivity noise sources 
system dynamic model assumed known 
state estimation ekf standard method choice achieve recursive approximate maximumlikelihood estimation state review ekf context section help motivate unscented kalman filter ukf 
parameter estimation classic machine learning problem involves determining nonlinear mapping input output nonlinear map parameterized vector nonlinear map example may feedforward recurrent neural network weights numerous applications regression classification dynamic modeling 
learning corresponds estimating parameters typically training set provided sample pairs consisting known input desired outputs error machine defined goal learning involves solving parameters order minimize expected squared error 
number optimization approaches exist gradient descent backpropagation ekf may estimate parameters writing new state space representation parameters correspond stationary process identity state transition matrix driven process noise choice variance determines tracking performance 
output corresponds nonlinear observation ekf applied directly efficient second order technique learning parameters 
linear case relationship kalman filter kf recursive squares rls 
ekf training neural networks developed singhal wu 
dual estimation special case machine learning arises input unobserved requires coupling state estimation parameter estimation 
dual estimation problems consider discrete time nonlinear dynamic system system states set model parameters dynamic system simultaneously estimated observed noisy signal approaches dual estimation discussed section 
section explain basic assumptions flaws ekf 
section introduce unscented kalman filter ukf method amend flaws ekf 
section results ukf different areas nonlinear estimation 

ekf flaws consider basic state space estimation framework equations 
noisy observation recursive estimation expressed form see prediction prediction recursion provides optimal minimum mean squared error mmse estimate assuming prior estimate current observation gaussian random vari ables 
need assume linearity model 
optimal terms recursion optimal prediction written corresponds expectation nonlinear function random variables similar interpretation optimal prediction 
optimal gain term expressed function posterior covariance matrices 
note terms require tak ing expectations nonlinear function prior state estimates 
kalman filter calculates quantities exactly linear case viewed efficient method analytically propagating linear system dynamics 
nonlinear models ekf approximates optimal terms predictions approximated simply function prior mean value estimates expectation taken covariance determined linearizing dynamic equations determining posterior covariance matrices analytically linear system 
words ekf state distribution approximated propagated analytically order linearization nonlinear system 
readers referred explicit equations 
ekf viewed providing order approximations optimal terms approximations introduce large errors true posterior mean covariance transformed gaussian random variable may lead sub optimal performance divergence filter 
flaws amended section ukf 

unscented kalman filter ukf addresses approximation issues ekf 
state distribution represented specified minimal set carefully chosen sample points 
sample points completely capture true mean covariance propagated true non linear system captures posterior mean covariance accurately rd order taylor series expansion nonlinearity 
elaborate noise means denoted usually assumed equal zero 
second order versions ekf exist increased implementation computational complexity tend prohibit 
start explaining unscented transformation 
unscented transformation ut method calculating statistics random variable undergoes nonlinear transformation 
consider propagating random variable nonlinear function assume mean covariance dimension calculate statistics form matrix vectors sigma weights corresponding accord ing mines spread sigma points usually set small positive value 
secondary scaling parameter usually set incorporate prior knowledge distribution scaling parameter 
deter distributions gaussian optimal 
propagated nonlinear function th row matrix square root 
sigma vectors mean covariance approximated weighted sample mean covariance posterior sigma points note method differs substantially general sampling methods monte carlo methods particle filters require orders magnitude sample points attempt propagate accurate possibly non gaussian distribution state 
deceptively simple approach taken ut results approximations accurate third order gaussian inputs nonlinearities 
non gaussian inputs approximations accurate second order accuracy third higher order moments determined choice see detailed discussion ut 
simple example shown dimensional system left plot shows true mean covariance propagation monte carlo sampling center plots actual sampling linearized ekf ut mean true mean covariance true covariance sigma points ut mean weighted sample mean covariance ut covariance transformed sigma points example ut mean covariance propagation 
actual order linearization ekf ut show results linearization approach done ekf right plots show performance ut note sigma points required 
superior performance ut clear 
unscented kalman filter ukf straightforward extension ut recursive estimation equation state rv redefined concatenation original state noise variables ut sigma point selection scheme equation applied new augmented state rv calculate corresponding matrix sigma ukf equations algorithm 
note explicit calculation jacobians hessians necessary implement algorithm 
furthermore number computations order ekf 

applications results ukf originally designed state estimation problem applied nonlinear control applications requiring full state feedback 
applications dynamic model represents physically parametric model assumed known 
section extend ukf broader class nonlinear estimation problems results 

ukf state estimation order illustrate ukf state estimation provide new application example corresponding noisy timeseries estimation 
example ukf estimate underlying clean time series corrupted additive gaussian white noise 
time series mackey glass chaotic initialize calculate sigma points time update measurement update equations composite scaling parameter dimension augmented state process noise cov measurement noise cov calculated eqn 

weights algorithm unscented kalman filter ukf equations series 
clean times series modeled nonlinear autoregression model parameterized approximated training feedforward neural network clean sequence 
residual error convergence taken process noise variance 
white gaussian noise added clean mackey glass series generate noisy time series corresponding state space representation 

estimation problem noisy time series observed input ekf ukf algorithms utilize known neural network model 
note state space formulation ekf ukf order complexity 
shows sub segment estimates generated ekf ukf original noisy time series db snr 
superior performance ukf clearly visible 
normalized mse estimation mackey glass time series ekf clean noisy ekf estimation mackey glass time series ukf clean noisy ukf estimation error ekf vs ukf mackey glass ekf ukf estimation mackey glass time series ekf ukf known model 
bottom graph shows comparison estimation errors complete sequence 

ukf dual estimation recall dual estimation problem consists simultaneously estimating clean state model pa rameters noisy data see equation 
expressed earlier number algorithmic approaches exist problem 
results dual ukf joint ukf 
development unscented smoother em approach 
prior state estimation example utilize noisy time series application modeled neural networks illustration approaches 
dual extended kalman filter separate state space representation signal weights 
state space representation state equation 
context time series statespace representation weights set innovations covariance equal run simultaneously signal weight estimation 
time step current estimate weights signal filter current estimate signal state weight filter 
new dual ukf algorithm state weight estimation done ukf 
note state transition linear weight filter nonlinearity restricted measurement equation 
joint extended kalman filter signal state weight vectors concatenated single joint state vector estimation done recursively writ ing state space equations joint state running ekf joint state space produce simultaneous estimates states approach ukf ekf 
dual estimation experiments results time series provide clear illustration ukf ekf 
series mackey glass chaotic series additive noise snr db 
second time series chaotic comes autoregressive neural network random weights driven gaussian process noise usually set small constant related time constant rls weight decay 
data length 
covariance adapted rls weight decay method 
corrupted additive white gaussian noise snr db 
standard mlp hidden activation functions linear output layer filters mackey glass problem 
mlp second problem 
process measurement noise variances assumed known 
note contrast state estimation example previous section noisy time series observed 
clean provided training 
example training curves different dual joint kalman estimation methods shown 
final estimate mackey glass series shown dual ukf 
superior performance ukf algorithms clear 
improvements consistent statistically significant number additional experiments 
normalized mse normalized mse chaotic ar neural network dual ukf dual ekf joint ukf joint ekf epoch mackey glass chaotic time series epoch dual ekf dual ukf joint ekf joint ukf estimation mackey glass time series dual ukf clean noisy dual ukf comparative learning curves results dual estimation experiments 

ukf parameter estimation part dual ukf algorithm implemented ukf weight estimation 
represents new parameter estimation technique applied problems training feedforward neural networks regression classification problems 
recall case write state space representation unknown weight parameters equation 
note case ukf ekf order number weights 
advantage ukf ekf case obvious state transition function linear 
pointed earlier observation nonlinear 
effectively ekf builds approximation expected hessian outer products gradient 
ukf may provide accurate estimate direct approximation expectation hessian 
note distinct advantage ukf occurs architecture error metric differentiation respect parameters easily derived necessary ekf 
ukf effectively evaluates jacobian hessian precisely sigma point propagation need perform analytic differentiation 
performed number experiments applied training neural networks standard benchmark data 
illustrates differences learning curves averaged experiments different initial weights mackay robot arm dataset ikeda chaotic time series 
note slightly faster convergence lower final mse performance ukf weight training 
results clearly encouraging study necessary fully contrast differences ukf ekf weight training 
mean mse mackay robot arm learning curves epoch mean mse ikeda chaotic time series learning curves epoch comparison learning curves ekf ukf training 
mackay robot arm mlp ikeda time series mlp 
ukf ekf ukf ekf 
ekf widely accepted standard tool machine learning community 
alternative ekf unscented filter 
ukf consistently achieves better level accuracy ekf comparable level complexity 
demonstrated performance gain number application domains including state estimation dual estimation parameter estimation 
includes additional characterization performance benefits extensions batch learning non mse cost functions application neural non neural parametric architectures 
addition exploring ukf method improve particle filters extension ukf avoids linear update assumption direct bayesian update 

de freitas niranjan gee doucet 
sequential monte carlo methods optimisation neural network models 
technical report cues infeng tr dept engineering university cambridge nov 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
haykin 
adaptive filter theory 
prentice hall edition 
julier 
scaled unscented transformation 
appear automatica february 
julier uhlmann 
new extension kalman filter nonlinear systems 
proc 
aerosense th int 
symp 
aerospace defence sensing simulation controls 
lewis 
optimal estimation 
john wiley sons new york 
matthews 
state space approach adaptive nonlinear filtering recurrent neural networks 
proceedings iasted internat 
symp 
artificial intelligence application neural networks pages 

decoupled extended kalman filter training feedforward layered networks 
ijcnn volume pages 
singhal wu 
training multilayer perceptrons extended kalman filter 
advances neural information processing systems pages san mateo ca 
morgan kauffman 
van der merwe de freitas doucet wan 
unscented particle filter 
technical report dept engineering university cambridge 
preparation 
wan nelson 
neural dual extended kalman filtering applications speech enhancement monaural blind signal separation 
proc 
neural networks signal processing workshop 
ieee 
wan van der merwe 
unscented bayes filter 
technical report oregon graduate institute science technology 
preparation cse ogi edu 
wan van der merwe nelson 
dual estimation unscented transformation 
solla leen 
ller editors advances neural information processing systems pages 
mit press 
