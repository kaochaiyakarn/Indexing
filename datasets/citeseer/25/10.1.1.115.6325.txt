wrl research report limits instruction level parallelism david wall western research laboratory university avenue palo alto california usa western research laboratory wrl computer systems research group founded digital equipment 
focus computer science research relevant design application high performance scientific computers 
test ideas designing building real systems 
systems build research prototypes intended products 
research laboratories located palo alto network systems laboratory nsl systems research center src 
digital research groups located paris prl cambridge massachusetts crl 
research directed mainstream high performance computer systems 
prototypes intended computing environments digital customers 
long term goal wrl aid accelerate development high performance uni multi processors 
research projects wrl address various aspects high performance computing 
believe significant advances computer systems come single technological advance 
technologies hardware software advance pace 
system design art composing systems level technology appropriate balance 
major advance system performance require reexamination aspects system 
design fabrication packaging hardware language processing scaling issues system software design exploration new applications areas opening advent higher performance systems 
researchers wrl cooperate closely move freely various levels system design 
allows explore wide range tradeoffs meet system goals 
publish results variety journals conferences research reports technical notes 
document research report 
research reports normally accounts completed research may include material earlier technical notes 
technical notes rapid distribution technical material usually represents research progress 
research reports technical notes may ordered 
may mail order technical report distribution dec western research laboratory wrl university avenue palo alto california usa reports notes may ordered electronic mail 
addresses digital net wrl techreports internet wrl techreports dec com uucp wrl techreports obtain details ordering electronic mail send message addresses word help subject line receive detailed instructions 
limits instruction level parallelism david wall november western research laboratory university avenue palo alto california usa growing interest ambitious multiple issue machines machines requires careful examination parallelism exists typical programs 
examination complicated wide variety hardware software techniques increasing parallelism exploited including branch prediction register renaming alias analysis 
performing simulations instruction traces model techniques limits feasibility 
presents results simulations different test programs different models available parallelism analysis 
replaces technical note tn earlier version material 
author note years ago published preliminary results simulation study parallelism wall 
took advantage fast instruction level simulator computing environment dozen machines performance mips range night weeks 
space parallelism techniques explored large study scratched surface 
report reading attempt fill cracks simulating intermediate models considering ideas original study consider 
believe far extensive study kind requiring machine years simulating excess instructions 
original generated different opinions looked high parallelism available ambitious say unrealistic models millennium 
opinion pessimistic looked different things get right including things study doesn address 
moderated opinion somewhat consider negative results study important positive 
study produced far numbers text graphs complete results available appendix 
tried selection results detail careful study numbers appendix may reward reader 
years preliminary appeared multiple issue architectures changed interesting idea revealed truth little hard data available 
hope results helpful 
emphasized treated mandates 
new architecture substitute simulations include real pipeline details memory configuration larger program suite study include 
probably exactly opinions appeared 
limits instruction level parallelism years explosion interest multiple issue machines 
designed exploit usually compiler assistance parallelism programs exhibit instruction level 
shows example parallelism 
code fragment consists instructions executed time depend results 
code fragment dependencies executed parallel 
case parallelism number instructions divided number cycles required 
parallelism parallelism instruction level parallelism lack thereof architectures proposed take advantage kind parallelism 
superscalar machine ac issue multiple independent instructions cycle 
machine jw issues instruction cycle cycle time smaller typical instruction latency 
vliw machine nf superscalar machine parallel instructions explicitly packed compiler long instruction words 
ordinary pipelined machines degree parallelism operations multi cycle latencies instructions shorter unrelated instructions performed 
compute degree parallelism multiplying latency operation relative dynamic frequency typical programs 
latencies loads delayed branches floating point instructions give decstation example parallelism equal 
multiple issue machine hardware cost scalar machine equivalent technology 
cost may small large depending aggressively machine pursues instruction level parallelism 
case particular approach feasible depends cost parallelism obtained 
parallelism exploit 
question programs machines 
build machine amount instruction level parallelism choose 
parallelism go unused example learned programs consisted linear sequences instructions dependent predecessor result 
real programs bad illustrates 
parallelism find program limited hard willing find 
decstation trademark digital equipment 
limits instruction level parallelism number studies jw tf dating back years show parallelism basic block rarely exceeds average 
unsurprising basic blocks typically instructions long leaving little scope lot parallelism 
extreme study nicolau fisher nf finds average parallelism high considering highly parallel numeric programs simulating machine unlimited hardware parallelism omniscient scheduler 
lot space lot space analysis looks basic blocks analysis assumes omniscient scheduler 
space multi dimensional parallelism analysis consists growing body complementary techniques 
payoff choice depends strongly context choices 
purpose study explore multi dimensional space provide insight importance different techniques different contexts 
looked parallelism different programs points space 
section describes capabilities simulation system discusses various parallelism enhancing techniques model 
followed long section looking results complete table results appendix 
appendix gives details implementation techniques 
experimental framework studied instruction level parallelism eighteen test programs 
twelve taken spec suite common utility programs cad tools written wrl 
programs shown 
spec benchmarks run accompanying test data data usually official short data set data set cases modified source decrease iteration count outer loop 
appendix contains details modifications data sets 
programs compiled decstation mips processor 
mips version compilers 
studies instruction level parallelism oracle driven trace simulation 
obtaining trace instructions executed 
trace includes data addresses referenced results branches jumps 
greedy scheduling algorithm guided configurable oracle packs instructions sequence pending cycles 
resulting sequence cycles represents hypothetical execution program multiple issue machine 
dividing number instructions executed number cycles required gives average parallelism 
configurable oracle models particular combination techniques find enhance instruction level parallelism 
scheduling exploit parallelism constrained possibility dependencies instructions 
instructions dependency changing order changes effect changes data values instruction execution conditional 
trademark mips computer systems case simulating conventional instruction level simulator 
limits instruction level parallelism source lines instructions executed remarks egrep file search sed stream editor yacc compiler compiler eco recursive tree comparison grr pcb router timing verifier alvinn neural network training compress lempel ziv file compaction doduc simulation espresso boolean function minimizer fpppp quantum chemistry benchmark gcc pass gnu compiler hydro astrophysical simulation li lisp interpreter mdljsp molecular dynamics model ora ray tracing swm shallow water simulation tomcatv vectorized mesh generation eighteen test programs illustrates different kinds dependencies 
dependencies real reflecting true flow computation 
false dependencies accidents code generation lack precise knowledge flow data 
instructions true data dependency result operand second 
instructions uses old value location second sets location new value 
similarly instructions output dependency assign value location 
control dependency branch instruction execution conditional 
oracle uses actual program trace decisions 
lets predict basing scheduling decisions particular branch taken load store refer memory location 
construct perfect schedule constrained true data dependencies instructions provide insight real machine perform 
interesting oracle ways approximate capabilities real machine real compiler system 
configure oracle different levels expertise ranging nil perfect different kinds parallelism enhancement 
limits instruction level parallelism true data dependency 
register renaming output dependency 
dependencies anti dependency goto 
control dependency anti dependencies output dependencies registers accidents compiler register allocation technique 
figures different register new value second instruction remove dependency 
register allocation integrated compiler instruction scheduler beh gh eliminate 
current compilers exploit preferring reuse registers possible number registers needed minimized 
alternative hardware solution register renaming hardware imposes level indirection register number appearing instruction actual register 
time instruction sets register hardware selects actual register long value needed 
sense hardware register allocation dynamically 
register renaming additional advantage allowing hardware include registers fit instruction format reducing false dependencies 
kinds register renaming perfect finite 
perfect renaming assume infinite number registers false register dependencies occur 
finite renaming assume finite register set dynamically allocated lru discipline need new register select register measured cycles instruction count earliest 
finite renaming works best course lot registers 
simulations integer registers floating point registers interesting see happens reduce number base machine 
renaming simply registers specified code works course highly dependent register strategy compiler 
alias analysis limits instruction level parallelism registers memory locations carry true false dependencies 
assumption renaming memory locations option reasons 
memory larger register file renaming quite expensive 
important memory locations tend quite differently registers 
putting value memory location normally meaning logic program memory just scratchpad extent registers 
hard just telling memory carried dependency exists 
registers instruction manifest instruction memory location manifest fact may different different executions instruction 
multiple issue machine may forced assume dependency exists 
aliasing problem telling memory access memory location 
hardware mechanisms loads suggested help cope aliasing problem 
conventional approach compiler perform alias analysis knowledge semantics language program rule dependencies 
system provides levels alias analysis 
assume perfect alias analysis look actual memory address referenced load store store conflicts load store access location 
assume alias analysis store conflicts load store 
extremes alias analysis smart vectorizing compiler 
don compiler implemented intermediate schemes may give insight 
intermediate scheme alias instruction inspection 
common technique compile time instruction level code schedulers 
look instructions see obvious independent ways happen shown 
fp gp alias analysis inspection instructions conflict base register different displacements 
instructions conflict base registers show refers stack global data area 
intermediate scheme called alias analysis compiler compiler doesn 
model assume perfect analysis stack global regardless registers 
store address stack conflicts load store address 
heap hand resolved instruction inspection 
limits instruction level parallelism idea model alias analysis compiler outside heap resolved compiler doing dataflow dependency analysis loops arrays heap tractable 
assumptions particularly defensible 
languages allow pointers stack global areas rendering difficult heap 
practical considerations separate compilation may keep analyzing non heap perfectly 
side heap may hopeless model assumes cwz hhn jm lh 
range alternatives provide intuition effects alias analysis instruction level parallelism 
branch prediction parallelism basic block usually quite limited mainly basic blocks usually quite small 
approach speculative execution tries mitigate scheduling instructions branches 
hard don know way branches go path select instructions 
worse branches go way part time branch may followed possible code paths 
move instructions path point branch instructions harm harm undone take path 
may involve maintaining shadow registers values committed sure correctly predicted branch 
may involve selective instructions choose may willing execute memory stores speculatively example instructions raise exceptions 
may put partly compiler control designing instruction set explicitly instructions 
instruction tied explicitly condition evaluated instruction squashed hardware condition turns false 
compiler schedules instructions speculatively may insert code undo effects entry path 
common approach speculative execution uses branch prediction 
hardware software predicts way branch go speculatively schedules instructions path 
common hardware technique branch prediction ls smi maintains table bit counters 
low order bits branch address provide index table 
branch causes increment table entry causes decrement 
bit counters saturating wrap table entry reaches maximum minimum 
predict branch taken table entry 
bit prediction scheme typical loop exited 
branches map table entry interfere key identifies owner entry 
initial value table entries just barely predicting branch taken 
shows bit counter scheme works different table sizes eighteen programs test suite 
programs prediction success levels time table bit entries 
increasing number bits making counters bigger having little effect 
branches predicted statically comparable accuracy obtaining branch profile prediction success rate limits instruction level parallelism harmonic mean tomcatv swm alvinn sed doduc yacc mdljsp fpppp met hydro eco gcc egrep compress li espresso ora grr fraction branches predicted correctly bit counter prediction function total number bits predictor tells branch fraction executions taken 
profile branch profile obtained inserting counting code test program keep track times branch goes way 
branch profile seeing way branch goes scheduling instructions path 
expense undoing speculative execution branch goes way impose threshold don move instructions branch executed time 
studies explored sophisticated hardware prediction branch histories psr yp yp 
approaches maintain tables relating history branch branches program outcome branch 
approaches quite poorly small tables bit counter schemes benefit larger predictors 
example local history predictor yp 
maintains table bit shift registers indexed branch address 
branch taken shifted table entry branch shifted 
predict branch take bit history index table bit counters simple counter scheme described 
counter predict taken predict taken 
prediction proves correct increment counter decrement 
local history predictor works branches display regular pattern small period 
behavior branch correlated behavior 
predictor yp tries exploit effect 
replaces table shift registers single shift register records outcome executed branches uses history pattern index table counters 
allows exploit correlations behaviors nearby branches allows history longer predictor size 
prediction success rate limits instruction level parallelism loc gsh ctr gsh counter fraction branches predicted correctly different prediction schemes function total number bits predictor interesting variation gshare predictor mcf uses identity branch global history 
indexing array counters just global history register gshare predictor computes global history branch address 
mcfarling mcf got better results table bit counters dynamically choose different schemes running competition 
predictor prediction usual branch address select bit counter selector table selector value prediction second 
branch outcome known selector incremented decremented exactly predictor correct 
approach lets predictors compete authority branch awards authority predictor correct 
mcfarling combined predictors simpler schemes predictor size small quite large 
shows success rate different hardware predictors study averaged eighteen programs suite 
traditional bit counter approach described 
second combination bit counter predictor gshare predictor twice elements selector table size counter predictor 
third combination local predictor gshare predictor local tables gshare table selector table number elements 
axis graph total size predictor bits 
simple counter predictor works best small sizes bimodal gshare predictor takes lead large predictors local gshare predictor works best delivering accuracy limit 
simulator modeled degrees branch prediction 
extreme perfect prediction assume branches correctly predicted 
assume limits instruction level parallelism ctrs ctrs ctrs ctrs ctr gsh loc gsh loc gsh prof sign taken kb kb egrep sed yacc eco grr alvinn compress doduc espresso fpppp gcc hydro li mdljsp ora swm tomcatv success rates different branch prediction techniques hardware prediction schemes shown predictor size 
assume kinds static branch prediction profiled branch prediction predict branch go way went frequently profiled previous run signed branch prediction predict backward branch taken forward branch taken branch prediction predict branch taken 
assume branch prediction occurs assuming branch predicted wrong 
shows actual success rate prediction different sizes tables 
shows success rates kinds static prediction 
profiled prediction routinely beats bit counter prediction compete larger advanced techniques 
signed taken prediction quite poorly smallest dynamic tables taken prediction slightly better 
signed prediction lends better compiler technique moving little pieces conditionally executed code normal code stream improving program locality cache performance 
effect branch prediction scheduling easy state 
correctly predicted branches effect scheduling register dependencies involving operands 
instructions appearing mispredicted branch scheduled branch know scheduling find branch went way 
course branch instruction may scheduled instructions precede branch dependencies permit 
note generally assume penalty failure inability schedule instructions branch 
assumption optimistic real architectures failed prediction causes bubble pipeline resulting cycles execution whatsoever occur 
return topic 
branch fanout limits instruction level parallelism try predict destinations branches speculatively execute instructions possible paths squashing wrong path know hardware parallelism capability guaranteed wasted completely blindly wrong path 
unfortunately branches happen quite normal code large degrees parallelism may encounter branch resolved previous 
continue fan indefinitely eventually machine parallelism just exploring parallel paths right 
alternative branch probability available profile explore paths branch probability near explore path probability near 
system allows scheduler explore directions past branches 
scheduler working trace schedule instructions paths taken 
false paths hardware parallelism model assuming upper limit number branches look past 
call upper limit fanout limit 
terms simulator scheduling branches explore paths simply considered correctly predicted effect schedule identical course part fanout limit 
respects fanout duplicates benefits branch prediction effect 
dynamic branch prediction explore paths fanout limit explore predicted path point 
static branch prediction profile go 
easy implement profiler tells direction branch went frequency went way 
lets explore predicted path predicted probability threshold limited fanout ability explore paths probability threshold 
indirect jump prediction architectures kinds instructions change flow control 
branches conditional destination specified offset pc 
jumps unconditional may direct indirect 
direct jump destination explicitly instruction indirect jump destination expressed address computation involving register 
principle know destination direct jump advance 
destination indirect jump may require wait address computation possible 
predicting destination indirect jump pay instruction level parallelism 
consider jump prediction strategies simultaneously 
strategy simple scheme 
table maintained destination addresses 
address jump provides index table 
execute indirect jump put destination address table entry jump 
predict jump extract address table entry 
predict indirect jump go went time 
branch prediction prevent jumps mapping limits instruction level parallelism element return ring ring plus element table prof egrep sed yacc eco grr met alvinn compress doduc espresso fpppp gcc hydro li mdljsp ora swm tomcatv success rates different jump prediction techniques table entry interfering 
second strategy involves procedure returns common kind indirect jump 
machine distinguish returns indirect jumps better job predicting destinations follows 
machine maintains small ring buffer return addresses 
executes subroutine call instruction increments buffer pointer enters return address buffer 
return instruction predicted go address buffer decrements buffer pointer 
tail call optimization setjmp longjmp prediction right machine uses big ring buffer 
distinguish returns indirect jumps predominance worth predicting indirect jump return long decrement buffer pointer prediction succeeds 
system allows degrees kind jump prediction 
assume indirect jumps perfectly predicted 
prediction predict jump go went time table size 
subroutine returns predicted table return ring desired size 
predict returns return ring leave indirect jumps unpredicted 
assume jump prediction whatsoever 
branches correctly predicted jump effect scheduling 
mispredicted unpredicted jump may moved earlier instructions instruction moved jump 
shows actual success rates jump prediction return ring return ring destination table prediction common destination profile 
element return ring predict half indirect jumps slightly larger ring raises 
adding small destination table limits instruction level parallelism predict non returns produces substantial improvement success rate rise table bigger 
element return ring element table predict indirect jumps 
common destination profile contrast succeeds thirds time 
window size cycle width window size maximum number instructions appear pending cycles time 
default instructions 
manage window discretely continuously 
discrete windows fetch entire window instructions schedule cycles issue cycles start fresh new window 
missed prediction causes start full size new window 
continuous windows new instructions enter window time old cycles leave window number instructions reaches window size 
continuous windows norm results described implement hardware difficult 
smith assumed discrete windows 
cycle width maximum number instructions scheduled cycle 
default 
greedy scheduling algorithm works cycle width large small proportion cycles completely filled 
cycle widths traditional approach hg jm realistic 
cycles fixed finite size specify cycles unlimited width 
case effective limit imposed window size cycle contains window full instructions issued new cycle begun 
final option allow cycle width window size unlimited 
latency experiments assumed operation unit latency result computed cycle operand cycle 
obviously accomplished setting machine cycle time high slowest operations finish cycle general inefficient machine 
real machine cycle time long finish common operations integer add operations division multiplication floating point operations memory loads take cycle complete 
operation cycle latency result cycle earlier defined parallelism number instructions executed divided number cycles required 
adding non unit latency requires refine definition slightly 
want measure parallelism give proper credit scheduling quick operations times waiting unrelated slow ones 
define total latency program sum latencies instructions executed parallelism total latency divided final possibility limited cycle width unlimited window size implemented data structure attain size proportional length instruction trace 
deemed impractical implement 
limits instruction level parallelism multi cycle instruction critical path multi cycle instruction critical path effects increasing latency parallelism model model model model model int add sub logical load int mult int div single prec add sub single prec mult single prec div double prec add sub double prec mult double prec div operation latencies cycles latency models number cycles required 
instructions latency total latency just number instructions definition 
notice non unit latencies possible instruction level parallelism exceed cycle width time working instructions issued different cycles different stages execution pipeline 
obvious increasing latencies operations tend increase decrease instruction level parallelism 
illustrates opposing effects 
divide instruction critical path increase latency spend cycles working parallelism decrease 
contrast divide critical path increasing latency increase parallelism 
note parallelism increases nearly certain time required increase latency means decreased cycle time 
implemented different latency models 
lists 
assume functional units completely pipelined multi cycle instructions issued cycle result available cycles 
latency model unit latencies default specified 
stupid poor fair great superb perfect results branch predict counter kb ctr gsh kb loc gsh kb loc gsh fanout kb loc gsh perfect limits instruction level parallelism jump predict addr ring table addr ring addr table addr ring addr table perfect register renaming perfect alias analysis inspect perfect increasingly ambitious models stupid poor fair great superb perfect ran eighteen test programs wide range configurations 
show interesting trends complete results appear appendix 
provide framework exploration defined series increasingly ambitious models spanning possible range 
specified window size instructions cycle width instructions unit latencies assumed 
results show effects variations standard models 
note poor model fairly ambitious assumes rudimentary alias analysis branch predictor correct average models allows generous default window size cycle width 
parallelism limits instruction level parallelism harmonic mean fpppp tomcatv doduc egrep swm hydro espresso gcc mdljsp grr compress sed met yacc eco li ora alvinn stupid poor fair great superb perfect stupid poor fair parallelism models full scale left detail right parallelism models shows parallelism program models 
numeric programs shown dotted lines harmonic mean series circles 
unsurprisingly stupid model rarely exceeds exceeds numeric programs 
lack branch prediction means finds intra block parallelism lack renaming alias analysis means won find 
moving poor helps worst programs quite lot entirely branch prediction mean 
moving fair increases mean mainly suddenly assume perfect alias analysis 
model doubles mean parallelism introduces register renaming 
increasing number available registers great model takes proportional improvement smaller 
point effectiveness branch prediction add way branch fanout great model get superb model 
performance disappointing hoped improvement 
parallelism superb model half perfect model mainly imperfection branch prediction 
study perfect model lead dangerous garden path study included fpppp tomcatv 
tomcatv swm fpppp hydro doduc met mdljsp li sed ora yacc espresso grr eco gcc egrep compress alvinn parallelism tomcatv swm ora time limits instruction level parallelism mdljsp alvinn 
tomcatv swm ora mdljsp alvinn 
time parallelism model intervals cycles left cycles right effects measurement interval analyzed parallelism entire program executions avoided question constitutes representative interval 
select smaller interval time random run risk interval atypical program execution 
select particular interval program parallel misleading 
shows parallelism model successive intervals execution longer running programs 
left hand graph uses intervals cycles right hand graph cycles 
case parallelism interval computed exactly program number instructions executed interval divided number cycles required 
test programs quite stable parallelism 
quite unstable 
cycle intervals range instructions parallelism single program vary widely factor 
cycle intervals see variation factor 
alvinn program parallelism point drops half contrast swm program starts quite low climbs quite respectable number 
clear intervals cycles excessive selected care 
parallelism measurements isolated intervals fewer cycles viewed suspicion 
parallelism parallelism limits instruction level parallelism harmonic mean tomcatv doduc fpppp egrep hydro swm gcc mdljsp grr sed met yacc eco li ora alvinn stupid poor fair great superb perfect stupid poor fair great superb perfect ratio default parallelism models cycle width instructions left ratio parallelism cycles parallelism cycles right doduc tomcatv fpppp hydro egrep swm espresso gcc mdljsp grr compress sed met yacc eco li ora alvinn stupid poor fair great superb perfect stupid poor fair great superb perfect ratio default parallelism models unlimited cycle width left ratio parallelism unlimited cycles parallelism cycles right tomcatv doduc hydro fpppp swm egrep gcc yacc mdljsp grr doduc tomcatv hydro fpppp swm egrep gcc yacc mdljsp grr effects cycle width limits instruction level parallelism tomcatv fpppp attain high parallelism modest machine models 
average parallelism close maximum imposed normal cycle width instructions great model half cycles completely full 
suggests parallelism obtained widening cycles 
shows happens increase maximum cycle width instructions 
right hand graph shows parallelism increases go cycles instructions cycles 
doubling cycle width improves numeric programs appreciably perfect model improves tomcatv great model 
programs benefit appreciably wide cycles perfect model 
problem instruction cycles small 
remove limit cycle width altogether effectively cycle width window size case instructions 
results shown 
parallelism perfect model bit better outside perfect model see tomcatv benchmark benefit significantly 
cycle width instructions quite lot consider smaller cycles 
required replace quick easy greedy scheduling algorithm slower conventional scheduling technique gm hg limiting programs run completion 
techniques schedule static block instructions obvious extend continuous windows model 
parallelism parallelism limits instruction level parallelism fpppp tomcatv swm doduc espresso hydro met sed grr li egrep mdljsp gcc yacc ora eco compress alvinn parallelism different sizes continuously managed windows superb model left fair model right tomcatv fpppp swm doduc espresso hydro met sed li grr egrep mdljsp gcc yacc ora eco compress alvinn parallelism different sizes discretely managed windows superb model left fair model right hydro tomcatv sed met yacc li doduc compress ora eco egrep grr espresso gcc fpppp swm alvinn mdljsp hydro tomcatv sed met yacc li doduc compress ora eco egrep grr espresso gcc fpppp swm alvinn mdljsp parallelism limits instruction level parallelism swm hydro tomcatv doduc alvinn yacc egrep fpppp espresso gcc mdljsp grr compress met sed eco li ora stupid poor fair great superb perfect stupid poor fair great superb perfect ratio default parallelism models unlimited window size cycle width left ratio parallelism unlimited windows cycles parallelism instruction windows instruction cycles right effects window size standard models window size instructions scheduler allowed keep instructions pending cycles time 
typical superscalar hardware handle windows size software techniques trace scheduling vliw machine 
shows effect varying window size instructions superb fair models 
superb model programs instruction window larger 
parallelism drops quickly 
poor model limited analysis severely restricts mobility instructions parallelism levels window size instructions 
ambitious parallelism manager manage windows discretely getting window full instructions scheduling relative executing starting fresh window 
tend result lower parallelism continuous window model 
shows models assumes discrete windows continuous 
expect discretely managed windows need larger best curves don level window size instructions superb instructions poor reached 
continuous windows sizes instructions necessary 
small windows continuous management multiplying window size 
eliminate limit window size cycle width get results shown 
see kind high parallelism reported studies nicolau alvinn swm yacc hydro tomcatv doduc gcc egrep grr fpppp mdljsp met li eco limits instruction level parallelism fisher nf reaching high swm perfect model 
interesting note slightly realistic models maximum parallelism drops mean parallelism 
advantage unlimited window size cycle width outside perfect model shows tomcatv advantage modest 
effects loop unrolling loop unrolling old compiler optimization technique increase parallelism 
unroll loop times removing branches effectively increase basic block size tenfold 
larger basic block may hold parallelism unavailable branches inherent sequentiality loop control 
studied parallelism unrolled code manually unrolling inner loops programs 
case loops constituted sizable fraction original program total runtime 
displays details 
alvinn inner loops accumulator loop iteration computes value iteration depends values successively added single accumulator variable 
parallelize loop duplicated loop body times successively replaced occurred collapsed assignments accumulator single assignment restructured resulting large right hand side balanced tree expression 
alvinn swm tomcatv procedure loop location type loop instrs execution input hidden hidden input calc main line backprop line backprop line swm line tomcatv accumulator independent independent independent unrolled loops remaining loops perfectly parallelizable iteration completely independent rest 
unrolled different ways 
simply duplicated loop body times replacing 
second duplicated loop body times changed assignments array members assignments local scalars followed moves scalars array members moved assignments array members loop 
model second simple models poor alias analysis array loads successive unrolled iterations separated array stores difficult tell safe interleave parts accum accum limits instruction level parallelism sophisticated models simple models accum loop accum accum techniques loop unrolling successive iterations 
hand leaving stores place means lifetimes computed values shorter allowing compiler better job register allocation moving stores loop means compiler start memory locations temporaries removes values control register renaming facility available smarter models 
shows examples transformations 
followed unrolled loop copy original loop starting unrolled loop left finish cases loop count multiple unrolling factor 
fact little difference methods poorer models differences better models direction 
results reported larger parallelism obtained different methods 
shows result 
unrolling profound difference alvinn better models parallelism limits instruction level parallelism effects loop unrolling tom perf swm perf alv perf tom swm alv tom swm alv effect decreased unrolling factor increased 
little difference cases hurt parallelism instances 
difference probably inner loop alvinn quite short replicated times creating great register pressure giving compiler balls juggle 
quite possible parallelism go performance goes 
rolled loop loop bookkeeping instructions parallel meat loop body unrolled loop gets rid half bookkeeping 
unrolling creates new opportunities parallelism course point cause net parallelism decrease 
loop unrolling way increase available parallelism silver bullet 
effects branch prediction saw earlier new techniques dynamic history branch prediction allow benefit quite large branch predictor giving success rates improving slightly megabit predictor 
natural ask affects instruction level parallelism 
answers question fair great models 
fair model relatively insensitive size predictor tiny bit predictor improves mean parallelism 
tiny bit table doubles parallelism great model increasing huge quarter megabit table doubles 
great model parallel programs quite insensitive size predictor 
exactly programs conditional branches account instructions executed nearest contender doduc 
mask effect plotting results function predictor size function parallelism parallelism limits instruction level parallelism hydro sed tomcatv met yacc li egrep doduc compress ora eco grr gcc espresso fpppp swm alvinn mdljsp parallelism different sizes dynamic table fair model left great model right hydro sed tomcatv met yacc li egrep doduc compress ora eco grr gcc espresso fpppp swm alvinn mdljsp parallelism function mean number instructions mispredicted branches fair model left great model right fpppp tomcatv swm doduc hydro espresso met sed li mdljsp egrep yacc ora grr gcc eco compress alvinn fpppp tomcatv swm doduc hydro espresso met sed li mdljsp egrep yacc ora grr gcc eco compress alvinn parallelism parallelism limits instruction level parallelism tomcatv fpppp swm doduc mdljsp hydro met espresso ora li sed grr gcc compress yacc eco egrep alvinn parallelism great model different levels fanout scheduling conditional branches branch prediction left branch prediction right fanout exhausted hydro sed compress tomcatv met egrep yacc li doduc ora grr gcc eco espresso fpppp swm mdljsp alvinn parallelism fair model different levels fanout scheduling conditional branches branch prediction left branch prediction right fanout exhausted fpppp tomcatv swm doduc espresso hydro met grr sed egrep li mdljsp gcc yacc compress ora eco alvinn hydro sed compress yacc tomcatv met li egrep doduc ora grr gcc eco espresso fpppp swm mdljsp alvinn limits instruction level parallelism average number instructions executed mispredicted branches 
shows results 
numeric programs stand anomalies evidently involved parallelism predictability branches 
effects fanout figures show effects adding various levels fanout great fair models 
left hand graphs assume look paths conditional branches fanout limit look past branches point 
right hand graphs assume reach fanout limit dynamic prediction great fair level look instructions predicted path schedule 
graph leftmost point represents fanout 
see fanout followed branch prediction fanout buy 
branch prediction hand modest amounts fanout quite rewarding adding fanout branches fair model adding fair branch prediction 
fisher fis proposed fanout conjunction profiled branch prediction 
scheme software control profile gives information helps decide explore branch fanout capability prediction 
possible profile easily record just way branch goes 
fisher combines profile information static scheduling information payoff scheduling instruction early assumption branch goes direction 
traces payoff information fisher uses investigate simpler variation idea 
pick threshold partition branches classes predict go way particular fan 
call scheme profile guided integrated prediction fanout 
modified perfect model profile guided integrated prediction fanout 
shows parallelism different threshold levels 
setting threshold low means profile predict branches rarely benefit fanout threshold causes branches predicted fanout 
setting threshold high means fan branches nearly go way wasting hardware parallelism fanout enables 
high threshold better branches really go way essentially time 
benefit sensitive threshold curves quite flat holds experiment great model fair model 
best threshold 
shows parallelism variations fair superb models profile integrated scheme hardware approach fanout followed prediction 
profile guided integration works simple hardware approach fair model spite fact fair model predictor better profile predictor 
better hardware branch prediction superb model completely profile integrated approach 
parallelism limits instruction level parallelism fpppp tomcatv swm doduc mdljsp hydro sed li met gcc eco ora espresso yacc alvinn grr compress egrep parallelism perfect model profile guided integrated prediction fanout fanout left fanout right fair superb fanout fanout fanout fanout hardware hardware hardware hardware egrep sed yacc eco grr alvinn compress doduc espresso fpppp gcc hydro li mdljsp ora swm tomcatv har 
mean parallelism fair superb models fanout profile guided integrated fanout prediction hardware technique fpppp tomcatv swm doduc mdljsp hydro li sed gcc met espresso eco yacc ora grr compress alvinn egrep parallelism limits instruction level parallelism fpppp tomcatv swm doduc espresso hydro met mdljsp egrep yacc ora li grr gcc sed eco compress alvinn ring ring ring ring ring inf ring ring ring ring ring ring inf ring parallelism varying sizes return prediction ring jump prediction great model left perfect model right effects jump prediction subroutine returns easy predict return ring technique discussed section 
shows effect parallelism different sizes return ring jump prediction 
leftmost point return ring entries means jump prediction 
small return prediction ring improves programs lot great model 
large return ring better small 
predict indirect jumps returns previous destinations predicting go time 
shows effect predicting returns element return ring indirect jumps table 
mean behavior quite flat table size increases handful programs benefit noticeably small table 
tomcatv fpppp egrep doduc swm espresso mdljsp hydro grr compress yacc met sed gcc eco li ora alvinn parallelism limits instruction level parallelism fpppp tomcatv swm doduc hydro espresso met sed li mdljsp egrep yacc ora grr gcc eco compress alvinn tab tab tab tab tab tab tab tab tab tab tab tab parallelism jump prediction huge return ring destination cache table various sizes great model left perfect model right effects penalty misprediction branch jump prediction little effect parallelism may worthwhile include 
pipelined machine branch jump predicted incorrectly results bubble pipeline 
bubble series cycles execution occur correct instructions fetched decoded started execution pipeline 
size bubble function pipeline granularity applies prediction done hardware software 
penalty serious effect performance 
shows degradation parallelism poor models assuming mispredicted branch jump adds cycles instructions 
poor model deteriorates quickly limited branch prediction jump prediction 
model affected prediction better 
poor model negative effect misprediction greater positive effects resulting parallelism 
multiple issue course behavior worse 
parallel numeric programs stay relatively horizontal entire range 
shown fewer branches jumps comparatively predictable 
increasing penalty degrades programs relatively jumps tomcatv fewer instruction indirect jump 
programs high misprediction penalty result speedups negligible non bubble cycles highly parallel 
fpppp tomcatv doduc egrep swm espresso hydro mdljsp grr compress sed met yacc gcc li eco ora alvinn parallelism limits instruction level parallelism tomcatv fpppp swm doduc hydro mdljsp alvinn ora yacc egrep sed compress espresso met grr eco gcc li parallelism function misprediction penalty poor model left model right branches jumps egrep yacc sed eco grr met alvinn compress doduc espresso fpppp gcc hydro li mdljsp ora swm tomcatv dynamic ratios conditional branches indirect jumps instructions tomcatv swm fpppp doduc ora met mdljsp hydro sed alvinn yacc espresso li eco grr egrep gcc compress parallelism limits instruction level parallelism tomcatv fpppp swm hydro doduc met mdljsp li sed ora yacc espresso grr eco gcc egrep compress alvinn comp perf comp perf parallelism different levels alias analysis model left superb model right effects alias analysis shows alias analysis inspection better rarely increased parallelism quarter 
alias analysis compiler definition identical perfect alias analysis programs heap 
programs heap improved parallelism superb model alias analysis inspection 
perfect analysis improved programs percent alias analysis compiler suggesting payoff results heap disambiguation 
fpppp tomcatv swm doduc espresso hydro met sed grr li egrep mdljsp gcc yacc ora eco compress alvinn parallelism limits instruction level parallelism fpppp tomcatv swm mdljsp doduc hydro espresso met sed li yacc ora grr gcc egrep eco alvinn compress perfect perfect parallelism parallelism different numbers registers model left superb model right effects register renaming shows effect register renaming parallelism 
dropping infinitely registers cpu fpu little effect parallelism non numerical programs numerical programs suffered noticeably 
badly 
situation actual number decstation code targeted interesting 
adding renaming improve parallelism fact degraded cases 
real registers hardware dynamic renaming offers little reasonable static allocator 
fpppp tomcatv swm doduc mdljsp hydro espresso met sed li grr egrep gcc yacc ora eco compress alvinn parallelism limits instruction level parallelism tomcatv swm doduc fpppp egrep hydro espresso gcc mdljsp grr compress sed met yacc eco li alvinn ora stupid poor fair great superb perfect stupid poor fair great superb perfect parallelism standard models latency model left latency model right effects latency shows parallelism models latency models discussed section increasing latency operations act increase parallelism decrease 
fact surprisingly little difference graphs default unit latency model looks picture direction considering effect changing latency model keeping rest model constant 
bulk programs insensitive latency model increased decreased parallelism greater latencies 
doduc fpppp interesting 
latencies increase parallelism oscillates decreasing increasing decreasing 
behavior probably reflects limited nature assortment latency models represent points single spectrum small sample vast multi dimensional space path represent space bit 
superscalar processing vector processing scalar programs appears truth claim 
nontrivial currently known techniques consistently got parallelism programs test suite 
vectorizable nearly vectorizable programs went higher 
speculative execution driven branch prediction critical exploitation tomcatv swm doduc hydro fpppp egrep espresso gcc mdljsp met grr yacc compress sed li eco alvinn ora parallelism limits instruction level parallelism tomcatv swm fpppp hydro sed met yacc li espresso doduc mdljsp egrep grr gcc eco compress alvinn ora parallelism model left superb model right different latency models modest amounts instruction level parallelism 
start perfect model remove branch prediction median parallelism removing alias analysis register renaming jump prediction results acceptable median parallelism respectively 
fortunately branch prediction hard 
mean time misses multiplied bit prediction modestly sized table software profiling 
obtain factor mean time misses willing devote large chip area predictor 
complementing branch prediction simultaneous speculative execution different branching code paths cake raising observed parallelism 
fact parallel exploration depth branches remove need prediction altogether probably economical substitute practice 
numbers grounds optimism remember result optimistic assumptions 
assumed unlimited resources including copies functional unit need perfect memory system cache misses 
duplicate functional units take chip real estate better spent chip cache especially processors get faster memory bottleneck gets worse 
part assumed penalty pipeline refill cycles undo catch instructions missed prediction 
wasted cycles lower parallelism cycles full reducing advantage instruction parallel architecture 
assumed machines modeled cycle time adding superscalar capability surely decrease cycle time may fact increase 
assumed machines built comparable swm tomcatv fpppp hydro mdljsp espresso doduc sed met li egrep yacc grr gcc compress eco alvinn ora limits instruction level parallelism technology simpler machine may shorter time market advantage newer faster technology 
considerations reduce expected payoff instruction parallel machine third eliminate completely 
appendix 
implementation details parallelism simulator conceptually simple 
sequence pending cycles contains sets instructions issued 
obtain trace instructions executed benchmark place successive instruction earliest cycle consistent model 
cycle full total number pending instructions exceeds window size cycle retired 
new instruction placed latest pending cycle create new cycle 
simple algorithm take new instruction consider instruction pending cycle starting latest cycle moving backward time 
find dependency place instruction cycle 
proper cycle full place instruction non full cycle 
typically considering models windows thousands instructions doing linear search instruction trace quite expensive 
solution maintain sort reverse index 
number new cycle consecutively maintain data structure individual dependencies instruction instruction 
data structure tells cycle number instruction cause kind dependency 
schedule instruction consider dependency look cycle number barrier imposed dependency 
latest barriers tells put instruction 
update data structure needed reflect effects instruction ones 
simple examples idea clear 
assignment register exchanged register maintain timestamp register 
instruction assigns register updates register timestamp instruction uses register knows instruction scheduled timestamp conflict register 
similarly model specifies alias analysis maintain timestamp memory updated store instruction new load store scheduled timestamp 
hand model specifies perfect alias analysis instructions conflict refer location memory maintain separate timestamp word memory 
examples complicated 
descriptions follow parts data structure code performed schedule instruction code performed update data structure 
final points worth making plunge details 
parallelism simulator responsible orchestrating execution 
simply consuming trace information available real system 
example perfect alias analysis simply determining memory location accessed scheduling instruction pending cycle known 
mod index limits instruction level parallelism number range obtained subtracting multiple mod byte address index table size elseif elseif bit mod mod bump schedule instruction cycle applying constraints tells earliest possible time 
maintain timestamp latest occurrence event latest far new occurrence possibly abbreviations implementation descriptions second algorithm simulator temporally backwards 
real multiple issue machine particular cycle looking ahead possible instructions decide execute 
simulator hand keeps collection cycles pushes instruction order single issue trace far back time legally 
implementation configurations unintuitive particularly fanout imperfect alias analysis 
third architecture requires op instructions inserted load delay branch delay filled useful 
means instructions executed ops 
ops artificially inflate program parallelism schedule ops pending cycles count instructions executed 
consider various options turn 
describing implementations different options abbreviations helpful 
data structure limits instruction level parallelism cycle jump machine mispredicted ring return ring index ring destination table frequent destination jump address schedule scheduling instruction instruction call mod ring instruction indirect jump addr address jump destination address jumped trace perfect false instruction return ring destination mod true addr index destination destination addr destination true bump jump prediction jump prediction simulator ways predicting indirect jumps hardware 
return ring destination table 
supports software jump prediction profile 
case successfully predicted jump direct jump instructions jump trace move freely jump absent 
mispredicted jump may move earlier instructions instructions scheduled mispredicted jump 
trace tells indirect jump went simulator simply prediction algorithm model specifies checks see right 
limits instruction level parallelism mispredicted jump affects subsequent instructions suffices single timestamp 
destination table containing code addresses model specifies table size 
instruction trace indirect jump look table entry index word address jump modulo 
predict jump code address table entry 
jump replace table entry actual destination 
return ring circular containing code addresses model specifies ring size 
index ring range gives zero decrementing 
instruction trace return predict destination 
instruction trace call store return address call 
instruction set explicit return instruction 
indirect jump certainly return compilers libraries return register allowed 
identify return register fact return ring correctly predicts 
realistic course return ring assumes returns identified compiler analysis specific return instruction don want model handicapped detail 
jump profile table obtained identical previous run entry indirect jump program telling address frequent destination jump 
predict jump looking entry table successful actual destination 
leaves trivial cases perfect jump prediction jump prediction 
case ignore trace says simply assume success failure 
branch prediction fanout branch prediction analogous jump prediction 
correctly predicted conditional branch just unconditional branch instructions branch move freely branch 
jumps instruction may moved incorrectly predicted branch 
possibility fanout affects consider incorrectly predicted branch 
simple counter predictor uses containing counter range model specifies table size 
bit counters saturating incrementing gives decrementing gives 
instruction trace conditional branch look counter index word address branch 
counter predict branch taken 
branch increment counter branch really taken decrement subject saturation case 
hardware predictors combined techniques similarly 
branch profile table obtained identical previous run entry conditional branch program telling fraction times branch executed taken 
fraction half predict branch taken data structure limits instruction level parallelism cycle branch machine mispredicted bit counters counter predictor bit counters gshare predictor bit counters local predictor select bit counters choosing combined predictor log bit history shift register log bit history shift registers frequency branch address taken queue cycles previous branches schedule scheduling instruction instruction conditional branch addr address branch address branch go taken taken trace says branch taken true taken taken addr counters addr index pred pred taken taken elsif counters gshare addr index pred taken xor addr index pred taken bit taken mod addr index pred select pred pred pred taken select taken pred taken pred elsif local gshare addr index pred taken bit taken mod xor addr index pred taken bit taken mod addr index pred select pred pred pred taken select taken pred taken pred branch prediction limits instruction level parallelism taken addr false addr threshold addr threshold remove head append bump head elsif fill elements bump remove head append bump head continued predict taken 
model specifies signed branch prediction see address branch address possible destination 
forward branch predict taken backward branch predict taken 
model specifies taken branch prediction predict branch taken 
successful 
perfect branch prediction branch prediction trivial simply assume successful unsuccessful 
model include fanout deal success failure just jumps 
successfully predicted branch allows instructions moved back time mispredicted branch acts barrier preventing instructions moved 
model includes fanout situation little complicated 
assume cycle hypothetical multiple issue machine looks ahead possible paths past conditional branches encounters point looks path form branch prediction specified 
branch may fanout branch predicted branch depending far ahead machine looking encounters branch 
simulator temporally backward point view means tentatively predict branch 
move instruction backward number successfully predicted branches followed branches predicted successfully 
words unsuccessfully predicted branch means barrier situated cycle 
notice iff branch branch reduce previous case 
maintain containing timestamps previous limits instruction level parallelism branches 
instruction trace branch remove element append timestamp new branch 
branch prediction successful 
unsuccessful impose barrier cycle timestamp element 
profile guided integration prediction fanout works little differently 
instruction conditional branch look profile find probability taken 
probability taken taken greater model threshold call predictable branch predict taken taken 
call unpredictable branch level fanout 
note terminology independent predicted correctly instance 
simulator point view means move backward past number predictable branches successfully predicted interspersed branches 
fanout branch try unsuccessfully predict 
maintain queue time advance unpredictable branches 
tentatively predict branch 
mispredict unpredictable branch impose branches back head tells 
mispredict predictable branch impose barrier current branch profile try fanout branch 
register renaming limits instruction level parallelism idea register renaming reduce eliminate false dependencies arise re registers 
instructions appear small set registers implied size register specifier registers really register names acting placeholders probably larger set actual registers 
allows instructions anti dependency executed order actual registers different 
instruction assigns register name allocate actual register hold value 
actual register associated register name instruction assigns new value register name 
point know actual register fact dead return pool available registers 
keep track cycle available register allocate available registers need new lets instructions pushed far back time possible 
instruction uses registers look names mapping find actual registers 
instruction sets register allocate available actual available earliest cycle 
update mapping reflect allocation 
means register rr previously mapped register name free record time available time referenced rr 
conversely want mark allocated unavailable infinitely late value 
instruction uses actual issued 
instruction sets actual issued model specifies perfect register renaming 
considered timing constraints instruction determined issued update dependency barriers 
instruction sets actual record time result accessible usually instruction instruction non unit latency 
don care latency setting time relevant register immediately available value assigned 
instruction uses actual 
stated far algorithm fairly expensive allocation new actual register requires search table find earliest available register 
speed maintaining tournament tree top lets find earliest entry constant time looking root 
change entry update tournament tree path register root takes time logarithmic number actual registers 
discussion ignored fact disjoint register sets cpu implicitly uses special registers division operations treat named explicitly instruction include renaming system 
don bump perfect register renaming previous values actual register irrelevant 
doesn hurt imperfect renaming register assignment won able move previous uses 
data structure limits instruction level parallelism rn actual register associated register name rn cycle value cycle register set cycle register available 
root pointer root tournament tree leaf pointer leaf entry tournament tree tournament tree records reg register described record avail time register available parent pointer parent record sib pointer sibling record record identifies child smaller value avail 
procedure oldest return root reg procedure leaf avail repeat parent parent sib sib parent avail avail sib avail parent reg reg sib reg whichever min avail parent root schedule instruction sets uses register name rn rn determine actual register currently mapped instruction sets register name rn rr rn oldest rn infinity rr rr rr instruction uses actual register instruction sets actual register register renaming scheduling instruction limits instruction level parallelism instruction sets actual register bump instruction uses actual register bump continued fpu registers 
really need instances data structures algorithms register set 
model specifies perfect renaming assign actual register register name change 
maintain table exactly described instruction scheduled 
model specifies renaming assign actual register register name change 
renaming instruction uses scheduled 
alias analysis model specifies alias analysis store exchanged load store 
store establishes barrier stores loads load establishes barrier store maintain 
model specifies perfect alias analysis load store exchanged store memory locations referenced different 
barriers associated distinct memory location set entry word memory 
feasible know memory benchmarks need need cover entire address space table 
assume granularity memory bit words byte stores different bytes word deemed conflict 
program trace tells address load store 
alias analysis inspection complicated 
store base register exchanged load store base register displacement different value base register hasn changed 
exchanged base registers different long manifestly stack base register manifestly static data base register 
terms barriers state pairs exchanged blocks instruction moving farther back time 
consider instructions different base registers 
register analysis done compile time scheduler 
register renaming effect register name instruction actual register renaming pool analysis 
data structure limits instruction level parallelism cycle store cycle store word address cycle store base register store gp fp sp base register store gp fp sp cycle store followed change cycle load cycle load word address cycle load base register load gp fp sp base register load gp fp sp cycle load followed change procedure old old ifr sp fp sp fp gp sp fp gp procedure fp sp gp schedule loads stores memory stores memory alias analysis limits instruction level parallelism loads stores memory location stores memory location loads stores memory location base register stores memory location base register loads stores memory location base register heap stores memory location base register heap scheduling instruction instruction sets register allowable base register bump bump instruction stores memory location base register bump bump heap bump instruction loads memory location base register bump bump heap bump continued limits instruction level parallelism keep track load store base register 
keep track base registers sp load store 
information lets find conflicting load store looking members 
case instructions base register value base register changed 
model specifies register renaming point moot instruction assigns base register blocked earlier blocked turn register assignment 
register renaming allows register assignment move previous leaving prevent uses exchanged may location 
handle maintain 
contain cycle latest store load obsolete redefinition ofr 
load store blocked store base register value base register hasn changed displacements 
case instructions referencing memory location 
account possibility just perfect alias analysis tell time stored loaded 
alias analysis compiler similar assume compiler perfectly disambiguate non heap heap non heap rely inspection disambiguate heap 
recognize heap range checking actual data address proceed just alias analysis inspection ignore non heap checking actual address conflicts 
limits instruction level parallelism latency models cycle width window size latency models easy covered part register renaming 
instruction scheduled show result instruction 
instruction unit latency ist ifthe latency models assume upper limit instructions issued single cycle 
fundamental reason limit value doesn affect amount memory simulator needs data structures 
limit generous guess constraints real machines doubling just matter relaxing test 
keep track number instructions currently pending cycle total pending cycles 
total reaches window size flush early cycles total number 
window managed discretely continuously flush pending cycles total reaches window size 
case flush cycles cycle indexed instructions moved earlier 
effect continuously managed windows serves fresh start discretely managed windows allowing instructions considered full window forces complete flush 
removing limit cycle width trivial keep track full pending cycle longer look past full cycle schedule instruction normally go 
removing limit window size simply means maintaining description pending cycles 
determine instruction scheduled existing barriers update barriers time decided 
limits instruction level parallelism appendix 
details program runs section specifies data command line options program changes official versions programs reduce runtime 
sed sed fprintf debug sed test egrep egrep regexp test eco eco old new eco test yacc yacc grammar grammar non blank lines met dma tuned cat pal cat dc cat misc cat cat output grr grr mc mc log mc pcb mc route hydro hydro short short gcc gcc cexp quiet cexp output compress compress ref ref espresso espresso opa opa ora ora short short fpppp fpppp small small li li lsp lsp queens problem doduc doduc small small swm cat swm sed swm swm tomcatv change initial value lmax line tomcatv output alvinn change definition macro num epochs line alvinn result mdljsp mv short dat dat mdljsp input file short limits instruction level parallelism appendix 
parallelism models appendix lists results running test programs different configurations lists configurations figures main body report 
configurations keyed abbreviations perfect branch prediction correct cnn loc gsh predictor cnn mm fanout mm branches loc gsh predictor bnn ctr gsh predictor bnn mm fanout mm branches ctr gsh predictor ann ctr predictor mm ff integrated mm way fanout profile prediction threshold ff predict branches profile taken predict branches taken sign predict backward branches taken forward branches taken mm fanout mm branches prediction branch prediction perfect indirect jump prediction correct mm predict returns nn element ring jumps mm elt 
dest table predict returns nn element ring don predict jumps jump prediction perfect register renaming infinitely registers rnn register renaming nn cpu nn fpu registers register renaming registers compiled perfect alias analysis actual addresses distinguish conflicts alias analysis compiler alias analysis inspection alias analysis stores conflict memory allow cycles issue instructions allow unlimited cycle width continuous window nn instructions default discrete window nn instructions unlimited window size cycle width specified latency model default size hardware branch predictors specified single integer parameter 
counter predictor parameter consists table bit counters 
counter gshare predictor parameter consists bit counters predictor bit global history register bit counters second predictor bit counters selector 
local gshare predictor parameter consists bit history registers bit counters predictor bit global history register bit counters second predictor bit counters selector 
yacc eco grr met comp gcc li ora swm figures lb lc ld le jp 

dw dw dw dw dw dw dw dw dw dw yacc eco grr met comp gcc li ora swm figures lb lc ld le lb lc ld le jp 
yacc eco grr met comp gcc li ora swm figures lb lc ld le 
yacc eco grr met comp gcc li ora swm figures dw dw dw dw dw dw dw dw dw dw lb lc ld le jp lb lc ld yacc eco grr met comp gcc li ora swm figures le yacc eco grr met comp gcc li ora swm figures yacc eco grr met comp gcc li ora swm figures taken sign taken sign lb lc ld le limits instruction level parallelism dw dw dw dw dw dw dw dw dw dw dw dw dw dw dw dw dw dw dw dw limits instruction level parallelism limits instruction level parallelism limits instruction level parallelism lb lb lb lb lc lb ld lb le lb lb lb ld lc ld ld ld le ld ld ld ld ac john cocke 
high performance reduced instruction set processors 
ibm thomas watson research center technical report march 
beh david susan eggers robert henry 
integrating register allocation instruction scheduling 
fourth international symposium architectural support programming languages operating systems pp 
april 
published computer architecture news operating systems review special issue sigplan notices 
cwz david chase mark wegman kenneth zadeck 
analysis pointers structures 
proceedings sigplan conference programming language design implementation pp 

published sigplan notices june 
fis fisher 
global code generation instruction level parallelism trace scheduling 
technical report hpl hewlett packard laboratories palo alto california 
gm phillip gibbons steven muchnick 
efficient instruction scheduling pipelined architecture 
proceedings sigplan symposium compiler construction pp 

published sigplan notices july 
gh james goodman wei chung hsu 
code scheduling register allocation large basic blocks 
international conference supercomputing pp 
july 
hhn laurie hendren joseph hummel alexandru 
abstractions recursive pointer data structures improving analysis transformation imperative programs 
proceedings sigplan conference programming language limits instruction level parallelism design implementation pp 

published sigplan notices july 
hg john hennessy thomas gross 
postpass code optimization pipeline constraints 
acm transactions programming languages systems pp 
july 
jm neil jones steven muchnick 
flexible approach interprocedural data flow analysis programs recursive data structures 
ninth annual acm symposium principles programming languages pp 
jan 
jw norman jouppi david wall 
available instruction level parallelism superscalar machines 
third international symposium architectural support programming languages operating systems pp 
april 
published computer architecture news operating systems review special issue sigplan notices special issue 
available wrl research report 
lh james larus paul hilfinger 
detecting conflicts structure accesses 
proceedings sigplan conference programming language design implementation pp 

published sigplan notices july 
ls johnny lee alan smith 
branch prediction strategies branch target buffer design 
computer pp 
january 
mcf scott mcfarling 
combining branch predictors 
wrl technical note tn june 
digital western research laboratory university ave palo alto ca 
nf alexandru nicolau joseph fisher 
measuring parallelism available long instruction word architectures 
ieee transactions computers pp 
november 
psr tai pan joseph 
improving accuracy dynamic branch prediction branch correlation 
fifth international symposium architectural support programming languages operating systems september 
published computer architecture news special issue operating systems review special issue sigplan notices special issue 
smi smith 
study branch prediction strategies 
eighth annual symposium computer architecture pp 

published computer architecture news 
michael smith mike johnson mark horowitz 
limits multiple instruction issue 
third international symposium architectural support programming languages operating systems pp 
april 
published computer architecture news operating systems review special issue sigplan notices special issue 
limits instruction level parallelism tf flynn 
detection parallel execution parallel instructions 
ieee transactions computers pp 
october 
wall david wall 
limits instruction level parallelism 
fourth international symposium architectural support programming languages operating systems april 
available wrl technical note tn reprinted david lilja architectural alternatives exploiting parallelism ieee computer society press 
yp tse yu yeh yale patt 
alternative implementations level adaptive branch prediction 
nineteenth annual international symposium computer architecture may 
published computer architecture news 
yp tse yu yeh yale patt 
comparison dynamic branch predictors levels branch history 
twentieth annual international symposium computer architecture may 
wrl research reports titan system manual architecture papers michael nielsen 
norman jouppi jeremy dion david boggs mich wrl research report september 
ael nielsen 
wrl research report april 
global register allocation link time david wall 
fast printed circuit board routing wrl research report october 
jeremy dion 
wrl research report march 
optimal heat sinks william 
compacting garbage collection ambiguous wrl research report october 
roots joel bartlett 
mahler experience intermediate wrl research report february 
language machine description david wall michael powell 
experimental literature internet wrl research report august 
annotated bibliography jeffrey mogul 
packet filter efficient mechanism wrl research report august 
user level network code jeffrey mogul richard rashid michael measured capacity ethernet myths accetta 
reality wrl research report november 
david boggs jeffrey mogul christopher kent 
fragmentation considered harmful wrl research report september 
christopher kent jeffrey mogul 
wrl research report december 
visa protocols controlling inter organizational datagram flow extended description cache coherence distributed systems deborah estrin jeffrey mogul gene tsudik christopher kent 
anand 
wrl research report december 
wrl research report december 
register windows vs register allocation scheme portable scheme compiler david wall 
joel bartlett 
wrl research report december 
wrl research report january 
editing graphical objects procedural optimal group distribution carry skip ad representations paul 
silvio 
wrl research report november 
wrl research report february 
usenet cookbook experiment precise robotic paste dot dispensing electronic publication william 
brian reid 
wrl research report february 
wrl research report december 
simple flexible datagram access controls link time code modification unix gateways david wall 
jeffrey mogul 
wrl research report march 
wrl research report september 
nfs implementation performance noise issues ecl circuit family cache consistency protocols jeffrey tang leon yang 
srinivasan jeffrey mogul 
wrl research report may 
wrl research report january 
efficient generation test patterns available instruction level parallelism super boolean scalar machines tracy 
norman jouppi david wall 
wrl research report july 
wrl research report february 
papers test pattern generation unified vector scalar floating point architec tracy 
ture norman jouppi jonathan david wrl research report march 
wall 
virtual memory vs file system wrl research report july 
architectural organizational tradeoffs michael nelson 
wrl research report march 
design cpu efficient workstations passive monitor norman jouppi 
ing local area networks wrl research report july 
integration packaging plateaus processor jeffrey mogul 
wrl research report july 
performance dimensional thermal model vax norman jouppi 
multi chip units wrl research report july 
mips sustained bit cmos john fitch 
wrl research report july 
sor high ratio sustained peak perfor livermore magic release mance robert mayo michael arnold walter scott norman jouppi jeffrey tang 
don stark gordon 
wrl research report july 
wrl research report september 
distribution instruction level machine parallelism effect performance norman jouppi 
wrl research report july 
long address traces risc machines generation analysis anita borg kessler georgia david wall 
wrl research report september 
pool boiling enhancement techniques water low pressure wade john fitch william van carey 
wrl research report december 
writing fast servers dumb color frame buffers joel 
wrl research report february 
simulation study tlb performance cache write policies performance bradley chen anita borg norman jouppi 
norman jouppi 
wrl research report november 
analysis power supply networks vlsi cir wrl research report december 
packaging bipolar ecl microprocessor don stark 
william john fitch 
wrl research report april 
wrl research report march 
adapter david boggs 
wrl research report april 
procedure merging instruction caches scott mcfarling 
wrl research report march 
observing tcp dynamics real networks jeffrey mogul 
wrl research report april 
systems late code modification david wall 
wrl research report may 
don widgets draw piecewise linear models switch level simula joel bartlett 
tion wrl research report may 
pool boiling small heat elements water pressure russell kao 
wrl research report september 
wade john fitch william practical system intermodule code optimiza van carey 
tion link time wrl research report june 
amitabh srivastava david wall 
incremental generational copying gar wrl research report december 
collection uncooperative environ smart frame buffer ments joel bob mcnamara 
may yip 
wrl research report june 
wrl research report january 
recovery nfs interleaved fin thermal connectors jeffrey mogul 
modules william 
wrl research report june 
wrl research report august 
tradeoffs level chip caching norman jouppi steven 
experience software defined machine architecture wrl research report october 
david wall 
unreachable procedures object oriented wrl research report august 
programing network locality scale processes jeffrey mogul 
amitabh srivastava 
wrl research report august 
wrl research report november 
limits instruction level parallelism david wall 
wrl research report november 
pressure pad design applications alberto makino william john fitch 
wrl research report november 
wrl technical notes tcp ip print server protocol predicting program behavior real es brian reid christopher kent 
profiles wrl technical note tn september 
david wall 
wrl technical note tn december 
tcp ip server architecture implementation cache replacement dynamic exclusion christopher kent 
scott mcfarling 
wrl technical note tn november 
wrl technical note tn november 
smart code stupid memory fast server boiling binary mixtures pres dumb color frame buffer sures joel 
wade john fitch william wrl technical note tn september 
van carey 
wrl technical note tn january 
aren operating systems getting faster fast hardware comparison acoustic infrared inspection john ousterhout 
techniques die attach wrl technical note tn october 
john fitch 
wrl technical note tn january 
copying garbage collection picks generations adapter joel bartlett 
david boggs 
wrl technical note tn october 
wrl technical note tn january 
effect context switches cache perfor recovery protocol nfs mance jeffrey mogul 
jeffrey mogul anita borg 
wrl technical note tn april 
wrl technical note tn december 
electrical evaluation package method detecting memory bot patrick boyle 
wrl technical note tn july 
aaron goldberg john hennessy 
wrl technical note tn december 
transparent controls interactive graphics joel bartlett 
wrl technical note tn july 
design tools jeremy dion louis 
wrl technical note tn december 
link time optimization address calculation bit architecture amitabh srivastava david wall 
wrl technical note tn june 
combining branch predictors scott mcfarling 
wrl technical note tn june 
boolean matching full custom ecl gates robert mayo touati 
wrl technical note tn june 

