combining labeled unlabeled data training avrim blum school computer science carnegie mellon university pittsburgh pa avrim cs cmu edu consider problem large unlabeled sample boost performance learning algorithm small set labeled examples available 
particular consider setting description example partitioned distinct views motivated task learning classify web pages 
example description web page partitioned words occurring page words occurring hyperlinks point page 
assume view example su cient learning labeled data goal views allow inexpensive unlabeled data augment smaller set labeled examples 
speci cally presence distinct views example suggests strategies learning algorithms trained separately view algorithm predictions new unlabeled examples enlarge training set 
goal provide pac style analysis setting broadly style framework general problem learning labeled unlabeled data 
provide empirical results real web page data indicating unlabeled examples lead signi cant improvement practice 
part analysis provide new re extended version appeared proceedings th annual conference computational learning theory pages 
research supported part darpa hpkb program contract nsf national young investigator ccr 
tom mitchell school computer science carnegie mellon university pittsburgh pa mitchell cs cmu edu sults learning misclassi cation noise believe interest 
machine learning settings unlabeled examples signi cantly easier come labeled ones :10.1.1.122.539
example web page classi cation 
suppose want program electronically visit web site download web pages interest cs faculty member pages course home pages university :10.1.1.35.6633
train system automatically classify web pages typically rely hand labeled web pages 
labeled examples fairly expensive obtain require human ort 
contrast web hundreds millions unlabeled web pages inexpensively gathered web crawler 
learning algorithm able take advantage unlabeled data possible 
web page learning problem interesting additional feature 
example domain naturally described di erent kinds information 
kind information web page text appearing document 
second kind information anchor text attached hyperlinks pointing page pages web 
problem characteristics mentioned availability unlabeled data existence di erent somewhat redundant sources information examples suggest learning strategy 
initial small set labeled examples nd weak predictors kind information instance nd phrase research interests web page weak indicator page faculty home page nd phrase advisor link indicator page pointed faculty page 
attempt bootstrap weak predictors unlabeled data 
instance search pages pointed links having phrase advisor probably positive examples train learning algorithm words text page vice versa 
call type bootstrapping cotraining close connection bootstrapping incomplete data expectation maximization setting see instance :10.1.1.56.6066
question raises reason believe training help 
goal address question developing pac style theoretical framework better understand issues involved approach 
process provide new results learning presence classi cation noise 
give preliminary empirical results classifying pages see section encouraging context 
broadly general question unlabeled examples augment labeled data slippery point view standard pac assumptions 
address issue proposing notion compatibility data distribution target function section discuss relates approaches combining labeled unlabeled data section 
framework de ne training model follows 
instance space correspond di erent views example 
example pair 
assume view su cient correct classi cation 
speci cally distribution concept classes de ned respectively 
assume labels examples non zero probability consistent target function consistent target function 
words denotes combined target concept entire example example observed label wehave 
means particular assigns probability zero example 
expect unlabeled data useful amplifying small labeled sample context 
think question lens standard pac supervised learning setting follows 
distribution talk target function compatible satis es condition assigns probability zero set examples 
pair compatible andd legal framework 
notice large concept classes high complexity say vc dimension measure distribution set compatible target concepts simpler smaller 
hope able unlabeled examples gain better sense target concepts compatible yielding information reduce number labeled examples needed learning algorithm 
general hope tradeo number unlabeled examples number labeled examples needed 
illustrate idea suppose graphs gd gs 
edges represent examples non zero probability solid edges represent examples observed nite sample notice assumptions seeing labels learning algorithm deduce examples belonging connected component gs classi cation 
conjunctions say known rst coordinate relevant target concept rst coordinate conjunction 
unlabeled example rst coordinate zero produce labeled negative example 
course unhelpful distribution nonzero probability pairs may give useful information 
tightly correlated 
instance suppose conditionally independent ofx classi cation 
case rst component set random negative example quite useful 
explore generalization idea section show weak hypothesis boosted unlabeled data conditional independence property target class learnable random classi cation noise 
terms pac style models think setting somewhat uniform distribution model distribution particularly neutral teacher models examples supplied helpful oracle 
graph representation way look training problem view distribution weighted bipartite graph write gd just gd clear context 
left hand side gd node point inx right hand side node point inx 
edge example non zero probability edge weight equal probability 
convenience remove degree corresponding views having zero probability 
see 
representation compatible concepts exactly corresponding partition graph cross edges 
reasonably de ne extent partition compatible weight cut induces words degree compatibility target function distribution de ned anumber wherep prd 
assume full compatibility 
set unlabeled examples similarly de ne graph gs bipartite graph having edge notice assumptions examples belonging connected classi cation 
instance web pages exact content representation view correspond edges left endpoint required label 
high level view relation approaches general form proposing add pac model notion compatibility concept data distribution 
postulate target concept compatible distribution allows unlabeled data reduce class smaller set functions compatible known 
wecan think intersecting concept class cd associated partially known unlabeled data observed 
training scenario speci notion compatibility previous section especially natural imagine postulating forms compatibility settings 
discuss relations model analyzing labeled unlabeled data 
standard setting problem analyzed assume data generated simple known parametric model 
assumptions form castelli cover precisely quantify relative values labeled unlabeled data bayesian optimal learners 
em algorithm widely practice learning data missing information analyzed 
instance common speci assumption positive examples generated dimensional gaussian centered point negative examples generated gaussian centered point unknown learning algorithm 
examples generated choosing positive point negative proba bility 
case bayes optimal hypothesis linear separator de ned hyperplane bisecting orthogonal line segment 
parametric model rigid pac compatibility setting sense incorporates noise bayes optimal hypothesis perfect classi er 
hand signi cantly restrictive underlying probability distribution ectively forced commit target concept 
instance case gaussians consider class linear separators really concepts compatible underlying distribution unlabeled examples optimal negation 
words knew underlying distribution possible target concepts left 
view surprising unlabeled data helpful set assumptions 
proposal compatibility function concept probability distribution attempt broadly consider distributions completely commit target function completely uncommitted 
approach unlabeled data yarowsky word sense disambiguation problem closer spirit cotraining nicely viewed model :10.1.1.122.539
problem yarowsky considers 
words quite di erent dictionary de nitions 
instance plant mean type life form factory 
text document instance word plant goal algorithm determine meaning intended 
yarowsky unlabeled data observation xed document highly instances word plant thesame intended meaning whichever meaning happens :10.1.1.122.539
uses observation learning algorithm learns predictions local context achieve results labeled examples unlabeled ones 
think yarowsky approach context training follows 
example instance word plant described distinct representations 
rst representation unique id document word 
second representation local context surrounding word 
instance bipartite graph view node left represents document degree number instances plant document node right represents di erent local context 
assumptions instances plant document label local context su cient determining word meaning equivalent assumption examples connected component classi cation 
rote learning order get feeling training model consider section simple problem rote learning 
particular consider case partitions consistent possible learning algorithm simply outputs don know example label deduce training data compatibility assumption 
jx jx imagine medium size number sense gathering unlabeled examples feasible labeling 
case just single view just portion see labeled examples order cover substantial fraction speci cally probability st example seen prd prd instance example probability rote learner need labeled examples order achieve 
hand views ofeach example allow smaller number labeled examples large unlabeled sample 
instance suppose extreme unlabeled sample contains edge graph gd example nonzero probability 
case rote learner con dent label new example exactly previously seen labeled example connected component 
connected components gd probability mass respectively probability labeled examples st example deduced algorithm just pj pj cj gd instance graph gd connected components achieve error examples 
generally views achieve tradeo number labeled unlabeled examples needed 
consider graph gs graph edge observed example see observe unlabeled examples number connected components drop components merge nally components gd 
furthermore set random subset label probability label random st example chosen remaining portion deduced algorithm cj gs jsj sj sj jsj plausible context web pages think document small set attributes document 
sj cj jsj formula approximately cj gs sj jsj sj jsj analogy equation 
fact results study random graph processes describe quantitatively expect components gs converge gd see unlabeled examples properties distribution connected component gd value minimum cut minimum cuts sum weights edges cut 
words probability random example cross speci minimum cut 
clearly sample contain spanning tree include component edge minimum cut 
expected number unlabeled samples needed occur course spanning tree include edge cut 
karger shows nearly su 
speci cally theorem shows log unlabeled samples su cient ensure spanning tree high probability 
hg log unlabeled samples su cient ensure number connected components sample equal number minimizing number labeled examples needed 
instance suppose points positive andn negative similarly distribution uniform subject placing zero probability illegal examples 
case legal example probability reduce observed graph connected components need see edges 
spanning trees 
minimum cut component value pn karger result log unlabeled examples su ce 
simple case analyzed easily rst principles 
generally bound number connected components expect see number labeled examples needed produce perfect hypothesis imagine algorithm allowed select unlabeled examples labeled terms number unlabeled examples mu follows 
theorem model independently appears observed graph probability mp pe weight edge expected number edges chosen 
speci cally karger concerned network reliability problem edge goes independently known probability want probability connectivity maintained 
hard convert setting concerned xed samples drawn independently distribution de ned 
fact karger handles conversion formally 
consider greedy process cut value gd edges removed process repeated connected component cut 
ncc bethe number connected components remaining 
log mu wherec constant karger theorem mu large singleton components components having edges remaining process ncc isan upper bound expected number labeled exam ples needed cover hand mu ncc bound greedy process ncc cuts expected number edges crossing cut 
pac learning large input spaces previous section training provide tradeo number labeled unlabeled examples needed setting jxj relatively small algorithm performing 
di cult case jxj large gn goal polynomial description length examples target concept 
show conditional independence assumption distribution target class learnable random classi cation noise standard pac model initial weak predictor boosted arbitrarily high accuracy unlabeled examples training 
speci cally target functions distribution satisfy conditional independence assumption xed nonzero probability pr similarly pr pr pr words conditionally independent label 
instance assuming words page words hyperlinks pointing independent ofeach conditioned classi cation somewhat plausible starting point page constructed adi erent user link 
hand theorem viewed showing really plausible 
bipartite graph view section order state theorem de ne predictor function function 
prd 
prd jh prd poly 
example seeing word web page weakly useful predictor page course homepage appears non negligible fraction pages probability page course homepage appears non negligibly higher probability course homepage word appear 
unbiased sense prd prd usual notion weak predictor prd poly 
unbiased requiring noticeably better simply predicting negative positive 
worth noting weakly useful predictor possible prd prd poly 
instance condition implies prd conditions imply prd theorem learnable pac model classi cation noise conditional independence assumption satis ed learnable training model unlabeled data initial weakly useful predictor 
instance conditional independence assumption implies concept class learnable statistical query model learnable unlabeled data initial weakly useful predictor 
proving theorem convenient de ne variation standard classi cation noise model noise rate positive examples may di erent noise rate negative examples 
speci cally classi cation noise setting true positive examples incorrectly labeled independently probability true negative examples incorrectly labeled independently probability extends standard model sense require goal learning algorithm setting produce close target function respect non noisy data 
case lemma lemma concept class learnable standard classi cation noise model learnable easy see distribution compatible target functions pair negation positive negative functions assuming give probability zero example 
theorem interpreted showing access slight bias unlabeled data polynomial time discover fact 
classi cation noise long 
running time polynomial min prd prd non noisy target function 
proof 
suppose known learning algorithm 
loss generality assume learn noise simply ip positive label negative label independently probability 
results standard classi cation noise noise rate 
run algorithm learning presence standard classi cation noise de nition running time polynomial known dealt making number guesses evaluating separate test set described 
turn evaluation step requires lower bound instance take extreme example impossible distinguish case positive case negative 
speci cally known proceed follows 
data set examples labeled positive create hypotheses hypothesis hi produced ipping labels random positive examples running classi cation noise algorithm hypothesis hi produced ipping labels random negative examples running algorithm 
expect hi procedure known viewed probability distribution experiments 
select hypotheses separate test set 
choose hypothesis selecting hi minimizes quantity hi pr hi pr hi observed noisy label straightforward calculation shows hi solves hi hi pr pr pr hi pr hi jf pr hi jf words quantity hi estimate noisy examples linearly related quantity hi measure true error hi 
selecting hypothesis hi minimizes observed value hi su ciently large sample sample size polynomial result note hi empirical error pr hi pr pr hi pr 
minimizing empirical error guaranteed succeed instance empirical error negative hypothesis half empirical error true target concept 
approximately minimizes hi 
furthermore hi property true error su ciently small function min approximately minimizing hi approximately minimize true error 
classi cation noise model thought kind constant partition classi cation noise 
results require noise rate 
need stronger statement su ces assume sum 
proof theorem 
target concept prd probability random example positive 
prd jh prd 
prd jf prd jh prd prd qc prd jf conditional independence assumption random example independent ofx 
noisy label equivalent classi cation noise qc equations 
sum noise rates satis es qc assumption weakly useful predictor quantity applying lemma theorem 
point analysis conditional independence minimizing empirical error noisy data labeled weak hypothesis may correspond minimizing true error 
dealt proof lemma measuring error positive negative regions equal weight 
experiment described section kind reweighting handled parameters setting equal correspond error measure proof lemma empirically performance algorithm sensitive issue 
relaxing assumptions far fairly stringent assumption shown examples suchthat target function 
show long conditional independence maintained assumption signi cantly weakened allow unlabeled data boost weakly useful predictor 
intuitively surprising proof theorem involves reduction problem learning classi cation noise relaxing assumptions just add noise 
surprising extent assumptions relaxed 
formally target function pair distribution pairs de ne prd prd prd prd previously assumed implicitly de nition weakly useful predictor extremely close 
replace assumption poly 
maintain conditional independence assumption view underlying distribution probability selecting random positive independent random positive probability selecting random positive independent random negative 
fully specify scenario labeling process instance probability example labeled positive andf 
issue simply assuming previous section obtained information labeled data obtain predictor unlabeled data 
particular get theorem 
theorem prd jf prd jf prd jf prd jf words produces usable classi cation noise usable sense produces usable classi cation noise 
assumption ensures quantity small 
proof 
proof just straightforward calculation 
prd jf prd jf experiments order test idea training applied problem learning classify web pages 
particular experiment motivated larger research ort apply machine learning problem extracting information world wide web :10.1.1.35.6633
data experiment consists web pages collected computer science department web sites universities cornell university university wisconsin 
pages hand labeled number categories 
experiments considered category course home page target function course home pages positive examples pages negative examples 
dataset web pages course pages 
example web page considered bag multi set words appearing web page bag words underlined links pointing web page pages database 
classi ers trained separately naive bayes algorithm 
refer page hyperlink classi ers respectively 
naive bayes algorithm empirically observed successful variety text categorization tasks 
training algorithm described table 
set labeled examples set unlabeled examples algorithm rst creates smaller pool containing unlabeled examples 
iterates procedure 
train distinct classi ers 
naive bayes classi er portion instance naive bayes classi er portion 
second allow examine unlabeled set select examples con dently labels positive examples con dently labels negative 
andn 
example selected way added label assigned selected 
drawing examples random 
earlier implementations training allowed select examples directly larger set obtained better results smaller pool presumably forces select data available www cs cmu edu afs cs project theo www labeled training examples unlabeled examples create pool examples choosing examples random loop iterations train classi er considers portion train classi er considers portion allow label positive andn negative examples allow label positive andn negative examples add self labeled examples randomly choose examples replenish table training algorithm 
experiments reported trained naive bayes algorithm algorithm parameters set 
examples representative underlying distribution generated experiments conducted determine training algorithm successfully unlabeled data outperform standard supervised training naivebayes classi ers 
experiment web pages rst selected random test set 
remaining data generate labeled set containing positive negative examples drawn random 
remaining examples drawn unlabeled pool experiments conducted di erent training test splits training parameters set 
compare training supervised training trained naive bayes classi ers labeled training examples trained classi er page classi er just training 
addition de ned third combined classi er outputs page hyperlink classi er 
keeping naive bayes assumption conditional independence combined classi er computes probability class cj instance probabilities output page classi ers results experiments summarized table 
numbers shown test set error rates averaged random train test splits 
rst row table shows test set accuracies classi ers formed supervised learning second row shows accuracies classi ers formed training 
note data default hypothesis predicts negative achieves error rate 
gives plot error versus number iterations runs 
notice types classi ers page combined trained classi er outperforms classi er formed supervised training 
fact page combined achieve error rates half error achieved supervised training 
hyperlink classi er helped training 
may due fact hyperlinks contain fewer words capable expressing accurate approximation target function 
experiment involves just data set target function 
experiments needed determine general behavior training algorithm determine exactly responsible pattern behavior observed 
results indicate training provide useful way unlabeled data 
open questions described model unlabeled data augment labeled data having views example redundant completely correlated 
theoretical model clearly simpli cation real world target functions distributions 
particular optimal pair functions expect occasionally see inconsistent examples examples 
provides way looking notion friendliness distribution terms components minimum cuts unlabeled examples potentially page classi er hyperlink classi er combined classi er supervised training training table error rate percent classifying web pages course home pages 
top row shows errors training labeled examples 
bottom row shows errors training labeled unlabeled examples 
percent error test data hyperlink page default training iterations error versus number iterations run training experiment 
prune away incompatible target concepts reduce number labeled examples needed learn 
open question extent consistency constraints model mutual independence assumption section relaxed allow provable results utility training unlabeled data 
preliminary experimental results suggest method unlabeled data potential signi cant bene ts practice studies clearly needed 
conjecture practical learning problems approximately training model 
example consider problem learning classify segments television broadcasts say learning identify segments containing president 
set possible video images set possible audio signals cross product 
small sample labeled segments learn weakly predictive recognizer spots full frontal images president face recognizer spots voice background noise 
training applied large volume unlabeled television broadcasts improve accuracy classi ers 
similar problems exist perception learning tasks involving multiple sensors 
example consider mobile robot learn recognize open doorways collection vision sonar laser range sensors 
important structure problems instance partitioned subcomponents xi xi perfectly correlated xi principle classi cation large volume unlabeled instances easily collected 
castelli cover 
exponential value labeled samples 
pattern recognition letters january 
castelli cover 
relative value labeled unlabeled samples unknown mixing parameter 
ieee transactions information theory november 
craven freitag mccallum mitchell nigam quek :10.1.1.35.6633
learning extract symbolic knowledge world wide web 
technical report carnegie mellon university january 

pac learning classi cation noise applications decision tree induction 
proceedings fourteenth international conference machine learning pages july 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
richard duda peter hart 
pattern classi cation scene analysis 
wiley 
ghahramani jordan 
supervised learning incomplete data em approach 
advances neural information processing systems nips 
morgan kau man 
goldman kearns 
complexity teaching 
journal computer system sciences february 
hauptmann witbrock 
informedia news demand multimedia information acquisition retrieval 
inm maybury editor intelligent multimedia information retrieval 
jackson tomkins 
computational model teaching 
proceedings fifth annual workshop computational learning theory pages 
morgan kaufmann 
karger 
random sampling cut ow network design problems 
proceedings sixth annual acm symposium theory computing pages may 
karger 
random sampling cut ow network design problems 
journal version draft 
kearns 
cient noise tolerant learning statistical queries 
proceedings fifth annual acm symposium theory computing pages 
lewis ringuette 
comparison learning algorithms text categorization 
third annual symposium document analysis information retrieval pages 
joel santosh venkatesh :10.1.1.122.539
learning mixture labeled unlabeled examples parametric side information 
proceedings th annual conference computational learning theory pages 
acm press new york ny 
witbrock hauptmann 
improving acoustic models watching television 
technical report cmu cs carnegie mellon university march 
yarowsky :10.1.1.122.539
unsupervised word sense disambiguation rivaling supervised methods 
proceedings rd annual meeting association computational linguistics pages 
