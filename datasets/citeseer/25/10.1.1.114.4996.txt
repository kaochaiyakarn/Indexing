statistical science vol 
doi institute mathematical statistics graphical models michael jordan 
statistical applications fields bioinformatics information retrieval speech processing image processing communications involve large scale models thousands millions random variables linked complex ways 
graphical models provide general methodology approaching problems models developed researchers applied fields instances general graphical model formalism 
review basic ideas underlying graphical models including algorithmic ideas allow graphical models deployed large scale data analysis problems 
examples graphical models bioinformatics error control coding language processing 
key words phrases probabilistic graphical models junction tree algorithm sum product algorithm markov chain monte carlo variational inference bioinformatics error control coding 

fields statistics computer science generally followed separate paths past decades field providing useful services core concerns fields rarely appearing intersect 
years increasingly evident long term goals fields closely aligned 
statisticians increasingly concerned computational aspects theoretical practical models inference procedures 
computer scientists increasingly concerned systems interact external world interpret uncertain data terms underlying probabilistic models 
area trends evident probabilistic graphical models 
graphical model family probability distributions defined terms directed undirected graph 
nodes graph identified random variables joint probability distributions defined products functions defined connected subsets nodes 
exploiting graph theoretic michael jordan professor computer science division department statistics university california berkeley california usa mail jordan stat berkeley edu 
representation formalism provides general algorithms computing marginal conditional probabilities interest 
formalism provides control computational complexity associated operations 
graphical model formalism agnostic distinction frequentist bayesian statistics 
providing general machinery manipulating joint probability distributions particular making hierarchical latent variable models easy represent manipulate formalism proved particularly popular bayesian paradigm 
viewing bayesian statistics systematic application probability theory statistics viewing graphical models systematic application graph theoretic algorithms probability theory surprising authors viewed graphical models general bayesian inference engine cowell dawid lauritzen spiegelhalter 
distinctive graphical model approach naturalness formulating probabilistic models complex phenomena applied fields maintaining control computational cost associated models 
accordingly article principal focus presentation graphical models proved useful applied domains ways formalism encourages exploration extensions classical methods 
turning examples overview basic concepts 

representation common forms graphical model directed graphical models undirected graphical models directed graphs undirected graphs respectively 
directed case 
directed acyclic graph nodes edges graph 
xv collection random variables indexed nodes graph 
node subset indices parents 
allow sets indices appear single index appears denotes vector random variables indexed parents collection kernels xv sum discrete case integrate continuous case respect xv define joint probability distribution probability mass function probability density appropriate xv xv 
easy verify joint probability distribution xv conditionals henceforth write xv xv 
note distinction data parameters natural include parameters nodes graph 
plate useful device capturing replication graphical models including factorial nested structures occur experimental designs 
simple graphical models example plate shown viewed graphical model representation de finetti exchangeability theorem 
directed graphical models familiar representations hierarchical bayesian models 
example 
graph provides appealing visual representation joint probability distribution provides great deal 
functional forms kernels xv factorization implies set conditional independence statements variables xv entire set conditional independence statements obtained polynomial time reachability algorithm graph pearl 
second discuss section graphical structure exploited algorithms probabilistic inference 
consider undirected case 
undirected graph xv collection random variables indexed nodes graph denote collection cliques graph fully connected subsets nodes 
associated clique xc denote nonnegative potential function 
define joint probability xv product potential functions normalizing xv xc normalization factor obtained integrating summing product respect xv 
see example undirected graphical model 
fig 

diagram shorthand graphical model 
model asserts variables zn conditionally independent identically distributed viewed graphical model representation de finetti theorem 
note shading denotes conditioning 
jordan fig 
example hierarchical bayesian model represented directed graphical model 
errors covariates logistic regression model richardson green 
core model logistic regression yi xi 
covariate xi observed general noisy measurements ui xi available additional observed covariates ci 
density model xi taken mixture model number components mixing proportions zi allocations parameterizes mixture components 
undirected graphs problems areas spatial statistics statistical natural language processing communication networks problems little causal structure guide construction directed graph 
need fig 

example undirected graphical model 
probability distributions associated graph factorized xv 
restrict undirected models domains particular possible include parameters nodes undirected graph yield alternative general tool bayesian modeling 
possible hybrids include directed undirected edges lauritzen 
general directed graphs undirected graphs different assertions conditional independence 
families probability distributions captured directed graph captured undirected graph vice versa pearl 
representations shown overly coarse purposes 
particular undirected formalism cliques may quite large useful consider potential functions factorized ways need equated conditional independencies 
general consider set factors fi index set nodes associated ith factor 
note particular subset repeated multiple times allow ci cj 
define joint probability product factors xv fi shown definition associated graphical representation factor graph kschischang frey loeliger 
factor graph bipartite graph random variables round nodes factors appear square nodes 
edge factor node fi variable node xv ci 
factor graphs provide fine grained representation factors joint probability useful defining inference algorithms exploit structure topic section 
fig 

example factor graph 
probability distributions associated graph factorized xv fa fb fc fd 
note factor fi written exp parameter fixed function case representation canonical form exponential family 
factor graphs widely graph theoretic representations exponential family distributions 

algorithms probabilistic inference general problem probabilistic inference computing conditional probabilities xf xe subsets section concerned algorithms performing computations role graphical structure play making computations efficient 
discussing inference algorithms proves useful treat directed graphs undirected graphs equal footing 
done converting 
note particular treated special case case factor xi function clique 
general parents node connected set clique 
force set clique adding undirected edges parents essentially constructing new graph graphical cover original graph 
convert directed edges parents children undirected edges result undirected graphical cover called moral graph arguments function xi contained clique 
moral graph factorization special case 
proceed working exclusively undirected formalism 
useful note computational point view conditioning plays little essential role problem 
condition event xe xe suffices redefine original clique potentials 
multiply potential xc kronecker delta function xi result unnormalized representation conditional probability factorized form 
computational point view suffices focus problem marginalization general factorized expression 
interested controlling growth computational complexity performing marginalization function cardinality inthe sections describe principal classes methods attempt deal graphical models computational issue exact algorithms sampling algorithms variational algorithms 
presentation brief fuller presentation see cowell 
jordan 
exact algorithms example 
consider graphical model shown suppose wish compute marginal probability 
obtain marginal summing assuming discrete random variables remaining variables 
naively sums applied summand involves variables computational complexity scales assuming simplicity variable cardinality 
obtain smaller complexity exploiting distributive law defined intermediate factors mi obvious notation 
obtain value jordan marginal summing final expression respect 
note variables appear summand computational complexity reduced notable improvement 
perform summation different order 
general choose order elimination order worst case summation small possible terms number arguments summand 
problem reduced graph theoretic problem 
note particular summation creates intermediate factor function variables summand variable summed 
functional dependence captured graphically 
particular define triangulation algorithm sequentially choosing node elimination order linking remaining nodes neighbors node removing node graph 
defines sequence graphs 
largest clique arises sequence characterizes complexity worst case summation complexity exponential size clique 
minimum elimination orders size maximal clique known treewidth graph 
convention treewidth maximal size 
minimize computational complexity inference wish choose elimination ordering achieves treewidth 
graph theoretic problem independent numerical values potentials 
directly applicable continuous variables 
replacing sums integrals care regularity conditions characterization computational complexity graphical terms applies 
course issue computing individual integrals introduces additional computational considerations 
problem finding elimination ordering achieves treewidth turns np hard arnborg corneil 
possible practice find optimal orderings specific graphs variety inference algorithms specific fields algorithms inference pedigrees section specific choices elimination orderings problems interest 
general class algorithms known probabilistic elimination forms important class exact inference algorithms 
limitation elimination approach inference yields single marginal probability 
require marginal probability wish avoid inefficiency requiring multiple runs elimination algorithm 
see compute general marginal probabilities restrict special case graph tree 
undirected tree cliques pairs nodes singleton nodes probability distribution parameterized potentials xi xj xi 
compute specific marginal xf consider rooted tree node taken root tree choose elimination order children node eliminated node eliminated 
choice steps elimination algorithm written general way see xi xj xi xj xj xj set neighbors node xi intermediate term created node eliminated note added subscript notation intermediate terms reason clear shortly 
desired marginal obtained xf xf xf proportionality constant obtained summing right hand side respect xf consider problem computing marginals xi nodes graph turns solution hand 
dropping notion elimination order define asynchronous algorithm message xi computed messages xj right hand side equation computed see example tree nodes 
shown algorithm computes possible messages associated tree number operations proportional diameter graph 
mij xj xi computed messages computed time proportional length longest path graph 
messages computed compute marginal node 
graphical models fig 

intermediate terms created elimination algorithm nodes eliminated fragment undirected tree 
set messages created sum product algorithm tree nodes 
algorithm known sum product algorithm 
essentially dynamic programming algorithm achieves effect multiple elimination orders computing intermediate terms needed marginal reusing intermediate terms marginals 
case discrete nodes cardinality algorithm computational complexity 
sum product algorithm generalized number ways 
variant algorithm defined factor graphs 
case kinds messages defined accordance bipartite structure factor graphs 
second algebraic operations underlie sum product algorithm justified fact sums products form commutative semiring algorithm generalizes immediately commutative semiring aji mceliece shenoy shafer 
particular maximization product form commutative semiring max product variant sum product algorithm compute modes posterior distributions 
describe generalization sum product algorithm known junction tree algorithm compute marginals general graphs 
junction tree algorithm viewed combining ideas elimination algorithm sum product algorithm 
basic idea data structure nodes cliques single nodes 
graph known hypergraph 
variant sum product algorithm defined defines messages pass cliques single nodes algorithm run tree cliques 
cliques form tree 
turns possible general cliques original graph cliques augmented graph obtained triangulating original graph 
conceptually go operations associated elimination algorithm specific elimination ordering 
performing operations perform graph theoretic triangulation process 
defines set cliques formed tree 
sum product algorithm running tree yields single marginal marginals marginal mean marginal probability variables clique 
computational complexity algorithm determined size largest clique lower bounded treewidth graph 
summary exact inference algorithms elimination algorithm sum product algorithm junction tree algorithm compute marginal probabilities systematically exploiting graphical structure essence exploiting conditional independencies encoded pattern edges graph 
best case treewidth graph small elimination order achieves treewidth easily 
classical graphical models including hidden markov models trees jordan state space models associated kalman filtering kind 
general treewidth overly large cases exact algorithms viable 
see proceed case complex models note large treewidth heuristically implies intermediate terms computed exact algorithms sums terms 
provides hope concentration phenomena exploited approximate inference methods 
concentrations exist necessarily dependent specific numerical values potentials 
sections overview algorithms aim exploit numerical graph theoretic properties graphical models 
sampling algorithms sampling algorithms importance sampling markov chain monte carlo mcmc provide general methodology probabilistic inference liu robert casella 
graphical model setting provides opportunity graph theoretic structure exploited design analysis implementation sampling algorithms 
note particular class mcmc algorithms known gibbs sampling requires computation probability individual variables conditioned remaining variables 
markov properties graphical models useful conditioning called markov blanket node renders node independent variables 
directed graphical models markov blanket set parents children parents node parents nodes child common node 
undirected case markov blanket simply set neighbors node 
definitions gibbs samplers set automatically graphical model specification fact exploited bugs software gibbs sampling graphical models gilks thomas spiegelhalter 
markov blanket useful design metropolis algorithms factors appear markov blanket set variables considered proposed update neglected 
variety hybrid algorithms defined exact inference algorithms locally sampling framework jensen kjaerulff kong murphy 
variational algorithms basic idea variational inference characterize probability distribution solution optimization problem perturb optimization problem solve perturbed problem 
methods applicable principle general probabilistic inference far main domain application graphical models 
earliest application general statistical inference variational methods formulated terms minimization kullback leibler kl divergence space optimization performed set simplified probability distributions generally obtained removing edges graphical model see titterington volume 
general perspective emerged relaxes constraint optimization performed set probability distributions longer focuses kl divergence sole optimization functional interest 
approach yield significantly tighter approximations 
briefly overview key ideas detailed presentation see wainwright jordan 
focus finitely parameterized probability distributions express exponential family form 
particular assume factors expressed exponential family form relative common measure product factors exponential family form write xv exp xv xv vector sufficient statistics vector components functions cliques graph cumulant generating function defined integral log exp xv denotes inner product 
important facts cumulant generating function convex function convex domain brown convex function expressed terms conjugate dual function rockafellar 
allows express cumulant generating function sup set realizable mean parameters 
xv xv conjugate dual function sup 
note duality 
expressed cumulant generating function solution optimization problem 
optimizing arguments precisely expectations wish solve example discrete case marginal probabilities focus section 
equation general expression inference problem algorithm junction tree algorithm aims solve 
approximate inference algorithms obtained perturbing optimization problem various ways 
approach restrict optimization class simplified tractable distributions known mean field approach jordan ghahramani jaakkola saul 
consider subset corresponds distributions tractable vis vis algorithm junction tree algorithm restrict optimization set sup 
optimizing values mean field approximations expected sufficient statistics 
restricted optimization inner approximation set obtain lower bound cumulant generating function 
class variational inference algorithms obtained considering outer approximations set particular parameters satisfy number consistency relationships expected sufficient statistics probability distribution 
bethe approximation involves retaining consistency relationships arise local neighborhood relationships graphical model dropping constraints yedidia freeman weiss 
example linked pair nodes marginal st xs xt equal xs sum xt marginal su xs xu equal xs sum xu graphical models link 
refer set containing vectors wherem 
carrying optimization set bethe variational problem sup bethe 
note replaced bethe expression 
assumption infeasible compute conjugate function shown infinite outside approximation needed 
quantity bethe known bethe entropy sum entropy terms associated edges graph natural counterpart 
attempt solve adding lagrange multipliers reflect constraints define differentiating obtain set fixed point equations 
surprisingly equations equivalent sum product algorithm trees 
messages mij xj simply exponentiated lagrange multipliers 
bethe approximation equivalent applying local message passing scheme developed trees graphs loops yedidia freeman weiss 
algorithm surprisingly successful practice particular algorithm choice applications error control codes discussed section 
area variational inference quite active years 
algorithms known cluster variation methods proposed extend bethe approximation high order clusters variables yedidia freeman weiss 
papers higher order variational methods include kappen minka 
wainwright jordan algorithms semidefinite relaxations variational problem 
theoretical analysis variational inference infancy see jordan initial steps analysis convergence 
variational inference methods involve treating inference problems optimization problems empirical bayes procedures particularly easy formulate variational framework applications variational methods date empirical bayesian 
framework require short full bayesian inference 
see example attias ghahramani beal papers devoted full bayesian applications variational inference 
jordan 
bioinformatics field bioinformatics fertile ground application graphical models 
classical probabilistic models field viewed instances graphical models variations models readily handled formalism 
graphical models naturally accommodate need fuse multiple sources information characteristic feature modern bioinformatics 
phylogenetic trees phylogenetic trees viewed graphical models 
briefly outline key ideas consider extensions 
assume set homologous biological sequences member set species homologous means sequences assumed derive common ancestor 
focus dna protein sequences individual elements sequences referred sites phylogenetic trees sequences characters morphological traits 
essentially current methods inferring phylogenetic trees assume sites independent making assumption 
represent phylogeny binary tree leaves tree observed values site different species nonterminals values site putative ancestral species see 
case dna nodes tree multinomial random variables states case proteins nodes multinomial states 
treating tree directed graphical model parameterize tree recipe annotating edge conditional probability state ancestral state 
fact likelihood phylogenetic tree generally independent choice root undirected formalism appropriate 
conditional probabilities generally parameterized terms evolutionary distance parameter estimated parametric form exponential decay equilibrium distribution nucleotides amino acids 
product local parameterizations obtain joint probability states site product sites plate obtain joint probability sites 
likelihood easily computed elimination algorithm pruning algorithm fig 

simple example phylogeny extant organisms sites 
tree encodes assumption speciation event speciation events lead extant organisms 
plate represents assumption sites evolve independently 
note classical generally drawn varying edge lengths denote evolutionary distance graphical model formalism distance simply parameter conditional distribution xv left implicit 
early instance graphical model elimination algorithm 
conditional probabilities ancestral states computed sum product algorithm 
classical methods fitting phylogenetic trees rely expectationmaximization algorithm parameter estimation expectation step computed sum product algorithm search tree topologies find maximum likelihood tree possible mcmc methods bayesian framework 
assumptions lead classical phylogenetic tree model wanting respects 
assumption site independence generally incorrect 
biochemical interactions affect mutation probabilities neighboring sites global constraints alter mutation rates conserved regions dna proteins 
second genes necessarily evolve tree topologies example bacteria mechanisms lateral gene transfer species 
graphical model formalism provides natural upgrade path considering realistic phylogenetic models capture phenomena 
lateral gene transfer readily accommodated simply re moving restriction tree topology 
lack independence sites captured replacing plate explicit array graphs site horizontal edges capturing interactions 
example consider markovian models edges ancestral nodes neighboring sites 
general models create loops underlying graph approximate inference methods generally required 
pedigrees linkage analysis attempt model relationships instances single gene different species evolutionary time pedigrees aimed finer level granularity 
pedigree displays parent child relationships group organisms single species attempts account presence variants gene flow population 
pedigree pedigree accounts flow multiple genes 
pedigrees turn special case graphical model known factorial hidden markov model 
briefly review relevant genetic terminology 
chromosome set loci correspond genes markers 
chromosomes occur pairs pair genes locus 
true humans chromosomes organisms 
models discuss easily specialized organisms chromosomes paired particular accommodate chromosomes humans 
gene occurs variant forms alleles population 
graphical models locus pair alleles 
full set pairs individual referred genotype individual 
genotype generally stochastic mapping phenotype set observable traits 
simplifying assumption trait determined single pair alleles see modeling formalism require generally inaccurate assumption 
process alleles pair locus selected transmitted offspring 
offspring receives allele locus father allele locus mother 
define multinomial genotype variables allele nth locus ith organism define im maternal allele locus 
denote corresponding phenotype variables 
denote binary haplotype variable variable equal organism receives father allele grandfather allele equal organism receives father maternal allele grandmother allele 
similarly im denote corresponding binary haplotype variable mother side 
relationships variables summarized graphical model 
letting haplotype variables take values probability probability offspring genotype parental genotypes simply mendel law 
graph simple example pedigree 
general pedigrees involve organisms 
fig 
pedigree organisms child father mother 
variables encode values genotype haplotype phenotype respectively locus organisms 
jordan graphical topology tree simplest cases 
general large numbers loops pedigrees sufficient size 
consider relationship haplotype variables different loci 
key biological fact pairs chromosomes cross alleles different chromosomes parent may chromosome child 
crossover modeled treating haplotype variables markov chain 
equal crossover occurs locus locus unequal 
probability crossover parameter model 
estimating parameters data yields estimate genetic distance loci yields genetic maps chromosomes 
connecting haplotype variables markovian assumption yields graphical model shown 
graph horizontal chains correspond chromosomes father mother single organism 
generally pedigrees involve multiple organisms pair horizontal chains organism 
chains uncoupled reflecting obvious fact independent different organisms 
coupling organisms restricted couplings variables couplings contained fig 
representation pedigree organism loci 
obtained pedigree diagram grouping genotype variables locus single oval denoted grouping phenotype variables single oval denoted connecting haplotype variables horizontally loci 
ovals suppressed simplify diagram 
model instance graphical model family known factorial hidden markov model fhmm ghahramani jordan see generic example 
classical algorithms inference pedigrees variants elimination algorithm fhmm correspond different choices elimination order lander green stewart 
algorithms viable small problems exact inference intractable general pedigrees 
focusing haplotype variables verified treewidth bounded number organisms computational complexity exponential number organisms 
gibbs sampling methods studied particular thomas bansal proposed blocking gibbs sampler takes advantage graphical structure 
ghahramani jordan suite variational gibbs sampling algorithms fhmms developments murphy 

error control codes graphical models play important role modern theory error control coding 
ties graphs codes explored early gallager seminal line research largely forgotten due part lack sufficiently powerful computational tools 
flurry built gallager shown surprisingly effective codes built graphical models 
codes graphical models effective codes known channels achieving rates near shannon capacity 
basic problem error control coding transmitting message sequence bits noisy channel way receiver recover original message despite noise 
general achieved transmitting additional redundant bits conjunction original message sequence 
receiver uses redundancy detect possibly correct corruption message due noise 
key problems deciding mapping messages redundant bits problem code design computing redundant bits message encoding problem estimating original message transmitted message decoding problem 
ways probability enters problem 
set possible messages source prior distribution 
treat distribution uniform assuming essence source code developed extracts statistical redundancy source redundancy distinct redundancy wish impose message redundancy designed appropriate channel 
second channel noisy 
simple example channel model binary symmetric channel message bit transmitted correctly probability flipped probability 
transmission events assumed independent identically distributed bits message sequence channel assumed memoryless 
assumption simplicity note graphical model formalism readily cope channels 
code taken random 
graphical model setting instance code identified graph means considering random ensembles graphs 
assumption inherent feature problem imposed allow probabilistic tools applied theoretical analysis properties code see 
show specific example graph ensemble known low density parity check ldpc code 
variable nodes graph binary valued represent message bits message specific instance possible states nodes 
factor nodes message nodes represent channel 
message variable xi factor node fi represents likelihood yi xi observed value transmitted message 
factor nodes message nodes parity check nodes equal number nodes connected equal 
prior probability distribution messages obtained instance parity check factors 
distribution assigns zero probability message violates parity checks assigns uniform probability message satisfies parity checks 
factor nodes node imposes independent constraint message sequences possible graphical models fig 

factor graph representation ldpc code 
code message bits parity checks 
factor nodes message bits represent parity checks factor nodes message bits represent channel 
message sequences 
ratio known rate code 
impose upper bound degree message nodes number parity check nodes message node connected upper bound degree parity check nodes number message nodes parity check node connected obtain family graphs referred low density parity check code 
practice theory ldpc codes incorporate additional level randomness degrees nodes selected distribution 
graphical model provides representation uniform prior messages likelihood transmitted bits original message bits 
formed inference problem goal computing posterior probability distribution original message bits transmitted bits 
mean mode distribution estimate original message 
principle inference algorithms associated general graphical models ldpc codes presence loops graph large scale graphs may large tens thousands rule exact inference 
mcmc algorithms appear viable domain 
algorithm practice sum product algorithm 
algorithm quite successful practice large block lengths large values 
theoretical convergence results available sum product algorithm jordan setting richardson shokrollahi urbanke 
averaging ensemble graphs shown average error probability goes zero iterations sum product algorithm conditions channel degree distributions block length code rate 
martingale argument invoked show codes behave average code justifies random selection specific code ensemble practice 
graphical models continue play central role development error control codes 
new codes designed proposing alternative graphical structures analysis decoding performance direct graphical structure 
graphical framework allows exploration complex channel models factor nodes represent channel connect multiple message nodes case channels memory 

speech language information retrieval fields speech recognition natural language processing information retrieval involve study complex phenomena exhibit kinds structural relationships 
graphical models play increasingly important role attempts model phenomena extract information needed problem domain 
markov hidden markov models markov models hidden markov models graphical models attempt capture simplest sequential structure inherent speech language 
cases graphical model chain multinomial state nodes xt links nodes parameterized state transition matrix 
case order markov model edge state xt state xt higher order markov models edges earlier states xt hidden markov models hmms additional set output nodes yt edges xt yt 
simple important application hmms arises part speech problem 
problem data word sequences goal tag words part speech noun verb preposition 
states xt take values parts speech typically dozen outputs yt take values words vocabulary typically tens thousands 
training data generally consist tagged data xt yt pairs subsequent inference problem infer sequence xt values sequence yt values 
speech recognition provides wide ranging set examples application hmms 
setting observables yt generally short term acoustic spectra continuous valued discretized 
single hmm designed cover small phonetic unit speech syllable diphone states xt generally treated unobserved latent variables 
library hmms created corpus training data 
hmms library assembled lattice large graphical model edges elemental hmms 
inference problem lattice hmms generally find mode posterior distribution state sequences computation effectively segments long observation sequence component speech units 
elemental hmms lattice trained segmented data portion speech sequence appropriate hmm known advance 
necessary estimate parameters associated transitions speech units problem known language modeling 
inthis setting markov models widely 
particular generally necessary provide estimate probability word previous words trigram model 
large number words vocabulary involves large number parameters relative amount available data fully bayesian methods ad hoc smoothing techniques generally necessary parameter estimation 
returning briefly bioinformatics worth noting hmms large number applications including problems gene finding dna domain modeling proteins 
see durbin eddy krogh mitchison discussion applications 
variations markovian models large number variations markovian models currently explored fields speech language processing bioinformatics 
models readily seen members larger family graphical models 
speech recognition models state output distribution yt xt commonly taken mixture gaussians reflecting multimodality graphical models fig 

variations hidden markov model theme 
model emissions mixture models 
coupled hmm 
factorial hmm 
model transition distribution mixture model 
frequently observed practice 
shown represented graphical model additional multinomial variables introduced encode allocations mixture components 
model remains eminently tractable exact inference 
serious departure coupled hidden markov model shown saul jordan 
model involves chains state variables coupled links chains 
model hybrid directed undirected formalisms instance family chain graphs lauritzen 
triangulating graph yields cliques size model remains tractable exact inference 
generally factorial hidden markov model shown instance model involves multiple chains ghahramani jordan 
particular model states coupled connection common set output variables variations considered links chains 
factorial hmm allows large state spaces represented small number parameters 
note triangulation model yields cliques size number chains moderate values model intractable 
variational mcmc methods employed successfully setting ghahramani jordan 
mixed memory markov model shown transition distribution mixture pairwise transitions saul jordan 
model possible approximate high order markov models small number parameters 
examples variations markovian models include hierarchical hmms murphy variable length hmms ron singer tishby buried hmms bilmes 
overview models context applications speech language problems see bilmes 
hierarchical bayesian model document collections large scale collections documents world wide web generally computationally infeasible attempt model sequential structure jordan fig 

latent dirichlet allocation model document collections 
outer plate represents corpus contains documents inner plate represents word document corpus 
individual documents field information retrieval generally built bag words assumption assumption word order document neglected purposes indexing retrieving documents 
simply assumption exchangeability leads de finetti theorem consideration latent variable models documents 
neglecting sequential structure may desirable attempt capture kinds statistical structure document collections particular notion documents characterized topics 
blei jordan ng proposed hierarchical latent variable model explicit representations documents topics words 
model shown 
words represented multinomial variable topics represented multinomial variable generally cardinality significantly smaller shown innermost plate words document generated repeatedly choosing topic variable choosing word corresponding topic 
probabilities topics document specific assigned value dirichlet random variable 
shown outermost plate variable sampled documents corpus 
example demonstrates graphical model formalism useful design wide variety mixed effects models hierarchical latent variable models 

discussion close remarks state graphical models statistics 
graphical models relegated periphery statistics viewed useful specialized situations central theme 
factors responsible increasing prominence 
hierarchical bayesian models naturally specified directed graphical models ongoing interest raised visibility 
second graph theoretic concepts key attempts provide theoretical guarantees mcmc algorithms 
third increasing awareness importance graph theoretic representations probability distributions fields statistical quantum physics bioinformatics signal processing econometrics information theory accompanied general increase interest applications statistics 
realization seemingly specialized methods developed disciplines instances general class variational inference algorithms led increasing awareness may alternatives mcmc general statistical inference worth exploring 
links graph theory computational issues major virtue graphical model formalism lacking 
setting large scale graphical models desirable general notion tradeoff computation accuracy base choices model specification design inference algorithms 
trade course missing graphical model formalism statistics large 
decision theoretic perspective ask loss functions reflect computational complexity statistical fidelity 
having foot graph theory probability theory graphical models may provide hints proceed wish aim significantly deeper linkage statistical science computational science 
aji 

generalized distributive law 
ieee trans 
inform 
theory 
arnborg corneil 

complexity finding embeddings tree 
siam algebraic discrete methods 
attias 

variational bayesian framework graphical models 
advances neural information processing systems solla leen 
ller eds 

mit press cambridge ma 
bilmes 

graphical models automatic speech recognition 
mathematical foundations speech language processing johnson khudanpur ostendorf eds 
springer new york 
blei jordan 

hierarchical bayesian models applications information retrieval discussion 
bayesian statistics bernardo berger dawid heckerman smith west eds 

oxford univ press 
brown 

fundamentals statistical exponential families ims hayward ca 
cowell dawid lauritzen spiegelhalter 

probabilistic networks expert systems 
springer new york 
durbin eddy krogh 

biological sequence analysis 
cambridge univ press 


general model genetic analysis pedigree data 
human 


evolutionary trees dna sequences maximum likelihood approach 
molecular evolution 
gallager 

low density parity check codes 
mit press cambridge ma 
ghahramani 

propagation algorithms variational bayesian learning 
advances neural information processing systems touretzky mozer hasselmo eds 

mit press cambridge ma 
ghahramani 

factorial hidden markov models 
machine learning 
gilks thomas 

language program complex bayesian modelling 
statistician 


empirical hierarchical bayesian estimation ancestral states 
systematic biology 
jensen kjaerulff 

blocking gibbs sampling large probabilistic expert systems 
international human computer studies 
jordan ed 

learning graphical models 
mit press cambridge ma 
jordan ghahramani jaakkola saul 

variational methods graphical models 
machine learning 
kschischang frey 

factor graphs sum product algorithm 
ieee trans 
inform 
theory 
lander 

construction genetic linkage maps humans 
proc 
nat 
acad 
sci 

lauritzen 

graphical models 
clarendon press oxford 


general lower bounds computer generated higher order expansions 
proc 
th conf 
uncertainty artificial intelligence 
morgan kaufmann san mateo ca 
liu 

monte carlo strategies scientific computing 
springer new york 
graphical models minka 

family algorithms approximate bayesian inference 
ph dissertation massachusetts institute technology 
murphy 

dynamic bayesian networks representation inference learning 
ph dissertation univ california berkeley 
murphy 

linear time inference hierarchical hmms 
advances neural information processing systems 
mit press cambridge ma 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann san mateo ca 
richardson 

mixture models measurement error problems epidemiological studies 
unpublished manuscript 
richardson shokrollahi urbanke 

design capacity approaching irregular low density parity check codes 
ieee trans 
inform 
theory 
robert 

monte carlo statistical methods nd ed 
springer new york 
appear 
rockafellar 

convex analysis 
princeton univ press 
ron singer 

power amnesia learning probabilistic automata variable memory length 
machine learning 
saul 

boltzmann chains hidden markov models 
advances neural information processing systems tesauro touretzky leen eds 

mit press cambridge ma 
saul 

mixed memory markov models decomposing complex stochastic processes mixtures simpler ones 
machine learning 
shenoy 

axioms probability belief function propagation 
proc 
th conf 
uncertainty artificial intelligence 
morgan kaufmann san mateo ca 


loopy belief propagation gibbs measures 
proc 
th conf 
uncertainty artificial intelligence 
morgan kaufmann san mateo ca 
thomas bansal 

linkage analysis blocked gibbs sampling 
statist 
comput 

titterington 

bayesian methods neural networks related models 
statist 
sci 

wainwright 

graphical models exponential families variational inference 
technical report dept statistics univ california berkeley 
wainwright 

semidefinite relaxations approximate inference graphs cycles 
advances neural information processing systems 
mit press cambridge ma 
yedidia freeman 

generalized belief propagation 
advances neural information processing systems leen dietterich tresp eds 

mit press cambridge ma 
