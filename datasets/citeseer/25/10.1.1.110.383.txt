sequential monte carlo sampling methods bayesian filtering arnaud doucet corresponding author simon godsill christophe andrieu signal processing group department engineering university cambridge street cb pz cambridge uk email ad eng cam ac uk article overview methods sequential simulation posterior distributions 
methods particular interest bayesian filtering discrete time dynamic models typically nonlinear non gaussian 
general importance sampling framework developed unifies methods proposed decades different scientific disciplines 
novel extensions existing methods proposed 
show particular incorporate local linearisation methods similar previously employed determin istic filtering literature lead effective importance distributions 
furthermore describe method uses rao blackwellisation order take advantage analytic structure important classes state space models 
final section develop algorithms prediction smoothing evaluation likelihood dynamic models 
keywords bayesian filtering nonlinear non gaussian state space models sequential monte carlo methods importance sampling rao blackwellised estimates problems applied statistics statistical signal processing time series analysis econometrics stated state space form follows 
transition equation describes prior distribution hidden markov process xk called hidden state process observation equation describes likelihood observations yk discrete time index 
bayesian framework relevant information 
xk observations including time obtained posterior distribution 
xk 
yk 
applications interested estimating recursively time distribution particularly marginals called filtering distribution xk 
yk 
filtering distribution routinely proceed filtered point estimates posterior mode mean state 
problem known bayesian filtering problem optimal filtering problem 
practical applications include target tracking gordon blind deconvolution digital communications channels clapp liu estimation stochastic volatility pitt digital enhancement speech audio signals godsill 
special cases including linear gaussian state space models kalman filter hidden finite state space markov chains impossible evaluate dis tributions analytically 
mid great deal attention devoted approximating filtering distributions see example 
popular algorithms extended kalman filter gaussian sum filter rely ical approximations anderson 
interesting automatic control field carried sequential monte carlo mc integration methods see 
possibly owing severe computational limitations time monte carlo algorithms largely neglected 
late massive increases computational power allowed numerical integration methods bayesian filtering kitagawa 
current research focused mc integration methods great advantage subject assumption linearity gaus model relevant includes ller west gordon kong liu 
main objective article include unified framework old algorithms proposed independently number applied science areas 
liu doucet doucet underline central le sequential importance sampling bayesian filtering 
contrary liu em hybrid schemes combining elements importance sampling markov chain monte carlo mcmc focus computationally cheaper alternatives 
describe possible improve current existing methods rao blackwellisation useful class dynamic models 
show extend methods compute prediction fixed interval smoothing distributions likelihood 
organised follows 
section briefly review bayesian filtering problem classical bayesian importance sampling proposed solution 
sequential version method allows obtain general recursive mc filter sequential importance sampling sis filter 
criterion minimum conditional variance importance weights obtain optimal importance function method 
unfortunately numerous models applied interest optimal importance function leads non analytic importance weights propose suboptimal distributions show obtain special cases algorithms literature 
firstly consider local linearisation methods state space model optimal importance function giving important examples 
linearisation methods promising way proceed problems type 
secondly consider simple importance functions lead algorithms currently known literature 
section resampling scheme limit practically degeneracy algorithm 
section apply rao blackwellisation method sis obtain efficient hybrid analytical mc filters 
section show mc filter compute prediction fixed interval smoothing distributions likelihood 
simulations section 
ii 
filtering sequential importance sampling preliminaries filtering state space model state sequence xk xk nx assumed unobserved hidden markov process initial distribution subsequently denote tational convenience transition distribution xk xk nx dimension state vector 
observations yk yk ny conditionally independent process xk distribution yk xk ny dimension observation vector 
sum model hidden markov state space model hmm described denote xn xk xk yk xk yn respectively state sequence observations time aim estimate recursively time distribution associated features including xn expectations form fn fn dx integrable fn nx recursive formula yn xn xn xn yn denominator expression typically computed analytically rendering analytic approach infeasible special cases mentioned 
assumed samples easily drawn xk xk evaluate xk xk yk xk pointwise 
bayesian sequential importance sampling sis generally impossible sample state posterior directly adopt importance sampling approach 
suppose samples drawn independently normalised importance function support state posterior 
estimate fn posterior expectation fn obtained bayesian geweke fn fn unnormalised importance weight 
weak assumptions fn converges fn see example geweke 
method recursive 
show obtain sequential mc filter bayesian suppose chooses importance function form xk importance function allows recursive evaluation time importance weights successive observations yk available 
obtain directly sequential importance sampling filter 
times sample sequential importance sampling sis xk evaluate importance weights normalising constant yk normalise importance weights special case algorithm introduced hand 
algorithms proposed literature shown special cases general simple algorithm 
choice importance function course crucial obtains poor performance importance function chosen 
issue forms topic subsection 
degeneracy algorithm bayesian interpreted monte carlo sampling method monte carlo integration method best possible choice importance function course posterior distribution 
ideally close case 
importance functions form variance importance weights increase stochastically time 
proposition unconditional variance importance weights obser interpreted random variables increases time 

proof proposition straightforward extension kong liu wong theorem kong case importance function form 
impossible avoid degeneracy phenomenon 
practice iterations algorithm normalised importance weights close zero large computational effort devoted updating trajectories contribution final estimate zero 
selection importance function limit degeneracy algorithm natural strategy consists selecting impor tance function minimises variance importance weights conditional simulated trajectory observations proposition xk xk yk importance function min variance importance weight var xk conditional proof 
straightforward calculations yield yk yk xk xk dxk xk variance zero xk xk yk 
optimal importance function yk optimal importance function xk yk introduced particular case 
importance function chen kong liu 
distribution obtain importance weight yk optimal importance function suffers major drawbacks 
requires ability sample xk yk evaluate proportionality constant yk yk xk xk dxk 
integral analytic form general case 
analytic evaluation possible important class models gaussian state space model non linear transition equation 
example nonlinear gaussian state space models 
consider model xk xk vk vk yk wk wk nx nx real valued non linear function ny nx observation matrix vk wk mutually independent gaussian sequences assumed known 
defining obtains mk xk yk xk xk yk mk yk xk exp yk cf xk wc yk cf xk models evaluations impossible 
suboptimal methods allow approximation optimal importance function 
monte carlo methods proposed approximate importance function asso ciated importance weight importance sampling doucet doucet markov chain monte carlo methods liu 
itera tive algorithms computationally intensive lack theoretical convergence results 
methods may useful non iterative schemes fail 
fact general framework sis allows consider importance functions built ap proximate analytically optimal importance function 
advantages alternative approach computationally expensive monte carlo methods standard convergence results bayesian importance sampling valid 
general method build suboptimal importance functions necessary build case case basis dependent model studied 
possible base developments previous suboptimal filtering anderson west considered subsection 

importance distribution obtained local linearisation simple choice selects importance function xk xk yk parametric distribution xk xk yk finite dimensional parameter determined xk yk nx ny deterministic mapping 
strategies possible idea 
illustrate methods novel schemes result gaussian importance function parameters evaluated local dependent simulated trajectory approach promising way proceeding models readily cheaply available 
auxiliary variables framework pitt shephard related suboptimal importance distributions proposed sample efficiently finite mixture distribution approximating filtering distribution 
follow different approach filtering distribution approximated directly resort auxiliary indicator variables 
local linearisation state space model propose model locally similar way extended kalman filter 
case linearisation performed aim obtaining importance function algorithm obtained converges asymptotically required filtering distribution usual assumptions importance functions 
example consider model nx nx xk xk vk vk nv yk xk wk wk nw nx ny differentiable vk wk mutually independent sequences 
performing approximation order observation equation anderson get yk xk wk xk xk xk xk xk wk xk xk defined new model similar evolution equation linear gaussian observation equation obtained xk xk 
model markovian depends xk 
form perform similar calculations obtain gaussian importance function xk xk yk mk mean mk covariance evaluated trajectory formula mk xk xk xk xk xk xk xk xk xk xk yk xk xk xk xk associated importance weight evaluated 
xk xk xk xk xk local linearisation optimal importance function assume xk log xk xk yk twice differentiable wrt xk nx define xk xk xk xk xk xt xk second order taylor expansion get xk xk xk xk point perform expansion arbitrary determined deterministic mapping xk yk 
additional assumption negative definite true xk concave setting yields xk xk xk xk xk suggests adoption importance function xk xk yk xk xk yk unimodal judicious adopt mode xk xk yk nx associated importance weight evaluated 
example linear gaussian dynamic observations distribution exponential family 
assume evolution equation satisfies xk vk vk nv observations distributed distribution exponential family yk xk exp yk real ny nx matrix ny ny models numerous applications allow consideration poisson binomial observations see example west 
yields xk xk xk xk xk covariance matrix yk xk definite negative 
determine mode distribution applying iterative newton raphson method initialised xk satisfies iteration simpler importance functions lead algorithms previ ously appeared literature 

prior importance function simple choice uses prior distribution hidden markov model importance func tion 
choice seminal 
methods proposed tanizaki 
case xk xk xk yk method inefficient simulations state space explored knowledge observations 
especially sensitive outliers 
advantage importance weights easily evaluated 
prior importance function closely related bootstrap filter method gordon see section iii 

fixed importance function simpler choice fixes importance function independently simulated observations 
case xk xk yk importance function adopted tanizaki tanizaki method stochastic alternative numerical integration method kitagawa 
results obtained poor dynamic model observations taken account leads cases unbounded unnormalised importance weights give poor results geweke 
iii 
resampling previously illustrated degeneracy sis algorithm unavoidable 
basic idea resampling methods eliminate trajectories small normalised importance weights concentrate trajectories large weights 
suitable measure degeneracy algorithm effective sample size neff introduced kong liu defined neff var evaluate neff exactly estimate neff neff neff neff fixed threshold sir resampling procedure rubin 
note possible implement sir procedure exactly operations classical algorithm ripley carpenter doucet doucet pitt 
resampling procedures reduce mc variation stratified sampling carpenter residual resampling liu may applied alternative sir 
appropriate algorithm sir scheme proceeds follows time 
importance sampling sis resampling monte carlo filter sample xk evaluate importance weights normalising constant yk normalise importance weights evaluate 
resampling neff neff 
sample index distributed discrete distribution elements satisfying pr 
xj neff algorithm subsection modified neff sir algorithm applied obtains dx dx resampling procedures decrease algorithmically degeneracy problem introduce practical theoretical problems 
theoretical point view resampling step simulated trajectories longer statistically independent lose simple convergence results previously 
established central limit theorem estimate fk obtained sir procedure applied iteration 
practical point view resampling scheme limits opportunity particles combined steps realized parallel 
trajectories high importance weights trajectories statistically selected times 
numerous fact equal 

loss diversity 
various heuristic methods proposed solve problem gordon higuchi 
iv 
rao blackwellisation sequential importance sampling section describe variance reduction methods designed structure model studied 
numerous methods developed reducing variance mc estimates including sampling hand control variates 
apply rao blackwellisation method see casella general topic 
sequential framework maceachern applied similar ideas dirichlet process models kong liu rao blackwellisation fixed parameter estimation 
focus application dynamic models 
show possible successfully apply method important class state space model obtain hybrid filters part calculations realised analytically part mc methods 
method useful cases partition state xk analytically marginalize component partition say instance demon example component partition conditionally linear gaussian state space model integrations performed analytically line rewrite posterior expectation kalman filter 
define 
fn terms marginal quantities fn fn dx dx dx dx dx dx fn dx assumption conditional realisation evaluated analytically estimates fn possible 
classical obtained importance distribution fn fn blackwellised estimate obtained analytically integrating distribution fn second rao im dx new estimate decomposition variance straightforward show variances importance weights obtained rao blackwellisation smaller obtained direct monte carlo method see example doucet doucet maceachern 
method estimate fn marginal quantities cautious applying mc methods developed previous sec tions marginal state space observations independent conditional generally longer independent conditional single process required modifications straightforward 
exam ple obtain optimal importance function associated importance weight yk important applications general method 
example conditionally linear gaussian state space model consider model xk ak xk xk bk xk vk yk ck xk xk dk xk wk markov process vk nv inv wk nw 
wants estimate possible mc filter rao blackwellisation 
conditional linear gaussian state space model integrations required rao blackwellisation method realized kalman filter 
introduced algorithm name rsa random sampling algorithm particular case homogeneous scalar finite state space markov chain 
case adopted optimal importance function possible sample discrete distribution evaluate importance weight yk kalman filter 
similar developments special case proposed liu 
algorithm blind deconvolution proposed liu particular case method time invariant channel gaussian prior distribution 
rao blackwellisation method framework particularly attractive xk continuous components restrict exploration discrete state space 
example finite state space hmm consider model xk xk yk markov process finite state space markov chain parameters time depend want estimate possible rao blackwellised mc filter 
conditional finite state space markov chain known parameters inte required rao blackwellisation method done analytically anderson 
prediction smoothing likelihood estimate joint distribution sis practice coupled resampling procedure limit degeneracy time form dx dx show possible obtain distribution approximations prediction smoothing distributions likelihood 
prediction approximation filtering distribution dxk want estimate step ahead prediction distribution xk xk xj xj dxk replacing xk approximation obtained obtain xk xj xj dxk evaluate integrals sufficient extend trajectories evolution equation 
step ahead prediction sample xk obtain random samples estimate dx dx dx fixed lag smoothing dxk dxk want estimate fixed lag smoothing distribution xk length lag 
time mc filter yields approximation dx dx obtain estimate fixed lag smoothing distribution dxk high approximation generally perform poorly 
fixed interval smoothing dxk want estimate xk time filtering algorithm yields approximation dx dx theoretically obtain xk distribution 
practically method soon significant problem requires resampling algorithm 
time simulated trajectories usually resampled times dis trajectories times approximation xk bad 
problem severe bootstrap filter resamples time instant 
necessary develop alternative algorithm 
propose original algorithm solve problem 
algorithm formula kitagawa xk xk xk xk xk xk dxk seek approximation fixed interval smoothing distribution form dxk dxk dxk support 
filtering distribution dxk weights different 
algorithm obtain weights 


initialisation time 

fixed interval smoothing evaluate importance weight algorithm obtained argument 
replacing xk approximation yields xk xk xk dxk xk xk owing approximated xk approximation dxk xk xk dxk dxk dxk xk dxk algorithm follows 
dxk algorithm requires storage marginal distributions dxk weights supports memory requirement nn 
complexity nn quite important 
complexity little lower previous developed algorithms kitagawa tanizaki require new simulation step 
likelihood applications particular model choice kitagawa kitagawa may wish estimate likelihood data simple estimate likelihood dx practice resampling steps approach impossible 
alternative decomposition likelihood yk yk estimate quantity samples yk yk xk xk dxk yk xk xk dxk yk 
obtained step ahead prediction approximation dxk xk 
expression pos sible avoid mc integration know analytically yk yk vi 
simulations yk section apply methods developed previously linear gaussian state space model classical nonlinear model 
models simulations length evaluate empirical standard deviation filtering estimates xk obtained mc methods ar xj simulated state jth simulation mc estimate xk th test signal th simulated trajectory associated signal 
denote calculations realised 
implemented filtering algorithms bootstrap filter sis prior importance function sis optimal suboptimal importance function 
fixed interval smoothers associated sis filters computed 
sis algorithms sir procedure neff 
state percentage iterations sir step importance function 
linear gaussian model consider model xk xk vk yk xk wk vk wk white gaussian noises mutually independent vk wk 
model optimal filter kalman filter anderson 

optimal importance function optimal importance function xk xk yk mk xk yk mk associated importance weight equal 
results yk xk exp yk xk kalman filter obtain ar 
different mc filters results table table 
trajectories estimates obtained mc methods similar obtained kalman 
sis algorithms similar performances bootstrap filter smaller computational cost 
interesting algorithm optimal importance function limits seriously number resampling steps 
nonlinear series consider nonlinear model gordon kitagawa tanizaki xk xk vk xk xk cos vk xk yk xk wk xk wk vk wk mutually independent white gaussian noises vk wk 
case possible evaluate analytically yk xk sample simply xk xk yk 
propose apply method described 
consists locally observation equation 

importance function obtained local linearisation get yk xk xk xk xk xk xk xk xk xk wk xk xk xk xk wk xk wk obtain linearised importance function xk xk yk 
results mk xk xk xk yk xk xk mk case possible estimate optimal filter 
mc filters results displayed table 
average percentages sir steps table 
model requires simulation samples preceding 
fact variance dynamic noise important trajectories necessary explore space 
interesting algorithm sis suboptimal importance function greatly limits number resampling steps prior importance function avoiding mc integration step needed evaluate optimal importance function 
roughly explained fact observation noise small yk highly informative allows limitation regions explored 
vii 
overview sequential simulation methods bayesian filtering general state space models 
include general framework sis numer ous approaches proposed independently literature years 
original extensions described including local linearisation tech niques yield effective importance distributions 
shown rao blackwellisation allows analytic structure important dynamic models described procedures prediction fixed lag smoothing likelihood evaluation 
methods efficient suffer drawbacks 
de samples inevitably occurs methods described time proceeds 
sample regeneration methods mcmc steps improve situation maceachern 
second problem simulating fixed ters covariance matrices noise variances assumed known examples 
methods described allow regeneration new values non dynamic parameters expect rapid sample set 
combination techniques mcmc steps useful rao blackwellisation methods liu give insight approached 
technical challenges posed problem wide range important applications rapidly increasing computational power stimulate new exciting developments field 
construction discrete time nonlinear filter monte carlo methods variance reducing techniques 
systems control japanese 
random sampling approach state estimation switching environments 
automatica 
anderson moore 
optimal filtering englewood cliffs 
best gilks 
dynamic conditional independence models markov chain monte carlo methods 
journal american statistical association pp 

switching state space models likelihood function filtering smoothing 
journal statistical planning inference pp 
carpenter clifford 
improved particle filter non linear problems 
technical report university oxford dept statistics 
casella robert 
rao blackwellisation sampling schemes 
biometrika pp 

chen liu 
predictive updating methods application bayesian classification 
journal royal statistical society 
clapp godsill 
fixed lag smoothing sequential importance sampling 
forthcoming bayesian statistics bernardo berger dawid smith eds oxford university press 
doucet 
monte carlo methods bayesian estimation hidden markov models 
application radiation signals 
ph thesis university paris sud orsay french 
doucet 
sequential simulation methods bayesian filtering 
tech nical report university cambridge dept engineering cued eng tr 
available mcmc preprint service www stats bris ac uk mcmc 
geweke 
bayesian inference econometrics models monte carlo inte 
econometrica 
godsill rayner 
digital audio restoration statistical model approach springer 
gordon salmond smith 
novel approach nonlinear non gaussian bayesian state estimation 
iee proceedings 
gordon 
hybrid bootstrap filter target tracking clutter 
ieee transactions aerospace electronic systems 
mayne 
monte carlo techniques estimate condi tional expectation multi stage non linear filtering 
international journal control 

monte carlo techniques prediction filtering non linear stochastic processes 
automatica 
higuchi 
monte carlo filtering genetic algorithm operators 
journal statistical computation simulation 

stochastic processes filtering theory academic press 
kitagawa 
non gaussian state space modeling nonstationary time series 
journal american statistical association 
kitagawa 
smoothness priors analysis time series lecture notes statistics springer 
kong liu wong 
sequential imputations bayesian missing data problems 
journal american statistical association 
liu chen blind deconvolution sequential imputation 
journal american statistical association 
liu 
independent sampling comparison rejection sam pling importance sampling 
statistics computing 
liu chen 
sequential monte carlo methods dynamic systems 
journal american statistical association 
maceachern clyde liu 
sequential importance sampling nonparametric bayes models generation forthcoming canadian journal statistics 
ller 
monte carlo integration general dynamic models 
contemporary mathematics 
ller 
posterior integration dynamic models 
computing science statistics 
pitt shephard 
filtering simulation auxiliary particle filters 
forthcoming journal american statistical association 
ripley stochastic simulation wiley new york 
rubin 
sir algorithm simulate posterior distributions 
bayesian statistics eds bernardo degroot lindley smith oxford university press 
smith gelfand 
bayesian statistics tears sampling resampling perspective 
american statistician 
stewart mccarty 
bayesian belief networks fuse contin uous discrete information target recognition tracking situation assess ment 
proceeding conference spie 

applying monte carlo method optimum estimation systems random disturbances 
automation remote control 
tanizaki 
nonlinear filters estimation applications lecture notes economics mathematical systems springer berlin 
tanizaki mariano 
prediction filtering smoothing non linear non normal cases monte carlo integration 
journal applied econometrics 
tanizaki mariano 
nonlinear non gaussian state space modeling monte carlo simulations 
journal econometrics 

detection estimation abruptly changing systems 
auto 
west 
mixtures models monte carlo bayesian updating dynamic mod els 
computer science statistics 
west harrison 
bayesian forecasting dynamic models springer verlag series statistics nd edition 

monte carlo technique prob lems optimal data processing 
automation remote control 
viii 
tables ar bootstrap prior dist optimal dist table mc filters linear gaussian model percentage sir prior dist optimal dist table percentage sir steps linear gaussian model ar bootstrap prior dist linearised dist table mc filters nonlinear time series percentage sir prior dist linearised dist table percentage sir steps nonlinear time series 
