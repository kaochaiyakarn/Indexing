machine learning kluwer academic publishers boston 
manufactured netherlands 
similarity models word cooccurrence probabilities ido dagan dagan macs ac il dept mathematics computer science bar ilan university ramat gan israel lillian lee cs cornell edu department computer science cornell university ithaca ny usa fernando pereira pereira research att com labs research park ave florham park nj usa editors claire cardie raymond mooney 
applications natural language processing nlp necessary determine likelihood word combination 
example speech recognizer may need determine word combinations eat eat beach 
statistical nlp methods determine likelihood word combination frequency training corpus 
nature language word combinations infrequent occur corpus 
propose method estimating probability previously unseen word combinations available information similar words 
describe probabilistic word association models distributional word similarity apply tasks language modeling pseudo word disambiguation 
language modeling task similarity model improve probability estimates unseen bigrams back language model 
similaritybased method yields perplexity improvement prediction unseen bigrams statistically significant reductions speech recognition error 
compare similarity estimation methods back maximum likelihood estimation methods pseudo word sense disambiguation task controlled unigram bigram frequency avoid giving weight easy disambiguate high frequency configurations 
similaritybased methods perform better particular task 
keywords statistical language modeling sense disambiguation 
data sparseness inherent problem statistical methods natural language processing 
methods statistics relative frequencies configurations elements training corpus learn evaluate alternative analyses interpretations new samples text speech 
analysis taken contains frequent configurations 
problem data sparseness known zero frequency problem witten bell arises analyses contain configurations occurred training corpus 
possible estimate probabilities observed frequencies estimation scheme generalize training data 
language processing applications sparse data problem occurs large data sets 
example essen steinbiss report split word lob corpus bigrams test partition occur dagan lee pereira training portion 
trigrams sparse data problem severe instance researchers ibm brown dellapietra desouza lai mercer examined training corpus consisting english words discovered expect word triples new english text absent training sample 
estimating probability unseen configurations crucial accurate language modeling aggregate probability unseen events significant 
focus particular kind configuration word cooccurrence 
examples cooccurrences include relationships head words syntactic constructions verb object adjective noun instance word sequences grams 
commonly models probability estimate previously unseen cooccurrence function probability estimates words cooccurrence 
example word bigram models probability conditioned word occurred training conditioning word typically calculated probability estimated frequency corpus jelinek mercer roukos katz 
method independence assumption cooccurrence frequent higher estimate regardless 
class similarity models provide alternative independence assumption 
models relationship words modeled analogy words sense similar ones 
instance brown 
suggest class gram model words similar cooccurrence distributions clustered word classes 
cooccurrence probability pair words estimated averaged cooccurrence probability corresponding classes 
pereira tishby lee propose soft distributional clustering scheme certain grammatical cooccurrences membership word class probabilistic 
cooccurrence probabilities words modeled averaged cooccurrence probabilities word clusters 
dagan marcus markovitch similarity model avoids building clusters 
word modeled specific class set words similar 
scheme predict unobserved cooccurrences 
model provide probability estimates component larger probabilistic model required say speech recognition 
class similarity methods cooccurrence modeling may sight special cases clustering weighted nearest neighbor approaches widely machine learning pattern recognition aha kibler albert cover hart duda hart stanfill waltz devroye gy rfi lugosi atkeson moore schaal 
important differences methods 
clustering nearest neighbor techniques rely representing objects points multidimensional space coordinates determined values intrinsic object features 
language modeling settings know word frequencies cooccurrences words certain configurations 
purpose modeling estimate probabilities cooccurrences cooccurrence statistics basis similarity measure model predictions 
means measuring word similarity predictions words words cooccur typical instance non distributional clustering similarity models learning methods word similarity defined intrinsic features independently predictions cooccurrence probabilities classifications associated particular words see instance cardie ng lee ng zavrel daelemans 

main contributions main contributions general scheme word similarity improve probability estimates back models comparative analysis similarity measures parameter settings important language processing tasks language modeling disambiguation showing similarity estimates useful 
initial study language model evaluation similarity model estimate unseen bigram probabilities wall street journal text compared standard back model katz 
testing held sample similarity model achieved perplexity reduction back unseen bigrams 
constituted test sample leading reduction test set perplexity 
similaritybased model tested speech recognition task yielded statistically significant reduction versus mistakes cases disagreement back model recognition error 
disambiguation evaluation compared variants initial method cooccurrence smoothing method essen steinbiss estimation method katz decision task involving unseen pairs direct objects verbs 
similarity models performed better back yielded accuracy experimental setting 
furthermore scheme jensen shannon divergence rao lin yielded statistically significant improvement error rate cooccurrence smoothing 
investigated effect removing extremely low frequency events training set 
contrast back smoothing events discarded training little discernible effect similarity smoothing methods suffer noticeable performance degradation singletons events occur exactly omitted 
organized follows 
section describes general similarity framework particular section presents functions measures similarity 
section details initial language modeling experiments 
section describes comparison experiments pseudo word disambiguation task 
section discusses related 
section summarizes contributions outlines directions 

distributional similarity models wish model conditional probability distributions arising cooccurrence linguistic objects typically words certain configurations 
consider pairs appropriate sets necessarily disjoint 
follows subscript th element pair conditional probability empirical estimate drawn base language model true probability unknown pair second element element dagan lee pereira denotes probability estimate base language model word pair second word 
denotes base estimate unigram probability word similarity language model consists parts scheme deciding word pairs require similarity estimate method combining information similar words course function measuring similarity words 
give details parts sections 
concerned similarity words conditioning events probabilities want estimate 

discounting redistribution data sparseness maximum likelihood estimate mle word pair probabilities unreliable 
mle probability word pair conditional appearance word simply pml frequency training corpus frequency 
pml zero unseen word pair pair predicted impossible 
generally mle unreliable events small nonzero counts zero counts 
language modeling literature term smoothing refer methods adjusting probability estimates small count events away mle try alleviate unreliability 
proposals address zero count problem exclusively rely existing techniques smooth small counts 
previous proposals zero count problem jelinek katz church gale adjust mle total probability seen word pairs leaving probability mass redistributed unseen pairs 
general adjustment involves interpolation mle linear combination estimator guaranteed nonzero unseen word pairs discounting reduced mle seen word pairs probability mass left reduction model unseen pairs 
back method katz prime example discounting pd pr pd represents turing discounted estimate katz seen word pairs pr denotes model probability redistribution unseen word pairs 
normalization factor 
extensive comparison study chen goodman indicated back better interpolation estimating bigram probabilities consider interpolation methods easily incorporate similarity estimates interpolation framework 
original back model katz model predicting unseen word pairs model backed unigram model unseen bigrams 
similarity models conceivable backing detailed model unigrams advantageous 
generalize katz formulation writing pr enabling similarity estimates unseen word pairs unigram frequency 
observe similarity estimates unseen word pairs 
investigate estimates pr derived averaging information words distributionally similar 

combining evidence similarity models assumption word similar word yield information probability unseen word pairs involving 
weighted average evidence provided similar words neighbors weight particular word depends similarity 
precisely denote increasing function similarity denote set words similar 
general form similarity model consider weighted linear combination predictions similar words psim norm norm normalization factor 
formula occur tends occur words similar 
considerable latitude allowed defining set evidenced previous put form 
essen steinbiss karov edelman implicitly set 
may desirable restrict fashion efficiency reasons especially large 
instance language modeling application section closest fewer words dissimilarity threshold value tuned experimentally 
directly replace pr back equation psim 
variations possible interpolating unigram probability pr psim represents effect linear combination similarity estimate back estimate exactly katz back scheme 
language modeling task section set experimentally simplify comparison different similarity models sense disambiguation section set 
possible depend contribution similarity estimate vary words 
dependences interpolated models jelinek mercer jelinek saul pereira advantageous 
introduce hidden variables require complex training algorithm pursue direction 
dagan lee pereira 
measures similarity consider word similarity measures derived automatically statistics training corpus opposed derived manually constructed word classes yarowsky resnik luk lin 
sections discuss related information theoretic functions kl divergence jensen shannon divergence 
section describes norm geometric distance function 
section examines confusion probability previously employed language modeling tasks 
course possible functions opted restrict attention reasonably diverse set 
function corresponding weight function 
choice weight function extent arbitrary requirement increasing similarity extremely constraining 
clearly performance depends weight function impossible try conceivable 
section describe experiments evaluating similarity models weight functions 
similarity functions describe depend base language model may may katz discounted model section 
discuss complexity computing similarity function noted current implementation time cost construct matrix word word similarities parameter training takes place 

kl divergence kullback leibler kl divergence standard informationtheoretic measure dissimilarity probability mass functions kullback cover thomas 
apply conditional distributions induced words words log 
non negative zero 
kl divergence non symmetric obey triangle inequality 
defined case 
unfortunately generally hold samples smoothed estimates redistribute probability mass zero frequency events 
forces sum calculation expensive large vocabularies 
divergence computed set wd role free parameter control relative influence neighbors closest high wd non negligible extremely close low distant neighbors contribute estimate 
chose negative exponential function kl divergence weight function analogy similarity models form cluster membership function related distributional clustering pereira form probability distribution arose sample drawn distribution cover thomas lee 
reasons heuristic theoretical rigorous probabilistic justification similarity methods 

jensen shannon divergence related measure jensen shannon divergence rao lin defined average kl divergence distributions average distribution shorthand distribution kl divergence nonnegative nonnegative 
letting furthermore easy see wq log entropy discrete density equation shows gives information gain achieved distinguishing distributions conditioning contexts pooling distributions ignoring distinction 
easy see computed efficiently depends conditioned words occur contexts 
letting grouping terms appropriately obtain log bounded ranging log smoothed estimates required probability ratios involved 
kl divergence case set wj plays role 

norm norm defined 
grouping terms express form depending common dagan lee pereira follows triangle inequality equality words strictly positive 
require weighting scheme decreasing wl free 
higher relative influence accorded nearest neighbors 
interesting note relations norm kl divergence jensen shannon divergence 
cover thomas give lower bound log base logarithm function 
lin notes upper bound 
confusion probability extending sugawara nishimura kaneko essen steinbiss confusion probability estimate word cooccurrence probabilities 
report improvement test set perplexity defined small corpus 
confusion probability grishman sterling estimate likelihood selectional patterns 
confusion probability estimate probability word substituted word sense contexts pc wc serves normalization factor 
contrast distance functions described pc curious property may necessarily closest word may exist word pc pc see section example 
confusion probability computed empirical estimates provided unigram estimates nonzero assume 
fact smoothed estimates provided katz back scheme problematic estimates typically preserve consistency respect marginal estimates bayes rule may 
consistent estimates mle safely apply bayes rule rewrite pc follows pc 
similarity models table 
summary similarity function properties name range base lm constraints tune 
log pc maxw bayes consistency jensen shannon divergence norm sum requires computation common examination equation reveals important difference confusion probability functions described previous sections 
functions rate similar roughly high pc greater large ratio large may think exceptional infrequent expect large 

summary features measures similarity listed summarized table 
base lm constraints conditions satisfied probability estimates base language model 
column indicates weight associated similarity function depends parameter needs tuned experimentally 

language modeling goal set experiments described section provide proof concept showing similarity models achieve better language modeling performance back 
similarity measure 
success experiments convinced similarity methods worth examining closely results second set experiments comparing similarity functions pseudo word disambiguation task described section 
language modeling experiments similarity model kl divergence dis similarity measure alternative unigram frequency backing bigram model 
bigram language model defined pd pr pr psim psim norm dagan lee pereira table 
perplexity reduction unseen bigrams different model parameters training reduction test reduction entire vocabulary 
noted earlier estimates smoothed avoid division zero computing employed standard katz bigram back model purpose 
application considered small fraction computing psim tunable thresholds described section purpose 
standard evaluation metric language models likelihood test data model intuitively test set perplexity wi wi represents average number alternatives bigram model test word 
better model lower perplexity 
task lower perplexity indicate better prediction unseen bigrams 
evaluated model comparing test set perplexity effect accuracy baseline bigram back model developed mit lincoln laboratories wall street journal wsj text dictation corpora provided arpa hlt program paul 
baseline back model follows katz design sake compactness frequency bigrams ignored 
counts model obtained words wsj text years 
perplexity evaluation tuned similarity model parameters minimizing perplexity additional sample words wsj text drawn arpa hlt development test set 
best parameter values 
values improvement perplexity unseen bigrams held word sample arpa hlt evaluation test set just 
unseen bigrams comprise sample improvement unseen bigrams corresponds test set perplexity improvement 
table shows reductions training test perplexity sorted training reduction different choices number closest neighbors 
values best ones similarity models equation clear computational cost applying similarity model unseen bigram 
lower values computationally preferable 
table see reducing incurs penalty perplexity improvement relatively low values appear sufficient achieve benefit similarity model 
table shows best value increases decreases lower greater weight conditioned word frequency 
suggests predictive power neighbors closest modeled fairly frequency conditioned word 
bigram similarity model tested language model speech recognition 
test data experiment pruned word lattices wsj closed vocabulary test sentences 
arc scores lattices sums acoustic score negative log likelihood language model score case negative log probability provided baseline bigram model 
lattices constructed new lattices arc scores modified similarity model baseline model 
compared best sentence hypothesis original lattice best hypothesis modified counted word disagreements hypotheses correct 
total disagreements similarity model correct cases back model 
advantage similarity model statistically significant level 
reduction error rate small number disagreements small compared number errors recognition setup employed experiments 
table shows examples speech recognition disagreements models 
hypotheses labeled back similarity boldface words errors 
similarity model better modeling regularities semantic parallelism lists avoiding past tense form hand similarity model mistakes function word inserted place punctuation written text 

word sense disambiguation experiments described previous section demonstrated promising results similarity estimation ran second set experiments designed help compare analyze somewhat diverse set similarity measures table 
unfortunately kl divergence confusion probability different requirements base language model run direct way comparison 
explained elected omit kl divergence consideration 
chose evaluate remaining measures word sense disambiguation task method noun verbs asked verb noun direct object 
measure absolute quality assignment probabilities case perplexity evaluation relative quality 
ignore constant factors normalize similarity measures 
dagan lee pereira 
task definition table 
speech recognition disagreements models commitments 
leaders felt point dollars commitments 
leaders fell point dollars followed france agreed italy followed france greece 
italy aide necessity change exist necessity change exists 
additional reserves reported 
additional reserves reported darkness past church darkness passed church usual word sense disambiguation problem method tested ambiguous word context asked identify correct sense word context 
example test instance sentence fragment bank question bank refers river bank savings bank alternative meaning 
sense disambiguation clearly important problem language processing applications evaluation task presents numerous experimental difficulties 
notion sense clearly defined instance dictionaries may provide sense distinctions fine coarse data hand 
needs training data correct senses assigned acquiring correct senses generally requires considerable human effort 
furthermore words possible senses essentially monosemous means test cases uniformly hard 
circumvent difficulties set pseudo word disambiguation experiment sch tze gale church yarowsky format follows 
list pseudo words constructed combination different words 
word contributes exactly pseudo word 
test set replaced corresponding pseudo word 
example pseudo word created words take data altered follows similarity models plans take plans take action take action method tested choose words pseudo word 
advantages pseudo words fold 
alternative senses control experimenter 
test instance presents exactly alternatives disambiguation method alternatives chosen frequency part speech 
secondly pre transformation data yields correct answer hand tagging word senses necessary 
advantages pseudo word experiments elegant simple means test efficacy different language models course may provide completely accurate picture models perform real disambiguation tasks create realistic settings making pseudo words words varying frequencies alternative pseudo senses 
ease comparison consider interpolation unigram probabilities 
model experiments differs slightly language modeling tests summarized follows 
data pd pr pr psim norm psim statistical part speech tagger church pattern matching tools due david yarowsky identify transitive main verbs head nouns corresponding direct objects words associated press newswire 
selected noun verb pairs frequent nouns corpus 
pairs undoubtedly somewhat noisy errors inherent part speech tagging pattern matching 
pairs derived building models reserving testing purposes 
similarity measures require smoothed models calculated katz back model equation pr maximum likelihood model pml 
furthermore wished evaluate hypothesis compact language model built affecting model quality deleting singletons word pairs occur training set 
claim particular language modeling katz 
built base models summarized table 
wished test effectiveness similarity unseen word cooccurrences removed test data verb object pairs occurred training set resulted unseen pairs occurred multiple times 
unseen pairs dagan lee pereira table 
base language models singletons singletons pairs pairs mle mle mle katz bo bo divided equal sized parts formed basis cross validation runs ti performance test set combined set tuning parameters necessary simple grid search evaluated error tuning set regularly spaced points parameter space 
test pseudo words created pairs verbs similar frequencies control word frequency decision task 
method simply rank verbs frequency create pseudo words adjacent pairs verb participated exactly pseudoword 
table lists randomly chosen pseudowords frequencies corresponding verbs 
table 
sample pseudoword verbs frequencies 
word typo occurring corpus 
take fetch magnify exit relabel error rate performance metric defined incorrect choices ties size test corpus 
tie occurs words making pseudo word deemed equally 

baseline experiments performances base language models shown table 
mle mle error rates exactly test sets consist unseen bigrams assigned probability maximum likelihood estimates ties method 
back models bo bo perform similarly 
back models consistently performed worse mle models chose mle models subsequent experiments 
ran comparisons measures utilize unsmoothed data norm jensen shannon divergence confusion probability pc 
similarity models 
sample closest words table 
base language model error rates mle mle bo bo table 
closest words word guy mle base language model 
rank words role kid shown top 
pc guy guy role kid kid people lot thing fire thing lot guy man man man doctor mother year girl doctor lot rest friend today son boy way bit son part role rank role rank kid rank section examine closest words randomly selected noun guy measures pc 
table shows closest words order base language model mle 
overlap closest words closest words little overlap closest words measures closest words respect pc words man lot common 
observe word guy fourth list words highest confusion probability respect guy 
examine case nouns kid role closely 
similarity functions kid second closest word guy role considered relatively distant 
pc case role highest confusion probability respect guy kid th highest confusion probability 
accounts differences 
table gives verbs occur guy kid role indicates rate words similar tend cooccur verbs 
observe verbs occur kid dagan lee pereira table 
noun verbs highest 
bold face verbs occur noun guy base language model mle 
noun verbs guy see get play give catch tell pick need kid get see take help want tell teach send give love role play take lead support assume star expand accept sing limit table 
verbs highest guy ratios 
numbers parentheses ranks 
admire bore fool play get table 
closest words word guy mle base language model 
pc guy guy role kid kid people lot thing fire thing lot guy reason mother break answer man ball reason lot answer doctor job tape boost thing rest ball reporter occur guy verb play commonly occurs role guy 
sort verbs decreasing guy different order emerges table play verb cooccur role ranked higher get verb cooccur kid indicating role higher confusion probability respect guy kid 
examine effect deleting singletons base language model 
table shows closest words order base language model mle 
relative order closest words remains words quite different mle 
data suggests effect singletons calculations similarity quite strong borne experimental evaluations described section 
similarity models conjecture effect due fact low frequency verbs data verbs appeared fewer nouns common verb occurred nouns 
omitting singletons involving verbs may drastically alter number verbs cooccur nouns 
similarity functions consider set experiments depend words surprising effect deleting singletons dramatic 
contrast backoff language model sensitive missing singletons turing discounting small counts inflation zero counts 

performance similarity methods shows results experiments test sets mle base language model 
parameter set optimal value corresponding training set 
rand shown comparison purposes simply chooses weights randomly 
set equal cases 
similarity methods consistently outperformed katz back method mle recall yielded error rates large margin indicating information word pairs useful unseen pairs unigram frequency informative 
similarity methods better rand indicates simply combine information words arbitrarily word similarity taken account 
cases edged methods 
average improvement pc difference significant level paired test 
error rate error rates test sets base language model mle 
error rates test set base language model mle 
methods going left right rand performances shown settings optimal corresponding training set 
ranged rand lj dagan lee pereira results mle case depicted 
see similaritybased methods achieving far lower error rates mle back rand methods performed best 
omitting singletons amplified disparity pc average difference significant level paired test 
important observation methods including rand suffered performance hit singletons deleted base language model 
indicate seen bigrams treated differently unseen bigrams seen bigrams extremely rare 
conclude create compressed similarity language model omitting singletons hurting performance task 
error rate error rates test sets base language model mle 
error rates test set base language model mle 
ranged analyze role parameter 
recall appears weight functions jensen shannon divergence norm rand lj wj wl controls relative influence similar words influence increases higher values 
shows value affects disambiguation performance 
curves shown corresponding choice similarity function base language model 
error bars depict average range error rates disjoint test sets 
immediately clear get performance results set higher jensen shannon divergence norm 
phenomenon results fact range possible values smaller compression values requires large scale differences distances correctly 
observe setting low causes substantially worse error rates curves level moving upwards 
long sufficiently large similarity models error rate effect beta test set error different similarities jensen mle jensen mle mle mle beta 
average range test set error rates varied 
similarity function indicated point style base language model indicated line style 
value chosen setting greatly impact performance 
furthermore shape curves base language models suggesting relation test set performance relatively insensitive variations training data 
fact higher values lead better error rates suggests role filter distant neighbors 
test hypothesis experimented similar neighbors 
shows error rate depends different fixed values 
lowest curves depict performance jensen shannon divergence norm set optimal value respect average test set performance appears distant neighbors essentially effect error rate contribution sum negligible 
contrast low value chosen upper curves distant neighbors weighted heavily 
case including distant neighbors causes serious degradation performance 
interestingly behavior confusion probability different cases adding neighbors improves error rate 
indicate confusion probability correctly ranking similar words order informativeness 
alternative explanation pc disadvantage employed context tunable weighting scheme 
distinguish possibilities ran experiment dispensed weights altogether 
took vote similar neighbors alternative chosen preferred majority similar neighbors note ignored degree alternatives preferred 
results shown 
see similar neighbors informative chosen confusion probability largest performance dagan lee pereira error rate jensen beta jensen beta beta beta confusion effect test set error different similarities mle 
average range test set error rates varied 
base language model mle 
similarity function indicated point style dashed dotted lines indicate suboptimal choice 
error rate jensen mle jensen mle mle mle confusion mle confusion mle effect test set error ignoring weights probabilities 
average range voting scheme test set error rates varied 
similarity function indicated point style base language model indicated line style 
gaps occurring low course methods performed case set neighbors 
graph provides clear evidence confusion probability measure informativeness words 
similarity models 
related large body notions similarity word clustering applications 
impossible compare methods directly assumptions experimental settings applications methods vary widely 
discussion mainly descriptive highlighting main similarities differences methods 

statistical similarity clustering disambiguation language modeling instance growing body research word similarity improve performance language processing problems 
similarity algorithms similarity scores word words directly making predictions rely similarity scores word representatives precomputed similarity classes 
early attempt automatically classify words semantic classes carried linguistic string project grishman hirschman 
semantic classes derived similar cooccurrence patterns words syntactic relations 
cooccurrence statistics considered class level alleviate data sparseness syntactic disambiguation 
sch tze captures contextual word similarity reducing dimensionality context representation singular value decomposition reduced dimensionality representation characterize possible contexts word 
information word sense disambiguation 
occurrences ambiguous word clustered cluster mapped manually senses word 
context vector new occurrence ambiguous word mapped nearest cluster determines sense occurrence 
sch tze emphasizes method avoids clustering words pre defined set classes claiming clustering introduce artificial boundaries cut words part semantic neighborhood 
karov edelman addressed data sparseness problem word sense disambiguation word similarity 
circular definition word similarity measure context similarity measure 
circularity resolved iterative process system learns set typical usages senses ambiguous word 
new occurrence ambiguous word system selects sense typical context similar current context applying procedure resembles sense selection process sh tze 
scheme employing word similarity disambiguation influenced dagan 

method computes word similarity measure directly word cooccurrence data 
word modeled set similar words plausibility unseen cooccurrence judged cooccurrence statistics words set 
similarity measure weighted tanimoto measure version grefenstette 
word association measured mutual information earlier word similarity hindle 
method dagan provide probabilistic models 
disambiguation decisions comparing scores different alternatives produce dagan lee pereira explicit probability estimates integrated directly larger probabilistic framework 
cooccurrence smoothing model essen steinbiss model produces explicit estimates word cooccurrence probabilities cooccurrence statistics similar words 
similarity estimates interpolated direct estimates gram probabilities form smoothed gram language model 
word similarity model computed confusion probability measure described evaluated earlier 
language modeling methods produce similarity probability estimates class models 
methods direct measure similarity word words cluster words classes global optimization criterion 
brown 
class gram model records probabilities sequences word classes sequences individual words 
probability estimate bigram contains particular word affected bigram statistics words class words class considered similar cooccurrence behavior 
word classes formed bottom hard clustering algorithm objective function average mutual information class cooccurrence 
introduces improvements mutual information clustering 
method applied part speech tagging records classes contained particular word bottom merging process 
word represented mixture classes single class 
algorithms kneser ney similar brown 
different optimization criterion number clusters remains constant membership assignment process 
pereira 
formalism statistical mechanics derive top soft clustering algorithm probabilistic class membership 
word cooccurrence probability modeled weighted average class cooccurrence probabilities weights correspond membership probabilities words classes 

thesaurus similarity approaches described previous section induce word similarity relationships word clusters cooccurrence statistics corpus 
researchers developed methods quantify similarity relationships information manually crafted wordnet thesaurus miller beckwith fellbaum gross miller 
resnik proposes node approach measuring similarity pair words thesaurus applies various disambiguation tasks 
similarity function information theoretic measure informativeness general common ancestor words thesaurus classification 
jiang conrath combine node approach edge approach similarity nodes thesaurus influenced path connects 
similarity method tested data set word pair similarity ratings derived human judgments 
lin derives general concept similarity measure assumptions desired properties similarity 
measure function number bits required describe concepts commonality 
describes similarity models instantiation measure hierarchical thesaurus applies wordnet part word sense disambiguation algorithm 

contextual similarity information retrieval query expansion information retrieval ir provides additional motivation automatic identification word similarity 
line ir literature considers words similar occur documents 
line considers type word similarity concerned similarity measured derived word cooccurrence statistics 
grefenstette argues cooccurrence document yields similarity judgements sharp query expansion 
extracts coarse syntactic relationships texts represents word set word cooccurrences relation 
word similarity defined weighted version tanimoto measure compares cooccurrence statistics words 
similarity method evaluated measuring impact retrieval performance 
extracted word cooccurrences syntactic relationships evaluated similarity measures data focusing versions cosine measure 
similarity rankings obtained measures compared produced human judges 

similarity language models provide appealing approach dealing data sparseness 
proposed general method similarity models improve estimates existing language models evaluated range similaritybased models parameter settings important language processing tasks 
pilot study compared language modeling performance similarity model standard back model 
improvement achieved bigram back model statistically significant relatively modest effect small proportion unseen events 
second detailed study compared similarity models parameter settings smaller manageable word sense disambiguation task 
observed similarity methods perform better unseen word pairs measure jensen shannon divergence best 
experiments restricted bigram probability estimation reasons simplicity computational cost 
relatively small proportion unseen bigrams test data effect similarity methods necessarily modest tasks 
believe benefits similarity methods substantial tasks larger proportion unseen events instance language modeling longer contexts 
obstacle principle doing trigram case example determining probability pairs consist word pairs single words 
number possible similar events element larger bigram case 
direct tabulation events similar event practical compact approximate representations dagan lee pereira investigated 
worth investigating benefit similaritybased methods improve estimates low frequency seen events 
need replace back model combines multiple estimates event example interpolated model context dependent interpolation parameters 
area investigation relationship similarity class approaches 
mentioned rely common intuition events modeled extent similar events 
class methods computationally expensive training time nearest neighbor methods require searching best model structure number classes hard clustering class membership estimation hidden parameters class membership probabilities soft clustering 
hand class methods reduce dimensionality smaller efficient test time 
dimensionality reduction claimed improve generalization test data evidence mixed 
furthermore class models theoretically satisfying probabilistic interpretations saul pereira justification similarity models heuristic empirical 
variety class language modeling algorithms described section related scope compare performance approaches 
comparison especially bring approaches common probabilistic interpretation worth pursuing 
acknowledgments alshawi joshua goodman rebecca hwa slava katz doug mcilroy stuart shieber yoram singer helpful discussions doug paul help bigram back model andrej michael riley providing word lattices speech recognition evaluation 
reviewers constructive criticisms editors issue claire cardie ray mooney help suggestions 
portions appeared previously dagan pereira lee dagan lee pereira reviewers papers comments 
part done author member technical staff visitor labs second author graduate student harvard university summer visitor labs 
second author received partial support national science foundation 
iri national science foundation graduate fellowship 
notes 
best knowledge particular distribution dissimilarity function statistical language processing 
function implicit earlier distributional clustering pereira tishby distributional similarity 
finch discusses word clustering provide experimental evaluation actual data 

experimented yielded poorer performance results 
similarity models 
alternative definitions 
model yielded best experimental results 

arpa wsj development corpora come versions verbalized punctuation 
experiments 

values refer base logarithms exponentials calculations 

noted bo data kl divergence performed slightly better norm 
aha kibler albert 

instance learning algorithms 
machine learning 
atkeson moore schaal 

locally weighted learning 
artificial intelligence review 
brown dellapietra desouza lai mercer 

class gram models natural language 
computational linguistics 
cardie 

case approach knowledge acquisition domain specific sentence analysis 
th national conference artifical intelligence pp 

menlo park california aaai 
chen goodman 

empirical study smoothing techniques language modeling 
th annual meeting acl 
somerset new jersey association computational linguistics 
distributed morgan kaufmann san francisco church 

stochastic parts program noun phrase parser unrestricted text 
second conference applied natural language processing 
somerset new jersey association computational linguistics 
distributed morgan kaufmann san francisco church gale 

comparison enhanced turing deleted estimation methods estimating probabilities english bigrams 
computer speech language 
cover hart 

nearest neighbor pattern classification 
ieee transactions information theory 
cover thomas 

elements information theory 
new york john wiley 
dagan lee pereira 

similarity methods word sense disambiguation 
th annual meeting acl pp 

somerset new jersey association computational linguistics 
distributed morgan kaufmann san francisco dagan marcus markovitch 

contextual word similarity estimation sparse data 
st annual meeting acl pp 

somerset new jersey association computational linguistics 
distributed morgan kaufmann san francisco dagan marcus markovitch 

contextual word similarity estimation sparse data 
computer speech language 
dagan pereira lee 

similarity estimation word cooccurrence probabilities 
nd annual meeting acl 
somerset new jersey association computational linguistics 
distributed morgan kaufmann san francisco devroye gy rfi lugosi 

probabilistic theory pattern recognition 
new york springer verlag 
duda hart 

pattern classification scene analysis 
new york wiley interscience 
essen steinbiss 

occurrence smoothing stochastic language modeling 
icassp vol 

piscataway new jersey ieee 
finch 

finding structure language 
unpublished doctoral dissertation university edinburgh 
gale church yarowsky 

statistical methods word sense disambiguation 
goldman ed fall symposium probabilistic approaches natural language pp 

menlo park california aaai 


population frequencies species estimation population parameters 
biometrika 
grefenstette 

syntactic context produce term association lists text retrieval 
international conference research development information retrieval sigir 
new york acm 
grefenstette 

explorations automatic thesaurus discovery 
boston kluwer academic publishers 
grishman hirschman 

discovery procedures sublanguage selectional patterns initial experiments 
computational linguistics 
dagan lee pereira grishman sterling 

smoothing automatically generated selectional constraints 
human language technology proceedings arpa workshop pp 

san francisco morgan kaufmann 
hindle 

noun classification predicate argument structures 
th annual meeting acl pp 

somerset new jersey association computational linguistics 
distributed morgan kaufmann san francisco jelinek mercer 

interpolated estimation markov source parameters sparse data 
proceedings workshop pattern recognition practice 
amsterdam north holland 
jelinek mercer roukos 

principles lexical language modeling speech recognition 
furui sondhi eds advances speech signal processing pp 

new york marcel dekker 
jiang conrath 

semantic similarity corpus statistics lexical taxonomy 
international conference 
taiwan academia sinica 
karov edelman 

learning similarity word sense disambiguation sparse data 
dagan eds fourth workshop large corpora pp 

somerset new jersey association computational linguistics 
katz 

estimation probabilities sparse data language model component speech recognizer 
ieee transactions acoustics speech signal processing assp 
kneser ney 

improved clustering techniques class statistical language modelling 
eurospeech pp 

grenoble france european speech communication association 
kullback 

information theory statistics 
new york john wiley sons 
lee 

similarity approaches natural language processing 
unpublished doctoral dissertation harvard university cambridge massachusetts 
lin 

syntactic dependency local context resolve word sense ambiguity 
th annual meeting acl pp 

somerset new jersey association computational linguistics 
distributed morgan kaufmann san francisco lin 

information theoretic definition similarity 
machine learning proceedings international conference icml 
san francisco morgan kaufmann 
lin 

divergence measures shannon entropy 
ieee transactions information theory 
luk 

statistical sense disambiguation relatively small corpora dictionary definitions 
rd annual meeting acl pp 

somerset new jersey association computational linguistics 
distributed morgan kaufmann san francisco miller beckwith fellbaum gross miller 

wordnet online lexical database 
international journal lexicography 
ng 

exemplar word sense disambiguation improvements 
cardie weischedel eds second conference empirical methods natural language processing emnlp pp 

somerset new jersey association computational linguistics 
ng lee 

integrating multiple knowledge sources disambiguate word sense exemplar approach 
th annual meeting acl pp 

somerset new jersey association computational linguistics 
distributed morgan kaufmann san francisco paul 

experience stack decoder hmm csr back gram language models 
arpa speech natural language workshop pp 

san francisco morgan kaufmann 
pereira tishby lee 

distributional clustering english words 
st annual meeting acl 
somerset new jersey association computational linguistics 
distributed morgan kaufmann san francisco rao 

diversity measurement decomposition analysis 
indian journal statistics 
resnik 

wordnet distributional analysis class approach lexical discovery 
workshop statistically natural language processing techniques 
menlo park california aaai 
resnik 

disambiguating noun groupings respect wordnet senses 
yarowsky church eds third workshop large corpora pp 

somerset new jersey association computational linguistics 


experiments linguistically term associations 
information processing management 
saul pereira 

aggregate mixed order markov models statistical language processing 
cardie weischedel eds second conference empirical methods natural language processing emnlp pp 

somerset new jersey association computational linguistics 
similarity models sch tze 

context space 
goldman ed fall symposium probabilistic approaches natural language pp 

menlo park california aaai 
sch tze 

dimensions meaning 
supercomputing proceedings acm ieee conference 
new york acm 
sch tze 

word space 
hanson cowan giles eds advances neural information processing systems pp 

san francisco morgan kaufmann 
stanfill waltz 

memory reasoning 
communications acm 
sugawara nishimura kaneko 

isolated word recognition hidden markov models 
icassp pp 

piscataway new jersey ieee 


extended clustering algorithm statistical language models tech 
rep 
dra cis cse rn 
forum technology dra 


hierarchical clustering words applications nlp tasks 
dagan eds fourth workshop large corpora pp 

somerset new jersey association computational linguistics 
witten bell 

zero frequency problem estimating probabilities novel events adaptive text compression 
ieee transactions information theory 
yarowsky 

word sense disambiguation statistical models roget categories trained large corpora 
coling pp 

nantes france 
zavrel daelemans 

memory learning similarity smoothing 
th annual meeting acl pp 

somerset new jersey association computational linguistics 
distributed morgan kaufmann san francisco 
