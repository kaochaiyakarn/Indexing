neural information processing letters reviews vol october independent component analysis extensions noise time bayesian ying yang learning perspective lei xu department computer science engineering chinese university hong kong hong kong china email cse cuhk edu hk submitted july accepted october summarizing typical approaches solving independent component analysis ica problems advances ica studies consider hybrid sources super gaussians ica extensions consider noise temporal dependence observations overviewed perspective bayesian ying yang independence learning 
new insights provided existing results literature number results 
keywords byy harmony learning ica pca mca nonlinear hebbian regularization model selection factor analysis non gaussian factor analysis nfa binary factor analysis bfa lmser learning layer net temporal extensions kalman filter independent state space 
independent component analysis ica popular literature neural networks signal processing aims blindly solving linear system observation 
linear system simply instantaneous system convolution system time series 
system source unknown solution types linear systems determined 
early effort problem eq backtracked name blind deconvolution period literature geophysical exploration oil gas called seismic wavelet describes reflection nature underground layers represents seismic waves received sensor array laid ground 
tackle indeterminacy problem efforts regarding eq system deterministic stochastic 
regarding deterministic system typical strategy assume minimum phase phase spectrum obtainable priori knowledge 
alternatively assumed finite length problem solved certain cases knowing endpoint partly samples 
regarding eq stochastic system problem solvable assumed bernoulli white noise series 
backtracked typical early effort problem eq widely field communication name blind channel equalization identification estimation 
readers referred survey 
typical early effort problem eq solving blind source separation assuming components mutually independent density takes form ica extensions noise time xu effort supported result tong inouye liu 
long component wise independent showed recovers constant scales permutation components components mutually independent gaussian 
problem comon name independent component analysis ica 
mapping recover waveform series subject unknown scales said blindly separate sources 
studies ica problem quite popularized literature neural networks extensions 
direction extending ica problem noisy environments 
typical example adding noise observation subject satisfying eq usually consists samples independent particularly denoting comes eq classic factor analysis fa model widely applied various data analyses 
direction extend ica problem noisy extensions considering temporal relation data 
direction going linear system considering certain nonlinear systems name nonlinear ica 
survey papers ica studies 
including studies focus directions systematic sketch unified perspective called bayesian ying yang byy harmony learning 
byy harmony learning firstly proposed developed past years statistical learning framework featured new regularization techniques parameter learning new mechanism implements model selection automatically parameter learning new class model selection criteria parameter learning 
main focus put problem eq extensions consider noise observation temporal dependence 
main contents listed follows typical solving approaches solving equations higher order statistics vs hebbian learning cost function approaches vs equation approaches cost functions pca nonlinear extensions pca vs mca ica vs ica byy independence learning bayesian ying yang system byy harmony learning regularization vs model selection ml hl regularization regularization vs model selection ii kl hl spectrum ying yang alternative procedure parameter learning regularization vs model selection iii convex function ica studies marginal densities prefixed vs learned loosely matching bit conjecture open issues natural gradient non invertible matrix extensions extensions ica noisy environment non gaussian factor analysis algebraic equation ml learning byy harmony learning nongaussian fa neural information processing letters reviews vol october model selection vs automatic model selection special cases lmser auto association approximate ml exact ml em algorithm non negative data analyses supervised learning layer net minimizing fitting error vs enforcing factor independence extensions ica temporal dependence typical approaches ica temporal dependence types temporal byy systems tfa temporal nfa space dimension selection temporal ica kalman filter identifiable state spaces temporal bfa temporal lmser supervised recurrent net new insights provided existing results systematic manner number new results introduced appropriate places context summarized concluding section 
due close relation linear systems eq eq discussions problem eq may similarly omitted 

typical solving approaches different approaches attempted solving ica problems 
approaches classified families solving equations optimizing costs 

solving equations higher order statistics vs hebbian learning higher order statistics algebraic equations higher order statistics moments subject constraints eq equivalently eq algebraic equations obtained matrix higher order statistics consideration 
nonlinear equations algebraic equations problems solving roots equation 
generally speaking algorithms developed literature numerical analysis solving roots algebraic equations adopted purpose 
approaches type features equations obtained batch samples statistics need computed collectively 
type difficult adaptively sample comes 
intuitively obtain number equations increasing order statistics solved eq satisfied 
usually statistics th order considered 
lack theoretical guides order considered statistics different orders usually certain dependence obtained joint equations may involve complicated dependent relations difficult described understood explicitly 
nonlinear joint equations usually roots 
course desire roots corresponds eq satisfied 
clearly conditions situation occurs 
encounter case roots corresponds eq satisfied roots 
find algorithm tackle problem 
number statistics considered increases exponentially order increases 
result consider large set joint nonlinear equations suffer high expense computation problems solved 
hebbian learning stochastic equations inspired fact implementing anti hebbian learning recurrent weights remove second order correlations outputs linear recurrent neural network higher order dependences outputs expected removed modifications 
implementing anti hebbian learning recurrent weights linear neuron replaced sigmoid neuron linear neuron followed sigmoid scalar function 
implementing nonlinear anti hebbian learning recurrent weights linear recurrent neural network 
modifications equivalent shown experiments partially theoretical analyses ica extensions noise time xu higher order dependences outputs removed 
type results originally motivated early studies ica 
similarly inspired fact implementing hebbian learning forward weights linear forward net perform pca second order correlations outputs removed studies removing higher order dependences outputs linear forward net implementing hebbian learning linear neuron replaced sigmoid neuron implementing nonlinear hebbian learning weights linear forward neural network 
types hebbian learning extensions closely related 
type linear recurrent net equivalently turned type linear forward net dimension inputs outputs 
denote matrix recurrent weights forward weights learned processes types hebbian learning extensions summarized stochastic equation input random vector output vector 
denotes nonlinear scalar function closely relates sigmoid non linearity linear neuron 
eq key term implements nonlinear hebbian learning expected tend eq satisfied 
dynamic stochastic equations contains may stably 
avoid problem appropriate function designed dynamic system stabilized 
type approaches usually referred heuristic features motivation nonlinear hebbian learning satisfy eq intuitively sound learning eq works guaranteed function tested experimentally convergence analysis usually quite easy task 
conducted convergence analysis usually able simple mean convergence sense hints lacks systematic way guide selecting appropriate lacks general guideline control convergence performance 
learning equation eq usually concise regular 
easy implemented 
especially learning may parallel level weight coefficient favorable hardware implementation 

cost function approaches vs equation approaches cost function approaches featured optimizing cost called contrast function 
maximize minimize cost function respect optimal point searched point eq satisfied 
cost function obtained typical aspects discussed follows 
solving joint equations directly turned cost minimized advantage considering problem way providing control errors solution ensure exactly 
dynamic stochastic equations eq converges stable solution corresponds descent process stochastic cost function 
getting cost function facilitate convergence analysis eq help optimization theory 
task finding cost usually straight forward may hints discussed subsection 
literature pca mca learning known number cost functions optimizations lead performing pca mca 
may extend costs implement ica simply replacing linear neuron sigmoid neuron 
independence eq implies cross dependence exists statistics components order 
motivates build cost bases higher order statistics higher order cumulants :10.1.1.131.165:10.1.1.10.7237
cost function comes typical statistical learning principles estimate product independent densities eq 
typical examples include maximizing likelihood ml minimizing mutual information mmi maximum information transfer infomax max negentropy neural information processing letters reviews vol october different statistical learning principles resulted cost functions may relate closely equivalent 
case invertible mmi infomax equivalent ml maximizes cost estimate density positive cost function equivalently turned likelihood called gibbs distribution long cost maximized gradient ascent related algorithm batch samples adaptively sample comes 
algorithms relate closely nonlinear hebbian learning approaches 
point clearly observed fact exactly nonlinear hebbian term eq 
clear link observed case pre whitening processing case ica mapping subject orthogonal constraint stochastic gradient ascent algorithm follows exactly nonlinear hebbian learning stiefel manifold specifically implemented approaches tab 
follows choice approach get stiefel manifold hebbian learning rule follows reduces known formula neglect simply setting eq neglect simply setting respectively 
furthermore pre whitening process implemented adaptively general input implemented jointly eq 

cost functions pca nonlinear extensions typical cost functions pca nonlinear extensions systematically discussed 
brief summary ica perspective 
losing generality focus families cost functions maximum output variation bounded linear system family featured principle consists key points 
choosing maximizes variation output denotes matrix norm maximum information transfer 
constraint maximization tend infinite undefined 
key point avoid imposing system bounded constant 
mathematically family cost functions share general form denotes measure describes variation nd order statistics usually describes major part variation measures literature nd order statistics 
summarized general form ica extensions noise time xu monotonically increasing scalar increases 
special examples eigenvalues covariance matrix nd order statistics accurate way describe variation include higher order statistics 
general measure entropy bases entire density covers statistics orders 
observed eq maximization subject reached boundary widely considered case orthogonal matrix get unconstrained version equivalently nonsingular subject systems eq eq gaussian results spans principal subspace spanned eigenvectors eigenvectors correspond largest eigen values updating maximization implemented adaptively sample oja subspace rule updating gradient direction generally consider diagonal matrix 
shown similar analysis maximum reached row vectors eigenvectors correspond largest eigen values respectively 
exactly performs pca 
degenerates equivalent case performs principal subspace analysis psa oja subspace rule 
best reconstruction family featured principle reconstruction best fit fitting usually measured second order 
neural information processing letters reviews vol october widely studied field signal image processing 
explicit constraint needed 
shown results performs psa 
help global convergence oja subspace rule firstly proved mathematically 
eq know minimum reached performs exactly pca 
similarly go nd order considering maximum likelihood fitting follows maximum relative variation family regarded extension family 
variation output maximized maximum information transfer 
avoid unbounded output system considered denotes input zero mean uncorrelated elements known diagonal matrix 
imposing constraint directly minimize variation words maximize variation purely contributed excluding contribution standard measure relative variation obtained combing variation general form follows variation monotonically increasing scalar increases 
special examples particularly simply case eq 
firstly proposed general directions extend pca cost function nonlinear cases implement sort ica 
simply replace linear system semi linear system sigmoid function monotonically increases increases 
typical example lmser learning obtained eq replace eq resulting firstly studied batch algorithm adaptive gradient algorithm provided experimental finding replacement linear neuron sigmoid neuron leads automatic breaking symmetry components subspace 
years lmser learning adaptive algorithm directly adopted implement ica promising results name nonlinear pca 
relations lmser learning ica approaches explored 
similarly corresponding eq considers higher order statistics indirectly implementing ica 
constraints removed bounded 
second general direction replace second order error error measures 
norm replaced general norm robust piecewise error measure discussed 
alternatively consider entire density eq eq 
various extensions obtained combining directions different ways 

pca vs mca ica vs ica literature ica regarded extension pca 
strictly speaking inexact 
ica extension whitening process eq called de correlated component analyses dca 
ica extensions noise time xu de correlated components require independence components second order statistics special case independent components require independence components orders statistics 
pca extreme case dca chooses de correlated components largest variances 
contrast called minor component analysis mca chooses de correlated components smallest variances 
mca extreme case dca 
lots combinations various types dca 
pca wide application pattern recognition signal processing neural networks various data analyses 
pca implemented adaptively sample constrained hebbian learning 
components resulted mca spans complement subspace orthogonal subspace spanned components resulted pca 
area signal processing spectrum estimation name svd singular value decomposition 
got current name pca representing pattern class called dual pattern recognition approach 
studied anti hebbian learning adaptive implementation application total square fitting extension higher order hebbian learning 
mca widely studied literature neural networks 
pca mca pair dual representations 
adapting direction hebbian pca anti hebbian subject looks sign difference learning direction 
appropriate implementation ensures eventually satisfying learning process converged different pca mca 
level updating rules existing rules pca turned ones mca simply changing sign adapting direction 
efforts searching rule implement pca mca simply changing sign learning 
argue apparent difficulty difficult solve 
eq results principal subspace satisfied automatically 
contrast get minor subspace directly tend infinity 
apparent difficulty comes simplification discards type constraints eq 
restoring constraint bounded stably reach solution spans minor subspace 
similarly follows eq lead minor subspace 
words dual representation apply constraint eq constraint may ignored pca mca 
losing generality take general form example leads pca unconstrained minimization 
case satisfied matrix learning process stabilized learning process remains converged 
gradient follows get gradient direction updating pca mca difference sign pca versus mca 
may stabilize terminate learning process learning stepsize stabilized maximized minimized 
furthermore extending de correlation components independence components orders statistics correspondingly pca principal ica ica chooses independent components largest variances 
example lmser learning eq implements ica 
mca minor ica ica chooses independent components smallest variances 
example follows eq anti hebbian learning mca 
eq implements ica 

byy independence learning 
bayesian ying yang system type cost function comes called byy independence learning special case bayesian ying yang byy harmony learning firstly proposed developed past neural information processing letters reviews vol october 
bayesian ying yang system years 
shown fig consider joint distribution observation inner representation learning system complement aspects 
hand interpreted generated backward path inner distribution structure subject certain learning tasks hand interpreted mapped inner representation forward path match target density aspects reflect types bayesian decomposition joint density constraint theoretically identical 
types bayesian decomposition may necessarily equal practical consideration denote different notations differences may come different biological physical structural constraints components usually different complementary bayesian representations famous chinese ancient ying yang philosophy called yang machine consists observation space called yang space forward pathway called yang pathway called ying machine consists invisible domain ying space ying backward pathway pair ying yang models called bayesian ying yang byy system 
usually parzen window estimate denotes gaussian density mean vector covariance matrix empirical density particularly function task learning byy system consists specifying rest components system 
shown fig architecture byy system featured combination specific structures depending parametric typical architectures backward architecture forward architecture bi directional architecture shortly architecture bi architecture respectively 
implement learning task nature different perspective different performance 
ica extensions noise time xu free parametric parametric system backward architecture forward architecture bi directional architecture parametric free parametric input 
typical architectures structure describes nature inner representation features nature learning tasks specific byy system performs 
considering eq shown fig get independence byy system performs learning tasks ica various extensions 
considering structures lead typical learning tasks 
specific byy architecture specific structure tasks 
pa rameter learning determining value consists unknown parameters 
selecting dimension vector call second task model selection collection specific byy systems different values corresponds family specific models share system configuration different scales 

byy harmony learning basic learning principle ying machine yang machine best harmony twofold sense mathematically principle implemented 
ying yang pair eq harmony measure follows measure comes cross entropy discrete densities form infinitesimal volume observed maximizing eq interesting natures matching nature fixed pushes complexity nature fixed pushes simplest form 

neural information processing letters reviews vol october discussed eq kind complexity form statistical perspective 
words maximization functional implements harmony learning shortly hl principle mathematically 
set form eq normalization samples discrete continuous density represented considering infinitesimal volume approximately large 
follows eq lead eq 
clearly returns eq discrete 
simplest special case know eq complexity nature eq 
discussed matching nature eq equivalent implementing maximum likelihood ml learning principle parametric model role ignored simply setting general cases hl learning differs ml learning extra features 
term takes role learning finite size samples 
discussed sec type regularization shortly called regularization consists regularization techniques data smoothing normalization 
second ying yang pair eq known directly set samples eq complexity nature eq bring big different role model selection possible 
detailed discussion 
implementation model selection parameter learning parameter learning conventional stage style 
specifically implementation strategies eq 
parallel implementation discussed letting variance zero equivalent removing th dimension reducing dimension complexity nature eq push density extra dimension 
set large implement take specific value effectively reduced appropriate model selection automatically parallel parameter learning 
demonstrated fig learning eq push set extra parameters large effectively smallest value maximum reached 
feature shared existing approaches literature 
conventional approaches parameter learning model selection phase style 
parameter learning usually maximum likelihood principle 
model selection different criterion aic bic mdl 
model selection criteria usually parameter learning maximum likelihood criterion model selection especially small size training samples 
stage implementation want problem eq implemented stages follows stage enumerate small value incrementally value perform parameter learning eq get best parameter value direct implementation eq lead discussed case fig avoided simply assuming comes family equal variances components 
covariance matrix real binary 
stage ii select best upper bound 
demonstrated fig learning eq enumerated possible range constrained fixed covariance matrix takes shape shown fig 
maximum reached value increases 
get insight consider classic factor analysis eq eq case ica extensions noise time xu constraint vs set variance equivalent reduce 
model selection parameter learning interval automatic model selection parameter learning value large 
neural information processing letters reviews vol october finite set samples decreases st term decreases second term trades decreasing decreases reaches minimum increases 
increases zero brings drops rapidly generally inverse shape shown fig 
denote smallest value rapidly tend best dimension selected eq model eq generally considered noise 
particularly best dimension selected case eq noise 
alternatively parameter learning stage eq replaced systematically investigated early study byy learning shortly called kl learning 
particularly architecture minimization respect free results equivalent ml learning eq 
case implement ml learning phase model selection eq second phase 
complexity nature eq implementation eq lead case fig need impose assumption comes family equal variances components 

regularization vs model selection ml hl regularization regularization model selection different strategies tackling problem finite size samples 
model selection prefers model complexity compact inner representation aimed extra representation space released 
contrast regularization imposed model fixed scale representation space dimension larger needed inner representation spread uniformly possible representation space distribution simple possible equivalent model reduced complexity 
harmony learning eq attempts compress representation space complexity eq 
architecture demonstrated maximization respect free results concentrates peak point get winner take wta competition variant eq 
discussed complexity nature byy learning eq aim compact inner representation automatic model selection discarding extra representation space parameter learning 
free lunch 
wta operation eq locally sample learning sensitive initialization parameters way samples resulting samples aggregated small representation space 
usually leads local maximum solution eq 
pre specifying uniform inner representation imposing unity covariance matrix wta operation 
feature automatic model selection lost representation space scale fixed 
model selection eq second phase 
soft competition eq replace wta competition eq ml learning equivalently kl learning eq architecture eq regularized spread inner representation improves local maximum problem 
free lunch 
model selection ability considerably weaken especially small size samples 
model selection eq needed second phase 
phase style regularization wta eq may imposed harmony learning eq automatic model selection occurs external help certain internal mechanism 
externally combine kl learning eq harmony learning eq ways ica extensions noise time xu simplest way kl learning eq resulted parameters initialization harmony learning eq 
way suggested eq replaced appropriate gradually reducing zero value regularization role eq takes effect gradually decades learning goes 
combine kl hl simulated annealing way kl implemented early period learning gradually switched hl learning goes 
disadvantage computing cost high parameter learning repeatedly conducted 
internally regularization wta eq may imposed harmony learning eq bi architecture role letting free decided eq consider bi architecture designed structure able wta eq 
specifically structure bundled ying machine posteriori estimation eq decoupled ying machine facilitate computation 
case suggested binary vector eq 
real vector may case eq harmony learning eq push form complexity extra constraints imposed prevent eq simplified maximum searched simply enumerating possibilities 
regularization observed perspective number local considerably reduces comparison eq 
free lunch 
problem transferred difficulty specifying function form simple representation ability limited far optimal 
complicated free parameters creates certain problems needs regularization imposed 
regularization wta eq may internally imposed regularization 
type regularization implemented data smoothing normalization data smoothing regularization simple way considering smoothing eq discussed regularization determined help smoothing imposed replacing eq 
case object normalization regularization different choices implementing strictly speaking consider neural information processing letters reviews vol october accurately computed takes discrete value binary vector case integral summation 
integral analytically solved gaussian 
cases integral difficult compute 
computable summation binary vector computing cost increase exponentially solution integral entire domain approximated summation set consists number samples follows 
specifically choice represent discrete probabilities weight closer marginal density choice provides crude simplification longer discussed yang step sec set obtained way randomly picking set samples ways getting peak point eq mean point eq cases sample choice choice 
eq eq architecture eq bi architecture eq unified representation denotes gradient respect takes form eq eq case cases data smoothing imposed appropriate regularization strength determined maximizing details referred 
case normalization regularization imposed observed difference eq 
introduces degree conscience de learning updating direction avoid fitting sample pair role takes format adaptive updating form sample 
regularization vs model selection ii kl hl spectrum kl learning eq byy system limited just ml learning 
determined eq eq kl learning eq performs regularized ml learning 
ica extensions noise time xu kl learning eq bi architecture suggested family parametric posteriori estimation eq contained family situation equivalent kl learning eq architecture posteriori estimation eq approximated closest family architecture leads advantage computing difficulty integral eq avoided easy implementing parametric model 
equivalent approaches called variational approximation ml learning 
approximation purpose studies kl learning eq bi architecture directions 
design parametric model inner representation spreading eq ml learning regularized 
design parametric model inner representation concentrated tends facilitate automatic model selection 
family designs follows returns eq ml learning inner representation spreading regularized ml learning concentrated facilitate model selection tend eq concentrated cases 
simply form varies spreading cases increases discussed previous subsection harmony learning wta eq architecture regularized bi architecture spreading representation 
extreme cases equivalent wta eq eq generally leads regularized harmony learning 
lead ml learning harmony learning regularized architecture free replaced posteriori estimation eq 
discussion observe kl learning eq harmony learning eq closely related appropriately designing difference lays term deterministic eq kl learning eq harmony learning eq equivalent special bi directional architecture 
harmony learning eq architecture results eq 
particular harmony learning eq kl learning eq equivalent 
furthermore kl learning eq harmony learning eq related constant irrelevant unknown parameters eq kl learning eq harmony learning eq longer equivalent unknown determined 
special case prefixed advance kl learning eq equivalent discussions apply byy systems architecture free decided eq 
uniform distribution minimizing equivalent maximizing entropy eq maximizes information transfer input data inner representation forward path 
generally describes incremental information contained neural information processing letters reviews vol october representation information transfer 
minimizing equivalent making incremental maximized maximizing information transfer information inner representation 
particularly eq eq kl learning eq harmony learning eq equivalent minimum mutual information approach ica previously discussed eq 
cases featured maximum information transfer shortly called max inform approach 
generally kl learning eq harmony learning eq different satisfy difference observed learning results cases kl learning eq results complexity nature harmony learning eq results minimized entropy model complexity 
summary family kl learning eq family harmony learning eq share intersection consists interesting models 
families different containing useful models outside intersection 
union families consists spectrum learning models ranging regularized ml max inform versions original ml max inform versions reaching regularized versions harmony learning harmony learning 
addition discussed previously sec certain regularized versions ml max inform harmony learning obtainable role data smoothing normalization 
spectrum extended convex combination minimization equivalent kl learning tends harmony learning decreases varies harmony learning regularized kl learning 
combination may go spectrum observed considering architecture free 
follows ignoring regularization role setting get firstly proposed 
minimization respect free lead special case eq inner representation concentrated eq 
minimizing different kl learning eq ml learning spectrum extended considering linear combination eq applies 
difference inner representation spreading ml learning regularization strength increasing increases 
large strong regularization system loose ability adapting input data 
longer convex combination meaningful observing 
ying yang alternative procedure parameter learning discussed xu learning yang machine ying machine implemented alternatively ying step fixing update unknowns ica extensions noise time xu yang step fixing update unknowns eq eq decreases gradually converged 
details discussed follows 
ying step fixed regularization term ignored eq eq share updating format follows consist unknown parameters respectively 
updates reach local maximum increase certain extent 
typically updating increases index stepsize gradient direction small positive number defines stepsize 
gradient direction unit matrix direction positive projection gradient direction positive definite matrix 
particularly known natural gradient direction inverse metric tensor eq integral eq eq disappear 
eq integral removed help approximation tab 
similar previous cases eq integrals analytically solved gaussian computable summation takes discrete values binary vector integrals difficult compute 
computable summation binary vector computing cost increase exponentially problem tackled letting integral entire domain approximated summation set finite number samples obtained yang step 
typical case consists sample case updating form eq place respectively 
considering eq regularization harmony learning eq kl learning eq share format gradients eq kl learning eq eq harmony learning eq put eq updating 
yang step implemented typical choices eq eq learning architecture bi architecture consideration 
details follows implementing kl learning architecture eq parameter learning equivalently ml learning yang step get posteriori estimation eq 
exactly known em algorithm ying step generalized step 
words em algorithm specific case ying yang alternative procedure eq 
integral encountered getting analytically solvable gaussian densities computable summation takes discrete values binary vector computable binary vector computing cost increase exponentially cases integral difficult compute 
previously suggested implemented approximately computational expensive monte carlo simulation 
obtained randomly picking set samples see choice tab choice eqn sec 
rough approximation see step tab ii 
implementing kl learning bi architecture eq parameter learning yang step update consists parameters eq eq 
update neural information processing letters reviews vol october reaches local minimum reduces certain extent 
integral getting eq avoided 
encounter integrals eq firstly suggested name mean field approximation see choice tab choice eqn get consisting mean point computable eq eq eq eq applicable eq encountered get 
implementing harmony learning eq parameter learning yang step simply getting consists peak point firstly suggested see choice tab choice eqn encountered eq architecture 
nonlinear optimization implemented help iterative procedure specific algorithms type proposed suit typical structures consider ying machine typical structure eq specifically real random number comes gaussian mixture solve nonlinear optimization iterative algorithm called fixed posterior approximation summarized tab 
binary random number comes bernoulli distribution counterpart eq complexity making nonlinear optimization considerably making integrals usually need iterations eq waiting converge 
salient advantage complexity nature eq provides addition making model selection possible 
implementing harmony learning bi architecture yang step consists parts 
simply implementing eq 
eq get eq simple comparison reduces significantly computational complexity making nonlinear optimization eq 
eq get 
second part attempts increase help chain rule gradient implemented update form eq follows ica extensions noise time xu 
regularization vs model selection iii convex function function typical example convex function family monotonically increases interesting investigate happen replaced return back consider extension eq 
observed maximization respect complexity nature eq match nature eq modified returns eq classify decreases rate slower super sub super said said typical family called function follows case eq rewritten returns eq cases attempts take lower probabilities increased higher probabilities decreased certain degree controlled spreading increases super sub uniform words function leads conscience de learning regularization 
example family negated function follows leads adopts higher probabilities increased lower probabilities decreased certain degree 
concentrated increases 
words sub function leads competition effectively similar complexity nature 
discussions apply continuous case eq term provides regularization previously discussed super jointly introduce discussed regularization 
super perform regularization role 
eq eq likelihood function differs log likelihood super takes position samples small outliers locate drastic varying range contribute big portion affect ml learning vulnerable disturbance outliers 
contrast eq varying range small smaller affected outliers small words get type robust ml learning kullback divergence extended 
ways 
simply replace resulting neural information processing letters reviews vol october considering equivalent form maximizing replace results maximization different eq return exactly eq empirical density eq 
special case detailed studies considering certain specific features satisfy case eq rewrite eq eq results applied byy system needs simply put eq eq eq eq eq eq 
maximizing eq respect free lead eq 
minimizing eq maximizing eq respect free lead eq 
second lead discussed sec 
trading strength regularization ability model selection help designing parametric model third new perspective get trading may achieved choosing super sub matching process ying machine yang machine learning implemented ying yang alternative procedure sec 
ying step longer separated integral format eq 
get gradient format similar eq 
difference replaced learning eq learning eq yang step discussions architecture sec 
remains 
eq replaced eq 
harmony learning eq rest concentrate special case byy learning constrained satisfy eq 
shown fig special case called byy independence learning sense interpreted generated inner independent factors mapped match independent factors 
byy independence learning provide unified view solving ica extensions 

ica studies 
marginal densities prefixed vs learned return eq continue overview ica studies 
know exact density function form solution satisfies eq reach global maximum 
point clearly observed equivalence minimizing mutual information ica extensions noise time xu choice dy jis ji ji xu yang amari xu choice ii wx max ji ji ji nongaussian yt 
ln ln yt ay 
lpm ica special case byy learning reaches true density due early efforts accurately estimating typical example nonparametric estimator 
updating nonparametric estimate samples computationally expensive 
alternatively truncated gram charlier expansion suggested truncated edgeworth expansion suggested 
simply sigmoid neurons equivalently estimating derivative simple sigmoid function 
approaches prefixed learning typical algorithm known natural gradient eq 
prefixed approaches cases components sub gaussians way super gaussians way 
implementing histogram equalization mixture sigmoid neurons estimate cumulated distribution function cdf flexibly 
type cdf mixture suggested model cdf represented mixture simple sigmoid scalar function learning learned maximizing parameters adapted source super gaussian sub gaussian automatically detected adapted model learning 
result get called learned parametric mixture ica shortly lmp ica algorithm shown experimentally combination super gaussian sub gaussian components em algorithm proposed learning learning modeled mixture sigmoid cdfs finite mixture especially gaussian mixture eq 
idea modeling mixture cdfs help sigmoid functions suggested clearly target working combination super gaussian sub gaussian components gaussian mixture em algorithm revisited implementing ica noiseless case implementing noisy ica 
shown fig discussed studies viewed special case byy learning eq special bi architecture empirical density eq shown eq revisited eq eq substituted 
modeled cdf eq neural information processing letters reviews vol october mentioned lpm ica revisited motivated perspective 
shown fig discussed sec get extension eq place workable cases dimension called ica 
furthermore modeled finite mixture eq follows ying step eq updated em algorithm 
yang step eq leads eq 
addition byy harmony learning eq special bi architecture eq lead variants ica 
especially simply setting lead goes pre whitening lead eq subsequent algorithm 

loosely matching bit conjecture open issues interesting point demonstrates eq works modeling roughly derivative single sigmoid function result echoes successes early efforts nonlinear hebbian learning eq separating sources simple sigmoid non linearity action 
community pursuit accurately estimating may really necessary certain simple non linearity yield solution satisfies eq 
generalized sigmoid function suggested sigmoid function may flexibly change loosely match sources altering parameters learning 
phenomenon loosely matching sources may lead correct ica performance certain experiments observed researchers 
seeking nature loose matching experimentally model super gaussian usually works source super gaussian model sub gaussian usually works source sub gaussian indirectly success previous studies separate blind sources contrast function bases kurtosis 
observation summarized called bit matching conjecture sources separated long sign correspondence kurtosis signs sources kurtosis signs way pair sources pair corresponding source positive product kurtosis unfortunately lack theoretical results prove disprove conjecture 
special case sources sub gaussians mathematically proved correct ica performance guaranteed simple non linearity eq showing correct solutions stable local local exist 
unfortunately result type available cases sources 
alternatively theoretical analysis provided conditions non linearity correct solutions stable local conditions learning converge correct solution remains open challenge 
bit conjecture experimentally demonstrated repeatedly 
examples experiments shown modeling eq sigmoid functions result ica performance 
eq implemented success modeling directly switching sub gaussian super gaussian detecting kurtosis learning 
furthermore switching technique implemented modeling mixture component densities switches sub gaussian super gaussian altering parameter value 
switching technique closely related lmp ica considerably simplifying mixture density simply switching prefixed densities 
report result 
approximate truncated gram charlier expansion denotes standard gaussian density th order moment th order chebyshev hermite polynomials 
approximate corresponding parametric model similarly 
mathematically prove bit conjecture follows ica extensions noise time xu theorem assume rd order moment parametric model zero ignore moments higher th order global minimum eq independent components long sign kurtosis sign kurtosis corresponding source result step forward bit conjecture 
says considering th order moments condition ica workable satisfaction bit conjecture 
unclear algorithm eq lead solution independent components may converge local minimum eq remains unclear ica surely satisfaction bit conjecture 
problems remain explored 
condition satisfied local correspond correct solutions eq satisfied 
second control learning process enter correct local cases local correspond correct solutions correspond wrong solutions 
third get effective algorithm able search global maximum 
rd order moments help problem solving 
theorem model choices pre specify supergaussian eq theorem remaining condition satisfied match kurtosis signs 
varying parameter kurtosis vary negative positive smoothly 
learning eq learn appropriate value adapt kurtosis sign source maximizing unknown parameters 
follows directly new variant lpm ica algorithm eq simplified updating small learning step size 

natural gradient non invertible matrix extensions issue efficiently learning process converges solution 
known result issue natural gradient algorithm provides considerable improvement convergence computing efficiency classic gradient algorithm 
studies direction 
interesting relate natural gradient algorithm newton algorithm quasi newton algorithms various super linear gradient algorithms literature nonlinear optimization 
useful compare number existing typical ica algorithms mathematically analyzing convergence rates 
eq works ica problem eq case invertible 
results obtained case extended cases non invertible full rank name byy learning ica model special case eq special bi architecture studied 
shown special case leads replaces eq case result obtained directly considering eq 
observing pseudo inverse insert eq eq get neural information processing letters reviews vol october minimization push see st term ignored irrelevant parameters learned 
eq minimization terms equivalent eq studied name complete ica 
interesting observe needs simply replacing eq equivalent interestingly natural gradient algorithm eq keep original form 
eq eq extended replaced generalized convex function discussed sec resulted ica algorithm shown experimentally robust outliers 
ica extension consider probabilistic mapping place deterministic mapping effort name maximum balanced mapping certainty max bmc principle maximizes strictly monotonic increasing typical examples represents mutual information maximized information transferred maximally preserved 
free maximization push infinite large number irrelevant problem equivalent maximizing equivalent infomax approach 
max bmc principle extends infomax ica aspects 
deterministic mapping extended probabilistic mapping eq 
second function extended general function third provides information transfer learning unsupervised classification minimum mis classification 
supervised learning 
principle eq special case training hidden units layer networks self organizing map 
efforts extending ica problem eq nonlinear cases 
extended post nonlinear system 
nonlinear blind source separation attempted self organizing map special case byy learning eq lead general cost implementing nonlinear ica information transferred particularly certain specific nonlinear extension extensions ica nonlinear case usually longer retain nature recovers waveform series subject unknown scales 
fact waveform may undergo arbitrary nonlinear distortion 
generally speaking nonlinear ica defined 
nonlinear mapping uniform distribution defined dimensions 
case estimate density follows 
early example case mixture cdfs eq 
early example multi dimensional density roughly simple sigmoid function 
examples regarded special case byy learning eq simple sigmoid function ica extensions noise time xu mixture cdfs eq 
denoting considering monotonic equivalent case eq discussions extended noisy mapping independent equivalent noiseless case 
affect learning sense regularization 
extensions cases dimension larger dimension simply augmented vector vector dimension uniform distribution similar eq simplified choosing spanned subspace spanned orthogonal subspace eq ways 
ml learning estimated maximization respect chosen irrelevant follows maximization respect equivalent max respect mapping uniform distribution mutually independent components 
particularly equivalent eq eq 

extensions ica noisy environment 
non gaussian factor analysis algebraic equation ml learning extend ica problem eq noisy environments considering independent noise added observation 
focus considering eq subject satisfying eq cases general matrix orthogonal matrix 
subject satisfying eq usually consists samples independent start classic factor analysis fa model eq 
widely applied various data analyses 
unfortunately fa suffers called rotation indeterminacy 
considering rotation form eq result able recovered unknown scales rotation indeterminacy 
rotation indeterminacy solved comes non gaussian density gaussian mixture shown fig 
case call eq non gaussian factor analysis nfa previously called independent factor model ifm dependence reduction dr independent factor analysis ifa 
prefer name nfa addressing difference classic fa type ifm ifa nd order sense 
nfa classified specific types eq 
get binary bernoulli nfa shortly bfa binary value bernoulli distribution eq get real nfa shortly nfa real value gaussian mixture eq 
neural information processing letters reviews vol october mln ln harmony learning arg max max kl learning min ml versus subject ji ji ji dy dy nongaussian ay free xt ay dy 
ml equivalent learning versus harmony learning byy architecture nfa usually consider cases comes gaussian 
generally consider nongaussian density 
furthermore eq may extended nonlinear model white noise independent examining solving approaches sec find approaches nonlinear hebbian learning type mmi type infomax type applicable cases eq eq forward function take noise consideration 
approaches applicable solving algebraic equations eq eq help eq get set algebraic equations term matrix statistics higher order statistics techniques developed literature nonlinear algebraic equations conceptually may solve unknowns 
studies line 
ml approach eq eq special cases generative density satisfying eq eq shown fig 
problems estimating ml learning set samples equivalent making parameter learning eq byy backward architecture eq optimal forward mapping match satisfies eq 
making bfa integral eq summation eq enumeration 
known em algorithm suggested effectively implement ml learning 
learning alternatively transforming problems clustering problem solved existing clustering algorithm 
learning making clustering dimension determined 
making nfa em algorithm directly applicable integral eq solved analytically 
straight forward way solving integrals approximation method monte carlo random sampling approach 
computation usually involved 

byy harmony learning nongaussian fa quite different scenario byy harmony learning eq making bfa nfa eq 
shown fig difficulty making integral eq avoided getting sample help eq bi architecture eq architecture nonlinear optimization iterative procedure eq 
learning implemented ying yang alternative procedure eq 
simplicity consider data smoothing regularization rest simply letting eq 
readers referred details implementing data smoothing regularization 
ica extensions noise time xu follows eq eq ying step equivalent maximize case case case case contains sample case case case role ignored eq eq consists consists parameters obtained yang step ying step detailed form get tab general orthogonal approaches tab bfa eq get tab nfa eq update eq nfa eq tends zero constantly discard delete th bfa eq small stepsize 
parts previously tab eqns special cases term specifically nfa eq eq update nfa eq eq update way tab 
yang step different architecture bi architecture 
firstly suggested see choice tab choice eqn get eq eq architecture eqs bi architecture 
specifically yang step details eq run tab iterations architecture eqs bi architecture action architecture updating eq bi architecture 
bi architecture searching best forward nonlinear mapping iterative procedure eq approximated forward parametric structure form eq function form pre specified 
better get knowledge optimal follows eq optimal satisfy specific form approximately solve root returns eq equivalent case architecture 
estimate exact neural information processing letters reviews vol october case optimal solution takes non linearity consideration 
large nd term dominates inverse function intuitively justified select post linear function scalar nonlinear function 
particularly noise large 
observation noise follows eq step tab get linear mapping simply pseudo inverse bi architecture degenerated eq 
revisit lpm ica shown fig 

model selection vs automatic model selection ying yang alternative learning types model selection fig apply act differently bfa fa nfa 
model selection bfa learning eq maximizing push variance constantly th dimension extra 
case discard delete th column words get automatic model selection shown fig 
alternatively may implement bfa stage style 
stage parameter learning ml learning ying yang alternative learning simply fixed second stage select best number factors eq detailed form row tab 
type criteria special case firstly proposed see eqn eqn 
model selection nfa eq real value vector general suffer type scaling indeterminacy 
scaling transform diagonal matrix result density satisfies eq 
solve problem constraint imposed eq nfa implementation learning eq followed normalization ensuring model selection shown fig best selected eq detailed form rd row tab 
type criteria special case firstly proposed see eqn 
longer satisfies scaling indeterminacy removed imposing constraint orthogonal need eq ensure automatic model selection apply help step eq 
model selection fa special case eq lead back classic factor analysis eq eq unnecessary 
yang step eq simply ica extensions noise time xu gaussian factors fa tfa non gaussian factors nfa tnfa binary factors bfa lmser tab 
model selection criteria ay min 
ln 
hx tr 
ln 
ln empirical learning 

ln general harmony 
ln ln orthogonal ml ln ln ln learning learning 
ln general harmony learning 

ln ln orthogonal ml learning 
subject ey 
time time ln 
learned ml learning ln ln 
ln ln ln ln ln ln ying step part updating eq 
ying yang procedure provides adaptive algorithm equivalently implements ml learning 
model selection eq detailed form nd row tab 
type criteria special case firstly proposed see eqn eqn eqn eqn 
interesting special case get density eq eq general ml learning sense cases lead back classic factor analysis implemented em algorithm adaptive em algorithm 
model selection eq detailed form nd row tab 
cases different sense byy harmony learning eq 
eq yang step ying step includes part updating eq updated approaches tab different updated maximizing way tab pushed zero dimension extra 
words model selection automatically parameter learning eq 
model selection uncorrelated nfa nfa eq general possible automatic model selection turning eq help singular value decomposition general matrix 
eq eq equivalent sense ml learning case eq falls paradigm conventional factor analysis call uncorrelated nfa covariance matrix diagonal matrix different conventional factor analysis matrix learned invertible mapping transfer uncorrelated factors independent factors rotation indeterminacy removed 
strictly speaking goes byy independence learning eq may longer satisfy eq 
neural information processing letters reviews vol october learned sense byy harmony learning eq 
mapping invertible joint density eq differs eq effect getting eq 
yang step architecture eq 
bi architecture yang step format eq difference eq eq 
ying step consists part updating eq updated approaches tab help part updating eq replaced plus updating update approaches tab approaches constantly discard delete th column th element learning maximizing push variance zero dimension extra 
model selection happens automatically 
acts recovered independent factors 
performs indepen dence mapping takes consideration noise 
model selection ica special case observation noise follows eq mapping linear lead back lpm ica shown fig 
tab observe decide zero nonzero firstly proposed eqn eqn decide checking eq 
equivalent choosing shown fig subsequent discussions 
practical problem certain noise 
choosing fig criteria tab useful 

special cases lmser auto association previously discussed lmser learning eq performs ica sigmoid post linear mapping constraint attempts minimize mean square error role originally interpreted extracting features experimentally demonstrated lmser learning performs ica performance superior nonlinear hebbian learning consider noise 
successfully implementing binary ica noise 
relaxing constraint eq referred auto association learning layer net trained way training layer net back propagation technique 
experimentally demonstrated sigmoid layer feature extraction noticed performed ica 
shown fig lmser auto association learning understood special case byy learning bi architecture 
considering simply sigmoid function mixture cdfs eq expect mapped match uniform density interpreted generated uniform distribution plus observation noise case zero maximizing direction eq reduces ica extensions noise time xu uniform density wx cr ay real dn xt auto lmser wx association new 
lmser qj old lmser wx cr ay real discard 
auto association lmser dn 
byy bi architecture performs lmser auto association learning wx equivalent minimizing eq 
words lmser auto association learning attempt perform specific ica maps uniform density specific nfa fits independent factors uniform density 
heuristic justified sigmoid function eq 
scaling uniform density consists impulse impulse superposing linear function follows eq function linear takes sigmoid function acts reasonable approximation shown fig lmser auto association learning understood eq eq 
sigmoid function stiff value near probability probability particularly constant relevant maximizing direction eq reduces equivalent minimizing eq 
just providing new insights lmser auto association learning perspective byy harmony learning fig leads new results 
follows eq eq get criterion term longer constant balance force determining appropriate learning algorithm new algorithm eq fixed 
learning algorithm estimate eq improved follows connection byy learning lmser learning criteria eq eq firstly developed see eqn eqn 
second follows eq get new adaptive algorithm place learning algorithm implementing lmser auto association learning 
specifically ying step eq yang step consists simply getting eq updating follows neural information processing letters reviews vol october particularly simply early studies learning started see tab 
eqn 
third shown fig automation selection harmony learning detecting tends means dimension extra 
furthermore extensions aspects mixture cdfs eq parameters adapted learning best sigmoid mapping sought 
consider longer limited sigmoid function parametric model adapted parameters 
correspondingly combines feature eq feature eq role replaced understood ica extension noise situation name principal ica ica 
type learning started see eqn eqn eqn eqn eqn eqn eqn 
particularly equivalent making pca singular value decomposition 
case eq degenerates eqn 
get eq eqn 
closing subsection deserves mention studies extended case noise nongaussian considering form eq place cases general form eq 
learning implemented ying yang alternative procedure eq ying yang steps modified accordingly 

approximate ml exact ml em algorithm discussed sec eq performs best ica mapping noise sense implementing ml learning eq 
substitution eq means deviation handling noise ml sense 
deviation reduced considering byy learning minimizing kullback divergence eq non parametric density equivalent making type variational approximation ml learning eq discussed previously sec 
specifically form eq mean field approximation replaced conditional mean get rid integrals getting eq eq able implement learning form eq 
mean field implementation variational approximation useful discrete 
eq eq implemented approximation case computational cost high especially large 
typical example eq case eq includes special case result summation values avoided 
detailed learning algorithm implementing eq table 
efforts implementing ml learning eq number research groups 
called joint maximum likelihood considered maximized respect closely relates eq key different points 
iterative procedure eq rough solution expanding order approximation 
second ica extensions noise time xu 
recovered factors denoted original sources denoted factor normalized zero mean unit variance 
ifa mean square errors factor average nfa mean square errors factor average eq natural result byy learning eq 
eq pre problem specify scalar remaining open problem 
approach exactly implements ml learning model eq firstly proposed 
similar considered independence product eq modeled gaussian mixture eq 
key difference dealt product gaussian mixtures introducing set random variable stochastically takes number number indicates component th mixture 
follows product summations eq equivalently exchanged summation products 
result integral eq summation analytically computable integrals gaussians results mixture gaussians 
reason able implement exact ml learning problem eq exact em algorithm 
results published name independent factor analysis 
summation terms computed step em algorithm 
complexity increases exponentially number factors contrast implementation nfa byy harmony learning eq eq turns integral domain problem nonlinear optimization eq 
solved iterative algorithm searches domain trace usually lower order subspace 
shown fig experiments comparing nfa implemented yang step eq ying step eq em algorithm exact ml learning algorithm shortly denoted ifa 
data sets come model factors factors respectively 
gaussian mixture eq consisting gaussian components nfa ifa 
shown fig recovered factors factor model comparison corresponding original sources 
observed recovered factors figures corresponding mean square errors nfa outperforms ifa 
shown fig number factors increasing time nfa increase consumed ifa increases times 
empirically time complexity nfa increases linearly number factors ifa increases exponentially nfa outperforms ifa significantly aspect complexity 
major computing load removed making pre whitening needs consider orthogonal matrix result exponential complexity reduced linear product mixture gaussians alternative step em algorithm 
parameters updated help variational approximation mean field approximation computing cost estimating exact posteriori density significantly reduced 
effort similar case byy learning eq eq 
eq constrained form contrary follows eq satisfying eq performs best ica mapping usually form neural information processing letters reviews vol october time cost nfa ifa factor model factor model 
time complexity comparison nfa ifa ay argmax yt bi architecture architecture ay 
independence analysis binary non negative data discussed ml learning nfa equivalent performing pca requiring completely necessary equivalence previously proved 
non negative data analyses supervised learning layer net applications observed data take non negative value binary value shown fig nfa data considered byy learning architecture choice bi architecture choice help letting appropriate format 
case non negative value consider exponential density shown fig 
previous results apply directly replaced updated simply case considered 
case binary value regarded generated inner binary code studies literature 
example explores direction early called multiple cause mixture 
models bit observed bit heuristic cost function 
learning combinatorial optimization problem searches values maximum likelihood learning proposed model binary code interpreted bernoulli defining probability help generating model binary matches shown fig model selection modifying fig replaced get adaptive algorithm eq tab 
automation selection learning detecting tends constantly 
shown fig square learning conventional layer forward networks revisited auto association learning fig 
decomposing parts consider ica extensions noise time xu new old qj yj ay real free 
specified discard 
dn 
byy supervised learning layer net versus nfa byy architecture special bi architecture fig comes cascade mapping case eq dimension maximization term equivalent square learning widely studied layer net eq maximization equivalent minimize entropy hidden units compact inner representations 
hidden unit constantly regarded extra discarded 
third term likelihood describes inverse optimal searched maximizing shown get eq 
regarding cdf describes product independent densities 
having format eq maximizing independent components 
jointly convention square error learning minimizing improved maximizing automatic selection hidden units possible maximizing learning pushing hidden units layer nets independent 
making learning conventional back propagation technique follows eq get new adaptive algorithm updating eq place getting eq place updating extra hidden unit discarded detecting constantly 
learning simplified fixed 
appropriate selected eq enumerating number approximately conventional backpropagation technique select criteria eq eq previously studied binary stochastic hidden units see eqn 
deterministic real hidden units see eqn eqn 
details studies referred 

minimizing fitting error vs enforcing factor independence fact discussions sec 
deal twofold effort targets minimizing fitting error sample reconstruction regularizing indeterminacy modeling 
imposing independence eq components neural information processing letters reviews vol october directly imposing independence eq learning independence may gradually approached learning optimizing cost minimum 
typical examples independence eq satisfied choice pushing minimum equivalent maximizing entropy discussed eq uniform distribution extreme case independence family eq 
minimum equivalently minimizing entropy pushes 
choice pushing arbitrary distribution locates point satisfies eq example extreme type independence family 
time minimizing fitting error reached optimizing cost learning process 
typical examples discussed previously include making usually suffers indeterminacy infinite solutions 
joint implementation eliminate indeterminacy lead appropriate solution 
observed appropriate combination discussed typical models 
independence eq lead previously case eq case eq plus imposing independence eq equivalent harmony learning eq 
case case eq case equivalent extended lmser learning eq 
need consider noise ignore case eq plus imposing independence eq lead eq eq eq eq eq respectively 
ways combining may result different variants deserve explored 
different combinations comes different choices class variants may obtained replacing case eq case 
targeting family distribution regularization role enhanced targeting unique uniform 
distribution 
helps number factors pre specified 
contrast case eq enhances harmony learning eq eq pushing extra dimension modeling automatic selection factors implemented 
prefer case cases unknown number decided 
addition choices choices eq combined 
addition eq eq specific forms eq eq 
differences come way consider combining manners eq eq results case equivalent lmser learning eq eq eq case results case eq 
constrained learning specifically equivalent lagrange constrained neural network 
eq extends eq cases observation noises 
extensions choices 
case replaced case get counterpart eq follows ica extensions noise time xu selection ability similar various previously discussed types harmony learning 
generally monotonically increasing called combination formula 
studies may deserve conducted 

extensions ica temporal dependence 
typical approaches ica temporal dependence considering independence higher order statistics indeterminacy linear system eq removed considering temporal dependence samples equivalently different times 
key feature nd order temporal dependence may removing indeterminacy degenerated cases 
number approaches studied exploring temporal dependences solving ica problems 
roughly classify major types joint diagonalization term channel denote series component time diagonal 
follows matrix determined jointly diagonalize correlation matrices usually determined correlation matrices identical 
solution may remain varying time stationary signals varies time non stationary signals 
techniques studied implement joint diagonalization 
readers referred 
goes 
say de correlated channels means correlation matrices solving algebraic equations similar situations discussed sec examining joint statistics time series different time delays constraint constraint requiring independence channels lead number algebraic equations 
result determined jointly solving equations 
optimizing cost function conceptually cost purpose long reaches minimum equivalently maximum independence channels satisfied 
ways get cost 
get cost allowing errors joint diagonalization equations joint statistics algebraic equations 
second costs obtained high order statistics especially fourth order cumulants 
third cost may heuristically motivated 
belief temporal predictability signal mixture equal component source signals measure temporal predictability suggested short term long term moving average 
measure maximized determine usually costs consider independence channels sense statistics finite order th order contrast cost eq considers independence component densities conceptually covers statistics orders 
efforts extend eq cover temporal dependence 
replaced ar model name context sensitive ica past estimates basing ar coefficients updated parameters imposed says components independent conditioning past values 
satisfies independence channels special case temporal byy learning architecture called temporal ica proposed extends eq replaced current estimate directly improved temporal dependence ii replaced non linear dependence covered 
simplified algorithm explores nd order independence 
extend studies noise case eq eq considering temporal dependence 
considering correlated come similar situation joint diagonalization approach solving second constraint replaced constraint come approach joint algebraic equations solving neural information processing letters reviews vol october different noiseless case need set inverse mapping implement ica need model special case temporal byy learning autoregressive ar modeling eq lead known linear state space model literature control theory model extensively studied kalman filter estimates current conditions studies known number states dimension 
practical cases 
special case temporal byy learning architecture alternative state space model suggested independence condition eq extended times remain unknowns specified 
alternative state space model understood extension classical factor analysis eq widely studied literature statistics decades 
temporal dependence added independence constraint eq imposed call extension temporal factor analysis tfa 
temporal byy learning provides general guidance extending binary fa nfa lmser supervised variant temporal cases introduced rest section 

types temporal byy systems conceptually bayesian ying yang learn temporal dependence eq eq inserting pair processes temporal place variables eq called temporal bayesian ying yang tbyy process system 
implementation situation usually complicated handled directly simplification needed 
sample knowing definitely means irrelevant environment 
second adopt causal assumption depends values past assumptions form eq summation takes role similar eq eq 
harmony measure process uniformly contributed time follows set consists part past samples considering tab 
approximately imposing markovian assumption depend finite number past sample values finite dimensional vector designed regular structures 
special cases st order markovian relates 
knowing implies independent inner code simply relates 
similarly knowing implies independent result suffice consider st order byy state space system inserted eq 
ica extensions noise time xu follows eq free architecture follows maximizing alternatively get different extension byy system temporal dependence 
consider pair eq simply time instantaneously subject satisfaction temporal dependence general st order 
temporal dependence considered constraint imposed ying yang matching learning 
byy system called tbyy instantaneous system 
follows eq harmony measure simply subject constraint eq 
particularly architecture follows considering contributions time uniformly harmony measure entire process simply summation eq 
discrete gaussian integral eq summation analytically solved directly inserted implementing learning 
real nongaussian random variable integral eq difficult handle 
help tab 
follows eq approximately decided maximizing eq resulting putting eq eq eq eq observe tbyy process system approximation eq tbyy instantaneous system approximation eq equivalent 
types tbyy systems conceptually different 
tbyy process system considers best harmony representations form joint distribution entire temporal processes including temporal dependences consideration 
contrast tbyy instantaneous system emphasizes best harmony representations joint distribution pair instantaneously time directly including temporal dependences harmony 
temporal dependence considered constraint eq 
implementation get way similar eq consideration 
get specific form eq directly tbyy instantaneous system appearances replaced tbyy process system 
addition making harmony learning discussed conduct learning stages parameter learning stage eq 
similar way get tbyy process system tbyy instantaneous system 
neural information processing letters reviews vol october yt byt yt yt free xt ay ay dy yt yt yt yt free dy xt ay ay dy 
types temporal nfa estimate available system time considered time particularly architecture minimizing leads tbyy process system tbyy instantaneous system 
putting eq eq observe temporal byy types equivalent approximation eq 
imposing independence condition eq types tbyy systems lead tbyy independence learning 
recalling eq eq find key difference byy independence learning introduced sec additional structure imposed directly regression assembly marginal density relation eq 
correspondingly forward path modified dependence added 
changes result changes parameter learning model selection making tbyy independence learning introduced rest section 

tfa temporal nfa space dimension selection shown fig start extending architecture fig tbyy architecture follows diagonal matrix temporal structure imposed stability guaranteed norm positive root largest eigen value example consider satisfied 
white process strict sense independent components gaussian mixture follows independent 
typical examples gaussian gaussian eq replaced 
ica extensions noise time xu setting process satisfy eq long components initialized independent 
specifically eq case temporal extension classic factor analysis called temporal factor analysis tfa 
shown elaborated subsection key feature tfa rotation indeterminacy eq classic fa removed 
similarly eq case temporal extension nfa fig called temporal nfa 
learning implemented ying yang alternative procedure eq 
ying step updated way eq updating modified 
implementation tbyy process system updated increase follows temporal nfa updating comes tfa get eq tfa eq eq replaced temporal nfa 
implementation tbyy instantaneous system difficult handle temporal nfa due integral eq 
handle tfa simply updated increasing follows get eq updating comes yang step obtained different degree difficulty performing tfa temporal nfa respectively 
tfa follows eq eq directly get tbyy process system tbyy instantaneous system 
temporal nfa difficult get tbyy instantaneous system due integral eq 
get tbyy process system solving nonlinear optimization running tab iterations appearances replaced special case tfa returns back fa temporal nfa returns back nfa respectively 
cases eq return back eq fa eq returns back eq nfa 
discussions dimension fixed variance select best value space dimension enumerate number values parameter learning 
tbyy process system selection eq tab 
simplicity approximately fig tfa temporal nfa 
tbyy instantaneous system selection directly eq temporal nfa tab 
enumerating selection costs extensively 
similar eq follows singular value decomposition general matrix consider linear mapping linear state space eq turned form neural information processing letters reviews vol october eq alternative state space maximizing eq maximizing push variance zero constantly th dimension extra 
result model selection happens automatically parameter learning simply discarding dimension discarding components implementation discarding approaches near zero avoid computing difficulty longer diagonal eq longer directly implements tfa equivalent tfa 
specifically adaptive learning algorithm obtained 
yang step simply replace case eq eliminating th column eliminating th column th row ying step consists part updating eq updated help approaches tab part updating help gradients directly update approaches tab 

update subject 
temporal ica kalman filter identifiable state spaces get eq 
consider special case observation noise types temporal byy learning follows eq eq get simply pseudo inverse independent time lead degenerated case eq get temporal extension ica 
temporal ica implemented format eq updated eq eq updated eq implementation tbyy process system 
nongaussian implements temporal ica 
gaussian firstly shown rotation indeterminacy eq removed elements diagonal matrix different observed fact rotation longer diagonal independence condition eq broken 
updated eq implementation tbyy instantaneous system 
difficult handle case nongaussian due integral eq 
case gaussian handled rotation indeterminacy eq removed observed fact eq remain diagonal rotation furthermore select appropriate dimension discussed sec 

interesting special case eq widely studied decades literature control theory 
usually assumed come gaussian processes parametric matrices variances gaussian known 
task estimate observing get result filtering noise optimal solution known kalman filter 
ica extensions noise time xu shown estimate kalman filter equivalent alternatively observed gaussians obtained eq equivalent subject eq eq results special settings eq eq learning maximizing learning minimizing equivalent known kalman filter estimating cases eq 
providing different insight get new results maximizing examples introduced follows literature kalman filter studies knowing dimension implied requiring parametric matrices known 
little studies determine appropriate applications may sets models different dimensions 
may choose smallest tab 
nongaussian get generalized kalman filter approximating density gaussian density estimates cases eq 
help eq tab estimate density modeled gaussian mixture 
eq 
instantaneous bayesian ying yang system requiring matrices known restrictive temporal modeling problems 
problem estimating unknowns suffer intrinsic indeterminacy observed perspective making linear mapping general matrix follows eq get form remains gaussian form eq 
identifiable state space model eq eq 
type indeterminacy removed imposing certain constraints possibility tfa satisfaction eq eq means diagonal matrices 
follows eq linear mapping longer satisfying constraints 
kalman filter remain unchanged means families diagonal permutation matrix diagonal elements different observing equivalently correspondence series series series series series waveform difference constant scale permutation order change 
applications type difference say identifiable able specify sense eq 
types indeterminacy exist 
additive decomposition additive decomposition generally identifiable problem studied perspective uniquely estimating parameters subject constraints state space model eq eq 
set samples large size constraints consist line comes consideration neural information processing letters reviews vol october 
temporal bfa temporal lmser supervised recurrent net consider cases binary vector 
start extensions bernoulli nfa bfa temporal bfa 
considering binary vector satisfies eq key point extend bernoulli distribution eq encoding temporal relations 
simplest case considering order temporal relation follows describes probability transfer process eq consists independent markovian chains st order respectively directly observable 
fig observation process generated ying path encounter variant classic hidden markov model hmm variable takes discrete values replaced binary vector satisfies eq 
called type hmm independent hmm 
especially call independent binary hmm takes literature learning hmm maximum likelihood ml principle known baum welch algorithm type em algorithm 
addition high computing cost baum welch algorithm known maximum likelihood principle usually weak making model selection deciding number especially case small size training samples 
usually number assumed known advance 
contrast learning independent hmm implemented help tbyy harmony learning model selection automatically adaptive learning subsequently learning new class model selection criteria 
specifically tbyy harmony learning implemented ying yang alternative procedure eq detailed form modified follows 
yang step obtained eq simply replaced implementation temporal byy instantaneous system implementation tbyy process system 
ying step updated way eq updating modified 
case eq update increase follows approaches constantly discard state related parameters eq 
implementation temporal byy instantaneous system eq subject eq step replaced large initial value automatically reduce appropriate learning due step eq 
alternatively parameter learning model selection sequentially stages 
stage parameter learning eq equivalently ml learning help baum welch algorithm 
number decided stage ii help eq takes detailed form tab ica extensions noise time xu real ij free ay automatic model selection discard free real 
types temporal bfa ay historically eq selecting classic hmm firstly suggested cumbersome form directly uses selecting independent hmm firstly suggested direct eq simplification 
simplified form obtained tedious second term contrast criteria tab firstly proposed compact representation simple computed accurately 
furthermore go order temporal relation defining consisting past samples order number free parameters increases order alternatively define transfer probability purpose vector adaptive learning algorithm implements learning eq page 
similar making temporal nfa temporal bfa harmony learning described eq 
consider extensions lmser temporal lmser 
similar relation bfa lmser key point directly forward mapping solving nonlinear maximization 
implementation tbyy instantaneous system remains eq 
ying step discussed yang step eq simply replaced similarly criteria tab 
implementation tbyy process system eq modified consistent format eq 
effect past needs taken consideration 
ying step discussed criteria tab 
eq modified eq updating added decomposing parts supervised learning layer net eq extended temporal versions shown fig 
implementation tbyy instantaneous system eq remains 
ying step discussed place yang step eq simply replaced replaced similarly criteria tab 
implementation tbyy process system eq implemented neural information processing letters reviews vol october real ij automatic model selection discard ky ky ay ay free specified free specified real 
types recurrent networks ying step discussed place criteria tab 
yang step consists eq eq eq updating shown fig types temporal extensions supervised learning layer nets types recurrent networks supervised learning 
implementation tbyy instantaneous system layer net hidden units recurrently input past values 
implementation tbyy process system layer net past values hidden units recurrently input hidden units input units words discussed ying yang learning procedures provide new tools handle types recurrent networks 

studies ica problem quite popularized literature neural networks 
typical approaches solving ica problems summarized 
particularly advances ica studies consider hybrid sources ica extensions consider noise temporal dependence overviewed 
targeting complete survey main existing results survey papers literature systematic sketch perspective bayesian ying yang independence learning 
new insights provided existing results literature number results 
clarity new results summarized follows stiefel manifold nonlinear hebbian learning see sec 
simplified lpm ica algorithm works hybrid sources see sec 
forward mapping model estimating density see sec 
regularized adaptive learning fa nfa criteria selecting factors algorithms automatic selection see sec 
sec 
extensions lmser learning hidden factors selected criteria automatically pa rameter learning see sec 

adaptive learning layer net ica type algorithm place conventional back propagation algorithm hidden units selected easy implemented criterion automatically parameter learning see sec 

comparative understanding types temporal byy systems 
ica extensions noise time xu modified algorithms tfa temporal nfa states selected automatically parameter learning see sec 
algorithms variants hmm models extended bfa lmser states selected criteria automatically parameter learning see sec 

acknowledgment described fully supported research council hong kong sar project cuhk 
akaike 
new look statistical model identification ieee tr 
automatic control 
moulines 
prediction error method second order blind identification ieee trans 
signal processing 
amari 
natural gradient learning complete bases ica neural computation 
amari cichocki 
adaptive blind signal processing neural network approaches proc 
ieee vol 

amari chen cichocki 
stability analysis adaptive blind source separation neural networks 
amari cichocki yang 
new learning algorithm blind signal separation advances nips mit press 
anderson rubin 
statistical inference factor analysis proc 
berkeley symp 
math 
statist 
prob 
rd uc berkeley 
attias 
independent factor analysis neural computation 
baldi brunak 
bioinformatics machine learning approach mit press cambridge ma 
bell sejnowski 
information maximization approach blind separation blind deconvolution neural computation 
cardoso 
maximum likelihood source separation expectationmaximization technique deterministic stochastic implementation proc 

bourlard kamp 
auto association multilayer perceptrons singular value decomposition biol 
cyb 


model selection akaike information criterion general theory analytical extension psychometrika 
brown hwang random signals applied kalman filtering john wiley sons 
cardoso 
laheld 
equivariant adaptive source separation ieee trans 
signal processing 
cardoso 
comon 
independent component analysis survey algebraic methods proc 
ieee iscas vol 
cardoso 
blind signal separation statistical principles proc 
ieee vol 

cao 
liu 
general approach blind source separation ieee trans 
signal processing pp mar 
chen amari 
unified stabilization approach principal minor components extraction algorithm neural networks 
cheung xu 
global local convergence analysis information theoretic independent component analysis approach neurocomputing 
cheung xu 
rival penalized competitive learning approach discrete valued source separation intl 
neural systems 
comon 
independent component analysis new concept 
signal processing dayan zemel competition multiple cause models neural computation 
dempster laird rubin maximum likelihood incomplete data em algorithm royal statistical society 
edelman arias smith geometry algorithms orthogonality constraints siam matrix anal 
appl vol 
pp 



source separation prior knowledge maximum likelihood solution proc 

neural information processing letters reviews vol october girolami 
alternative perspective adaptive independent component analysis algorithms neural computation 
comon 
blind separation discrete sources ieee signal processing letters 
herault jutten 
space time adaptive signal processing neural network models denker ed neural networks computing proceedings aip conference pp 
american institute physics 
hinton dayan frey neal wake sleep algorithm unsupervised learning neural networks science 
hu xu 
comparative study cluster number selection criteria proc 
ideal lecture notes computer science lncs springer verlag pp 
hyvarinen oja 
fast fixed point algorithm independent component analysis neural computation 
hyvarinen independent component analysis presence gaussian noise maximizing joint likelihood neurocomputing 
hyvarinen 
survey independent component analysis neural computing surveys 
jutten herault 
blind separation sources part adaptive algorithm neuromimetic architecture signal processing 
kalman new approach linear filtering prediction problems trans 
asme basic engineering march 
competitive learning mutual information maximization proc 
ijcnn washington dc july 
karhunen 
representation separation signals nonlinear pca type learning neural networks 
karhunen pajunen oja 
nonlinear pca criterion blind source separation relations approaches neurocomputing 
kawamoto matsuoka 
method blind separation convolved nonstationary signals neurocomputing 
king xu 
adaptive contrast enhancement entropy maximization constrained network proc 
iconip 
mendel maximum likelihood seismic deconvolution ieee trans 

remote sensing 
lee girolami sejnowski 

independent component analysis extended infomax algorithm mixed sub gaussian super gaussian sources 
neural computation 
liu chiu xu 
bit matching conjecture independent component analysis press neural computation 
lu 
neural network complete independent component analysis proc 
th european symposium artificial neural networks bruges belgium 
ma wang xu 
gradient byy harmony learning rule gaussian mixture automated model selection press neurocomputing matsuoka ohya kawamoto 
neural net blind separation non stationary signals neural networks 
mcdonald factor analysis related techniques lawrence erlbaum hillsdale nj 
mendel 
optimal seismic deconvolution estimation approach new york academic 
miller horn 
probability density estimation entropy maximization neural computation 
schuster 

separation mixture independent signals time delayed correlations physical review letters 
moulines cardoso 
maximum likelihood blind separation deconvolution noisy signals mixture models proc 
icassp 
moreau 
high order contrasts self adaptive source separation international journal adaptive control signal processing pp 
moreau 
generalization joint diagonalization criteria source separation ieee trans 
signal processing pp 
oja 
simplified neuron model principal component analyzer journal mathematical biology 
oja 

neural networks principal components subspaces international journal neural systems 
ica extensions noise time xu oja ogawa 
learning nonlinear constrained hebbian networks proc 
icann 
oja 
principal components minor components linear neural networks neural networks 
oja 
nonlinear pca learning rule independent component analysis neurocomputing 
pajunen hyvarinen karhunen 
nonlinear blind source separation self organizing maps proc 
iconip 
pajunen 
competitive learning algorithm separating binary sources proc 
esann 
pearlmutter parra 

context sensitive generalization ica proc 
iconip 
pham jutten 
separation mixture independent sources maximum likelihood approach proc 
eusipco 
rabiner juang fundamentals speech recognition prentice hall 
redner walker 
mixture densities maximum likelihood em algorithm siam review 
ridder duin kittler texture description independent components proc 
joint international workshop syntactical structural pattern recognition windsor canada 
rissanen 
stochastic complexity modeling annals statistics 
rissanen 
hypothesis selection testing mdl principle computer journal 
robinson predictive decomposition time series application seismic exploration geophysics 
roth baram 
multidimensional density shaping sigmoids ieee trans neural networks 
rumelhart hinton williams learning internal representations error propagation parallel distributed processing mit press 
thayer 
em algorithm ml factor analysis psychometrika 
saul jordan 
exploiting tractable structures intractable networks advances neural information processing systems mit press 
saund multiple cause mixture model unsupervised learning neural computation vol pp 
sato 
method self recovering equalization multilevel amplitude modulation system ieee trans communication 
schwarz 
estimating dimension model annals statistics 
weinstein 
new criteria blind deconvolution non minimum phase systems channels ieee trans information theory 
simon jutten 
blind source separation convolutive mixtures maximization fourth order cumulants non case icassp 


blind separation sources part iii stability analysis signal processing 
stone 
blind source separation temporal predictability neural computation 
comparison lagrange constrained neural network traditional ica methods proc 
ijcnn may honolulu hawaii pp 
taleb jutten 
nonlinear source separation post nonlinear mixtures proc 
esann 
tong inouye liu 
waveform preserving blind estimation multiple independent sources ieee trans 
signal processing 
tong 
blind signal separation statistical principles proc 
ieee vol 

welling weber 
constrained em algorithm independent component analysis neural computation wiggins 
minimum entropy deconvolution 
xu 
byy learning regularized implementation model selection modular networks hidden layer binary units neurocomputing vol 
xu 
data smoothing regularization multi sets learning problem solving strategies neural networks vol 
nos 

neural information processing letters reviews vol october xu 
byy harmony learning structural topological self organizing mixture models neural networks vol 
nos 

xu bayesian ying yang harmony learning handbook brain theory neural networks second edition arbib ed cambridge ma mit press pp 
xu 

mining dependence structures statistical learning perspective yin eds proc 
ideal lecture notes computer science springer verlag 
xu 
byy harmony learning independent state space generalized apt financial analyses ieee trans neural networks 
xu best harmony unified automated model selection unsupervised supervised learning gaussian mixtures layer nets rbf svm models international journal neural systems 
xu 
temporal byy learning state space approach hidden markov model blind source separation ieee trans signal processing 
xu byy learning system theory parameter estimation data smoothing regularization model selection neural parallel scientific computations vol 
pp 
xu 
temporal bayesian ying yang dependence reduction blind source separation principal independent components proc 
ijcnn july washington vol pp 
xu 
bayesian ying yang unsupervised supervised learning theory applications proc 
chinese conf 
neural networks signal processing pp china 
xu 
bayesian ying yang system theory unified statistical learning approach temporal modeling temporal perception control proc 
iconip october japan vol pp 
xu 
bayesian kullback ying yang dependence reduction theory neurocomputing 
xu 
rbf nets mixture experts bayesian ying yang learning neurocomputing vol 

xu bayesian ying yang learning theory data dimension reduction determination journal computational intelligence finance finance technology pub vol pp 
xu bayesian ying yang system theory unified statistical learning approach vii data smoothing proc 
iconip oct japan vol pp 
xu layer net learning em algorithm selection criterion hidden unit number proc 
iconip oct japan vol pp 
xu bayesian ying yang system theory unified statistical learning approach vi convex divergence convex entropy convex likelihood proc 
ideal pp 
xu dimension reduction determination proc 
ieee inns international joint conference neural networks ijcnn may anchorage alaska vol iii pp 
xu cheung amari 
learned parametric mixture ica algorithm neurocomputing 
xu cheung amari 
results nonlinearity separation capability linear mixture ica method learned parametric mixture algorithm proc 
ann feb spain pp 
xu 
bayesian ying yang system theory unified statistical learning approach unsupervised semi unsupervised learning amari eds brain computing intelligent information systems springer verlag pp 
xu 
bayesian ying yang system theory unified statistical learning approach iii models algorithms dependence reduction data dimension reduction ica supervised learning wong eds theoretical aspects neural computation multidisciplinary perspective springer verlag pp 
xu 
bayesian ying yang learning ica models proc 
ieee signal processing society workshop neural networks signal processing vii sept florida pp 
xu cheung yang amari 
independent component analysis information theoretic approach mixture density proc 
ijcnn vol 
iii 
xu cheung ruan amari 
nonlinearity separation capability justification ica algorithm learned mixture parametric densities proc 
esann bruges april pp 
xu bayesian ying yang machine clustering number clusters pattern recognition letters 
ica extensions noise time xu xu new advances bayesian ying yang learning system kullback non kullback separation functionals proc 
ieee inns international joint conference neural networks ijcnn june houston tx usa vol 
iii pp 
xu yang amari 
signal source separation mixtures accumulative distribution functions mixture bell shape density distribution functions research proposal frontier forum speakers tanaka xu cardoso organized amari tanaka cichocki riken japan april 
xu amari 
general independent component analysis framework bayesian kullback ying yang learning proc 
iconip 
xu maximum balanced mapping certainty principle pattern recognition associative mapping proc 
inns sept san diego ca pp 
xu jordan 
convergence properties em algorithm gaussian mixtures neural computation 
xu bayesian kullback coupled ying yang machines unified new results vector quantization proc 
iconip oct nov beijing china pp 
xu 
pca learning linear nonlinear global representation local representation proc 
iconip oct seoul korea vol pp 
xu 
theories unsupervised learning pca nonlinear extensions invited talk proc 
ieee icnn june july orlando florida vol ii pp 
xu oja suen 
modified hebbian learning curve surface fitting neural networks pp 
xu 
mean square error reconstruction self organizing neural nets neural networks 
early version proc 
ijcnn singapore 
xu oja 
neural net dual subspace pattern recognition methods international journal neural systems 
xu yan chang 
phase information richly contained point finite length digital signal proc 
ieee iscas vol 
xu yan chang 
unique specification discrete finite length signal point fourier transform magnitude proc 
icassp vol 
xu yan chang 
semi blind deconvolution finite length sequence linear problem ii nonlinear problem sinica series vol xxx 
xu yan chang 
investigation semi blind deconvolution nonlinear programming science bulletin 
yellin weinstein 
multi channel signal separation methods analysis ieee trans 
signal processing 
zhang xu fu 
learning multiple causes competition enhanced mean square error reconstruction intl neural systems 
lei xu ieee fellow iapr fellow chair professor computer science engineering chinese university hong kong cuhk guest professor universities prc uk 
receiving ph tsinghua univ early joined univ university level exceptionally promoted young associate professors exceptionally promoted full professor 
worked universities finland canada usa including harvard mit 
joined cuhk senior lecturer professor took current position 
prof xu published academic papers number cited literature 
numerous keynote invited tutorial talks intl 
major neural networks nn conferences ieee icnn ijcnn iconip governor board intl 
nn society chair computational finance technical committee ieee neural networks society past president asia pacific nn assembly associate editor intl 
journals nn including neural networks ieee trans 
neural networks 
iconip program committee chair general chair ideal ideal ieee joint icann iconip program committee chair 
served program committee members intl 
major nn conferences past decade including ijcnn ieee icnn received chinese national prestigious academic awards including national nature science prize international awards including inns leadership award 
prof xu fellow ieee fellow iapr international association pattern recognition member european academy sciences member sigma xi member american association advancement science 

