empirical study learning speed back propagation networks scott fahlman september cmu cs connectionist neural network learning systems form back propagation algorithm 
back propagation learning slow applications scales poorly tasks larger complex 
factors governing learning speed poorly understood 
begun systematic empirical study learning speed backprop algorithms measured variety benchmark problems 
goal twofold develop faster learning algorithms contribute development methodology value studies kind 
progress report describing results obtained months study 
date looked limited set benchmark problems results encouraging developed new learning algorithm faster standard backprop order magnitude appears scale problem size increases 
research sponsored part national science foundation contract number eet defense advanced research projects agency dod arpa order contract monitored avionics laboratory air force wright aeronautical laboratories aeronautical systems division afsc wright patterson afb oh 
views contained document authors interpreted representing official policies expressed implied agencies government 

note attempt review basic ideas connectionism back propagation learning 
see brief overview area chapters detailed treatment 
refer standard back propagation mean back propagation algorithm momentum described 
greatest single obstacle widespread connectionist learning networks real world applications slow speed current algorithms learn 
fastest learning algorithm purposes algorithm generally known back propagation backprop 
back propagation learning algorithm runs faster earlier learning methods slower 
relatively simple problems standard back propagation requires complete set training examples hundreds thousands times 
means limited investigating small networks trainable weights 
problems real world importance tackled networks size tasks connectionist technology appropriate large complex handled current learning network technology 
solution run network simulations faster computers implement network elements directly vlsi chips 
number groups working faster implementations including group cmu processor warp machine 
important network implemented directly hardware slow learning algorithms limit range problems attack 
advances learning algorithms implementation technology complementary 
combine hardware runs orders magnitude faster learning algorithms scale large networks position tackle larger universe possible applications 
january conducting empirical study learning speed simulated networks 
studied standard backprop algorithm number variations standard back propagation applying set moderate sized benchmark problems 
variations investigated proposed researchers systematic studies compare methods individually various combinations standard set learning problems 
systematic studies hope understand methods best situations 
report results obtained months study 
important result identification new learning method combination ideas range encoder decoder problems faster standard back propagation order magnitude 
new method appears scale better standard backprop size complexity learning task grows 
emphasize progress report 
learning speed study far complete 
concentrated effort single class benchmarks encoder decoder problems 
family benchmarks taken isolation encoder decoder problems certain peculiarities may bias results study 
comprehensive set benchmarks run premature draw sweeping strong claims widespread applicability techniques 

methodology 
benchmark 
widely accepted methodology measuring comparing speed various connectionist learning algorithms 
researchers proposed new algorithms theoretical analysis problem 
hard determine theoretical models fit actual practice 
researchers implement ideas run benchmarks demonstrate speed resulting system 
unfortunately researchers choose benchmark different parameters adopt different criteria success 
hard determine algorithm best application 
measurement problem compounded widespread confusion speed standard backpropagation 
selection back propagation learning parameters black art small differences parameters lead large differences learning times 
uncommon see learning times reported literature differ order magnitude problem essentially learning method 
net effect confusion faced vast space possible learning algorithms isolated points explored points hard compare claims various explorers 
need careful systematic effort fill rest map 
primary goal study develop faster learning algorithms connectionist networks hope contribute development new coherent methodology studies kind 
way select family benchmark problems learning speed researchers standard way evaluate algorithms 
try choose benchmarks give insight various learning algorithms perform real world tasks eventually want tackle 
benchmark appears literature exclusive problem called xor network input units hidden units output unit problem train weights network output unit turn inputs 
researchers notably tesauro janssens generalize input parity problem output odd number inputs 
xor parity problem looms large history theory connectionist models see important piece history goal develop learning algorithms real world pattern classification tasks xor parity wrong problem concentrate 
classification tasks take advantage learning network ability generalize input patterns seen training nearby patterns space possible inputs 
networks occasionally sharp distinctions similar input patterns exception rule 
xor parity hand exactly opposite character generalization punished nearest neighbors input pattern produce opposite answer pattern 
popular benchmark problems cluster counting task anti generalizing quality 
change bit big change answer 
part suite benchmarks tasks valuable isolation may encourage develop learning algorithms generalize 
view better concentrate call noisy associative memory benchmarks select set input patterns binary strings chosen random expand cluster similar inputs adding random noise flipping input bits train network produce different output pattern input clusters 
occasionally ask network map different clusters output 
noise modified input patterns training determine network capturing central idea cluster just memorizing specific input patterns seen 
believe performance learning algorithm type problem correlate performance real world pattern classification tasks 
family benchmarks scaled various ways 
vary number pattern clusters number input output units amount noise added inputs 
treat digital problem inputs analog input patterns 
vary number hidden units number layers 
obviously take time accumulated solid set baseline results new algorithms measured 

encoder decoder task coming months intend concentrate noisy associative memory benchmarks plus benchmarks adapted real world applications domains speech understanding road 
early stages study wise stick encoder decoder problems 
family problems popular connectionist community years base shared experience applying older learning algorithms 
encoder decoder problems called simply encoders familiar readers report 
speak encoder mean network layers units layers modifiable weights 
units input layer hidden units units output layer 
connection input unit hidden unit connection hidden unit output unit 
addition unit modifiable threshold 
direct connections input units output units 
normally smaller network bottleneck information flow 
network distinct input patterns input units turned set input bits turned set 
task duplicate input pattern output units 
information flow hidden units network develop unique encoding patterns hidden units set connection weights working perform encoding decoding operations 
encoder network log speak tight encoder 
example encoders tight sense 
hidden units assume values network assign input pattern possible binary codes hidden layer wasting codes 
practice backprop network distinct analog values hidden units tight encoder networks really forced search optimal binary encodings 
possible learn perform encoder decoder task ultra tight network takes longer learning task network 
real world pattern classification task usually give network hidden units perform task easily 
want provide hidden units allows network memorize training examples extracting general features allow handle cases seen training want force network spend lot extra time trying find optimal representation 
suggests relatively loose encoder problems realistic informative benchmarks tight ultra tight encoders 
reported done encoder tight small 
possible run large number experiments problem including slow learning algorithms 
standard encoder problem unusual feature seen typical pattern classification tasks learning example connections input side carries non zero activation training pattern 
standard back propagation algorithm input side weight modified errors due pattern 
separation effects standard encoder decoder somewhat easier typical pattern classification task unrealistic benchmark 
effort understand kind bias may introducing looked complement encoder problems input output patterns ones single zero position 
see complement encoders require learning time standard encoders difference minimized right learning techniques 
encoder complement encoder problems learning times reported epochs 
epoch presentation entire set training patterns 
algorithms studied date perform forward pass backward pass pattern collecting error data weights updated epoch entire set patterns seen 

learning complete 
source confusion learning speed studies done date researcher chosen different criterion successful learning task 
complex pattern classification tasks hard problem reasons 
inputs noisy may impossible perform perfect classification 
second outputs analog nature accuracy required depend demands specific task 
tasks manipulate learning parameters trade speed accuracy generalization reasonable criterion success depend demands particular problem 
believe researchers field need discuss issues arrive consensus various benchmark problems run evaluated 
encoder decoder problem little difficulty agreeing criteria success agreement exists 
researchers accept output output threshold zero 
apply criterion real analog hardware devices slightest bit additional noise drive bits side threshold 
researchers require output close specified target value 
networks performing task binary outputs unnecessarily strict learning algorithms may produce useful outputs quickly take longer adjust output values specified tolerances 
researchers declare success sum squared error outputs falls fixed value 
odd choice binary application want individual outputs correct don want trade error output error 
possibility consider network learned problem input pattern output correct unit larger output 
criterion met earlier training criterion fixed threshold 
criterion training actual hardware network need additional circuitry select largest output better learning network job possible 
addition criterion applicable problems multiple bits output pattern 
suggest problems binary outputs adopt threshold margin criterion similar digital logic designers total range output units value considered zero value considered values considered marginal counted correct training 
output range different scale thresholds appropriately 
creating man land classes outputs produce network tolerate small amount noise outputs 
examples criterion measure success 
training continues entire epoch observe output correct declare learning successful 
back propagation networks deterministic get successful results trials long change weights 

report learning times 
epoch defined single presentation patterns training set convenient unit measure learning time benchmarks reported 
alternative pattern presentation basic unit learning time 
presentation single pattern associated forward propagation results back propagation error modification weights 
presentation natural measure problems finite training set cycle entire training set weight updates 
long clear units reported easy convert epochs presentations researchers choose units fear confusion 
measuring new learning algorithm particular benchmark problem desirable run large number trials weights initialized different random values case 
single trial may misleading choice initial weights greater influence time required difference algorithms 
results reported averages trials large slow problems forced fewer trials 
reporting learning times simple matter trials succeed 
case encoder examples run 
case useful report average learning time trials best worst results standard deviation measure variation results 
unfortunately problems xor network occasionally stuck local minimum escape 
learning trials converge learning time infinite 
trials may take long time mixing long trials average may give distorted picture data 
results reported 
option robert jacobs simply report failures column average successful trials 
problem hard choose learning method fewer failures better average 
addition unclear average polluted long near failures 
second option adopted tesauro janssens 
averaging times trials usual way define average training time inverse average training rate 
training rate trial defined inverse time required trial 
method gives single defined number set trials includes trials long infinite learning rate trials goes zero 
kind average emphasizes short trials de emphasizes long ones results computed way look faster conventional average 
note algorithms equally fast measured conventional average training rate average rate consistent algorithms slower 
algorithms take risky unsound steps get short convergence times trials favored algorithms poorly measures 
option favor lack better idea allow learning program restart trial new random weights network failed converge certain number epochs 
time reported trial include time spent restart 
consider restarts part learning algorithm restart threshold just parameter experimenter adjust best results 
realistic faced method usually converges epochs occasionally gets stuck natural give start point 
xor results reported method reporting 

implementation experiments reported run back propagation simulator developed specifically purpose 
simulator written cmu common lisp runs ibm rt pc workstation mach operating system window system 
machines study provided ibm part joint research agreement computer science department carnegie mellon university 
simulator soon converted window system interface 
relatively easy port code implementation common lisp taken time code portable 
coded common lisp simulator flexible 
easy try program variations hours 
displays turned simulator runs back propagation algorithm encoder roughly epochs second processing rate connection presentations second 
fast experimentation small benchmarks especially new algorithms learn tasks relatively epochs 
larger problems real applications need faster simulator 
hand coding inner loops assembler possible speed simulator considerably factor 
point move inner loops faster machine 
comparison warp machine running standard back propagation rate second times faster simulator difficult machine program 
simulator designed easy experimenter see going inside network 
set windows displays changing values unit outputs unit statistics 
set windows displays weights weight changes weight statistics 
control panel provided operator alter learning parameters real time single step processing see clearly going 
displays immense value helping understand problems developing learning procedure done 

experiments results 
tuning backprop learning parameters task undertaken study understand learning parameters affected learning time relatively small benchmark encoder 
parameters interest learning rate momentum factor range random initial weights 
start learning trial weights thresholds network initialized random value chosen range formula updating weights error derivative weight accumulated epoch 
refer derivative slope weight 
standard back propagation learning slow exhaustively scan dimensional space defined 
cursory exploration done trials point tested order find regions promising 
suggested value value value produce fastest learning problem 
researchers reported results values problem high led poor results requiring epochs 
promising area explored intensively trials point tested 
surprisingly best result obtained momentum problem trials max min average notation says trials parameters specified longest trial required epochs shortest required epochs average runs epochs standard deviation 
standard deviations included give reader crude idea variation values distribution learning times seldom looks normal distribution exhibiting multiple humps example 
average learning time obtained smaller value problem trials max min average hold vary see shaped curve rising average learning time rising steeply value 
increase see shaped valley lowest point close epochs 
floor valley begins rise steeply 
varying parameter small amounts little difference trials 
changing large amount led greatly increased learning times 
value better 
plaut nowlan hinton analysis suggesting may beneficial different values different weights network 
specifically suggest value tuning weight inversely proportional fan unit receiving activation weight 
tried variety parameter values network standard backprop consistently performed worse single constant value weights 
see split epsilon technique turn useful networks variation fan large 
suggests parameter values varied network learns small increased network chosen direction 
best schedule increasing apparently different problem 
explore idea extensively quickprop algorithm described roughly job way adapts problem requiring human operator job 

eliminating flat spot able display weights unit outputs course learning problem standard backprop algorithm clear units turned hard early stages learning getting stuck zero state 
hidden units network output units get stuck 
problem due flat spots derivative sigmoid function approaches zero 
standard back propagation algorithm back propagate error network multiply error seen unit derivative sigmoid function current output unit derivative equal call sigmoid prime function 
note value sigmoid prime function goes zero unit output approaches 
output value represents maximum possible error unit output close pass back tiny fraction error incoming weights units earlier layers 
unit theoretically recover may take long time machine roundoff error potential truncating small values units may recover 

possibility suggested james mcclelland tested michael error measure goes infinity sigmoid prime function goes zero 
mathematically elegant fairly hard implement 
chose explore simpler solution alter sigmoid prime function go zero output value 
modification tried worked best simply added constant sigmoid prime value scale error 
curve goes back curve goes back 
modification dramatic difference cutting learning time half 
see valley space running best learning speed roughly constant increases range 
best values obtained problem trials max min average tried ways altering sigmoid prime function 
radical simply replace function constant value effect eliminates multiplication derivative sigmoid altogether 
best performance obtained variation problem trials max min average tried replacing sigmoid prime function returned random number range 
probably learning trials wasted random number happened small 
best result obtained scheme problem trials max min average tried replacing sigmoid prime function sum constant random value range 
constant problem trials max min average cases kind valley space observed able find value gave near optimal performance 
primary lesson experiments useful eliminate flat spots means 
standard backprop carefully choose learning parameters avoid problem stuck units modified sigmoid prime function optimize parameters best performance 
slight modification classic sigmoid prime function job best replacing step constant constant plus noise reduces learning speed 
suggests general family learning algorithms robust give decent results scale error long don change sign eliminate error signal letting sigmoid prime function go zero 
course results may hold problems multi layer networks 

non linear error function mentioned previous section researchers eliminated flat spots sigmoid prime function units output layer network error function grows infinity difference desired observed outputs goes 
error approaches extreme values product non linear error function sigmoid prime remains finite error signal gets unit get stuck 
david plaut suggested non linear error function speed learning problem stuck units handled means 
idea small differences output desired output error behave linearly difference increased error function grow faster linearly heading infinity errors approach maximum values 
function meets requirements hyperbolic difference 
tried difference error signal fed output units network 
function competing going zero grow arbitrarily large cut values extreme differences 
modest beneficial effect 
encoder backprop hyperbolic arctan error function adding sigmoid prime able get result improvement problem trials max min average tried applying nonlinear error functions units interior layers network plan investigate near 
sort non linear error function may value increasing learning speed networks multiple hidden layers 

quickprop algorithm back propagation relatives calculating partial derivative error respect weight 
information gradient descent weight space 
take infinitesimal steps gradient guaranteed reach local minimum empirically determined problems local minimum global minimum solution purposes 
course want find solution shortest possible time want take infinitesimal steps want take largest steps possible overshooting solution 
unfortunately set partial derivatives collected single point tells little large step may safely take weight space 
knew higher order derivatives curvature error function presumably better 
kinds approaches problem tried 
approach tries dynamically adjust learning rate globally separately weight heuristic way history computation 
momentum term standard back propagation form strategy fixed schedules parameter adjustment recommended case adjustment experience programmer network 
investigated technique heuristically adjusts global parameter increasing successive gradient vectors nearly decreasing 
jacobs conducted empirical study comparing standard backprop momentum rule dynamically adjusts separate learning rate parameter weight 
cater uses complex heuristic adjusting learning rate 
methods improve learning speed degree 
kind approach explicit second derivative error respect weight 
information select new set weights newton method sophisticated optimization technique 
unfortunately requires costly global computation derive true second derivative approximation 
parker watrous becker lecun active area 
watrous implemented algorithms tried xor problem 
claims improvement back propagation appear methods scale larger problems 
developed algorithm call quickprop connection traditions 
second order method loosely newton method spirit heuristic formal 
proceeds standard back propagation weight keep copy error derivative computed previous training epoch difference current previous values weight 
value current training epoch available weight update time 
risky assumptions error vs weight curve weight approximated parabola arms open upward second change slope error curve seen weight affected weights changing time 
weight independently previous current error slopes weight change points slopes measured determine parabola jump directly minimum point parabola 
computation simple uses information local weight updated current previous values course new value crude approximation optimum value weight applied iteratively method surprisingly effective 
notice old parameter gone need keep see 
update formula current slope somewhat smaller previous direction weight change direction 
step may large small depending slope reduced previous step 
current slope opposite direction previous means crossed minimum opposite side valley 
case step place current previous positions 
third case occurs current slope direction previous slope size larger magnitude 
blindly follow formula case infinite step moving backwards current slope local maximum 
experimented ways handling third situation 
method best create new parameter call maximum growth factor 
weight step allowed greater magnitude times previous step weight step computed quickprop formula large infinite uphill current slope times previous step size new step 
idea flattening error curve steeper move afford accelerate limits 
noise coming simultaneous update units don want extrapolate far finite baseline 
experiments show large network behaves fails converge 
optimal value depends extent type problem value works wide range problems 
quickprop changes weights happened previous weight update need way bootstrap process 
addition need way restart learning process weight previously taken step size zero seeing non zero slope changed network 
obvious move gradient descent current slope learning rate start process restart process weight previous step size zero 
took tries get ignition process working 
originally picked small threshold switched quadratic approximation gradient descent previous weight fell threshold 
worked fairly came suspect odd things happening vicinity threshold especially large encoder problems 
replaced mechanism added gradient descent term step computed quadratic method 
worked weight moving slope led oscillation weight minimum come back quadratic method accurately locate bottom parabola gradient descent term push weight past point 
current version quickprop adds times current slope value computed quadratic formula current slope opposite sign previous slope case quadratic term 
final refinement required 
problems quickprop allow weights grow large 
leads floating point overflow errors middle training session 
fix adding small weight decay term slope computed weight 
keeps weights acceptable range 
quickprop suffer flat spot problems standard backprop run sigmoid prime function modified addition described previous section 
normal linear error function result best obtained quickprop problem trials max min average addition hyperbolic arctan error function quickprop better problem trials max min average result better factor time obtained modified non quadratic version backprop order magnitude better value obtained standard backprop 
quickprop parameter require problem specific tuning tuned carefully reasonably results 

scaling experiments step see combination quickprop adding sigmoid prime hyperbolic arctan error scale larger encoder problems 
decided run series tight encoders 
larger problems series fan hidden units greater fan output units proved beneficial divide value fan unit receiving activation weight updated 
proved useful gradually reduce problem size increased 
results obtained series follows problem trials max min average times significantly better seen tight encoder problems 
literature field gives specific timings problems especially large ones 
best time obtained encoder standard backprop epochs average time trials 
sigmoid prime function modified add time goes 
david plaut run backprop simulations graduate student career cmu able get times generally low encoder backprop non linear error function 
accomplishes watching progress learning trial display adjusting hand learning progresses 
method hard replicate unclear scales 
suspect analysis real time adjustments show doing similar quickprop 
juergen schmidhuber investigated class problems methods standard backprop adjusted weights presentation training example full epoch second learning technique measures total error derivative tries converge zero error function 
encoder schmidhuber reports learning time epochs backprop method gets backprop method 
exciting aspect learning times table way scale problem size increases 
take number patterns learned learning time measured epochs growing slowly log past generally believed tight encoder problems time grow exponentially problem size linearly 
course measurement learning time epochs deceiving 
number training examples epoch grows factor time required run forward backward pass serial machine proportional number connections roughly factor means serial machine techniques described actual clock time required grows factor log parallel network clock time required grows factor nlog scaling result holds larger networks kinds problems news applicability connectionist techniques 
order get feeling learning time affected number units single hidden unit layer ran problem different values 
results quickprop hyperbolic arctan error added sigmoid prime epsilon divided fan 
problem trials max min average interesting result learning time goes monotonically increasing greater researchers suggested certain point learning slower add hidden units 
belief probably came standard backprop additional hidden units tend push output units deeper flat spot 
course serial simulation clock time may increase units added extra connections simulated 

complement encoder problem mentioned earlier standard encoder problem peculiar feature connections input side active training patterns 
quickprop scheme assumption weight changes strongly coupled guess quickprop looks better encoder problems 
test ran series experiments complement encoder problem input output patterns string bits single zero 
standard encoder unusually easy quickprop complement encoder unusually hard 
complement encoder problem run learning algorithms standard backprop backprop added sigmoid prime function hyperbolic error function quickprop hyperbolic arctan error 
case trials run quick search run determine best learning parameters method 
epsilon values marked asterisk divided fan 
results summarized table comparison rightmost column shows time required method normal encoder problem 
method max min average normal standard 
bp bp sig prime bp sig prime hyper err qp hyper err qp hyper err symmetric method tried learning complement encoder took twice long normal encoder 
believe complement encoder information gathered trial unable affect weight relevant information indirect effect 
quickprop case takes times long learn complement encoder learn normal encoder 
row table shows time required quickprop hyperbolic arctan error units activation function symmetric zero ranging 
input output patterns thresholds detect successful learning shifted 
stornetta huberman advocate symmetric units 
empirical results report sketchy claim speedups ranging depending problem 
occurred symmetric units encoder complement encoder problems equivalent clear meant complement encoder learned times faster regular encoder learned times slower 
table shows result extremes closer speed normal encoder 
normal encoder problems suited asymmetric activation functions symmetric activation problems 

exclusive problem argued xor problem receives attention learning speed studies popular benchmark felt try xor new methods 
problem conservative parameter settings trials got caught local minimum showed signs convergence restart method described section 
quickprop hyperbolic arctan error epsilon divided fan symmetric activation restart limit set epochs got result xor hidden units problem trials max min average xor trials restarts 
median learning time epochs 
time epochs compares favorably times reported problem backprop backprop algorithms 
jacobs reports time plus failure standard backprop plus failures delta bar delta algorithm 
tesauro janssens report time epochs rate averaging function 
watrous reports learning time epochs slightly different version xor uses single hidden unit additional connections bfgs method learns problem epochs involves expensive non local computation update epoch 
rumelhart hinton williams report yves chauvin got learning times xor problem hidden units 
course studies slightly different success criteria usually stricter criterion 

problems tested date learning algorithms described run faster standard backprop scale better 
results encouraging conclusive tested new techniques small atypical set benchmark problems 
important task immediate apply new algorithms additional modifications variety additional benchmarks real world applications 
speedup scaling results hold tests achieved breakthrough especially algorithms implemented fastest available machines 
development new algorithms driven theoretical analysis observing problems occurred standard back propagation learning attempting cure problems 
result admittedly patchwork 
seen accomplished useful try develop theoretical understanding tricks 
example possible develop better understanding expected performance quickprop various classes problems 
sort understanding ultimately lead elegant faster learning algorithms 
issues important incremental learning adding new knowledge network trained development networks handle recognize produce time varying sequences inputs individual patterns 
interesting see learning techniques applicable complex domains 
acknowledgments geoff hinton interest networks kind dave touretzky persistent efforts promote active interchange ideas local connectionist community 
david plaut mark barak pearlmutter kevin lang offered valuable suggestions helped folklore back propagation learning systems 
becker lecun feasibility applying numerical optimization techniques back propagation 
proceedings connectionist models summer school 
morgan kaufman 
appear 
cater successfully peak learning rates greater back propagation networks heuristic learning algorithm 
proceedings ieee international conference neural networks pages 
san diego ca 
fahlman hinton connectionist architectures artificial intelligence 
ieee computer january 
speech recognition back propagation 
proceedings ninth annual conference ieee engineering medicine biology society 

jacobs increased rates convergence learning rate adaptation 
technical report coins tr university massachusetts amherst dept computer information science amherst ma 
lecun une procedure apprentissage pour learning procedure asymmetric threshold network 
proceedings pages 
paris 
parker learning logic 
technical report tr massachusetts institute technology center computational research economics management science cambridge ma 
parker optimal algorithms adaptive networks second order back propagation second order direct propagation second order hebbian learning 
proceedings ieee international conference neural networks pages 
san diego ca 
rumelhart hinton williams learning internal representations error propagation 
rumelhart mcclelland 
editor parallel distributed processing explorations microstructure cognition chapter 
mit press cambridge ma london england 
rumelhart mcclelland parallel distributed processing explorations microstructure cognition 
mit press cambridge ma london england 
minsky papert perceptrons 
mit press cambridge ma london england 
plaut nowlan hinton experiments learning back propagation 
technical report cmu cs carnegie mellon university computer science dept pittsburgh pa 
pomerleau touretzky kung neural network simulation warp speed got connections second 
proceedings ieee international conference neural networks 
san diego ca 
appear 
schmidhuber accelerated learning back propagation nets 
technical report technische universitaet muenchen institut fuer informatik munich federal republic germany 
stornetta huberman improved layer back propagation algorithm 
proceedings ieee international conference neural networks pages 
san diego ca 
tesauro janssens scaling relationships back propagation learning 
complex systems 
watrous learning algorithms connectionist networks applied gradient methods non linear 
proceedings ieee international conference neural networks pages 
san diego ca 
werbos regression new tools prediction analysis behavioral sciences 
phd thesis harvard university 
table contents 

methodology 
benchmark 

encoder decoder task 
learning complete 

report learning times 

implementation 
experiments results 
tuning backprop learning parameters 
eliminating flat spot 
non linear error function 
quickprop algorithm 
scaling experiments 
complement encoder problem 
exclusive problem 
acknowledgments 
