correcting sample selection bias unlabeled data huang school computer science university waterloo canada huang cs uwaterloo ca arthur mpi biological cybernetics bingen germany arthur tuebingen mpg de bernhard sch lkopf mpi biological cybernetics bingen germany bs tuebingen mpg de technical report cs alexander smola anu canberra australia alex smola anu edu au karsten ludwig maximilians university munich germany kb dbs ifi lmu de consider scenario training test data drawn different distributions commonly referred sample selection bias 
algorithms setting try recover sampling distributions appropriate corrections distribution estimate 
nonparametric method directly produces resampling weights distribution estimation 
method works matching distributions training testing sets feature space 
experimental results demonstrate method works practice 
default assumption learning scenarios training test data independently identically iid drawn distribution 
distributions training test set match facing sample selection bias covariate shift 
specifically domain patterns labels obtain training samples xm ym borel probability distribution pr test samples drawn distribution pr 
exists previous addressing problem sample selection bias typically ignored standard estimation algorithms 
reality problem occurs frequently available data collected biased manner test usually performed general target population 
give examples similar situations occur domains 

suppose wish generate model diagnose breast cancer 
suppose women participate breast screening test middle aged attended screening preceding years 
consequently sample includes older women low risk breast cancer tested 
examples reflect general population respect age amounts bias pr contain diseased cases bias pr 

consider performing data analysis brain computer interface distribution incoming signals known change experiments go subjects get tired sensor setup changes case necessary adapt estimator new distribution patterns order improve performance 

gene expression profile studies dna microarrays tumor diagnosis 
common problem samples obtained certain protocols microarray platforms analysis techniques 
addition typically small sample sizes 
test cases recorded different conditions resulting different distribution gene expression values 
utilize availability unlabeled data direct sample selection procedure various learning methods 
previous infer resampling weight directly distribution matching training testing sets feature space non parametric manner 
require estimation biased densities selection probabilities assumption probabilities different classes known 
account difference pr pr reweighting training points means training test points reproducing kernel hilbert space rkhs close 
call reweighting process kernel mean matching kmm 
rkhs universal population solution exactly ratio pr pr derive cautionary result states granted ideal population reweighting convergence empirical means rkhs depends upper bound ratio distributions dimension space extremely slow ratio large 
required optimisation simple qp problem reweighted sample incorporated straightforwardly different regression classification algorithms 
apply method variety regression classification benchmarks uci classification microarrays prostate breast cancer patients 
experiments demonstrate kmm greatly improves learning performance compared training unweighted data reweighting scheme cases outperform reweighting true sample bias distribution 
key assumption general estimation problem different distributions pr pr unsolvable terms arbitrarily far apart 
particular arbitrary pr pr way infer estimator training sample 
simplifying assumption pr pr differ pr pr pr pr pr 
words conditional probabilities remain unchanged particular case sample selection bias termed covariate shift 
see experimentally situations key assumption valid method perform see section 
sample reweighting stating problem regularized risk minimization 
general learning method minimizes expected risk pr pr loss function depends parameter 
instance loss function negative log likelihood log pr misclassification loss form regression loss 
typically observe examples drawn pr pr resort computing empirical average remp xi yi 
avoid overfitting minimizing remp directly minimize regularized variant remp regularizer 
sample correction problem involved pr pr different 
training set drawn pr really minimize pr wish generalize test examples drawn pr observation field importance sampling pr pr pr pr pr pr provided support pr contained support pr 
compute risk respect pr pr 
similarly estimate risk respect pr computing remp 
key problem coefficients usually unknown need estimate data 
pr pr differ pr pr pr pr reweighting factor training examples 
reweight observation observations represented pr obtain higher weight represented cases 
estimate pr pr subsequently compute estimates 
closely related methods estimate selection probabilities prior knowledge class distributions 
intuitive approach major problems works density estimates pr pr potentially selection probabilities class distributions 
particular small errors estimating pr lead large coefficients consequently serious corresponding observations 
second estimating densities just purpose computing reweighting coefficients may overkill may able directly estimate coefficients xi yi having estimate distributions 
furthermore regularize directly flexibility prior knowledge account similar learning methods problems 
sample reweighting learning algorithms describe estimate reweighting coefficients briefly discuss minimize reweighted regularized risk classification regression settings 
il xi yi support vector classification utilizing setting minimization problem original svms formulated way minimize subject xi yi xi yi 
feature map feature space denotes discrepancy function dual minimize iy jy xi xj iy subject iy iy yi ic 
denotes inner product feature maps 
generalizes observation dependent binary sv classification described 
modifications existing solvers straightforward 
penalized lms regression assume minimize yi xi 
denote diagonal matrix diagonal rm kernel matrix kij xi xj 
case minimizing equivalent minimizing respect 
assuming full rank minimization yields 
advantage formulation solved easily solving standard penalized regression problem 
essentially rescale regularizer depending pattern weights higher weight observation regularize 
penalized logistic regression referred gaussian process classification 
minimize log yi xi yields modified optimization problem log yi xi 
exponential families kernel approach log log exp invoke representer theorem leads minimize ig xi iy jk xi xj yj xi log exp distribution matching jy xi xj iy jy xi xj kernel mean matching relation importance sampling map feature space denote expectation operator pr ex pr 
clearly linear operator mapping space probability distributions feature space 
denote pr pr image 
set referred marginal polytope 
theorem theorem operator bijective rkhs universal kernel sense 
proof universal rkhs unit ball need prove pr pr pr pr equivalently pr pr 
may write pr pr sup pr pr sup epr epr pr pr clear pr pr zero pr pr prove converse result lemma pr pr borel probability defined separable metric space pr pr epr pr space continuous bounded functions show pr pr implies pr pr equivalent pr pr implying pr pr result implies pr pr 
pr pr exists epr pr 
definition universality dense respect norm means find satisfying 
obtain epr epr consequently epr pr epr pr 
pr pr 
epr pr feature space means compare distributions explored 
practical consequence result know pr infer suitable solving minimization problem minimize pr ex pr subject ex pr 
kernel mean matching kmm procedure 
lemma problem convex 
assume pr absolutely continuous respect pr pr implies pr 
assume universal 
solution pr pr 
proof convexity objective function comes facts norm convex function integral linear functional 
constraints convex 
virtue constraints feasible solution corresponds distribution pr 
hard see pr pr feasible minimizes objective function value exists due absolute continuity pr respect pr 
theorem implies distribution pr pr pr 
pr pr convergence reweighted means feature space lemma shows principle knew pr pr fully recover pr solving simple quadratic program 
practice pr pr known 
samples size drawn iid pr pr respectively 
naively just replace expectations empirical averages hope resulting optimization problem provides estimate 
expected empirical averages differ due finite sample size effects 
section explore effects 
demonstrate finite sample case fixed empirical estimate expectation normally distributed provides natural limit precision enforce constraint pr empirical expectations return point section 
lemma fixed function xi pr iid xi finite mean non zero variance sample mean xi converges distribution gaussian mean pr standard deviation bounded lemma direct consequence central limit theorem theorem 
alternatively straightforward get large deviation bound likewise converges 
second result demonstrates deviation empirical means pr pr feature space chosen perfectly population sense 
particular result shows convergence means slow large difference probability mass pr pr bound ratio probability masses large 
lemma addition lemma conditions assume draw iid pr pr probability xi xi proof lemma need mcdiarmid theorem 
log theorem denote xm function independent random variables 
xm xi xi xm ci xm denote case pr xm ex xm xm exp 
proof xi xi 
proof follows firstly tail behavior concentration inequality subsequently bounding expectation 
apply mcdiarmid tail bound need bound change replace xi xi likewise replace arbitrary triangle inequality replacement xi arbitrary change xi xi br likewise replacement changes br pr ex exp probability deviation random variable expectation bounded ex log bound expected value ex ex terms mean pr obtain ex xi xi ex pr pr ex pr pr pr combining bounds mean tail proves claim 
note lemma shows correct population sense bound deviation feature space mean pr reweighted feature space mean pr 
guarantee find coefficients close xi gives useful upper bound outcome optimization 
lemma implies convergence means different distributions need large equivalent sample size get reasonable convergence 
result implies unrealistic assume empirical means reweighted match exactly 
empirical kmm optimization find suitable values rm want minimize discrepancy means subject constraints 
limits scope discrepancy pr pr ensures measure pr close probability distribution 
objective function discrepancy term empirical means 
kij xi xj xi may check xi const 
necessary ingredients formulate quadratic problem find suitable minimize subject 
accordance lemma conclude choice 
note quadratic program solved efficiently interior point methods successive optimization procedure 
point resembles single class svm trick 
approximate equality constraint main difference linear correction term means 
large values correspond particularly important observations xi lead large risk estimates concerned distribution matching purpose finding reweighting scheme empirical means training test set show long means test set approximated able obtain unbiased risk estimates regardless actual values vs importance sampling weights xi 
price increase variance estimate 
act effective sample size 
simplicity consider transductive case 
uniform convergence statements respect ey ey 
extension unconditional expected risks straightforward 
interested behavior loss induced function class difference section relates parameterization model current section relating loss 
proceeds steps show expected loss ey xl coefficients obtain risk estimate low bias 
secondly show random variable il xi yi concentrated il xi condition key assumption require expressed inner product feature space member reproducing kernel hilbert space rkhs bounded norm 
lemma require key assumptions satisfied iid samples drawn pr pr respectively 
class loss induced functions assume exist xi 
case bound empirical risk estimates follows sup il xi yi ey ey proof see claim note key assumption conditional distributions pr pr pr linearity apply ey summand individually 
key assumption expected loss written may rewrite lhs sup il xi sup xi definition norms bounded proves claim 
final step relating reweighted empirical average expected risk respect pr requires bound deviations term 
lemma require key assumption satisfied drawn iid pr 
assume expressed element rkhs probability sup il xi yi il xi log cr proof proof strategy identical lemma 
denote sup xi yi xi maximum deviation empirical mean expectation 
key random variables ym conditionally independent replacing yi arbitrary leads change bounded mc xi yi xi cr mcdiarmid theorem bound pr words ey exp acts effective sample size comes determining large deviations 
symmetrization obtain bound expectation ey ey xe sup il xi yi il xi yi sup il xi yi 
ey xe inequality follows convexity 
second follows fact yi yi pairs independently identically distributed swap pairs 
reflected binary rademacher random variables equal probability 
constant rhs commonly referred rademacher average 
actual progress computing condition lemma allows bound supremum 
convexity yields bounds rhs rhs ey xe xi yi ey xe xi yi eyi xi xi yi cr cr combining bound expectation solving tail bound proves claim 
combine bounds lemmas obtain main result section corollary assumptions lemma probability sup il xi yi ey log cr means minimize reweighted empirical risk high probability minimizing upper bound expected risk test set 
exactly study section 
experiments toy regression example experiment toy data intended mainly provide comparison approach 
method uses information criterion optimise weights certain restrictions pr pr pr known pr known exactly gaussian unknown parameters approximated kernel density estimation 
data generated polynomial regression example section pr pr normal distributions 
observations generated observed gaussian noise standard deviation see blue curve noise free signal 
sampled training blue circles testing red circles points pr pr respectively 
attempted model observations degree polynomial 
black dashed line best case scenario shown purposes represents model fit ordinary squared ols labeled test points 
red line second result derived training data ols predicts test data poorly 
dashed lines fit weighted ordinary square weighting schemes ratio underlying training test densities kmm information criterion 
summary performance trials shown 
method outperforms reweighting methods 
real world datasets test approach real world data sets select training examples deliberately biased procedure 
describe biased selection scheme need define additional random variable si point pool possible training samples si means ith sample included si indicates excluded sample 
situations considered selection bias corresponds assumption regarding relation training test distributions si xi yi si xi si dependent yi si xi yi si yi potentially creates greater challenge violates key assumption 
compare method labeled kmm baseline unweighted method unweighted modification weighting true fitting model ols fitting ols fitting ratio kmm min ic sum square loss ratio kmm ic ols polynomial models degree fit ols average performances methods ols test data 
labels ratio ratio test training density kmm approach min ic approach ols model trained labeled test points 
inverse true sampling distribution importance sampling 
emphasise method require prior knowledge true sampling probabilities 
experiments gaussian kernel exp xi xj kernel classification regression algorithms parameters optimization 
breast cancer dataset dataset uci archive binary classification task 
includes examples classes benign positive label malignant negative label 
data randomly split training test sets proportion examples training varies 
test results averaged trials obtained support vector classifier kernel size 
consider biased sampling scheme input features integer values 
smaller feature values predominate unbiased data sample repeating experiment features turn 
results average random training test splits data training testing 
performance shown consistently outperform unweighted method match exceed performance obtained known distribution ratio 
consider sampling bias operates jointly multiple features 
select samples test error unweighted importance sampling kmm biased feature simple bias features test error bias labels unweighted importance sampling kmm training set proportion test error unweighted importance sampling kmm training set proportion joint bias features optimal weights inverse true sampling vs inverse sampling prob 
classification performance analysis breast cancer dataset uci 
sample mean training data si xi exp xi 
performance method better unweighted case better reweighting sampling model 
consider simple biased sampling scheme depends label data average twice positive negative examples uniformly sampled 
average performance different training testing split proportions remarkably despite assumption regarding difference training test distributions violated method improves test performance outperforms reweighting density ratio large training set sizes 
shows weights proportional inverse true sampling probabilities positive examples higher weights negative ones lower weights 
benchmark datasets table test results methods datasets different sampling schemes 
datasets marked regression problems 
results averages trials regression problems trials classification problems 
nmse test err 
dataset selected unweighted import 
sampling kmm 
abalone 
ca housing 
delta ailerons 
ailerons 
haberman 
usps vs 
usps vs 
bank fm 
bank nh 
cpu act 
cpu small 
delta ailerons 
boston house 
kin nm 
puma nh 
haberman 
usps vs 
usps vs 
usps vs 
breast cancer 

ionosphere 
german credit compare performance benchmark datasets selecting training data various biased sampling schemes 
specifically sampling distribution bias labels exp exp datasets simple step distribution datasets 
remaining regression datasets cf www pt regression datasets html classification sets uci 
sets numbers brackets examined different sampling schemes 
datasets generate biased sampling schemes features 
pca selecting principal component training data corresponding projection values 
denoting minimum value projection mean apply normal distribution mean variance biased sampling scheme 
please refer appendix detailed parameter settings 
penalized lms regression problems svm classification problems 
evaluate generalization performance utilize normalized mean square error nmse yi regression problems average var test error classification problems 
experiments reweighting approach accurate see table despite having prior information bias test sample cases despite additional fact data reweighting conform key assumption 
addition kmm improves test performance compared unweighted case 
tumor diagnosis microarrays benchmark dataset microarrays prostate cancer patients 
microarrays measures expression levels genes 
dataset comprises samples normal tissues positive label tumor tissues negative label 
simulate scenario sets microarrays dissimilar proportions tumor samples want perform cancer diagnosis classification training predicting select training examples biased selection scheme 
remaining data points form test set 
perform svm classification unweighted kmm importance sampling approaches 
experiment repeated independent draws dataset biased scheme resulting test errors plotted 
kmm achieves higher accuracy levels unweighted approach close importance sampling approach 
study similar scenario breast cancer microarray datasets measuring expression levels common genes normal cancer patients 
train svm test 
reweighting method achieves significant improvement classification accuracy unweighted svm see 
method promises valuable tool cross platform microarray classification 
discussion new kernel method dealing sampling bias various learning problems 
directly estimate resampling weights matching training testing distributions feature space 
addition develop general theory bounding matching error terms test error kmm unweighted importance sampling iterations classification accuracy unweighted kmm west west test errors trials cancer diagnosis index sorted error values 
classification results training testing different sources microarray examples breast cancer support distribution sample sizes 
experiments demonstrated advantage correcting sampling bias unlabeled data various problems 
authors patrick heidelberg providing microarray datasets 
partially supported bmbf project part german genome analysis network 
funded australian government backing australia ability initiative part arc supported part ist programme ec pascal network excellence ist 
casella berger 
statistical inference 
duxbury pacific grove ca nd edition 
schapire phillips 
correcting sample selection bias maximum entropy density estimation 
advances neural information processing systems 
dudley 
real analysis probability 
cambridge university press 
sch lkopf smola 
kernel method sample problem 
nips 
mit press 
chen peterson borg meltzer 
estrogen receptor status breast cancer associated remarkably distinct gene expression patterns 
cancer research 
heckman 
sample selection bias specification error 
econometrica 
hoeffding 
probability inequalities sums bounded random variables 
journal american statistical association 
lin lee wahba 
support vector machines classification nonstandard situations 
machine learning 
mcdiarmid 
method bounded differences 
survey combinatorics pages 
cambridge university press 
mendelson 
notes statistical learning theory 
advanced lectures machine learning number lnai pages 
springer 
rosset zhu zou hastie 
method inferring label sampling mechanisms semi supervised learning 
advances neural information processing systems 
schmidt gish 
speaker identification support vector classifiers 
proc 
icassp pages atlanta ga may 
sch lkopf platt shawe taylor smola williamson 
estimating support high dimensional distribution 
neural computation 

improving predictive inference shift weighting log likelihood function 
journal statistical planning inference 
singh ross jackson manola ladd tamayo 
gene expression correlates clinical prostate cancer behavior 
cancer cell 

influence kernel consistency support vector machines 
journal machine learning research 

support vector machines universally consistent 
compl 
sugiyama 
ller 
generalization error estimation covariate shift 
workshop information induction sciences pages tokyo japan 
tsochantaridis joachims hofmann altun 
large margin methods structured interdependent output variables 
journal machine learning research 

cross platform analysis cancer microarray data improves gene expression classification phenotypes 
bmc bioinformatics nov 
west blanchette huang ishida olson jr marks 
predicting clinical status human breast cancer gene expression profiles 
pnas 
williams barber 
bayesian classification gaussian processes 
ieee transactions pattern analysis machine intelligence pami 
zadrozny 
learning evaluating classifiers sample selection bias 
international conference machine learning icml 
experiments statistics datasets experiments section 

