errata preface discrete event dynamic systems theory applications kluwer academic publishers 
manufactured netherlands 
preliminary unedited version incorrectly published part volume numbers april special issue learning optimization decision making guest edited xi ren cao 
publisher offers apology printing incorrect version special issue renders true correct 
discrete event dynamic systems theory applications kluwer academic publishers 
manufactured netherlands 
advances hierarchical reinforcement learning andrew barto barto cs umass edu autonomous learning laboratory department computer science university massachusetts amherst ma sridhar mahadevan cs umass edu autonomous learning laboratory department computer science university massachusetts amherst ma 
reinforcement learning curse dimensionality number parameters learned grows exponentially size compact encoding state 
attempts combat curse dimensionality turned principled ways exploiting temporal abstraction decisions required step invoke execution temporally extended activities follow policies termination 
leads naturally hierarchical control architectures associated learning algorithms 
review approaches temporal abstraction hierarchical organization machine learning researchers developed 
common approaches reliance theory semi markov decision processes emphasize review 
discuss extensions ideas concurrent activities multiagent coordination hierarchical memory addressing partial observability 
concluding remarks address open challenges facing development reinforcement learning hierarchical setting 
keywords reinforcement learning markov decision processes semi markov decision processes hierarchy temporal abstraction 
reinforcement learning rl bertsekas tsitsiklis sutton barto active area machine learning research receiving attention elds decision theory operations research control engineering :10.1.1.32.7692
rl algorithms address problem behaving agent learn approximate optimal behavioral strategy interacting directly environment 
control terms involves line approximation solutions stochastic optimal control problems usually conditions incomplete knowledge system controlled 
rl algorithms adapt standard methods stochastic dynamic programming dp line problems large state spaces 
focusing computational effort behavioral trajectories function approximation methods accumulating value function information rl algorithms produced results problems pose signi cant challenges standard methods crites barto tesauro 
current rl methods means completely circumvent curse dimensionality exponential growth number parameters learned barto mahadevan size compact encoding system state 
attempts combat curse dimensionality turned principled ways exploiting temporal abstraction decisions required step invoke execution activities follow policies termination 
leads naturally hierarchical control architectures learning algorithms 
article review related approaches temporal abstraction hierarchical control developed machine learning researchers discuss extensions ideas open challenges facing development rl hierarchical setting 
despite fact research area began machine learning community provide survey related literature quite extensive 
attempt provide information interested readers probe relevant topics deeply 
important goal attempt thoroughly relate machine learning research topics extensive literature systems control engineering hierarchical multilayer control hybrid systems closely related topics 
obvious parallels approaches escaped careful scope article 
largely adhere notation terminology typical machine learning community 
gained integrating perspectives different groups researchers hope article contribute required dialog 
brief introductions markov semi markov decision processes introduce basic ideas rl review approaches hierarchical rl options formalism sutton 
hierarchies machines approach parr parr russell maxq framework dietterich 
approaches developed relatively independently elements common 
particular rely theory semi markov decision processes provide formal basis 
take advantage commonalities exposition thorough integration approaches awaits 
full justice approaches attempt provide detail approaches concrete reader directed original papers detailed treatments 
nal sections brie extensions ideas focus conducted laboratory concurrent activities multiagent coordination hierarchical memory addressing partial observability 
concluding remarks address open challenges facing development reinforcement learning hierarchical setting 

markov semi markov decision processes rl research formalism markov decision processes mdps 
rl means restricted mdps discrete time countable fact usually nite state action formalism provides simplest framework study basic algorithms properties 
brie describe known framework twists characteristic rl research additional details bertsekas bertsekas tsitsiklis advances hierarchical reinforcement learning puterman ross sutton barto :10.1.1.32.7692
nite mdp models type problem 
stage sequence stages agent controller observes system state contained nite set executes action control selected nite non empty set admissible actions 
agent receives immediate reward having expected value state stage probability js 
expected immediate rewards state transition probabilities js comprise rl researchers call step model action stationary stochastic policy speci es agent executes action probability observes state policy denotes expected nite horizon discounted return agent uses policy letting denote state stage immediate reward acting stage de ned efr gr js pg discount factor 
value value function corresponding nite nite horizon discounted mdp 
objective nd optimal policy policy maximizes value state 
unique optimal value function value function corresponding optimal policies 
rl research addresses discounted mdps comprise simplest class mdps restrict attention discounted problems 
rl algorithms developed average reward mdps mahadevan schwartz research done extending aspects hierarchical approaches discuss article average reward case mahadevan 
playing important roles rl action value functions assign values admissible state action pairs 
policy value denoted expected nite horizon discounted return executing state efr gr js pg optimal action value function assigns admissible state action pair expected nite horizon discounted return executing optimal policy 
action value functions de nitions return de ned analogously 
exploit fact value functions satisfy various bellman equations js barto mahadevan max js analogous equations exist example bellman equation js max example consider value iteration successively approximates follows 
iteration updates approximation applying operation state vk max js vk call operation backup updates state value transferring information approximate values possible successor states 
applying backup operation state called sweep 
starting arbitrary initial function sequence fv kg produced repeated sweeps converges similar algorithm exists successively approximating backup js max qk optimal policy policy assigns non zero probability actions realize maximum right hand side 
similarly optimal policy policy assigns non zero probability actions maximize 
maximizing actions called greedy actions policy de ned way stochastic greedy policy 
suf ciently close approximations obtained value iteration optimal policies taken corresponding greedy policies 
note nding greedy actions require access step action models js available right hand side evaluated 
reasons action value functions play signi cant role rl 
mdp sequential nature decision process relevant amount time passes decision stages 
generalization semi markov decision process smdp amount time decision random variable real integer valued 
real valued case smdps model continuous time discrete event systems mahadevan puterman 
advances hierarchical reinforcement learning discrete time smdp howard decisions positive integer multiples underlying time step 
case usual treat system remaining state random waiting time howard instantaneous transition occurs state 
due relative simplicity discretetime underlies approaches hierarchical rl signi cant obstacles extending approaches continuous time case 
random variable denote positive waiting time state action executed 
transition probabilities generalize give joint probability transition state state occurs time steps action executed 
write joint probability tjs 
expected immediate rewards bounded give amount discounted reward expected accumulate waiting time action bellman equations max tjs tjs max correspondingly extend smdps howard puterman 

reinforcement learning complexity polynomial number states require prohibitive amounts computation large state sets result discretizing multi dimensional continuous spaces representing state sets consisting possible con gurations nite set structural elements possible con gurations backgammon board tesauro 
methods proposed approximating effort required conventional dp rl methods novel monte carlo stochastic approximation function approximation techniques 
speci cally rl algorithms combine features 
avoid exhaustive sweeps restricting computation states neighborhood multiple sample trajectories real simulated 
computation guided sample trajectories approach exploit situations states low probabilities occurring actual experience 
barto mahadevan 
simplify basic sampling 
generating evaluating state possible immediate successors estimate backup effect sampling appropriate distribution 

represent value functions policies compactly lookup table representations function approximation methods linear combinations basis functions neural networks methods 
features re ect nature approximations usually sought rl 
policies close optimal uniformly entire state space rl methods arrive non uniform approximations re ect behavior agent 
agent policy need high precision states rarely visited 
feature understood aspect rl results exist linear case notably tsitsiklis van roy numerous examples illustrate function approximation schemes nonlinear adjustable parameters multilayer networks effective dif cult problems crites barto mahadevan singh bertsekas tesauro :10.1.1.22.3540
rl algorithms widely learning watkins watkins dayan sarsa rummery niranjan sutton :10.1.1.51.4764
learning expected immediate reward expected maximum action value successor state right hand side respectively replaced sample reward maximum action value sample state 
common way obtain samples generate sample trajectories simulation observing actual decision process time 
suppose agent observes current state executes action receives immediate reward observes state learning algorithm updates current estimate update qk ak qk ak max qk time varying learning rate parameter 
values state action pairs remain unchanged update 
limit action values admissible state action pairs updated nitely decays increasing obeying usual stochastic approximation conditions fq kg converges probability jaakkola bertsekas tsitsiklis 
long conditions satis ed policy followed agent learning irrelevant 
course learning agent policy matter usually interested agent performance learning process just asymptotically 
usual practice agent select actions policy greedy respect current estimate introducing non greedy exploratory actions attempt widely sample state action pairs 
sarsa similar learning maximum action value state right hand side replaced action value actual state action pair advances hierarchical reinforcement learning gq action executed state 
sutton called algorithm sarsa due dependence equation special case called sarsa :10.1.1.51.4764
learning agent policy matter 
singh 
show policy property action executed nitely state visited nitely greedy respect current action value function limit singh 
call glie greedy limit nite exploration policy appropriately decaying sequence fq kg generated sarsa converges probability 
learning sarsa learning algorithms apply smdps continuous discrete time interprets immediate reward return accumulated waiting time state appropriately adjusts discounting re ect waiting time 
example discrete time case executed state time step transition follows time steps gr max qk immediate reward received time step return accumulated waiting time bounded computed recursively waiting time 
bradtke duff showed continuous time smdps parr proved converges essentially conditions required qlearning convergence das 
developed average reward case 
crites crites barto learning continuous time discrete event formulation elevator dispatching problem application illustrates useful features rl methods discrete event systems 
learning sarsa require explicit knowledge expected immediate rewards state transition 
samples respective distributions come stochastic simulation real world 
signi cant advantage problems easier produce simulation expected rewards transition probabilities explicit 
crites elevator dispatcher example learning applied trajectories generated simulation story elevator system 
explicit system dif cult explicit 
illustrates advantages socalled model free rl algorithms learning sarsa meaning need access explicit representations expected immediate reward function state transition probabilities 
importantly rl restricted model free methods sutton barto bertsekas tsitsiklis :10.1.1.32.7692
rl algorithms estimate action values learning sarsa second advantage applied discrete event systems 
mentioned section nding optimal actions require access step action models barto mahadevan js available 
step ahead search needed determine optimal actions 
problems involving discrete event systems elevator dispatching problem clear conduct step ahead search event occur nite number times 
action values eliminates dif culty 
point brief presentation rl algorithms assumed possible explicitly store values state action values stateaction pair 
obviously feasible large scale problems extensions algorithms need considered adjust parameters parametric representations value functions 
relatively easy produce extensions mentioned theory behavior contains open questions scope article 
view dp rl just outlined assembled researchers roughly years represents current state understanding intuition underlying origination methods 
dp learning originated far back samuel famous checkers player samuel existing time 
early rl research explicitly motivated animal behavior neural basis minsky klopf sutton barto 
current interest attributable werbos watkins tesauro backgammon playing system td gammon tesauro 
additional information rl barto bertsekas tsitsiklis kaelbling sutton barto :10.1.1.134.2462:10.1.1.32.7692:10.1.1.117.6173
despite utility rl methods applications amount time take form acceptable approximate solutions unacceptable 
result rl researchers investigating various means introducing abstraction hierarchical structure rl algorithms 
sections review proposed approaches 

approaches hierarchical reinforcement learning arti cial intelligence researchers addressed need large scale planning problem solving introducing various forms abstraction problem solving planning systems fikes 
korf 
abstraction allows system ignore details irrelevant task hand 
simplest types abstraction idea macro operator just macro sequence operators actions invoked name primitive operator action 
macros form basis hierarchical speci cations operator action sequences macros include macros de nitions macro call macros 
familiar idea subroutine call subroutines execute primitive commands 
current research hierarchical rl focuses action hierarchies follow roughly semantics hierarchies macros subroutines 
control macro open loop control policy inappropriate interesting control purposes especially control stochastic advances hierarchical reinforcement learning systems 
hierarchical approaches rl generalize macro idea closed loop policies precisely closed loop partial policies generally de ned subset state set 
partial policies de ned termination conditions 
partial policies called temporally extended actions options sutton skills thrun schwartz behaviors brooks huber grupen control theoretic modes ungar :10.1.1.25.1865
discussing speci formalism term activity suggested harel 
mdps extension adds sets admissible actions sets activities invoke activities allowing hierarchical speci cation policy 
original step actions called primitive actions may may remain admissible activity primitive action admissible state 
extensions general lines result decision processes modeled smdps waiting time state corresponds duration selected activity 
waiting time state execution activity takes steps complete initiated distribution random variable depends policies termination conditions lower level activities comprise best knowledge approaches closely related control literature varaiya pointed parr discuss brie section time aggregation approach cao 


options sutton 
formalize approach including activities rl notion option 
starting nite mdp call core mdp simplest kind option consists stationary stochastic policy termination condition input set option hi bi available state option executed actions selected option terminates stochastically example current state action probability environment transition state option terminates probability continues determining action probability 
option terminates agent select option set available termination state options sets contain termination state select primitive action 
usual assume state option continue initiated fs implies option policy needs de ned input set note action core mdp primitive action option called step option fs sg assumed option possibly step option available state 
sutton 
give example option named open hypothetical robot control system 
option consists policy reaching grasping turning door knob termination condition recognizing barto mahadevan door opened input set restricting execution open door states door reach 
option type just de ned called markov option policy markov sets action probabilities solely current state core mdp 
allow exibility especially respect hierarchical architectures include semi markov options policies set action probabilities entire history states actions rewards option initiated sutton :10.1.1.25.1865
semi markov options include options terminate pre speci ed number time steps importantly needed policies options considered policies non empty set admissible options state include step options corresponding admissible primitive actions 
policy options selects option state probability policy turn selects options terminates 
policy selected options selects options step options selected correspond actions core mdp 
see policy options determines policy core mdp sutton 
call non hierarchical policy corresponding denoted 
flat policies corresponding policies options generally markov options markov 
probability primitive action time step depends current core state plus policies options currently involved hierarchical speci cation 
dependence explicit parr dietterich discuss 
machinery precise precup de ne hierarchical options triples hi bi markov options semi markov policy options 
value functions option policies de ned terms value functions semi markov policies 
semi markov policy efr gr je event initiated time note value depend complete history onwards events earlier semi markov 
de nition policies value policy options de ned flat 
similarly de ne option value function follows efr gr je om om semi markov policy follows terminates time steps continues adding set semi markov options core nite de ned discrete time actions options rewards returns delivered course option execution 
policy option semi markov distributions de ning state state option termination advances hierarchical reinforcement learning waiting time rewards depend option executed state execution initiated 
theory algorithms applicable smdps appropriated decision making options 
effort treat options possible conventional actions sutton 
introduced interesting concept multi time model option generalizes single step model consisting js conventional action option denote event initiated state time reward part multi time model efr gr random time terminates 
state prediction part model js 
probability terminates steps initiated state probability js combination probability state terminates measure delayed outcome terms quantities js respectively generalize reward transition probabilities js usual way write generalized form bellman optimality equation 
denotes optimal value function option set max js vo reduces usual bellman optimality equation options step options 
bellman equation analogous written js max qo os system equations solved respectively exactly approximately methods generalize usual rl algorithms precup 
example analogous computing option values js max qk os barto mahadevan corresponding learning update analogous qk ak qk ak max qk os update applied termination state executing time steps return accumulated execution 
specialization results adding options mdp 
reduces conventional learning options step options 
addition precup sutton 
see precup 
precup sutton parr discussion formulation relation conventional 
case conventional mdps optimal policies options determined stochastic greedy policies 
set admissible options contains step options corresponding primitive actions core mdp clear optimal policies options optimal policies core mdp primitive actions give re ned degree control 
hand primitive actions available step options optimal policies set available options general suboptimal policies core mdp 
theory described falls conventional specialized focus semi markov options 
shortcoming internal structure option readily exploited 
precup writes apply options treated opaque indivisible units 
option selected methods require policy followed option terminates 
interesting potentially powerful methods possible looking inside options altering internal structure 
precup 
example learning update options option terminates apply non terminating options applies option time 
motivation intra option learning methods allow learning useful information option terminates multiple options simultaneously 
example variant learning called step intra option learning precup works follows 
suppose primitive action taken state immediate reward respectively 
markov option hi bi policy selected distribution update applied gu max advances hierarchical reinforcement learning estimate value state option pair arrival state case deterministic option policies example update applied options policies select option executing 
options deterministic markov option converges probability provided decay appropriately limit primitive action executed nitely state precup 
utility hierarchical rl limited due restriction markov options step intra option learning example class methods take advantage structure core mdp 
related methods developed estimating multi time models options simultaneously exploiting bellman equations relating components js multi time models successive states 
results exist interrupting option execution favor option highly valued current state adjusting option termination condition allow longest expected execution sacri cing performance 
see sutton 
precup details option related algorithms illustrations performance 
primary motivation options framework permit add activities repertoire choices available rl agent time precluding planning learning ner grain core mdp 
emphasis augmentation simpli cation core mdp 
primitive actions remain option set step options clearly space realizable policies unrestricted optimal policies options optimal policies core mdp 
nding optimal policies case takes computation conventional just solving core mdp tempted ask gains augmentation core mdp 
answer rl methods 
rl availability temporally extended activities dramatically improve agent performance learning especially initial stages learning 
invoking multi step options provides way prevent prolonged period sees rl systems 
options facilitate transfer learning related tasks 
course options facilitate learning way key question system designer decide options provide 
hand set options include step options corresponding primitive actions space policies options proper subset set policies core mdp 
case resulting smdp easier solve core mdp options simplify augment 
primary motivation abstraction rl consider sections 
current state art designer rl system typically uses prior knowledge task add speci set options set primitive actions available agent 
cases complete option policies provided cases option policies learned example intra option learning methods option speci reward functions provided designer 
providing options policies priori opportunity background knowledge task try accelerate learning provide guarantees system performance learning 
perkins barto example consider collections barto mahadevan options descends lyapunov function 
learning accelerated goal state reached learning trial agent learns reach goal quickly approximating minimum time policy options 
option policies learned usually policies ef ciently achieving subgoals subgoal state region state space reaching state region assumed facilitate achieving goal task 
canonical example useful subgoal doorway robot navigation scenario doorway passed reach goal outside room 
collection subgoals de ne subgoal speci reward functions positively reward agent achieving subgoal possibly penalizing subgoal achieved 
options de ned terminate achieving subgoal policies learned subgoal speci reward function standard rl methods 
precup discusses way introducing subgoal values dietterich approach discuss section proposes similar scheme pseudo reward functions 
natural question useful subgoals determined 
mcgovern mcgovern barto developed method automatically identifying potentially useful subgoals detecting regions agent visits frequently successful trajectories unsuccessful trajectories 
agent method selects regions appear early learning persist learning creates options achieving learns policies time learns higher level policy invokes options appropriately solve task 
experiments method suggest useful accelerating learning single tasks facilitate knowledge transfer previously discovered options reused related tasks 
approach builds previous arti cial intelligence addresses abstraction particularly iba proposed method discovering problem solving 
related ideas studied 

hierarchies machines parr parr russell developed approach hierarchically structuring called 
options formalism exploit theory smdps emphasis simplifying complex mdps restricting class realizable policies expanding action choices 
respect pointed parr common multilayer approach controlling large markov chains described varaiya considered layer structure lower level controls plant set pre de ned regulators 
higher level supervisor monitors behavior plant intervenes state enters set boundary states 
intervention takes form switching new low level regulator 
hybrid control methods branicky low level process formalized nite supervisor task nite smdp 
supervisor decisions occur plant reaches boundary state effectively erases intervening states supervisor decision problem reducing complexity varaiya 
options framework option corresponds low level advances hierarchical reinforcement learning regulator option set contain step options corresponding primitive actions simpli cation results 
extend idea allowing policies speci ed hierarchies stochastic nite state machines 
idea ham approach policies core de ned programs execute states current states core mdp 
departing somewhat parr notation nite state set action sets ham policy de ned collection stochastic nite state machines fh ig state sets stochastic transition functions input sets equal state set machine stochastic function sets initial state manner described 
types states action call choice 
action state generates action core mdp current state current state currently executing machine say thatis time step action st current state current state call state suspends execution currently executing initiates execution machine say function state called state set 
choice state nondeterministically selects state state terminates execution returns control machine called execution commences suspended 
core mdp receiving action transition state transition probabilities generates immediate reward 
action generated step remains current state 
parr de nes ham initial machine closure machine states machines reachable possible initial states initial machine 
call state set convenience assumes initial machine state nite probability loops contain action states 
ensures core mdp continues receive primitive actions 
shows simple ham state transition diagram similar example 
state transition structure simple ham parr russell 
barto mahadevan parr russell control simple simulated mobile robot 
ham runs robot reaches intersection 
obstacle encountered choice state entered allows robot decide back away obstacle calling machine back try get obstacle calling machine 
machines state transition structure possibly containing additional choice call states 
ham selected deterministically starts calling follow wall machine 
composition ham mdp described yields discrete time state set transitions determined parallel action transition functions actions choices allowed choice points states components choice states 
actions change ham component state 
choice system composition runs autonomously choice point reached 
primitive actions period fully determined action states expected immediate rewards expected returns accumulated periods choice points determined immediate rewards rewards zero time steps state change 
think ham method delineating possibly drastically restricted set policies restriction determined prior knowledge ham designer programmer ways control step corresponds main observation varaiya note determining optimal policy relevant states choice points rest erased smdp called reduce equivalent states just choice points optimal policies reduce optimal policies course close policies optimal policies depend programmer knowledge skill 
rl enter ham framework 
easy see learning applied reduce approximate optimal policies important strength rl method learning context applied reduce performing explicit reduction trajectories control ham trajectories generated simulation observed real system 
update applied choice point choice point 
detail learning system maintains table entries state ofm choice state ofh action taken corresponding choice point 
store previous choice point action selected choice point suppose choice point encountered time step choice point encountered learning update appears follows qk sc mc ac ak qk sc mc ac ak rt grt rt max qk advances hierarchical reinforcement learning max taken actions available choice point 
clearly choice return gr accumulated iteratively visits choice points 
learning convergence conditions force section sequence action values functions generated algorithm converges reduce probability 
consequently sequence policies greedy respect successive action value functions converges probability optimal policy 
know large scale applications time parr parr russell illustrate advantages provide simulated robot navigation tasks 
andre russell increased expressive power introducing programmable add interrupts aborts local state variables ability pass parameters 
may occur point way methods theoretically grounded stochastic optimal control expressive programming languages provide knowledge rich context rl 

maxq value function decomposition dietterich developed approach hierarchical rl called maxq value function decomposition call simply maxq 
options approach relies theory smdps 
options maxq approach rely directly reducing entire problem single smdp 
smdps created solutions learned simultaneously 
maxq approach starts decomposition core mdp set subtasks fm ng 
subtasks form hierarchy root subtask means solving solves actions taken solving consist executing primitive actions policies solve subtasks turn invoke primitive actions policies subtasks structure hierarchy summarized task graph example taxi problem dietterich illustration 
episode task consists picking transporting dropping passenger 
problem corresponding root node graph decomposed subtask get subtask going passenger location picking subtask put subtask going passenger destination dropping 
subtasks turn respectively decomposed primitive actions pickup dropoff respectively pick drop passenger subtask navigate consists navigating locations indicated parameter 
subtask parameterized shorthand multiple copies subtask value parameter 
subtask navigate decomposed primitive actions moves north south east 
subtasks primitive actions subtask decomposed called children important aspect task graph order subtask children shown arbitrary 
choice higher level controller depends policy 
barto mahadevan 
task graph taxi problem dietterich 
graph just restricts action choices level 
see dietterich details 
subtask consists components 
subtask policy select subtasks set children 
options primitive actions special cases subtasks 
assume subtask policies deterministic 
second subtask termination predicate partitions state set core set active states policy execute set termination states entered causes policy terminate 
third subtask pseudo reward function assigns reward values states function learning discuss rst describing task graph hierarchy allows value functions decomposed 
subtask hierarchical option hi ii de ned section addition pseudo reward function 
policy options corresponds subtask termination condition case assigns states termination probabilities option input set corresponds option formalism treats semi markov options maxq explicitly adds component state gives current contents pushdown stack containing names parameter values hierarchy calling subtasks subroutine handling ordinary programming languages 
time step top stack contains name subtask currently executed 
subtask policy non markov respect state set core mdp markov respect augmented state set 
consequence subtask policy assign actions combination core state stack contents hierarchical decomposition subtasks task graph hierarchical policy de ned fp ng policy hierarchical value function gives value expected return pair followed stack contents value denoted 
top level value state nil indicating stack empty 
value policy induced hierarchy subtask calls starting root hierarchy 
projected value function advances hierarchical reinforcement learning hierarchical policy subtask mi gives expected return state assumption pi executed terminates 
projected value denoted vp 
value function hierarchical option corresponding mi de ned section 
hierarchical decomposition hierarchical policy subtask mi de nes discrete time state set si 
actions child subtasks ma transition probabilities pi tjs cf 
equations de ned policies lower level subtasks 
key observation follows singh smdp expected immediate reward ri executing action subtask projected value subtask ma 
si child subtasks ma mi ri vp 
see true suppose core state subtask mi selects child subtasks ma execution 
expanding ma policy primitive actions results policy accumulates rewards core ma terminates state ta 
waiting time action chosen execution time ma policy 
reward accumulated waiting time vp 
write bellman equation subtask mi pi tjs pi vp expected return completing subtask mi starting state cf 
equation 
action value function de ned extended apply subtasks hierarchical policy qp expected return action primitive action child subtask executed subtask mi followed mi terminates 
terms subtask action value function observation expressed takes form tjs dietterich calls second term right equation completion function tjs gives expected return completing subtask subtask terminates 
rewriting completion function equations provide recursive way write value state barto mahadevan hierarchical policy 
result maxq hierarchical value function decomposition 
hierarchical policy state core mdp suppose policy toplevel subtask selects subtask subtask policy selects subtask policy turn selects subtask nally subtask policy selects primitive action executed core mdp 
projected value root subtask value core mdp written follows js js 
decomposition basis learning algorithm able learn hierarchical policies sample trajectories 
details algorithm somewhat complex scope review describe basic idea 
algorithm recursively applied form learning takes advantage maxq value function decomposition update estimates subtask completion functions 
simplest version algorithm applies pseudo reward functions subtasks identically zero 
key component algorithm function called maxq special case calls recursively descend hierarchy nally execute primitive actions 
returns call updates completion function corresponding appropriate subtask discounting determined returned count number primitive actions executed 
details see dietterich 
agent follows policy glie section constrained break ties order step size parameter converges zero usual stochastic approximation conditions algorithm sketched converges probability unique recursively optimal policy consistent task graph 
recursively optimal policy hierarchical policy fp ng subtask optimal smdp corresponding policies children subtasks 
form optimal policy stands contrast hierarchically optimal policy hierarchical policy optimal policies expressed constraints imposed hierarchical structure 
examples policies recursively optimal hierarchically optimal easy construct 
hierarchical optimality subtask generally depends subtask children subtask participates higher level subtasks 
example hierarchically optimal way travel destination may depend intend arriving addition properties trip 
rl methods applied mdps options policies xed priori yield hierarchically optimal policies relatively easy de ne learning algorithm maxq hierarchies produces hierarchically optimal policies 
dietterich interest weaker recursive optimality stems fact form optimality determined considering context subtask 
facilitates subtask policies building blocks tasks advances hierarchical reinforcement learning implications state abstraction touch aspect review 
pseudo reward functions 
functions allow system designer maxq framework specify subtasks de ning subgoals achieve specifying policies achieving 
respect play role auxiliary reward functions options framework learning option policies 
maxq learning algorithm sketched extended algorithm called maxq learns policy recursively optimal hierarchical respect sum original reward function pseudo reward functions 
able justice maxq approach associated algorithms short review 
provide basis seeing plays central role approach rl 
maxq excellent example concepts programming languages fruitfully integrated stochastic optimal control framework 

advances hierarchical rl far surveyed different approaches hierarchical rl underlying framework smdps 
approaches collectively suffer key limitations policies restricted sequential combinations activities agents assumed act environment nally states considered fully accessible 
section address assumptions describe smdp framework extended concurrent activities multiagent domains partially observable states 
unfortunately space permit complete descriptions extensions provide citations literature details 

concurrent activities summarize mahadevan press general framework modeling concurrent activities 
framework motivated situations single agent execute multiple parallel processes multiagent case agents act parallel addressed section 
managing concurrent activities clearly central component everyday behavior making breakfast interleave making toast coffee activities getting milk driving search road signs controlling wheel accelerator brakes 
building described approach focuses modeling concurrent activities component activity temporally extended 
immediate question arises modeling concurrent activities termination purely sequential case multiple activities executed simultaneously concurrently executing activities generally terminate time 
de ne termination set concurrent activities 
termination barto mahadevan schemes studied mahadevan 
scheme terminates activities rst activity terminates scheme waits existing activities terminate choosing new concurrent set activities 
researchers studied schemes continue replaces terminated activity set new activities includes activities executing 
simplicity restrict discussion rst schemes 
concreteness describe concurrent activity model options formalism described section 
treatment restricted options discrete time smdps having deterministic policies main ideas extend readily variants maxq continuous time smdps 
see mahadevan treatment hierarchical rl continuoustime smdps 
sequential option model generalised multi option set options executed parallel 
discuss simple case options comprising multi option uence different sets state variables interact 
assumption generalized simplicity restrict treatment 
example turning radio pressing brake executed parallel affect different non interacting state variables 
de ne multi option denoted precisely 
hi bi option de ned section core state set range state variable suppose option executing uences subset state variables fs ng 
assume executing variables uence uenced variables set options fo mg de ned core mdp coherent oi oj ensures option affect different components state set subset executed parallel interfering 
words coherent set options executing system admits parallel decomposition component corresponding state variables uenced options 
driving example brake options comprise coherent set turn right accelerate options state variable position uenced 
multi option mdp coherent set options multi option executed state options initiated 
option terminate random time oi de ne termination events max oi options terminate multi option declared terminated min oi rst options terminate options executing time interrupted 
result 
nite collection multi options de ned underlying options markov decision process selects multi options executes termination termination conditions discrete time smdp 
proof requires showing state transition probabilities rewards corresponding concurrent option de nes smdp mahadevan 
signi cance result learning methods extended learn policies concurrent options model 
advances hierarchical reinforcement learning extended learning algorithm learning policies multi options updates multi option value function decision epoch multi option taken state terminates state termination condition qk ak qk max qk os denotes number time steps initiation multi option state termination state denotes cumulative discounted reward period 
learning rule generalizes multi options 
straightforward generalize learning algorithms intra option learning algorithm multi options 
simple illustrative example concurrent learning algorithm mahadevan considered environment consisting rooms divided discrete set cells 
room contains doors opened keys 
agent options getting door interior room state opening locked door 
learn shortest path goal concurrently combining options 
agent reach goal quickly learns parallelize option retrieving key reaches locked door 
retrieving key early 
illustrative problem concurrent options 
barto mahadevan 
multi option comparison 
graph compares multi option learning system different termination schemes learns sequential policies 
policies multi options easily outperform sequential policies termination large difference convergence rate quality learned policy 
counterproductive dropped probability 
process retrieving key modeled linear chain states model waiting process agent holding key 
compares policies learned strictly sequential options described section learning algorithm policies multi options learned extended learning rule 
vertical axis plots median steps goal trial 
note sequential solution slowest agent steps reach goal 
termination condition faster sequential case steps reach goal 
signi cantly slower continue policies reach goal half time 
converges slowest terminates multi options fairly aggressively resulting decision epochs 
continue provides best tradeoff speed convergence optimality learned policy 
multi option formalism extended allow cases options executing parallel modify shared variables time 
details extension scope article 

multiagent coordination concurrency basis modeling coordination multiple simultaneously behaving agents 
theoretical point view matters little concurrent activities executed single agent multiple cooperating agents 
multiagent problem usually involves complexities fact agent usually observe actions states agents environment 
advances hierarchical reinforcement learning address issue hidden state section focus problem learning policy joint states joint actions 
general problems involving multiagent coordination modeled assuming states represent joint state agents agent may access partial view joint action represented agent may knowledge agents actions 
typically assumption multiagent studies set joint actions de nes smdp mdp joint state set 
course practical problems joint state action sets exponential number agents aim nd distributed solution require combinatorial enumeration joint states actions 
general approach learning task level coordination extend concurrency model joint state action spaces action xed option pre speci ed policy 
approach requires minimal modi cation approach described previous section 
contrast describe extension approach due 
agents learn coordination skills base level policies multiagent maxq task graph 
convergence hierarchically optimal policies longer assured lower level subtask policies varying time learning higher level policies 
ideas extended formalisms sake clarity focus maxq value function decomposition approach described section 
necessary generalize maxq decomposition original sequential single agent setting multiagent coordination problem 
denote multi option option executed agent denote joint state 
joint action value multi option joint state context doing parent task denoted 
maxq decomposition function extended joint action values follows 
joint completion function agent assigns values giving discounted return agent completing multi option context doing parent task agents performing multi options ng joint multi option value approximated agent local state sj max sj parent task non primitive sj primitive action 
rst term expansion refers discounted sum rewards received agent doing concurrent action state second term completes sum accounting rewards earned completing parent task nishing completion function updated sample values rule 
note correct action value approximated considering local state ignoring effect concurrent actions agents agent barto mahadevan 
robot trash collection task 
robots learn coordinate rapidly task structure attempted coordinate level primitive movements 
right shown maxq graph maxq task graph kinds nodes representing subtasks actions subtasks select 
reproduced 
acm reprinted permission 
performing practice human designer con gure task graph store joint concurrent action values highest level hierarchy needed 
illustrates robot trash collection task agents maximize performance task learn coordinate 
want design learning algorithms cooperative multiagent tasks weiss agents learn coordination skills trial error 
key idea coordination skills learned ef ciently agents learn synchronize hierarchical representation task structure sugawara lesser 
particular robot learning response low level primitive actions robots instance goes forward learn high level coordination knowledge utility picking trash ifa picking bin 
proposed approach differs signi cantly previous multiagent reinforcement learning littman tan hierarchical task structure accelerate learning concurrent activities :10.1.1.55.8066:10.1.1.135.717
illustrate decomposition learning multiagent coordination robot trash collection task joint action values restricted highest level task graph root get value function decomposition agent root root represents value agent doing task context root task agent doing task 
note value decomposed value advances hierarchical reinforcement learning agent doing subtask completion sum remainder task done agents 
example multiagent maxq decomposition embodies constraint value navigating trash bin independent doing 

hierarchical memory multiagent environments agents observe joint states joint actions act estimates hidden variables problem hidden state occurs single agent tasks robot navigating indoor ce environment 
approach formalized terms partially observable markov decision processes pomdps agents learn policies belief states probability distributions underlying state set kaelbling :10.1.1.107.9127
shown belief states satisfy markov property consequently yield new complex information states 
belief states recursively updated transition model observation model specifying likelihood observing action performed resulted state mapping belief states optimal actions known intractable particularly decentralized multiagent formulation bernstein 
learning perfect model underlying pomdp challenging task 
empirically effective theoretically powerful approach nite memory models linear chains nonlinear trees histories mccallum 
nite memory structures defeated long sequences irrelevant observations actions conceal critical past observation 
brie summarize multiscale memory models explored hernandez mahadevan theocharous 
jonsson barto 
models combine temporal abstraction previous methods dealing hidden state 
hierarchical suf memory hsm hernandez mahadevan generalizes suf tree model mccallum smdp temporally extended activities 
suf memory constructs state estimators nite chains observation action reward triples 
addition extending suf models temporally extended activities hsm uses multiple layers temporal abstraction form longer term memories levels 
illustrates idea robot navigation simpler case linear chain tree model investigated 
important side effect agent look back steps back time ignoring exact sequence low level observations actions 
tests robot navigation domain showed hsm outperformed suf tree methods hierarchical methods memory hernandez mahadevan 
pomdps theoretically powerful nite memory models past pomdps studied models learning planning algorithms scale poorly model size 
theocharous 
developed hierarchical pomdp formalism termed pomdps extending hierarchical hidden markov model hhmm fine include rewards temporally extended activities :10.1.1.19.6143
barto mahadevan 
hierarchical suf memory state estimator robot navigation task 
navigation level observations decisions occur intersections 
lower corridor traversal level observations decisions occur corridor 
level agent constructs state representations past experience similar history shown shadows 
reproduced hernandez mahadevan mit press reprinted permission 

hierarchical pomdp model 
hierarchical pomdp model represents adjacent corridors robot navigation task 
model primitive actions go left indicated dotted arrows indicated dashed arrows 
hpomdp unobservable states state entry exit states 
hidden product states associated observation models 
reproduced theocharous georgios theocharous 
reprinted permission 
advances hierarchical reinforcement learning developed hierarchical em algorithm learning parameters pomdp model sequences observations actions 
extensive tests robot navigation domain show learning planning performance improved pomdp models theocharous theocharous mahadevan 
hierarchical em parameter estimation algorithm scales gracefully large models previously learned sub models reused learning higher levels 
addition effect temporally extended activities pomdps exit corridor modeled product level states supports planning multiple levels abstraction 
pomdps inherent advantage allowing belief states computed different levels tree 
addition uncertainty higher levels robot sure corridor exactly corridor 
number heuristics mapping belief states actions provide performance robot navigation state mls heuristic assumes agent state corresponding peak belief state distribution koenig simmons shatkay kaelbling nourbakhsh 
heuristics better pomdps applied multiple levels belief states states usually lower entropy 
detailed study pomdp model application robot navigation see theocharous 
jonsson barto addressed partial observability adapting suf tree methods hierarchical rl systems 
approach focused automating process constructing activity speci state representations applying mccallum tree algorithm mccallum individual options 
tree algorithm employs concept suf tree automatically construct state representation starting distinctions different observation vectors 
speci cation 
entropy sample robot navigation run 
graph shows sample robot navigation run trace right positional uncertainty measured belief state entropy corridor level product state level 
spatiotemporal abstraction reduces uncertainty requires frequent decision making allowing robot get goals positional information 
reproduced theocharous mahadevan ieee 
reprinted permission 
barto mahadevan state feature dependencies necessary prior learning 
separate tree assigned option possible perform state abstraction separately option 
tree algorithm retains history transition instances ht ti composed observation vector time step previous action reward received transition previous instance 
decision tree tree sorts new instance components assigns unique leaf tree 
distinctions associated leaf determined root leaf path 
leaf action pair algorithm keeps action value estimating discounted reward associated executing action values updated variety ways system model available learned rl algorithms learning 
tree algorithm periodically adds new distinctions tree form temporary nodes called fringe nodes performs statistical tests see added distinctions increase predictive power tree 
tree extended new distinctions estimated increase tree predictive power 
tree extended action values previous leaf node passed new leaf nodes 
distinction perceptual dimension observation previous action history index indicating far back current history dimension examined 
jonsson barto adapted tree algorithm options hierarchical learning architectures 
nite set options policies de ned assigned option separate tree updated tree algorithm modi cations option local history 
tree algorithm suitable performing option speci state abstraction tree simultaneously de nes state representation policy representation 
assigning tree option algorithm able perform state abstraction separately option modifying policy 
possible intra option learning methods section option learned behavior options 
version tree algorithm illustrated dietterich taxi task brie described section 
results show starting distinctions state representation states represented single block algorithm able solve task introducing distinctions partitioned state action set subsets requiring different action values signi cant savings approach initially distinguished possible states 
example merely illustrative suggests automated option speci state abstraction attractive approach making hierarchical learning systems powerful 

topics research 
compact representations interesting real world tasks states signi cant internal structure 
example states represented vectors state variables usually called factored states advances hierarchical reinforcement learning machine learning researchers possess richer relational structure driessens dzeroski 
arti cial intelligence focused exploiting structure develop compact representations single step actions dynamic bayes net representation dean kanazawa 
natural question consider extend single step compact models compact models activities options 
problem bit subtle actions limited single step uence state variables property generally hold extended activity 
approach mahadevan studying exploit results approximation structured stochastic processes boyen koller develop structured ways approximating state predictions temporally extended activities :10.1.1.119.6111
key idea clustering state variables disjoint subsets keeping track state distribution local cluster possible ef ciently approximate underlying state distribution temporally extended activity 
preliminary analysis approach appears promising theoretical experimental study way 

learning task hierarchies approaches discussed components hierarchy places hierarchy abstractions decided advance 
key open question form task hierarchies automatically maxq framework 
brie discussed section automated methods identifying useful mcgovern address aspects problem 
approach called hexq proposed 
exploits factored state representation sorts state variables ordered list variable changes rapidly 
hexq builds task hierarchy consisting level state variable level result partitioning states represented values variable simpler mdps connected set exit states transitions occur unpredictable projected level state variable 
limitation hexq limited considering state variable isolation approach fails complex problems 
required understanding build task hierarchies cases integrate approach related systems approaches singular perturbation methods 

dynamic abstraction systems outlined article naturally provide opportunities different state representations depending activity currently executing 
crucial distinction static abstractions remain xed phases sequential decision task call dynamic abstractions conditional execution particular temporally extended activities 
words variables dynamic abstraction renders relevant irrelevant afforded barto mahadevan status temporally con ned segment time 
example course driving steering wheel angle gear position radio status may relevant irrelevant activities 
dietterich introduced state abstraction maxq framework jonsson barto explored automatic methods constructing representation experience described section 
ability dynamic abstraction rl key reasons hierarchical rl approaches discussed article appear attractive machine learning researchers 
area research signi cant impact 

large applications proposed ideas hierarchical rl described appear promising date cient experience experimentally testing effectiveness ideas large applications 

extended maxq approach multiagent domains applied large multi vehicle autonomous guided vehicle agv routing problem 
demonstrated policies learned problem better standard heuristics industry go nearest free machine heuristic 
stone sutton applied framework options keep away task robot soccer 
task involves set players team passing ball keeping ball possession defending opponents 
initial studies promising necessary establish effectiveness hierarchical rl particularly large complex continuous control tasks 

goal article review closely related approaches temporal abstraction hierarchical control developed machine learning researchers options formalism sutton 
hierarchies machines approach parr russell dietterich maxq framework 
discussed extensions ideas addressing concurrent activities multiagent coordination hierarchical memory partial observability 
attempt exhaustive review machine learning research topics closely related similar approaches systems control engineering hierarchical hybrid multilayer control attempt provide careful areas machine learning researchers developing 
strongly believe gained sides hope article prove useful stimulating needed dialog 
advances hierarchical reinforcement learning acknowledgments authors anders jonsson mohammad reading early draft article 
research funded nsf knowledge distributed intelligence awards ecs andrew barto ecs sridhar mahadevan darpa mars dabt sridhar mahadevan 
opinions ndings recommendations expressed material authors necessarily re ect views national science foundation 
notes 
follow sutton barto denoting reward action stage usual 
parr restricts ham call graph tree call stack contents need treated part program state point gloss discussion 
kind machine hierarchy instance transition network discussed woods 

multiagent problems treat multi option tuple set elements associated speci agents 
possible generalize oi multi option 
andre russell 
programmable reinforcement learning agents 
advances neural information processing systems proceedings conference 
cambridge ma mit press pp 

barto bradtke singh 
learning act real time dynamic programming 
arti cial intelligence 
bernstein zilberstein immerman 
complexity decentralized control markov decision processes 
boutilier goldszmidt eds uncertainty arti cial intelligence proceedings th conference 
san francisco ca morgan kaufmann pp 

bertsekas 
dynamic programming deterministic stochastic models 
englewood cliffs nj prentice hall 
bertsekas tsitsiklis 
neuro dynamic programming 
belmont ma athena scienti boyen koller 
tractable inference complex stochastic processes 
cooper moral eds proceedings fourteenth conference uncertainty ai san francisco ca morgan kaufmann pp 

bradtke duff 
reinforcement learning methods continuous time markov decision problems 
tesauro touretzky leen eds advances neural information processing systems proceedings conference 
cambridge ma mit press pp 

branicky borkar mitter 
uni ed framework hybrid control model optimal control theory 
ieee transactions automatic control 
brooks 
achieving arti cial intelligence building robots 
technical report memo cambridge ma massachusetts institute technology arti cial intelligence laboratory 
cao ren fu marcus 
time aggregation approach markov decision processes 
automatica 
crites 
large scale dynamic optimization teams reinforcement learning agents 
ph thesis amherst ma university massachusetts 
barto mahadevan crites barto 
elevator group control multiple reinforcement learning agents 
machine learning 
das mahadevan 
solving semi markov decision problems average reward reinforcement learning 
management science 
dean kanazawa 
model reasoning persistence causation 
computational intelligence 
dietterich 
hierarchical reinforcement learning maxq value function decomposition 
journal arti cial intelligence research 

emergent hierarchical control structures learning reactive hierarchical relationships reinforcement environments 
meas mataric eds animals animats fourth conference simulation adaptive behavior 
cambridge ma mit press 

learning hierarchical control structure multiple tasks changing environments 
animals animats fifth conference simulation adaptive behavior 
cambridge ma mit press 
driessens dzeroski 
integrating experimentation guidance relational reinforcement learning 
learning proceedings nineteenth international conference machine learning 
san francisco ca morgan kaufmann pp 

fikes hart nilsson 
learning executing generalized robot plans 
arti cial intelligence 
fine singer tishby 
hierarchical hidden markov model analysis applications 
machine learning july 
varaiya 
multilayer control large markov chains 
ieee transactions automatic control ac 
mahadevan 
continuous time hierarchical reinforcement learning 
proceedings eighteenth international conference machine learning 
san francisco ca morgan kaufmann pp 

mahadevan 
hierarchically optimal average reward reinforcement learning 
sammut goldszmidt eds proceedings nineteenth international conference machine learning 
san francisco ca morgan kaufmann pp 

ungar 
localizing search reinforcement learning 
proceedings th national conference arti cial intelligence aaai pp 

harel 
statecharts complex systems 
science computer programming 

discovering hierarchy reinforcement learning hexq 
learning proceedings nineteenth international conference machine learning 
san francisco ca morgan kaufmann pp 

hernandez mahadevan 
hierarchical memory reinforcement learning 
advances neural information processing systems proceedings conference 
cambridge ma mit press pp 

howard 
dynamic probabilistic systems semi markov decision processes 
new york wiley 
huber grupen 
feedback control structure line learning tasks 
robotics autonomous systems 
iba 
heuristic approach discovery macro operators 
machine learning 
jaakkola jordan singh 
convergence stochastic iterative dynamic programming algorithms 
neural computation 
jonsson barto 
automated state abstraction options tree algorithm 
advances neural information processing systems proceedings conference 
cambridge ma mit press pp 

kaelbling littman cassandra 
planning acting partially observable stochastic domains 
arti cial intelligence 
kaelbling littman moore 
reinforcement learning survey 
journal arti cial intelligence research 
advances hierarchical reinforcement learning klopf 
brain function adaptive systems theory 
technical report bedford ma air force cambridge research laboratories 
summary appears proceedings international conference systems man cybernetics ieee systems man cybernetics society dallas tx 
klopf 
neuron theory memory learning intelligence 
washington hemisphere 
koenig simmons 
xavier robot navigation architecture partially observable markov decision process models 
kortenkamp bonasso murphy eds ai mobile robots successful robot systems 
cambridge ma mit press 
khalil reilly 
singular perturbation methods control analysis design 
london academic press 
korf 
learning solve problems searching macro operators 
boston ma pitman 
littman 
markov games framework multi agent reinforcement learning 
proceedings eleventh international conference machine learning 
san francisco ca morgan kaufmann pp 

mahadevan 
average reward reinforcement learning foundations algorithms empirical results 
machine learning 
mahadevan das 
self improving factory simulation continuous time average reward reinforcement learning 
proceedings fourteenth international conference 
san francisco ca morgan kaufmann pp 

mahadevan 
hierarchical multi agent reinforcement learning 
mu ller andre sen eds proceedings fifth international conference autonomous agents 
new york acm press pp 

mccallum 
reinforcement learning selective hidden state 
ph thesis university rochester 
mcgovern 
autonomous discovery temporal abstractions interaction environment 
ph thesis university massachusetts 
mcgovern barto 
automatic discovery subgoals reinforcement learning diverse density 
proceedings eighteenth international conference machine learning 
san francisco ca morgan kaufmann pp 

minsky 
theory neural analog reinforcement systems application brain model problem 
ph thesis princeton university 

singular perturbation methodology control systems 
london peter nourbakhsh powers birch eld 
ce navigation robot 
ai magazine 
parr 
hierarchical control learning markov decision processes 
phd thesis berkeley ca university california 
parr russell 
reinforcement learning hierarchies machines 
advances neural information processing systems proceedings conference 
cambridge ma mit press 
perkins barto 
lyapunov constrained action sets reinforcement learning 
brodley eds proceedings eighteenth international conference machine learning 
san francisco ca morgan kaufmann pp 

perkins barto 
lyapunov design safe reinforcement learning 
journal machine learning research 
precup 
temporal abstraction reinforcement learning 
ph thesis amherst ma university massachusetts 
precup sutton 
multi time models temporally planning 
advances neural information processing systems proceedings conference 
cambridge ma mit press pp 

precup sutton singh 
theoretical results reinforcement learning temporally options 
proceedings th european conference machine learning ecml 
springer verlag pp 

puterman 
markov decision problems 
wiley ny 
barto mahadevan mahadevan learning take concurrent actions 
advances neural information processing systems proceedings conference 
cambridge mit press 
press 
mahadevan structured approximation stochastic temporally extended actions 
preparation 
mahadevan 
decision theoretic planning concurrent temporally extended actions 
proceedings seventeenth conference uncertainty arti cial intelligence 
ross 
stochastic dynamic programming 
new york academic press 
rummery niranjan 
line learning connectionist systems 
technical report cued infeng tr cambridge university engineering department 
samuel 
studies machine learning game checkers 
ibm journal research development 
reprinted feigenbaum feldman eds computers thought 
new york mcgraw hill pp 

samuel 
studies machine learning game checkers 
ii progress 
ibm journal research development 
schwartz 
reinforcement learning method maximizing undiscounted rewards 
proceedings tenth international conference machine learning 
morgan kaufmann pp 

shatkay kaelbling 
learning topological maps weak local odometric information 
ijcai 
singh bertsekas 
reinforcement learning dynamic channel allocation cellular telephone systems 
advances neural information processing systems proceedings conference 
cambridge ma mit press pp 

singh jaakkola littman ri 
convergence results single step policy reinforcement learning algorithms 
machine learning 
singh 
reinforcement learning hierarchy models 
proceedings tenth national conference arti cial intelligence 
menlo park ca aaai press mit press pp 

singh 
scaling reinforcement learning algorithms learning variable temporal resolution models 
proceedings ninth international machine learning conference 
san mateo ca morgan kaufmann pp 

stone sutton 
scaling reinforcement learning robocup soccer 
brodley eds proceedings eighteenth international conference machine learning 
san francisco ca morgan kaufmann pp 

sugawara lesser 
learning improve coordinated actions cooperative distributed problemsolving environments 
machine learning 
sutton 
generalization reinforcement learning successful examples sparse coarse coding 
touretzky mozer hasselmo eds advances neural information processing systems proceedings conference 
cambridge ma mit press pp 

sutton barto 
modern theory adaptive networks expectation prediction 
psychological review 
sutton barto 
reinforcement learning 
cambridge ma mit press 
sutton precup singh 
mdps semi mdps framework temporal abstraction reinforcement learning 
arti cial intelligence 
tan 
multi agent reinforcement learning independent vs cooperative agents 
proceedings tenth international conference machine learning 
san francisco ca morgan kaufmann pp 

tesauro 
practical issues temporal difference learning 
machine learning 
tesauro 
td gammon self teaching backgammon program achieves master level play 
neural computation 
theocharous 
hierarchical learning planning partially observable markov decision processes 
ph thesis michigan state university 
theocharous mahadevan 
approximate planning hierarchical partially observable markov decision robot navigation 
proceedings ieee international conference robotics automation icra 
theocharous mahadevan 
learning hierarchical partially observable markov advances hierarchical reinforcement learning decision robot navigation 
proceedings ieee international conference robotics automation icra 
thrun schwartz 
finding structure reinforcement learning 
tesauro touretzky leen eds advances neural information processing systems proceedings conference 
cambridge ma mit press pp 

tsitsiklis van roy 
analysis temporal difference learning function approximation 
ieee transactions automatic control 
watkins 
learning delayed rewards 
ph thesis cambridge england cambridge university 
watkins dayan 
learning 
machine learning 
weiss 
multiagent systems modern approach distributed arti cial intelligence 
cambridge ma mit press 
werbos 
advanced forecasting methods global crisis warning models intelligence 
general systems yearbook 
werbos 
building understanding adaptive systems statistical numerical approach factory automation brain research 
ieee transactions systems man cybernetics 
werbos 
approximate dynamic programming real time control neural modeling 
white eds handbook intelligent control neural fuzzy adaptive approaches 
new york van nostrand reinhold pp 

woods 
transition network grammars natural language analysis 
communications acm 
