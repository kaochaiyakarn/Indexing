ieee transactions pattern analysis machine intelligence vol 
march unsupervised feature selection feature similarity mitra student member ieee murthy sankar pal fellow ieee article describe unsupervised feature selection algorithm suitable data sets large dimension size 
method measuring similarity features redundancy removed 
need search fast 
new feature similarity measure called maximum information compression index introduced 
algorithm generic nature capability multiscale representation data sets 
superiority algorithm terms speed performance established extensively various real life data sets different sizes dimensions 
demonstrated redundancy information loss feature selection quantified entropy measure 
index terms data mining pattern recognition dimensionality reduction feature clustering multiscale representation entropy measures 
important problem related mining large data sets dimension size selecting subset original features 
preprocessing data obtain smaller set representative features retaining optimal salient data decreases processing time leads compactness models learned better generalization 
class labels data available supervised feature selection unsupervised feature selection 
data mining applications class labels unknown indicating significance unsupervised feature selection 
conventional methods feature selection involve evaluating different feature subsets index selecting best 
index usually measures capability respective subsets classification clustering depending selection process supervised unsupervised 
problem methods applied large data sets high computational complexity involved searching 
complexity exponential terms data dimension exhaustive search 
heuristic techniques developed circumvent problem 
branch bound algorithm suggested devijver kittler obtains optimal subset exponential computations feature evaluation criterion monotonic nature 
greedy algorithms sequential forward backward search popular 
algorithms quadratic complexity perform poorly nonmonotonic indices 
cases sequential floating searches provide better results cost higher computational complexity 
beam search variants authors machine intelligence unit indian statistical institute calcutta india 
mail murthy sankar ac 
manuscript received apr revised oct accepted oct 
recommended acceptance brodley 
information obtaining reprints article please send mail tpami computer org ieeecs log number 
ieee sequential algorithms reduce computational complexity :10.1.1.17.626
robust methods finding optimal subset arbitrary evaluation indices developed genetic algorithms gas 
ga feature selection methods usually perform better heuristic search methods large medium sized data sets require considerable computation time large data sets 
attempts decrease computational time feature selection include probabilistic search methods random hill climbing schemata las vegas filter lvf approach 
comparison discussion methods real life data sets may 
interest article lies unsupervised feature selection discuss existing methods broadly classified categories 
methods category involve maximization clustering performance quantified index 
include sequential unsupervised feature selection algorithm wrapper approach expectation maximization em maximum entropy method developed neuro fuzzy approach 
category considers selection features feature dependency relevance 
principle feature carrying little additional information subsumed remaining features redundant eliminated 
various dependence measures correlation coefficients measures statistical redundancy linear dependence 
relief algorithm extensions identify statistically relevant features reported 
algorithm conditional independence uses concept markov blanket 
methods involve search require significantly high computation time large data sets 
algorithm involve search selects features hierarchically merging similar feature pairs described 
algorithm crude nature performs poorly real life data sets 
may noted principal component analysis pca performs unsupervised dimensionality reduction information content ieee transactions pattern analysis machine intelligence vol 
march features 
pca involves feature transformation obtains set transformed features subset original features 
article propose unsupervised algorithm uses feature dependency similarity redundancy reduction requiring search 
method involves partitioning original feature set distinct subsets clusters features cluster highly similar different clusters dissimilar 
single feature cluster selected constitute resulting reduced subset 
new similarity measure called maximal information compression index clustering 
comparison measures correlation square regression error 
demonstrated representation entropy quantifying redundancy set 
nature proposed clustering algorithm newly introduced feature similarity measure geared goals minimizing information loss terms second order statistics incurred process feature reduction minimizing redundancy reduced feature subset 
feature selection algorithm owes low computational complexity factors conventional algorithms search best subset requiring multiple evaluation indices involved new feature similarity measure computed time compared indices supervised unsupervised feature selection methods 
method achieves dimensionality reduction removal redundant features related feature selection compression classification 
superiority algorithm related methods branch bound algorithm sequential floating forward search sequential forward search stepwise clustering demonstrated extensively real life data large small sample sizes dimensions ranging 
comparison basis clustering classification performance redundancy reduction 
effectiveness maximal information compression index effect scale parameter studied 
organization article follows section describe measures similarity pair features 
section proposed feature selection algorithm similarity measure discuss 
section provide experimental results comparisons 
feature similarity measure section discuss criteria measuring similarity random variables linear dependency 
context propose new measure called maximal information compression index feature selection 
broadly possible approaches measuring similarity random variables 
test closeness probability distributions variables 
test run test may purpose 
tests sensitive location dispersion distributions suited purpose feature selection 
approach measure amount functional linear higher dependency variables 
benefits choosing linear dependency feature similarity measure 
known features linearly dependent data linearly separable original representation data linearly separable linearly dependent features removed 
far information content variables concerned second order statistics data important criterion mean values 
linear dependency measures discuss related amount error terms second order statistics predicting variables 
discuss existing linear dependency measures explaining proposed maximal information compression index 
correlation coefficient 
known measure similarity random variables correlation coefficient 
correlation coefficient random variables defined cov var var var denotes variance variable cov covariance variables 
completely correlated exact linear dependency exist ifx totally uncorrelated 
measure similarity variables stated measure 

linearly related 

symmetric 

constants measure invariant scaling translation variables 

measure sensitive rotation scatter diagram plane 
correlation coefficient contains desirable properties feature similarity measure properties mentioned somewhat unsuitable feature selection 
measure invariant scaling pairs variables having different variances may value similarity measure desirable variance high information content 
sensitivity rotation desirable applications 
square regression error 
measure degree linear dependency variables error predicting linear model bx 
regression coefficients obtained minimizing mean square error yi 
coefficients cov var mean square error var ify linearly related completely uncorrelated var 
measure known residual mitra unsupervised feature selection feature similarity fig 

nature errors linear regression square fit square projection fit 
variance 
amount variance unexplained linear model 
properties 
var 

linearly related 

unsymmetric 

constant measure sensitive scaling variables 
clear invariant translation variables 

measure sensitive rotation scatter diagram plane 
note measure symmetric property 
sensitive rotation property 
suggest measure linear dependency desirable properties feature selection measures 
maximal information compression index 
covariance matrix random variables define maximal information compression index smallest eigenvalue var var var var var var value zero features linearly dependent increases amount dependency decreases 
may noted measure eigenvalue direction normal principle component direction feature pair 
shown maximum information compression achieved multivariate case bivariate data projected principal component direction 
corresponding loss information reconstruction pattern terms second order statistics equal eigenvalue direction normal principal component 
amount reconstruction error committed data projected reduced case reduced dimension best possible way 
measure minimum amount information loss maximum amount information compression possible 
significance explained geometrically terms linear regression 
easily shown value equal sum squares perpendicular distances points best fit line bx obtained minimizing sum squared perpendicular distances 
coefficients best fit line cot tan cov var var nature errors best fit lines square regression principal component analysis illustrated fig 

properties 
var var 

linearly related 

symmetric 
constant measure sensitive scaling variables 
expression 
contain mean variance covariance terms invariant translation data set 
invariant rotation variables origin easily verified geometric interpretation considering property perpendicular distance point line change rotation axes 
measure possesses desirable properties symmetry property sensitivity scaling property invariance rotation property 
property variable pair reflecting amount error committed maximal information compression performed reducing variable pair single variable 
may suitably redundancy reduction 
feature selection method task feature selection involves steps partitioning original feature set number homogeneous subsets clusters selecting representative feature cluster 
partitioning features done nn principle feature similarity measures described section 
doing compute nearest features ieee transactions pattern analysis machine intelligence vol 
march feature 
feature having compact subset determined distance farthest neighbor selected neighboring features discarded 
process repeated remaining features selected discarded 
determining nearest neighbors features assign constant error threshold set equal distance kth nearest neighbor feature selected iteration 
subsequent iterations check value corresponding subset feature greater 
decrease value may varying iterations 
concept clustering features homogeneous groups varying sizes illustrated fig 

algorithm may stated follows algorithm original number features original feature set ffi dg 
represent dissimilarity features fi fj fi fj 
higher value dissimilar features 
measures linear dependency described section may computing rk represent dissimilarity feature fi kth nearestneighbor feature step choose initial value 
initialize reduced feature subset original feature set step feature fi compute rk step find feature fi rk minimum 
retain feature discard nearest features fi 
note denotes feature removing cause minimum error features 
rk 
step cardinality step go step 
cardinality 
step rk 
rk rk nearest neighbor select remaining features step go step 
step return feature set reduced feature set 
remarks computational complexity 
algorithm low computational complexity respect number features number samples original data 
respect dimension method complexity decremented kth nearest neighbor features dissimilar feature go step 
feature dissimilar 
existing search schemes sequential forward backward search complexity evaluation time consuming 
algorithms plus take sequential floating search branch bound algorithm complexity higher quadratic 
probabilistic search algorithms require quadratic number evaluations 
second factor contributes speedup achieved proposed algorithm low computational complexity evaluating linear measures feature similarity 
data set contains samples evaluation similarity measure feature pair complexity 
feature selection scheme complexity 
supervised unsupervised feature evaluation indices entropy class nn classification accuracy fig 

feature clusters 
complexity computation 
evaluation linear dependency measures involves computation dimensional variables measures involve distance computations higher dimensions 
factors contribute large speedup achieved proposed algorithm compared feature selection schemes 
notion scale feature selection choice algorithm controls size reduced set 
determines error threshold representation data different degrees details controlled choice 
useful data mining multiscale representation data necessary 
note said property may possessed algorithms input usually desired size reduced feature set 
reason changing size reduced set may necessarily result change levels details 
contrast proposed algorithm acts scale parameter controls degree details direct manner 
nonmetric nature similarity measure 
similarity measures proposed algorithm need metric 
conventional agglomerative clustering algorithms utilize metric property similarity measures 
stepwise clustering method previously feature selection clustering algorithm partitional nonhierarchical nature 
feature evaluation indices describe indices considered evaluating effectiveness selected feature subsets 
indices class nn classification accuracy naive bayes classification accuracy need class information samples remaining entropy fuzzy feature evaluation index representation entropy mitra unsupervised feature selection feature similarity 
discuss mention convenience notations number sample points data set number classes data set number features original feature set number features reduced feature set original feature space dimension transformed feature space dimension 
class separability 
class data set defined trace sw 
sw class scatter matrix sb class scatter matrix defined sw xc sb xc mo xc table comparison feature selection algorithms large dimensional data sets entropy fuzzy feature evaluation index class separability nn classification accuracy naive bayes classification accuracy sd standard deviation 
sfs sequential forward search swc stepwise clustering 
number features number features parameter proposed method 
jef jg xc mo mo priori probability pattern belongs class feature vector sample mean vector class mo sample mean vector entire data points sample covariance matrix class ef expectation operator 
lower value separability criteria ensures classes separated scatter means 

nn classification accuracy 
nn rule evaluating effectiveness reduced set classification 
cross validation performed manner randomly select percent data training set classify remaining percent points 
independent runs performed average classification accuracy test set 
value chosen nn rule square root number data points training set 

naive bayes classification accuracy 
bayes maximum likelihood classifier assuming normal distribution classes evaluating classification performance 
mean covariance classes estimated randomly selected percent training sample remaining percent points test set 
independent runs performed average classification accuracy test set provided 

entropy 
distance data points xm xp xq maxj xp denotes feature value jth direction maxj maximum minimum values computed samples jth axis number features 
similarity sim ieee transactions pattern analysis machine intelligence vol 
march positive constant 
possible value ln 
average distance data points computed entire data set 
entropy defined xl sim log sim sim log sim 
data uniformly distributed feature space entropy maximum 
data wellformed clusters uncertainty low entropy 
fuzzy feature evaluation index 
fuzzy feature evaluation index defined pq pq pq pq pq pq table comparison feature selection algorithms medium dimensional data sets bb branch bound sffs sequential floating forward search 
degrees patterns belong cluster feature spaces respectively 
membership function pq may defined pq dmax dmax distance patterns dmax maximum separation patterns respective feature spaces 
value intercluster intracluster distances increase decrease 
lower value crisp cluster structure 
note indices class nn accuracy measure effectiveness feature subsets classification indices entropy fuzzy feature evaluation index evaluate clustering performance feature subsets 
describe quantitative index measures amount redundancy reduced subset 

representation entropy 
eigenvalues covariance matrix feature set size pd mitra unsupervised feature selection feature similarity table comparison feature selection algorithms low dimensional data sets similar properties probability pd 
entropy function defined hr xd log function hr attains minimum value zero eigenvalues zero words information single coordinate direction 
eigenvalues equal information equally distributed features hr maximum uncertainty involved feature reduction 
measure known representation entropy 
property data set represented particular set features measure amount information compression possible dimensionality reduction 
equivalent amount redundancy particular representation data set 
proposed algorithm involves partitioning original feature set number homogeneous highly compressible clusters expected representation entropy individual clusters low possible final reduced set features low redundancy high value representation entropy 
may noted dimensional subspaces original dimensional data set corresponding karhunen loeve coordinates eigenvalues highest representation entropy redundant 
large dimensional data sets transform directions difficult compute 
transform results general transformed variables exact subsets original features 
experimental results comparisons organization experimental results follows data sets discussed briefly 
performance proposed algorithm terms feature evaluation indices discussed section compared feature selection schemes 
studied redundancy reduction aspect algorithm quantitatively comparisons 
effect varying parameter feature clustering studied 
ieee transactions pattern analysis machine intelligence vol 
march fig 

variation classification accuracy size reduced subset multiple features ionosphere cancer data sets 
vertical dotted line marks point results reported tables 
categories real life public domain data sets low dimensional medium dimensional high dimensional containing large relatively smaller number points 
available learning repository 
described 

isolet 
data consists spectral coefficients utterances english alphabets subjects 
features real range instances classes 

multiple features 
data set consists features handwritten numerals extracted collection dutch utility maps 
total patterns features classes 

arrhythmia 
contains samples having attributes 
attributes real valued experiments 
attributes represent parameters ecg measurements task classify patient classes cardiac arrhythmia 


task classify email spam category 
instances continuous valued attributes denoting word frequencies classes 

waveform 
consists instances having attributes 
attributes continuous valued noise 
task classify instance categories waves 

ionosphere 
data represents autocorrelation functions radar measurements 
task classify classes denoting passage obstruction ionosphere 
instances attributes continuous 

forest cover type 
gis data set representing forest region 
attributes select numeric valued attributes 
instances classes 

wisconsin cancer 
popular wisconsin breast cancer data set contains features instances classes 

iris 
data set contains instances features classes iris flowers 
comparison classification clustering performance indices entropy fuzzy feature evaluation index class nn naive bayes classification accuracy considered demonstrate efficacy proposed methodology comparing methods 
unsupervised feature selection schemes considered comparison 
branch bound algorithm bb 
search method possible subsets implicitly inspected exhaustive search 
feature selection criterion monotonic bb returns optimal subset 

sequential forward search sfs 
suboptimal search procedure feature time added current feature set 
stage feature included feature set selected remaining available features new enlarged feature set yields maximum value criterion function 

sequential floating forward search sffs 
near optimal search procedure lower computational cost bb 
performs sequential forward search provision backtracking 
mitra unsupervised feature selection feature similarity table comparison feature selection algorithms large data sets search algorithms selection criterion 
stepwise clustering correlation coefficient swc 
scheme obtains reduced subset discarding correlated features 
experiments mainly entropy feature selection criterion search algorithms 
comparisons terms indices different sizes reduced feature subsets 
tables provide comparative result corresponding high medium low dimensional data sets size reduced feature subset taken half original size example 
comparison sizes reduced feature set provided fig 
considering data set categories multiple features high ionosphere medium cancer low 
cpu time required algorithms sun ultrasparc mhz workstation reported tables 
branch bound bb sequential floating forward search sffs algorithms require high computation time large data sets provide figures table 
classification accuracies nn bayes mean standard deviations sd computed independent runs 
compared search algorithms bb sffs sfs performance proposed scheme comparable slightly superior computational time requirement proposed scheme 
hand compared similarity swc method performance proposed algorithm superior keeping time requirement comparable 
noted superiority terms computational time increases dimensionality sample size increase 
example case low dimensional data sets speedup factor proposed scheme compared bb sffs algorithms forest data low dimensional large sample size factor medium dimensional data sets bb sffs times slower sfs times slower high dimensional data sets sfs times slower bb sffs compared require high run time 
may noted unsupervised feature selection algorithms bb sffs sfs usually consider entropy selection criterion 
keeping mind detailed results provided tables 
run experiments unsupervised measure fuzzy feature evaluation index 
table shows illustration results large data sets isolet multiple features arrhythmia forest 
results corroborate findings obtained entropy 
part experiments compared performance supervised method relief widely 
percent samples design set relief algorithm 
results tables 
relief algorithm provides classification performance comparable proposed scheme class label information 
higher time requirement specially data sets large number samples forest data 
performance terms unsupervised indices poor 
statistical significance classification performance proposed method compared algorithms tested 
means sd values accuracies computed independent runs purpose 
generalized version paired test suitable means variances 
problem classical fisher problem hypothesis testing suitable test statistic described tabled ieee transactions pattern analysis machine intelligence vol 
march table representation entropy hs subsets selected algorithms table reduction different feature similarity measures average representation entropy groups hs representation entropy subset maximal information compression index square regression error correlation coefficients 
respectively 
observed proposed method significantly better performance compared swc algorithm data sets sfs algorithm data sets 
algorithms relief bb sffs performance comparable difference mean values classification scores statistically insignificant 
redundancy reduction quantitative study mentioned proposed algorithm involves partitioning original feature set certain number homogeneous groups replacing group single feature resulting reduced feature set 

test statistic form means standard deviations number observations 
representation entropy hr defined section measure redundancy homogeneous clusters final selected feature subset 
hr computed individual clusters low possible indicating high redundancy features belonging single cluster giving high value possible selected subset indicating minimum redundancy 
denote average value hr computed homogeneous groups value hr final selected subset table shows comparative results proposed method feature selection algorithms terms hs seen subset obtained proposed scheme redundant having highest hs values 
demonstrate superiority maximal information compression index compared feature similarity measures previously provide table compared hs hg values mitra unsupervised feature selection feature similarity obtained similarity measures clustering algorithm 
seen table superior information compression capability compared measures indicated lowest highest values hs respectively 
effect parameter algorithm size reduced feature subset scale details data representation controlled parameter fig 
illustrates effect data sets multiple features ionosphere cancer considering data high medium low categories 
expected size reduced subset decreases increase medium particularly large dimensional data fig 
observed certain ranges lower side change size reduced subset reduction dimension occurs 
interesting fact observed data sets considered values case small dimensional data sets high values case medium large dimensional data sets size selected subset varies linearly seen cases size reduced subset size original feature set 
discussion algorithm unsupervised feature selection feature similarity measures described 
novelty scheme compared conventional feature selection algorithms absence search process contributes high computational time requirement feature selection algorithms 
algorithm pairwise feature similarity measures fast compute 
require orders cpu time compared schemes 
approaches optimizing classification clustering performance explicitly determine set maximally independent features discarding redundant ones 
enhances applicability resulting features compression tasks forecasting summarization association mining addition classification clustering 
proposed algorithm capability multiscale representation data sets 
scale parameter feature clustering efficiently tradeoff representation accuracy feature subset size 
suitable wide variety data mining tasks involving large terms dimension size data sets 
formulating novel clustering algorithm defined feature similarity measure called maximal information compression index 
may note definition said parameter new feature subset selection framework novel 
superiority measure feature selection established experimentally 
demonstrated extensive experiments representation entropy index quantifying redundancy reduction information loss feature selection method 
article measured information loss terms second order statistics 
similarity measure fig 

variation size reduced subset parameter multiple features ionosphere cancer data 
feature selection algorithm selected defined accordingly 
may modify measures suitably case higher order statistics 
regard modifications correlation indices measure higher order polynomial dependency variables may considered 
similarity measure valid numeric features extension accomodate kinds variables symbolic categorical hybrid may investigated 
ieee transactions pattern analysis machine intelligence vol 
march fayyad uthurusamy data mining knowledge discovery databases comm 
acm vol 
pp 
nov 
devijver kittler pattern recognition statistical approach 
englewood cliffs prentice hall 
kittler floating search methods feature selection pattern recognition letters vol 
pp 

dw :10.1.1.17.626
aha bankert comparative evaluation sequential feature selection algorithms artificial intelligence statistics fisher 
lenz eds new york springer verlag 
genetic algorithms pattern recognition pal wang eds 
boca raton crc press 
kudo sklansky comparison algorithms selects features pattern classifiers pattern recognition vol 
pp 

skalak prototype feature selection sampling random mutation hill climbing algorithms proc 
th int 
machine learning conf pp 

moore lee efficient algorithms minimizing cross validation error proc 
th int 
conf 
machine learning 
liu setiono issues scalable feature selection expert systems applications vol 
pp 

dash liu unsupervised feature selection proc 
pacific asia conf 
knowledge discovery data mining pp 

dy brodley feature subset selection order identification unsupervised learning proc 
th int 
conf 
machine learning 
basu micchelli olsen maximum entropy maximum likelihood criteria feature selection multivariate data proc 
ieee int 
symp 
circuits systems pp 
iii 
pal de unsupervised feature evaluation neuro fuzzy approach ieee trans 
neural network vol 
pp 

hall correlation feature selection discrete numeric class machine learning proc 
th int 
conf 
machine learning 
redundancy feature extraction ieee trans 
computers pp 

das feature selection linear dependence measure ieee trans 
computers pp 

toussaint comments feature selection linear dependence measure ieee trans 
computers 
kira rendell practical approach feature selection proc 
ninth int 
workshop machine learning pp 

kononenko estimating attributes analysis extension relief proc 
seventh european machine learning conf pp 

koller sahami optimal feature selection proc 
th int 
conf 
machine learning pp 

king step wise clustering procedures am 
statistical assoc pp 

rao linear statistical inference applications 
john wiley 
merz univ california irvine dept information computer sciences www ics uci edu mlearn mlrepository html 
lehmann testing statistical hypotheses 
new york john wiley 
tables comparisons accuracy involves variances biometrika vol 
pp 

mitra received degree electrical engineering indian institute technology 
worked scientist institute robotics intelligent systems india 
currently senior research fellow indian statistical institute calcutta 
research interests area data mining knowledge discovery pattern recognition learning theory soft computing 
student member ieee 
murthy received phd degrees indian statistical institute isi 
visited michigan state university east lansing pennsylvania state university university park 
professor machine intelligence unit indian institute 
fields interest include pattern recognition image processing machine learning neural networks fractals genetic algorithms wavelets data mining 
received best award computer science institute engineers india 
fellow national academy engineering india 
sankar pal sm received phd degrees radio physics electronics respectively university calcutta 
received phd degree electrical engineering dic imperial college university london 
professor distinguished scientist indian statistical institute calcutta 
founding head machine intelligence unit 
worked university california berkeley university maryland college park fulbright postdoctoral visiting fellow nasa johnson space center houston texas guest investigator nrc nasa senior research program hong kong polytechnic university hong kong visiting professor 
served distinguished visitor ieee computer society usa asia pacific region 
fellow ieee third world academy sciences italy national science engineering india 
research interests include pattern recognition image processing data mining soft computing neural nets genetic algorithms fuzzy systems 
coauthor books including fuzzy mathematical approach pattern recognition john wiley new york neuro fuzzy pattern recognition methods soft computing john wiley new york published research publications 
received prize award scientist india fellowship vikram research award nasa tech brief award ieee transactions neural networks outstanding award nasa patent application award ram lal gold medal om foundation award award scientific research international award st winner republic iran medal indian national science academy award engineering technology 
professor pal associate editor ieee transactions neural networks pattern recognition letters international journal pattern recognition artificial intelligence neurocomputing applied intelligence information sciences fuzzy sets systems fundamenta informaticae 
member executive advisory editorial board ieee transactions fuzzy systems international journal image graphics international journal reasoning guest editor journals including ieee computer 
information computing topic please visit digital library computer org publications 
