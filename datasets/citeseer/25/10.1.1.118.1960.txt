advances nonlinear blind source separation christian jutten lab 
des images des avenue felix grenoble cedex france email christian jutten fr briefly review advances blind source separation bss nonlinear mixing models 
general nonlinear bss ica independent component analysis problems discuss detail uniqueness issues presenting new results 
fundamental difficulty nonlinear bss problem nonlinear ica problem nonunique extra constraints implemented suitable regularization 
post nonlinear mixtures important special case nonlinearity applied linear mixtures 
mixtures ambiguities essentially linear ica bss problems 
part various separation techniques proposed post nonlinear mixtures general nonlinear mixtures reviewed 
nonlinear ica bss problems consider samples observed data vector modeled unknown mixing matrix column vectors unknown dimensional source vector containing source signals assumed statistically independent 
general dimensionality vectors different 
usually assumed mixtures sources mixing matrix full rank sources gaussian 
ica method consists estimating matrix statistically independent observed data 
model different situations example multidimensional signal processing sensor receives unknown superimposition unknown source signals time instants 
sparse coding tries code data vectors sparse linear combination independent components 
problem goal recover unknown actual source signals rise observed mixtures 
referred blind source separation bss problem blind little prior information sources required 
second problem related hypothetical rough model tries juha karhunen neural networks research centre helsinki university technology box hut finland email juha karhunen hut fi approximate data possible suitably chosen independent components 
cases assumption independence sources ica solving problems 
proved ica bss equivalent known assumptions basic linear case understood quite 
see historical review early performing bss ica algorithms developed applied increasing number applications 
linear ica bss books 
source signals plain random variables temporal structure linear blind source separation achieved utilizing temporal correlations nonstationarity somewhat different assumptions see chapter 
basic model simple describing observed data adequately 
natural extension linear model consider nonlinear mixing models 
instantaneous mixtures nonlinear mixing model general form denote data source vectors unknown real valued component mixing function 
assume simplicity number independent components equals number mixtures 
general nonlinear ica problem consists finding mapping yields components statistically independent 
fundamental characteristic nonlinear ica problem general case solutions exist highly nonunique 
reason independent random variables functions independent 
serious problem nonlinear case mixed statistically independent see section ii 
contrary linear case bss problem general nonlinear mixtures differs greatly nonlinear ica problem defined 
respective nonlinear bss problem find original source signals generated observed data 
usually clearly meaningful unique problem nonlinear ica problem defined provided suitable prior information available sources mixing mapping 
arbitrary independent components data generated may quite different true source signals 
generally solving nonlinear bss problem easy requires additional prior information suitable regularizing constraints 
ii 
existence uniqueness nonlinear ica bss authors addressed important issues existence uniqueness solutions nonlinear ica bss problems 
main results direct consequences results factorial analysis reported section 
recall definition independent random vector 
definition random vector statistically independent joint probability density function pdf satisfies marginal pdfs random variables 
product permutation matrix diagonal mapping preserves independence insures separability 
definition mapping called trivial transforms random vector independent components random vector independent components 
set trivial transformations denoted 
trivial mappings preserve independence property random vector 
easily show mapping trivial satisfies arbitrary functions permutation 
result establishes link independence assumption objective source separation 
soon clear objective source separation global transformation trivial independence assumption 
clear sources separated permutation nonlinear function 
invertible mapping component scalar nonlinear mapping evident possible imposing additional constraints shall see subsection 
results factor analysis general case mapping particular form known statistical result shows preserving independence strong constraint ensuring sense equation 
result established early 
simple constructive method similar known gram schmidt orthogonalization procedure linear algebra decomposing random vector non trivial mapping independent variables 
hyv rinen pajunen applied idea construction parameterized families nonlinear ica solutions 
result negative sense shows exist non trivial transformations mix variables preserving statistical independence 
blind source separation simply impossible general nonlinear transformations resorting statistical independence constraints transformation model 
clearly states properties 
clarify general problem factor analysis showing great presents soon leaves wide field linear diagrams simple example give simple example mixing mappings preserving independence derived 
suppose rayleigh distributed variable pdf independent having uniform pdf consider nonlinear mapping non diagonal jacobian matrix joint pdf shows random variables independent nonlinear mixtures sources 
examples literature see example easily constructed 
specific model basic reason negative result constraints assumed transformation 
constraining transformation certain set transformations reduce great 
smooth mappings multi layer perceptron mlp networks see estimating generic nonlinear mappings 
conjectured smooth mappings providing mlp networks sufficient ensuring nonlinear ica leads nonlinear bss 
example shows smoothness sufficient separation 
loss generality consider independent random variables uniformly distributed interval nonlinear smooth mapping rotation rotation angle depends radius follows smooth mapping preserves independence jacobian transformation equal mixing jacobian matrix diagonal 
counterexample proves restricting mapping smooth sufficient 
smoothness vague explore defining sufficient conditions 
hyv rinen pajunen gave partial answer question proving unique solution rotation obtained dimensional special case mixing mapping constrained conformal mapping 
structural constraints natural way regularizing solution consists looking separating mappings belonging specific subspace 
characterize specific model solve tricky independence preservation equation written algebra denote set independent components source distributions exists non trivial mapping belonging model preserving independence components source vector 
ideally empty contain identity unique element 
general fulfilled 
say source separation possible distributions sources belong set complement 
sources restored trivial transformation belonging set 
equation denotes difference sets 
mixing system separating system fig 

mixing separating system pnl mixtures 
example linear models case regular linear models transformation linear represented square invertible matrix 
case suffices constrain separating model lie subspace invertible square matrices estimate matrix independent components 
global transform restricted subspace invertible square matrices 
set linear trivial transformations set matrices equal product permutation diagonal matrix 
theorem clear set contains distributions sources gaussian 
comon known theorem blind source separation possible gaussian source sources restored permutation diagonal matrix 
separability pnl mixtures post nonlinear pnl model nonlinear observations specific form see pnl model consists linear mixture followed componentwise nonlinearity acting output independently 
nonlinear functions distortions assumed invertible 
theoretical interest model belonging family suits perfectly lot real world applications 
instance models appear sensors array processing satellite microwave communications biological systems 
discussed important thing dealing nonlinear mixtures separability issue 
separation structure constrained invert mixing system sense eq 

simple possible reducing residual distortions result independence assumption 
constraints choice selecting separating system mirror structure mixing system see 
global transform stands linear stands zero memory nonlinearity 
restricted subspace transforms consists cascade invertible linear mixture regular matrix followed componentwise invertible distortions invertible linear mixture regular matrix 
shown mixtures separable distributions having gaussian sources set contains distributions having gaussian components linear mixtures set linear trivial transformations set matrices equal product permutation diagonal matrix nonzero entries row column 
separability pnl mixtures generalized convolutive pnl mixtures instantaneous mixtures matrix replaced linear filters matrix filters 
suitable parameterization wiener systems viewed particular pnl mixtures 
consequently separability pnl mixtures ensures blind invertibility wiener systems 
theis studied separability cascade pnl stages constituting structure similar multi layer perceptron networks 
separable nonlinear mixtures due interesting result linear mixtures clear nonlinear mixtures reduced linear mixtures simple mapping separable 
simple example example consider multiplicative mixtures positive independent sources 
logarithm yields linear model new independent random variables instance type mixtures modeling dependency temperature magnetic field hall silicon sensor gray level images product incident light reflected light 
considering detail example hall voltage equal depends semiconductor type temperature effect related mobility majority carriers 
types sensors simplifying equations drop variable 
temperature positive sign magnetic field vary logarithm leads equations equations describe linear mixture sources easily solved simple decorrelation approach appears power equations 
simpler directly compute ratio equations depends temperature separating magnetic field sufficient estimate parameter uncorrelated 
deduce multiplicative constant 
final estimation values requires sign reconstruction calibration steps 
generalization class mappings extension theorem nonlinear functions addressed 
results revisited framework bss nonlinear mixtures eriksson 
main idea consider particular mappings satisfying addition theorem sense theory functional equations 
simple example mapping consider nonlinear mixture independent random variables variable transforms nonlinear model applying transformation yields linear mixture independent variables nice result due fact mapping 
generally property hold provided exists mapping invertible function satisfying addition theorem range 
basic properties required mapping case variables extension straightforward continuous separately variables commutative associative exists identity element exists inverse element 
words denoting conditions imply set abelian group 
condition aczel proved exists monotonic continuous function clearly applying exists monotonic equation leads property define product integer extend real variables inverse denoting yields constants random variables relation holds stated theorem theorem independent random variables independent operators satisfy conditions 
denoting function defined operator gaussian theorem easily extended source separation mixtures separation algorithm consists practical steps apply nonlinear observations providing linear mixtures solve linear mixtures bss method 
restore actual independent sources applying unfortunately algorithm blind function known 
known suitable separation structure cascade identical nonlinear componentwise blocks able approximate followed linear matrix able separate sources linear mixtures 
stage followed identical nonlinear componentwise blocks approximate restoring actual sources 
blocks structure identical separation structure pnl mixtures fact slightly simpler nonlinear blocks similar 
estimate independent distorted sources pnl mixture separation algorithm 
computing nonlinear block estimates approximate restore actual sources 
pnl mixtures close mappings 
fact general nonlinear functions different unknown 
consequently algorithms developed separating sources pnl mixtures blindly separating nonlinear mappings avoiding step 
examples mappings satisfying addition theorem 
realistic mixtures belonging class unusual pnl mixtures multiplicative mixtures 
taleb jutten considered separability nonlinear mixtures 
general earlier separation impossible additional prior knowledge model independence assumption strong general nonlinear case 
prior information sources subsection show prior information sources simplify relax nonlinear mixtures 
bounded sources pnl mixtures consider sources pdf bounded support nonzero values edges support 
example uniform distribution distribution random sample sine wave satisfy condition 
simplicity discuss pnl mixtures sources results easily extended sources 
joint distribution sources contained rectangle 
linear mixing joint distribution lies inside parallelogram 
componentwise invertible nonlinear distortions joint distribution pnl mixtures contained distorted parallelogram 
zadeh jutten proved distribution contained parallelogram dcp transformed componentwise invertible mappings dcp mappings linear 
nonlinearities compensating estimated borders distorted parallelogram associated joint distribution pnl mixtures straight lines 
details algorithm experimental results 
method proves simple prior information nonlinear distortions estimated independence assumption 
words bounded sources provide useful extra information simplifying separation algorithms pnl 
time correlated sources nonlinear mixtures consider independent identically distributed random signals decomposition procedure construct new signals statistically independent underlying mapping mixing mapping denotes cumulative probability function random variable sources temporally correlated jutten proved mapping longer preserve independence 
course partial theoretical result give proof separability nonlinear mixtures temporally correlated sources shows fairly weak prior information sources reduce typical ica encountered nonlinear mixtures 
iii 
separation methods post nonlinear mixtures minimization mutual information consider bss methods proposed simpler case post nonlinear mixtures 
taleb jutten studied case papers start brief discussion results 
short overview studies main results represented 
separation algorithm post nonlinear mixtures generally consists subsequent parts stages nonlinear stage cancel nonlinear distortions 
part consists nonlinear functions linear stage separates approximately linear mixtures obtained nonlinear stage 
done usual learning separating matrix components output vector separating system statistically independent independent possible 
taleb jutten mutual information components output vector cost function independence criterion stages 
linear part minimization mutual information leads estimation equations linear mixtures components vector score functions components output vector pdf derivative 
practice natural gradient algorithm providing equivariant performance depend mixing matrix provided noise 
nonlinear stage derive estimating equations gradient learning rule th component observation vector element separating matrix derivative th nonlinear function 
exact computation algorithm depends naturally specific parametric form nonlinear mapping multilayer perceptron network modeling functions 
contrary bss linear mixtures separation performance nonlinear mixtures strongly related estimation accuracy score functions 
score functions estimated adaptively output vector 
alternative ways considered 
approach estimate pdf compute differentiation score function 
pdf estimation gram charlier expansion performs appropriately mild post nonlinear distortions 
hard nonlinearities simple pdf estimation kernel methods preferable 
second method estimates score functions directly provides results hard nonlinearities 
performing batch type method estimating score functions introduced 
methods post nonlinear mixtures authors studied methods blind separation post nonlinear mixtures starting different viewpoints 
early method proposed lee koehler extension natural gradient algorithm uses parametric sigmoidal nonlinearities flexible higher order polynomials 
pham jutten proposed parameterization mutual information criterion involving derivatives nonlinearities 
derivatives parameterized piecewise constant functions algorithms simple 
approach uses alternating conditional expectation ace method nonparametric statistics approximate inversion post nonlinearities 
bss method called temporal decorrelation introduced earlier authors see recovering source signals 
independently sol improved method directly computing estimating inverse see nonlinear mapping formula cumulative distribution function random variable cumulative gaussian distribution 
peng chi siu introduced semi parametric hybrid neural network model mlp network separating post nonlinear mixtures 
main advantage method able consider cross channel post nonlinearities experimental results separation sources 
similar generalized post nonlinear mixture model addressed adaptive spline neural networks 
proposed combining geometric approach neural network learning separating special class post nonlinear mixtures assumed powers linear mixtures source signals 
geometrical approach separating bounded sources pnl mixtures proposed zadeh jutten 
extension pnl mixtures wiener system consists cascade linear filter followed memoryless nonlinearity input independent identically distributed signal output suitably chosen parameterization taleb sol jutten proved wiener systems expressed pnl mixtures proposed non parametric parametric algorithms minimization mutual information rate 
similar problem appearing satellite communications solved monte carlo markov chain mcmc simulation methods 
convolutive post nonlinear mixtures introduced zadeh jutten account propagation commonplace realistic situations 
observation vector separation algorithms generalization mutual information minimization random processes proposed 
iv 
separation methods general nonlinear mixtures variational bayesian methods advanced bayesian inference methods increasingly popular neural networks statistical signal processing obtain excellent results provided assumed model correct type 
allow utilization available prior information modeling suitable prior distributions fully bayesian treatment possible select optimal model order making methods robust overfitting 
main disadvantages fully bayesian estimation methods quite high computational load intractable computations approximations 
obstacles prevented application realistic unsupervised blind learning problems number unknown parameters estimated grows easily large 
variational bayesian learning called bayesian ensemble learning utilizes approximation fitted posterior distribution parameter estimated 
approximative distribution chosen gaussian simplicity computational efficiency 
mean gaussian distribution provides point estimate unknown parameter considered variance gives somewhat crude useful measure reliability point estimate 
approximative posterior distribution fitted posterior distribution estimated data kullback leibler information divergence 
measures difference probability densities sensitive mass distributions peak value resulting robust estimates 
variational bayesian methods applied standard linear ica bss research groups bayesian approaches handle various blind problems linear models see 
valpola earlier authors introduced methods bayesian ensemble learning blind estimation separation nonlinear mixture data models 
methods nonlinear mapping modeled multilayer perceptron mlp network nonlinear hidden layer data model contains additive noise 
necessary regularization nonlinear bss achieved choosing model sources probably generated observed data 
assuming source signals input layer mlp network simple gaussian distributions obtains nonlinear principal component analysis pca solution called nonlinear factor analysis nfa 
nfa solution usually model quite nonlinear mixtures observed data provide estimates independent source signals sources plain gaussian distributions nfa method 
simplest way achieve nonlinear bss apply standard linear ica nfa solution 
quality nonlinear bss solution improved somewhat continuing bayesian ensemble learning mixture gaussians model sources 
known suitable mixtures gaussian distributions able model sufficient accuracy source distributions 
method called nonlinear independent factor analysis 
nfa methods introduced principled theoretical derivation 
experimental results artificially generated data showing nfa method followed linear ica method able approximate pretty true sources 
methods applied real world data sets including dimensional pulp data speech data interpretation results somewhat difficult requiring problem specific expertise 
somewhat nfa method extended include nonlinear dynamic model sources 
developed nonlinear dynamic factor analysis method thoroughly results obtained far summarized 
matlab codes nfa methods available www site 
specifically data model method equation unknown nonlinear function controls dynamics sources similar additive noise term static nonlinear data model 
similarly nfa method function modeled mlp network unknown mappings sources learned bayesian ensemble learning 
model discussed detail 
real world data sets appropriately described nonlinear dynamic systems nonlinear bss dynamical systems may fact practical applications static nonlinear bss 
nonlinear dynamical ica knowledge authors considered state space models hyper radial basis function rbf network nonlinear mixtures 
method introduced completely blind partly resorts supervised learning 
experiments difficult chaotic data method performed outperforming example prediction results nonlinear autoregressive modeling learned standard back propagation see chapter order magnitude 
method applied bss biomedical meg data provided clearly better results standard linear ica 
application static data model standard linear ica model dynamic model nonlinear 
method blind estimation dynamic system source signals detection changes states sources process 
method performed better compared state art techniques change detection 
problem particular method nfa methods computational load high problems realistic size spite efficient gaussian approximation 
problem bayesian ensemble learning procedure may get stuck local minimum requires careful initialization 
combat problems simpler block approach neglects posterior dependencies developed 
block approach allows straightforward construction bayesian ensemble learning variety models computationally clearly efficient robust local minima 
learning variance sources linear nonlinear models tested nonlinear bss 
results artificial data slightly worse nfa method followed linear ica method preliminary experiments real world speech data quite encouraging 
occasionally approximation block method neglects posterior dependencies may simple providing true ica bss solution leading inferior performance 
problem solutions discussed 
ensemble learning accelerated applying improved updating scheme parameters estimated 
approaches section briefly review methods proposed nonlinear ica bss 
jutten soft nonlinear mixtures assessing robustness performance seminal jutten algorithm 
probably introduce algorithm specifically nonlinear ica 
method restricted known nonlinearities unknown parameters back propagation type neural learning suffers huge computational complexity problems local minima 
years deco parra coauthors developed series papers nonlinear ica methods volume conserving symplectic transformations 
fact constraint volume conservation somewhat arbitrary methods usually able recover original sources 
earliest ideas achieving general nonlinear bss ica introduced pajunen authors known self organizing map som see example 
som learns unsupervised manner nonlinear mapping data usually rectangular grid 
suitable modifications mapping provided som roughly uniformly distributed grid 
marginal densities sides rectangular grid statistically independent 
som mapping provides regularization mechanism needed nonlinear bss tries preserve structure data nonlinear mapping simple possible 
som nonlinear bss method successfully applied denoising images corrupted multiplicative noise journal 
comparison standard som modified version suitable dealing multiplicative noise experimental separation results test real images 
general som nonlinear bss method able approximately separate sources distributions close uniform results poorer farther away distributions sources uniform 
difficulty som nonlinear bss ica computational complexity increases rapidly number sources limiting potential application method small scale problems 
results applicability som method linear nonlinear bss simple cases 
lin cowan independently proposed som nonlinear ica bss different manner treating ica local computational geometry problem 
restriction uniform distributions alleviated som called generative topographic mapping gtm method introduced principled theoretically founded alternative somewhat heuristic som method 
nonlinear bss method relying slightly modified version gtm introduced discussed somewhat thoroughly section book 
method requires knowledge distributions sources longer need close uniform problem curse dimensionality remains 
addition variational bayesian approaches previous subsection mlp networks employed nonlinear bss ica methods flexible models nonlinear mixing mapping 
autoassociative mlp networks desired output vector network input mixture vector tried task 
generative model inversion learned simultaneously separately utilizing fact models connected 
autoassociative mlps shown success nonlinear data representation generally suffer slow learning prone local minima 
works autoassociative mlps point estimates weights sources obtained minimizing representation error data 
impossible reliably choose structure model problems underfitting severe 
hecht nielsen proposed called replicator networks universal optimal nonlinear coding input data 
replicator networks mlp networks data vectors mapped unit hypercube mapped data uniformly distributed inside hypercube 
coordinates mapped data axes hypercube called natural coordinates form fact nonlinear ica solution noticed original papers 
hochreiter schmidhuber context mlp networks method minimum description length called lococode 
method estimate distribution weights model sources 
impossible measure description length sources 
anyway experimental results yielded lococode method show interesting connections ica method provides nonlinear ica solution 
known information theoretic criterion mutual information applied measuring statistical independence 
papers various mlp network methods introduced nonlinear blind separation 
particular yang amari cichocki deal extensions basic natural gradient method nonlinear bss furthermore extension entropy maximization experiments post nonlinear mixtures 
technique generalized mixtures sigmoidal nonlinearities allowing improved fitting complicated nonlinear mixing functions 
suggesting maximum entropy method nonlinear ica mlp network structure 
kernel methods particular kernel ica kernel pca nonlinear extension standard pca see brief description starting point developing algorithms nonlinear ica bss 
line research kernel canonical correlation analysis suggested nonlinear ica extensions standard linear ica 
efficient algorithm successful example blind separation nonlinearly mixed speech signals introduced 
open problem somewhat heuristic kernel methods choose nonlinear transformation higher dimensional space mixtures roughly linearly separable 
tan proposed radial basis function rbf neural network structure approximating separating mapping 
contrast function consists mutual information partial moments estimated separated sources provide regularization needed nonlinear bss 
simulation results artificially generated nonlinear mixture sets confirming validity method introduced 
marques almeida introduced pattern repulsion method maximum entropy principle solving nonlinear ica bss problems 
model origins lie statistical physics studied theoretically 
xiong huang propose generalization classic bell sejnowski algorithm uses power series nonlinear mixtures approximate taylor expansion separating mapping 
genetic algorithms considered improving estimation parameters separating mapping 
technique inspired properties electric fields applicable nonlinear ica extensions ica suggested 
viewed method constructing local density approximations joint factorial distributions providing rigorous theoretical foundation method 
researchers considered possibility applying methods nonlinear ica bss presenting complete study including experimental results 
xu developed general bayesian ying yang framework applied nonlinear ica 
hinton interpreted ica novel way energy probability density model mentioning easy extend approach nonlinear ica models 
related 
chen gopinath propose iterative technique provide computationally efficient solution nonlinear ica 
idea applying methods separating mixtures introduced independently see section iii local ica bss nonlinear independent component analysis blind source separation generally computationally conceptually difficult problems 
local linear ica bss methods received attention practical compromise linear ica completely nonlinear ica bss 
methods general standard linear ica different linear ica models describe observed data 
local linear ica models overlapping promising bayesian ica methods introduced nonoverlapping clustering methods proposed 
variational bayesian approach determining number local independent components high dimensional data sets successful application difficult real world medical data 
feature detection tools image analysis estimating local ica coordinate systems 
lin presents interesting theoretical considerations local geometric structure bss nonlinear mixtures 
concluding remarks considered ica bss problems nonlinear data models 
case ica characterized huge extra constraints regularization necessary achieving solutions coincide source separation 
main results stated 
solving nonlinear bss problem appropriately independence assumption possible mixtures separation structure structurally constrained example mixtures mappings satisfying addition theorem ii 
second prior information sources example bounded temporally correlated sources simplify algorithms reduce solutions 
promising method consists regularizing solution fully bayesian variational ensemble learning approach 
tries find sources mapping probably generated observed data 
ensemble learning method allows nonlinear source separation problems realistic size easily extended various directions 
lot remains done studying nonlinear ica bss problems 
studies needed separability problem 
second nonlinear problems difficult ill posed suitable regularization feel available information possible 
example incorporation temporal statistics quite helpful 
better modeling relationship independent components sources observations essential choosing suitable separation structure subsequently studying separability 
research addressed mainly theoretical problems 
results widely interesting validated realistic problems real world data 
acknowledgments 
supported european commission project bliss ist academy finland 
authors colleagues particular drs 
taleb valpola contributing results useful cooperation comments 
hyv rinen karhunen oja independent component analysis 
wiley 
comon independent component analysis new concept signal processing vol 
pp 


cardoso blind signal separation statistical principles proc 
ieee vol 
pp 

cichocki 
amari adaptive blind signal image processing learning algorithms applications 
wiley 
jutten taleb source separation till dawn proc 
nd int 
workshop independent component analysis blind source separation ica helsinki finland pp 

jutten blind separation sources part adaptive algorithm neuromimetic architecture signal processing vol 
pp 


cardoso blind beamforming non gaussian signals iee proceedings vol 
pp 

cichocki unbehauen robust learning algorithm blind separation signals electronics letters vol 
pp 

bell sejnowski information maximization approach blind separation blind deconvolution neural computation vol 


cardoso laheld equivariant adaptive source separation ieee trans 
signal processing vol 
pp 

amari cichocki yang new learning algorithm blind signal separation advances neural information processing systems denver colorado december pp 

hyv rinen oja fast fixed point algorithm independent component analysis neural computation vol 
pp 


cardoso jutten eds proc 
int 
workshop independent component analysis signal separation france january 
pajunen karhunen eds proc 
nd int 
workshop independent component analysis blind signal separation helsinki finland june 

lee makeig sejnowski eds proc 
rd int 
conf 
independent component analysis signal separation san diego ca usa december 
girolami ed advances independent component analysis 
springer verlag 
haykin ed unsupervised adaptive filtering vol 
blind source separation 
wiley 
roberts eds independent component analysis principles practice 
cambridge univ press 
tong soon huang liu new blind identification algorithm proc 
ieee int 
symp 
circuits systems iscas new orleans la usa 

cardoso moulines blind source separation technique second order statistics ieee trans 
signal processing vol 
pp 

matsuoka ohya kawamoto neural net blind separation nonstationary signals neural networks vol 
pp 

pham 
cardoso blind separation instantaneous mixtures nonstationary sources ieee trans 
signal processing vol 
pp 

hyv rinen pajunen nonlinear independent component analysis existence uniqueness results neural networks vol 
pp 

taleb jutten source separation post nonlinear mixtures ieee trans 
signal processing vol 
pp 

taleb generic framework blind source separation structured nonlinear models ieee trans 
signal processing vol 
pp 

eriksson blind identifiability class nonlinear instantaneous ica models proc 
xi european signal proc 
conf 
eusipco vol 
toulouse france september pp 

analyse des de probabilit proc 
int 
statistics conferences vol 
iii washington 
characterization gamma distribution annals mathematical statistics pp 

haykin neural networks comprehensive foundation nd ed 
prentice hall 
almeida linear nonlinear ica mutual information proc 
ieee adaptive systems signal processing communications control symposium lake louise canada october pp 

zadeh blind source separation convolutive nonlinear mixtures ph dissertation grenoble france september 
jutten high performance magnetic field smart sensor arrays source separation proc 
st int 
conf 
modeling simulation microsystems msm santa clara ca usa april pp 

blind identification lti lti nonlinear channel models ieee trans 
signal processing vol 
pp 
december 
hunter identification nonlinear biological systems lnl cascade models biol 
cybernetics vol 
pp 
december 
zadeh jutten separating convolutive post non linear mixtures proc 
rd workshop independent component analysis signal separation ica san diego california usa pp 

taleb sole jutten quasi nonparametric blind inversion wiener systems ieee trans 
signal processing vol 
pp 

theis lang maximum entropy minimal mutual information nonlinear model proc 
int 
conf 
independent component analysis signal separation ica san diego ca usa pp 

almeida jutten valpola realistic models nonlinear mixtures hut inesc bliss ist project report may 
popovic hall effect devices 
bristol adam 
rao extension theorem functions random variables satisfying addition theorem communications statistics vol 
pp 

aczel lectures functional equations applications 
new york academic press 
zadeh jutten geometric approach separating post nonlinear mixtures proc 
xi european signal processing conf 
eusipco vol 
ii toulouse france pp 

jutten separability nonlinear mixtures temporally correlated sources ieee signal processing letters appear 
taleb jutten nonlinear source separation mixtures proc 
europ 
symp 
artificial neural networks esann bruges belgium pp 

batch algorithm source separation post nonlinear mixtures proc 
int 
workshop independent component analysis signal separation ica france pp 


lee koehler blind source separation nonlinear mixing models neural networks signal processing vii 
ieee press pp 

pham jutten blind source separation post nonlinear mixtures proceedings ica san diego ca usa pp 


ller separation post nonlinear mixtures ace temporal decorrelation proc 
int 
conf 
independent component analysis signal separation ica san diego usa pp 

sol zadeh jutten 
pham improving algorithm speed pnl mixture separation wiener system inversion nara japan april submitted ica symposium 

ller blind separation post nonlinear mixtures transformations temporal decorrelation nara japan april submitted ica symposium 
peng chi siu semi parametric hybrid neural model nonlinear blind signal separation int 
neural systems vol 
pp 

parisi blind source separation nonlinear mixtures adaptive spline neural networks proc 
int 
conf 
independent component analysis signal separation ica san diego ca usa pp 

alvarez prieto prieto separation sources class post nonlinear mixtures proc 
th european symp 
artificial neural networks esann bruges belgium april pp 

sole jutten taleb parametric approach blind deconvolution nonlinear channels neurocomputing vol 
pp 

cover thomas elements information theory 
wiley series telecommunications 
cal 
blind equalization nonlinear satellite system mcmc simulation methods applied signal processing vol 
pp 

zadeh jutten approach separating post nonlinear mixtures nara japan april submitted th int 
symp 
independent component analysis blind signal separation ica 
ensemble learning advances independent component analysis girolami ed 
berlin springer verlag pp 

attias independent factor analysis neural computation vol 
pp 

ensemble learning independent component analysis proc 
int 
workshop independent component analysis signal separation ica france pp 

valpola karhunen unsupervised ensemble learning method nonlinear dynamic state space models neural computation vol 
pp 

honkela bayesian nonlinear independent component analysis multi layer perceptrons advances independent component analysis girolami ed 
springer verlag pp 

valpola nonlinear independent analysis ensemble learning theory proc 
nd int 
workshop independent component analysis blind signal separation ica helsinki finland pp 

valpola honkela karhunen nonlinear independent component analysis ensemble learning experiments discussion proc 
nd int 
workshop independent component analysis blind signal separation ica helsinki finland pp 

valpola oja honkela karhunen nonlinear blind source separation variational bayesian learning ieice transactions japan vol 
march appear special section blind signal processing invited 
valpola honkela karhunen ensemble learning approach nonlinear dynamic blind source separation state space models proc 
int 
joint conf 
neural networks ijcnn honolulu hi usa pp 

valpola honkela matlab codes nfa algorithms www cis hut fi projects ica bayes 
cichocki zhang choi 
amari nonlinear dynamic independent component analysis state space neural network models proc 
int 
workshop independent component analysis signal separation ica france pp 

rel valpola oja dynamical factor analysis rhythmic activity proc 
int 
conf 
independent component analysis signal separation ica san diego ca usa pp 

valpola oja detecting process state changes nonlinear blind source separation proc 
int 
conf 
independent component analysis signal separation ica san diego usa december pp 

valpola karhunen building blocks hierarchical latent variable models proc 
rd int 
conf 
independent component analysis signal separation ica san diego usa pp 

valpola karhunen hierarchical models variance sources proc 
th int 
symp 
independent component analysis blind source separation ica cichocki murata eds nara japan april submitted 
valpola karhunen nonlinear independent factor analysis hierarchical models proc 
th int 
symp 
independent component analysis blind source separation ica cichocki murata eds nara japan april submitted 
valpola effect form posterior approximation variational learning ica models proc 
th int 
symp 
independent component analysis blind signal separation ica nara japan april submitted 
honkela speeding cyclic update schemes pattern searches proc 
th int 
conf 
neural inform 
processing iconip singapore pp 

jutten calcul traitement du signal analyse en ind ph dissertation univ grenoble france french 
blind separation sources nonlinear neural algorithm neural networks vol 
pp 

deco brauer nonlinear higher order statistical decorrelation volume conserving neural architectures neural networks vol 
pp 

parra deco redundancy reduction information preserving nonlinear maps network vol 
pp 

parra symplectic nonlinear component analysis advances neural inform 
proc 
systems 
cambridge mass mit press vol 
pp 

deco information theoretic approach neural computing 
springer verlag 
parra deco statistical independence novelty detection information preserving nonlinear maps neural computation vol 
pp 

pajunen nonlinear independent component analysis selforganizing maps proc 
int 
conf 
artificial neural networks icann von der malsburg ed 
bochum germany springer verlag pp 

pajunen hyv rinen karhunen nonlinear blind source separation self organizing maps proc 
int 
conf 
neural information processing hong kong pp 

yin allison image denoising self organizing map nonlinear independent component analysis neural networks vol 
pp 

herrmann yang perspectives limitations selforganizing maps proc 
int 
conf 
neural information processing iconip hong kong pp 

lin cowan faithful representation separable distributions neural computation vol 
pp 

bishop williams gtm generative topographic mapping neural computation vol 
pp 

pajunen karhunen maximum likelihood approach nonlinear blind source separation proc 
int 
conf 
artificial neural networks icann lausanne switzerland pp 

hecht nielsen replicator neural networks universal optimal source coding science vol 
pp 

data manifolds natural coordinates replicator neural networks optimal source coding proc 
int 
conf 
neural information processing iconip hong kong pp 

hochreiter schmidhuber feature extraction lo neural computation vol 
pp 

yang 
amari cichocki information theoretic approach blind separation sources non linear mixture signal processing vol 
pp 

kokkinakis neural network blind source separation non linear mixtures proc 
int 
conf 
artificial neural networks icann 
wien austria springer verlag pp 

fisher principe entropy manipulation arbitrary nonlinear mappings neural networks signal processing vii etal 
ed 
new york ieee press pp 

bach jordan kernel independent component analysis journal learning research vol 
pp 

fyfe lai ica kernel canonical correlation analysis proc 
nd int 
workshop independent component analysis blind signal separation ica helsinki finland pp 


ller nonlinear blind source separation kernel feature spaces proc 
int 
conf 
independent component analysis signal separation ica san diego usa pp 

tan wang nonlinear blind source separation radial basis function network ieee trans 
neural networks vol 
pp 
january 
marques almeida separation nonlinear mixtures pattern repulsion proc 
int 
workshop independent component analysis signal separation ica france pp 

theis bauer lang pattern repulsion revisited proc 
int 
conference artificial natural neural networks mira prieto eds 
granada spain springer verlag june pp 

huang nonlinear independent component analysis ica power series application blind source separation proc 
rd int 
conf 
independent component analysis signal separation ica san diego ca usa pp 

rojas rojas nonlinear blind source separation genetic algorithms proc 
int 
conf 
independent component analysis signal separation ica san diego usa pp 

hochreiter mozer electric field approach independent component analysis proc 
int 
workshop independent component analysis blind signal separation ica helsinki finland pp 

maximum likelihood density estimation criterion unsupervised learning complex models advances neural information processing systems leen dietterich tresp eds vol 

mit press pp 

xu temporal byy learning state space approach hidden markov model blind source separation ieee trans 
signal processing vol 
pp 

hinton welling teh osindero new view ica proc 
int 
conf 
independent component analysis signal separation ica san diego ca usa pp 

marks movellan diffusion networks product experts factor analysis proc 
int 
conf 
independent component analysis signal separation ica san diego ca usa pp 

chen gopinath advances neural inform 
proc 
systems leen dietterich tresp eds vol 

mit press pp 


lee lewicki sejnowski ica mixture models unsupervised classification non gaussian sources automatic context switching blind signal separation ieee trans 
pattern recognition machine intelligence vol 
pp 


lee lewicki image processing methods ica mixture models independent component analysis principles practice roberts eds 
cambridge university press pp 

karhunen local linear independent component analysis clustering int 
neural systems vol 
pp 

multi class independent component analysis advances independent component analysis girolami ed 
springer verlag pp 

chan 
lee sejnowski variational learning clusters nonsymmetric independent components proc 
int 
conf 
independent component analysis signal separation ica san diego ca usa pp 

variational learning clusters nonsymmetric independent components machine learning research vol 
pp 
august 
lin cowan feature extraction approach blind source separation neural networks signal processing vii ed 
new york ieee press pp 

lin factorizing probability density functions generalizing ica proc 
int 
workshop independent component analysis signal separation ica france pp 

