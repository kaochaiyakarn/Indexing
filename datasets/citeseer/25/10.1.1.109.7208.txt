online learning approximate dependency parsing algorithms ryan mcdonald fernando pereira department computer information science university pennsylvania philadelphia pa pereira cis upenn edu extend maximum spanning tree mst dependency parsing framework mcdonald 
incorporate higher order feature representations allow dependency structures multiple parents word 
show extensions mst framework computationally intractable intractability circumvented new approximate parsing algorithms 
conclude experiments showing discriminative online learning approximate algorithms achieves best reported parsing accuracy czech danish 
dependency representations sentences hudson uk model head dependent syntactic relations edges directed graph 
displays dependency representation sentence john hit ball bat 
sentence example projective nested tree representation edges drawn plane crossing 
non projective representations preferred sentence 
particular word order languages non projectivity common phenomenon relative positional constraints dependents rigid 
dependency structures figures satisfy tree constraint weakly connected graphs unique root node non root node exactly parent 
trees examples drawn mcdonald 

common formalisms allow words modify multiple parents hudson 
mcdonald 
shown treating dependency parsing search highest scoring maximum spanning tree mst graph yields efficient algorithms projective non projective trees 
combined discriminative online learning algorithm rich feature set models provide state art performance multiple languages 
parsing algorithms require score dependency tree factors sum scores edges 
order factorization restrictive allows features defined single attachment decisions 
previous shown conditioning neighboring decisions lead significant improvements accuracy yamada matsumoto charniak 
extend mst parsing framework incorporate higher order feature representations bounded size connected subgraphs 
algorithm acyclic dependency graphs dependency graphs word may depend multiple heads 
cases parsing general intractable provide novel approximate algorithms cases tractable 
evaluate algorithms online learning framework shown robust respect approximate inference describe experiments displaying new models lead state art accuracy english best accuracy know czech danish 
maximum spanning tree parsing dependency tree parsing search maximum spanning tree mst graph root john saw dog yesterday root hit john ball example non projective dependency structure 
bat root john hit ball bat example dependency structure 
proposed mcdonald 

formulation leads efficient parsing algorithms projective non projective dependency trees eisner algorithm eisner chu liu edmonds algorithm chu liu edmonds respectively 
formulation works defining score dependency tree sum edge scores xn input sentence dependency tree view set tree edges write indicate edge word xi word xj 
consider example subscripts index nodes tree 
score tree call order dependency parsing scores restricted single edge dependency tree 
score edge turn computed inner product high dimensional feature representation edge corresponding weight vector standard linear classifier weight vector parameters learned training 
note arbitrary features edge input sequence directed graph maximum spanning tree mst problem find highest scoring subgraph satisfies tree constraint vertices defining graph words sentence vertices directed edge words score calculated mcdonald 
showed dependency parsing equivalent finding mst graph 
furthermore shown formulation lead state art results combined discriminative learning algorithms 
mst formulation applies directed graph feature representations parsing algorithms eisner rely linear ordering vertices order words sentence 
second order mst parsing restricting scores single edge dependency tree gives impoverished view dependency parsing 
yamada matsumoto showed keeping small amount parsing history crucial improving parsing performance locally trained shift reduce svm parser 
reasonable assume parsing models benefit features previous decisions 
focus methods parsing second order spanning trees 
models factor score tree sum adjacent edge pair scores 
quantify consider example 
second order spanning tree model score second order score function score creating pair adjacent edges word xi words xk xj 
instance score creating edges hit hit ball 
score functions relative left right parent score adjacent edges different sides parent instance adjacent edges hit john ball 
independence left right descendants allow second order projective parsing algorithm see 
write xi xj xj left right dependent word xi 
example score creating dependency hit ball ball child right hit 
formally word children shown picture xi xi xi 
xij xij 
xim score factors follows ik ik ij ij ik ik second order factorization subsumes order factorization score function just ignore middle argument simulate order scoring 
score tree secondorder parsing adjacent side children tree second order model allows condition parsing decision dependent picked particular word analogous markov conditioning charniak parser charniak 
exact projective parsing projective mst parsing order algorithm extended second order case noted eisner 
intuition algorithm shown graphically displays order secondorder algorithms 
order algorithm word gather left right dependents independently gathering half subtree rooted dependent separate stages 
splitting chart items left right components eisner algorithm requires indices maintained step discussed detail eisner mcdonald 
second order algorithm key insight delay scoring edges pairs order non proj approx sentence 
xn root weight function 
order proj 
true 




tree continue 







return 
approximate second order parsing algorithm 
dependents gathered 
allows collection pairs adjacent dependents single stage allows incorporation second order scores maintaining parsing 
eisner algorithm extended arbitrary th order model complexity 
th order parsing algorithm similarly second order algorithm collect pairs adjacent dependents succession attaching parent 
approximate non projective parsing unfortunately second order non projective mst parsing np hard shown appendix circumvent designed approximate algorithm exact second order projective eisner algorithm 
approximation works finding highest scoring projective parse 
rearranges edges tree time long rearrangements increase score violate tree constraint 
easily motivate approximation observing non projective languages czech danish trees primarily projective just non projective edges nilsson 
starting highest scoring projective tree typically small number transformations away highest scoring non projective tree 
algorithm shown 
expression denotes dependency graph identical xj parent xi order second order extension eisner algorithm second order dependency parsing 
shows creates dependency second order knowledge dependent 
done creation sibling item part 
order model dependency created algorithm forgotten dependent 
test tree true iff dependency graph satisfies tree constraint 
detail line algorithm sets highest scoring second order projective tree 
loop lines exits score improvement possible 
iteration seeks single highest scoring parent change break tree constraint 
effect nested loops starting lines enumerate pairs 
line sets dependency graph obtained changing xj parent xi 
line checks move valid testing xj parent xi tree 
line computes score change change larger previous best change record new tree created lines 
considering possible valid edge changes tree algorithm checks see best new tree higher score 
case change tree permanently re enter loop 
exit single edge switches improve score 
algorithm allows non projective edges restrict edge changes maintain tree property 
fact edge change resulting tree guaranteed higher scoring projective tree exact projective parsing algorithm 
difficult find examples approximation terminate returning highest scoring non projective parse 
clear approximation terminate finite number dependency trees sentence iteration loop requires increase score continue 
loop potentially take exponential time bound number edge transformations fixed value easy argue hurt performance 
word order languages czech non projective dependency trees primarily projective modulo edges 
inference algorithm starts highest scoring projective parse best non projective parse differs small number edge transformations 
furthermore easy show iteration loop takes time resulting mn runtime algorithm 
practice approximation terminates small number transformations need bound number iterations experiments 
note possible approximations 
reasonable approach find highest scoring order non projective parse re arrange edges second order scores similar manner algorithm described 
implemented method results slightly worse 
danish parsing secondary parents argued dependency formalism called discontinuous grammar annotated large set danish sentences formalism create danish dependency treebank 
formalism allows root han og ser looks sees elephants example dependency tree danish dependency treebank 
word multiple parents 
examples include verb coordination subject object argument verbs relative clauses words satisfy dependencies inside outside clause 
example shown sentence looks sees elephants 
pronoun subject verbs sentence noun elephants corresponding object 
danish dependency treebank roughly words parent breaks single parent tree constraint previously required dependency structures 
allows cyclic dependencies deal acyclic dependency graphs 
common trees dependency graphs involving multiple parents established literature hudson 
unfortunately problem finding dependency structure highest score setting intractable chickering 
create approximate parsing algorithm dependency structures multiple parents start approximate second order algorithm outlined 
non projective algorithm danish dependency treebank contains small number non projective arcs 
modify lines algorithm looks change parent addition new parent causes highest change score create cycle change iteration change depend resulting score new tree 
simple new approximate parsing algorithm train new parser produce multiple parents 
online learning approximate inference section review mcdonald 
online large margin dependency concerned violating tree constraint 
parsing 
usual supervised learning assume training set xt yt consisting pairs sentence xt correct dependency representation yt 
algorithm extension margin relaxed algorithm mira crammer singer learning structured outputs case dependency structures 
gives pseudo code algorithm 
online learning algorithm considers single training instance update weight vector common method setting final weight vector average weight vectors iteration collins shown alleviate overfitting 
iteration algorithm considers single training instance 
parse instance obtain predicted dependency graph find smallest norm update weight vector ensures training graph predicted graph margin proportional loss predicted graph relative training graph number words incorrect parents predicted tree mcdonald 
note impose margin constraints single highest scoring graph correct graph relative current weight setting 
past tree structured outputs constraints best scoring tree mc donald possible trees factored representations taskar mcdonald 
single margin constraint example leads faster training negligible degradation performance 
furthermore formulation relates learning directly inference important want model set weights relative errors approximate inference algorithm 
algorithm viewed large margin version perceptron algorithm structured outputs collins 
online learning algorithms shown robust approximate exact inference problems word alignment moore sequence analysis marcu mcdonald phrase structure parsing collins roark 
robustness approximations comes fact online framework sets weights respect inference 
words learning method sees common errors due training data xt yt 



min xt yt xt yt arg max xt 


mira learning algorithm 
write mean score tree weight vector approximate inference adjusts weights correct 
marcu formalizes intuition presenting online learning framework parameter updates directly respect errors inference algorithm 
show section robustness extends approximate dependency parsing 
experiments score adjacent edges relies definition feature representation 
noted earlier representation subsumes order representation mcdonald 
incorporate features new second order features describe 
old order features built parent child words pos tags pos tags surrounding words words child parent direction distance parent child 
second order features built conjunctions word pos identity predicates xi pos xk pos xj pos xk pos xj pos xk word xj word xk word xj pos xk pos xj word xi pos part speech th word sentence 
include conjunctions features direction distance sibling sibling determined usefulness features development set helped find features pos tags words siblings improve accuracy 
ignored fea english accuracy complete st order projective nd order projective table dependency parsing results english 
czech accuracy complete st order projective nd order projective st order non projective nd order non projective table dependency parsing results czech 
tures triples words explode size feature space 
evaluate dependencies word accuracy percentage words sentence correct parent tree complete dependency analysis 
evaluation exclude punctuation english include czech danish standard 
english results create data sets english yamada matsumoto head rules extract dependency trees wsj setting sections training section development section evaluation 
models rely part speech tags input ratnaparkhi tagger provide development evaluation set 
data sets exclusively projective compare projective parsers exact projective parsing algorithms 
purpose experiments gauge benefit including second order features exact parsing algorithms attained projective setting 
results shown table 
see clearly advantage introducing second order features 
particular complete tree metric improved considerably 
czech results czech data predefined training development testing split prague dependency treebank haji automatically generated pos tags supplied data reduce pos tag set collins 

average sentences training development test sets non projective dependency total edges ac danish precision recall measure nd order projective nd order non projective nd order non projective multiple parents non projective 
results shown table 
mcdonald 
showed substantial improvement accuracy modeling edges czech shown difference order models 
table shows second order model provides comparable accuracy boost approximate non projective algorithm 
second order model accuracy highest reported accuracy single parser data 
similar results obtained hall accuracy take best output charniak parser extended czech rerank slight variations output introduce non projective edges 
system relies slower phrase structure parser base model auxiliary reranking module 
second order projective parser analyzes test set non projective approximate parser needs parse entire evaluation set showing runtime approximation completely dominated initial call second order projective algorithm post process edge transformation loop typically iterates times sentence 
danish results experiments danish dependency treebank 
treebank contains small number inter sentence cyclic dependencies removed sentences contained structures 
resulting data set contained sentences 
partitioned data contiguous training testing splits 
held subset training data development purposes 
compared systems standard second order projective non projective parsing models modified second order non projective model allows multiple parents section 
systems gold standard part speech trained tagger readily available danish 
results shown 
expected non table dependency parsing results danish 
projective parser slightly better projective parser edges non projective 
word may arbitrary number parents precision recall accuracy measure performance 
means correct training loss longer hamming loss 
false positives plus false negatives edge decisions balances precision recall ultimate performance metric 
expected basic projective parsers recall roughly lower precision models pick parent word 
parser introduce multiple parents see increase recall nearly absolute slight drop precision 
results promising show robustness discriminative online learning approximate parsing algorithms 
discussion described approximate dependency parsing algorithms support higher order features multiple parents 
showed approximations combined online learning achieve fast parsing competitive parsing accuracy 
results show gain allowing richer representations outweighs loss approximate parsing shows robustness online learning algorithms approximate inference 
approximations simple 
start reasonably baseline small transformations score structure converges 
approximations word order languages studied primarily projective making approximate starting point close goal parse 
investigate benefits parsing principled approaches approximate learning inference techniques learning search optimization framework marcu 
framework possibly allow include effectively global features dependency structure current second order model 
acknowledgments supported nsf itr 
charniak 

maximum entropy inspired parser 
proc 
naacl 
chickering geiger heckerman 

learning bayesian networks combination knowledge statistical data 
technical report msr tr microsoft research 
chu liu 

shortest arborescence directed graph 
science sinica 
collins roark 

incremental parsing perceptron algorithm 
proc 
acl 
collins haji ramshaw tillmann 

statistical parser czech 
proc 
acl 
collins 

discriminative training methods hidden markov models theory experiments perceptron algorithms 
proc 
emnlp 
crammer singer 

ultraconservative online algorithms multiclass problems 
jmlr 
marcu 

learning search optimization approximate large margin methods structured prediction 
proc 
icml 
edmonds 

optimum branchings 
journal research national bureau standards 
eisner 

new probabilistic models dependency parsing exploration 
proc 
col ing 
haji sgall 

prague dependency treebank cdrom 
linguistics data consortium cat 
ldc 
hall 
corrective modeling non projective dependency parsing 
proc 
iwpt 
hudson 

word grammar 
blackwell 


parsing local cost functions discontinuous grammars 
proc 
fg mol 


danish dependency treebank treebank tool 
proc 

mcdonald crammer pereira 

flexible text segmentation structured multilabel classification 
proc 
hlt emnlp 
mcdonald crammer pereira 

online large margin training dependency parsers 
proc 
acl 
mcdonald pereira haji 

non projective dependency parsing spanning tree algorithms 
proc 
hlt emnlp 
uk 

dependency syntax theory practice 
state university new york press 
moore 

discriminative framework bilingual word alignment 
proc 
hlt emnlp 
nilsson 

pseudo projective dependency parsing 
proc 
acl 
ratnaparkhi 

maximum entropy model part speech tagging 
proc 
emnlp 
taskar klein collins koller manning 

max margin parsing 
proc 
emnlp 
yamada matsumoto 

statistical dependency analysis support vector machines 
proc 
iwpt 
nd order non projective mst parsing np hard proof reduction matching dm 
dm disjoint sets distinct elements set question subset occurs exactly element reduction instance dm define graph vertices elements artificial root node 
insert edges root xi edges xi yi zi order words root left followed elements define second order score function follows root xi xj xi xj xi yj xi yj xi yj zk xi yj zk scores defined including edges pairs defined original graph 
theorem matching iff second order mst score proof observe tree score greater require pairs edges form xi yj zk 
happen xi multiple yj children multiple zk children 
true introduce scored edge pair xi yj 
highest scoring second order mst score means xi unique pair children yj zk represents matching triples 
furthermore yj zk match incoming edge tree 
hand dm tree weight consisting second order edges xi yj zk element matching tree weight greater highest scoring second order mst 
find highest scoring second order mst polynomial time dm solvable 

