learning local global consistency zhou olivier bousquet thomas lal jason weston bernhard sch lkopf max planck institute biological cybernetics tuebingen germany firstname tuebingen mpg de consider general problem learning labeled unlabeled data called semi supervised learning transductive inference 
principled approach semi supervised learning design classifying function sufficiently smooth respect intrinsic structure collectively revealed known labeled unlabeled points 
simple algorithm obtain smooth solution 
method yields encouraging experimental results number classification problems demonstrates effective unlabeled data 
consider general problem learning labeled unlabeled data 
point set 
xl xl 
xn label set 
points labels 
yl remaining points unlabeled 
goal predict labels unlabeled points 
performance algorithm measured error rate unlabeled points 
learning problem called semi supervised transductive 
labeling requires expensive human labor unlabeled data far easier obtain semisupervised learning useful real world problems attracted considerable amount research :10.1.1.28.850
typical application web categorization manually classified web pages small part entire web number unlabeled examples large 
key semi supervised learning problems prior assumption consistency means nearby points label points structure typically referred cluster manifold label 
argument akin called cluster assumption :10.1.1.19.3957
note assumption local second global 
orthodox supervised learning algorithms nn general depend assumption local consistency 
illustrate prior assumption consistency underlying semi supervised learning consider toy dataset generated pattern intertwining moons 
point similar points local neighborhood furthermore points moon similar points moon 
classification results support vector machine svm rbf kernel toy data moons unlabeled point labeled point labeled point nn svm rbf kernel ideal classification classification moons pattern 
toy data set labeled points classifying result svm rbf kernel nn ideal classification hope obtain 
nn shown respectively 
assumption consistency moons classified shown 
main differences various semi supervised learning algorithms spectral methods random walks graph transductive svm lie way realizing assumption consistency 
principled approach formalize assumption design classifying function sufficiently smooth respect intrinsic structure revealed known labeled unlabeled points 
propose simple iteration algorithm construct smooth function inspired spreading activation networks diffusion kernels semi supervised learning clustering specifically zhu :10.1.1.19.8100:10.1.1.14.8498
keynote method point iteratively spread label information neighbors global stable state achieved 
organize follows section shows algorithm detail discusses possible variants section introduces regularization framework method section presents experimental results toy data digit recognition text classification section concludes points researches 
algorithm point set 
xl xl 
xn label set 
points xi labeled yi remaining points xu unlabeled 
goal predict label unlabeled points 
denote set matrices nonnegative entries 
matrix 
corresponds classification dataset labeling point xi label yi arg maxj fij 
understand vectorial function assigns vector fi point xi 
define matrix yij xi labeled yi yij 
clearly consistent initial labels decision rule 
algorithm follows 
form affinity matrix defined wij exp xi xj wii 
construct matrix diagonal matrix element equal sum th row 
iterate sf convergence parameter 

denote limit sequence 
label point xi label yi arg maxj ij algorithm understood intuitively terms spreading activation networks experimental psychology 
define pairwise relationship dataset diagonal elements zero 
think graph defined vertex set just edges weighted second step weight matrix normalized symmetrically necessary convergence iteration 
steps exactly spectral clustering 
iteration third step point receives information neighbors term retains initial information second term 
parameter specifies relative amount information neighbors initial label information 
worth mentioning self reinforcement avoided diagonal elements affinity matrix set zero step 
information spread symmetrically symmetric matrix 
label unlabeled point set class received information iteration process 
show sequence converges loss generality suppose iteration equation sf algorithm 
eigenvalues note similar stochastic matrix sd lim lim 
lim classification clearly equivalent 
compute directly iterations 
shows iteration result depend initial value iteration 
addition worth notice fact graph diffusion kernel 
discuss possible variants method 
simplest modification repeat iteration convergence arbitrary positive integer 
addition similar consider substitute third step corresponding closed form interesting replace transpose classifying function hard see equivalent compare variants original algorithm experiments 
regularization framework develop regularization framework iteration algorithm 
cost function associated defined wij fi dii fj fi yi djj regularization parameter 
classifying function 
arg min term right hand side cost function smoothness constraint means classifying function change nearby points 
second term fitting constraint means classifying function change initial label assignment 
trade competing constraints captured positive parameter 
note fitting constraint contains labeled unlabeled data 
understand smoothness term sum local variations local changes function nearby points 
mentioned points involving pairwise relationships thought undirected weighted graph weights represent pairwise relationships 
local variation fact measured edge 
simply define local variation edge difference function values ends edge 
smoothness term essentially splits function value point edges attached computing local changes value assigned edge proportional weight 
differentiating respect transformed sf 
introduce new variables note 
sf invertible 
recovers closed form expression iteration algorithm 
similarly develop optimization frameworks variants omit discussions due lack space 
experiments nn vs rest svms baselines compared method variants compared zhu harmonic gaussian field method coupled class mass normalization cmn closely related 
best knowledge reliable approach model selection labeled points available 
algorithms respective optimal parameters parameter methods variants simply fixed 
classification pattern moons 
convergence process iteration algorithm increasing shown 
note initial label information diffused moons 
real valued classifying function flatter flatter respect moons pattern increasing note clear moons emerge 
svm rbf kernel labeled point labeled point smooth global consistency smooth classification results supervised classifiers global consistency classification result svm rbf kernel smooth result svm consistency method 
toy problem experiment considered toy problem mentioned section 
affinity matrix defined rbf kernel diagonal elements set zero 
convergence process iteration algorithm increasing shown 
note initial label information diffused moons 
assumption consistency essentially means classifying function change slowly coherent structure aggregated large amount data 
illustrated toy problem clearly 
define function xi accordingly decision function sign xi equivalent decision rule described section 
show xi successively flatter respect moons pattern increasing note clear moons emerge 
basic idea method construct smooth function 
natural consider method improve supervised classifier smoothing classifying result 
words classifying result supervised classifier input algorithm 
conjecture demonstrated toy problem 
classification result svm rbf kernel 
result assigned method 
output method shown 
note points classified incorrectly svm successfully smoothed consistency method 
digit recognition experiment addressed classification task usps handwritten digits dataset 
digits experiments classes 
examples class total 
nn set 
width rbf kernel svm set harmonic gaussian field method set 
method variants affinity matrix constructed rbf kernel width harmonic gaussian method diagonal elements set 
test errors averaged trials summarized left panel 
samples chosen contain labeled point class 
consistency method variant clearly superior orthodox supervised learning algorithms nn svm better harmonic gaussian method 
note approach require affinity matrix positive definite 
enables incorporate prior knowledge digit image invariance elegant way jittered kernel compute affinity matrix 
kernel methods test error labeled points nn svm rbf kernel harmonic gaussian consistency method variant consistency variant consistency test error labeled points nn svm rbf kernel harmonic gaussian consistency method variant consistency variant consistency left panel error rates digit recognition usps handwritten digits dataset total subset containing digits 
right panel error rates text classification document vectors dimensional space 
samples chosen contain labeled point class 
known problems method 
case jittering pixel translation leads error rate labeled points 
text classification experiment investigated task text classification newsgroups dataset 
chose topic rec contains autos motorcycles baseball hockey version news 
articles processed rainbow software package options passing words porter stemmer counting tossing token stoplist smart system skipping headers ignoring words occur fewer documents 
preprocessing done 
removing empty documents obtained document vectors dimensional space 
documents normalized tfidf representation 
distance points xi xj defined xi xj xi xj xi xj 
nn set 
width rbf kernel svm set harmonic gaussian method set 
methods affinity matrix constructed rbf kernel width harmonic gaussian method diagonal elements set 
test errors averaged trials summarized right panel 
samples chosen contain labeled point class 
interesting note harmonic method number labeled points labeled point class 
think equal proportions different classes dataset labeled points proportions happen estimated exactly 
harmonic method worse slightly labeled points instance labeled points leads pretty poor estimation 
number labeled points increases harmonic method works somewhat better method proportions classes estimated successfully 
decision rule simpler fact corresponds called naive threshold baseline harmonic method 
key semi supervised learning problems consistency assumption essentially requires classifying function sufficiently smooth respect intrinsic structure revealed huge amount labeled unlabeled points 
proposed simple algorithm obtain solution demonstrated effective unlabeled data experiments including toy data digit recognition text categorization 
research focus model selection theoretic analysis 
acknowledgments vladimir vapnik olivier chapelle arthur andre elisseeff help 
andrew ng helpful discussions spectral clustering anonymous reviewers constructive comments 
special go zhu zoubin ghahramani john lafferty communicated important post processing step class mass normalization method provided detailed experimental data 
anderson 
architecture cognition 
harvard univ press cambridge ma 
belkin niyogi 
semi supervised learning manifolds 
machine learning journal appear 
blum chawla 
learning labeled unlabeled data graph 
icml 
chapelle weston sch lkopf 
cluster kernels semi supervised learning 
nips 
decoste sch lkopf 
training invariant support vector machines 
machine learning 
joachims 
transductive learning spectral graph partitioning 
icml 
kandola shawe taylor cristianini 
learning semantic similarity 
nips 
kondor lafferty 
diffusion kernels graphs discrete input spaces 
icml 
ng jordan weiss 
spectral clustering analysis algorithm 
nips 
seeger :10.1.1.28.850
learning labeled unlabeled data 
technical report university edinburgh 
hogg huberman 
observation phase transitions spreading activation networks 
science 
smola kondor 
kernels regularization graphs 
learning theory kernel machines berlin heidelberg germany 
springer verlag 
szummer jaakkola 
partially labeled classification markov random walks 
nips 
vapnik 
statistical learning theory 
wiley ny 
zhu ghahramani lafferty 
semi supervised learning gaussian fields harmonic functions 
icml 
