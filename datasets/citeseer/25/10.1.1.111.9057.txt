runahead execution alternative large instruction windows order processors ece department university austin patt ece utexas edu jared stark chris wilkerson yale patt today high performance processors tolerate long latency operations means order execution 
latencies increase size instruction window increase faster continue tolerate latencies 
reached point size instruction window handle latencies prohibitively large terms design complexity power consumption 
problem getting worse 
proposes runahead execution effective way increase memory latency tolerance order processor requiring unreasonably large instruction window 
runahead execution unblocks instruction window blocked long latency operations allowing processor execute far ahead program path 
results data prefetched caches long needed 
machine model intel pentium processor having entry instruction window adding runahead execution improves ipc instructions cycle wide range memory intensive applications 
machine model runahead execution combined entry window performs machine runahead execution entry instruction window 

today high performance processors tolerate long latency operations implementing order instruction execution 
order execution engine tolerates long latencies moving long latency operation ofthe way ofthe operations come instruction stream depend 
accomplish intel pentium trademarks registered trademarks united states countries 
microprocessor research intel labs jared stark intel com proceedings ninth international symposium high performance computer architecture hpca ieee desktop platforms group intel chris wilkerson intel com processor buffers operations instruction window size determines amount order engine tolerate 
today processors facing increasingly larger latencies 
growing disparity processor memory speeds operations cause cache misses main memory take hundreds cycles complete execution 
tolerating latencies solely order execution difficult requires larger instruction windows increases design complexity power consumption 
reason computer architects developed software hardware prefetching methods tolerate long memory latencies 
propose runahead execution substitute building large instruction windows tolerate long latency operations 
long latency operation ofthe way requires buffering instructions follow instruction window runahead execution order execution processor tosses ofthe instruction window 
instruction window blocked operation state ofthe architectural register file checkpointed 
processor enters runahead mode distributes bogus result blocking operation tosses ofthe instruction window 
instructions blocking operation fetched executed pseudo retired instruction window 
mean instructions executed completed conventional sense update architectural state 
blocking operation completes processor re enters normal mode restores checkpointed state re executes instructions starting blocking operation 
runahead benefit comes transforming small instruction window blocked long latency operations non blocking window giving performance ofa larger window 
instructions fetched executed runahead mode create accurate prefetches data instruction caches 
benefits come modest hardware cost describe 
evaluate runahead memory operations second level cache initiated long latency operation blocks instruction window 
intel ia isa microarchitectural parameters instruction window size ipc instructions cycle performance reported terms operations 
machine model intel pentium processor entry instruction window show current order execution engines unable tolerate long main memory latencies 
show runahead execution better tolerate latencies achieve performance machine larger instruction window 
results show baseline machine realistic memory latency ipc performance machine cache hit ratio ipc 
adding runahead increases baseline ipc ofthe ipc ofan identical machine entry instruction window 

related memory access important long latency operation concerned researchers long time 
caches tolerate memory latency exploiting temporal spatial locality applications 
improved latency tolerance caches allowing handle multiple outstanding misses service cache hits presence misses 
software prefetching techniques effective applications compiler statically predict memory cause cache misses 
applications trivial task 
techniques insert prefetch instructions applications increasing instruction bandwidth requirements 
hardware prefetching techniques dynamic information predict prefetch 
require instruction bandwidth 
different prefetch algorithms cover different types access patterns 
main problem hardware prefetching hardware cost complexity ofa prefetcher cover different types access patterns 
accuracy hardware prefetcher low cache pollution unnecessary bandwidth consumption degrades performance 
thread prefetching techniques idle thread contexts multithreaded processor run threads help primary thread 
helper threads execute code prefetches primary thread 
main disadvantage ofthese techniques require idle thread contexts spare resources fetch execu tion bandwidth available processor 
runahead execution proposed evaluated method improve data cache performance ofa stage pipelined order execution machine 
shown effective tolerating level data cache instruction cache misses 
order execution unable tolerate cache misses order execution tolerate cache latency executing instructions independent ofthe show order execution tolerate memory operations large expensive instruction window runahead alternative large window 
introduce runahead cache effectively handle store load communication runahead mode 
proposed mechanism execute instructions long latency instruction blocks retirement 
mechanism dynamically allocates portion ofthe register file thread launched primary thread stalls 
mechanism requires partial hardware support different contexts 
unfortunately resources partitioned threads thread ofthe machine full resources decreases thread benefit increases primary thread stalls 
runahead execution normal runahead mode ofthe machine full resources helps machine get ahead runahead mode 
lebeck proposed instructions dependent long latency operation removed relatively small scheduling window placed relatively big waiting instruction buffer operation complete point instructions moved back scheduling window 
combines latency tolerance benefit ofa large instruction window fast cycle time benefit small scheduling window 
requires large instruction window large physical register file associated cost 

order execution memory latency tolerance 
instruction scheduling windows proceedings ninth international symposium high performance computer architecture hpca ieee order execution tolerate cache misses better order execution scheduling operations independent ofthe order execution machine accomplishes windows instruction window scheduling window 
instruction window holds instructions decoded committed architectural state 
main purpose guarantee order retirement order support precise exceptions 
scheduling window holds subset ofthe instructions instruction window 
main purpose search instructions cycle ready execute schedule execution 
long latency operation blocks instruction window completed 
instructions may completed execution retire instruction window 
ifthe latency ofthe operation long instruction window large instructions pile instruction window full 
machine stalls stops making forward progress 

memory latency tolerance order processor section show idealized version ofa current order execution machine spends time stalling waiting main memory 
model entry instruction window 
machine buffers scheduler set entries create bottlenecks 
fetch engine ideal suffers cache misses supplies fetch width worth instructions cycle 
fetch stalls 
real branch predictor 
machine parameters ofthe current baseline intel pentium processor shown table 
shows percentage instruction window stalled different machines 
number top ofeach bar ipc ofthe machine 
data average benchmarks simulated 
see section 
cycles full inst window stalls finite sched real entry inst window infinite sched real entry inst window finite sched perfect entry inst window infinite sched perfect entry inst window infinite sched real entry inst window infinite sched perfect entry inst window finite sched runahead real entry inst window 
percentage cycles full instruction window stalls 
number top bar ipc machine 
machine fetch buffer instructions decode schedule execute retire 
machine entry instruction window finite scheduling window size real cache spends cycles full instruction window stalls progress 
ifthe scheduler removed bottleneck making scheduling window size infinite instruction window remains bottleneck cycles spent full window stalls 
main memory latency removed bottleneck making cache perfect machine wastes cycles full window stalls 
ofthe stalls due main memory latency 
eliminating latency increases ipc 
machine entry instruction window real cache able tolerate main memory latency better machines entry instruction windows real caches 
percentage window stalls similar ofthe machines entry instruction windows perfect caches 
ipc high misses free 
machine entry instruction window perfect cache shown 
highest ipc full window stalls 
shown rightmost bar runahead execution eliminates ofthe full window stalls due main memory latency entry window machine increasing ipc 

insight runahead proceedings ninth international symposium high performance computer architecture hpca ieee processor unable progress instruction window blocked waiting main memory 
runahead execution removes blocking instruction window fetches instructions follow executes independent 
runahead performance benefit comes fetching instructions fetch engine caches executing independent loads stores second level caches 
cache misses serviced parallel main memory initiated runahead mode provide useful prefetch requests 
premise nonblocking mechanism lets processor fetch execute useful instructions instruction window normally permits 
case runahead provides performance benefit order execution 

implementation runahead execution order processor section describe implementation execution order processor instructions access register file scheduled execute 
intel pentium processor mips microprocessor names brands may claimed property 
compaq alpha processor examples microarchitecture 
microarchitectures intel pentium pro processor instructions access register file placed scheduler 
implementation details slightly different microarchitectures basic mechanism works way 
shows simulated processor pipeline 
dashed lines show flow traffic ofthe caches 
shaded structures constitute hardware required support runahead execution explained section 
frontend register alias table rat renaming incoming instructions contains speculative mapping registers physical registers 
retirement rat contains pointers physical registers contain committed architectural values 
recovery state branch mispredictions exceptions 
important machine parameters 

entering runahead mode processor enter runahead mode time 
data cache instruction cache scheduling window stall possible events trigger transition runahead mode 
implementation processor enters runahead mode memory operation misses second level cache memory operation reaches head ofthe instruction window 
address ofthe instruction causes entry runahead mode recorded 
correctly recover architectural state exit runahead mode processor checkpoints state ofthe architectural register file 
performance reasons processor checkpoints state branch history register return address stack 
instructions instruction window marked runahead operations treated differently microarchitecture 
instruction fetched runahead mode marked runahead operation 
checkpointing ofthe architectural register file accomplished copying contents ofthe physical registers pointed retirement rat may take time 
avoid performance loss due copying processor update checkpointed architectural register file normal mode 
non runahead instruction retires instruction window updates architectural destination register checkpointed register file result 
cycles lost checkpointing 
checkpointing mechanisms discussion scope ofthis 
updates checkpointed register file allowed runahead mode 
worthwhile note kind runahead execution introduces second level mechanism pipeline 
retirement rat points architectural register state normal mode points pseudo architectural register state runahead mode reflects state updated pseudo retired instructions 

execution runahead mode proceedings ninth international symposium high performance computer architecture hpca ieee main complexities involved execution instructions memory communication propagation results 
describe rules ofthe machine hardware required support 
invalid bits instructions 
physical register invalid inv bit associated indicate bogus value 
instruction sources register invalid bit set invalid instruction 
inv bits prevent bogus prefetches resolution bogus data 
ifa store instruction invalid introduces inv value memory image runahead 
handle communication values inv values memory runahead mode small runahead cache accessed parallel level data cache 
describe rationale runahead cache design section 
propagation inv values 
instruction introduces inv value instruction causes processor enter runahead mode 
instruction load marks physical destination register inv 
store allocates line runahead cache marks destination bytes inv 
invalid instruction writes register marks register inv scheduled executed 
valid operation writes register resets inv bit destination register 
runahead store operations runahead cache 
previous runahead store instructions write results 
runahead loads dependent valid runahead stores regarded invalid instructions dropped 
experiments partly due limited number ia isa forwarding results runahead stores runahead loads essential high performance see section 
store dependent load instruction window forwarding accomplished store buffer exists current order processors 
ifa runahead load depends runahead store pseudo retired means store longer store buffer get result ofthe store location 
trace cache fetch unit instruction decoder queue frontend rat renamer stream hardware prefetcher access queue cache fp queue int queue mem queue fp scheduler int scheduler mem scheduler fp physical reg 
file int physical reg 
file inv inv fp exec units int exec units addr gen units front side bus access queue data cache store buffer runahead cache checkpointed architectural register file inv memory memory selection logic reorder buffer retirement rat 
processor model description evaluation runahead 
scale 
ity write result ofthe pseudo retired store data cache 
introduces extra complexity design ofthe data cache possibly second level cache data cache needs modified data written speculative runahead stores non runahead instructions 
writing data stores data cache evict useful cache lines 
possibility large fully associative buffer stores results retired runahead store instructions 
size access time ofthis associative structure prohibitively large 
structure handle case load depends multiple stores increased complexity 
simpler alternative propose runahead cache hold results inv status ofthe runahead stores 
runahead cache addressed just data cache smaller size small number instructions runahead mode 
call cache physically structure traditional cache purpose ofthe runahead cache cache data 
purpose provide communication inv status instructions 
evicted cache lines stored back larger storage simply dropped 
runahead cache accessed runahead loads stores 
normal mode instruction accesses 
proceedings ninth international symposium high performance computer architecture hpca ieee support correct communication bits stores loads entry store buffer byte runahead cache corresponding inv bit 
byte runahead cache bit associated sto bit indicating store written byte 
access runahead cache results hit ifthe accessed byte written store sto bit set accessed runahead cache line valid 
runahead stores follow rules update inv sto bits store results 
valid runahead store completes execution writes data store buffer entry just normal processor resets associated inv bit entry 
queries data cache sends prefetch request memory hierarchy misses data cache 

invalid runahead store scheduled sets inv bit associated store buffer entry 

valid runahead store exits instruction window writes result runahead cache resets inv bits ofthe written bytes 
sets sto bits ofthe bytes writes 

invalid runahead store exits instruction window sets inv bits sto bits ofthe bytes writes address valid 

runahead stores write results data cache 
complication arises address ofa store operation invalid 
case store operation simply treated nop 
loads unable identify dependencies stores incorrectly load stale value memory 
problem mitigated dependence predictors identify dependence store dependent load 
dependence identified load marked inv ifthe data value ofthe store inv 
ifthe data value ofthe store valid forwarded load 
runahead load operations 
runahead load operation invalid due different reasons 
may source inv physical register 

may dependent store marked inv store buffer 

may dependent store inv 
case detected runahead cache 
valid load executes accesses structures parallel data cache runahead cache store buffer 
hits store buffer entry hits marked valid load gets data store buffer 
load hits store buffer entry marked inv load marks physical destination register inv 
load considered hit runahead cache cache line accesses valid sto bit ofany ofthe bytes accesses cache line set 
ifthe load misses store buffer hits runahead cache checks inv bits ofthe bytes accessing runahead cache 
load executes data runahead cache ofthe inv bits set 
ofthe sourced data bytes marked inv load marks destination inv 
ifthe load misses store buffer runahead cache hits data cache uses value data cache considered valid 
may invalid oftwo reasons may dependent store inv address may dependent inv store marked destination bytes runahead cache inv corresponding line runahead cache deallocated due conflict 
rare cases affect performance significantly 
ifthe load misses structures sends request second level cache fetch data 
request hits second level cache data transferred second level cache level cache load completes execution 
ifthe request misses second level cache load marks destination register inv removed scheduler just load caused entry runahead mode 
request sent memory normal load request misses cache 
proceedings ninth international symposium high performance computer architecture hpca ieee execution prediction branches 
branches predicted resolved runahead mode exactly way normal mode difference branch inv source branches predicted updates global branch history register speculatively branches resolved 
problem ifthe branch correctly predicted 
ifthe branch mispredicted processor wrong path fetch branch hits control flow independent point 
call point program mispredicted inv branch fetched divergence point existence points necessarily bad performance show occur runahead mode better performance improvement 
interesting issue branch prediction training policy ofthe branch predictor tables runahead mode 
option option implementation train branch predictor tables 
ifa branch executes runahead mode normal mode policy results branch predictor trained twice branch 
predictor tables strengthened counters may lose hysteresis 
second option train branch predictor runahead mode 
results lower branch prediction accuracy runahead mode degrades performance moves divergence point closer time runahead entry point 
third option train branch predictor runahead mode queue communicate results runahead mode normal mode 
branches normal mode predicted predictions queue ifa prediction exists 
ifa branch predicted prediction queue train predictor tables 
fourth option separate predictor tables runahead mode normal mode copy table information normal mode runahead mode runahead entry 
option costly implement hardware simulated determine twice training policy ofthe option matters 
results show training branch predictor table entries twice show significant performance loss compared fourth option 
instruction pseudo retirement runahead mode 
runahead mode instructions leave instruction window program order 
instruction reaches head ofthe instruction window considered pseudo retirement 
ifthe instruction considered inv moved ofthe window immediately 
valid needs wait executed point may inv result written physical register file 
pseudo retirement instruction releases resources allocated execution 
valid invalid instructions update retirement rat leave instruction window 
retirement rat need store inv bits associated register physical registers inv bits associated 

exiting runahead mode exit runahead mode initiated time 
simplicity handle exit runahead mode way branch misprediction handled 
instructions machine flushed buffers deallocated 
checkpointed architectural register file copied pre determined portion ofthe physical register file 
frontend retirement rats repaired point physical registers hold values architectural registers 
recovery accomplished reloading hard coded mapping ofthe alias tables 
lines runahead cache invalidated sto bits set checkpointed branch history register return address stack restored exit runahead mode 
processor starts fetching instructions starting address ofthe instruction caused entry runahead mode 
policy exit runahead mode data ofthe blocking load returns memory 
alternative policy exit time earlier timer portion ofthe pipeline fill penalty window fill penalty eliminated 
alternative performs benchmarks performs badly 
exiting early performs slightly worse 
reason performs worse benchmarks second level cache prefetch requests generated processor exit runahead mode early 
aggressive runahead implementation may dynamically decide exit runahead mode 
benchmarks benefit staying runahead mode hundreds cycles original returns memory 
investigating potential feasibility ofa mechanism dynamically decides exit runahead mode 

simulation methodology simulator built top ofa level ia architectural simulator executes long instruction traces 
lit trace checkpoint ofthe processor state including memory initialize execution performance simulator 
lit includes list lit injections system interrupts needed simulate events dma 
lit includes entire snapshot simulate user kernel instructions wrong path instructions 
table shows benchmark suites 
evaluated runahead execution gain ipc improvement perfect cache 
benchmarks comprising 
lit ia instructions long carefully selected representative ofthe benchmark 
stated averages harmonic averages 
description suite bench 
sample benchmarks spec cpu vortex fp specfp fp specfp specint int specint internet web specjbb multimedia mm mpeg speech recognition quake productivity prod server serv tpc workstation ws cad verilog table 
simulated benchmark suites 
performance simulator execution driven simulator models superscalar order execution microarchitecture similar ofthe intel pentium processor 
simulator includes detailed memory subsystem fully models buses bus contention 
evaluate runahead baselines 
wide machine microarchitecture parameters similar intel pentium processor call current baseline second aggressive wide machine pipeline twice deep buffers times large ofthe current baseline call baseline table gives parameters baselines 
note baselines include stream hardware prefetcher 
noted results relative baseline prefetcher 

results proceedings ninth international symposium high performance computer architecture hpca ieee evaluate runahead prefetching performs compared stream hardware prefetcher 
shows ipc different machine models 
suite bars left right correspond model prefetcher runahead current baseline prefetcher model stream prefetcher runahead current baseline model runahead prefetcher model stream prefetcher runahead current baseline runahead 
percentage numbers top ofthe bars ipc improvements execution model current baseline model 
instructions cycle parameter current processor frequency ghz ghz fetch issue retire width branch misprediction penalty stages stages instruction window size scheduling window size int mem fp int mem fp load store buffer sizes load store load store functional units int mem fp int mem fp branch predictor entry bit history perceptron entry bit history perceptron hardware data prefetcher stream streams stream streams trace cache way way memory disambiguation perfect perfect memory subsystem data cache kb way byte line size kb way byte line size data cache hit latency cycles cycles data cache bandwidth gb accesses cycle tb accesses cycle unified cache kb way byte line size mb way byte line size unified cache hit latency cycles cycles unified cache bandwidth gb gb bus latency processor cycles processor cycles bus bandwidth gb gb max pending bus transactions table 
parameters current baselines 
runahead runahead current baseline runahead runahead fp int web mm prod serv ws avg suite 
runahead current model 
model runahead outperforms model benchmark suites spec 
means runahead effective prefetching scheme stream hardware prefetcher benchmarks 
model runahead ipc outperforms model runahead ipc 
outperforms model ipc 
model best performance leverages hardware prefetcher runahead ipc 
model higher ipc model runahead 
higher ipc current baseline 
ipc improvement current baseline ranges spec suite workstation suite 
proceedings ninth international symposium high performance computer architecture hpca ieee 
interaction runahead hardware data prefetcher execution implemented machine hardware prefetcher prefetcher tables trained new prefetch streams created processor runahead mode 
runahead memory access instructions generate prefetches data need trigger hardware data prefetches 
triggered hardware data prefetches useful initiated earlier machine runahead 
prefetcher accurate runahead execution machine prefetcher usually performs best 
prefetcher inaccurate may degrade performance improvement ofa processor runahead 
prefetches generated runahead instructions inherently quite accurate instructions program path 
traces performance degraded due execution 
ipc improvement execution ranges baseline prefetcher 
range baseline stream prefetcher 
section show behavior applications demonstrate different patterns interaction runahead execution hardware data prefetcher 
shows ipcs ofa selection specint benchmarks models 
number top ofeach benchmark denotes percentage ipc im provement model 
gcc mcf vortex mgrid swim runahead execution hardware prefetcher improve ipc 
combined performance benchmarks better technique 
instructions cycle runahead runahead base runahead runahead gcc ammp apsi swim benchmark 
prefetcher runahead interaction 
having prefetcher machine implements runahead execution degrades performance 
twolf ammp examples ofthis case 
twolf prefetcher causes bandwidth contention sending useless prefetches 
ammp prefetcher sends inaccurate prefetches causes ipc machine runahead streaming prefetcher ofa machine runahead prefetcher 

runahead large instruction windows section shows prefetching benefit runahead execution processor attain performance ofa machine larger instruction window 
shows ipcs different models 
leftmost bar ofeach suite ipc ofthe current baseline runahead 
bars show machines runahead entry instruction windows respectively 
sizes buffers ofthese large window machines scaled instruction window size 
percentages top ofthe bars show ipc improvement ofthe runahead machine machine entry window 
average runahead entry window baseline ipc greater model entry window 
ipc ofa machine entry window 
suites specint workstation runahead current baseline performs better model entry window 
spec runahead entry window lower ipc machine entry window 
due fact suite contains floating point benchmarks long latency fp operations 
entry window tolerate latency operations better instructions cycle current baseline runahead entry instruction window model entry instruction window model entry instruction window model entry instruction window fp int web mm prod serv ws avg suite 
performance runahead versus models larger instruction windows 
entry 
runahead execution current implementation offer solution tolerating latency operations 
note behavior ofthe specfp suite different 
specfp runahead performs better model entry window traces suite memory limited fp operation limited 

effect better frontend proceedings ninth international symposium high performance computer architecture hpca ieee reasons machine aggressive frontend increase performance benefit 
machine better instruction supply increases number executed runahead increases likelihood 

machine better branch prediction decreases likelihood ofan inv branch mispredicted runahead 
increases likelihood correct path prefetches 
effect moves divergence point time runahead 
ifa mispredicted inv branch encountered runahead mode doesn necessarily mean processor generate useful prefetches program path 
processor reach control flow independent point continues correct program path 
may case data shows usually better eliminate divergence points 
averaged traces number instructions pseudo retired divergence point runahead mode entry number instructions pseudo retired divergence point 
number ofl requests generated divergence point runahead entry number divergence point 
far fewer prefetches instruction generated divergence point 
shows performance runahead execution frontend machine ideal 
models perfect trace cache model machine misses trace cache trace cache limited fetch breaks supplies maximum instruction fetch bandwidth possible 
model traces formed real branch predictor 
instructions cycle current baseline runahead perfect trace cache perfect trace cache runahead perfect branch prediction perfect trace cache perfect branch prediction perfect trace cache runahead fp int web mm prod serv ws avg suite 
performance runahead frontend machine gets ideal 
frontend ideal performance improvement execution increases 
suites specint ipc improvement bars shown percentage bars larger ipc improvement bars 
mentioned runahead execution improves ipc current baseline 
runahead current baseline perfect trace cache real branch predictor improves ipc machine 
furthermore runahead current baseline perfect trace cache branch prediction improves performance machine 
machine perfect branch prediction perfect trace cache number retired instructions runahead entry increases number requests generated increases 

effect runahead cache section shows importance store data communication runahead execution 
evaluate performance difference model uses byte way set associative runahead cache byte lines versus model perform memory data forwarding pseudo retired runahead stores dependent loads performs data forwarding store buffer 
model instructions dependent pseudo retired stores marked inv 
model assume communication bits memory handled correctly magically hardware performance cost gives unfair advantage model 
advantage machine perform store load data communication memory runahead mode loses performance benefit runahead execution 
shows results 
suites specfp inhibiting store load data communication memory significantly decreases performance gain runahead execution 
performance improvement runahead runahead cache versus runahead cache 
suites specfp workstation ipc improvement model runahead cache remains 
improvement ranges spec specfp 
runahead cache study correctly handles stores dependent loads 
may possible achieve similar performance smaller runahead cache tune parameters 
instructions cycle current baseline runahead runahead cache runahead runahead cache fp int web mm prod serv ws avg suite 
performance runahead runahead cache 

runahead execution model proceedings ninth international symposium high performance computer architecture hpca ieee shows ipc ofthe baseline machine baseline runahead baseline perfect cache 
number top bar percentage ipc improvement due adding runahead baseline 
due long simulation times benchmarks art mcf excluded evaluation model 
runahead execution improves performance ofthe baseline increasing average ipc 
data shows runahead effective wider deeper larger machine 
instructions cycle baseline runahead perfect fp int web mm prod serv ws avg suite 
runahead model 
effect runahead machine better frontend pronounced larger machine model shown 
number top ofeach bar percentage improvement due runahead 
average ipc baseline perfect instruction supply ipc ofthe baseline perfect instruction supply runahead higher 
due fact branch mispredictions fetch breaks adversely affect number instructions processor pre execute runahead mode wider larger machine 
instructions cycle baseline perfect tc perfect bp baseline perfect tc perfect bp runahead baseline perfect tc perfect bp perfect fp int web mm prod serv ws avg suite 
effect perfect frontend runahead performance model 

main memory latency significant performance limiter processors 
building instruction window large tolerate full main memory latency difficult task 
runahead execution achieves performance larger window preventing window stalling long latency operations 
instructions executed runahead mode provide useful prefetching improves ipc performance aggressive baseline processor 
baseline includes hardware prefetcher improvement addition improvement provided hardware prefetching 
extension mechanism described investigating aggressive runahead execution engine dynamically extend length runahead periods ofthe information exposed runahead mode 
architecting mechanism communicate information runahead normal modes hope ofthe potential runahead execution allowing build processors having resort larger instruction windows 
tom kim konrad lai srikanth srinivasan anonymous referees help insights comments folks desktop platforms group built provided simulator research 
members ofthe hps intel labs research groups fertile environments help create 
supported internship provided intel 
proceedings ninth international symposium high performance computer architecture hpca ieee gonz lez gonz lez smith 
dual path instruction processing 
proceedings international conference supercomputing 
baer chen 
effective chip preloading scheme reduce data access penalty 
proceedings supercomputing 
dwarkadas 
dynamically allocating processor resources nearby distant ilp 
proceedings th annual international symposium computer architecture 
benchmarks 
www com 
callahan kennedy porterfield 
software prefetching 
proceedings th international conference architectural support programming languages operating systems 
chappell stark kim reinhardt patt 
simultaneous subordinate 
proceedings th annual international symposium computer architecture 
emer 
memory dependence prediction store sets 
proceedings th annual international symposium computer architecture 
collins tullsen wang shen 
dynamic speculative precomputation 
proceedings th annual acm ieee international symposium microarchitecture 

content sensitive data prefetching 
university boulder 
mudge 
improving data cache performance pre executing instructions cache proceedings international conference supercomputing 

improving processor performance dynamically pre processing instruction stream 
phd thesis university 

intel uses decoupled superscalar design 
microprocessor report feb 
hinton sager boggs 
microarchitecture ofthe pentium processor 
intel technical journal feb 
issue 
intel 
intel pentium processor optimization manual 
jim nez lin 
dynamic branch prediction perceptrons 
proceedings seventh ieee international symposium high performance computer architecture 
joseph grunwald 
prefetching markov predictors 
proceedings th annual international symposium computer architecture 
jouppi 
improving direct mapped cache performance addition ofa small fully associative cache prefetch buffers 
proceedings th annual international symposium computer architecture 
kessler 
alpha microprocessor 
ieee micro 

lockup free instruction fetch prefetch cache organization 
proceedings th annual international symposium computer architecture 
lebeck li rotenberg 
large fast instruction window tolerating cache misses 
proceedings th annual international symposium computer architecture 
proceedings ninth international symposium high performance computer architecture hpca ieee 
luk 
tolerating memory latency pre execution simultaneous multithreading processors 
proceedings th annual international symposium computer architecture 

luk mowry 
compiler prefetching recursive data structures 
proceedings th international conference architectural support programming languages operating systems 
sohi 
streamlining inter operation memory communication data dependence prediction 
proceedings th annual acm ieee international symposium microarchitecture 
mowry lam gupta 
design evaluation compiler algorithm prefetching 
proceedings th international conference architectural support programming languages operating systems 

increasing processor performance implementing deeper pipelines 
proceedings th annual international symposium computer architecture 
standard performance evaluation 
welcome spec 
www spec org 

www com 
ziff davis media benchmarks 
www com benchmarks 
wilkes 
slave memories dynamic storage allocation 
ieee transactions electronic computers 

mips superscalar microprocessor 
ieee micro apr 
zilles sohi 
execution prediction speculative slices 
proceedings th annual international symposium computer architecture 
