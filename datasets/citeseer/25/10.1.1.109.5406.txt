vldb journal digital object identifier doi fast accurate text classification multiple linear discriminant projections soumen chakrabarti roy mahesh iit bombay mail soumen cse iitb ac edited received september accepted march published online july springer verlag 
support vector machines svms shown superb performance text classification tasks accurate robust quick apply test instances potential drawback training time memory requirement training instances held memory best known svm implementations take time proportional typically 
svms trained data sets instances web directories today contain millions instances valuable mapping billions web pages yahoo directories simpl nearly linear time classification algorithm mimics strengths svms avoiding training bottleneck uses fisher linear discriminant classical tool statistical pattern recognition project training instances carefully selected low dimensional subspace inducing decision tree projected instances 
simpl uses efficient sequential scans sorts comparable speed memory scalability widely naive bayes nb classifiers beats nb accuracy decisively approaches exceeds svm accuracy beats running time popular svm implementation orders magnitude describing simpl detailed experimental comparison svm generated discriminants fisher discriminants report analysis cache performance popular svm implementation analysis shows simpl potential method choice practitioners want accuracy svms simplicity speed naive bayes classifiers 
keywords text classification discriminative learning linear discriminants text classification studied problem document management classifier learner training documents assigned label drawn possible labels depending application label may indicate property document news article sustainable energy email spam learner processes training documents generally collecting term statistics estimating various model parameters test instances label learner choose labels test document 
application demands labels directory broad topics top level common train learner topic documents marked labeled documents labeled called vs rest classification :10.1.1.11.6124:10.1.1.161.6020
text classification numerous potential applications including automatic maintenance topic directories open directory called dmoz see dmoz org yahoo 
www yahoo com filtering email spam collaborative filtering naive bayes nb maximum entropy maxent support vector machines svms best known classifiers employed date text data :10.1.1.11.6124:10.1.1.161.6020
surprisingly trade simplicity accuracy nb classifiers simple understand easy implement access disk resident data efficiently run fast may show mediocre accuracy svms accurate classifiers known text applications beat nb accuracy decisive margin generally better maxent classifiers nb classifiers tend score lower maxent classifiers terms accuracy accuracy differences intriguing svm maxent nb classifiers learn hyperplane separates positive examples negative ones documents represented vectors high dimensional term space 
svm maxent undertake complex nonlinear numeric optimizations highly nontrivial understand implement search high quality separator nb quick generally inferior choice 
nb takes time essentially linear number training documents svms take time proportional typically 
clever implementations svms trained instances despite near quadratic com chakrabarti text classification linear projections plexity scaling hundreds thousands instances appears infeasible time memory footprint issue popular svm packages store training vectors memory exceptions noted sect scalability memory footprint critical issues enormous training sets increasingly available :10.1.1.43.4376
web directories open directory 
contain millions training instances occupy tens gigabytes high servers limited gb ram sparsity data text domain sampling training data dangerous sample may exclude thousands useful features joachims shows large fraction terms reveal useful class information additional training document potentially source features confirm sect see ability scale better translates better accuracy time trade :10.1.1.11.6124
summary despite theoretical elegance superiority svms io behavior cpu scaling important concerns need easy implement text classifiers match simplicity efficiency nb classifiers giving accuracy comparable svms 
contribution design implement evaluate new simple text classification algorithm requires little ram deals gracefully ram training data accesses strictly linearly beats nb accuracy decisively matches svm accuracy main idea find series projections training data fisher classical linear discriminant sect subroutine project training instances low dimensional subspace previous step induce decision tree projected low dimensional data call general framework simpl simple iterative multiple projection lines simpl important features small footprint linear number terms dimensions plus number documents fast sequential scans input cpu time linear total size training data expressed simply terms joins sorts group operations parallelized easily 
give quick impression simpl implemented lines code trained document collection seconds svm took seconds 
undertake careful comparison simpl svm regard accuracy performance 
find spite simplicity efficiency simpl comparable superior svm terms accuracy theoretical bound number linear projections simpl may need projections usually achieve high accuracy 
ability scale training sets larger main memory key concern data mining community simpl available www cse iitb ac soumen 
resulted excellent core implementations traditional classifiers decision trees years machine learning text mining communities evolved powerful classifiers svms maxent classifiers scaling io behavior new important class svm learners clearly understood carefully study performance popular svm implementation accessing documents lru cache having limited size svm implementation cache size comparable ram required simpl spends significant portion time servicing cache misses performance gap simpl svm implementation grows :10.1.1.104.152
related aware hybrid learning strategy similar proposal ideas discuss early hints projection approach promising theorem frankl showed projection set points random subspace dimension log preserves factor relative interpoint distances high probability established random projection valuable general technique dealing high dimensional data kleinberg projected points log randomly directed lines answer approximate nearest neighbor queries efficiently 
dasgupta random projection learn mixture gaussians showing en route separated gaussians remain separated projection 
classification task need preserve distances carefully simply need subspace separates positive negative instances special case projection pursuits early study sch tze hull pedersen single linear discriminants compared favorably neural networks document routing problem :10.1.1.21.466
lewis reported accurate prediction variety regression strategies single linear predictors success linear svms adds evidence projections adequate text domain 
shashua established decision surface linear svm fisher discriminant support vectors see sect svm result directly yield better svm algorithm gave basic intuition idea 
closely related linear discriminants svms discuss detail sect sect independently cooke suggested discarding separated training points finding fisher linear discriminant multiple projections generate surrogate representation powerful learning algorithm decision tree 
researchers worked reducing memory footprint running time svm optimizations strategy pursued mangasarian coworkers change objective function slightly enables efficient mathematical programming machinery affecting utility solution practice lagrangian svm proximal svm incremental svm examples paradigm incremental svm fact retire add new chakrabarti text classification linear projections training data efficiently svm variants involve inverting matrix number dimensions readily done moderately large values hundreds thousands demands main memory time text domain uncommon inverted matrix generally sparse 
techniques family successive relaxation svm reduced svm sampling technique may compare favorably simpl apart theoretically elegant pavlov mao dom developed different sampling technique 
simpl may interpreted approximation boosting boosting seeks improve weak learner decisions slightly better random guessing running times successively altered training distributions training distribution generated training data assigning equal probability instance subsequent distributions generated boosting probability instances weak learner labeled incorrectly learner sequence gets score error rate learner weighted majority set weak learners weights depend scores weak learners simpl simply throws away correctly learned points uses complex combination weak learners 
approach related oblique decision trees try find nonorthogonal hyperplane cuts decision tree setting inducing ordinary decision tree raw term space large document collection extremely time consuming draw complex hypothesis space decision trees arrangement simplicial polytopes involve regression potentially dimensions node decision tree consequently simpl faster induction somewhat faster apply test instances need compute small fixed number projections usually simpl accurate orthogonal decision trees comparison may worthwhile trained reasonable time high dimensional data 
preliminaries host linear classifiers text classification document represented feature vector component dt term vocabulary overload mean confusion generally occurs larger value dt linear classifier characterized vector scalar constant predicts class sign indicates dot product sign sign sign 
linear classifiers document classification lines broad topics document game cricket tend terms run ball pitch frequently tend compounding evidence document cricket 
represent hyperplane cuts axis corresponding term offset offset acts threshold term occurs frequently document assigned label general linear combination frequencies important terms exceeds threshold document lie positive side hyperplane 
discussion may help explain linear discriminants text classification 
naive bayes nb classifiers bayesian classifiers estimate class conditional document distribution pr training documents bayes rule estimate pr test documents documents modeled terms multinomial naive bayes model assumes document bag multiset terms term counts generated multinomial distribution fixing document length fixed document lets write pr number times occurs suitably estimated multinomial probability parameters class scenario need compare pr pr equivalently log pr log pr simplifies comparison log pr log log pr log pr called class priors fractions training instances respective classes simplifying eq see nb linear classifier decision thresholding value nb suitable vector nb depends parameters constant overloaded denote vector term frequencies see sect denotes dot product 
maximum entropy classifiers bayesian classifiers estimate class conditional distributions pr class maxent classifier directly estimates parametric model pr model parameter class term nb commonly parametric form pr pr introducing normalizing constant pr add weget pr nigam discuss detail optimize parameters training data class case maxent chakrabarti text classification linear projections classifier classify test document amounts comparing logs log log log log clearly linear discriminant 
regression techniques regard classification problem inducing linear regression form estimated data di ci 
view common variety information retrieval ir applications common objective minimize square error observed predicted class variable square optimization frequently uses gradient descent methods widrow hoff wh update rule vector augmented extra element set corresponding extra dimension added simplify notation get rid wh approach starts estimate extra dimension representing considers di ci updates follows di ci di 
final classification usually average way sch tze lewis applied wh update methods exponentiated gradient method design high accuracy linear classifiers text improving traditional rocchio style relevance feedback follow wh approach minimize square error dependent single linear predictor goal maximize separation classes projected subspace optimize fisher linear discriminant :10.1.1.21.466
linear support vector machines nb linear svms decision thresholding svm estimated class depending quantity greater suitable vector svm constant svm chosen far carefully nb initially assume training points classes linearly separable hyperplane perpendicular suitable svm seeks maximizes distance training point hyperplane written minimize subject ci di dn training document vectors cn corresponding classes want sign di ci product positive distance training point optimized hyperplane called margin 
handle general case single hyperplane may able correctly separate training points variables introduced eq enhanced minimize subject ci di sum violations misclassified training points traded margin width tuned constant svm packages solve dual eq involving scalars maximize di dj subject ci having optimized recovered svm di support vector estimated cj svm dj dj document common set radius smallest ball containing training vectors average euclidean norm vectors fixed choice saves time rarely best possible value practice principle best value fall training process practice tuning helps practitioners generally held validation data set search large range values smaller range values means run time consuming svm induction program times natural want avoid extra compare simpl versions svm tuned search call svm best 
equation represents quadratic optimization problem 
svm packages iteratively refine time called working set holding fixed small training sets precompute store inner products di dj scaled example average document costs bytes ram documents corpus size bytes inner products stored byte floats occupy bytes times corpus size inner products computed demand lru cache values reduce recomputation svm implementations know document vectors kept memory inner products quickly recomputed necessary observation excludes svm variants need quadratic programming ones discussed sect 
observations leading approach natural question observed difference accuracy nb classifiers linear svms chakrabarti text classification linear projections hypothesis space half spaces assuming attribute independence nb starts large inductive bias loosely speaking constraint guided training data space separating hyperplanes draw 
svms propose generative probability distribution data points suffer form bias weakness nb classifier parameters sample means takes variance fisher discriminant take variance account see fig 
linear svm carefully finds single discriminative hyperplane consequently instances projected direction svm normal hyperplane show large perfect separation intuitively hope slightly sloppy compared svms finding discriminative directions provided quickly find number directions collectively help decision tree learner separate classes projected space achieve speed scalability possible cast computation terms efficient sequential scans data small number accumulators collecting sufficient statistics 
proposed algorithm proposed training algorithm broad outline shown outer loop simpl finds linear discriminants compute need hill climbing step prepare stage need remove instances training set 
explain rationalize steps section 
testing simple essentially fast nb svm 
preserve linear discriminants decision tree practice adequate test document find dimensional representation submit vector decision tree classifier outputs class 
hill climbing step points fisher linear discriminant unit vector positive negative training instances projected direction separated possible separation shown equation quantified ratio square difference projected means sum projected variances 
matrix notation means centroids covariance matrices point sets best linear discriminant closed form pattern recognition arg max provided matrix inverse exists 
inverting covariance matrix impractical document classification domain large ill conditioned inversion discard sparsity gradient ascent hill climbing approach start reasonable starting value repeatedly find denoting numerator respectively denominator rhs eq respectively dx dy easily write dx dy shown fig 
values easily derive value term vocabulary find standard wh update rule learning rate current svm setting learning rate practiced science art small slows learning large may lead instability divergence neural networks researchers adapt rate online training progresses :10.1.1.21.466
experience wh users published web tried values settled 
single fixed value gave results essentially get tuning separately data set 
fix convergence policy simpl repeat hill climbing increase successive iterations policy protects mild oscillations near local optimum 
gist need maintain set accumulators scan documents sequentially scalar scalar xi numbers yi numbers xi numbers yi numbers current numbers 
total memory footprint numbers bytes size vocabulary dmoz data set see sect means need mb ram vectors dense array representations time hill climbing step exactly linear size input data situation available ram smaller bytes expressions fig expressed group aggregate operations executed efficiently limited memory 
pruning training set suitable number hill climbing steps discard points correctly classified current achieved projecting points points line marked sweeping line minimum error position points side points side number points wrong side minimum possible 
easy sort element array sweep extra memory total time identifying correctly classified documents log total space needed 
chakrabarti text classification linear projections initialize set training documents positive negative instance initialize vector direction joining positive class centroid negative class centroid hill climbing find linear discriminant remove instances correctly classified orthogonalize scale norm linear discriminant vectors document vector original training set represent vector projections train decision tree classifier dimensional training vector fig 

proposed algorithm simpl finds linear discriminants hill climbing procedure dx dy dx dy dx dy dx dy xi dx yi xi xi fig 

main equations involved hill climbing step find fisher linear discriminant intuition algorithm quite simple having discriminant retain points fail separated direction hill climbing steps converge local optima bound number need extract practice new helps discard avoid scanning original hill climbing pass write surviving documents new file finding 
set reduces correlation components dimensional representation documents decision trees implement orthogonal cuts mild improvement accuracy orthogonalization step hurt accuracy replace decision tree kind learner step may needed 
inducing decision tree roughly equivalent decision tree packages quinlan algorithm www cse unsw edu au quinlan decision tree package weka simply call weka see www cs waikato ac nz ml weka context decision tree seeks partition set labeled points geometric space vector dk 
decision tree induction algorithm uses series guillotine cuts space expressed comparison component di constant final rectangular region positive negative points hierarchy comparisons induces decision tree leaves correspond final rectangular regions achieve recall precision trade just tune offset svm assign different weights positive negative instances weka 
decision tree pure single class leaves usually overfits training data generalize heldout test data better generalization achieved pruning tree trading complexity tree impurity leaf rectangles terms mixing points belonging different classes large number dimensions decision trees induced raw text show poor accuracy dimensionality reduction projection pays 
weka hold entire training data memory usually problem stage transformed training data points dimensions example documents projections need mb 
space chakrabarti text classification linear projections issue efficient core decision tree implementations sprint :10.1.1.104.152
simpl induces fairly small decision trees number projected coordinates small start rare see cuts projected dimension experiments observe typically depth decision tree total decision nodes tree 
experiments core simpl excluding document scanning preprocessing implemented lines code making generous ansi templates core roughly lines code contrast svmlight popular svm package lines 
compilation programs run pentium iii machines mhz cpus mb ram 
implementations svm widely publicly available sequential minimum optimization smo john platt gary flake www neci nec com homepages flake html svmlight thorsten joachims svmlight comparable better accuracy compared published smo numbers built support massive sparse input data time :10.1.1.43.4376:10.1.1.161.6020
experiments svmlight evaluating svmlight default reciprocal average value range tfidf representation evaluate values suitable band separator settings flags left default values noted 
standard accuracy measures contingency table number estimated class documents actual class recall precision defined respectively 
rp commonly measure classifier may parameters trade vice versa parameters adjusted get value called break point data sets synthetic data set standard real life benchmark data sets synthetic data study properties discriminants svm simpl report precision recall numbers real life data sets 
synthetic data synthetic data generator concept defined joachims concept specified set webkb course pi ni fi feature type high frequency positive high frequency negative hfn medium frequency positive mfp medium frequency negative mfn low frequency positive lfp low frequency negative lfn rest reuters earn pi ni fi feature type high frequency positive high frequency negative hfn medium frequency positive mfp medium frequency negative mfn low frequency positive lfp low frequency negative lfn rest fig 

parameters synthetic data generator tuples pi ni fi ith tuple specifies vocabulary subset having fi terms vocabulary union subsets positive documents pi fi terms subset negative documents ni fi terms subset note term may picked positive negative documents 
joachims argued examples concepts closely model real life text classification tasks estimated parameters known classification benchmarks sect 
shown fig 
experiments 
simple data generator chooses pi respectively ni terms uniformly random replacement fi available features set implies synthetic documents length number words realistic note synthetic data purely compare linear discriminants computed svm simpl controlled circumstances judge accuracy joachims characterization powerful manner proved concepts learnable small testing error making assumptions words picked repeated derived results involving noise term distributions added basic model 
real life data real life data sets known ir literature small size suitable controlled experiments accuracy cpu scaling data sets large approach scale envisage real applications mainly compare run time performance 
chakrabarti text classification linear projections reuters training test documents mod apte split terms categories raw text takes mb 
ng total documents organized directory structure topics topic files listed alphabetically chosen training documents terms raw concatenated text takes mb downloaded kdd ics uci edu databases newsgroups newsgroups html webkb documents categories pages categories faculty project collected universities miscellaneous pages collected universities classification task university pages selected test documents rest training documents raw text mb downloaded www cs cmu edu afs cs cmu edu project theo www data 
ohsumed abstracts medical journals having terms topics available ftp ics uci edu pub machine learning databases ohsumed raw text mb selected training documents rest test documents 
dmoz rdf file published dmoz org picked sample urls successfully crawled stripped html tags leaving plain text number distinct tokens twelve top level topics dmoz populated labeled documents raw text available request occupied mb 
document representation standard multinomial model eq nb standard tfidf document representation ir svm simpl keeping best systems trec trec nist gov idf term log dt document collection dt set documents containing term frequency tf ln ln raw frequency document tf zero 
represented sparse vector tth component idf tf norm document vector scaled submitting classifiers 
data sets sport labels label cocoa class problem cocoa vs cocoa formulated tokens turned lowercase standard smart stopwords relative alpha convergence alpha acq grain ship wheat iterations fig 

hill climbing maximize fast relative values scaled convergence plotted number hill climbing steps ftp ftp cs cornell edu pub smart removed stemming performed feature selection prior running classification algorithms naive bayes classifier rainbow library laplace methods evaluated parameter smoothing svmlight simpl alternatively may preprocess collection common feature selector submit classifier adds fixed time classifier 
hill climbing hill climbing approach fast practical usually settle maximum iterations fig shows quickly grows stabilizes successive iterations 
convergence policy see guards mild problems local maxima overshoots case learning rate eq set best possible choice condition usually manifests small oscillations topics ship wheat results insensitive wide range specific choices parameters 
important concern lack guarantee global optimality hill climbing step suppose hillclimbing process gets trapped local maximum accept value continuing completely upset optimization subsequent lead arbitrarily astray best set difficult find perfect answer question exhaustive search optimal infeasible access ground truth get heuristic evidence robustness simpl calculated different values perform hill climbing convergence may give global local optimum suppose takes iterations 
degrade value deliberately running hillclimbing process iterations 
chakrabarti text classification linear projections rp data sets sloppy acq interest money fx earn crude fig 

quitting hill climbing early little impact accuracy fact cases accuracy improves execution time data set time time acq interest earn crude fig 

topic bar shows time compute convergence time compute convergence second bar shows time compute followed computing convergence earlier take time compute may save time finding subsequent computed usual wish study implications choices accuracy speed simpl 
data sets early termination hill climbing negligible effect accuracy simpl see fig determined compensate finding earlier fact cases precision recall falls may gain accuracy terms resilience variation policy parameters desirable 
shows effect incomplete hill climbing running time iteration hill climbing takes fixed time halving number iterations halves time taken find finding potential related effects fewer training points may eliminated larger number subsequent may required focus run time finding fig 
time find generally increases extra time required subsequent small compared time saved finding save time lop current training set additional find example dmoz data numbers surviving documents finding respectively consequently data sets simpl generates linear projections running training documents 
preceding discussion raises question simpl behave push limit perform hill climbing initializing 
need find computed trivially main experimental observations suitably modified version simpl number surviving training points tends larger hill climbing case hill climbing hill climbing number extra required compared previous number obtained hill climbing 
time saved avoiding hill climbing overwhelms cost finding extra total time smaller hill climbing performed example runs real life data cut total time sets times substantially smaller time taken svm 
generally drop accuracy example runs reduced 
cases hill climbing difference beating vs beating svm 
discriminants projections set measurements shows nature discriminants simpl algorithms quality separation training test data discriminants 
indicator promise simpl general agreement directions svm simpl ing vectors scaled unit norm agreement expressed dot product cosine angle single number perfect perfect correlation visualize scatterplot points svm simpl term note low correlation simpl svm necessarily mean simpl trouble simpl derives directions projection 
fig show scatterplots discriminants svm simpl synthetic data discriminants svm simpl virtually identical typical cosines real data sets contain noise clear scatterplots appear spread apparent density points print misleading quadrants overwhelming majority points compared quadrants 
cosine remains quite large typically may appear small compared keep mind vectors tens thousands dimensions slight noise dimension capable turning vectors away chakrabarti text classification linear projections simpl course simpl webkb student svm svm svm disagree disagree disagree cosine cosine cosine simpl reuters acq fig 

comparison svm simpl disagree means coordinates differed sign coordinates points quadrants points quadrants cosine described text relative density points projection alpha train test money fx lf train test money fx af money fx lt money fx fig 
may fail separate labels training data assistance subsequent general manages separate labels sufficiently clarity separate training test points parts see projections cluster training test data similar ways hinting high accuracy decision trees induced reduced dimension data cosine values indicate substantial agreement discriminants svm simpl 
agreement discriminants gives hope linear svm manages separate projected training data wide margin fisher discriminant achieve separation case fig shows confusion zone project ing training points significant fisher discriminant classification expect simpl accuracy poorer luckily simpl multiple projections shows result projecting training points discriminants confusion zone fig stretched axis better separating training points different labels chakrabarti text classification linear projections prec recall prec recall crude interest alphas alphas prec recall prec recall alphas ship alphas wheat fig 

variation precision recall number linear projections 
projections adequate topics interest money fx topic avg corn wheat trade grain ship crude earn acq avg project course faculty student reuters nb simpl svm svm best nb webkb simpl svm svm best comp os ms windows misc comp sys mac hardware topics ng avg comp graphics sci electronics misc talk religion misc comp windows comp sys ibm pc hardware alt atheism talk politics misc rec motorcycles rec autos rec sport baseball talk politics guns sci space soc religion christian sci crypt rec sport hockey sci med talk politics mideast nb simpl svm svm best fig 

aggregate scores nb simpl svm populated topics reuters ng webkb data sets 
decisively beat nb margin cases frequently beat svm average margin lose svm best narrow margin average cases win svm best chakrabarti text classification linear projections tions generally suffice separate training collections seen 
obviously reassuring know projected test points clustered similar projected training points decision tree may perform poorly projected representations show projections training test data separated different plots classes clearly projected points training test data appear distributed similarly plane 
projections needed retain information achieve high accuracy 
shows effect including cases sufficient peak accuracy general trend precision recall approach include projections improving loss compensated gain result shows improve linear regression sect additional projections reassuring including necessary hurt accuracy 
accuracy simpl measure svm terms accuracy 
shows bird eye view data sets algorithms compares score naive bayes nb simpl svm svm best see sect simpl beats nb cases usually large margins 
simpl beats svm cases average percent short svm best cases marked stars beat svm best extent tuning offset parameter improved svm surprised reported 
tuning effect 
note comprehensive study diverse standard data sets high accuracy simpl stable board 
harmonic mean favors algorithms recall close precision case simpl fair closer look shows simpl usually loses small margin recall precision beats fig 
cases beat svm recall precision possible limited single planar separator decision tree projected space learn say function exor don expect text classification task pose challenge linear svm svms complex kernels may slower train linear svms 
compared accuracy run directly raw text reported earlier see accuracy substantially better novel feature combination transformation steps addition simpl runs faster raw data :10.1.1.11.6124:10.1.1.161.6020
show scatterplot scores positive class skew ratio number documents number documents fig methods suffer somewhat skew clearly nb suffers simpl suffers 
talk politics misc talk politics mideast sci med rec motorcycles wheat money fx crude svm best simpl recall precision fig 

lose svm best small margin recall precision beat stars mark win nb simpl svm acq skew fig 

simpl shows adverse effect class skew accuracy naive bayes nb simpl svm performance having established accuracy comparable svm turn detailed investigation scalability io behavior algorithms part svm means svmlight comparison svm variants discussed sect fruitful area 
include initial time required turn raw training documents compact representation required sequential scans case simpl lru cache required svm started timing algorithms initial disk structures ready time consumed preprocessing depends host nonstandard factors raw representation system policies single vs files stopword detection word stemming truncation term weighting ohsumed dmoz data performance measurements report dmoz ohsumed broadly similar 
compare scaling cpu time training set size assuming ram available observe fig svm number iterations needed hill climbing step largely independent number chakrabarti text classification linear projections iterations svm iterations training docs simpl simpl training docs fig 

number iterations needed run svm runs simpl finding number svm iterations scale problem size simpl time svm time simpl time simpl time cpu scaling relative sample size fig 

scaling cpu time excluding preprocessing time training set size svm simpl keeping training data memory line marked simpl time shows time finding just line marked simpl shows total time simpl sample documents chosen dmoz iterations iters time iter max new vars iter time ms iter fig 

cache misses time iteration increase number new variables optimized iteration capped drastic savings number iterations reduces time corpus cache sizes documents training documents time taken single hill climbing iteration linear total input size defined plus log documents log small time sorting small compared update step total time simpl expected essentially linear 
confirmed log log plot shown fig square fit simpl roughly time docs cache cpu hit evict fig 

cpu time cache hit time time eviction time plotted relative size cache available svm time simpl svm sample fraction fig 

svm simpl amount ram scale size training set far better execution time contrast regression running time svm corroborates earlier measurements platt joachims difference translates running time ratio svm simpl orders magnitude small collections scale yahoo 
full dmoz data set millions documents ratio reach orders magnitude 
public quadratic programming svm implementations know including svmlight smo load training document vectors memory limited memory expect performance gap svm simpl larger final set experiments study behavior svm limited memory 
mentioned sect svm optimizes corresponding small working set documents time 
svmlight size working set typically set option standard heuristics picking working set speed convergence applying may replace entire working set reducing cache locality 
svmlight provides option limits number new permitted enter working set iteration reducing increases cache locality lead iterations sample trade shown chakrabarti text classification linear projections top computers top health simpl relative time simpl relative time top recreation simpl relative time fig 

simpl scales better svmlight achieve comparable svmlight shorter time match time svmlight sample training data reduces accuracy dmoz topics plot vs relative execution time fig heavy replacement rate decreases cache hits increases time iteration number iterations cut drastically large value better choice 
cache hits misses translate running time overheads 
measured limiting physical memory available computer linux line form append mem conf letting svm document cache management instrumenting cache option appealing user system processes os buffer cache device interrupts paging measurements unreliable os cache physical exploit structure svm computation expect large scale svm packages implement caching 
disk file system raw block device ata rpm seagate st drive mb board cache dev hdc precluded interference owing os buffering built lru cache main memory quota servicing usually involved exactly seek disk data disk board cache 
shows break times spent computation cpu hit eviction processing sample dmoz documents mb sparse term vectors features simpl needs mb ram runs close cpu utilization contrast svm mb cache documents takes seconds spent servicing evictions misses 
extending small scale experiment fig large scale experiment go sample dmoz data documents mb ram sample determine amount ram needed simpl give quantity cache svm compare running times graph superficially similar fig closer look shows ratio svm simpl running times larger owing cache overheads summarizing simpl beats svm respect cpu cache performance near quadratic cpu scaling svm cache overheads appear serious really total time spent simpl time spent svm cache management 
show examples trade accuracy running time simpl quadratic programming svms fig consider top level topics dmoz plots show accuracy training time points generated uniform random samples different sizes entire training set documents 
svmlight approaches best large value execution time simpl achieves peak short execution times handle larger samples time obviously smarter sampling techniques fare better uniform sampling compared simpl 
simpl new classifier highdimensional data text simpl simple understand easy implement simpl fast scaling linearly input size opposed svm involving quadratic programming shows quadratic scaling simpl uses efficient sequential scans training data popular svm implementations efficient disk cache access patterns poorer locality 
performance boost carries little penalty terms accuracy beat svm measure closely match svm recall precision simpl beats naive bayes decision tree classifiers decisively text learning tasks 
shows svms elegant powerful theoretically appealing rendered search practical io efficient alternatives unnecessary fruitless natural area identify properties data sets guarantee near svm accuracy simpl area applied test simpl vis avis nonlinear svm training data difficult separate text interest compare simpl svm variants success simpl guide search optimization process svms 

pedro domingos gary flake vladimir vapnik helpful discussions thorsten joachims generous help svmlight help preparing data sets helpful comments manuscript research partially supported tata consultancy services ibm 
chakrabarti text classification linear projections agrawal bayardo rj srikant athena interactive management text databases proceedings th international conference extending database technology edbt konstanz germany march 
www almaden ibm com cs people papers athena ps basu hirsh cohen ww recommendation classification social content information recommendation proceedings th national conference artificial intelligence madison wi july pp chakrabarti dom agrawal raghavan scalable feature selection classification signature generation organizing large text databases hierarchical topic taxonomies 
vldb www cs berkeley edu soumen vldb pdf cooke variations fisher linear discriminant pattern recognition ieee trans patt analysis machine intell pami www computer org tpami tp abs htm dasgupta learning mixtures gaussians focs pp charlotte ucsd edu users dasgupta papers focs ps dasgupta experiments random projection uai charlotte ucsd edu users dasgupta papers random ps duda hart pattern classification scene analysis 
wiley new york dumais platt heckerman sahami inductive learning algorithms representations text categorization proceedings th conference information knowledge management 
www research microsoft com cikm pdf frankl johnson lindenstrauss lemma sphericity graphs combin theory friedman jh exploratory projection pursuit am stat assoc fung mangasarian ol proximal support vector classifiers provost srikant eds proceedings th acm sigkdd international conference knowledge discovery data mining san francisco august pp university wisconsin data mining institute technical report www cs wisc edu fung mangasarian ol incremental support vector machine classification proceedings nd siam international conference data mining arlington va april pp university data mining institute technical report ftp ftp cs wisc edu pub dmi tech reports ps graefe fayyad um chaudhuri efficient gathering sufficient statistics classification large sql databases knowledge discovery data mining vol 
aaai press new york pp joachims text categorization support vector machines learning relevant features dellec rouveirol eds proceedings ecml th european conference machine learning lecture notes computer science vol springer berlin heidelberg new york pp joachims making large scale svm learning practical 
sch lkopf burges smola eds advances kernel methods support vector learning mit press cambridge ma www ai cs uni dortmund de joachims pdf joachims statistical learning model text classification support vector machines croft wb harper dj kraft dh zobel eds proceedings international conference research development information retrieval vol new orleans september acm press new york pp johnson ra dw applied multivariate statistical analysis rd edn prentice hall new delhi kleinberg jm algorithms nearest neighbor search high dimensions proceedings acm symposium theory computing pp lecun simard py automatic learning rate maximization line estimation hessian eigenvectors hanson sj cowan jd lee giles eds advances neural information processing systems vol morgan kaufmann san mateo ca pp lee yj mangasarian ol reduced support vector machines proceedings st siam international conference data mining chicago april 
www siam org meetings sdm pdf sdm pdf lewis dd reuters text categorization test collection 
kdd ics uci edu databases reuters reuters html lewis dd schapire re callan jp papka training algorithms linear text classifiers frei hp harman sch wilkinson eds proceedings sigir th acm international conference research development information retrieval acm press new york pp mangasarian ol dr successive overrelaxation support vector machines ieee trans neural netw ftp ftp cs wisc edu math prog tech reports ps mangasarian ol dr lagrangian support vector machines technical report data mining institute university wisconsin madison june 
www cs wisc edu mccallum bow toolkit statistical language modeling text retrieval classification clustering software available www cs cmu edu mccallum bow mccallum nigam comparison event models naive bayes text classification aaai icml workshop learning text categorization aaai press pp technical report ws cmu www cs cmu edu papers multinomial pdf 
murthy sk kasif salzberg system induction oblique decision trees artif intell res nigam lafferty mccallum maximum entropy text classification ijcai workshop machine learning information filtering pp 
www cs cmu edu www cs cmu edu mccallum papers maxent ps gz pavlov mao dom scaling support vector machines boosting algorithm proceedings international conference pattern recognition icpr barcelona september 
www cvc uab es icpr chakrabarti text classification linear projections platt sequential minimal optimization fast algorithm training support vector machines 
technical report msr tr microsoft research 
www research microsoft com users pdf sahami dumais heckerman horvitz bayesian approach filtering junk mail learning text categorization papers workshop madison wi aaai technical report ws schapire re boosting approach machine learning overview proceedings msri workshop nonlinear estimation classification berkeley ca march 
stat bell labs com nec www research att com schapire boost html schutze hull da pederson jo comparison classifiers document representations routing problem sigir pp 
ftp xerox com pub sigir ps shafer jc agrawal mehta sprint scalable parallel classifier data mining vldb pp shashua equivalence support vector machine classification fisher linear discriminant neural processing lett www cs huji ac il shashua papers fisher npl pdf df cook buja xgobi interactive dynamic data visualization window system computat graph stat lib stat cmu edu general xgobi vapnik smola aj support vector method function approximation regression estimation signal processing advances neural information processing systems mit press cambridge ma witten ih frank data mining practical machine learning tools techniques java implementations morgan kaufmann san francisco 
