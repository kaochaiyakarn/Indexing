arxiv cs cs cl sep boosting trees anti spam email filtering xavier carreras research center lsi department universitat polit cnica de catalunya upc jordi girona barcelona catalonia carreras lsi upc es describes set comparative experiments problem automatically filtering unwanted electronic mail messages 
variants adaboost algorithm confidence rated predictions schapire singer applied differ complexity base learners considered 
main drawn experiments boosting methods clearly outperform baseline learning algorithms naive bayes induction decision trees pu corpus achieving high levels measure increasing complexity base learners allows obtain better high precision classifiers important issue misclassification costs considered 
spam mail filtering problem automatically filtering unwanted electronic mail messages 
term spam mail commonly referred junk mail unsolicited commercial mail 
nowadays problem achieved big impact bulk take advantage great popularity electronic mail communication channel indiscriminately flooding email accounts unwanted advertisements 
major factors contribute proliferation unsolicited spam email bulk email inexpensive send pseudonyms inexpensive obtain cranor lamacchia 
contrary individuals may waste large amount time transferring unwanted messages computers sorting messages transferred point may overwhelmed spam 
automatic ir methods suited addressing problem spam messages distinguished legitimate email messages particular form vocabulary word patterns header body messages 
spam filtering problem seen particular instance text categorization problem tc classes possible spam legitimate 
opposite seen problem identifying single class spam 
way evaluation automatic spam filtering systems done common measures ir precision recall 
important issue relative importance types possible misclassifications automated filter misses small percentage spam may acceptable users fewer people accept filter incorrectly identifies small percentage legitimate mail spam especially implies automatic discarding legitimate messages 
problem suggests consideration misclassification costs learning evaluation spam filter systems 
years vast amount techniques applied tc achieving impressive performances cases 
top performing methods ensembles decision trees weiss support vector machines joachims boosting schapire singer instance learning yang liu 
spam filtering treated particular case tc 
cohen method tf idf weighting rule learning algorithm ripper classify filter email 
sahami naive bayes approach filter spam email 
drucker compared support vector machines svm boosting trees ripper rocchio concluding svm boosting top performing methods suggesting svm slightly better distinguishing types misclassification 
androutsopoulos sahami naive bayes compared timbl memory learner 
androutsopoulos authors new public data set standard benchmark corpus introduce cost sensitive evaluation measures 
show adaboost algorithm confidence rated predictions suited algorithm addressing spam filtering problem 
obtained accurate classifiers pu corpus observed algorithm robust overfitting 
advantage adaboost prior feature filtering needed able efficiently manage large feature sets tens thousands 
second part show increasing expressiveness base learners exploited obtaining high precision filters needed real user applications 
evaluated results filters measures introduced androutsopoulos take account misclassification costs 
substantially improved results mentioned 
organized follows section devoted explain adaboost learning algorithm variants comparative experiments 
section setting detail including corpus experimental methodology 
section reports experiments carried results obtained 
section concludes outlines lines research 
adaboost algorithm section adaboost algorithm schapire singer described restricting case binary classification 
purpose boosting find highly accurate classification rule combining weak rules weak hypotheses may moderately accurate 
assumed existence separate procedure called acquiring weak hypotheses 
boosting algorithm finds set weak hypotheses calling weak learner repeatedly series rounds 
weak hypotheses linearly combined single rule called combined hypothesis 

xm ym set training examples instance xi belongs instance space yi class label associated xi 
goal learning produce function form example sign interpreted predicted class magnitude interpreted measure confidence prediction 
function classifying ranking new unseen examples 
pseudo code adaboost 
maintains vector weights distribution examples 
goal algorithm find weak hypothesis moderately low error respect weights 
initially distribution uniform boosting algorithm exponentially updates weights round force weak learner concentrate examples hardest predict preceding hypotheses 
precisely dt distribution round ht weak rule acquired dt 
setting weak hypotheses ht real valued confidence rated predictions sign ht predicted class ht interpreted measure confidence prediction 
parameter chosen distribution dt updated 
choice determined type weak learner see section 
typical case positive updating function decreases increases weights dt ht procedure adaboost xi yi set training examples initialize distribution get weak hypothesis ht ht dt choose update distribution dt dt dt exp xi zt zt chosen dt distribution return combined hypothesis adaboost tht adaboost algorithm bad prediction variation proportional confidence ht xi 
final hypothesis computes predictions weighted vote weak hypotheses 
schapire singer proven training error adaboost algorithm training set fraction training examples sign xi differs yi zt zt normalization factor computed round upper bound guiding design parameter algorithm attempts find weak hypothesis ht minimizes zt 
learning weak rules schapire singer different variants adaboost mh defined corresponding different methods choosing values calculating predictions weak hypotheses 
concentrate adaboost real valued predictions achieved best results text categorization domain schapire singer 
setting weak hypotheses simple rules real valued predictions 
simple rules test value boolean predicate prediction value 
predicates refer presence certain word text word money appears message 
formally predicate interest lies weak hypotheses predictions form holds real numbers 
predicate values calculated follows 
subset examples predicate holds subset examples predicate hold 
predicate holds 
current distribution dt real numbers calculated dt xi xj yi weight respect distribution dt training examples partition xj class shown schapire singer zt minimized particular predicate choosing cj ln 
setting 
settings imply zt 
predicate chosen value zt smallest 
small zero values parameters cause cj predictions large infinite magnitude 
practice large predictions may cause numerical problems algorithm increase tendency overfit 
suggested schapire singer smoothed values cj considered 
important see far weak rules directly seen decision trees single internal node tests value boolean predicate leaf nodes give real valued predictions possible outcomes test 
extremely simple decision trees called decision stumps 
turn boolean predicates seen binary features word feature predicate described criterion finding best weak rule best feature seen natural splitting criterion performing decision tree induction schapire singer 
idea suggested schapire singer extended algorithm induce arbitrarily deep decision trees 
splitting criterion consists choosing feature minimizes equation predictions leaves boosted trees equation 
note general adaboost procedure remains unchanged 
denote treeboost adaboost mh algorithm including extended weak learner 
treeboost stand learned classifier weak rules depth special case treeboost denoted stumps 
setting domain application evaluated system pu benchmark corpus anti spam email filtering problem 
consists messages spam remaining legitimate 
corpus partitioned folds size maintain distribution spam messages androutsopoulos 
experiments performed fold cross validation 
feature set corpus bag words model 
versions available stemming word removal 
experiments reported performed non stemming non stopword removal version consists set features 
experimental methodology evaluation measures 
measures evaluating spam filtering system introduced 
number spam legitimate messages corpus respectively denote number spam messages correctly classified system number spam messages misclassified legitimate 
way number legitimate messages classified system spam legitimate respectively 
values form contingency table summarizes behaviour system 
widely measures precision recall defined follows pr measure combines precision recall gives equal combined measures 
additionally experiments consider accuracy measure acc 
way distinguish types misclassification utility measures lewis trec evaluations robertson hull 
general measure positions contingency table associated loss values indicate desirable outcomes user defined scenario 
performance system terms utility 
androutsopoulos 
androutsopoulos propose particular scenarios misclassifying legitimate message spam times costly symmetric misclassification 
terms utility scenarios translated 
introduce weighted accuracy wacc measure version pu corpus freely available publications section www iit gr accuracy sensitive cost wacc evaluating filtering systems measure suffers problems standard accuracy yang 
despite fact comparison purposes 
baseline algorithms order compare boosting methods techniques include baseline measures decision trees 
standard tdidt learning algorithm rlm distance function feature selection 
see complete details particular implementation 
naive bayes 
include best results pu corpus reported androutsopoulos corresponding naive bayes classifier 
experiments section explains set experiments carried 
said section experiments pu corpus 
comparing methods corpus purpose experiment show general performance boosting methods domain 
adaboost classifiers learned setting depth weak rules denote classifier treeboost stands depth weak rules particular case denote treeboost classifier stumps 
version treeboost learned weak rules 
shows measure classifier function number rounds 
plot obtained rates baseline algorithms 
seen treeboost clearly outperforms baseline algorithms 
experiment shows certain number rounds treeboost versions achieve consistent results overfitting process 
rounds boosting versions reach value 
noticed deeper weak rules smaller number rounds needed achieve performance 
surprising deeper weak rules handle information 
additionally shows different number rounds produce slight variations error rate 
concrete value parameter tree boost learning algorithm order obtain real classifiers able comparisons different versions treeboost baseline methods 
knowledge unclear stumps treeboost treeboost treeboost treeboost treeboost naive bayes decision trees number rounds measure stumps treeboost increasing number rounds best way choosing estimated parameter validation set built task procedure trial cross validation experiment training subsets learn rounds boosting remaining validation set testing classifier respect number rounds steps :10.1.1.91.8665
outputs classifiers compute measure 
minimum maximum chosen estimated optimal number rounds classifiers 
table presents results classifiers 
include number rounds estimated validation set recall precision maximum achieved rounds learned 
results boosting classifiers clearly outperform algorithms 
naive bayes achieves precision slightly lower obtained boosting classifiers worse recall point lower 
recall prec 
max bayes trees stumps treeboost treeboost treeboost treeboost treeboost table performance classifiers accuracy results compared fold cross validated paired test :10.1.1.91.8665
boosting classifiers perform significantly better decision trees 
contrary significant differences observed naive bayes classifiers tests ran presumably boosting methods significantly better 
different versions treeboost 
interestingly noticed accuracy precision rates slightly increase expressiveness weak rules improvement affect recall rate 
fact exploited experiments 
high precision classifiers section devoted evaluate treeboost scenarios low null proportion legitimate spam misclassifications allowed 
rejection curves 
start evaluating confidence prediction magnitude prediction indicator quality prediction 
purpose rejection curves computed classifier 
procedure compute rejection curve points reject predictions confidences score lowest positive negative compute accuracy remaining predictions 
results higher accuracy values long increases 
plots rejection curves computed learned classifiers 
drawn confidence prediction indicator quality 
depth weak rules greatly improves quality predictions 
stumps needs reject confident examples achieve accuracy treeboost needs 
words deeper treeboost filters concentrate misclassified examples closer decision threshold 
previous fact important consequences potential final email filtering application specification messages prediction confidence greater threshold automatically classified spam messages blocked legitimate messages delivered user 
messages prediction confidence lower stored special fold dubious messages 
user verify legitimate messages 
specification suitable having automatic filters different degrees strictness different values parameter 
values tuned validation set 
cost sensitive evaluation 
section tree boost classifiers evaluated cost measures introduced section 
scenarios strictness androutsopoulos cost considered corresponding semiautomatic scenario moderately accurate filter giving completely automatic scenario stumps treeboost treeboost treeboost treeboost treeboost rejection curves treeboost classifiers :10.1.1.91.8665
axis percentage rejected predictions axis accuracy 
high accurate filter assigning 
noted section consider scenarios particular utility matrices 
schapire modification ada boost algorithm handling general utility matrices 
idea initialize weight distribution examples utility matrix run learning algorithm usual 
performed experiments setting results convincing initial rounds boosting affected initialization utility number rounds performance utility considered 
procedure tuning number rounds determine initial stage ends rejected approach 
think modification adaboost algorithm consider weight update 
approach consists adjusting decision threshold 
default scenario corresponding example classified spam prediction greater case 
increasing value results higher precision classifier 
lewis procedure calculating optimal decision threshold system arbitrary utility matrix 
procedure valid system outputs probabilities predic tion scores resulting boosting classifications mapped probabilities 
method estimating probabilities output adaboost suggested friedman ar logistic function 
initial experiments function worked properly relatively low predictions sent extreme probability values 
possible solution scale predictions applying probability estimate observed prediction scores grow number depth weak rules 
parameters involved scaling rejected probability estimation predictions 
alternatively classification scheme sensitive factor tuning parameter value maximizes weighted accuracy measure 
concrete value obtained validation set values parameter tested 
table summarizes results obtained procedures giving factor values 
results obtained androutsopoulos reported 
treeboost clearly outperforms baseline methods 
high precision rates achieved maintaining considerably high recall rates depth treeboost slightly improves performance significant differences achieved 
precision rates implicit goal scenario achieved stumps maintaining fair levels recall 
recall rates slightly unstable respect depth treeboost varying 
impression high values factor introduce instability evaluation outliers 
particular corpus contains examples weighted accuracy properly giving values misclassification legitimate message leads score worse email filtered give wacc 
precision values recall variation affects measure units 
order give clearer picture behaviour classifiers moving decision threshold include precision recall curves classifier 
curves built giving wide range values computing value recall precision rates 
curves high precision rates fixed obtain recall rate points 
table summarizes samples 
variants indistinguishable level precision 
moving higher values precision significant difference occur stumps rest variants deeper weak rules 
fact proves increasing expressiveness weak rules improve performance requiring high pre cision filters 
unfortunately clear drawn appropriate depth 
noted treeboost achieves best recall rates particular corpus 
method stumps treeboost treeboost treeboost treeboost treeboost table recall rate filtered spam messages respect fixed points precision rate stumps treeboost treeboost treeboost treeboost treeboost precision recall curves recall values fixed precision rates :10.1.1.91.8665
axis recall axis precision 
experiments show adaboost learning algorithm clearly outperforms decision trees naive bayes methods public benchmark pu corpus 
data set method resistant overfitting rates achieved 
procedures automatically tune classifier parameters number boosting rounds provided 
method recall prec 
wacc recall prec 
wacc naive bayes stumps treeboost treeboost treeboost treeboost treeboost scenarios high precision classifiers required adaboost classifiers proved properly :10.1.1.91.8665
experiments exploited expressiveness weak rules increasing depth 
concluded deeper weak rules tend suitable looking high precision classifier 
situation achieved results pu corpus fairly satisfactory 
adaboost classifiers capabilities shown useful final email filtering applications confidence predictions suggests filter blocks confident messages delivering remaining messages final user 
classification threshold tuned obtain high precision classifier 
research line study techniques larger corpus 
think pu corpus small easy default parameters produce results tuning procedures result slight improvements 
experiments reported study effect number rounds richer feature spaces shown confidence classifiers depends parameters 
larger corpus effectiveness tuning procedures explicit hopefully clear optimal parameter settings adaboost drawn 
line research misclassification costs inside adaboost learning algorithm 
initial experiments method proposed schapire worked properly believe learning directly classifiers utility settings perform better tuning classifier learned 
acknowledgments research partially funded eu ist spanish tic tic catalan governments 
xavier carreras holds department universities research information society catalan government 
table cost sensitive evaluation results androutsopoulos androutsopoulos androutsopoulos spyropoulos 
learning filter spam mail comparison naive bayesian memory approach 
th pkdd workshop machine learning textual information access 
androutsopoulos androutsopoulos androutsopoulos spyropoulos 
experimental comparison naive bayesian anti spam filtering personal mail messages 
proceedings rd acm sigir annual conference pages 
cohen cohen william cohen 
learning rules classify mail 
proceedings aaai spring symposium information access 
cranor lamacchia cranor lamacchia cranor lamacchia 
spam 
communications acm 
drucker drucker drucker wu vapnik 
support vector machines spam categorization 
ieee trans 
neural networks 
friedman ar friedman ar friedman hastie tibshirani 
additive logistic regression statistical view boosting 
annals statistics appear 
joachims joachims joachims 
text categorization support vector machines learning relevant features 
proceedings th european conference machine learning number lncs 
springer verlag heidelberg de 
lewis lewis david lewis 
evaluating optimizing autonomous text classification systems 
proceedings th acm sigir annual conference 

part speech tagging machine learning approach decision trees 
phd 
thesis dep 
inform tics 
universitat polit cnica de catalunya 
robertson hull robertson hull stephen robertson david hull 
trec filtering track final report 
th text retrieval conference trec 
sahami sahami sahami dumais heckerman horvitz 
bayesian approach filtering junk mail 
learning text categorization papers aaai workshop pages 
schapire singer schapire singer schapire singer 
improved boosting algorithms predictions 
machine learning 
schapire singer schapire singer schapire singer 
boostexter boosting system text categorization 
machine learning 
schapire schapire schapire singer singhal 
boosting rocchio applied text filtering 
proceedings st acm sigir annual conference 
weiss weiss weiss apte fred damerau david johnson frank oles goetz thomas 
maximizing text mining performance 
ieee intelligent systems 
yang liu yang liu yang liu 
reexamination text categorization methods 
proceedings nd acm sigir annual conference 
yang yang yiming yang 
evaluation statistical approaches text categorization 
journal information retrieval 
