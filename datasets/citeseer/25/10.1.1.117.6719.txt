ieee transactions computers vol 
may effective hardware data prefetching high performance processors tien fu chen member ieee jean baer fellow ieee memory latency bandwidth progressing slower pace processor performance 
describe evaluate performance variations hardware function unit goal assist data cache prefetching data accesses memory latency hidden possible 
basic idea prefetching scheme keep track data access patterns prediction table rpt organized instruction cache 
designs differ timing prefetching 
simplest scheme basic prefetches generated iteration ahead actual 
lookahead variation takes advantage lookahead pro gram counter ideally stays memory latency time ahead real program counter control mecha nism generate prefetches 
correlated scheme uses sophisticated design detect patterns loop levels 
designs evaluated simulating spec benchmarks cycle cycle basis 
results show hardware prefetching schemes yield significant reductions data access penalty compared lar caches benefits greater hardware assist augments small chip caches lookahead scheme preferred cost performance wise 
index terms prefetching hardware function unit prediction branch prediction data cache cycle cycle simulations 
performance increased dramatically years surpassed mips level 
memory latency bandwidth progressed slower pace 
caches shown effective way bridge gap processor cycle memory latency times 
caches eliminating main memory accesses reduce memory latency 
essential investigate techniques reduce effects imbalance processor memory cycle times 
techniques proposed tolerating high memory latencies 
hardware list manuscript received mar revised aug 

chen department computer science information engineering national chung cheng university taiwan republic china mail chen cs ccu edu tw 

baer department computer science engineering university washington seattle wa mail baer cs washington edu 
ieeecs log number 
ieee cache hierarchies medium high performance systems cache assists write buffers victim caches lock free caches various degrees sophistication hardware prefetching relaxed memory consistency models case shared memory multiprocessors outside scope 
software side mention prefetching software cache coherence schemes data placement increasing locality reducing false sharing items importance cessors 
focus techniques hardware prefetching 
briefly discuss sec tion hardware software prefetching schemes advantages drawbacks 
intent demonstrate simple hardware assist chip reap important benefits reducing data access penalty chip data cache level mem ory hierarchy level hit sec ond level penalty roughly order magnitude 
mechanism meant reduce latency lowest levels memory hierarchy non intrusive 
calls hardware scheme critical path steal cycles exe cution instruction stream 
prefetching scheme goal reduce proces sor stall time bringing data cache accessed delay 
data prefetched far advance run risk ing cache 
ideally perfect prefetching scheme totally mask memory latency time practically latency reduced impediments prevent perfect prediction instruction stream imperfect branch prediction data stream data dependent addresses 
basic idea hardware prefetching scheme introduced keep track data access patterns effective address stride prediction table rpt 
design strives simplicity 
combines simplest look approach direct mapped caches unsophisticated bit states state transition mechanism implemented branch ieee transactions computers vol 
may target buffers 
section describe variations basic design 
differ timing pre fetching 
simplest scheme called basic prefetches generated iteration ahead actual 
lookahead variation takes advantage lookahead program counter stays possible memory latency time po iterations ahead real program counter 
lookahead program counter serves control mecha nism generate prefetches 
correlated scheme uses sophisticated design detect patterns loop levels 
variations evaluated simulating spec benchmarks cycle cycle 
evaluation methodol ogy parameters simulated processor memory architectures described section iv 
results analyzed section show hardware pre fetching schemes yield significant reductions data access penalty compared regular caches benefits greater hardware assist augments small chip caches lookahead scheme preferred cost performance wise 
concluding remarks section vi 

related pattern description examples scalar simple variable index count zero lp subscript aii lp stride expression unchanged tab lp constant lp subscript lp irregular lp 
li linked list obviously standard caches scalar zero stride 
caches large block sizes slightly improve performance constant stride category stride small help stride large 
goal prefetching techniques generate prefetches ad vance uncached blocks scalar zero stride con stant stride access categories independently size stride 
time erroneous prefetches irregular accesses overhead caused prefetches cached data avoided 
prefetching triggered hardware mecha nism software instruction combination 
hardware approach detects accesses regular patterns issues prefetches run time software ap proach relies compiler analyze programs insert prefetch instructions 
hardware solutions proposed literature require little compiler support 
hardware prefetching hardware approaches classified cate gories spatial access current block basis prefetch decision temporal lookahead de coding instruction stream implied 
spatial schemes prefetches occur cache block 
smith studied variations block lookahead obl policy le block refer block prefetched 
jouppi proposed extension obl consecutive data streams prefetched fifo stream buffers 
obl extensions rates reduced direct mapped caches expense increase memory traffic 
schemes take advantage limited sequential spatial locality prefetching data separated small constant strides able deal large strides 
stride informa tion carried vector instructions led fu pro pose prefetch strategies vector processors 
subse quently derived similar approach scalar processors 
major mechanism record previous memory ad dress history table generate prefetch requests calculating stride current address previ ous address stride nonzero 
approach lacks control preventing unnecessary prefetches irregular accesses corresponds degenerated version basic scheme le state transition mechanism 
general idea hardware assist performance cost evaluation 
note spatial schemes opportune time initiate prefetch linked closely time 
temporal mechanisms attempt data cache just time 
data prefetching instruction lookahead implicit prefetching decoupled architectures fetch speculatively data operands needed near 
time window prefetching occur limited instruction decoding buffer size wide large memory latencies 
address data prefetched values speculated operands related chen baer effective hardware data prefetching high performance processors current locality cache patterns described 
seen lookahead scheme describe combines advantages spatial temporal ap proaches 
spatial mechanism realized capturing dy data access patterns prediction table rpt 
lookahead program counter la pc section ii dynamically controls arrival time prefetched data 
branch prediction mechanism lieu complex lookahead instruction decoding 
software directed prefetching totally different approach prefetching soft ware directed techniques rely static program analysis detect regular data access patterns 
intelligent compiler inserts data prefetch instructions cycles corresponding memory instructions 
prefetch instruc tions explicitly executed processor initiate pre fetch requests 
porterfield examined effect prefetching array inner loop inserting prefetches iteration ahead 
recognized led overhead prefetch instructions directed data cache 
lo proposed algorithm find earliest point loop entire subarray prefetched 
approach focuses fetching block data single cache line time 
levy showed time prefetch depend memory latency loop execution time feature hard ware lookahead scheme 
proposed prefetching separate fetch buffer unified cache 
study software prefetching non scientific code chen difficult generate prefetch addresses early access patterns irregular 
mowry gupta showed manually inserting prefetch instructions prefetching effective tolerating large memory la shared memory multiprocessor environment 
considered prefetch instructions read write ac cesses 
developed compiler algorithm automatically inserts prefetch instructions 
algorithm locality analysis loop transformations proper prefetch predicates 
prefetches inserted selectively constant stride pattern cause cache misses 
hardware techniques software directed ap proaches successful identifying prefetches data simple constant stride patterns 
schemes require lockup free caches architectural support 
architectural requirement software approach avail ability prefetch instruction 
efficient hardware schemes non trivial hardware mechanism needed detect items prefetched initiate prefetching 
performance viewpoint software approach identify prefetches accesses complex patterns primarily loop domain 
addition software prefetching multiprocessor environment take ac count factors data coherence task scheduling task migration 
side software prefetch instructions introduce overhead execu tion prefetch instruction possibly computa tions effective addresses prefetch predicates 
code needs expanded loop unrolling prefetch occur sufficiently advance 
side effects increased register pressure result prefetch optimizations 
intelligent compiler may able reduce unnecessary overhead remainder significant especially mem ory latency large 
software approach cater prefetching data replaced conflict capacity cache misses fre quent cache size small 
similarly estimates execu tion time loops calling subroutines may uncovered compile time preventing prefetch right time 
optimizations language compiler depend ent hardware schemes require change executable code 
cited case software pre fetching 
goal show hardware prefetch mechanism cost performance efficient mechanism placed context small chip cache large memory latency 
intent compare hardware software directed prefetching techniques 
report preliminary study type shared data multiprocessor environments dissertation author 
addition clear compiler interaction help hardware scheme combination hardware software approaches certainly investigated thoroughly 
hi 
hardware data prefetching schemes motivation sequential prefetching successful optimiza tion caches caches 
section describe variations increasing order complexity basic lookahead correlated hardware pre fetching scheme caches 
common basis schemes predict instruction execution stream data access patterns far advance required data prefetched cache real memory access instruction executed 
schemes common goals generate prefetches advance uncached blocks scalar zero stride constant stride access categories independently size stride avoid unnecessary prefetching irregular accesses incur execution time penalty prefetch requests data cached design hardware assist increase processor cycle time interfere critical path timing 
basis designs rpt holds data ac cess patterns instructions 
rpt organized ieee transactions computers vol 
may instruction cache 
minimally entry table contain tag related instruction address fields record memory operand address stride state transi tion field 
illustrate concept consider usual matrix mul loop detail see section iii pseudo assembly risc code version computational part inner loop shown fig 

code assume subscripts kept registers 
steady state rpt contain entries load lw store instructions 
iteration inner loop accesses location zero stride prefetch re 
depending block size constant stride prefetched eration block size iteration block size 
load constant stride stride larger block size generate prefetch instruction iteration 
int fori fork ijj kj multiplication instruction lw iw mu lw rl addu sw rl addu addu addu rll rll bne fig 
example matrix multiplication comment load load kj stride kj load ij store ij stride ref ref kj increase loop assembly code compiler difficulty identifying ac cess patterns prefetching hardware scheme provides advantages dynamic prefetching detection code compatibility 
particular shown sections scheme appropriate high performance processors small level caches small block size software directed approaches may significant instruction overhead 
entered 
itera tion predictable defined prefetch issued 
basic scheme involves pc rpt 
shown fig 
hardware requirement basic design subset complex lookahead variation described section ii introduce design rpt basic scheme 
prediction table rpt rpt organized instruction cache keeps track previous addresses associated strides load store instructions 
rpt entry indexed instruction address includes effective address data access stride information regularity status instruction 
entry format see fig 
tag corresponds address struction prev addr operand address referenced pc reached instruction 
stride difference addresses generated 
state bit encoding states past history indicates prefetching generated 
states initial set entry rpt entry experienced incorrect prediction steady state 
transient corresponds case system sure previous prediction 
new stride obtained subtracting previous address currently referenced address 
steady indicates prediction sta ble 
prediction disables prefetching en try time 
effective prediction table update state transition fig 
prediction basic prediction basic idea prediction predict past history memory ac 
cess instructions 
intuitive prediction scheme prefetches iteration generated basic mechanism rpt record effective address memory operand compute stride ith iteration executed 
program counter pc access set state controlling prefetching load store instruction check see ing previously recorded stride just computed 
entry corresponding instruction rpt 
stride information obtained difference chen baer effective hardware data prefetching high performance processors effective addresses ac cesses instruction 
load store instruction encountered time instruction entered rpt initial state 
stride obtained time le second access state set transient known pattern regular irregular 
stride computed equal previously re le stride occurred twice row entry set steady state 
remain steady state different stride detected inner loop 
point state reset initial 
different stride computed transient state entry set prediction state pattern regular want prevent erroneous prefetching 
presence irregular patterns state entry stay prediction state oscillate tran prediction states regular stride detected 
pc encounters instruction effective operand address addr rpt updated follows clear denote correct condition addr rev addr stride incorrect condition addr rev addr stride 
corresponding entry 
instruction entered rpt prev addr field set addr stride state initial 

corresponding entry 
transition incorrect state initial set prev addr addr stride addr prev addr state transient 
moving steady state correct state initial transient steady set prev addr addr leave stride unchanged set state steady 
steady state back initialization incorrect state steady set prev addr addr leave stride unchanged set state initial 
detection irregular pattern incorrect state transient set prev addr addr stride addr prev addr state prediction 
prediction state back transient correct state prediction set prev addr addr leave stride unchanged set state transient 
irregular pattern incorrect state prediction set prev addr addr stride addr prev addr leave state unchanged 
update prefetch request generated presence state entry 
note gen eration prefetch block execution struction stream particular increment pc 
mutually exclusive possibilities action 
existing entry entry state prediction 

potential prefetch 
entry init transient steady state 
data block address rev addr stride generated 
pre fetch initiated block uncached address outstanding request list orl see section ii 
implies sending request level memory hierarchy buffering communication channel busy 
address re quest entered orl 

example discussion fig 
illustrates rpt filled inner loop matrix multiplication code shown previously executed 
restrict example handling load instructions addresses 
assume base addresses matrices respectively locations 
tag prev ad state empty iteration tag prev addr state transient transient steady iteration fig 

example filling rpt entries tag prev ad state steady iteration start iteration rpt consid ered empty entry corresponding addresses fig 

assume element cached 
pc executes time load instruction address corresponding entry 
instruction entered rpt tag prev addr field set address operand stride set state initial 
similar actions taken load instructions fig 

cases cache misses prefetches 
pc executes load instruction address second iteration situation described transition 
actions take place normal access address 
results cache hit block size larger 
update entry rpt 
prev addr field comes stride set state transient 
potential prefetch block address 
prefetch occurs block size 
similar actions take place load address instance certainty prefetch generated fig 

load instruction situa tion moving steady state 
prev stride fields unchanged state steady 
course cache hit prefetch 
third iteration loads result cache hits indications prefetches referenced items progress 
rpt entries updated shown fig 
note transient steady transitions prefetches generated blocks addresses needed 
subsequent iterations follow pattern 
observed fig 
scalar zero stride refer ences pass initial steady state transition instruction 
constant stride pass transient state obtain stride stay steady state instructions 
wrong predictions row shown example prevented prefetched passing prediction state re enter transient state provided addresses predictable 
instance accesses elements triangular matrix may follow pattern 
note stride field updated transition steady initial incorrect prediction 
lookahead prediction basic scheme potential weakness associated timing prefetch hiding memory latency prefetching depends execution time loop iteration 
loop body small prefetched data may arrive late access loop body large early arrival prefetched data may re place replaced useful blocks data 
compiler may easily solve prob lem loop unrolling assume hardware scheme rely software support preserves code com various hardware implementations 
lookahead prediction scheme seeks remedy drawback basic scheme 
assuming contention interconnect access cache ideal time issue prefetch request cycles ahead actual latency access level memory hierarchy 
data arrive just time prevent cache displace potentially useful data 
prefetching iteration ahead lookahead prediction approximate ideal prefetch time help pseudo program counter called look ahead program counter la pc remain possible cycles ahead regular pc access rpt generate prefetches 
la pc incremented regular pc 
ieee transactions computers vol 
may branch prediction table bpt take full advantage look ahead feature 
block diagram target processor shown fig 

bottom part abstracts common high performance processor chip data instruction caches 
upper left part shows rpt orl keeps track addresses fetches progress standing requests 
basic scheme rpt ac cessed pc 
implement lookahead mechanism la pc associated logic added top part right 
la pc secondary pc predict execution stream 
addition assume bpt branch target buffer btb branch prediction mechanism pc high performance processor modifying la pc 
fig 

block diagram data prefetching 
lookahead pred entries rpt bpt initialized updated pc encounters corresponding instruction 
lookahead scheme contrast basic prediction la pc pc triggers potential prefetches rules previous section 
cycle la pc simply incremented 
la pc finds entry bpt indicates la pc points branch instruction 
case prediction result branch entry bpt provided modify la pc 
note instruction prefetch structure decoupled architectures la pc need decode predicted instruction stream 
lookahead mechanism history information execution stream la pc just pointer detect prefetching rpt 
lookahead program counter la pc rpt basic prediction scheme prefetching occur iteration ahead mentioned earlier pre fetched data cache real access takes place 
situation occur loop iteration time smaller memory latency 
help lookahead mechanism la pc may wrap loop revisit load instruction execution time loop iteration smaller memory latency 
way may multiple iterations lookahead 
extra field times entries rpt keep chen baer effective hardware data prefetching high performance processors track iterations la pc ahead pc fig 

lookahead design la pc detects gen prefetch requests pc accesses rpt effective address obtained 
result rpt dual ported allows simultaneous accesses pc la pc 
la pc hits instruction cor responding entry rpt address potential pre fetch determined computing rev addr stride times 
times field incremented la pc hits entry decremented pc catches entry 
times field reset prediction corresponding entry incorrect 
tag addr times state effective address fig 

rpt lookahead mechanism 

lookahead distance limit bit correct incorrect times ideal look ahead distance la distance le time execution instruction pointed pc instruction pointed la pc equal latency level memory hierarchy 
clearly approximated la distance vari able 
initially wrong branch prediction la distance set la pc pointing instruction current pc 
real cache occurs prefetch completed time data needed current execution stalled value pc change la pc move ahead generate new requests recall role orl 
shown fig 
la pc maintained help branch prediction mechanism bpt 
bpt designs thoroughly investigated repeat studies 
experiments branch tar get buffer btb bit state transition design described assume btb implemented core processor purposes 
la distance increases data prefetch early memory latency com pletely hidden 
pc la pc apart prediction execution stream incorrect la distance cross basic block 
want prefetched data cached early displace needed data 
introduce system parameter called look ahead limit la limit specify maximum distance pc la pc 
la pc stalled normal execution resumed situa tions la distance reaches specific limit orl full 

handling cache misses cache read cache controller checks orl 
block requested normal lengthy stall occurs 
call hit wait cycles cycles dur ing cpu waits prefetched block cache 
orl block entry regular load issued priority buffered prefetch requests 
write back write allocate strategy write data cache cause system fetch data block update desired word 
block size larger single word initiate prefetching read block size word prefetch needs issued check orl needed consistency purposes 
match entry orl tagged discard status data ignored arrives 
la pc reset incorrect branch prediction buffered prefetch requests flushed 
prefetch raises exception page fault range violation ignore prefetch 
drawbacks wrong page fault prediction far weigh small benefits correct prefetch 
correlated prediction previous designs prediction regularity adjacent data accesses 
general schemes predicting inner loops 
results significant execution segments small inner loop bodies triangle shaped loop patterns frequent stride change outer erations 
example look livermore kernel loop shown fig 

int oj continue code fig 

livermore loop access matrix executing inner loop accesses matrix regular strides example stride 
pattern picked schemes 
incorrect predic tion occur time loop finished ac cessing 
observe correlation accesses termination inner loop stride 
correlation led design accurate branch prediction equally applied data prediction 
key idea correlated prediction keep track adjacent accesses inner loops schemes ieee transactions computers vol 
may correlated changes loop level 
branches inner loop taken iteration branch trigger correlation level 

implementation correlated rpt implementation correlated scheme bring additions lookahead mechanism shift register record outcome branches extended rpt separate fields computing strides various correlated accesses 
general case bit shift register keep track results branches serve mask address various fields extended rpt 
prefetching far advance detrimental restrict correlation level nested loops 
rpt extended fig 
second pair prev addr stride fields recording access patterns outer loop 
note outer loop level times state fields longer relevant 
bit shift register recording outcome loop branches 
assume bit encodes taken branch steady state encoding executing inner loop case prefetching entry rpt cor responding inner loop right part 
branch taken shift register contain inner loop branch followed taken outer loop branch prefetching part entry corresponding outer loop left part 
updating left prev stride right prev fields take place outer iteration shift register contains right fields updated consecutive inner iterations register contains 
effective address branch history fig 

correlated rpt fig 
shows rpt entry filled updated execution iterations outer loop kernel loop 
left times field ease explanation 
loss generality assume initial content shift register entry rpt empty 
initial access fields filled previous schemes row left table fig 

second access row right table right fields modified previous schemes shift register contains 
second outer iteration le access shift register contain branch non taken 
prefetching needed done accesses levels loop ing pair fields prev formed second row top table 
subsequent accesses prefetching updating right fields second row bottom table 
third outer iteration steady state row ta ble 
time prefetched second iteration 
prefetch generated activated line size large line 
st er iteration outer prev prev nd mer iteration prev prev iteration state state lb tb ol fig 

example correlated rpt entries 
issues regarding implementation correlated prediction scheme need addressed 
cases reasonable assumptions keep design simple possible 
assumption easy compiler distinguish loop branches branches flagged branch prediction table shift register modified encountered 
assumption removed letting shift register modified branch 
predictable patterns emerge com plexity implementation warranted 
second assume loop iterations controlled backward branches third assume prefetches correlated outer iterations issued basic case la pc pc incor rect branch prediction 
prefetches generated attempt control prefetch issue cycles ahead data 
assumption reasonable accesses outer loop separated execution iterations inner loop likelihood take longer cycles 
iv 
evaluation methodology trace driven simulation evaluated proposed architectures cycle cycle trace generation combined fly simulation 
comparison purposes baseline architecture consisting processor perfect pipelining direct mapped cache bytes block size bytes specified simulated 
benchmarks instrumented 
backward branch passes execution control location address branch instruction 
transformation compiler required programs forward conditional branches control loop iterations 
chen baer effective hardware data prefetching high performance processors decstation mips cpu pixie facility 
traces include data instruction simulator emulate detail behavior overlapping computation data access 
experiment results collected clock cycle level individual configurations 
applications spec benchmark compiled mips compiler mips com piler optimization options 
traces captured execution phase benchmarks discarded traces initial routines gen erate test data benchmarks 
statistical data recorded system simulated data accesses 
fill cache bpt rpt simulate warm start 
initialization phase warm start period simulations results collected instructions programs 
table shows dynamic characteristics workload 
statistics collected simulating program infinitely large rpt 
columns show proportions data weighted frequency belong categories mentioned previously 
indication predictability programs 
scalar zero stride beneficial data cache prefetching schemes useful bring back advance blocks displaced small cache size capacity misses small associativity conflict misses 
constant stride may substantially contribute cache misses helped rpt schemes 
prefetching avoided unpredictable ir regular 
column prediction ratio shows outcome branch predictions entry bpt functions state bit branch target buffer 
second indication predictability illustrating possible benefits exploited lookahead approach 
name tomcatv fpppp matrix spice doduc nasa eqntott espresso gcc xlisp table characteristics benchmarks data scalar constant irregular zero stride stride branch pred 
ratio experimented architectural choices 
spec trademark standard performance evaluation 
varying architectural parameters described previously 
results experiments cycle instruction contributed memory accesses mcpi main metric 
assume processor execute instruction cycle perfect pipelining ideal instruction cache extra contribution cpi stems data access penalty 
mcpi caused data access penalty obtained total data access penalty mcp number instructions executed reason choosing mcpi metric rate average effective access time mcpi reflect actual stall time observed processor processor execution cache behavior account 
figures give percentage data access penalty reduced prefetching scheme 
percentage number computed data penalty data penalty ch penalty reduced data penalty memory models data bandwidth important consideration design architecture allows overlap computation data accesses data requests cache misses prefetching requests simultaneously 
associated orl prefetching caches 
requirement list searched associatively 
assume conservatively fetch progress aborted 
demand cache priority buffered prefetch requests 
memory interfaces increasing capabilities concurrency simulated 
see fig 
timing charts block diagrams soon request sent level request initiated sole request progress completed 
model typical chip cache backed second level cache 
interface supports cache request time 
overlapped access time memory request decomposed parts request issue cycle memory latency transfer cycles 
assume period memory latency data requests request issue transfer phases 
request issue transfer take place time 
model represents split buses bank interleaved memory modules secondary caches 
orl entries associated module 
pipelined request issued cycle 
model representative processor cache pairs linked memory modules pipelined interconnection network 
assume loud mechanism le desired word available soon data response arrives 
entry orl associated cache 
pipelined fig 

memory models timing data configurations experiment overlapped pipelined respectively 
memory latency usually equal 
overlapped model default model show general results implementation high formance processors 
cycle time phases requesting accessing memory transferring cycles respectively 
simulation results section experimental results show benefits prefetching schemes 
compare architecture baseline cache architecture augmented prefetching schemes 
comparisons performed spec benchmarks 
remainder section brevity sake restrict selves reporting benchmarks salient fea tures 
examine impact memory models sociated latencies effect block size variations means improve efficiency rpt 
dis setting lookahead limit relevant lookahead prefetching scheme 
general results figs 
show results simulation architectures data access penalty mcpi function cache size 
overlapped memory model block size bytes rpt bpt prefetching schemes entries 
results show prefetching organizations superior pure cache scheme amount cache addition prefetching component 
cache small contain working set application best prefetching scheme reduce data access penalty 
additional cost paid prefetching justified significant performance improvement 
additional cost rpt logic approximately equivalent byte cache section 
examine performance curves dividing benchmarks groups prefetching performs extremely prefetching yields moderate im provement performance prefetching ieee transactions computers vol 
may uuu na cd ppd cn matrix request list caches cache cache size tomcatv 
nasa eqntott 
cache ze fig 

simulation results overlapped xlisp tion reduction data access penalty slight 
group formed matrix espresso benchmarks data access penalty reduced 
practical purposes mcpi completely eliminated 
predictability programs flat performance curves pre fetching illustrate cache small size sufficient capture locality compulsory cache misses eliminated prefetching 
second group includes tomcatv nasa eqntott xlisp prefetching yields performance im provement reduction data access penalty range 
eqntott xlisp mcpi penalty chen baer effective hardware data prefetching high performance processors quarter cycle small cache 
seeking fur ther improvement worthwhile 
tomcatv nasa cache size increases penalty pure prefetching caches minimized working set cap tured tomcatv 
notice absolute reduction mcpi significant cycle cases dependent cache size 
results groups seen performance data mod erate cache size argues forcefully spending cache real estate rpt bpt cache size 
spice gcc cache size fpppp cache lu cache size fig 
io 
simulation results overlapped third group consists spice doduc gcc fpppp 
spice doduc prefetching valuable data access penalty reduced 
fpppp lesser extent gcc pure cache captured working set prefetching help 
factors responsible small advantage brought prefetching 
fraction scalar zero stride categories dominates fpppp benchmarks table performance tion prefetching accesses nonzero strides significant 
second significant branch prediction ra tio gcc precludes successful prefetching 
third rpt may capable hold active memory instructions time limited small number entries 
examine issue section 
compare relative performances prediction schemes 
expected level hardware complexity pays 
difference lookahead correlated variations small eqntott tion data point showing improvement 
difference lookahead basic significant 
notable differences benchmarks tomcatv large loop body basic scheme pre fetched data arrive early displacing useful data replaced espresso small basic block basic scheme data arrive late gen erating hit wait cycles 
results show lookahead logic worth implementing allows flexibility prefetch correct time complexity required help data accesses outer loops correlated scheme plays role 
summary prefetching schemes effective reduc ing data access penalty 
prefetching hardware unit particularly worthwhile chip area limited choice added unit increasing slightly chip cache capacity 
effect memory models latencies fig 
presents data access penalties baseline cache lookahead scheme memory models memory latency varying cycles benchmarks tomcatv espresso eqntott xlisp 
bar corresponds architecture memory latency mcpi pipelined access overhead coming overlapped models stacked top 
numbers inside bars lookahead prefetching give percentages pen reduced prefetching model worst pipelined model best respectively 
overhead case baseline cache comes waiting time incurred cache write back progress assume request progress aborted 
similarly overhead prefetching scheme includes stall time real demand cache waiting prefetching write back requests progress 
note meaningful large access time say cycles model small latency cycles pipelined model 
simply intend show effect stall penalty large spectrum memory bandwidth 
expected memory interface restricted bandwidth model result poorer relative performance improvements longer mem ory latencies 
noticeable benchmarks espresso tomcatv larger benchmarks 
large portion busy time eliminated passing model overlapped model pipelined model 
benchmarks difference models significant models 
required parallelism exploited overlapped model 
results shown fig 
indicate adequate interface necessary meet memory bandwidth demand prefetching techniques exploit parallelism memory requests 
cache reduction sufficient assess value prefetching scheme 
memory latency increases relative access penalty prefetching scheme models increases 
ieee transactions computers vol 
may tomcatv fig 

effect memory models latencies eqntott main reason model lack concurrency requests resulting hit wait cycles orl full 
reason com mon models lookahead scheme relies branch prediction la pc 
correctness prediction sensitive large latency see section wrong prefetches interface occur larger latencies 
better results obtained small memory latencies reinforce previous claim scheme beneficial high performance proc limited chip cache 
benefits de grade interface secondary cache limited concurrency model 
effect block size known cache capacity block size leads best hit ratio compromise large sizes increase spatial locality small sizes reduce conflict misses 
prefetching scheme increase spatial locality predict best block size prefetching scheme smaller equal pure cache 
fig 
presents performance various architec tures function block size 
baseline byte direct mapped cache 
prefetched blocks size blocks fetched real misses 
ex espresso xlisp qp fig 

mcpi vs block size cache overlapped 
periments overlapped model transfer rate bytes cycle request memory latency cycles respectively 
seen fig ure best block size baseline architecture bytes choice lead significant improve ments example reduction mcpi factor matrix factor eqntott passing chen baer effective hardware data prefetching high performance processors gcc fig 

hit ratio attempted prefetch rpt 
block size block size 
contrast pre fetching scheme sensitive block size best results obtained block size 
result time argues hardware prefetching associated chip cache limited bandwidth small number pins le small block size ment performance 
organizing prediction table discussed section benefits incurred prefetching schemes depend primarily program behavior specifically amount predictable 
second factor organization rpt size associativity 
stated earlier cost entry rpt experiments equivalent byte data cache 
entry basic scheme prev addr stride fields need bytes tag state transi tion bits require amount memory tag direc tory status bits cache 
lookahead scheme add bits entry times field 
corre lated scheme requires significantly space 
memory cost entry rpt lookahead scheme roughly equivalent byte data cache block size bytes 
rpt requires comparator tag cache adder shifter computation stride effective address prefetched minimal logic state transition table 
need implement write strategies conflict rpt need differentiate clean dirty blocks 
rpt logic slightly complex cache uses standard techniques 
similarly logic needed implement la pc simple presence requires cache btb rpt dual ported 
hit ratio instructions referencing rpt spec benchmarks 
eighth benchmark fpppp hit ratio low low lo double size rpt 
primarily main part program consists large loop body long sequence scalar accesses 
ref erences recorded rpt replaced loop starts iteration 
remaining benchmarks gcc doduc fig 
shows fractions instructions hit rpt function size associativity doduc rpt 
addition line entitled prefetch attempt number rpt entries shows percentage accesses hitting entries prediction state nonzero stride prefetching attempted 
increasing associativity rpt minimal effect fig 

sequential nature instructions reason lack improvement 
hand increasing size rpt improves hit ratio small rpt hold referencing instructions frequently executed loops benchmarks 
note prefetch attempt increases larger hit ratio 
takes accesses regain necessary stride information instructions re placed 
adequate hit ratio obtained percentage accesses prefetches roughly equal data accesses constant stride category table 
question arise extra chip capacity devoted larger complex cache larger complex rpt 
hand hit ratio rpt may directly translated smaller ratio data cache depending fraction nonzero stride accesses 
hand adding complexity data cache may yield better performance care taken increase basic cycle time cache example extra gate delays comparators multiplex ors 
basis experiments argue larger sophisticated rpt 
possible solu tion improving rpt hit ratio enlarging ta ble replace entries nonzero strides 
useful patterns preserved needlessly locking rpt entries corresponding instructions executed 
better approach enter rpt instructions may nonzero stride 
compiler easily provide information 
experiment decstation simply excluded memory instruc tions stack pointer general register sp gp entered rpt general non scalar registers base register 
hit ratios fpppp doduc increased rpt entries 
summary cases moderate sized rpt order entries sufficient capture access patterns frequently executed instructions 
possible op timization useful programs large basic blocks selective storing entries rpt 
ieee transactions computers vol 
may varying lookahead limit lookahead correlated schemes la pc con timing prefetches 
forward progress bounded lookahead limit maximum number cycles allowed la pc pc 
setting take account opposite effects 
certainly issue prefetches early greater number hit wait cycles reduced 
crucial data misses clustered mem ory model restrictive lesser extent overlapped model 
hand large cross basic blocks branch prediction mechanism loses reliability increased want prefetched data replace replaced useful blocks 
fig 
shows performance lookahead scheme overlapped model function lookahead limit representative programs 
memory cycle time cycles access prefetched block progress hit wait access contributes hit wait cycles total access penalty 
contributed hit wait cycles decreasing approaches 
local minimum mcpi happens 
increase result slight mcpi increase cause aforementioned factors incorrect branch prediction data replacement 
overlapped model appears setting value slightly give best results 
tomcatv espresso fig 

mcpi vs la limit overlapped vi 
described evaluated design hardware prefetching scheme 
goal support unit reduce cpi contribution associated data cache misses 
basic idea data prefetching predict data addresses keeping track past data access pat terns prediction table 
various times prefetch issued investigated variations basic lookahead correlated prefetching schemes 
evaluated schemes comparing pure cache design various cache sizes 
performed comparisons cycle cycle simulations spec benchmarks 
results show prefetching schemes generally effective reducing data access penalty 
cost hardware unit prohibitive moderately sized rpt cost equivalent cache capture access patterns fre quently executed instructions 
observed lookahead scheme moderately superior basic scheme performance difference lookahead correlated schemes small 
examined performance prefetching scheme vary architectural parameters block size memory latency memory bandwidth 
main re sults performance lookahead prefetching best small blocks bytes effective ness significant small memory latency assuming restricted bandwidth interface level memory hierarchy 
observations lead argue hardware prefetching scheme valuable cost effective assist chip data cache backed second level cache access time order magni tude larger 
advocate effective prefetching hardware support unit designed close processor introducing extra gate delays critical path 
hardware pre fetching schemes effective higher levels memory hierarchy 
case latencies magnitude larger processor cycle time prefetch ing data software directed techniques may cial 
software approach lend better mul environments 
research look pos sible ways combine hardware software prefetching 
acknowledgments supported part nsf ccr cda apple computer 
baer 
chen effective chip preloading scheme reduce data access penalty proc 
supercomputing pp 


baer 
wang multi level cache hierarchies tions protocols performance parallel distributed com puting vol 
pp 

ball larus branch prediction free technical report computer science dept univ wis madison 


chen data prefetching high performance processors phd thesis dept computer science engineering univ wash 

chen 
baer reducing memory latency non blocking prefetching caches proc 
fifth int architectural support languages operating systems pp 

chen mahlke chang 
hwu data access microarchitectures superscalar processors compiler assisted data prefetching proc 
th int symp 
microarchitecture 
fu patel data prefetching multiprocessor vector cache memories proc 
th ann 
int symp 
computer architecture pp 

chen baer effective hardware data prefetching high performance processors fu patel stride directed prefetching scalar processors proc 
th int symp 
microarchitecture pp 
io dec 
gannon jalby gallivan strategies cache local memory management global program transformation parallel distributed computing vol 
pp 
oct 
compiler directed data prefetching multiprocessors memory hierarchies proc 
int conf supercomputing pp 

jouppi improving direct mapped cache performance addition small fully associative cache prefetch buffers proc 
th ann 
int symp 
computer architecture pp 
may 
levy architecture software controlled data prefetching proc 
th ann 
int symp 
computer architecture pp 

lockup free instruction cache organization proc 
eighth ann 
int symp 
computer architecture pp 

lee smith branch prediction strategies branch target buffer design computer pp 
jan 
lee 
yew lawrie data prefetching shared memory multiprocessors proc 
int cant parallel processing pp 

lee 
yew lawrie multiprocessor cache design considerations proc 
th ann 
int symp 
computer architecture pp 

mowry gupta tolerating latency prefetching shared memory multiprocessors parallel distributed computing vol 
pp 
june 
mowry lam gupta design evaluation compiler algorithm prefetching proc 
int con architectural support programming languages operating systems pp 


pa improving accuracy dynamic branch prediction branch correlation proc 
int con architectural support programming languages operating systems pp 

smith branch target buffer design optimization technical report ucb csd univ calif berkeley dec 
porterfield software methods improvement cache performance supercomputer applications technical report comp tr rice univ may 
performance impact block sizes fetch strategies proc 
th ann 
int symp 
computer architecture pp 
may 
prefetch unit vector operations scalar computers computer architecture news vol 
pp 
sept 
smith cache memories acm computing surveys vol 
pp 
sept 
smith decoupled access execute computer architectures proc 
ninth ann 
int symp 
pp 

yeh patt implementation level adaptive branch prediction proc 
th ann 
int symp 
computer architecture pp 

tien fu chen received bsc degree computer science national taiwan university taiwan ms phd degrees computer science university washington seattle respec tively 
august faculty member national chung chen university taiwan 
dr chen research interests include design performance evaluation cache memory systems parallel processing distributed systems 
jean baer sm re electrical engineer ing doctorat cycle computer science de grenoble france phd university los les 
dr baer professor computer science engineering adjunct professor electrical engineering university washington 
current research inter ests architecture concentration design evaluation memory hierarchies parallel distributed processing 
dr baer served ieee computer science distinguished visitor acm national lecturer 
fellow editor ieee transactions parallel distributed systems editor jour nal parallel distributed computing editor computer languages 
served program chair international conference parallel processing program chair th international symposium computer architecture general chair th international symposium computer architecture 
