hierarchical reinforcement learning subgoal discovery specialization bakker ias informatics institute university amsterdam kruislaan sj amsterdam science uva nl method described hierarchical reinforcement learning 
high level policies automatically discover subgoals low level policies learn specialize different subgoals 
subgoals represented desired observations cluster raw input data 
experiments shows method outperforms flat methods 
full :10.1.1.114.7182

promise scaling reinforcement learning rl hierarchical rl hrl widely acknowledged 
idea low level policies emit actual primitive actions fast timescale solve parts task 
higher level policies slower timescale solving task sequentially invoking lower level policies considering high level observations actions macro actions options 
level search space reduced temporal credit assignment facilitated low level policies easily reusable task different tasks 
previous hrl hierarchical structure designer 
minimize responsibility learn hierarchical structure 
presents step direction :10.1.1.114.7182
propose hierarchical assignment subgoals learning algorithm high level policies automatically discover subgoals low level policies learn specialize different subgoals 
follows intuition explained contains formal description :10.1.1.114.7182
algorithm 
high level low level policies learn essentially standard value function reinforcement learning algorithms 
high level value function covers complete state space coarse level 
updated external rewards received interaction environment 
low level value functions cover parts state space fine grained level 
updated internal rewards provided high level policy 
action high level policy selection subgoal response new high level observation 
standard value function rl algorithm learn mapping external reward signals received goals reached defined task 
high level observations correspond clusters low level observations obtained unsupervised clustering algorithm 
current high level observation subgoal come set high level observations subgoal high level observation high level policy wants see 
high level observations state abstraction difference existing hrl algorithms 
job low level policies reach subgoal selected highlevel policy 
limited fixed set low level policies 
initially associated possible subgoals 
addition standard state action value function low level policy contains table initially set 
value represents capability low level policy reach subgoals 
values low level policies current subgoal low level policy selected stochastically 
attempts reach current subgoal 
reaches subgoal receives positive internal reward update value subgoal making selection 
reach subgoal receives zero internal reward value updated accordingly selection 
way low level policy may learn reach subgoal subgoal realizes specialization 
may learn capability encompasses multiple subgoals 
realizes generalization 
low level policies specialize generalize 
idea low level policies learning specialize learning additional values important difference existing hrl algorithms 
internal reward provided high level policy active low level policy update state action value function standard rl algorithm 
function approximators represent stateaction value functions low level policy learn focus parts low level observation space relevant specialization 
experiments 
experiments done large office navigation mdps stochastic order test compare standard flat rl methods 
results indicate example mdps works learns significantly faster flat rl methods worked 
low level policies develop meaningful specializations instance office navigation task low level policy exit rooms low level policy navigate corridors 
bakker schmidhuber :10.1.1.114.7182
hierarchical reinforcement learning subgoal discovery specialization 
groen amato yoshida kr se editors proceedings th conference intelligent autonomous systems ias amsterdam netherlands pages 
