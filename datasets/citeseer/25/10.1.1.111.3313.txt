nonlinear dimensionality reduction locally linear embedding sam roweis science doi science resources related article available online www org information current july updated information services including high resolution figures online version article www org cgi content full list selected additional articles science web sites related article www org cgi content full related content article cites articles accessed free www org cgi content full article cited article isi web science 
article cited articles hosted press see www org cgi content full article appears subject collections computers mathematics www org cgi collection comp math information obtaining reprints article obtaining permission reproduce article part www org permissions dtl american association advancement science rights reserved 
title science registered trademark aaas 
copyright science print issn online issn published weekly week december american association advancement science new york avenue nw washington dc 
july www org downloaded 
shepard 
bull 
rev 

tenenbaum adv 
neural info 
proc 
syst 


martinetz schulten neural netw 


kumar gupta karypis parallel computing design analysis algorithms benjamin cummings redwood city ca pp 


beymer poggio science 

available www research att com yann ocr mnist 

simard lecun denker adv 
neural info 
proc 
syst 


order evaluate fits pca mds isomap comparable grounds residual variance 
matrix euclidean distances low dimensional embedding recovered algorithm 
algorithm best estimate intrinsic manifold distances isomap graph distance matrix pca mds euclidean input space distance matrix handwritten mds uses tangent distance 
standard linear correlation coefficient taken entries 
sequence shown intermediate images closest points way endpoints 
synthesize explicit mapping input space low dimensional embedding vice versa nonlinear dimensionality reduction locally linear embedding sam roweis lawrence saul areas science depend exploratory data analysis visualization 
need analyze large amounts multivariate data raises fundamental problem dimensionality reduction discover compact representations high dimensional data 
introduce locally linear embedding lle unsupervised learning algorithm computes low dimensional neighborhood preserving embeddings high dimensional inputs 
clustering methods local dimensionality reduction lle maps inputs single global coordinate system lower dimensionality optimizations involve local minima 
exploiting local symmetries linear reconstructions lle able learn global structure nonlinear manifolds generated images faces documents text 
judge similarity 
mental representations world formed processing large numbers sensory inputs including example pixel intensities images power spectra sounds joint angles articulated bodies 
complex stimuli form represented points high dimensional vector space typically compact description 
coherent structure world leads strong correlations inputs neighboring pixels images generating observations lie close smooth low dimensional manifold 
compare classify observations effect reason world depends crucially modeling nonlinear geometry low dimensional manifolds 
scientists interested exploratory analysis visualization multivariate data face similar problem dimensionality reduction 
problem illustrated fig 
involves mapping high dimensional inputs lowdimensional description space gatsby computational neuroscience unit university college london queen square london wc ar uk 
lab research park avenue florham park nj usa 
mail roweis gatsby ucl ac uk research 
att com coordinates observed modes variability 
previous approaches problem multidimensional scaling mds computed embeddings attempt preserve pairwise distances generalized disparities data points distances measured straight lines sophisticated usages mds isomap ing coordinates corresponding points spaces provided isomap standard supervised learning techniques 

supported mitsubishi electric research laboratories schlumberger foundation nsf dbs darpa human id program 
lecun making available mnist database roweis saul sharing related unpublished 
helpful discussions carlsson freeman griffiths lehrer mahajan reich richards tenenbaum weiss especially bernstein 
august accepted november shortest paths confined manifold observed inputs 
take different approach called locally linear embedding lle eliminates need estimate pairwise distances widely separated data points 
previous methods lle recovers global nonlinear structure locally linear fits 
lle algorithm summarized fig 
simple geometric intuitions 
suppose data consist real valued vectors xi dimensionality sampled underlying manifold 
provided sufficient data manifold sampled expect data point neighbors lie close locally linear patch manifold 
characterize local geometry patches linear coefficients reconstruct data point neighbors 
reconstruction errors measured cost function jw ij xj adds squared distances data points reconstructions 
weights wij summarize contribution jth data point ith reconstruction 
compute weights wij minimize cost fig 

problem nonlinear dimensionality reduction illustrated dimensional data sampled dimensional manifold 
unsupervised learning algorithm discover global internal coordinates manifold signals explicitly indicate data embedded dimensions 
color coding illustrates mapping discovered lle black outlines show neighborhood single point 
lle projections data principal component analysis pca classical mds map faraway data points nearby points plane failing identify underlying structure manifold 
note mixture models local dimensionality reduction cluster data perform pca cluster address problem considered map high dimensional data single global coordinate system lower dimensionality 
www org science vol december july www org downloaded function subject constraints data point xi reconstructed neighbors enforcing wij xj fig 

steps locally linear embedding assign neighbors data point example nearest neighbors 
compute weights ij best linearly reconstruct neighbors solving constrained squares problem eq 

compute low dimensional embedding vectors best reconstructed ij minimizing eq 
finding smallest eigenmodes sparse symmetric matrix eq 

weights ij vectors computed methods linear algebra constraint points reconstructed neighbors result highly nonlinear embeddings 
belong set neighbors xi second rows weight matrix sum ij 
optimal weights fig 

images faces mapped embedding space described coordinates lle 
representative faces shown circled points different parts space 
bottom images correspond points top right path linked solid line illustrating particular mode variability pose expression 
december vol science www org wij subject constraints solving squares problem 
constrained weights minimize reconstruction errors obey important symmetry particular data point invariant rotations rescalings translations data point neighbors 
symmetry follows reconstruction weights characterize intrinsic geometric properties neighborhood opposed properties depend particular frame 
note invariance translations specifically enforced sum constraint rows weight matrix 
suppose data lie near smooth nonlinear manifold lower dimensionality approximation exists linear mapping consisting translation rotation rescaling maps high dimensional coordinates neighborhood global internal coordinates manifold 
design reconstruction weights wij reflect intrinsic geometric properties data invariant exactly transformations 
expect characterization local geometry original data space equally valid local patches manifold 
particular weights wij reconstruct ith data point dimensions reconstruct embedded manifold coordinates dimensions 
lle constructs neighborhood preserving mapping idea 
final step algorithm high dimensional observation xi mapped low dimensional vector yi representing global internal coordinates manifold 
done choosing dimensional coordinates yi minimize embedding cost function yi jw cost function previous locally linear reconstruction errors fix weights ij opti coordinates yi 
embedding cost eq 
defines quadratic form vectors yi subject constraints problem posed minimized solving sparse eigenvalue problem bottom nonzero eigenvectors provide ordered set orthogonal coordinates centered origin 
implementation algorithm straightforward 
experiments data points reconstructed nearest neighbors measured euclidean distance normalized dot products 
implementations lle algorithm free parameter number neighbors neighbors chosen optimal weights wij coordinates yi july www org downloaded computed standard methods linear algebra 
algorithm involves single pass steps fig 
finds global minima reconstruction embedding costs eqs 

addition example fig 
true manifold structure known applied lle images faces vectors word document counts 
dimensional embeddings faces words shown figs 

note coordinates embedding spaces related meaningful attributes pose expression human faces semantic associations words 
popular learning algorithms nonlinear dimensionality reduction share favorable properties lle 
iterative hill climbing methods autoencoder neural networks self organizing maps latent variable models guarantees global optimality convergence tend involve free parameters learning rates convergence criteria ar fig 

arranging words continuous semantic space 
word initially represented high dimensional vector counted number times appeared different encyclopedia articles 
lle applied word document count vectors resulting embedding location word 
shown words different bounded regions embedding space discovered lle 
panel shows twodimensional projection third fourth coordinates lle dimensions regions highly overlapped 
inset shows dimensional projection third fourth fifth coordinates revealing extra dimension regions separated 
words lie intersection regions capitalized 
note lle words similar contexts continuous semantic space 
specifications 
nonlinear methods rely deterministic annealing schemes avoid local minima optimizations lle especially tractable 
lle scales intrinsic manifold dimensionality require discretized embedding space 
dimensions added embedding space existing ones change lle rerun compute higher dimensional embeddings 
methods principal curves surfaces additive component models lle limited practice manifolds extremely low dimensionality 
intrinsic value estimated analyzing reciprocal cost function reconstruction weights derived embedding vectors yi applied data points xi lle illustrates general principle manifold learning elucidated martinetz schulten tenenbaum overlapping local neighborhoods collectively provide information global geometry 
virtues lle shared tenenbaum algorithm isomap successfully applied similar problems nonlinear dimensionality reduction 
isomap embeddings optimized preserve geodesic distances general pairs data points estimated computing shortest paths large sublattices data 
lle takes different approach analyzing local symmetries linear coefficients reconstruction errors global constraints pairwise distances stress functions 
avoids need solve large dynamic programming problems tends accumulate sparse matrices structure exploited savings time space 
lle useful combination methods data analysis statistical learning 
example parametric mapping observation embedding spaces learned supervised neural networks target values generated lle 
lle generalized harder settings case disjoint data manifolds specialized simpler ones case time ordered observations 
greatest potential lies applying lle diverse problems considered 
broad appeal traditional methods pca mds algorithm find widespread areas science 
notes 
littman dean buja computing science statistics proceedings th symposium interface newton ed 
interface foundation north america fairfax station va pp 


cox cox multidimensional scaling chapman hall london 

young psychometrika 

tenenbaum advances neural information processing jordan kearns solla eds 
mit press cambridge ma pp 


set neighbors data point assigned variety ways choosing nearest neighbors euclidean distance considering data points ball fixed radius prior knowledge 
note fixed number neighbors maximum number embedding dimensions lle expected recover strictly number neighbors 

certain applications constrain weights positive requiring reconstruction data point lie convex hull neighbors 

fits constrained weights best reconstruct data point neighbors computed closed form 
consider particular data point neighbors sum reconstruction weights wj reconstruction error wj minimized steps 
evaluate inner products neighbors compute neighborhood correlation matrix cjk matrix inverse second compute lagrange multiplier enforces sum constraint jk jk third compute reconstruction weights wj kc jk 
correlation matrix www org science vol december july www org downloaded nearly singular conditioned inversion adding small multiple identity matrix 
amounts penalizing large weights exploit correlations level precision data sampling process 

lle require original data described single coordinate system data point located relation neighbors 

embedding vectors yi minimizing cost function jw ij fixed weights wij optimization performed subject constraints problem posed 
clear coordinates translated constant displacement affecting cost 
remove degree freedom requiring coordinates centered origin 
avoid degenerate solutions constrain embedding vectors unit covariance outer products satisfy identity matrix 
cost defines quadratic form ij mij involving inner products embedding vectors symmetric matrix mij ij wij wji ij ifi 
optimal embedding global rotation embedding space computing bottom eigenvectors matrix 
bottom eigenvector matrix discard unit vector equal components represents free translation mode eigenvalue zero 
discarding enforces constraint embeddings zero mean 
remaining eigenvectors form embedding coordinates lle 
note matrix stored manipulated sparse matrix giving substantial computational savings large values bottom eigenvectors corresponding smallest eigenvalues efficiently performing full matrix diagonalization 

manifold data points fig 
sampled manifold shown fig 

nearest neighbors determined euclidean distance 
particular manifold introduced tenenbaum showed global structure learned isomap algorithm 

faces multiple photographs face digitized grayscale images 
image treated lle data vector elements corresponding raw pixel intensities 
nearest neighbors determined euclidean distance pixel space 

words word document counts tabulated words articles encyclopedia 
nearest neighbors determined dot products count vectors normalized unit length 

demers cottrell advances neural information processing systems hanson cowan giles eds 
kaufmann san mateo ca pp 


kramer aiche 

kohonen self organization associative memory springer verlag berlin 

bishop williams neural comput 


buhmann pattern recognition 

hastie stuetzle am 
stat 
assoc 


donnell buja stuetzle ann 
stat 


martinetz schulten neural networks 

beymer poggio science 

examples considered data single connected component possible formulate lle data lies disjoint manifolds possibly different underlying dimensionality 
suppose form graph connecting data point neighbors 
number connected components detected ex december vol science www org powers adjacency matrix 
different connected components data essentially decoupled eigenvector problem lle 
best interpreted lying distinct manifolds best analyzed separately lle 

neighbors correspond nearby observations time reconstruction weights computed online data collected embedding diagonalizing sparse banded matrix 

horn johnson matrix analysis cambridge univ press cambridge 

bai demmel dongarra ruhe van der vorst eds templates solution algebraic eigenvalue problems practical guide society industrial applied mathematics philadelphia pa 

lee seung nature 

tarjan data structures network algorithms cbms society industrial applied mathematics philadelphia pa 

jolliffe principal component analysis springer verlag new york 

leen neural comput 


hinton revow sharing unpublished university toronto segmentation pose estimation motivated think globally fit locally tenenbaum stanford university stimulating discussions sharing code isomap algorithm lee bell labs frey university waterloo making available word face data previous buja dayan ghahramani hinton jaakkola lee pereira helpful comments 
acknowledges support gatsby charitable foundation national science foundation national sciences engineering research council canada 
august accepted november july www org downloaded 
