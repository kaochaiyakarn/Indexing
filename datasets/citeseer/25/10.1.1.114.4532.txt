libsvm library support vector machines chih chung chang chih jen lin updated january libsvm library support vector machines svm 
goal help users easily svm tool 
document implementation details 
libsvm readme file included package libsvm faq provide information 
libsvm library support vector classification svm regression 
goal users easily svm tool 
document implementation details 
libsvm readme file included package provides information 
section show formulations libsvm support vector classification svc support vector classification svc distribution estimation class svm support vector regression svr support vector regression svr 
discuss implementation solving quadratic problems section 
section describes implementation techniques shrinking caching 
section discuss imple mentation multi class classification 
support different penalty parameters unbalanced data 
details section 
model selection important obtain ing best generalization 
libsvm provides simple useful tools discussed 
formulations support vector classification binary case training vectors xi 
classes vector yi svc cortes vapnik vapnik solves primal problem min wt subject yi xi 

department computer science information engineering national taiwan university taipei taiwan www csie ntu edu tw cjlin 
mail cjlin csie ntu edu tw dual min subject 
vector ones upper bound positive semidefinite matrix qij xi xj xi xj xi xj kernel 
training vectors xi mapped higher infinite dimensional space function 
decision function sgn yi ik xi 
support vector classification binary case support vector classification sch lkopf uses new parameter control number support vectors errors 
parameter upper bound fraction training errors lower bound fraction support vectors 
training vectors xi 
classes vector yi primal form considered dual qij xi xj 
decision function min wt subject yi xi min 

subject 
sgn 
yi xi 
crisp burges chang lin shown replaced 
property libsvm solve scaled version min subject 

output computed decision function margins sgn yi xi svc 
yi xi distribution estimation class svm class svm proposed sch lkopf estimating support high dimensional distribution 
training vectors xi 
class information primal form sch lkopf dual min wt subject xi min qij xi xj xi xj 

subject 
libsvm solve scaled version min subject 
decision function sgn ik xi 
support vector regression svr set data points 
xl zl xi input zi target output standard form support vector regression vapnik dual min subject min wt subject xi zi zi xi 
zi 
qij xi xj xi xj 
approximate function xi support vector regression svr similar svc regression sch lkopf parameter control number support vectors 
svc replaced replaces parameter svr 
primal form min wt subject xi zi zi xi 

dual min subject 
similarly inequality replaced equality 
libsvm consider dual problem solved min decision function svr 
subject cl 

xi solving quadratic problems decomposition method svc svr class svm consider general form svc svr class svm min subject 
yt 
clearly seen svc class svm form 
svr consider reformulation min subject zt zt 
vector yt 
yt 

difficulty solving density qij general zero 
libsvm consider decomposition method conquer difficulty 
method example osuna joachims platt 
method modifies subset iteration 
subset denoted working set leads small sub problem minimized iteration 
extreme case sequential minimal optimization smo platt restricts elements 
iteration solves simple variable problem needing optimization software 
consider smo type decomposition method proposed fan 
algorithm smo type decomposition method fan 
find initial feasible solution 
set 
stationary point 
find element working set wss described section 
define 
sub vectors corresponding respectively 

aij kii kjj kij solve sub problem variable min qij qij pb subject solve 
set step 
min yi yj qij qij aij subject constraints 
optimal solution pb set goto note updated iteration 
simplify notation simply aij concave problem 
convex modification 
stopping criteria working set selection svc svr class svm karush kuhn tucker kkt optimality condition shows vector stationary point number nonnegative vectors 
gradient 
condition rewritten yi defining 
yt yt yt yt feasible stationary point max yi min yi stopping condition 
selection working set set consider procedure wss 
define select ats kss bts yt ys ts ats ats 
arg max yt arg min yt yi 

return 
details choose working set fan section ii 
convergence decomposition method see fan section iii chen detailed discussion convergence algorithm 
decomposition method svc svr svc svr considered general form define min kkt condition shows yi kkt condition hand yi subject 


tolerance stopping condition max min yi max yi min yi max yi 
working set selection extending wss wss extending wss svm 
find 
find ip arg mp jp arg min ipt ipt arg mn jn arg min int int yt yt yip ip 
yt yt yin 

return ip jp jn depending gives smaller ij ij 
analytical solutions details described section discuss solution general sub problem 
calculation solution dual optimization problem obtained variables calculated decision function 
simply describe case svc svr appear 
formulations simplified cases 
kkt condition shown 
consider case yi 
satisfy practically avoid numerical errors average yi yi hand satisfy take midpoint range 
max yi min yi yi calculate similar way 
obtained shrinking caching shrinking problems number free support vectors small shrinking technique reduces size working problem considering bounded variables joachims 
near iterative process tion method identifies possible set final free may reside 
theorem shows final iterations decomposition proposed section variables corresponding small set allowed move theorem theorem iv fan assume positive semi definite 

set independent optimal solution yi yi 
problem unique bounded optimal solutions 
assume algorithm generates infinite sequence 
reached unique bounded optimal solution 
remains subsequent iterations yt 
solving problem decomposition method works smaller problem min pa subject 

set shrunken variables 
course heuristic may fail optimal solution corresponding part 
happens problem starting point optimal solution bounded variables identified shrinking process 
note solving shrunken problem know gradient pa 
problem reconstruct gradient quite expensive 
implementations began shrinking procedure near iterative process libsvm start shrinking process 
procedure follows 
min iterations try shrink variables 
note iterative process satisfied 
theorem conjecture variables set shrunken bounded bounded yt yt yt yt 
set activated variables dynamically reduced min iterations 

course shrinking strategy may aggressive 
decomposition method slow convergence large portion iterations spent achieving final digit required accuracy iterations wasted wrongly shrunken problem 
decomposition method achieves tolerance specified stopping criteria reconstruct gradient 
inactive variables current set 
decomposition method continues 
libsvm size set dynamically reduced 
decrease cost reconstructing gradient iterations keep gi gradient qij 
qij pi gi qij pi 
svc svr stopping condition different set modified 
yt shrink elements set yt yt 
set yt similar 
caching technique reducing computational time caching 
fully dense may stored computer memory elements qij calculated needed 
usually special storage idea cache store qij joachims 
computational cost iterations reduced 
theorem supports cache final iterations columns matrix needed 
cache contain columns avoid kernel evaluations final iterations 
libsvm implement simple strategy cache 
dynam ically cache columns 
computational complexity discussion section asymptotic convergence decomposition method 
discuss computational complexity 
main operations finding pb update 
note working set selection stopping condition 
considered pb sub matrix column kth iteration right hand side construct sub problem 
sub problem solved employed 
elements solving sub problem easy main cost 
operation takes available cache kernel evaluation costs column indexes needs ln 
complexity 
iterations columns cached iterations 

iterations nl columns cached iterations kernel evaluation 
note shrinking incorporated gradually decrease iterations 
unbalanced data classification problems numbers data different classes unbalanced 
researchers 
osuna proposed different penalty param eters svm formulation example svm dual min wt yi yi subject yi xi min 
subject yi yi 
note replacing different ci 
analysis earlier correct 
just special case 
implementation 
main difference solution sub problem 
min qij qi qj subject yi yj ci cj ci cj depending yi yj 
di dj di dj 
written min di dj di dj qij di di qij dj dj subject di dj define aij bij 
note aij modification similar 
di dj objective function written ij bij dj 
new ij new ij modify back feasible region consider case yi yj write new feasible new region new ij new ij new regions set ci ci 
course check region 
ci cj new ci 
cases similar 
procedure identify new different regions change back feasible set 
double quad coef quad coef quad coef tau double delta quad coef double diff alpha alpha new alpha delta alpha delta diff alpha region alpha alpha diff alpha region alpha alpha diff diff alpha region alpha alpha diff alpha region alpha alpha diff multi class classification approach classifiers constructed trains data different classes 
strategy svm friedman kre el 
training data ith jth classes solve binary classification problem min ij ij ij wij ij ij subject ij xt ij ij xt ith class ij xt ij ij xt jth class ij 
classification voting strategy binary classification considered voting votes cast data points point designated class maximum number votes 
case classes identical votes may strategy simply select smallest index 
methods multi class classification 
reasons choose approach detailed comparisons hsu lin 
model selection libsvm provides model selection tool rbf kernel cross validation parallel grid search 
currently support svc parameters considered 
easily modified kernels linear polynomial 
median sized problems cross validation reliable way model selection 
training data separated folds 
sequentially fold considered validation set rest training 
average accuracy predicting validation sets cross validation accuracy 
implementation follows 
users provide possible interval grid space 
grid points tried see gives highest cross validation accuracy 
users best parameter train training set generate final model 
easy implementation consider svm parameters indepen dent problem 
different jobs easily solve parallel 
currently libsvm provides simple tool jobs dispatched cluster computers share file system 
note method training multi class data 
final model decision functions share 
contour plot heart scale included libsvm package libsvm outputs contour plot cross validation accuracy 
example 
probability estimates originally support vector classification regression predicts class label approximate target value probability information 
briefly describe extend svm probability estimates 
details wu classification lin weng regression 
classes data goal estimate pi 
setting pairwise approach multi class classi fication estimated pairwise class probabilities rij improved implementation lin platt rij ea estimated minimizing negative log likelihood function known training data decision values labels decision values required independent conduct fold cross validation obtain decision values 
second approach wu obtain pi rij solves optimization problem min objective function comes equality subject pi pi 
reformulated qij min pt qp si 
problem convex optimality conditions scalar vector ones vector zeros lagrangian multiplier equality constraint pi 
directly solving linear system derive simple iterative method 
solution satisfies qp bq bp qp 
consider algorithm algorithm 
start initial pi pi 

repeat 

satisfied 
pt qp normalize procedure guarantees find global optimum 
tricks need recalculate qp iteration 
detailed implementation notes appendix wu 
consider relative stopping condition algorithm qp max qp qp large closer zero decrease tolerance factor discuss svr probability inference 
set training data xi yi xi yi 
suppose data collected model yi xi underlying function independent identically distributed random noises 
test data distribution allows draw probabilistic inferences example construct predictive interval pre specified probability 
denoting estimated function svr sample residual prediction error equivalent 
propose model distribution set sample residuals training data generated conducting fold cross validation get fj 
setting yi fj xi xi yi jth fold 
conceptually clear distribution may resemble prediction error 
illustrates real data 
basically discretized distribution model data complex retained 
contrary distributions gaussian laplace commonly noise models require location scale parameters 
plot fitted curves families histogram shows distri bution symmetric zero gaussian laplace reasonably capture shape propose model zero mean gaussian laplace equivalently model conditional distribution gaussian laplace mean 
lin weng discussed method judge laplace gaussian distribution 
experimentally show cases tried laplace better 
consider zero mean laplace density function 
assuming independent estimate scale parameter maximizing likelihood 
laplace maximum likelihood estimate 
lin weng pointed extreme may cause inaccurate esti mation 
propose estimate scale parameter discarding exceed standard deviation 
new data consider random variable laplace distribution parameter 
theory distribution may depend input assume free similar model classification 
assumption works practice leads simple model 
histogram data set modeling laplace gaussian distributions 
axis fold cv axis normalized number data bin width 
acknowledgments supported part national science council taiwan nsc nsc 
authors chih wei hsu jen hao lee helpful discussions comments 
lily tian useful comments 

chang 
lin 
training support vector classifiers theory algorithms 
neural computation 

chen 
fan 
lin 
study smo type decomposition methods support vector machines 
ieee transactions neural networks 
url www csie ntu edu tw cjlin papers pdf 
appear 
cortes vapnik 
support vector network 
machine learning 
crisp burges 
geometric interpretation svm classifiers 
solla leen 
ller editors advances neural information processing systems volume cambridge ma 
mit press 

fan 
chen 
lin 
working set selection second order informa tion training svm 
journal machine learning research 
url www csie ntu edu tw cjlin papers pdf 
friedman 
approach classification 
technical report department statistics stanford university 
available www stat stanford edu reports friedman poly ps 
hsu 
lin 
comparison methods multi class support vector machines 
ieee transactions neural networks 
joachims 
making large scale svm learning practical 
sch lkopf burges smola editors advances kernel methods support vector learning cam bridge ma 
mit press 
personnaz dreyfus 
single layer learning revisited stepwise procedure building training neural network 
fogelman editor neurocomputing algorithms architectures applications 
springer verlag 
kre el 
pairwise classification support vector machines 
sch lkopf burges smola editors advances kernel methods support vector learn ing pages cambridge ma 
mit press 

lin weng 
simple probabilistic predictions support vector regression 
technical report department computer science national taiwan university 
url www csie ntu edu tw cjlin papers pdf 

lin 
lin weng 
note platt probabilistic outputs support vector machines 
technical report department computer science national taiwan university 
url www csie ntu edu tw cjlin papers ps 
osuna freund girosi 
training support vector machines application face detection 
proceedings cvpr pages new york ny 
ieee 
osuna freund girosi 
support vector machines training applications 
ai memo massachusetts institute technology 
platt 
probabilistic outputs support vector machines comparison regularized likelihood methods 
smola bartlett sch lkopf schuurmans editors advances large margin classifiers cambridge ma 
mit press 
url citeseer 
nj nec com platt probabilistic html 
platt 
fast training support vector machines sequential minimal optimization 
sch lkopf burges smola editors advances kernel methods support vector learning cambridge ma 
mit press 
sch lkopf smola williamson bartlett 
new support vector algo rithms 
neural computation 
sch lkopf platt shawe taylor smola williamson 
estimating support high dimensional distribution 
neural computation 
vapnik 
statistical learning theory 
wiley new york ny 

wu 
lin weng 
probability estimates multi class classification pairwise coupling 
journal machine learning research 
url www csie ntu edu tw cjlin papers pdf 

