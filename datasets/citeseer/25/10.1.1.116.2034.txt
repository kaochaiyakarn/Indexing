maximum entropy markov models information extraction segmentation andrew mccallum mccallum com dayne freitag dayne com just research henry street pittsburgh pa usa fernando pereira pereira research att com labs research park ave florham park nj usa hidden markov models hmms powerful probabilistic tool modeling sequential data applied success text related tasks part speech tagging text segmentation information extraction 
cases observations usually modeled multinomial distributions discrete vocabulary hmm parameters set maximize likelihood observations 
presents new markovian sequence model closely related hmms allows observations represented arbitrary overlapping features word capitalization formatting part speech defines conditional probability state sequences observation sequences 
maximum entropy framework fit set exponential models represent probability state observation previous state 
positive experimental results segmentation faq 
large volume text available internet causing increasing interest algorithms automatically process mine information text 
hidden markov models hmms powerful tool representing sequential data applied significant success text related tasks including part speech tagging kupiec text segmentation event tracking yamron carp gillick lowe van named entity recognition bikel schwartz weischedel information extraction leek freitag mccallum 
hmms probabilistic finite state models parameters state transition probabilities state specific observation probabilities 
greatly contributing popularity availability straightforward procedures training maximum likelihood baum welch trained models find hidden state sequence corresponding observation sequence viterbi 
text related tasks observation probabilities typically represented multinomial distribution discrete finite vocabulary words baum welch training learn parameters maximize probability observation sequences training data 
problems traditional approach 
tasks benefit richer representation observations particular representation describes observations terms overlapping features capitalization word endings part speech formatting position page node memberships wordnet addition traditional word identity 
example trying extract previously unseen names newswire article identity word predictive knowing word capitalized noun appositive appears near top article quite predictive conjunction context provided state transition structure 
note features independent 
furthermore applications set possible observations reasonably enumerable 
example may beneficial observations lines text 
unreasonable build multinomial distribution dimensions possible lines text 
consider task segmenting questions answers frequently asked questions list faq 
features indicative segmentation just individual words features line line length indentation total amount whitespace percentage non alphabetic characters grammatical features 
observations parameterized overlapping features 
second problem traditional approach sets hmm parameters maximize likelihood observation sequence text applications including listed task predict state sequence observation sequence 
words traditional approach inappropriately uses generative joint model order solve conditional problem observations 
introduces maximum entropy markov models memms address concerns 
allow non independent difficult enumerate observation features move away generative joint probability parameterization hmms conditional model represents probability reaching state observation previous state 
conditional probabilities specified exponential models arbitrary observation features 
exponential models follow maximum entropy argument trained generalized iterative scaling gis darroch ratcliff similar form computational cost expectation maximization em algorithm dempster laird rubin 
classic problems rabiner hmms straightforwardly solved new model new variants forward backward viterbi baum welch algorithms :10.1.1.131.2084
remainder describes alternative model detail explains fit parameters gis known unknown state sequences presents variant forward backward procedure solutions classic problems follow naturally 
give experimental results problem extracting question answer pairs lists frequently asked questions faqs showing model increases precision recall factor 

maximum entropy markov models hidden markov model hmm finite state automaton stochastic state transitions observations rabiner :10.1.1.131.2084
automaton models probabilistic generative process sequence observations produced starting state emitting observation selected state transitioning new state emitting observation designated final state reached 
formally hmm finite set states set possible observations conditional probability distributions state transition probability observation probability states observations represented features defer discussion refinement 
st st 
dependency graph traditional hmm conditional maximum entropy markov model 
distribution initial state distribution run hmm pairs observation sequence state sequence tasks set possible observations typically finite character set vocabulary 
supervised task information extraction sequence labels attached training observation sequence novel observation objective recover label sequence 
typically done models associate states possible label 
mapping labels states sequence states known training instance state sequence estimated 
label unlabeled observation sequence viterbi path calculated labels associated path returned 
new model alternative hmms propose maximum entropy markov models memms hmm transition observation functions replaced single function provides probability current state previous state current observation model applications hmms observations reflecting fact don care probability probability state sequence label sequence induce 
contrast hmms current observation depends current state current observation memm may depend previous state 
helpful think observations associated state transitions states 
model form probabilistic finite state acceptor paz probability transition state state input follows split separately trained transition functions functions exponential model described section 
discuss solve state estimation problem new framework 
state estimation observations despite differences new model hmms efficient dynamic programming solution classic problem identifying state sequence observation sequence 
viterbi algorithm hmms fills dynamic programming table forward probabilities defined probability producing observation sequence time state time recursive viterbi step new model redefine probability state time observation sequence time recursive viterbi step corresponding backward probability baum welch discussed probability starting state time observation sequence time recursive step simply space limitations prevent full description viterbi baum welch see rabiner excellent tutorial 
exponential model transitions state observation transition functions separate transition observation functions hmms allows model transitions terms multiple features observations believe valuable contribution 
turn exponential models fit maximum entropy 
maximum entropy framework estimating probability distributions data 
principle best model data consistent certain constraints derived training data fewest possible assumptions 
probabilistic framework distribution fewest possible assumptions closest uniform distribution highest entropy 
constraint expresses characteristic training data learned distribution 
constraints binary features 
examples features observation word apple observation capitalized word observations lines text time observation line text noun phrases conditional maximum entropy models features binary features maximum entropy framework general handle real valued features 
depend observation outcome predicted function modeled 
function transition function outcome new current state feature gives function arguments current observation possible new current state pair binary feature observation destination state true algorithm description follows expressed terms generic feature particular feature decomposition 
furthermore suggest general features may useful 
constraints apply expected value feature learned distribution average training observation sequence corresponding state sequence 
formally previous state feature transition function property time steps time steps involve transition function maximum entropy distribution satisfies constraints della pietra della pietra lafferty unique agrees maximum likelihood distribution exponential form parameters learned normalizing factor distribution sum states parameter estimation generalized iterative scaling gis darroch ratcliff finds iteratively values form maximum entropy solution transition function eq 

requires values features sum arbitrary constant context true true adding new ordinal valued correction feature chosen large 
inputs observation sequence corresponding sequence labels certain number states label potentially having restricted transition structure 
set observation state features 
determine state sequence associated observation label sequence 
ambiguous determined probabilistically iterating steps em 
deposit state observation pairs corresponding previous states training data state transition function find entropy solution state discriminative function running gis 
output maximum entropy markov model takes unlabeled sequence observations predicts corresponding labels 
table 
outline algorithm estimating parameters maximum entropy markov model 
application gis learning transition function state consists steps 
calculate training data average feature 
start iteration gis arbitrary parameter values say 
iteration current values eq 
calculate expected value feature 
step satisfying constraints changing bring expected value feature closer corresponding training data average 
convergence reached return step 
summarize memm training procedure split training data events state pairs relevant transitions state 
assume moment labels training sequence state sequence unambiguously known 
apply gis feature statistics events assigned order induce transition function set functions defines desired maximum entropy markov model 
table contains overview maximum entropy markov model training algorithm 
parameter estimation unknown state procedure described assumes state sequence training observation sequence known states predicted test time training time 
useful able train state sequence known 
example may state label label sequence may ambiguous state produced label instance 
variant baum welch algorithm 
step calculates state occupancies forwardbackward algorithm current transition functions 
step uses gis procedure feature frequencies step state occupancies compute new transition functions 
maximize likelihood label sequence observations 
note gis run convergence step doing example generalized expectation maximization gem guaranteed converge local maximum 
notice baum welch variant unlabeled partially labeled training sequences state unknown label missing 
models trained combination labeled unlabeled data extremely helpful labeled data sparse 
variations far described particular method maximum entropy markov models possibilities 
factored state representation 
difficulty memms share hmms transition parameters making data sparseness serious problem number states increases 
recall model observations associated transitions states 
advantages expressive power comes cost having parameters 
hmms related graphical models tied parameters factored state representations ghahramani jordan kanazawa koller russell alleviate difficulty 
achieve similar effect memms splitting different functions distributed representation previous state collection features weights set maximum entropy just done observations 
example state features include consumed start time extraction field haven exited preamble document subject previous sentence female paragraph answer second order features linking observation state features 
representation information shared different source states reducing number parameters improving generalization 
furthermore proposal require difficult step hand crafting parameter tying scheme graphical model state transition function required hmms graphical models 
observations states transitions 
combining transition emission parameters single function transition probabilities represented traditional multinomial influence observations represented maximum entropy exponential method correcting simple multinomial prior adding extra features maximum entropy previously various statistical language modeling problems 
include combination traditional trigrams trigger word features rosenfeld combination arbitrary features sentences trigram models chen rosenfeld 
note observation previous state treated independent evidence current state 
approach put observations back states transitions 
reduce number parameters useful training data especially sparse 
environmental model reinforcement learning 
transition function include action resulting model suitable representing reinforcement agent 
dependency action modeled separate functions action factored actions terms arbitrary overlapping features steer left beep raise arm certain particular actions states observations strong interactions modeled features represent conjunction 

experimental results tested method collection files belonging usenet multi part faqs downloaded internet 
documents data set organized basic structure contains header see www cs cmu edu mccallum 
table 
excerpt labeled faq 
lines truncated reasons space 
tags beginnings lines inserted manually 
head nntp poster head head archive name faq part head frequency monthly head question configuration serial cable answer answer follows diagram necessary connections answer programs properly 
far know answer agreed commercial software developers fo answer answer pins connected inside answer avoid known serial port chip bugs 
table 
line features experiments 
begins number contains question mark begins ordinal contains question word begins punctuation ends question mark begins question word alpha capitalized begins subject indented blank indented contains indented contains bracketed number third space contains punctuation contains non space prev blank contains number prev begins ordinal contains pipe shorter includes text usenet header format occasionally preamble table contents series question answer pairs tail typically includes items copyright notices acknowledgments various artifacts reflecting origin document 
formatting regularities indentation numbered questions styles paragraph breaks 
multiple documents belonging single faq formatted consistent manner considerable variation different faqs 
labeled line document collection categories role document head question answer tail corresponding parts documents described previous paragraph 
table shows excerpt labeled faq 
object task recover labels 
excerpt demonstrates difficulty recovering line classifications looking tokens occur line 
particular numerals answer easily confuse token classifier 
defined boolean features lines shown table believed useful determining class line 
effort control statistical dependence pairs features 
set contains feature pairs mutually disjoint features represent partitions data overlap varying de 
note usefulness particular feature indented depends formatting conventions particular faq 
results section meant answer question memm trained single manually labeled document label novel documents formatted conventions 
experiments treat group documents belonging faq separate dataset 
train model single document group test remaining documents group 
words perform leave minus evaluation 
group documents yields results 
scores average performance faqs collection 
sequence lines test document memm viterbi algorithm compute state sequence 
consider metrics evaluating predicted sequences 
occurrence agreement probability proposed beeferman berger lafferty act pred act pred probability distribution set distances lines act lines actual segment pred similar indicator function predicted segmentation xnor function 
metric gives empirical probability actual predicted segmentations agree placement lines drawn computing define segment unbroken sequence lines label 
beeferman 
exponential distribution depending parameter calculated features dataset average document length 
simplicity set uniform distribution width 
words measures probability lines lines placed correctly predicted segmentation 
constrast reflects probability segment boundaries properly identified learner ignores labels assigned segments metrics count correct predicted segments right labels 
segment counted correct boundaries label question actual segment 
segmentation precision sp number correctly identified segments divided number segments predicted 
segmentation recall sr number correctly identified segments divided number actual segments 
tested different models dataset table 
occurrence agreement probability segmentation precision segmentation recall learners faq dataset 
averages confidence intervals 
learner stateless memm stateless single maximum entropy classifier trained applied line independently features shown table 
stateless considered typical approach treats lines isolation context 
traditional fully connected hmm states line categories 
states hmm emit individual tokens groups alphanumeric characters individual punctuation characters 
observation distribution state smoothed multinomial possible tokens 
label assigned line assigned state responsible emitting tokens line 
computing state sequence document model allowed switch states line boundaries ensuring tokens line share label 
model previous information extraction hmms freitag mccallum 
identical lines document converted sequences features table 
feature tests true line unique symbol inserted corresponding line converted document 
hmm trained emit symbols 
notice emission model state case na bayes model 
memm maximum entropy markov model described 
hmms model contains labeled states fully connected 
note training fully supervised sequence states training document passes unambiguous 
consequently training involve baum welch reestimation 
table shows performance models faq data 
clear table memm best methods tested 
results support claims underpin research problem 
representing lines terms features salient problem hand far effective token level representation essential difference 
surprisingly access meaningful features segmentation problem 
performance stateless show possible classify lines unambiguously features 
second claim structural regularities left header classify lines header lines critical 
markov models provide convenient way model regularities 
purposes segmentation results suggest important model structure access line features 
scores markov model methods metric indicate reasonably segmenting faqs constituent parts 
particular segments memm 
stringent metrics sp sr punish misclassified line predicted segment hint main shortcoming non maximum entropy markov models occasionally interpolate bad predictions correctly handled segments 
precision sp score memm significant practical implications 
results segmentation automatic system precision critical 
case faqs system question answering system described literature burke hammond lytinen 
segmentation returned probably high quality manual intervention rule postprocessing memm segmentation 

related wide range machine learning techniques information extraction text segmentation 
focus exclusively techniques probabilistic models 
non probabilistic methods memory techniques argamon dagan transformation learning brill winnow combinations linear classifiers roth give normalized scores decision combined scores decision sequences 
support standard dynamic programming methods finding best segmentation viterbi resort sub optimal methods label observation sequence 
furthermore support hidden variable reestimation baum welch methods required missing incomplete training labels 
exponential models derived maximum entropy applied considerable success natural language tasks including language modeling speech recognition rosenfeld chen rosenfeld segmentation newswire stories beeferman part speech tagging prepositional phrase attachment parsing ratnaparkhi 
hmms successful similar naturallanguage tasks including part speech tagging kupiec named entity recognition bikel information extraction leek freitag mccallum 
know previous general method combines rich state representation markov models flexible feature combination exponential models 
mene named entity recognizer borthwick sterling agichtein grishman uses exponential model label word label indicating position word labeled entity class start inside singleton conditioning information include previous label model 
closer stateless model 
possible inferior performance compared hmm named entity recognizer bikel may similar causes corresponding weakness stateless relative experiments lack representation sequential dependencies 
model closest proposal part speech tagger ratnaparkhi 
starts model joint distribution word sequences corresponding part speech tags practical form model conditional markov model states encode past parts speech features previous words 
model splits transition functions different source states ratnaparkhi combines single exponential model complex may handle sparse data better 
note allow arbitrary state transition structures relatively expressive context representation allow 
direct inspiration model markov processes curves saul defines class conditional markov models mapping continuous segments trajectory acoustic space states representing phonetic distinctions 
model simpler discrete time version observation conditional markovian architecture 

shown possible combine advantages hmms maximum entropy models gen eral model allows state transitions depend features sequence analysis 
new model performs considerably better hmms stateless maximum entropy models task segmenting faqs questions answers believe technique advantageously applied text related applications example named entity recognition 
believe distributed state representation connected topologies may facilitate applications demanding tasks information extraction large vocabulary features automatic feature generation selection della pietra 

believe worth investigating training partially labeled data combination baum welch gis discussed earlier 
longer term combination maximum entropy conditional parameterization may useful wider range graphical models finite state networks 
john lafferty helpful discussions training unknown state associating observations states transitions kamal nigam help gis michael collins guidance related 
argamon dagan 

memorybased approach learning shallow natural language patterns 
coling acl pp 
new brunswick new jersey 
association computational linguistics 
beeferman berger lafferty 

statistical models text segmentation 
machine learning 
bikel schwartz weischedel 

algorithm learns name 
machine learning journal 
borthwick sterling agichtein grishman 

exploiting diverse knowledge sources maximum entropy named entity recognition 
proceedings sixth workshop large corpora new brunswick new jersey 
association computational linguistics 
brill 

transformation error driven learning natural language processing case study part speech tagging 
computational linguistics 
burke hammond lytinen 

question answering frequently asked question files experiences faq finder system 
ai magazine 
chen rosenfeld 

efficient sampling feature selection sentence maximum entropy language models 
proceedings icassp 
ieee 
darroch ratcliff 

iterative scaling log linear models 
annals mathematical statistics 
della pietra della pietra lafferty 

inducing features random fields 
ieee transactions pattern analysis machine intelligence 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
freitag mccallum 

information extraction hmms shrinkage 
papers aaai workshop machine learning information pp 
menlo park california 
aaai 
ghahramani jordan 

factorial hidden markov models 
mozer touretzky perrone 
eds advances neural information processing systems 
mit press 
kanazawa koller russell 

stochastic simulation algorithms dynamic probabilistic networks 
proceedings eleventh conference uncertainty artificial intelligence montreal canada 
morgan kaufmann 
kupiec 

robust part speech tagging hidden markov model 
computer speech language 
leek 

information extraction hidden markov models 
master thesis uc san diego 
paz 

probabilistic automata 
academic press 
rabiner 

tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
ratnaparkhi 

maximum entropy models natural language ambiguity resolution 
ph thesis university pennsylvania 
rosenfeld 

adaptive statistical language modeling maximum entropy approach 
ph thesis carnegie mellon university 
roth 

learning resolve natural language ambiguities unified approach 
proceedings fifteenth national conference artificial intelligence pp 
menlo park california 
aaai press 
saul 

markov processes curves automatic speech recognition 
kearns solla cohn 
eds advances neural information processing systems vol 
cambridge massachusetts 
mit press 
yamron carp gillick lowe van 

hidden markov model approach text segmentation event tracking 
proceedings icassp 
ieee 
