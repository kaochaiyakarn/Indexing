tapestry infrastructure fault tolerant wide area location routing ben zhao john kubiatowicz anthony joseph computer science division university california berkeley cs berkeley edu report 
ucb csd april computer science division eecs university california berkeley california tapestry infrastructure fault tolerant wide area location routing ben zhao john kubiatowicz anthony joseph computer science division university california berkeley cs berkeley edu april today chaotic network data services mobile replicated widely availability durability locality 
components infrastructure interact rich complex ways greatly stressing traditional approaches name service routing 
explores alternative traditional approaches called tapestry 
tapestry overlay location routing infrastructure provides location independent routing messages directly closest copy object service point point links centralized resources 
routing directory information infrastructure purely soft state easily repaired 
tapestry self administering faulttolerant resilient load 
presents architecture algorithms tapestry explores advantages number experiments 
milieu moore law growth spawned revolution 
today computing environments significantly complex chaotic early days connectivity precious cpu cycles limited storage capacity 
data services mobile replicated widely availability performance durability locality 
components infrastructure constantly motion interact rich complex ways attempting achieve consistency utility face changing circumstances 
dynamic nature environment stresses ways traditional approaches providing object name service consistency location routing 
project current trends growing numbers complexities overlay network services may headed state world computing infrastructure collapses weight 
today object location routing technologies extremely fragile subject flash crowd loads denial service attacks security breaches server failures network outages 
scaling current solutions promoting greater interoperability invite disaster house built individual houses cards 
tapestry routing architecture self organizing scalable robust wide area infrastructure efficiently routes requests content presence heavy load network node faults 
tapestry explicit notion locality providing location independent routing messages directly closest copy object service point point links centralized services 
paradoxically tapestry employs randomness achieve load distribution routing locality 
roots plaxton distributed search technique augmented additional mechanisms provide availability scalability adaptation presence failures attacks :10.1.1.38.1850
routing directory information infrastructure purely soft state easily repaired 
tapestry fault tolerant resilient load fundamental component oceanstore system 
tapestry propose architecture creating environment offers system wide stability statistics 
faulty components transparently masked failed routes bypassed nodes attack removed service communication topologies rapidly adapted circumstances 
alternative novel parallels biological world 
absent world computing extremely elusive 
section argue integrated location routing crucial component achieving optimistic result 
need paradigm shift 
requirements stability statistics moore law growth processor performance network bandwidth disk storage name spawned opportunity shift focus away optimizing cycle transmitted bit disk block redundancy aggregate statistical behavior interacting components achieve uniformity behavior 
done properly system highly resilient failures normal state sufficiently large system 
capability achieved continuous monitoring adaptation redundancy elimination single points failure 
furthermore centralized solutions impractical require long distance communications vulnerable availability problems 
locality communication critical absolutely necessary serialization 
redundancy may take forms 
lowest level send messages different paths just increasing probability message arrive destination reducing standard deviation communication latency may redundant links failure detected 
likewise month doubling period disk storage network suggests wide spread information redundancy increases likelihood information located links nodes 
leverage traditional forms redundancy caching copies object far find copy quickly close find communication possible 
alternatively spreading copies erasure coded fragments information widely achieve strong durability data extremely hard destroy 
course consistency concerns important wide scale caching addressed ways 
particular consistency handled application level techniques decoupling mechanism fault tolerance policies consistency 
combined location routing attempting solve world problems extract simple lesson widearea resilient architectures applications require information mechanism redundancy 
instance consider basic primitive combines location routing specifically ability address messages location independent names request messages routed directly closest copy interesting alternative leverages doubling core internet bandwidth months object service addressed name primitive enables higher level architectures applications interact objects data ignoring knowledge object locations physical servers 
likewise primitive allows data migration replication optimizations performance durability availability issues correctness 
oceanstore system particular leverages mechanism decouple object names process route messages object uses possible discuss section 
meeting goals wide scale resilience eliminates ability centralized directory services broadcast communication requires location information distributed routing infrastructure incremental forwarding messages point point reach destination 
furthermore mechanism tolerant wide array failures mundane byzantine possibly sending multiple messages different paths ensure high probability delivery 
requirements argue location routing provided integrated mechanism composed distinct name service routing infrastructures 
fault tolerance repair self organization keeping wide scale requirements distributed location information soft state consistency checked fly may lost due failures destroyed time easily rebuilt refreshed 
ability retransmit requests suggests slight chaos inconsistency weak consistency distributed directory tolerable assuming bound amount duration inconsistency directory state 
pushing concept routing location infrastructure verify routes variety ways including checking cryptographic signatures information destination malicious servers filtered treating routes corrupted directory entries need discarded 
topology location routing infrastructure self organizing routers nodes data repositories come go network latencies vary individual links fail vary rates 
operating state continuous change routing location infrastructure able adapt topology search incorporating removing routers redistributing directory information adapting changes network latency 
thinking large numbers components suggests adaptation automatic reasonable sized group humans continuously tune infrastructure 
outline pages tapestry architecture algorithms data structures 
explore theoretical implications design probe advantages number experiments 
rest organized follows 
section gives overview plaxton location scheme 
section highlights tapestry improvements basic scheme describes location routing static tapestry topology 
section describes algorithms adapt topology tapestry changing circumstances 
discuss simulation results section 
section discusses tapestry relates previous research wide area routing location 
sum current progress discuss directions section conclude section 
closest defined variety mechanisms including network latency geographic locality 
background plaxton richa section discuss inspiration tapestry design location routing mechanisms introduced plaxton richa followed discussion benefits limitations plaxton mechanisms :10.1.1.38.1850:10.1.1.38.1850
plaxton distributed data structure optimized support network overlay locating named objects routing messages objects :10.1.1.38.1850:10.1.1.38.1850
refer forwarding overlay messages plaxton tapestry routing forwarding overlay nodes routers 
plaxton data structure call plaxton mesh novel allows messages locate objects route arbitrarily sized network small constant sized routing map hop 
additionally combining location object routing location plaxton mesh guarantees delivery time small factor optimal delivery time point network 
note plaxton assumption plaxton mesh static data structure node object insertions deletions 
plaxton node machine take roles servers objects stored routers forward messages clients origins requests 
discussions terms interchangeably node 
objects nodes names independent location semantic properties form random fixed length bit sequences represented common base hex digits representing bits 
system assumes entries roughly evenly distributed node object namespaces achieved output hashing algorithms sha 
routing plaxton uses local routing maps node call neighbor maps incrementally route overlay messages destination id digit digit represent wildcards 
approach similar longest prefix routing cidr ip address allocation architecture 
discussions resolve digits right left decision arbitrary 
node neighbor map multiple levels level represents matching suffix digit position id level neighbor map contains number entries equal base id th entry th level id location closest node ends suffix 
example th entry th level node ae node closest ae network distance ends ae 
definition th node message reaches shares suffix length destination id find router look th level map look entry matching value digit destination id assuming consistent neighbor maps routing method guarantees existing unique node system logical hops system size namespace ids base 
single neighbor map node assumes preceding digits match current node suffix needs keep small constant size entries route level yielding neighbor map fixed constant size way visualize routing mechanism destination node root node tree unique spanning tree nodes 
leaf traverse number intermediate nodes en route root node 
short plaxton mesh neighbor maps large set embedded trees network rooted node 
shows example plaxton routing 
ca bb plaxton routing example 
see path taken message originating node destined node plaxton mesh hexadecimal digits length nodes namespace 
location location mechanism allows client locate send messages named object residing server plaxton mesh 
server publishes object routing message root node 
root node unique node network place root embedded tree object 
publishing process consists sending message root node 
hop way publish message stores location information form mapping object id server id 
note mappings simply pointers server stored copy object 
multiple objects exist closest object saved hop root 
location query clients send messages objects 
message destined initially routed root 
step message encounters node contains location mapping immediately redirected server containing object 
message forward step closer root 
message reaches root guaranteed find mapping location 
root node object serves important role providing guaranteed surrogate node location mapping object 
special correlation object root node assigned 
plaxton uses globally consistent deterministic algorithm choosing root nodes caveat global knowledge deterministically pick existing node large sparse namespace 
intermediate hops absolutely necessary improve responsiveness providing routing locality mentioned root node serves critical purpose 
kind single point failure 
benefits limitations plaxton location routing system provides desirable properties routing location 
simple fault handling routing requires nodes match certain suffix potential route single link server failure choosing node similar suffix 
scalable inherently decentralized routing done locally available data 
point centralization possible bottleneck exists root node 
exploiting locality reasonably distributed namespace resolving additional digit suffix reduces number satisfying candidates factor id base number nodes satisfy suffix digit specified decreases geometrically 
path taken root node publisher server storing path taken client converge quickly number nodes route drops geometrically additional hop 
queries local objects quickly run router pointer object location 
proportional route distance plaxton proven total network distance traveled message location routing phases proportional underlying network distance assuring routing plaxton overlay incurs reasonable overhead :10.1.1.38.1850
serious limitations original plaxton scheme 
global knowledge order achieve unique mapping document identifiers root nodes plaxton scheme requires global knowledge time plaxton mesh constructed 
global knowledge greatly complicates process adding removing nodes network 
root node vulnerability location mechanism root node object single point failure node client relies provide object location information 
intermediate nodes location process interchangeable corrupted unreachable root node objects invisible distant clients meet intermediate hops way root 
lack ability adapt location mechanism exploits locality plaxton scheme lacks ability adapt dynamic query patterns distant hotspots 
correlated access patterns objects exploited potential trouble spots corrected cause overload cause congestion problems wide area 
similarly static nature plaxton mesh means insertions handled global knowledge recompute function mapping objects root nodes 
rest tapestry mechanisms distributed algorithms 
modeled plaxton scheme provide adaptability fault tolerance multiple faults introspective optimizations maintaining desirable properties associated plaxton scheme 
snapshot view operations tapestry overlay infrastructure designed ease creation scalable fault tolerant applications dynamic wide area network 
overlay network implies overhead relative ip key tapestry goals adaptivity self management fault resilience presence failures 
section examine snapshot tapestry routing infrastructure 
infrastructure certain properties achieved dynamic algorithms section 
basic location routing core location routing mechanisms tapestry similar plaxton 
node tapestry network capable forwarding messages algorithm described section 
object location pointers objid nodeid hotspot monitor oid nid freq object store neighbor map memory secondary neighbors pointers shown xx xx xx xx xx xx xx xx back pointers single tapestry node 
complete components tapestry node acts client object server router 
components include neighbor map hotspot monitor object location pointers store objects 
neighbor map organized routing levels level contains entries point set nodes closest network distance matches suffix level 
node maintains backpointer list points nodes referred neighbor 
node integration algorithm discussed section generate appropriate neighbor maps node integrate tapestry 
shows example complete tapestry node 
tapestry location mechanism similar plaxton location scheme 
multiple copies data exist plaxton node en route root node stores location closest replica 
tapestry stores locations replicas increase semantic flexibility 
plaxton mechanism returns object distance tapestry location provides semantic flexibility allowing application define selection operator 
object may include optional application specific metric addition distance metric 
applications choose operator define objects chosen 
example oceanstore global storage architecture see section queries issued find closest cached document replica satisfying freshness metric 
additionally archival pieces oceanstore issue queries collect distinct data fragments reconstruct lost data 
queries deviate simple find semantics ask tapestry route message closest distinct objects 
fault handling ability detect circumvent recover failures key tapestry goal 
discuss tapestry approaches operating efficiently accounting multitude failures 
key design choice tapestry components address issue fault adaptivity soft state maintain cached content graceful fault recovery provide reliability guarantees hard state 
soft state announce listen approach igmp clarified mbone session announcement protocol 
caches updated periodic refreshment messages purged lack 
allows tapestry handle faults normal part operations set special case fault handlers 
similar soft state fault handling appears context active services framework berkeley service discovery service 
faults expected part normal operation wide area 
furthermore faults detected circumvented previous hop router minimizing effect fault system 
section explains tapestry mechanisms detect operate recover faults affecting routing location functionality 
xxx xxx xxx xxx xxx xxx xxx xxx fault tolerant routing types expected faults impacting routing include server outages due high load hardware software failures link failures router hardware software faults neighbor table corruption server 
quickly detect failures operate recover router state failures repaired 
detect link server failures normal operations tapestry rely tcp timeouts 
additionally tapestry node uses backpointers send periodic heartbeats udp packets nodes neighbor 
simple hello message asserts message source viable neighbor routing 
checking id node message arrives quickly detect faulty corrupted neighbor tables 
operation faults entry neighbor map maintains backup neighbors addition closest primary neighbor 
plaxton refers secondary neighbors 
primary neighbor fails turn alternate neighbors order 
absence correlated failures provides fast switching overhead tcp timeout period 
want avoid costly recovered nodes failure repaired 
node detects neighbor unreachable removing pointer node marks invalid routes alternate 
node link failures discovered repaired relatively short time period maintain second chance period reasonable length day stream messages route failed server serving probe messages 
failed messages stream rerouted alternate path timeout 
successful message indicates failure repaired original route pointer marked valid 
control probe traffic volume simple probability function determine packet routes original router probability ratio desired probe traffic rate incoming traffic rate route 
failure repaired second chance period neighbor removed map alternates promoted additional sibling final alternate 
fault tolerant location discussed section object root node single point failure plaxton location mechanism 
correct tapestry assigning multiple roots object 
accomplish concatenate small globally constant sequence salt values object id hash result identify appropriate roots 
roots surrogate routing publishing process insert location information tapestry 
locating object tapestry performs hashing process target object id generating set roots search 
way view hashing technique salt values defines independent routing plane plaxton style location routing 
routing planes enable tradeoff reliability redundancy queries may sent planes simultaneously 
network roots data available roots presence complete network partition 
remove need maintain hard state associate object id location mappings soft state leases 
storage servers republish location information objects stores regular intervals 
object inaccessible deletion server failure location information object cached routers server root node times 
new objects newly recover objects published periodic mechanism making object advertisement removal transparent 
mechanisms fault tolerance limited physical resources extreme circumstances failure outgoing link wide spread partitioning wide area backbone 
surrogate routing original plaxton scheme object root surrogate node chosen node matches object id greatest number trailing bit positions 
may nodes match criteria plaxton scheme chooses unique root invoking total ordering nodes network candidate node greatest position ordering choosen root 
scheme plaxton location proceeds resolving object id digit time encounters empty neighbor entry 
point final hop root appropriate shortcut link 
set nodes network static cost constructing global order generating appropriate shortcut links incurred time network construction 
total number links half routing base routing level 
course set nodes network static 
real distributed system finding maintaining total global ordering possible 
remove global knowledge choose root node globally consistent fashion 
tapestry uses distributed algorithm called surrogate routing compute unique root node 
algorithm selecting root nodes deterministic scalable arrive consistent results point network 
surrogate routing tentatively chooses object root node name id 
sparse nature node name space node exist 
tapestry operates node exists attempting route 
route non existent identifier empty neighbor entries various positions way 
cases goal select existing link acts alternative desired link associated digit 
selection done deterministic selection existing neighbor pointers 
routing terminates neighbor map reached non empty entry belongs current node 
node designated surrogate root object 
note tapestry neighbor link empty qualifying nodes entire network 
neighbor nodes network empty entries neighbor map nodes suffix exactly empty entries 
follows deterministic algorithm arrive unique surrogate node location tapestry network 
surrogate routing provides technique identifier uniquely mapped existing node network 
surrogate routing may take additional hops reach root compared plaxton algorithm show additional number hops small 
examined hop calculating number additional hops reduced version coupon collector problem 
know tries constant probability finding coupons 
total possible entries hop neighbor map random entries fill entry map probability empty entry appears map probability unique nodes left current suffix hexadecimal digit representation 
expect hop reduce remaining potential routers approximate factor expected number hops occurrence empty entry single node left 
adaptable version surrogate routing tapestry minimal routing overhead relative static global plaxton algorithm 
null grab ith level fill jth level neighbor map dist nm neigh min nm sec neigh neigh sec neighbor sec neighbors neigh sec neighbors new id terminate null entry route current surrogate new id move relevant pointers current surrogate surrogate new id notify nodes flooding back levels surrogate routing necessary 
node insertion pseudocode 
pseudocode entire dynamic node insertion algorithm 
dynamic algorithms main limitation plaxton proposal static nature algorithms 
tapestry algorithms focus supporting dynamic operations decentralized manner 
algorithms tapestry infrastructure achieves desirable properties introduced section 
dynamic node insertion incremental algorithm allows nodes integrate tapestry dynamically 
algorithm guarantee ideal topology assert reasonable approximation converge ideal topology runtime optimization 
intuition incremental algorithm follows populate new node neighbor maps level routing new node id copying optimizing neighbor maps hop router 
inform relevant nodes entry tapestry may update neighbor maps 
example assume new node integrating network satisfies constraints tapestry network 
node requests new id contacts gateway node tapestry node known acts bridge network 
pseudocode dynamic insertion algorithm shown 
populating neighbor map assume node knows gateway node tapestry node close network distance latency 
achieved bootstrap mechanism expanding ring search band communication 
starting node node attempts route id copies approximate neighbor map th hop shows steps takes order gather single level neighbor map 
routing node id knows shares suffix length 
copies level neighbor map attempts optimize entry 
optimizing means comparing distances neighbor entry secondary neighbors 
entry secondary neighbor closer primary neighbor primary neighbor looks nodes neighbors neighbor maps compares distance determine hi digits null 
send hello 
send 
nm optimize 

hi node insertion part 
steps new node id takes generate locally optimal neighbor map 
better potential neighbors 
optimization repeats significant improvement looking neighbors 
repeating process entry near optimal neighbor map 
neighbor map population phase requires neighbor map optimized manner nodes put map due network sparsity 
new node stops copying neighbor maps neighbor map lookup shows empty entry hop 
routes current surrogate moves data meant 
neighbor notification step inform relevant nodes integration 
notify nodes empty entries filled traverse surrogate backpointers back level level level surrogate routing necessary 
showed section high probability surrogate routing take hops 
hops back reach relevant nodes higher probability 
notify local nodes benefit closer router send hello message neighbors secondary neighbors level 
notified nodes option measuring distance appropriate replacing existing neighbor entry 
process notifying relevant nodes fill entries may inadvertently change step surrogate routing node nodes 
example node previous route neighbor map wanting entry saw empty entry 
tried empty routed non empty entry 
new node causes filled routes route surrogate route 
solve problem noting router location mechanism node stores copy object location mappings 
proceed fill empty entry know algorithm range objects surrogate route moved entry 
explicitly delete entries republish objects establishing new surrogate routes account new inserted node 
alternatively simply rely soft state mechanism solve problem 
timeout period objects republished new neighbor map filled previously stored pointers time disappear 
small window vulnerability equal timeout period querying object multiple roots probability roots encountering insertion effect small multiple roots find date route return correct results 
note dynamic node insertion algorithm non trivial insertion take non negligible amount time 
part rationale quick reintegration repaired nodes 
deleting nodes trivial 
node actively inform relevant parties departure backpointers threshold improvement reduction network distance 
exiting node new location path old location path updating location pointers exiting nodes node leave tapestry network informs object server 
affected object new epoch doing uses location hop address delete old location pointers 
rely soft state remove time 
expect wide area network dynamic expect small portion network entering exiting overlay simultaneously 
reason tapestry currently unsuitable networks constantly changing sensor networks 
soft state vs explicit republishing soft state approach republishing regular intervals excellent simplifying solution keeping location pointers date implicitly highlights tradeoff bandwidth overhead republish operations level consistency location pointers 
similar tradeoff exists maintain date node information 
section discusses tradeoffs mechanisms supporting mobile objects explicit republishing delete operations 
example consider large network nodes storing objects roughly kb size objects node 
assuming bit namespaces objects bits nodes organized hexadecimal digits object republish operation results message objectid nodeid tuple logical hop en route root node total messages bytes bits bits 
works machine 
set republish interval day amount bandwidth amortized equal kb modern high speed ethernet networks expect resulting location timeout period days 
clearly bandwidth overhead significantly limits usefulness soft state approach state maintenance 
result modify approach including proactive explicit updates addition republishing 
take approach state maintenance nodes objects 
support algorithms modify object location mappings tuple objectid 
hop location path root node server keeps preceding nodeid 
additionally introduce notion epoch numbers primitive versioning mechanism location pointer updates 
rest section describe algorithms leveraging mechanisms explicitly manage node object state 
explicitly handling node state dynamic network expect nodes disappear tapestry due failures intentional disconnections 
case routing infrastructure quickly detect promote secondary routes location pointers automatically recover 
backtracking delete republish update old pointer path node failure mobile object moving server object root node supporting mobile objects explicit republishing object moving server server 
establishes forwarding pointer doing uses location hop addresses delete old location pointers 
explain algorithm example shown 
proactively modify object location pointers anticipation exit tapestry node notifies servers stored objects maintains location mappings 
server issues republish message new epoch number nodeid 
nodes affected exit forward messages update local epoch numbers 
preceding hop notes nodeid message routes republish message secondary route 
successive hop stores location mapping new epoch hop older epoch detected node 
marks junction alternate path caused exit merges original path 
forwards new epoch root uses send message backwards hop hop removing location mappings object stored 
proactive procedure updates path location pointers efficient way 
exiting node initiate full republish republish originated server establish authenticity authority 
additionally reverse delete message obstructed node failure node remaining orphaned object pointers removed timeout period 
client encounters forwarded wrong server inform node pointers removed confirmation 
client continue routing root node search correct location having experienced round trip overhead mistaken server 
supporting mobile objects generalized form previous algorithm proactively maintain consistency object location pointers demonstrated 
example mobile object moves server server 
algorithm establishes location pointers pointing guaranteeing correct location object available duration transition 
object migrates servers server establishes forwarding pointer forwards requests duration algorithm 
server issues republish message root node new epoch number 
republish proceeds normally detects older epoch number junction intersects old publish path root node example 
forwards original republish message root updating location pointers sends delete message back deleting location pointers way 
ca send replica hotspot caching example 
object stored root node 
detects frequent location requests notifies traces back traffic back reports back 
replica placed 
root node node guaranteeing correct pointer object pointers viewed location caches sends notification message directly receives new republish message 
server removes forwarding pointer receiving notification message 
point backtracking delete message may propagating way terminated due node failure way node example 
pointers node inconsistent state timeout period delete message reaches brief inconsistency tolerated client retrieving wrong server location continue routing root node overhead round trip inconsistent pointer confirmed removed failure 
algorithm supports rapidly moving objects forcing clients traverse large number forwarding pointers 
date location pointers removed soon possible inconsistent pointer incur round trip overhead old server single client bad pointer tested removed 
algorithms demonstrate complementary explicit proactive approach soft state approach state management 
note large distributed networks supported tapestry overhead costs imply longer data refresh intervals larger windows vulnerability solve problem proactive algorithms possible falling back soft state resort 
introspective optimizations tapestry goal provide architecture quickly detects environmental changes modifies node organization adapt 
describe introspective tapestry mechanisms improve performance adapting environmental changes 
changes network distance connectivity node pairs drastically affect system performance 
tapestry nodes tune neighbor pointers running thread uses network pings update network latency neighbor 
thread scheduled low priority mechanism runs periodically network traffic server load moderate levels 
neighbor latency increased significant rate node requests neighbor secondary neighbors traverses local minima network distance 
neighbor pointers higher levels compared substituted lower neighbors closer 
notify notify rdp rdp vs object distance ti locality pointers pointers object distance location pointers effect rdp tiers ratio network hops traveled tapestry intermediate location pointers network hops ip 
second algorithm detects query hotspots offers suggestions locations additional copies significantly improve query response time 
described previously caches object locations stored hops location object root node 
mechanism exploits locality query hotspots far away object overload servers links en route root node 
tapestry allow nodes store object location actively track sources high query load hotspot 
node detects request traffic preset threshold notifies source node traffic 
begins monitoring incoming query traffic notifies source nodes 
notified node source incoming traffic streams replies node id node id forwarded upstream root node 
object question cache able object notify application layer hotspot recommend copy placed source query traffic 
object static resource machine server place copy object location mapping source hotspot refresh update messages traffic stream remains proportional constant 
call hotspot cache recognizing logical hops longer closer root hotspot caches reduces hop traversals drastically reduce response time locate object 
shows example tracing back hotspot 
measurements section simulation results demonstrating benefits tapestry design performs adverse conditions 
examine variety factors affect location performance especially respect exploiting locality 
take closer look novel uses positively affect performance performance stability 
demonstrate compared replicated directory servers tapestry location servers show graceful degradation throughput response time ambient network traffic increases 
show overlay network tapestry routing incurs small overhead compared ip routing 
rdp rdp vs object distance ts locality pointers pointers distance object hops location pointers effect rdp transit stub location rdp tapestry intermediate location pointers 
locality effects experiments examine effectiveness location pointers stored intermediate hops storage server root node object 
plaxton richa prove locating routing object location pointers incurs small linear factor overhead compared routing physical layer confirm theoretical results :10.1.1.38.1850
ran experiments packet level simulator tapestry unit distance hop topologies measured location relative delay penalty ratio distance traveled tapestry location routing versus traveled direct routing object 
avoid topology specific results performed experiments representative set real artificial topologies including real networks jan mbone artificially generated topologies tiers transit stub 
jan graph models connectivity internet autonomous systems node topology represents 
generated national laboratory applied network research bgp tables 
mbone graph collected scan project usc isi node represents mbone router 
tiers graph includes nodes generated tiers generator 
gt itm package generate transit stub graph 
results experiments topologies show trend 
show results representative topologies tiers nodes transit stub nodes figures 
results largely confirm expected presence locality pointers helps maintain small relatively constant factor absence results large number hops root node large values 
multiple replicas performance stability reach goal stability statistics tapestry trades resources gain performance stability 
specifically propose improve performance lower variance response time redundant requests utilizing moore law growth computational bandwidth resources 
error bars shown part graphs section show deviation data point 
average time coalesce hop units time coalesce vs fragments requested fragments requested total threshold latency vs replicas requested time required receive threshold total fragments necessary archival object reconstruction 
average time coalesce hop units time coalesce presence failures fragments requested total threshold latency vs replicas link failures time required reconstruction threshold accounting timeouts retransmission overhead link failures occur 
error bars omitted clarity 
examine feasibility approach tapestry context archival utility oceanstore global storage infrastructure 
uses tapestry distribute locate collect archival fragments generated erasure coding original object block 
object erasure coded large number fragments distributed randomly placed fragment storage servers 
server advertises fragment tapestry location mechanisms id original object 
performance critical operation searching fragments group order reconstruct original object 
focus operation examine tradeoff stability statistics different scenarios 
key parameter experiments number simultaneous requests fragments issued 
threshold fragments needed total simulate time necessary receive fastest fragment responses 
experiments done packet level simulator tapestry network nodes running node gt itm generated transit stub topology 
apply memoryless distribution network hop average latency measure hop unit furthermore simulate effect server load queuing delays making half servers highly loaded adds hop units response time servers add unit response time 
plot time reach threshold function number fragments requested 
see increase number requests minimum threshold additional requests greatly reduce bandwidth consumed fragments hops aggregate bandwidth function server distance root hops hops hops hops fragments requested bandwidth overhead multiple fragment requests total aggregate bandwidth function simultaneous fragment requests 
unit represents bandwidth transmission fragment network hop 
latency hop units effect multiple roots latency root roots roots roots roots client distance object effect multiple roots location latency time taken find route object function client distance object root node number parallel requests 
error bars omitted clarity 
latency variance response time 
perform measurement allowing link failures network 
link fails requests retries incurs timeout overhead response time 
visible effect additional requests threshold increasingly dramatic link failure rate increases 
implies stability statistics works better real wide area internet packet errors resulting congestion uncommon 
show aggregate bandwidth fragment requests increases linearly number fragment requests increases distance client root node objectid 
results show tolerable amount bandwidth overhead significantly improve performance making requests threshold minimum 
multiple roots performance stability exploit application level redundancy shown archival simulations performance stability tapestry provides level redundancy location mechanism 
recall discussion producing multiple root nodes object hashing section 
show latency hop units retrieving objects multiple roots average latency aggregate bandwidth roots utilized page multiple roots aggregate bandwidth time find route object graphed aggregate bandwidth function number parallel requests 
simulation results sending simultaneous requests object hashed ids provides better variable performance 
packet level simulator tapestry nodes node transit stub topology applying memoryless distribution hop latencies 
assign random ids randomly placed object publish presence id experiment show latency taken find route object randomly placed clients function number parallel requests issued distance clients root object 
resulting shows key features 
clients immediately root node increasing number parallel requests drastically decreases response time 
second significant latency decrease occurs requests sent parallel benefit requests added 
smaller number requests shows jagged curve showing vulnerability random effects long hop latencies 
contrast factors hidden smoothed curve requests issued parallel 
second experiment shown new perspective experiment conjunction aggregate bandwidth 
plot location routing latency aggregate bandwidth number parallel requests 
chart confirms artifacts observed including significant decrease latency variability additional request 
furthermore plotting aggregate bandwidth secondary axis see greatest benefit latency stability gained minimal bandwidth overhead 
performance stress experiments shown figures compared simplified tapestry location mechanism centralized directory server node ns tcp ip simulation topology generated gt itm 
simulated simplified tapestry location mechanism benefit replication hotspot managers replicated roots assumed negligible lookup times directory servers 
experiments measured throughput response time synthetic query load artificially generating high background traffic random paths network 
query load models web traffic mainly composed serialized object requests requested objects receiving query traffic 
background traffic causes high packet loss rates multiple routers 
inherent replication nodes server object root node tapestry responds sporadic bandwidth message hops lookup requests system wide throughput heavy load normal single src ms ms ms ms varying external load directory lookup rand 
replica directory lookup opt 
replica throughput packet loss chart shows system wide throughput directory server versus throughput tapestry root node high background traffic causing packet loss 
average response time ms average response time heavy load normal single src ms ms ms ms varying external load tapestry location directory lookup rand 
replica directory lookup opt 
replica tapestry location objects tapestry location objects near congestion response time packet loss chart shows relative response time query tapestry location versus centralized directory server high background traffic causing packet loss 
packet loss favorably graceful performance degradation shown figures 
centralized directory servers isolated packet loss increases throughput response time degrade due tcp retransmissions exponential backoffs 
experiment demonstrate tapestry fault tolerance mechanisms augmented normal tcp fault aware routers top tapestry node topology 
tapestry nodes messages estimate reliability hop neighbor links 
simulation randomly placed nodes generate background traffic sending byte packets 
background packets flood network increasing frequency router queues overflow packet loss increases 
reliability falls threshold outgoing packets duplicated duplicate sent alternate hop neighbor 
hierarchical nature tapestry neighbor links routers path see duplicate packets quickly converge drop duplicate packets arrival 
shows results ns simulation drop tail queues gt itm transit stub topology measuring single connection packet loss statistics background traffic increases network 
modified tcp significantly reduces packet loss duplicate packets route congestive regions 
furthermore reduced packet loss results fewer higher throughput 
packet loss tcp connection packet loss ip tap ip background traffic tu tcp connection packet loss 
shows single connection packet loss levels tcp ip enhanced tcp ip background traffic causes packet drops drop tail network queues 
rdp th percentile rdp namespace nodes jan mbone tiers transit stub tapestry base measuring rdp different bases 
th percentile ideal tapestry mesh versus ip multiple network topologies including internet autonomous systems mbone tiers transit stub 
overlay routing overhead measure routing overhead tapestry overlay network incurs compared ip routing measured physical hop counts pairs nodes tapestry ip topologies 
topologies introduced section 
unit overlay overhead measurement relative delay penalty rdp introduced 
defined ratio tapestry routing distances ip routing distances 
definition ip unicast rdp 
plots th percentile rdp values topologies network models modify tapestry id base 
measured pair wise node distances hop counts 
related research area decentralized routing location active 
start tapestry project march projects begun attack decentralized routing problem different approaches including pastry chord :10.1.1.105.3673:10.1.1.28.5987
discuss tapestry approach context network location content routing network measurements overlay networks 
pastry past project begun microsoft research focusing peer peer anonymous storage 
past routing location layer called pastry location protocol sharing similarities tapestry :10.1.1.28.5987
key similarities include prefix suffix address routing similar insertion deletion algorithms similar storage overhead costs 
key differences pastry tapestry 
objects past replicated control owner 
publication object replicated replicas placed nodes nodeids closest namespace object objectid 
second tapestry places object location hops server root pastry assumes clients objectid attempt route directly vicinity replicas object kept 
placing actual replicas different nodes network may reduce location latency comes price storage overhead multiple servers brings set questions security confidentiality consistency 
pastry routing analogy tapestry surrogate routing algorithm see section provides weaker analytic bounds number logical hops taken 
tapestry analytically proven defined probabilistic bounds routing distances guaranteed find existing reachable object see section 
chord projects mit closely related tapestry pastry 
chord project provides efficient distributed lookup service uses logarithmic sized routing table route object queries :10.1.1.105.3673
focus providing hashtable functionality resolving key value pairs 
namespace defined sequence bits node keeps pointers nodes follow namespace modulo entry node routing table contains node succeeds namespace 
key stored node identifier equal immediately follows namespace 
chord provides similar logarithmic storage logarithmic logical hop limits tapestry provides weaker guarantees worst case performance 
main distinction worthy note natural correlation overlay namespace distance network distance underlying network opening possibility extremely long physical routes close logical hop 
problem partially alleviated heuristics 
projects mit relevant 
karger decentralized widearea location architecture geographic routing grid 
grid uses notion embedded hierarchies handle location queries scalably embedded trees tapestry 
second intentional naming system ins combines location routing mechanism 
resilient overlay networks leverages grid location mechanism semantic routing intentional naming system ins provide fault resilient overlay routing wide area 
scalability ins ron project focuses networks size nodes 
content addressable networks done center internet research icsi aciri 
model nodes mapped dimensional coordinate space top tcp ip way analogous assignment ids tapestry 
space divided dimensional blocks servers density load information block keeps information immediate neighbors 
addresses points inside coordinate space node simply routes neighbor progress destination coordinate 
object location works object server pushing copies location information back direction incoming queries 
key differences tapestry 
comparison tapestry hierarchical overlay structure high fanout node results paths different sources single destination converging quickly 
consequently compared queries local objects converge faster cached location information 
furthermore tapestry inherent locality paired introspective mechanisms means allows queries immediately benefit query locality adaptive query patterns allowing consistency issues handled application layer 
assumes objects immutable reinserted change values 
tapestry node organization uses local network latency distance metric shown reasonable approximation underlying network 
chord attempt approximate real network distances topology construction 
result logical distances routing arbitrarily expensive hop neighbors involve long trips underlying ip network 
main advantage simplicity node addition algorithm better adapt dynamically changing environments sensor networks 
summary pastry chord similar tapestry functionality run time properties 
particular pastry closest analogy offering locating routing object chord focus providing distributed hashtable functionality 
pastry controls replica placement chord optimized large objects tapestry system allows user control location consistency original data allowing system manipulate control object performance 
noteworthy tapestry pastry natural correlation overlay topology underlying network distance chord may incur high physical hop counts logical hop 
related triad project stanford university focuses problem content distribution integrating naming routing connection setup content layer 
propose query routing mechanism strives greater reliability access protocol level information 
previous efforts approached wide area location problem varying degrees success 
globe project location mechanisms focus wide area operation 
small fixed number hierarchies scale location data making unable scale increasingly large networks 
wide area extension service location protocol slp pair wise query routing model administrative domains presents potential bottleneck number domains increases 
berkeley service discovery service uses lossy bloom filters compress service metadata complement multiple wide area hierarchies 
area server location boggs introduced notion expanding ring search ph thesis 
partridge proposed anycast attempts deliver messages nearby host 
tapestry depends accurate network measurements optimize overlay topology 
paul francis propose solution idmaps uses distributed tracers build distance estimation map 
mark stemm proposed spand shared network measurement architecture adaptive application framework 
projects examined construction overlay topologies optimizing network distances 
overcast multicast system jannotti propose mechanisms constructing spanning trees minimize duplicate packets underlying network 
additionally system multicast scattercast utilize self configuring algorithms constructing efficient overlay topologies 
esm introduces relative delay penalty rdp metric measuring overhead overlay routing 
status implemented packet level simulators tapestry system finishing java implementation large scale deployment 
tapestry support applications 
driving application tapestry oceanstore wide area distributed storage system designed span globe provide continuous access persistent data 
data cached system available times 
tapestry provides object location routing functionality oceanstore requires meeting demands consistency performance 
particular tapestry efficiently robustly routes messages wide area routing heavily loaded failed links 
self contained archival storage layer oceanstore called uses tapestry distribution collection erasure coded data fragments 
developed bayeux application level multicast protocol top tapestry 
bayeux uses natural hierarchy tapestry routing provide single source multicast data delivery conserving bandwidth 
initial measurements show bayeux provides scalability thousands listeners leveraging tapestry provide fault tolerant time packet delivery minimal duplication packets 
current priority performance analysis variety conditions parameters 
help better understand tapestry position decentralized routing research space compares approaches pastry chord possibly allow define taxonomy research space 
working studying security requirements tapestry secure resilient attacks 
application side developing intelligent network applications exploit network level statistics utilize tapestry routing minimize data loss improve latency throughput 
exploring possibility offering mobile ip functionality location independent naming mechanism tapestry 
tapestry location routing architecture self organizing scalable robust wide area infrastructure efficiently routes requests content presence heavy load network node faults 
showed tapestry overlay network efficiently constructed support dynamic networks distributed algorithms 
tapestry similar plaxton distributed search technique additional mechanisms leverage soft state information provide self administration robustness scalability dynamic adaptation graceful degradation presence failures high load eliminating need global information root node vulnerabilities lack adaptability :10.1.1.38.1850
tapestry provides ideal solution dynamic wide area object naming message routing systems need deliver messages closest copy objects services location independent manner point point links centralized services 
tapestry part randomness achieve load distribution routing locality 
william adjie winoto elliot schwartz hari balakrishnan jeremy lilley 
design implementation intentional naming system 
proceedings acm sosp 
acm december 
elan amir steve mccanne randy katz 
active services framework application real time multimedia transcoding 
proceedings acm sigcomm vancouver bc 
hari balakrishnan frans kaashoek robert morris 
resilient overlay networks 
nms lcs mit edu projects ron 
david boggs 
internet broadcasting 
phd thesis xerox parc october 
available technical report csl 
lee breslau deborah estrin kevin fall sally floyd john heidemann ahmed helmy polly huang steven mccanne kannan varadhan ya xu yu 
advances network simulation 
ieee computer may 
john canny 
ucb cs fall lecture note 
www cs berkeley edu jfc cs lec lec html 
chawathe steven mccanne eric brewer 
rmx reliable multicast heterogeneous networks 
proceedings ieee infocom tel aviv israel march 
ieee 
yang hua chu sanjay rao hui zhang 
case system multicast 
proceedings acm sigmetrics pages june 
steve deering 
host extensions ip multicasting 
ietf sri international menlo park ca aug 
rfc 
peter druschel anthony rowstron :10.1.1.28.5987
pastry scalable distributed object location routing large scale peer peer systems 
submission acm sigcomm 
peter druschel antony rowstron 
past persistent anonymous storage peer peer networking environment 
submission acm hotos viii 
paul francis sugih jamin vern lixia zhang daniel jin 
architecture global internet host distance estimation service 
proceedings ieee infocom march 
mark david cheriton 
architecture content routing support internet 
proceedings usenix symposium internet technologies systems 
usenix march 
todd hodes steven czerwinski ben zhao anthony joseph randy katz 
architecture secure wide area service discovery service 
wireless networks 
special issue selected papers mobicom revision 
john jannotti david gifford kirk johnson frans kaashoek james toole 
overcast reliable multicasting overlay network 
proceedings osdi october 
dina john wroclawski 
framework scalable global ip anycast gia 
proceedings acm sigcomm pages august 
john kubiatowicz david bindel yan chen patrick eaton dennis geels ramakrishna gummadi sean rhea hakim weatherspoon weimer christopher wells ben zhao 
oceanstore architecture global scale persistent storage 
proceedings acm asplos 
acm november 
li john jannotti douglas de couto david karger robert morris 
scalable location service geographic ad hoc routing 
proceedings acm mobicom 
acm august 
maher colin perkins 
session announcement protocol version 
ietf internet draft november 
draft ietf sap txt 
charles perkins 
ip mobility support 
ietf october 
rfc 
greg plaxton rajaraman andrea richa :10.1.1.38.1850
accessing nearby copies replicated objects distributed environment 
proceedings acm spaa 
acm june 
sylvia ratnasamy paul francis mark handley richard karp scott schenker 
scalable network 
submission acm sigcomm 
rekhter tony li 
architecture ip address allocation cidr 
rfc www isi edu notes rfc txt 
sean rhea chris wells patrick eaton dennis geels ben zhao hakim weatherspoon john kubiatowicz 
maintenance free global storage oceanstore 
submission ieee internet computing 
matthew 
md md md sha hash functions 
technical report tr rsa laboratories 
version 
rosenberg schulzrinne 
wide area network service location 
ietf internet draft november 
mark stemm srinivasan seshan randy katz 
network measurement architecture adaptive applications 
proceedings ieee infocom tel aviv israel march 
ion stoica robert morris david karger frans kaashoek hari balakrishnan :10.1.1.105.3673
chord scalable peer peer lookup service internet applications 
submission acm sigcomm 
maarten van steen franz hauck philip andrew tanenbaum 
locating objects wide area systems 
ieee communications magazine pages january 
hakim weatherspoon chris wells patrick eaton ben zhao john kubiatowicz 
global scale archival system 
submitted publication acm sosp 
ellen zegura ken calvert bhattacharjee 
model internetwork 
proceedings ieee infocom 
shelley zhuang ben zhao anthony joseph randy katz john kubiatowicz 
bayeux architecture scalable fault tolerant wide area data dissemination 
submission acm nossdav 

