learning bayesian networks david heckerman microsoft com march revised november technical report msr tr microsoft research advanced technology division microsoft microsoft way redmond wa companion set lecture slides available ftp ftp research microsoft com pub david tutorial ps 
network graphical model encodes probabilistic relationships variables interest 
conjunction statistical techniques graph ical model advantages data analysis 
model encodes dependencies variables readily handles situations data entries missing 
bayesian network learn causal relationships gain understanding problem domain predict consequences intervention 
model causal prob semantics ideal representation combining prior knowledge comes causal form data 
bayesian statistical methods tion bayesian networks er cient principled approach avoiding tting data 
discuss methods constructing bayesian net works prior knowledge summarize bayesian statistical methods data improve models 
regard task describe methods learning parameters structure bayesian network including techniques learning incomplete data 
addition relate bayesian network methods learning techniques supervised unsupervised learning 
illustrate graphical modeling approach real world case study 
network graphical model probabilistic relationships set variables 
decade bayesian network popular representation encoding uncertain expert knowledge expert systems heckerman 
researchers developed methods learning bayesian networks data 
techniques developed new evolving shown remarkably ective data analysis problems 
provide tutorial bayesian networks associated bayesian techniques extracting encoding knowledge data 
numerous rep available data analysis including rule bases decision trees arti cial neural networks techniques data analysis density estima tion classi cation regression clustering 
bayesian networks bayesian methods er 
answers 
bayesian networks readily handle incomplete data sets 
example consider classi cation regression problem explanatory input variables strongly anti correlated 
correlation problem standard supervised learning techniques provided inputs measured case 
inputs observed models produce inaccurate prediction encode correlation input variables 
bayesian networks er natural way encode dependencies 
bayesian networks allow learn causal relationships 
learning causal relationships important reasons 
process useful trying gain understanding problem domain example exploratory data analysis 
addition knowledge causal relationships allows predictions presence interventions 
example marketing analyst may want know worthwhile increase exposure particular advertisement order increase sales product 
answer question analyst determine advertisement cause increased sales degree 
bayesian networks helps answer questions experiment ects increased exposure available 
bayesian networks conjunction bayesian statistical techniques facilitate combination domain knowledge data 
performed real world analysis knows importance prior domain knowledge especially data scarce expensive 
fact commercial systems expert systems built prior knowledge testament power prior knowledge 
bayesian networks causal semantics encoding causal prior knowledge particularly straightforward 
addition bayesian networks encode strength causal relationships probabilities 
consequently prior knowledge data combined studied techniques bayesian statistics 
bayesian methods conjunction bayesian networks types mod els ers cient principled approach avoiding tting data 
shall see need hold available data testing 
bayesian approach models smoothed away available data training 
tutorial organized follows 
section discuss bayesian interpretation probability review methods bayesian statistics combining prior knowledge data 
section describe bayesian networks discuss con structed prior knowledge 
section discuss algorithms probabilistic inference bayesian network 
sections show learn probabilities xed bayesian network structure describe techniques handling incomplete data including monte carlo methods gaussian approximation 
sections show learn probabilities structure bayesian network 
topics discussed include methods assessing priors bayesian network structure parame ters methods avoiding tting data including monte carlo laplace bic mdl approximations 
sections describe relationships bayesian network techniques methods supervised unsupervised learning 
section show networks facilitate learning causal relationships 
section illustrate techniques discussed tutorial real world case study 
section give pointers software additional literature 
bayesian approach probability statistics understand bayesian networks associated learning techniques important understand bayesian approach probability statistics 
section provide bayesian approach readers familiar classical view 
nutshell bayesian probability person degree event 
classical probability property world probability coin land heads bayesian probability property person assigns probability degree belief coin land heads 
keep concepts probability distinct refer classical probability ofan event true physical probability event refer degree belief event personal probability 
alternatively meaning clear refer bayesian probability simply probability 
important di erence physical probability personal probability measure need repeated trials 
example imagine repeated tosses sugar cube wet surface 
time cube tossed dimensions change slightly 
classical statistician hard time measuring probability cube land particular face bayesian simply restricts attention toss assigns probability 
example consider question probability chicago win championship 
classical statistician remain silent bayesian assign probability bit money process 
common criticism bayesian de nition probability probabilities arbitrary 
degrees belief satisfy rules probability 
scale probabilities measured 
particular sense assign probability zero event occur probabilities assign beliefs extremes 
surprisingly questions studied intensely 
regards rst question researchers suggested di erent sets properties satis ed degrees belief ramsey cox probability wheel tool assessing probabilities 
savage 
turns set properties leads rules rules probability 
set properties compelling fact di erent sets lead rules probability provides particularly strong argument probability measure beliefs 
answer question scale follows simple observation people nd fairly easy say events equally 
example imagine simpli ed wheel fortune having regions shaded shaded illustrated 
assuming wheel symmetric shading conclude equally wheel position 
judgment sum rule probability probabilities mutually exclusive collectively exhaustive sum follows probability wheel shaded region percent area wheel shaded case 
probability wheel provides measuring probabilities events 
example probability gore run democratic ticket 
ask question gore run wheel shaded region 
think gore run imagine wheel shaded region larger 
think wheel shaded region imagine wheel shaded region smaller 
repeat process think gore running wheel stopping shaded region equally 
point probability gore run just percent surface area shaded area wheel 
general process measuring degree belief commonly referred probability assessment 
technique assessment just described available techniques discussed management science operations research psychology literature 
problem probability assessment addressed literature precision 
really say probability event 
cases 
cases probabilities decisions decisions sensitive small variations probabilities 
established practices sensitivity analysis help know additional precision unnecessary howard matheson 
problem probability assessment accuracy 
example experiences way question phrased lead assessments re ect person true beliefs tversky kahneman 
methods improving accuracy decision analysis literature 

turn issue learning data 
illustrate bayesian approach consider common thumbtack round head supermarkets 
throw thumbtack air come rest point heads head tails 
suppose ip thumbtack times making sure physical properties thumbtack conditions itis remain stable time 
rst observations want determine probability heads th toss 
classical analysis problem assert physical probability heads unknown 
estimate physical probability observations criteria bias 
estimate probability heads th toss 
bayesian approach assert physical probability heads encode uncertainty physical probability bayesian probabilities rules probability compute probability heads th toss 
examine bayesian analysis problem need notation 
denote variable upper case letter xi state value corresponding variable letter lower case xi 
denote set variables bold face upper case letter xi 
corresponding bold face lower case letter xi denote assignment state value variable set 
say variable set con guration xj xj shorthand denote probability person state information xj denote probability distribution mass functions density functions 
xj refers probability probability density ora probability distribution clear context 
notation probability 
summary notation chapter 
returning thumbtack problem de ne variable values example taken howard 
strictly speaking probability belongs single person collection people 
parts discussion refer probability toavoid awkward english 
bayesians typically refer uncertain variable value uncertain 
con correspond possible true values physical probability 
refer parameter 
express uncertainty probability density function 
addition xl denote variable representing outcome lth ip fx xn xng denote set observations 
bayesian terms thumbtack problem reduces computing xn jd 
rst bayes rule obtain probability distribution background knowledge jd dj dj dj dj expand term dj 
bayesians classical statisticians agree term likelihood function binomial sampling 
particular value observations mutually independent probability heads tails observation 
consequently equation jd dj number heads tails observed respectively 
probability distributions jd commonly referred prior posterior respectively 
quantities said su cient statistics binomial sampling provide summarization data su cient compute posterior prior 
possible values expansion rule probability determine probability th toss thumbtack come heads xn xn jd jd jd jd denotes expectation respect distribution jd 
complete bayesian story example need method assess prior distribution common approach usually adopted convenience assume distribution beta distribution beta trast classical statisticians refer random variable 
text refer uncertain random variables simply variables 
beta beta beta beta beta distributions 
parameters beta distribution gamma function satis es 
quantities referred hyperparameters distinguish parameter hyperparameters greater zero distribution normalized 
examples beta distributions shown 
beta prior convenient reasons 
equation posterior distribu tion beta distribution jd beta say set beta distributions conjugate family distributions binomial sampling 
expectation respect distribution simple form beta beta prior simple expression probability heads th toss xn assuming beta distribution assessed number ways 
example assess probability heads rst toss thumbtack probability wheel 
imagine having seen outcomes ips probability heads toss 
equation heads probabilities solve assessment technique known method imagined data 
assessment method equation 
equation says start beta prior observe heads tails posterior new technically hyperparameters prior small positive numbers normalized 
prior beta distribution 
recognizing beta prior encodes state minimum information assess determining possibly fractional number observations heads tails equivalent actual knowledge ipping 
alternatively assess regarded equivalent sample size current knowledge 
technique known method equivalent samples 
techniques assessing beta distributions discussed winkler duncan 
beta prior convenient accurate problems 
example suppose think thumbtack mayhave purchased magic shop 
case appropriate prior may mixture beta distributions example beta beta beta probability thumbtack heavily weighted heads tails 
ect introduced additional hidden unobserved variable states correspond possibilities thumbtack biased heads thumbtack biased tails thumbtack normal wehave asserted conditioned state beta distribution 
general simple methods method imagined data determining beta prior accurate re ection beliefs 
cases beta prior inaccurate accurate prior assessed introducing additional hidden variables example 
far considered observations drawn binomial distribution 
general observations may drawn physical probability distribution xj likelihood function parameters purposes discussion assume number parameters nite 
example may continuous variable gaussian physical probability distribution mean variance vg 
xj regardless functional form learn parameters data bayesian approach 
done binomial case de ne variables corre sponding unknown parameters assign priors variables bayes rule update beliefs parameters data jd dj dj average possible values predictions 
example xn jd xn jd class distributions known exponential family computations done ciently closed form 
members class include binomial multi nomial normal gamma poisson multivariate normal distributions 
member family su cient statistics xed dimension random sample simple conjugate prior 
bernardo smith pp 
compiled important quantities bayesian computations commonly members exponential family 
summarize items multinomial sampling illustrate ideas 
multinomial sampling observed variable discrete having possible states likelihood function rg parameters 
parameter 
case case binomial sampling parameters correspond physical probabilities 
su cient statistics data set fx xn xng fn nrg ni number times simple conjugate prior multinomial sampling dirichlet distribution dir rk ry posterior distribution jd dir nr 
techniques assessing beta distribution including methods imagined data equivalent samples assess dirichlet distributions 
conjugate prior data set probability distribution observation xn jd dir nr nk shall see important quantity analysis marginal likelihood evidence dj 
case dj ry nk advances monte carlo methods possible ciently distribu tions outside exponential family 
see example gilks 

fact characterized exceptions exponential family class distributions su cient statistics xed dimension koopman pitman 
note explicit mention state knowledge useful reinforces notion probabilities subjective 
concept place notation simply adds clutter 
remainder tutorial shall mention explicitly 
closing section emphasize bayesian classical ap proaches may yield prediction fundamentally di erent meth ods learning data 
illustration revisit thumbtack problem 
bayesian estimate physical probability heads obtained manner essentially opposite classical approach 
classical approach xed albeit unknown imagine data sets size may generated sampling binomial distribution determined data set occur probability dj produce estimate 
evaluate estimator compute expectation variance estimate respect data sets ep dj dj dj dj ep dj choose estimator balances bias dj variance estimates possible values apply estimator data set observe 
commonly estimator maximum likelihood ml estimator selects value maximizes likelihood dj 
binomial sampling ml nk prk nk types sampling ml estimator unbiased 
values ml estimator zero bias 
addition values variance ml estimator greater unbiased estimator see schervish 
contrast bayesian approach xed imagine possible values data set generated 
estimate physical probability heads just 
uncertain nal estimate expectation respect posterior beliefs value jd jd low bias variance desirable properties estimator 
desirable properties include consistency robustness 
expectations equations di erent cases lead di erent estimates 
way frame di erence say classical bayesian approaches di erent de nitions means estimator 
solutions correct self consistent 
unfortunately methods drawbacks lead endless debates merit approach 
example bayesians argue sense consider expectations equation see single data set 
saw data set combine larger data set 
contrast classical statisticians argue su ciently accurate priors assessed situations 
common view emerging method sensible task hand 
share view believe bayesian approach especially light advantages mentioned points 
consequently concentrate bayesian approach 
bayesian networks far considered simple problems variables 
real learning problems typically interested looking relationships large 
bayesian network representation suited task 
graphical model ciently encodes joint probability distribution physical bayesian large set variables 
section de ne bayesian network show constructed prior knowledge 
network set variables fx xng consists network structure encodes set conditional independence assertions variables set local probability distributions associated variable 
components de ne joint probability distribution network structure directed acyclic graph 
nodes correspondence variables xi denote variable corresponding node pai denote parents node xi variables corresponding parents 
lack possible arcs encode conditional independencies 
particular structure joint probability distribution ny local probability distributions distributions corresponding terms product equation 
consequently pair encodes joint distribution 
probabilities encoded network may physical 
building bayesian networks prior knowledge probabilities bayesian 
learning networks data probabilities physical values may uncertain 
subsequent sections describe learn structure probabilities bayesian network data 
remainder section explore construction bayesian networks prior knowledge 
shall see section procedure useful learning bayesian networks 
illustrate process building bayesian network consider problem de credit card fraud 
determining variables model 
possible choice variables problem fraud gas jewelry age sex representing current purchase fraudulent gas purchase hours jewelry purchase hours age sex card holder respectively 
states variables shown 
course realistic problem include variables 
model states variables ner level detail 
example age continuous variable 
initial task straightforward 
part task correctly identify goals modeling prediction versus explanation versus exploration identify possible observations may relevant problem determine subset observations worthwhile model organize observations variables having mutually exclusive collectively exhaustive states 
di culties unique modeling bayesian networks common approaches 
clean solutions guidance ered decision analysts howard matheson data available statisticians tukey 
phase bayesian network construction build directed acyclic graph encodes assertions conditional independence 
approach doing observations 
chain rule probability wehave ny xi xi subset ifx xi xi fx xi gn conditionally independent combining equations obtain xi xij ny xij fraud age gas jewelry male sex male male male female female female bayesian network detecting credit card fraud 
arcs drawn cause ect 
local probability distribution associated node shown adjacent node 
asterisk shorthand state 
comparing equations see variables sets correspond bayesian network parents pa pan turn fully specify arcs network structure consequently determine structure bayesian network order vari ables determine variables sets satisfy equation example ordering conditional independencies sjf gjf gjf obtain structure shown 
jjf jjf approach serious drawback 
choose variable order resulting network structure may fail reveal conditional independencies variables 
example construct bayesian network fraud problem ordering obtain fully connected network structure 
worst case explore 
variable orderings nd best 
fortunately technique constructing bayesian networks require ordering 
approach observations people readily assert causal relationships variables causal relationships typically correspond assertions conditional dependence 
particular construct bayesian network set variables simply draw arcs cause variables immediate ects 
cases doing results network structure satis es de nition equation 
example assertions fraud direct cause gas fraud age sex direct causes jewelry obtain network structure 
causal semantics bayesian networks large part responsible success bayesian networks representation expert systems heckerman 
section see learn causal relationships data causal semantics 
nal step constructing bayesian network assess local probability distribution 
fraud example variables discrete assess distribution xi con guration pai 
example distributions shown 
note described construction steps simple sequence intermingled practice 
example judgments conditional independence cause ect uence problem formulation 
assessments probability lead changes network structure 
exercises help gain familiarity practice building bayesian networks jensen 
inference bayesian network constructed bayesian network prior knowledge data combina tion usually need determine various probabilities interest model 
example problem concerning fraud detection want know probability fraud observations variables 
probability stored directly model needs computed 
general computation probability interest model known probabilistic inference 
section describe probabilistic inference bayesian networks 
bayesian network determines joint probability distribution principle bayesian network compute probability 
example bayesian network probability fraud observations variables computed follows fja problems variables direct approach practical 
variables discrete exploit conditional independencies encoded bayesian network computation cient 
example conditional independencies equation equation fja gjf jjf gjf jjf gjf jjf gjf jjf researchers developed probabilistic inference algorithms bayesian net works discrete variables exploit conditional independence roughly described di erent twists 
example howard matheson shachter developed algorithm reverses arcs network structure answer probabilistic query read directly graph 
algorithm arc reversal corresponds application bayes theorem 
pearl developed message passing scheme updates probability distributions network response observations variables 
lauritzen spiegelhalter jensen 
dawid created gorithm rst transforms bayesian network tree node tree corresponds subset variables algorithm exploits ical properties tree perform probabilistic inference 
ambrosio developed inference algorithm simpli es sums products symbolically transformation equation 
commonly algorithm discrete variables lauritzen spiegelhalter jensen dawid 
methods exact inference bayesian networks encode multivariate gaussian gaussian mixture distributions developed shachter lauritzen respectively 
methods assertions conditional indepen dence simplify inference 
approximate methods inference bayesian networks distributions generalized linear regression model devel oped saul jaakkola jordan 
conditional independence simplify probabilistic inference exact ference arbitrary bayesian network discrete variables np hard cooper 
approximate inference example monte carlo methods np hard dagum luby 
source di culty lies undirected cycles bayesian network structure cycles structure ignore directionality arcs 
add arc age gas network structure obtain structure undirected cycle bayesian network structure contains undirected cycles inference intractable 
applications structures simple simpli ed su ciently sacri cing accuracy inference cient 
applications generic inference methods tical researchers developing techniques custom tailored particular network topologies heckerman suermondt cooper saul jaakkola jordan particular inference queries agogino shachter jensen andersen darwiche provan 
learning probabilities bayesian network sections show re ne structure local probability distributions bayesian network data 
result set techniques data analysis combines prior knowledge data produce improved knowledge 
section consider simplest version problem data update probabilities bayesian network structure 
recall thumbtack problem learn probability heads 
update posterior distribution variable represents physical probability heads 
follow approach probabilities bayesian network 
particular assume causal knowledge problem physical joint probability distribution encoded network structure write xj ny vector parameters distribution vector parameters denotes event hypothesis statistics physical joint probability distribution factored addition assume random sample fx xng physical joint probability distribution refer element xl case 
section encode uncertainty parameters de ning vector valued variable assessing prior probability density function sjs 
problem learning probabilities bayesian network stated simply random sample compute posterior distribution sjd 
de ned network structure hypotheses overlap 
example fx joint distribution factored network structure containing arc factored network structure 

overlap presents problems model averaging described section 
add conditions de nition insure overlap 
heckerman geiger describe set conditions 
refer distribution viewed function distri bution function 
readers familiar methods supervised learning recognize local distribution function probabilistic classi cation regression function 
network viewed collection probabilistic classi ca tion regression models organized conditional independence relationships 
examples classi cation regression models produce probabilistic outputs include linear regression generalized linear regression probabilistic neural networks mackay probabilistic decision trees buntine friedman goldszmidt kernel density estimation methods book dictionary methods friedman 
principle forms learn probabilities bayesian network cases bayesian techniques learning available 
stud ied models include unrestricted multinomial distribution cooper herskovits linear regression gaussian noise buntine heckerman geiger generalized linear regression mackay neal saul 
tutorial illustrate basic ideas learning probabilities structure unrestricted multinomial distribution 
case variable xi dis crete having ri possible values xr local distribution function collection multinomial distributions distribution con guration pai 
assume ijk pa qi pa ri denote con gurations pai ijk parameters 
parameter ij ijk 
convenience de ne vector parameters ij ij ijr term unrestricted contrast distribution multinomial distributions low dimensional functions pai example generalized linear regression model 
class local distribution functions compute posterior distribution sjd ciently closed form assumptions 
rst assumption missing data random sample wesay random sample complete 
second assumption parameter vectors ij mutually sample sample bayesian network structure depicting assumption parameter indepen dence learning parameters network structure 
variables binary 
denote states denote states independent 
sjs ny qi refer assumption introduced spiegelhalter lauritzen parameter independence 
joint physical probability distribution factors network structure assumption parameter independence represented larger bayesian network structure 
example network structure represents assumption parameter independence fx yg binary hypothesis network structure 
encodes physical joint probability distribution assumptions complete data parameter independence parameters remain independent random sample sjd ny qi update vector parameters ij independently just variable case 
assuming vector ij prior distribution dir ijj ij ijr obtain computation straightforward parameters equal 
details see thiesson 
posterior distribution dir ijj ij nij ijr nijk number cases xi pai pa thumbtack example average possible con gurations obtain predictions interest 
example compute xn jd sh xn case seen suppose case xn xi xk pai pa depend xn jd sjd compute expectation rst fact parameters remain independent xn jd ny equation obtain ijk sjd xn jd ny ny ijk nijk ij nij ijk ijk ij ij ijk nij nijk 
computations simple unrestricted multinomial distributions exponential family 
computations linear regression gaussian noise equally straightforward buntine heckerman geiger 
methods incomplete data discuss methods learning parameters random sample incomplete variables cases observed 
important distinction concerning missing data absence observation dependent actual states variables 
example missing datum drug study may indicate patient sick due side ects drug continue study 
contrast variable hidden observed case absence data independent state 
bayesian methods graphical models suited analysis situations methods handling missing data absence independent state simpler absence state dependent 
tutorial concentrate simpler situation 
readers interested complicated case see rubin robins pearl 
continuing example unrestricted multinomial distributions suppose observe single incomplete case 
denote observed served variables case respectively 
assumption parameter independence compute posterior distribution ij network structure follows zjy pa jy sh xr sh sh see spiegelhalter lauritzen derivation 
term curly brackets equation dirichlet distribution 
xi variables pai observed case posterior distribution ij linear combination dirichlet distributions dirichlet mixture mixing coe cients pa sh xk sh ri 
observe second incomplete case dirichlet components equation split dirichlet mixtures 
posterior distribution ij mixture dirichlet mixtures 
continue observe incomplete cases missing values posterior distribution ij contain number components exponential number cases 
general interesting set local likelihoods priors exact computation posterior distribution intractable 
require approximation incomplete data 
monte carlo methods class approximations monte carlo sampling methods 
approx extremely accurate provided willing wait long computations converge 
section discuss monte carlo methods known gibbs sampling introduced geman geman 
variables fx xng joint distribution gibbs sampler approximate expectation function respect follows 
choose initial state variables random 
pick variable xi current state compute probability distribution states variables 
sample state xi probability distribution compute 
iterate previous steps keeping track average value 
limit number cases approach nity average equal provided conditions met 
gibbs sampler irreducible probability distribution eventually sample possible con guration possible initial con guration example contains zero probabilities gibbs sampler irreducible 
second xi chosen nitely 
practice algorithm deterministically rotating variables typically 
introductions gibbs sampling monte carlo methods including methods initialization discussion convergence neal madigan york 
illustrate gibbs sampling approximate probability density sjd particular con guration incomplete data set fy bayesian network discrete variables independent dirichlet priors 
approximate sjd rst initialize states unobserved variables case 
result complete random sample dc 
second choose variable variable xi case observed original random sample reassign state probability distribution il il il denotes data set dc observation removed sum denominator runs states variable 
shall see section terms numerator denominator computed ciently see equation 
third repeat reassignment unobserved variables producing new complete random sample fourth compute posterior density sjd sh described equations 
previous steps average sjd approximation 
gaussian approximation monte carlo methods yield accurate results intractable example sample size large 
approximation cient monte carlo methods accurate relatively large samples gaussian approximation kass kass raftery 
idea approximation large amounts data sjd dj sjs approximated multivariate gaussian distribution 
particular log dj sjs de ne con guration maximizes 
con guration maximizes sjd known maximum posteriori map con guration second degree taylor polynomial approximate obtain transpose row vector negative hessian evaluated raising power equation obtain sjd dj sjs dj sjs expf sjd approximately gaussian 
compute gaussian approximation compute negative hessian evaluated section discuss methods nding meng rubin describe numerical technique computing second derivatives 
raftery shows approximate hessian likelihood ratio tests available statistical packages 
thiesson demonstrates unrestricted multinomial distributions second derivatives computed bayesian network inference 
map ml approximations em algorithm sample size data increases gaussian peak sharper tending delta function map con guration limit need com pute averages expectations 
simply predictions map con guration 
approximation observation sample size increases ect prior sjs diminishes 
approximate maximum maximum likelihood ml con guration arg max dj class techniques nding ml map gradient optimization 
example gradient ascent follow derivatives dj local maximum 
russell 
thiesson show compute derivatives likelihood bayesian network unrestricted multinomial distributions 
buntine discusses general case hood function comes exponential family 
course gradient methods nd local maxima 
technique nding local ml map expectation maximization em algorithm dempster 
nd local map ml assigning con guration random 
compute expected su cient statistics complete data set expectation taken respect joint distribution conditioned assigned con guration known data discrete example compute xjd nijk nx pa yl possibly incomplete lth case xi variables pai observed case xl term case requires trivial computation zero 
bayesian network inference algorithm evaluate term 
computation called expectation step em algorithm 
expected su cient statistics actual su cient statistics complete random sample dc 
ifwe doing ml calculation determine con guration maximize 
discrete example ijk xjd nijk ri xjd nijk doing map calculation determine con guration maximizes 
discrete example ijk xjd nijk ijk pri ijk ep xjd nijk assignment called maximization step em algorithm 
dempster 
showed certain regularity conditions iteration expectation maximization steps converge local maximum 
em algorithm typically applied su cient statistics exist local distribution functions exponential family generalizations em complicated local distributions see saul 
map con guration depends coordinate system parameter variables expressed 
expression map con guration obtained procedure 
transform variable set ij ij ijr new coordinate system ij ij ijr ijk log ijk ij ri 
coordinate system denote referred canonical coordinate system multinomial distribution see bernardo smith pp 

determine con guration maximizes 
transform map con guration original coordinate system 
map con guration corresponding coordinate system advantages discussed thiesson mackay 
learning parameters structure consider problem learning structure probabilities bayesian network data 
assuming think structure improved uncertain network structure encodes physical joint probability distribution bayesian approach encode uncertainty de ning discrete variable states correspond possible network structure hypotheses assessing probabilities 
random sample physical probability distribution compute posterior distribution jd posterior distributions sjd distributions turn compute expectations interest 
example predict case seeing compute xn jd jd xn sjd performing sum assume network structure hypotheses mutually exclusive 
return point section 
computation sjd described previous sections 
computation jd straightforward principle 
bayes theorem jd djs normalization constant depend structure 
de termine posterior distribution network structures need compute marginal likelihood data djs possible structure 
discuss computation marginal likelihoods detail section 
consider example unrestricted multinomial distributions parameter independence dirichlet priors complete data 
discussed missing data parameter vector ij updated independently 
ect separate multi sided thumbtack problem consequently marginal likelihood data just product marginal likelihoods pair equation djs ny qi ij ij nij yr ijk nijk ijk formula rst derived cooper herskovits 
unfortunately full bayesian approach described impractical 
important computation bottleneck produced average models equa tion 
consider bayesian network models variables number possible structure hypotheses exponential consequently situations user exclude hypotheses approach 
statisticians confronted problem decades context types models approaches address problem model selection selective model averaging 
approach select model structure hypothesis possible models correct model 
approach select manageable number models possible models pretend models exhaustive 
related approaches raise important questions 
particular approaches yield accurate results applied bayesian network structures 
dowe search models 
dowe decide model 
question accuracy di cult answer theory 
researchers shown experimentally selection single hypothesis yields ac predictions cooper herskovits aliferis cooper heckerman model averaging monte carlo methods cient yield better predictions madigan 
results somewhat surprising largely responsible great deal interest learning bayesian networks 
sections consider di erent de nitions means model discuss computations entailed de nitions 
section discuss model search 
note model averaging model selection lead models generalize new data 
techniques help avoid tting data 
suggested equation bayesian methods model averaging model selection cient sense cases smooth train model 
shall see sections advantage holds true bayesian approach general 
criteria model selection literature learning bayesian networks concerned model selection 
approaches criterion measure degree network structure equivalence class ts prior knowledge data 
search algorithm nd equivalence class receives high score criterion 
selective model averaging complex advantageous identify network structures signi cantly di erent 
cases single criterion identify complementary network structures 
section discuss criteria simpler problem model selection 
discussion selective model averaging see madigan raftery 
relative posterior probability criterion model selection log relative posterior proba bility log log log djs 
logarithm numerical 
criterion components log prior log marginal likelihood 
section examine computation log marginal likelihood 
section discuss assessment structure priors 
note comments terms relevant full bayesian approach 
log marginal likelihood interesting interpretation described dawid 
chain rule probability wehave log djs nx log xl term xl prediction xl model averaging parameters 
log term thought utility reward prediction utility function log 
model highest log marginal likelihood highest posterior probability assuming equal priors structure model best sequential predictor data log utility function 
dawid notes relationship criterion cross tion 
form cross validation known leave cross tion rst train model cases random sample say vl fx xl xl xng 
predict omitted case reward pre diction utility function 
repeat procedure case random sample sum rewards prediction 
prediction probabilistic utility function log obtain cross validation criterion cv nx log similar equation 
problem criterion training test cases interchanged 
example compute jv equation equivalent criterion log jd jd log log djs djs 
ratio djs djs known bayes factor 
utility function known proper scoring rule encourages people assess true probabilities 
proper scoring rules rule particular see bernardo 
training testing 
compute jv training testing 
interchanges lead selection model ts data dawid 
various approaches attenuating problem described see equation log marginal likelihood criterion avoids problem altogether 
criterion interchange training test cases 
local criteria consider problem diagnosing observation set ndings 
suppose set consideration mutually exclusive collectively exhaustive may represent single variable possible bayesian network classi cation problem shown 
posterior probability criterion global sense equally sensitive possible dependencies 
diagnosis problem posterior probability criterion just sensitive dependencies nding variables dependencies ndings 
assuming observe ndings reasonable criterion local sense ignores dependencies ndings sensitive dependencies ndings 
observation applies classi cation regression problems complete data 
local criterion suggested spiegelhalter 
variation sequential log marginal likelihood criterion lc nx log dl fl denote observation ndings lth case respectively 
words compute lth term product train model rst cases determine predicts ndings lth case 
view criterion log marginal likelihood form cross validation training test cases interchanged 
log utility function interesting theoretical properties real world problems 
general appropriate reward utility function depend decision making problem problems probabilistic models applied 
howard matheson collected series articles describing construct utility models speci decision problems 
construct utility models suitably modi ed forms equation model selection 
finding finding 
finding bayesian network structure medical diagnosis 
computation marginal likelihood mentioned criterion model selection log relative posterior prob ability log log log djs 
section discuss computation second component criterion log marginal likelihood 
local distribution functions exponential family mutual independence parameters conjugate priors parameters complete data log marginal likelihood computed ciently closed form 
equation example unrestricted multinomial distributions 
buntine heckerman geiger discuss computation local distribution functions 
concentrate approximations incomplete data 
monte carlo gaussian approximations learning parameters discussed section useful computing marginal likelihood incomplete data 
monte carlo approach described chib raftery uses bayes theorem djs sjs dj sjd con guration prior term numerator evaluated directly 
addition likelihood term numerator computed bayesian network inference 
posterior term denominator computed gibbs sampling described section 
sophisticated monte carlo methods described 

discussed monte carlo methods accurate computationally ine cient especially large databases 
contrast methods gaussian approx imation cient accurate monte carlo methods large data sets 
recall large amounts data dj sjs approximated gaussian distribution 
consequently djs dj sjs evaluated closed form 
particular substituting equation equation integrating logarithm result obtain approximation log djs log dj log sjs log dimension 
bayesian network unrestricted multinomial distributions dimension typically qi ri 
hidden variables dimension lower 
see geiger 
discussion point 
approximation technique integration known laplace method refer equation laplace approximation 
kass 
shown certain regularity conditions relative errors approximation number cases laplace approximation extremely accurate 
detailed discussions approximation see example kass 
kass raftery 
laplace approximation cient relative monte carlo approaches computation intensive large dimension models 
simpli cation diagonal elements hessian doing incorrectly impose independencies parameters researchers shown approximation accurate circumstances see becker le cun chickering heckerman 
cient variant laplace approximation described cheeseman stutz approximation autoclass program data clustering see chickering heckerman 
obtain cient accurate approximation retaining terms equation increase log dj increases linearly increases log large approximated ml con guration obtain log djs log dj log approximation called bayesian information criterion bic rst derived schwarz 
bic approximation interesting respects 
depend prior 
consequently approximation assessing prior 
sec technical assumptions derive approximation prior non zero ond approximation quite intuitive 
term measuring parameterized model predicts data log dj term punishes complexity model logn 
third bic approximation exactly minus minimum description length mdl criterion described rissanen 
recall ing discussion section see marginal likelihood provides connection cross validation mdl 
priors compute relative posterior probability network structure assess structure prior parameter priors sjs large sample approximations bic mdl 
parameter priors sjs required alternative scoring functions discussed section 
unfortunately network structures possible assessments intractable 
certain assumptions derive structure parameter priors network structures manageable number direct assessments 
authors discussed assumptions corresponding methods deriving priors cooper herskovits buntine spiegelhalter heckerman heckerman geiger 
section examine approaches 
priors network parameters consider assessment priors parameters network structures 
consider approach 
address case local distribution functions unrestricted multinomial distributions assumption parameter independence holds 
approach concepts independence equivalence distribution equivalence 
wesay network structures represent set conditional independence assertions verma pearl 
example fx zg network structures 

represent independence assertion conditionally independent consequently network structures equivalent 
example complete network structure missing edge encodes assertion conditional independence 
contains variables 
possible complete network structures network structure possible ordering variables 
complete network structures independence equivalent 
general network structures independence equivalent structure ignoring arc directions structures verma pearl 
structure ordered tuple arc arc concept distribution equivalence closely related independence equiva 
suppose bayesian networks consideration local distribution functions 
restriction se large family 
say bayesian network structures distribution equivalent respect wrt represent joint probability distributions exists xj xj vice versa 
distribution equivalence wrt implies independence equivalence con verse hold 
example family generalized linear regression models complete network structures variables represent sets distributions 
example unrestricted distributions linear regression models gaussian noise independence equivalence implies distribution equivalence heckerman geiger 
notion distribution equivalence important network structures distribution equivalent wrt hypotheses associated structures identical example distribution equivalent probabilities equal state information 
heckerman 
call property hypothesis equivalence 
light property associate hypothesis equivalence class structures single network structure methods learning network structure interpreted methods learning equivalence classes net structures sake brevity blur distinction 
example sum network structure hypotheses equation replaced sum equivalence class hypotheses 
cient algorithm identifying equivalence class network structure chickering 
note hypothesis equivalence holds provided interpret bayesian network structure simply representation conditional independence 
stronger def bayesian networks exist arcs causal interpretation see section 
heckerman 
heckerman argue unreasonable assume hypothesis equivalence working causal bayesian networks reasonable adopt weaker assumption likelihood equivalence says observations database help discriminate equivalent network structures 
return main issue section derivation priors man number assessments 
geiger heckerman show assumptions parameter independence likelihood equivalence imply parameters complete network structure sc dirichlet distribution constraints hyperparameters ijk pa user equivalent sample size xk computed user joint probability distribution 
result remarkable assumptions leading constrained dirichlet solution qualitative 
determine priors parameters incomplete network structures heckerman 
assumption parameter modularity says xi parents network structures qi 
call property parameter modularity says distributions parameters ij depend structure network local variable xi xi parents 
assumptions parameter modularity parameter independence simple matter construct priors parameters arbitrary network structure priors complete network structures 
particular parameter independence construct priors parameters node separately 
furthermore node xi parents pai network structure identify complete network structure xi parents equation parameter modularity determine priors node 
result terms ijk network structures determined equation 
assessments xjs derive parameter priors possible network structures 
combining equation equation obtain model selection criterion assigns equal marginal likelihoods independence equivalent network structures 
assess xjs constructing bayesian network called prior network encodes joint distribution 
heckerman 
discuss construction network 
priors structures consider assessment priors network structure hypotheses 
note alternative criteria described section incorporate prior biases network structure recall method equivalent samples assessing beta dirichlet distributions discussed section 
construction procedure assumes structure non zero prior probability 
hypotheses 
methods similar discussed section assess biases 
simplest approach assigning priors network structure hypotheses assume hypothesis equally 
course assumption typically inaccurate sake convenience 
simple re nement approach user exclude various hypotheses judgments cause ect impose uniform prior remaining hypotheses 
illustrate approach section 
buntine describes set assumptions leads richer cient approach assigning priors 
rst assumption variables ordered knowledge time precedence 
second assumption presence absence possible arcs mutually independent 
assumptions probability assessments possible arc ordering determines prior probability possible network structure hypothesis 
extension approach allow multiple possible orderings 
simpli cation assume probability arc absent independent speci arc question 
case probability assessment required 
alternative approach described heckerman 
uses prior network 
basic idea penalize prior probability ofany structure measure deviation structure prior network 
heckerman 
suggest reasonable measure deviation 
madigan 
approach imaginary data domain expert 
approach computer program helps user create hypothetical set complete data 
techniques section compute posterior probabilities network structure hypotheses data assuming prior probabilities hypotheses uniform 
posterior probabilities priors analysis real data 
search methods section examine search methods identifying network structures high scores criterion 
consider problem nding best network set networks node parents 
unfortunately problem np hard restrictive prior equation chickering 
researchers heuristic search algorithms including greedy search greedy search restarts best rst search monte carlo methods 
search methods cient model selection criterion separable 
network structure domain wesay criterion structure separable written product variable speci criteria ny xi pai di di data restricted variables xi pai 
example separable criterion bd criterion equations conjunction ofthe methods assessing structure priors described section 
commonly search methods bayesian networks successive arc changes network employ property separability toevaluate merit change 
possible changes easy identify 
pair variables arc connecting arc reversed removed 
arc connecting arc added direction 
changes subject constraint resulting network contains directed cycles 
denote set eligible changes graph denote change log score network resulting modi cation 
separable criterion arc xi added deleted xi pai di need evaluated determine 
arc xi xj reversed xi pai di xj dj need evaluated 
simple heuristic search algorithm greedy search 
choose network structure 
evaluate change isa maximum provided positive 
terminate search positive value 
criterion separable avoid recomputing terms change 
particular xi xj parents changed remains unchanged changes involving nodes long resulting network acyclic 
candidates initial graph include empty graph random graph graph determined polynomial algorithms described previously section prior network 
potential problem local search method getting stuck local maximum 
method escaping local maxima greedy search random restarts 
approach apply greedy search hit local maximum 
randomly perturb network structure repeat process manageable number iterations 
method escaping local maxima simulated annealing 
approach initialize system temperature 
pick eligible change random evaluate expression exp 
change change probability repeat selection evaluation process times changes 
repetitions searching 
temperature current temperature decay factor continue search process 
searching lowered temperature times 
algorithm controlled parameters initialize algorithm start empty graph large eligible change creating random graph 
alternatively wemay start lower temperature initialization methods described local search 
method escaping local maxima best rst search korf 
approach space network structures searched systematically heuristic measure determines best structure examine 
chickering shown xed amount computation time greedy search random restarts produces better models simulated annealing best rst search 
important consideration search algorithm search space 
methods described search space bayesian network structures 
assumption hypothesis equivalence holds search space network structure equivalence classes 
bene approach search space smaller 
drawback approach takes longer move element search space 
spirtes meek chickering con rm observations experimentally 
unfortunately com available determine bene ts equivalence class search outweigh costs 
simple example move issues step back look approach 
nutshell construct structure parameter priors constructing bayesian network prior network additional assessments equivalent sample size causal constraints 
bayesian model selection selective model averaging full model averaging obtain networks prediction explanation 
ect procedure data improve structure probabilities initial bayesian network 
arti cial examples illustrate process 
consider problem fraud detection section 
suppose database ta table imagined database fraud problem 
case fraud gas jewelry age sex female male male male female female male female male female ble want predict case compute xn jd 
assert network structure hypotheses appreciable probability hypothesis corresponding network structure hypothesis corresponding structure arc added age gas 
furthermore assert hypotheses equally sh sh 
addition parameter priors equation andp isgiven prior network 
equations obtain jd sh jd 
models consider model average equation xn jd xn jd xn jd xn jd equation 
don display probability distri butions 
choose model choose assuming posterior probability criterion appropriate 
note data favors presence arc age gas factor 
surprising cases database fraud absent gas purchased card holder years old 
application model selection described spirtes meek illustrated 
hand constructed bayesian network domain icu management called alarm network beinlich 
random sample alarm network size 
simple prior network domain 
network encodes mutual independence variables shown uniform probability distributions variable 
shows network structure pass greedy search equivalence class space 
rst pass arcs added model score improve 
second pass arcs deleted model score improve 
structure priors uniform parameter priors computed prior network equation 
network structure learned procedure di ers true network struc ture single arc deletion 
ect data improve dramatically original model user 
bayesian networks supervised learning discussed section local distribution functions essentially classi cation regression models 
doing supervised learning explanatory input variables cause outcome target variable data complete bayesian network classi cation regression approaches identical 
data complete input target variables simple cause ect relationship tradeo emerge bayesian network approach methods 
example consider classi cation problem 
bayesian network encodes dependencies ndings ndings classi cation model decision tree encodes relationships ndings 
decision tree may produce accurate classi cations encode necessary relationships fewer parameters 
local criteria bayesian network model selection mitigates advantage 
furthermore bayesian network provides natural representation encode prior knowledge giving model possible advantage su ciently small sample sizes 
argument bias variance analysis suggests approach dramatically outperform friedman 
singh provan compare classi cation accuracy bayesian networks decision trees complete data sets university california irvine tory machine learning databases 
speci cally compare algorithm learns structure probabilities bayesian network variation bayesian methods described 
algorithm includes model selection phase discards input variables 
show bayesian networks deci sions trees classi cation error 
results support argument case deleted alarm network structure 
prior network encoding user beliefs alarm domain 
random sample size generated alarm network 
network learned prior network random sample 
di erence learned true structure arc deletion noted 
network probabilities shown 
bayesian network structure autoclass 
variable hidden 
possible states correspond underlying classes data 
friedman 
input variables cause target variable data incomplete depen input variables important discussed 
bayesian networks provide natural framework learning encoding de 
unfortunately studies done comparing approaches methods handling missing data 
bayesian networks unsupervised learning techniques described unsupervised learning 
simple example autoclass program cheeseman stutz performs data clustering 
idea autoclass single hidden observed variable causes observations 
hidden variable discrete possible states correspond underlying classes data 
autoclass described network 
reasons computational ciency cheeseman stutz assume discrete variables gure user de ned sets continuous variables fc mutually independent data set autoclass searches variants model including number states hidden variable selects variant approximate posterior probability local maximum 
autoclass example user presupposes existence hidden variable 
situations may unsure presence hidden variable 
cases score models hidden variables reduce uncertainty 
illustrate approach real world case study section 
alternatively wemay little idea hidden variables model 
search algorithms spirtes bayesian network structure observed variables 
bayesian network structure hidden variables shaded suggested network structure 

provide method identifying possible hidden variables situations 
martin vanlehn suggest method 
approach observation set variables mutually depen dent simple explanation variables single hidden common cause rendering mutually independent 
identify possible hidden variables rst apply learning technique select model containing hidden variables 
look sets mutually dependent variables learned model 
set variables combinations thereof create new model containing hidden variable renders set variables conditionally independent 
score new models possibly nding better original 
example model sets mutually dependent variables 
shows model containing hidden variables suggested model 
learning causal relationships mentioned causal semantics bayesian network provide means learn causal relationships 
section examine semantics provide basic discussion causal relationships learned 
note methods new controversial 
critical discussions sides issue see spirtes 
pearl humphreys freedman 
purposes illustration suppose marketing analysts want know increase decrease leave exposure particular advertisement order maximize pro sales product 
variables ad buy represent individual seen advertisement purchased product respectively 
component analysis learn physical probability true force true physical probability true force false 
denote probabilities bj bj respectively 
method learn probabilities perform randomized experiment select similar populations random force true population false observe method conceptually simple may di cult expensive nd similar populations suitable study 
alternative method follows causal knowledge 
particular suppose causes force true simply observe true current population advertisement causal uence individual purchase 
consequently bj bja bja physical probability true observe true current population 
similarly bj bja 
contrast causes forcing state uence 
bj bj 
general knowledge causes allows equate yjx yj denotes intervention forces purposes discussion rule operational de nition cause 
pearl heckerman shachter discuss versions de nition complete precise 
example knowledge causes allows learn bj bj observations randomized experiment needed 
determine causes 
answer lies assumption connection causal probabilistic dependence known causal markov condition described spirtes 

say directed acyclic causal graph variables nodes correspondence arc node node direct cause causal markov condition says ifc causal graph bayesian network structure joint physical probability distribution section described method condition constructing bayesian network structure causal assertions 
researchers spirtes condition holds applications 
causal markov condition infer causal relationships conditional important interventions interfere normal ect see heckerman shachter discussion point 
independence conditional dependence relationships learn data 
illustrate process marketing example 
suppose learned high bayesian probability physical probabilities bja bja equal 
causal markov condition simple causal explanations dependence cause cause hidden common cause person income causes data selection 
explanation known selection bias 
selection bias occur example database failed include instances false 
causal tions presence arcs illustrated 
course complicated explanations presence hidden common cause selection bias pos sible 
far causal markov condition told causes sup pose observe additional variables income location represent income geographic location possible purchaser respectively 
furthermore suppose learn high probability bayesian network shown fig ure 
causal markov condition causal explanation conditional independence conditional dependence relationships encoded bayesian network ad cause buy 
explanations described pre vious paragraph combinations thereof produce probabilistic relationships encoded 
observation pearl verma spirtes 
created algorithms inferring causal relationships dependence relationships complicated situations 
case study college plans real world applications techniques discussed madigan raftery lauritzen 
singh provan friedman goldszmidt 
consider application comes study sewell shah investigated factors uence intention high school students attend college 
data analyzed groups statisticians including whittaker spirtes 
non bayesian techniques 
sewell shah measured variables wisconsin high school sex sex male female socioeconomic status ses low lower middle upper middle high intelligence quotient iq low lower middle upper middle high spirtes 
require assumption known faithfulness 
need assumption explicit follows assumption sjs probability density function 
ad ad buy buy ad buy ad buy income location ad buy causal graphs showing explanations observed dependence ad buy 
node corresponds hidden common cause ad buy 
shaded node indicates case included database 
bayesian network causes causal explanation causal markov condition 
parental encouragement pe low high college plans cp 
goal understand possibly causal relationships variables 
data described su cient statistics table 
entry denotes number cases variables take particular con guration 
rst entry corresponds con guration sex male ses low iq low pe low cp 
remaining entries correspond con gurations obtained cycling states variable variable cp varies quickly 
example upper lower half table corresponds male female students 
rst pass analyzed data assuming hidden variables 
generate priors network parameters method described section equivalent sample size prior network xjs uniform 
results sensitive choice parameter priors 
example results reported section changed qualitatively equivalent sample sizes ranging 
structure priors assumed network structures equally excluded structures sex ses parents cp children 
data set complete equations compute posterior probabilities network structures 
network structures exhaustive search structures shown 
note graph posterior probability extremely close 
adopt causal markov assumption assume hidden table su cient statistics shah study 
reproduced permission university chicago press 
university chicago 
rights reserved 
iq sex pe cp log pds ps ses iq sex pe cp log ps posteriori network structures hidden variables 
ses variables arcs graphs interpreted causally 
results surprising example causal uence socioeconomic status iq college plans 
results interesting 
example graph conclude sex uences college plans indirectly parental uence 
graphs di er orientation arc pe iq 
causal relationship plausible 
note second graph selected spirtes 
non bayesian approach slightly di erent assumptions 
suspicious result suggestion socioeconomic status direct uence iq 
question result considered new models obtained models replacing direct uence hidden variable pointing ses iq 
considered models hidden variable pointed ses iq pe connections ses pe pe iq removed 
structure varied number states hidden variable 
computed posterior probability models cheeseman stutz variant laplace approximation 
nd map em gorithm largest local maximum runs di erent random initializations models considered highest posterior probability shown 
model times best model containing hidden variable 
model containing hidden vari able additional arc hidden variable pe times best model 
adopt causal markov assumption assume omitted reasonable model consideration strong evidence hidden variable uencing socioeconomic status iq population sensible result 
examination probabilities suggests hidden variable corresponds measure parent quality 
pointers literature software tutorials incomplete 
readers interested learning graphical models methods learning er additional pointers software 
buntine provides guide literature 
spirtes 
pearl methods large sample approximations learn bayesian networks 
addition discussed describe methods learning causal relationships observational data 
addition directed models researchers explored network structures containing undirected edges knowledge representation 
representations discussed pe low low high high ses low low high high iq high pe sex pe high ses sex male female male female male iq log ps sex pe cp ses ses low low low low high high high high ses high low high iq low low high high low low high high pe low high low high low high low high cp ses iq pe posteriori network structure hidden variable 
proba bilities shown map values 
probabilities omitted lack space 
lauritzen verma pearl whittaker richardson 
bayesian methods learning models data described dawid lauritzen buntine 
research groups developed software systems learning graphical models 
example scheines 
developed software program called tetrad ii learning cause ect 

built systems learn mixed graphical models variety criteria model selection 
thomas spiegelhalter gilks created system called bugs takes learning problem speci ed bayesian network compiles problem gibbs sampler computer program 
acknowledgments max chickering usama fayyad eric horvitz chris meek padhraic smyth comments earlier versions manuscript 
max chickering implementing software analyze shah data chris meek bringing data set attention 
notation variables corresponding nodes bayesian network sets variables corresponding sets nodes variable state set variables con guration typically refer complete case incomplete case missing data case respectively xn variables data set set cases dl rst cases xjy probability describe probability density probability distribution probability density expectation respect network structure directed acyclic graph pai variable node corresponding parents pa node xi bayesian network structure con guration variables pai ri number states discrete variable xi qi number con gurations pai sc complete network structure hypothesis corresponding network structure ijk ij multinomial parameter corresponding probability xi xk pa ij ijr iq equivalent sample size ijk dirichlet hyperparameter corresponding ijk ij ijk nijk number cases data set xi pai pa nij nijk aliferis cooper aliferis cooper 

evaluation algo rithm inductive learning bayesian belief networks simulated data sets 
proceedings tenth conference uncertainty arti cial intelligence seattle wa pages 
morgan kaufmann 


model search contingency tables coco 
dodge whittaker editors computational statistics pages 
physica verlag heidelberg 
becker lecun becker lecun 

improving convergence back propagation learning second order methods 
proceedings connectionist models summer school pages 
morgan kaufmann 
beinlich beinlich suermondt chavez cooper 

alarm monitoring system case study probabilistic inference techniques belief networks 
proceedings second european conference onarti cial intelli gence london pages 
springer verlag berlin 
bernardo bernardo 

expected information expected utility 
annals statistics 
bernardo smith bernardo smith 

bayesian theory 
john wiley sons new york 
buntine buntine 

theory re nement networks 
proceed ings seventh conference uncertainty arti cial intelligence los angeles ca pages 
morgan kaufmann 
buntine buntine 

learning classi cation trees 
arti cial intelligence frontiers statistics ai statistics iii 
chapman hall new york 
buntine buntine 

guide literature learning graphical models 
ieee transactions knowledge data engineering 
duncan duncan 

assessment beta prior distribution pm elicitation 
statistician 
cheeseman stutz cheeseman stutz 

bayesian classi cation autoclass theory results 
fayyad shapiro smyth uthurusamy editors advances knowledge discovery data mining pages 
aaai press menlo park ca 
chib chib 

marginal likelihood gibbs output 
journal american statistical association 
chickering chickering 

transformational characterization equivalent bayesian network structures 
proceedings eleventh conference uncertainty arti cial intelligence montreal qu pages 
morgan kaufmann 
chickering chickering 

learning equivalence classes bayesian network structures 
proceedings twelfth conference uncertainty arti cial intelligence portland 
morgan kaufmann 
chickering chickering geiger heckerman 

learning bayesian networks search methods experimental results 
proceedings fifth conference onarti cial intelligence statistics ft lauderdale fl pages 
society arti cial intelligence statistics 
chickering heckerman chickering heckerman 
revised november 
cient approximations marginal likelihood incomplete data bayesian network 
technical report msr tr microsoft research redmond wa 
cooper cooper 

computational complexity probabilistic inference bayesian belief networks research note 
arti cial intelligence 
cooper herskovits cooper herskovits 

bayesian method induction probabilistic networks data 
machine learning 
cooper herskovits cooper herskovits 
january 
bayesian method induction probabilistic networks data 
technical report smi section medical informatics stanford university 
cox cox 

probability frequency reasonable expectation 
american journal physics 
dagum luby dagum luby 

approximating probabilistic inference bayesian belief networks np hard 
arti cial intelligence 
ambrosio ambrosio 

local expression languages probabilistic de 
proceedings seventh conference uncertainty arti cial intelligence los angeles ca pages 
morgan kaufmann 
darwiche provan darwiche provan 

query dags prac tical paradigm implementing belief network inference 
proceedings twelfth con ference arti cial intelligence portland pages 
morgan kaufmann 
dawid dawid 

statistical theory 
prequential approach discus sion 
journal royal statistical society 
dawid dawid 

applications general propagation algorithm prob expert 
statistics computing 
de finetti de finetti 

theory probability 
wiley sons new york 
dempster dempster laird rubin 

maximum hood incomplete data em algorithm 
journal royal statistical society 
kass raftery wasserman 
july 
computing bayes factors combining simulation asymptotic approximations 
tech nical report department statistics carnegie mellon university pa friedman friedman 

computational learning statis tical prediction 
technical report department statistics stanford university 
friedman friedman 

bias variance loss curse dimen 
data mining knowledge discovery 
friedman goldszmidt friedman goldszmidt 

building clas si ers bayesian networks 
proceedings aaai thirteenth national conference arti cial intelligence portland pages 
aaai press menlo park ca 


chain graph markov property 
scandinavian journal statistics 
geiger heckerman geiger heckerman 
revised february 
characterization dirichlet distribution applicable learning bayesian networks 
technical report msr tr microsoft research redmond wa 
geiger geiger heckerman meek 

asymptotic model selection directed networks hidden variables 
proceedings twelfth con ference arti cial intelligence portland pages 
morgan kaufmann 
geman geman geman geman 

stochastic relaxation gibbs distributions bayesian restoration images 
ieee transactions pattern analysis machine intelligence 
gilks gilks richardson spiegelhalter 

markov chain monte carlo practice 
chapman hall 


probability weighing evidence 
new york 
heckerman heckerman 

tractable algorithm diagnosing multiple diseases 
proceedings fifth workshop uncertainty arti cial intelligence windsor pages 
association uncertainty arti cial intelligence tain view ca 
henrion shachter kanal lemmer editors uncertainty arti cial intelligence pages 
north holland new york 
heckerman heckerman 

bayesian approach learning causal net works 
proceedings eleventh conference uncertainty arti cial intelligence montreal qu pages 
morgan kaufmann 
heckerman geiger heckerman geiger 
revised november 
likelihoods priors bayesian networks 
technical report msr tr mi research redmond wa 
heckerman heckerman geiger chickering 

learn ing bayesian networks combination knowledge statistical data 
machine learning 
heckerman heckerman mamdani wellman 

real world applications bayesian networks 
communications acm 
heckerman shachter heckerman shachter 

decision theoretic foundations causal reasoning 
journal arti cial intelligence research 
skj th thiesson 

user guide 
technical report department mathematics computer science aalborg denmark 
howard howard 

decision analysis perspectives inference decision experimentation 
proceedings ieee 
howard matheson howard matheson 

uence diagrams 
howard matheson editors readings principles applications decision analysis volume ii pages 
strategic decisions group menlo park ca 
howard matheson howard matheson editors 
principles applications decision analysis 
strategic decisions group menlo park ca 
humphreys freedman humphreys freedman 

grand leap 
british journal science 
jaakkola jordan jaakkola jordan 

computing upper lower bounds likelihoods intractable networks 
proceedings twelfth con ference arti cial intelligence portland pages 
morgan kaufmann 
jensen jensen 

bayesian networks 
springer 
jensen andersen jensen andersen 

approximations bayesian belief universes knowledge systems 
technical report institute electronic systems aalborg university aalborg denmark 
jensen jensen lauritzen olesen 

bayesian updating recursive graphical models local computations 
computational quarterly 
kass raftery kass raftery 

bayes factors 
journal american statistical association 
kass kass tierney kadane 

asymptotics bayesian computation 
bernardo degroot lindley smith editors bayesian statistics pages 
oxford university press 
koopman koopman 

distributions admitting su cient statistic 
transactions american mathematical society 
korf korf 

linear space best rst search 
arti cial intelligence 
lauritzen lauritzen 

lectures contingency tables 
university aal borg press aalborg denmark 
lauritzen lauritzen 

propagation probabilities means variances mixed graphical association models 
journal american statistical association 
lauritzen spiegelhalter lauritzen spiegelhalter 

local com probabilities graphical structures application expert sys tems 
royal statistical society 
lauritzen lauritzen thiesson spiegelhalter 

diagnostic systems created model selection methods case study 
cheeseman editors ai statistics iv volume lecture notes statistics pages 
springer verlag new york 
mackay mackay 

bayesian interpolation 
neural computation 
mackay mackay 

practical bayesian framework backpropagation networks 
neural computation 
mackay mackay 

choice basis laplace approximation 
tech nical report cavendish laboratory cambridge uk 
madigan madigan raftery 

eliciting prior information enhance predictive performance bayesian graphical models 
com statistics theory methods 
madigan raftery madigan raftery 

model selection accounting model uncertainty graphical models occam window 
journal american statistical association 
madigan madigan raftery 

bayesian model averaging 
proceedings aaai workshop integrating multiple learned models portland 
madigan york madigan york 

bayesian graphical models discrete data 
international statistical review 
martin vanlehn martin vanlehn 

discrete factor analysis learning hidden variables bayesian networks 
technical report department com puter science university pittsburgh pa available bert cs pitt edu van 
meng rubin meng rubin 

em obtain asymptotic variance covariance matrices sem algorithm 
journal american statistical association 
neal neal 

probabilistic inference monte carlo meth ods 
technical report crg tr department computer science university toronto 


representing solving decision problems 
phd thesis department engineering economic systems stanford university 
pearl pearl 

fusion propagation structuring belief networks 
arti cial intelligence 
pearl pearl 

causal diagrams empirical research 
biometrika 
pearl verma pearl verma 

theory inferred causation 
allen fikes sandewall editors knowledge representation reasoning proceedings second international conference pages 
morgan kaufmann new york 
pitman pitman 

su cient statistics intrinsic accuracy 
proceedings cambridge philosophy society 
raftery raftery 

bayesian model selection social research 
marsden editor sociological methodology 
cambridge ma 
raftery raftery 

hypothesis testing model selection chapter 
chapman hall 
agogino agogino 

real time ex pert system fault tolerant supervisory control 
patton editors computers engineering pages 
american society mechanical engineers ca 
ramsey ramsey 

truth probability 
editor foundations mathematics logical essays 
humanities press london 
reprinted kyburg 
richardson richardson 

extensions undirected acyclic directed graphical models 
proceedings sixth cial intelligence statis tics ft lauderdale fl pages 
society arti cial intelligence statistics 
rissanen rissanen 

stochastic complexity discussion 
journal royal statistical society series 
robins robins 

new approach causal mortality studies sustained exposure results 
mathematical modelling 
rubin rubin 

bayesian inference causal ects role random ization 
annals statistics 
russell russell binder koller kanazawa 

local learning probabilistic networks hidden variables 
proceedings fourteenth international joint cial intelligence montreal qu pages 
morgan kaufmann san mateo ca 
saul saul jaakkola jordan 

mean eld theory sigmoid belief networks 
journal arti cial intelligence research 
savage savage 

foundations statistics 
dover new york 
schervish schervish 

theory statistics 
springer verlag 
schwarz schwarz 

estimating dimension model 
annals statistics 
sewell shah sewell shah 

social class parental encourage ment educational aspirations 
american journal sociology 
shachter shachter 

probabilistic inference uence diagrams 
op erations research 
shachter shachter andersen 

directed reduction algorithms decomposable graphs 
proceedings sixth conference uncer tainty arti cial intelligence boston ma pages 
association uncertainty arti cial intelligence mountain view ca 
shachter shachter 

gaussian uence dia grams 
management science 
silverman silverman 

density estimation statistics data analy sis 
chapman hall new york 
singh provan singh provan 
november 
cient learning selective bayesian network classi ers 
technical report ms cis computer information science department university philadelphia pa von von 

probability encoding decision analysis 
management science 
spiegelhalter spiegelhalter dawid lauritzen cowell 

bayesian analysis expert systems 
statistical science 
spiegelhalter lauritzen spiegelhalter lauritzen 

sequential updating conditional probabilities directed graphical structures 
networks 
spirtes spirtes glymour scheines 

causation predic tion search 
springer verlag new york 
spirtes meek spirtes meek 

learning bayesian networks discrete variables data 
proceedings international conference knowledge discovery data mining montreal qu 
morgan kaufmann 
suermondt cooper suermondt cooper 

combination exact algorithms inference bayesian belief networks 
international journal approximate reasoning 
thiesson thiesson 

accelerated quanti cation bayesian networks incomplete data 
proceedings international conference knowledge discovery data mining montreal qu pages 
morgan kaufmann 
thiesson thiesson 

score information recursive exponential models incomplete data 
technical report institute electronic systems aalborg university aalborg denmark 
thomas thomas spiegelhalter gilks 

bugs pro gram perform bayesian inference gibbs sampling 
bernardo berger dawid smith editors bayesian statistics pages 
oxford univer sity press 
tukey tukey 

exploratory data analysis 
addison wesley 
tversky kahneman tversky kahneman 

judgment uncertainty heuristics biases 
science 
verma pearl verma pearl 

equivalence synthesis causal models 
proceedings sixth conference uncertainty arti cial intelli gence boston ma pages 
morgan kaufmann 
whittaker whittaker 

graphical models applied multivariate statistics 
john wiley sons 
winkler winkler 

assessment prior distributions bayesian anal ysis 
american statistical association journal 

