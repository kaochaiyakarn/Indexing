structured variational distributions vibes christopher bishop microsoft research cambridge cb hn microsoft com research microsoft com variational methods increasingly popular approximate solution complex probabilistic models machine learning computer vision information retrieval fields 
unfortunately new application necessary derive specific forms variational update equations particular probabilistic model implement equations applicationspecific software 
steps time consuming error prone 
developed general purpose inference engine called vibes variational inference bayesian networks allows wide variety probabilistic models implemented solved recourse coding 
new models specified directed acyclic graph interface analogous drawing package vibes automatically generates solves variational equations 
original version vibes assumed fully factorized variational posterior distribution 
extension vibes variational posterior distribution corresponds sub graph full probabilistic model 
structured distributions produce closer approximations true posterior distribution 
illustrate approach example bayesian hidden markov models 
variational methods successfully wide range models new applications constantly explored 
ways variational framework seen complementary approach markov chain monte carlo mcmc different strengths john department physics university cambridge cam ac uk www inference phy cam ac uk weaknesses 
variational approach finds deterministic approximation posterior distribution optimization analytical family distributions 
years existed powerful tool tackling new problems mcmc called bugs bayesian inference gibbs sampling 
bugs new probabilistic model expressed directed acyclic graph encoded simple scripting notation samples drawn posterior distribution data set observed values gibbs sampling way largely automatic 
furthermore extension called provides graphical front bugs user draws pictorial representation directed graph automatically generates required script 
inspired success bugs produce analogous tool solution problems variational methods 
challenge build system handle wide range graph structures broad variety common conditional probability distributions nodes range variational approximating distributions 
achieved whilst remaining computationally efficient 
vibes software uses graphical interface analogous order specify probabilistic model terms directed acyclic graph 
subset nodes graph represent observed variables data remainder represent hidden variables 
interested primarily models exact inference intractable 
original version vibes variational posterior distribution assumed fully factorized respect nodes graph 
represents powerful practical framework approximate inference widely applied 
allowing structure variational posterior access richer class approximations obtain better approximations true posterior distributions 
describe extended version vibes uses variational distributions subgraphs discrete continuous gaussian nodes obtained deleting links original graph 
section discuss framework variational inference generality review fully factorized approximation original form vibes 
tractability vibes achieved considering conditional probability distributions form graph drawn exponential family reviewed section conjugacy imposed parent child pairs graph 
vibes software implementation described section 
section discuss general structured variational distribution derive corresponding variational update equations 
illustrated applying vibes bayesian hidden markov model section 
directions development discussed section 
variational inference section briefly review general variational framework derive variational update equations case fully factorized variational posterior distribution 
denote set variables model visible observed variables latent hidden variables 
focus models specified terms acyclic directed graph treatment undirected graphical models equally possible somewhat straightforward 
joint distribution expressed terms conditional distributions wi pai node wi denotes variable group variables associated node pai denotes set variables corresponding parents node joint distribution variables product conditionals wi pai 
goal find variational distribution approximates true posterior distribution 
note decomposition log marginal probability observed data holds choice distribution ln kl ln kl ln sums replaced integrals case continuous variables 
kl kullback lieber divergence variational approximation true posterior 
satisfies kl follows quantity forms rigorous lower bound ln 
choose family distributions represent seek member family maximizes lower bound 
allow complete flexibility see maximum lower bound occurs variational posterior distribution equals true posterior 
case kullback leibler divergence vanishes ln 
working true posterior distribution computationally intractable wouldn resorting variational methods 
consider restricted family distributions property lower bound evaluated optimized efficiently sufficiently flexible give approximation true posterior distribution 
factorized distributions original version vibes focussed distributions factorize respect disjoint groups xi variables qi xi 
powerful approximation successfully applications variational methods :10.1.1.36.2841
substituting maximize respect qi xi keeping qj fixed leads solution ln xi ln const 
denotes expectation respect distribution qk xk 
exponentials sides normalizing obtain xi exp ln xi exp ln 
note coupled equations solution qi xi depends expectations respect factors qj variational optimization proceeds initializing qi xi cycling factor turn replacing current distribution revised estimate 
original version vibes factorization form factor qi xi corresponds nodes graph 
important property variational update equations point view vibes right hand side depend conditional distributions wi pai define joint distribution functional dependence xi conditional xi pai conditional distributions children node xi parent set 
expectations performed right hand side involve variables lying markov blanket node words parents children parents illustrated 
key concept vibes allows variational update equations formulated terms local operations expressed terms generic code independent global structure graph 
central observation variational update equations node xi depend expectations variables appearing markov blanket xi set parents children parents 
conjugate exponential models noted important simplifications variational update equations occur distributions variables conditioned parameters drawn exponential family conjugate respect prior distributions parameters 
adopt somewhat different viewpoint distinction latent variables model parameters 
bayesian setting correspond unobserved stochastic variables treated equal footing 
allows consider conjugacy just variables parameters hierarchically parent child pairs graph 
consider models conditional distribution takes standard exponential family form ln xi ui xi fi xi gi 
yk vector called natural parameter distribution 
consider node zj parent xi parents cp indicated 
far pair nodes xi zj concerned think xi prior xi conditional zj xi cp contribution likelihood function 
conjugacy requires function xi product conditionals take exponential family expressed form 
conditional zj xi cp ln zj xi cp xi cp uj zj fj zj gj xi cp 
conjugacy requires expressible form ln zj xi cp zj cp ui xi zj cp 
hold parents zj fol lows ln zj xi cp multi linear function uk xk parents xk node xi 
similarly observe dependence ln zj xi cp zj linear function uj zj 
apply similar argument conjugate relationship node xj parents showing contribution conditional xi expressed terms expectations natural parameters parent node distributions 
right hand side variational update equation particular node xi multi linear function expectations uk node markov blanket xi 
variational update equation takes form defined ln xj ui xi const 
zj cp zj cp 
involves summation bottom messages children top zj cp message parents 
note messages expressed terms basis ui xi write compact generic code updating type node having take account explicitly possible combinations node types markov blanket 
example consider gaussian single variable mean precision inverse variance 
natural coordinates ux natural parameterization function fi xi simply zero case 
conjugacy allows choose distribution parent gaussian prior gamma distribution 
corresponding natural parameterizations update messages ln similarly consider multi dimensional gaussian distributions gaussian prior mean wishart prior inverse covariance matrix 
generalization gaussian rectified gaussian defined moments expressed terms erf function 
corresponds step function fi xi carried variational update equations unchanged 
similarly consider doubly truncated gaussians non zero finite interval 
example discrete distribution categorical variables 
conveniently represented coding scheme sk 
sk sk 
distribution sk 
place conjugate dirichlet distribution parameters 
allowable distributions characterize class models solved vibes factorized variational distribution 
class distributions considered context structured distributions section 
note gaussian variable gaussian parent mean extend hierarchically number levels give sub graph dag gaussian nodes arbitrary topology 
gaussian gamma wishart prior precision 
observe discrete variables sk construct pick functions choose particular parent node conjugate parents yk yk sk written sk yk non linear function yk sk furthermore expectation takes form sk yk 
variational inference tractable model provided tractable parents yk individually 
handle general architecture arbitrary dag multi nomial discrete variables having dirichlet priors arbitrary dag linear gaussian nodes having wishart priors arbitrary pick links discrete nodes gaussian nodes 
graph represents generalization gaussian mixture model includes special cases hidden markov model kalman filters factor analysers principal component analysers mixtures hierarchical mixtures 
classes models tractable scheme example poisson variables having gamma priors may limited interest 
extend class tractable models considering nodes natural parameters formed deterministic functions states parents 
key property vibes approach bugs greatly extends applicability 
suppose conditional distribution 
want deterministic function states nodes 
zm 
effect pseudo parent deterministic function nodes represented explicitly additional deterministic nodes graphical interface vibes 
tractable vibes provided expectation expressed terms expectations corresponding functions uj zj parents 
pick functions discussed earlier special case deterministic functions 
gaussian node mean formed products sums states gaussian nodes provided function linear respect nodes individually 
similarly precision gaussian comprise products sums number gamma distributed variables 
wish able evaluate lower bound confirm correctness variational updates value bound decrease monitor convergence set termination criteria 
done efficiently largely quantities calculated variational updates 
vibes software implementation creation model vibes simply involves drawing graph operations similar simple drawing package assigning properties node functional form distribution list variables conditioned location corresponding data file node observed 
menu distributions available user dynamically adjusted stage ensure valid conjugate models constructed 
adopted convention making logical deterministic nodes explicit graphical representation greatly simplifies specification interpretation model 
plate notation box surrounding nodes denote nodes replicated number times specified parameter appearing bottom right hand corner box 
model completed file files containing observed variables specified compiled involves allocation memory variables initializing distributions qi done simple heuristics user 
desired monitoring lower bound switched expense slightly increased computation set termination criterion 
alternatively variational optimization run fixed number iterations 
illustrate vibes factorized distributions bayesian model independent component analysis shown 
independent component analysis relies vibes screen shot bayesian model independent component analysis 
square boxes correspond plates usual graphical models notation indicate multiple replicated copies nodes 
dashed red links indicates links removed defining distribution fully factorized 
non gaussian latent variable distributions defined model gaussian mixture representation 
node heavy outline indicating represents observed variable node labelled represents deterministic function inner product variables model gamma distributions discrete dirichlet gaussians 
structured variational distributions fully factorized variational approximation widely great success applications represents somewhat restrictive approximation 
instance capture posterior correlations variables 
wish extend vibes allow broader class variational distributions include fully factorized distribution special case general give closer approximations true posterior distribution 
ensure richer family distributions remains computationally tractable 
particularly interested distributions corresponding sub graphs original graphical model anticipate capture important dependencies distribution 
strategy take original graphical model delete links required order achieve tractability 
see shortly particular graphical structure general tractable provided comprises discrete nodes conditional distribution node parents pick function states parents comprises gaussian nodes mean multi linear functions node parents 
fact directed graph mixed discrete gaussian nodes tractable provided links representing gaussian parent discrete child consider complex case 
shall define distribution removing links original graph connect discrete nodes connect gaussian nodes linear gaussian relationship 
note link removed replaced corresponding variational parameter 
instance consider node represents gaussian variable parent nodes representing gaussian distribution mean gamma distribution precision 
delete link doing factor corresponding variable takes form variational parameter value optimized 
link left intact 
general distribution described number connected sub graphs discrete nodes number connected sub graphs linear gaussian nodes isolated nodes representing wishart dirichlet distributions 
user may choose delete links connected subgraphs order improve speed inference expense restriction form distribution 
straightforward assist user process providing guidance expected com putational time update clique sizes 
variational inference derive generalized variational update equations corresponding complex distribution 
note distribution represented product factors disconnected component distribution graph 
denote disjoint groups nodes associated factors corresponding factors distribution 
isolated nodes analysis section holds corresponding factor distribution updated 
requires course appropriate expectations respect factors distribution evaluated 
consider update factor corresponding connected sub graph 
perform exact inference sub graph conveniently handled exploiting junction tree formalism 
take directed subgraph adding links connecting pairs parents node dropping arrows links obtain undirected graph 
cluster potentials functional forms governed corresponding conditional distributions original directed graph appropriate variational parameters corresponding deleted links 
triangulate graph find junction tree 
represents tree structured cluster graph satisfying running intersection property 
dissect terms depend give ln ln ln ln const 
constant independent variables 
write general non disjoint clusters variables 
corresponding factor distribution written normalized product cluster potentials normalization constant 
substitute pull contribution potential give ln ln ln const 
enforce normalization constraint means lagrange multiplier seek maximize 
doing note distribution composed product conditional distributions nodes original directed graph wi pai 

obtain stationarity condition ln wi pai ln ln 
solve appears terms sum 
gives final results ln wi pai ln ln const 
denotes set nodes original graph conditional distributions wi pai variables intersect cluster similarly denotes set clusters non zero intersection cluster 
arrive local update scheme albeit little complex fully factorized case 
expectation taken respect conditional distribution defined 
note order implement variational update equations need able compute appropriate expectations requires cluster potentials interpretable un normalized marginals 
achieve standard procedure 
purpose convenient maintain separator sets junction tree distinct cluster potentials essential motivation junction tree representation connected sub graphs 
note cluster potentials separator potentials need updated 
clusters updated appropriate ordering performed exact inference connected sub graph 
note inference performed exactly sub graph situations procedure described far necessarily extract full marginals 
consider graph shown 
example showing need additional moralization order find optimal variational marginals 
distribution example defined subgraph comprising markov chain top dashed links removed 
dating factor need compute expectation respect variational posterior distribution 
stands represented product marginals 
extend formalism capture correctly correlation adding link connecting nodes ensuring cluster junction tree 
achieved general nodes removing links 
illustration bayesian hmm extended original vibes software implement framework described section 
currently implemented case connected subgraph comprises purely discrete nodes 
extension linear gaussian subgraphs complex due presence multi linear interactions analogous discrete case 
implementation currently limited tree structured sub graphs moralization triangulation steps required required extensions straightforward 
illustrate extended vibes bayesian hidden markov model put prior distributions probabilities initial state hidden variables transition emission matrices 
model described solved 
order highlight comparison structured framework allowed variables unobserved 
screen shot vibes directed graph defining distribution shown 
vibes screen shot showing graphical model defining distribution bayesian hidden markov model 
links shown dashed red removed defining structured distribution shown black remain 
point comparison solve model fully factorized variational approximation 
apply structured variational approximation shown 
links hidden markov chain retained leading flexible class distributions 
converged value lower bound vibes screen shot structured variational distribution showing cluster graph junction tree structured distribution compared fully factorized distribution 
discussion early experiences vibes shown dramatically simplifies construction testing new variational models readily allows range alternative models evaluated problem 
aim vibes freely available research community year 
note encompass possible tractable substructures 
instance gaussian node having gaussian prior mean wishart prior inverse covariance precision matrix tractable substructure described normal wishart distribution 
consider distributions represented tractable graph sub graph original distribution graph 
consider possibilities 
possible extensions basic vibes described 
example order broaden range models tackled combine variational methods techniques gibbs sampling optimization empirical bayes allow non conjugate hyper priors instance 
david spiegelhalter zoubin ghahramani matthew beal useful discussions relating 
christopher bishop david spiegelhalter john 
vibes variational inference engine bayesian networks 
advances neural information processing systems 
accepted publication 
jordan ghahramani jaakkola saul 
variational methods graphical models 
jordan editor learning graphical models pages 
kluwer 
thomas best spiegelhalter 
bayesian modelling framework concepts structure extensibility 
statistics computing 
www cam ac uk bugs 
saul jordan 
exploiting tractable substructures intractable networks 
touretzky mozer hasselmo editors advances neural information processing systems volume pages 
mit press 
ghahramani jordan 
factorial hidden markov models 
machine learning 

variational approximations mean field theory junction tree algorithm 
uncertainty artificial intelligence 
morgan kauffmann 
ghahramani beal 
propagation algorithms variational bayesian learning 
leen dietterich tresp editors advances neural information processing systems volume cambridge ma 
mit press 
attias 
variational bayesian framework graphical models 
solla leen muller editors advances neural information processing systems volume pages cambridge ma 
mit press 
bishop 
variational principal components 
proceedings ninth international conference artificial neural networks icann volume pages 
iee 
mackay 
ensemble learning blind image separation deconvolution 
girolami editor advances independent component analysis 
springer verlag 
lauritzen 
graphical models 
clarendon press oxford 
cowell dawid lauritzen spiegelhalter 
probabilistic networks expert systems 
statistics engineering information science 
springer verlag 
mackay 
ensemble learning hidden markov models 
unpublished manuscript department physics university cambridge 
