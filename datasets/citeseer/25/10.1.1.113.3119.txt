despite impressive advances file system throughput resulting technologies high bandwidth networks disk arrays file system latency improved cases worse 
consequently file system remains major bottlenecks operating system performance 
investigates automated predictive approach reducing file latency 
automatic prefetching uses past file accesses predict file system requests 
objective provide data advance request data effectively masking access latencies 
designed implement system measure performance benefits automatic prefetching 
current results obtained trace driven simulation show prefetching results improvement lru especially smaller caches 
alternatively prefetching reduce cache size 
motivation rapid improvements processor memory speeds created situation particular file system major bottleneck operating system performance 
advances high bandwidth devices raid atm networks large impact file system throughput 
unfortunately access latency remains problem improve significantly due physical limitations storage devices network transfer latencies 
increasing popularity certain file system designs raid cdrom wide area distributed file systems wireless networks mobile hosts exacerbated latency problem 
example distributed file systems experience network latency combined standard disk latency 
supported part nsf number ccr reducing file system latency predictive approach james griffioen randy appleton department computer science university kentucky lexington ky distributed file systems scale numerically geographically envisioned andrew file system designers network delays dominant factor remote file system access 
similarly local file systems built technologies cd roms suffer high latencies continue increase popularity due large amount storage space offer 
variety high bandwidth technologies available existing emerging low technologies serial lines running slip ppp kb isdn slower speed networks disappear near low cost wide spread 
communication technologies suffer high latencies low bandwidths 
distributed file systems build incorporate technologies experience latencies substantially higher conventional file systems 
appeal low cost widely available shared access files certainly prolong existence file systems despite poor performance 
goal research investigate methods successfully reducing perceived latency associated file system operations 
describe new method masking file system latency called automatic prefetching 
automatic prefetching takes heuristic approach knowledge past accesses predict access user application intervention 
result applications automatically receive reduced perceived latencies better available bandwidth batched file system requests improved cache utilization 
related caching prefetching variety settings improve performance 
briefly describes related involving caching prefetching improve file system performance 
caching caching successfully systems substantially reduce amount file system 
despite success caching precisely accesses satisfied cache current bottleneck file system performance 
unfortunately increasing cache size certain point results minor performance improvements 
experience shows relative benefit caching decreases cache size cache cost increases 
exists threshold performance improvements minor prohibitively expensive 
studies show natural cache size threshold substantially larger fraction forth third total memory due part larger files big applications databases video audio 
consequently new methods needed reduce perceived latency file accesses keep cache sizes check 
machines large memories available low workstations pcs mobile laptops notebooks pdas personal data assistants limited memory capacities enjoy widespread 
cost space constraints machines support large file caches 
desire smaller portable machines combined continually increasing files size means large caches assumed complete solution latency problem 
result rapid improvements bandwidth cache service times dominated latency 
note files quite small 
fact measurements existing distributed file systems show average file kilobytes long 
files size transmission rate little concern compared access latency wan slow device 
result access latency bandwidth dominate cost files cache 
distributed file systems open close functions represent synchronization points shared files 
file may reside client cache open close call executed server consistency reasons 
latency calls quite large tends dominate costs file file cache 
short benefits standard caching realized 
improve file system performance keep file cache sizes check caching need supplemented new methods algorithms 
prefetching concept prefetching variety environments including microprocessor designs virtual memory paging databases file read ahead 
long term prefetching file systems support disconnected operation 
prefetching improve parallel file access mimd architectures 
relatively straight forward method prefetching application inform operating system requirements 
approach proposed patterson 
approach application program informs operating system file requirements operating system attempts optimize accesses 
basic idea application knows files needed needed 
application directed prefetching certainly step right direction 
drawbacks approach 
approach applications rewritten inform operating system file requirements 
programmer learn reasonably complex set additional system directives strategically deployed program 
implies application writer thorough understanding application file access patterns 
ironically key goal languages particular objectoriented languages abstraction encapsulation hiding implementation details programmer 
details visible experience indicates complexity software systems creates situation experts may difficulty grasping complete picture file access patterns 
incorrectly placed directives incomplete set directives degrade performance improve 
second problem operating system needs significant lead time insure file available needed 
order benefit prefetching application significant amount computation time file predicted time file accessed 
applications know files need actual need arises 
instance preprocessor compiler know pattern nested include files files encountered input stream editor necessarily know files user normally edits 
approach attempts solve problem predicting need file advance application cases long application begins execute 
third problem application driven prefetching arises situations related file accesses span multiple executables 
typically applications written independently know file access patterns application 
situations series applications execute repeatedly edit compile run cycle certain commonly run shell scripts application knows cross application file access patterns inform operating system application file requirements 
cases batch type utilities unix facility instrumented understand cross application access patterns 
case complete view real cross application pattern unknown user requires extreme expertise determine pattern 
approach uses long term history information support prefetching application boundaries 
automatic prefetching investigating approach call automatic prefetching operating system application predicts file requirements 
basic idea hypothesis underlying automatic prefetching file activity successfully predicted past file activity 
knowledge improve file system performance 
automatic prefetching advantages existing approaches 
existing applications need rewritten modified new applications need incorporate non portable prefetching operations 
result applications receive benefits automatic prefetching including existing software 
second operating system automatically performs prefetching application behalf application writers concentrate solving problem hand worrying optimizing file system performance 
third operating system monitors file access application boundaries detect access patterns span multiple applications executed repeatedly 
consequently operating system prefetch files substantially earlier file needed application begins execute 
automatic prefetching allows operating system effectively overlap processing file transfers 
operating system past access information batch multiple file requests better available bandwidth 
past access formation improve cache management algorithm effectively reducing cache misses prefetching occurs 
goal research determine approach viable 
second goal develop effective prefetch policies quantify benefits automatic prefetching 
sections consider objectives describe results 
analysis existing systems determine viability automatic prefetching analyzed current file system usage patterns 
researchers gathered file system traces decided modify sunos kernel order gather traces extract specific information important research 
addition recording file system calls system kernel gathers precise information regarding issuing process timing operation 
timing information serves indicator system performance provides information prefetching substantial effects performance 
gathered variety traces normal daily usage researchers various synthetic workloads 
traces collected single sun sparcstation supporting users executing variety tasks 
traces collected varying time periods longest traces spanning days containing operations 
users restricted way 
typical daily usage included users processing email editing compiling preparing documents executing task typical academic environment 
particular set traces contains database activity 
data collected appears line studies similar workloads 
initial analysis trace data indicates typical file system usage realize substantial performance improvements prefetching provides guidelines successful prefetching policy 
data shows relatively little time moment file opened moment read occurs see 
fact median time traces milliseconds 
consequently prefetching occur significantly earlier open operation achieve significant performance improvement 
prefetching open time provide minor improvements 
second data shows average amount time successive opens substantial percent opens time ms histogram times open read file 
ms 
operating system accurately predict file accessed exists sufficient amount time prefetch file 
multi user multiprogramming environment concurrently executing tasks may generate interleaved stream file requests 
environment reliable access patterns may difficult obtain 
patterns discernable randomness concurrency may render prefetching effort ineffective 
analysis trace data consisting multiple users various daemons shows multiprogramming environment accesses tend sequential define sequential sensible predictable uninterrupted progression file accesses associated task 
fact measurements show accesses follow logically previous access 
multiprogramming little effect ability predict file referenced 
probability graph designed implemented simple analyzer attempts predict accesses past access patterns 
driven trace data analyzer dynamically creates logical graph called probability graph 
node graph represents file file system 
describing de fine lookahead period construct graph 
lookahead period defines means file opened soon file 
analyzer defines lookahead period fixed number file open operations occur current open 
file opened period open considered occurred soon current open 
physical time measure virtual time measure measure easily obtained argued better definition soon unknown execution times file access patterns applications 
results show measure works practice 
say files related files opened lookahead period 
example lookahead period file opened file considered related current file 
lookahead period file opened files current file considered related current file 
analyzer allocates node probability graph file interest file system 
unix exec system calls treated opens included probability graph 
graph derived trace described section generated approximately nodes accessed day period 
node consumes bytes efficiently stored disk inode associated file active portions cached better performance 
current graph storage scheme optimized wasteful 
begun investigating methods substantially reduce graph size graph pruning aging compression 
arcs probability graph represent related accesses 
open file follows lookahead period open second file directed arc drawn second 
larger lookaheads produce arcs 
analyzer weighs arc number times second file accessed file 
graph represents ordered list files demanded file system arc represents probability particular file opened soon file 
illustrates structure example probability graph 
probability graph provides config tm alloca nodes example 
formation necessary intelligent prefetch decisions 
define chance prediction correct probability file say file opened fact file file opened 
chance file file obtained probability graph ratio number arcs file file divided total number arcs leaving file say prediction reasonable estimated chance prediction tunable parameter minimum chance 
say prediction correct file predicted opened lookahead period 
establishing minimum chance requirement crucial avoid wasting system resources 
absence minimum requirement analyzer produce predictions file open consuming network cache resources prediction incorrect 
measure success analyzer define accuracy value 
accuracy set predictions number correct predictions divided total number predictions 
accuracy large minimum chance practice substantially higher 
number predictions open call varies required accuracy predictions 
requiring accurate predictions predictions wrong means limited number predictions 
set trace data relatively low minimum chance value predictor averaged files predicted open 
higher minimum chance values predictor averaged files predicted open 
relatively low minimum chance predictor able prediction time correct approximately predictions 
shows distribution estimated chance values lookahead 
large number predictions estimated chance 
setting minimum chance places system danger prefetching files 
setting minimum chance files prefetched missed 
distribution shows low minimum chance result high average accuracy 
simulation system evaluate performance systems automatic prefetching implemented simulator models file system 
order simulate variety file system architectures having variety performance characteristics simulator highly parameterized adjusted model file system designs 
flexibility allows measure compare performance various cache management policies mechanisms wide variety file system conditions 
simulator consists basic components driver cache manager disk subsystem predictor 
driver reads timestamped file system trace translates file access file system request simulator process 
driver generates file requests directly trace data workload exactly typical concurrent user level applications 
driver modify set requests special cases 
simulator interested file system activity driver removes accesses files representing devices terminals dev null 
percent arcs estimated chance percent histogram estimated chances lookahead 
certain standard shared libraries library eliminated 
accesses mmap calls libraries rarely require file system activity typically virtual memory cache 
cache manager manages simulated file cache services requests possible cache invoking disk subsystem 
implemented cache managers 
standard lru cache manager disk pages replaced order 
second cache manager prefetch cache manager 
prefetch cache manager operates lru manager updating timestamps access replacing page 
prefetch manager updates timestamps knowledge expected accesses predictor soon accessed pages replacement 
prefetch cache management improve performance prefetching occurs pages brought ahead time 
run prefetch mode simulator shows performance improvement comes pages prefetched 
task disk subsystem simulate file storage device 
current disk subsystem configured emulate local disks 
local disk relatively low latency compared target file systems wide area distributed file systems raids wireless networks 
consequently expect performance improvements realized local disk model amplified target environments 
assumed disk model access latency ms transfer rate mb sec factoring typical file system overhead 
simulator contains predictor 
predictor observes open requests arrive driver records data probability graph described earlier 
predictor builds probability graph dynamically just done real system 
longer simulator executes 
access simulator gains clearer understanding true access patterns 
open probability graph examined prefetch opportunities 
opportunity discovered read request sent cache manager 
cache contains appropriate data data access time set current time 
ensures data anticipated need possibly data impending flush cache 
prefetch request satisfied cache prefetched disk subject characteristics disk subsystem 
notice current disk subsystem reordering requests 
particular preempt defer prefetch requests satisfy subsequent appli percent prefetches time ms histogram times prefetch read access 
cation requests 
reordering prioritizing requests represents area potential performance improvements 
currently process implementing automatic prefetching system inside unix kernel running nfs measure performance actual system 
experimental results performed tests measure performance improvements achieved automatic prefetching 
particular set tests described trace taken day period containing unrestricted activity multiple users 
determine performance benefits prefetching ran simulations varying cache size lookahead value minimum chance measured lru performance case comparison purposes 
recall section time open file read small prefetching effective 
shows simulator able predict prefetching files sufficiently far advance read file 
measurements indicate files predicted subsequently access prefetched ms actual need resulting cache hits time read 
prefetch parameters effect performance parameters significantly affect predictions predictor lookahead minimum chance values 
recall lookahead represents close file opens need files considered related 
setting value large increases number files considered related file open may potentially cause files prefetched 
large lookaheads increase number files prefetched predictions response open request 
large lookaheads result files prefetched substantially earlier predictions advance 
result large lookaheads inappropriate smaller cache sizes perform larger caches case small caches large lookaheads tend prefetch files far advance need 
result data necessary current computation may forced cache replaced terms small large relative measures cache size meaning small large depend workload 
small cache cache misses large cache misses 
workload trace caches megabyte considered small caches considered large 
traces produce different values 
rate percent lookahead cache misses function lookahead cache 
performance varies depending lookahead settings 
rate percent lookahead cache misses function lookahead cache 
performance varies depending lookahead settings 
cache rates lookahead table data points corresponding 
cache rates lookahead useless data needed far 
larger cache sizes cache may sufficient space load file data required disturbing file data required current computation 
minimum estimated probability file needed near 
larger cache sizes smaller values perform better 
setting low results aggressive prefetching 
cache large incorrect prefetches minimal affect performance 
somewhat surprisingly aggressively low value benefits small caches 
hit rate low small caches correct predictions result large performance benefits 
low minimum chance increases total number correct predictions 
moderate cache sizes optimal function specific cache size limit number missed prefetch opportunities prefetching unnecessary files 
summary low aggressive large small caches higher intermediate size caches 
lookahead increase increasing cache size 
figures associated tables tables illustrate tradeoffs table data points corresponding 
kb cache kb cache respectively 
clearly lookahead parameters highly cache size adjusted accordance cache size 
multiple settings particular cache size may result approximately equal ratios 
case factors network congestion processing overhead aid selection appropriate parameter settings 
performance compared lru primary goal automatic prefetching bring necessary file data cache needed 
automatic prefetching successful expect number cache misses number cache misses experienced standard lru cache management 
shows number page misses file system incurred lru prefetching various cache sizes 
parameters prefetching performs better lru cache sizes cases outperforming lru 
note cache sizes shown prefetching provided better performance rate percent prefetch lru cache size kb lru cache half size 
particularly important machines large amounts memory available file caching 
large memory machines ability achieve similar performance smaller cache sizes results memory applications 
indicates number correctly prefetched pages offsets pages incorrectly forced cache prefetching small cache sizes 
particular trace lru prefetching realize relatively little improvement ratios caches larger mb lru performance begins approach prefetch performance cache size increases simulations cache sizes mb show prefetching results reduction number misses compared lru 
results show reasonable predictions past file activity 
result automatic prefetching substantially reduce latency better available bandwidth batched prefetch requests improve cache utilization 
wide area distributed file systems cdrom raid traces reported particular trace consisted unrestricted real user usage 
traces trace contained heavy users achieve reasonable rates mb cache 
cache misses function cache size 
high latency high bandwidth systems prevalent prefetching increasingly important mechanism high performance reviewers helpful comments suggestions 
mary baker reviewing early draft providing valuable feedback 
dcs users submitting traced 
baker ousterhout seltzer 
non volatile memory fast reliable file systems 
proceedings th international conference architectural support programming languages operating systems pages october 
mary baker john hartman michael kupfer ken shirriff john ousterhout 
measurements distributed file system 
proceedings th acm symposium operating systems principles pages 
association computing machinery sigops october 
james griffioen randy appleton 
automatic prefetching wan 
proceedings ieee workshop advances parallel distributed systems pages oct 
kotz ellis 
prefetching file systems mimd multiprocessors 
ieee transactions parallel distributed systems 
geoff kuenning gerald popek peter reiher 
analysis trace data predictive file caching mobile computing 
proceedings summer usenix conference june 
samuel leffler marshal mc michael karels john quarterman 
design implementation bsd unix operating system 
addison wesley 
morris satyanarayanan conner howard rosenthal smith 
andrew distributed personal computing environment 
cacm march 
nelson welch ousterhout 
caching sprite network file system 
acm transactions computer systems february 
ousterhout da costa harrison kunze kupfer thompson 
trace driven analysis unix bsd file system 
proceedings th symposium operating systems principles pages december 
john ousterhout 
aren operating systems getting faster fast hardware 
proceedings summer usenix conference pages june 
patterson gibson satyanarayanan 
status report research transparent informed prefetching 
sigops operating systems review april 
presotto pike thompson trickey 
plan distributed system 
proceedings spring conf pages may 
sandberg goldberg kleiman dan walsh bob lyon 
design implementation sun network file system 
proceedings summer usenix conference pages 
usenix association june 
satyanarayanan 
coda highly available file system distributed workstation environment 
ieee trans 
computers april 
peter gail kaiser 
disconnected operation multi user software development environment 
proceedings ieee workshop advances parallel distributed systems pages october 
smith 
cache memories 
computing surveys september 
van renesse tanenbaum wilschut 
design high performance file server 
proceedings ieee th international conference distributed computing systems 
author information james griffioen assistant professor computer science department university kentucky 
received computer science calvin college ph computer science purdue university respectively 
recipient usenix scholarship 
research interests include high performance distributed file systems scalable distributed shared memory systems high speed network protocols 
email address dcs edu 
randy appleton ph student computer science department university kentucky 
received degree university illinois university kentucky 
research interests distributed file systems operating systems databases 
email address dcs edu 
