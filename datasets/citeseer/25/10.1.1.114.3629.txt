unsupervised models named entity classification michael collins yoram singer labs research park avenue florham park nj research att com discusses unlabeled examples problem named entity classification 
large number rules needed coverage domain suggesting fairly large number labeled examples required train classifier 
show unlabeled data reduce requirements supervision just simple seed rules 
approach gains leverage natural redundancy data named entity instances spelling name context appears sufficient determine type 
algorithms 
method uses similar algorithm yarowsky modifications motivated blum mitchell 
second algorithm extends ideas boosting algorithms designed supervised learning tasks framework suggested blum mitchell 
statistical machine learning approaches natural language problems require relatively large amount supervision form labeled training examples 
results yarowsky brill blum mitchell suggested unlabeled data quite profitably reducing need supervision 
discusses unlabeled examples problem named entity classification 
task learn function input string proper name type assume categories person organization 
example classifier identify frank person johnson location 
approach uses spelling contextual rules 
spelling rule simple look string rule location rule looks words string rule string containing person 
contextual rule considers words surrounding string sentence appears rule proper name modified appositive head president person 
task considered component muc muc named entity task task segmentation pulling possible people places locations text sending classifier 
supervised methods applied quite successfully full muc named entity task bikel 
glance problem quite complex large number rules needed cover domain suggesting large number labeled examples required train accurate classifier 
show unlabeled data drastically reduce need supervision 
unlabeled examples methods described classify names accuracy 
supervision form seed rules new york california locations name containing person name containing incorporated organization microsoft organizations 
key methods describe redundancy unlabeled data 
cases inspection spelling context sufficient classify example 
example says cooper vice president 
spelling feature string contains contextual feature president modifies string strong indications cooper type person 
example labeled interpreted hint president imply category 
unlabeled data gives hints features predict label hints turn surprisingly useful building classifier 
algorithms 
method builds results yarowsky blum mitchell 
yarowsky describes algorithm word sense disambiguation exploits redundancy contextual features gives impressive performance 
unfortunately yarowsky method understood theoretical viewpoint formalize notion redundancy unlabeled data set learning task optimization appropriate objective function 
blum mitchell offer promising formulation redundancy prove results unlabeled examples help classification suggest objective function training unlabeled examples 
algorithm similar yarowsky important modifications motivated blum mitchell 
algorithm viewed heuristically optimizing objective function suggested blum mitchell empirically shown quite successful optimizing criterion 
second algorithm builds boosting algorithm called adaboost freund schapire schapire singer 
adaboost algorithm developed supervised learning 
adaboost finds weighted combination simple weak classifiers weights chosen minimize function bounds classification error set training examples 
roughly speaking new algorithm performs similar search minimizes bound number unlabeled examples classifiers disagree 
algorithm builds classifiers iteratively iteration involves minimization continuously differential function bounds number examples classifiers disagree 
additional related additional inducing lexicons knowledge sources large corpora 
brin describes system extracting author book title pairs world wide web approach bootstraps initial seed set examples 
charniak describe method extracting parts objects wholes car large corpus hand crafted patterns 
hearst describes method extracting hyponyms corpus pairs words isa relations 
riloff shepherd describe bootstrapping approach acquiring nouns particular categories vehicle weapon cate gories 
approach builds initial seed set category quite similar decision list approach described yarowsky 
riloff jones describe method term mutual bootstrapping simultaneously constructing lexicon contextual extraction patterns 
method shares characteristics decision list algorithm 
riloff jones brought attention preparing final version 
problem data sentences new york times text parsed parser collins 
word sequences met criteria extracted named entity examples word sequence sequence consecutive proper nouns words tagged nnp noun phrase word head noun phrase 
np containing word sequence appeared contexts 
appositive modifier np head singular noun tagged nn 
example take says cooper vice president case cooper extracted 
sequence proper nouns np word cooper head np np appositive modifier vice president head singular noun president 

np complement preposition head pp 
pp modifies np head singular noun 
example fraud related federally funded plant georgia case georgia extracted np containing complement preposition pp headed modifies np federally funded plant head singular noun plant 
addition named entity string cooper georgia contextual predictor extracted 
appositive case contextual chelba running parser providing data 
predictor head modifying appositive president cooper example second case contextual predictor preposition noun modifies plant georgia example 
refer named entity string spelling entity contextual predicate context 
feature extraction having spelling context pairs parsed data number features extracted 
features represent example learning algorithm 
principle feature arbitrary predicate spelling context pair reasons clear features limited querying spelling context 
features full string full string cooper full string cooper 
contains spelling contains word feature applies words string contains cooper contributes features contains contains cooper 
feature appears spelling single word capitals ibm contribute feature 
feature appears spelling single word capitals full periods contains period 
contribute feature ibm 
appears spelling contains characters upper lower case letters 
case string formed removing upper lower case letters spelling thomas 
context context entity 
cooper georgia examples contribute context president context plant respectively 
context type context type appositive case context type prep pp case 
table gives examples entities features 
unsupervised algorithms decision lists supervised decision list learning unsupervised algorithm describe decision list method yarowsky 
describing unsupervised case describe supervised version algorithm input learning algorithm labeled examples form xi yi 
yi label ith example possible labels yi member kg 
xi set mi features fxi xi xim associated ith example 
xij member set possible features 
output learning algorithm function 
estimate conditional probability yjx seeing label feature 
alternatively thought defining decision list rules ranked strength 
label test example features defined arg max define function counts seen training data count count count number times feature seen label training data count count 
smoothing parameter number possible labels 
labels person organization location set 
equation estimate conditional probability label feature yjx 
unsupervised algorithm introduce new algorithm learning unlabeled examples call dl dl stands decision list term taken blum mitchell 
yarowsky describes sophisticated smoothing methods 
clear apply methods unsupervised case required cross validation techniques reason simpler smoothing method shown 
sentence entities spelling context features robert jordan partner johnson took 
hiring 
hanson acquired incorporated parent credit 
robert jordan partner full string robert jordan contains robert contains jordan context partner context type johnson partner full string johnson contains contains contains johnson context partner context type prep full string 
context context type prep incorporated parent full string incorporated contains contains incorporated context parent context type credit parent full string credit contains contains credit context parent context type prep table example named entities features 
input unsupervised algorithm initial seed set rules 
named entity domain rules full string new york location full string california location full string location contains person contains incorporated organization full string microsoft organization full string organization rules strength 
algorithm induce new rules 
set maximum number rules type induced iteration 

initialization set spelling decision list equal set seed rules 

label training set current set spelling rules 
examples rule applies left unlabeled 

labeled examples induce decision list contextual rules method described section 
count number times feature seen known label training data 
label person organization location take contextual rules highest value count unsmoothed strength threshold pmin 
fewer rules precision greater pmin note top frequent rules method robust low count events smoothing allowing low count high precision features chosen iterations 
keep rules exceed precision threshold 
pmin fixed experiments 
iteration method induces rules number possible labels experiments 

label training set current set contextual rules 
examples rule applies left unlabeled 

new labeled set select spelling rules method step 
set spelling rules seed set plus rules selected 

set return step 
label training data combined spelling contextual decision list induce final decision list labeled examples rules regardless strength added decision list 
algorithm yarowsky compare algorithm yarowsky 
core yarowsky algorithm follows 
initialization set decision list equal set seed rules 

label training set current set rules 

labels learn decision list defined formula equation counts restricted training data examples labeled step 
set decision list include rules smoothed strength threshold pmin 

return step 
differences method dl algorithm dl algorithm cautious imposing gradually increasing limit number rules added iteration 
dl algorithm separated spelling contextual features alternating labeling learning types features 
explicit assumption redundancy features spelling context sufficient build classifier built algorithm 
measure contribution modification third intermediate algorithm yarowsky cautious tested 
yarowsky cautious separate spelling contextual features limit number rules added stage 
specifically limit starts increases iteration 
modification relatively minor change 
motivated observation yarowsky algorithm added large number rules iterations 
highest frequency rules safer tend accurate 
intuition born experimental results 
second modification important discussed section 
justification separation contextual spelling features important reason separating types features opens possibility theoretical analysis unlabeled examples 
blum mitchell describe learning situation example represented feature vector drawn set possible values instance space task learn classification function set possible labels 
features separated types correspond different views example 
named entity task instance space spelling features instance space contextual features 
assumption element represented 
view example sufficient classification 
exist functions example 
see example training test data 
method fairly strong assumption features partitioned types type sufficient classification 
correlated tightly 
example deterministic function 
assume pairs drawn pairs labels yi pairs unlabeled 
fully supervised setting task learn function yi 
cotraining case blum mitchell argue task induce functions 
yi 
correctly classify labeled examples agree unlabeled examples 
key point second constraint remarkably powerful reducing complexity learning problem 
blum mitchell give example illustrates just powerful second constraint 
consider case jx jx medium sized number feasible collect unlabeled examples 
assume classifiers rote learners defined look tables list label member 
problem binary classification problem 
problem represented graph vertices corresponding members 
unlabeled pair represented edge nodes corresponding graph 
edge indicates features label 
sufficient number randomly drawn unlabeled examples edges induce completely connected components span entire graph 
vertex connected component label binary classification case need single labeled example identify component get label 
blum mitchell go give pac results learning cotraining case 
describe application cotraining classifying web pages feature sets words page pages pointing page 
method halves error rate comparison method labeled examples 
limitations blum mitchell assumptions blum mitchell useful developing theoretical results intuition problem assumptions quite limited 
particular may possible learn functions noise data just realistic expect learn perfect classifiers features representation 
may realistic replace second criteria softer example blum mitchell suggest alternative 
yi 
choice minimize number examples 
alternatively probabilistic learners sense encode second constraint minimizing measure distance distributions learners 
question soft function pick design algorithms optimize open question appears promising way looking problem 
dl algorithm motivated greedy method satisfying constraints 
iteration algorithm increases number rules maintaining high level agreement spelling contextual decision lists 
inspection data shows classifiers give labels unlabeled examples give label cases 
success algorithm may due success maximizing number unlabeled examples decision lists agree 
section alternative approach builds classifiers attempting satisfy constraints possible 
algorithm called advantage general decision list learning input xm ym xi yi initialize get weak hypothesis ht training weak learner distribution dt 
choose update dt dt ty ih xi zt zt pm dt xi output final hypothesis sign pt tht adaboost algorithm binary problems schapire singer 
gorithm fact combined supervised machine learning algorithm 
boosting algorithm section describes algorithm boosting algorithms previously developed supervised machine learning problems 
give brief overview boosting algorithms 
discuss adapt generalize boosting algorithm adaboost problem named entity classification 
new algorithm call uses labeled unlabeled data builds classifiers parallel 
note previous boosting algorithms algorithm boosting algorithm valiant valiant probably approximately correct pac model 
adaboost algorithm section describes adaboost basis algorithm 
adaboost introduced freund schapire schapire singer gave generalization adaboost 
description application adaboost various nlp problems see abney schapire singer volume 
input adaboost set training examples xm ym xi set features constituting ith example 
moment assume possible labels yi 
adaboost access weak learning algorithm accepts input training examples distribution instances 
distribution specifies relative weight importance example typically weak learner attempt minimize weighted error training set distribution specifies weights 
weak learner class problems computes weak hypothesis input space reals sign interpreted predicted label magnitude jh confidence prediction large numbers jh indicate high confidence prediction numbers close zero indicate low confidence 
weak hypothesis abstain predicting label instance setting 
final strong hypothesis denoted sign weighted sum weak hypotheses sign pt tht weights determined run algorithm describe 
pseudo code describing generalized boosting algorithm schapire singer 
note zt normalization constant ensures distribution dt sums function weak hypothesis ht weight hypothesis chosen tth round 
normalization factor plays important role adaboost algorithm 
schapire singer show training error bounded mx exp yi tx tht xi zt order greedily minimize upper bound training error iteration search weak hypothesis ht weight minimize zt 
implementation simplest choice weak hypothesis 
ht function predicts label examples containing particular feature xt abstaining examples ht xt xt prediction strong hypothesis written sign define sign 
tht briefly describe choose ht iteration 
derivation slightly different schapire singer restrict positive 
zt written follows zt xi xi dt dt exp yi tht xi ht xi ht xi yi ht xi yi derivation schapire singer providing equ 
minimized setting ln feature may examples practice small leading extreme confidence values 
prevent smooth confidence adding small value giving ln plugging value equ 
ht equ 
gives zt order minimize zt iteration final algorithm choose weak hypothesis feature xt values minimize equ 

algorithm describe algorithm named entity problem 
convention earlier sections assume example instance pair xj xj 
problem example spelling context pair 
pairs labels yi pairs unlabeled 
assumption example sufficient determine label yi 
learning task find classifiers yi examples possible examples achieve goal extend auxiliary function bounds training error see equ 
defined unlabeled labeled instances 
denote gj strong hypothesis fj sign gj 
define function def mx mx exp exp nx nx exp exp small follows classifiers low error rate labeled examples give label large number unlabeled instances 
see note terms equation correspond function adaboost attempts minimize standard supervised setting equ 
term classifier 
new terms force classifiers agree possible unlabeled examples 
put way minimum equ 
sign xi sign xi jgj xi sign gj xi yi fact provides bound sum classification error labeled examples number disagreements classifiers unlabeled examples 
formally number classification errors second learner training data number unlabeled examples classifiers disagree 
verified derive algorithm means minimizing 
algorithm builds classifiers parallel labeled unlabeled data 
boosting algorithm works rounds 
round composed stages stage updates classifiers keeping classifier fixed 
denote classifiers rounds assume turn classifier updated second kept fixed 
define pseudo labels yi follows yi yi sign labels simply copied labeled examples remaining examples taken current output second classifier 
add new weak hypothesis feature confidence value chosen minimize function nx exp yi xi define virtual distribution exp normalization constant 
equ 
rewritten nx exp yi form function zt adaboost 
virtual distribution pseudo labels yi values calculated possible weak hypothesis feature weak hypothesis minimal value chosen weight weak hypothesis ln calculated 
procedure repeated rounds alternating classifiers 
pseudo code describing algorithm fig 

algorithm described divides function parts step searches feature weight minimize constant factor affect mini mization equ 
input initialize xi 
set pseudo labels yi yi sign set virtual distribution exp xj exp xj 
get weak hypothesis train ing weak learner distribution choose update xj xj th xj output final hypothesis sign xj algorithm 
practice greedy approach results decrease value 
note situations fact increases 
implementation issue deserves elaboration 
note formalism abstain 
fact rounds predictions zero 
corresponding pseudo labels instances gj abstain set zero instances contribute objective function 
learner free pick labels instances 
allow learners bootstrap filling labels instances side far 
algorithm just described case labels named entity task labels general useful generalize algorithm multiclass case 
extensions adaboost multiclass problems suggested freund schapire schapire singer 
extended adaboost mh schapire singer algorithm cotraining case 
ad mh maintains distribution instances labels addition weak hypothesis outputs confidence vector confidence value possible label 
adopt approach alternate classifiers classifier modified remains fixed 
pseudo labels formed seed labels labeled examples output fixed classifier unlabeled examples 
adaboost mh applied problem place supervised examples 
experiments couple additional modifications algorithm 
algorithm fig 
extended additional innermost loop possible labels 
weak hypothesis chosen restricted predictor favor label 
iteration algorithm forced pick features location person organization turn classifier trained 
modification brings method closer dl algorithm described earlier motivated intuition labels kept populated unlabeled examples preventing label dominating deserves theoretical investigation 
removed context type feature type approach 
default feature type coverage seen example low baseline precision 
feature type included chose default feature early iteration giving non abstaining pseudo labels examples eventual convergence classifiers agreeing assigning label examples 
deserves investigation 
note possible devise similar algorithms objective functions equ 
likelihood function maximum entropy problems generalized additive models lafferty 
currently exploring algorithms 
em approach expectation maximization em algorithm dempster laird rubin common approach unsupervised training section describe application named entity problem 
generative model applied similar naive bayes labels hidden vari ables unlabeled examples observed variables seed labeled examples 
model parameterized joint probability label feature set pair yi xi written yi xi yi xi xim yi mi mi model assumes pairs generated underlying process label chosen prior probability yi number features mi chosen probability mi features independently generated probabilities 
assume training set examples fx xng examples labels fy examples unlabeled 
purposes em observed data xm ym xm xng hidden data 
likelihood observed data model yi xi ny kx xi yi xi defined 
training model involves estimation parameter values xjy 
maximum likelihood estimates parameter values maximize analytically em algorithm hill climb local maximum likelihood function initial parameter settings 
experiments set parameter values randomly ran em convergence 
parameter estimates label test example defined arg max kg note model equation deficient assigns greater zero probability feature combinations impossible 
example independence assumptions mean model fails capture dependence specific general features example fact feature full string new york seen features contains new learning algorithm accuracy accuracy clean noise baseline em yarowsky yarowsky cautious dl table accuracy different learning methods 
baseline method tags entities frequent class type organization 
contains york seen feature contains group 
unfortunately modifying model account kind dependencies straightforward 
evaluation spelling context pairs extracted training data 
picked random labeled hand produce test set 
chose labels example location person organization noise noise category items outside categories 
numbers falling location person organization categories respectively 
examples fell noise category 
cases temporal expressions day week month year 
excluded evaluation easily identified list days months 
left examples noise 
nc number examples algorithm classified correctly gold standard items labeled noise counted incorrect calculated measures accuracy accuracy noise nc accuracy clean nc see tab 
accuracy different methods 
note examples test set altogether cases labeled test example baseline organization label 
fig 
shows learning curves 
number rounds accuracy test coverage train agreements train learning curves 
graph gives accuracy test set coverage proportion examples classifiers give label abstaining proportion examples classifiers agree 
iteration examples assigned labels classifiers high level agreement maintained 
test accuracy asymptotes 
unlabeled examples named entity classification problem reduce need supervision handful seed rules 
addition heuristic decision list learning boosting framework builds ideas blum mitchell 
method uses soft measure agreement classifiers objective function described algorithm directly optimizes function 
currently exploring methods employ similar ideas formal properties 
extend approach build complete named entity extractor method pulls proper names text classifies 
contextual rules restricted may applicable example spelling rules generally applicable coverage 
problem noise items fall categories needs addressed 
charniak 

finding parts large corpora 
proceedings th annual meeting association computational linguistics acl 
bikel miller schwartz weischedel 

nymble high performance learning name finder 
proceedings fifth conference applied natural language processing pages 
blum mitchell 

combining labeled unlabeled data training 
proceedings th annual conference computational learning theory colt 
brill 

unsupervised learning disambiguation rules part speech tagging 
proceedings third workshop large corpora 
brin 

extracting patterns relations world wide web 
webdb edbt 
collins 

new statistical parser bigram lexical dependencies 
proceedingsof th annual meeting association computational linguistics pages 
dempster laird rubin 
maximum likelihood incomplete data em algorithm journal royal statistical society ser 
freund 
boosting weak learning algorithm majority 
information computation 
freund schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences 
hearst 

automatic acquisition hyponyms large text corpora 
proceedings fourteenth international conference computational linguistics 
michael kearns 
thoughts hypothesis boosting 
unpublished manuscript december 
lafferty 
additive models boosting inference generalized divergences 
proceedings twelfth annual conference computational learning theory 
proceedings sixth message understanding conference muc 
morgan kaufmann san mateo ca 
riloff shepherd 

corpus approach building semantic lexicons 
proceedings second conference empirical methods natural language processing emnlp 
riloff jones 

learning dictionaries information extraction multi level bootstrapping 
proceedings sixteenth national conference artificial intelligence aaai 
schapire 
strength weak learnability 
machine learning 
schapire singer 
improved boosting algorithms confidence rated predictions 
proceedings eleventh annual conference computational learning theory pages 
appear machine learning 
valiant 
theory learnable 
communications acm november 
yarowsky 

unsupervised word sense disambiguation rivaling supervised methods proceedings rd annual meeting association computational linguistics 
cambridge ma pp 

