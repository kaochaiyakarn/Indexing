generalizing plans new environments relational mdps carlos guestrin daphne koller chris neal computer science department stanford university guestrin koller cmg cs stanford edu longstanding goal planning research ability generalize plans developed set environments new similar environment minimal replanning 
generalization reduce planning time allow tackle larger domains ones tractable direct planning 
approach generalization problem new framework relational markov decision processes 
rmdp model set similar environments representing objects instances different classes 
order generalize plans multiple environments define approximate value function specified terms classes objects multiagent setting classes agents 
class approximate value function optimized relative sampled subset environments computed efficient linear programming method 
prove polynomial number sampled environments suffices achieve performance close performance achievable optimizing entire space 
experimental results show method generalizes plans successfully new significantly larger environments minimal loss performance relative environment specific planning 
demonstrate approach real strategic computer war game 
planning methods optimize plan agent fixed environment 
real world settings agent face multiple environments lifetime experience environment help perform minimal replanning 
consider example agent designed play strategic computer war game game shown fig 
open source version popular game 
game agent faced scenarios 
scenario control set agents units different skills order defeat opponent 
scenarios share basic elements resources gold wood units peasants collect resources build structures footmen fight enemy units structures train footmen 
scenario composed basic building blocks differ terms map layout types units available amounts resources agent learn experience playing scenarios enabling tackle new scenarios significant amounts replanning 
particular agent generalize simple scenarios allowing deal scenarios complex effective planner 
idea generalization longstanding goal markov decision process mdp reinforcement learning strategic domain peasants castle forest gold mine footmen enemy executing generalized policy computed algorithm 
research earlier traditional planning :10.1.1.32.7692
problem challenging unclear translate solution obtained domain 
mdp solutions assign values actions states 
different mdps scenarios typically quite different different set number states actions 
cases mapping solution defined 
approach insight domains described terms objects relations 
particular domain involve multiple objects classes 
different tasks domain typically involve different sets objects related different ways 
example different tasks involve different numbers peasants footmen enemies define notion relational mdp rmdp probabilistic relational model prm framework 
rmdp particular domain provides general schema entire suite environments worlds domain 
specifies set classes dynamics rewards object class depend state object related objects 
class structure rmdp define value function generalized domain 
assumption value function approximated sum value subfunctions different objects domain 
value global state approximated sum terms corresponding state individual peasants footmen gold assume individual objects class similar value function 
define notion class value function class associated class subfunction 
objects class value subfunction class value function particular environment sum value subfunctions individual objects domain 
set value subfunctions different classes imme determines value function new environment domain acting 
compute set class subfunctions subset environments apply replanning 
provide optimality criterion evaluating classbased value function distribution environments show principle optimized linear program 
learn value function optimizing relative sample environments encountered agent 
prove polynomial number sampled environments suffice construct class value function close obtainable entire distribution environments 
show improve quality approximation automatically discovering subclasses objects similar value functions 
experiments computer systems administration task tasks 
results show successfully generalize class value functions 
importantly approach obtains effective policies problems significantly larger planning algorithm handle 
relational markov decision processes relational mdp defines system dynamics rewards level template task domain 
particular environment domain defines specific mdp instantiated environment 
prm framework domain defined schema specifies set object classes 
cc 
class associated set state variables 
sk describe state object class 
state variable domain possible values dom 
define sc set possible states object possible assignments state variables example domain classes footman gold class may state variable task domain dom task waiting mining harvesting building state variable health domain values 
case values combination values task health 
schema specifies set links 
ll class representing links objects domain 
link range example objects linked objects global gold wood resource objects 
complex situation link may relate instances class denote example enemy footmen footman indicates instance enemy class may related footman instances 
particular instance schema defined world specifying set objects class denote objects class denote total set objects 
world specifies links objects take fixed time 
link specifies set objects denoted example world containing peasants building 
dynamics rewards rmdp defined schema level 
class schema specifies action take values dom 
example dom wait mine harvest build 
class associated transition model specifies probability distribution state object class current state action taken states actions objects linked sc sc 
sc ll 
example status status depends status previous time step task performed build task amount wood gold transition model conditioned state li general entire set objects set peasants linked 
provide compact specification transition model depend state unbounded number variables 
deal issue idea aggregation 
model uses count aggregator probability status transitions built depends task built number peasants task build 
define rewards class level 
assume simplicity rewards associated states individual objects adding global dependencies possible complicates planning significantly 
define reward function sc represents contribution reward object example may reward function associated enemy class specifies reward state enemy object dead enemy enemy state dead 
assume reward object bounded rmax 
world rmdp uniquely defines ground factored mdp transition model specified usual dynamic bayesian network dbn 
random variables factored mdp state variables individual objects 
state system point time vector defining states individual objects world 
subset variables model define part instantiation corresponds variables ground dbn transition dynamics specifies dependence variables time variables time parents variable state variables objects linked example peasants random variables task task status parents time variable status time variables status task task gold amount wood amount 
transition model instances class 
status variables ma health footman enemy count enemy health time footman ee ne footman ee ne health ff health ff health ff ee health ee tactical domain schema resulting factored mdp world footmen enemies 
objects share conditional probability distribution 
note specific depends particular peasants linked 
actual parents dbn status variables different objects different 
reward function simply sum reward functions individual objects 
reward function enemy class described reward function state times number dead enemies state 
remains specify actions ground mdp 
rmdp specifies set possible actions object world 
setting single action taken time step agent choose object act action perform object 
set actions ground mdp simply union dom 
setting multiple actions performed parallel say multiagent setting possible perform action object domain step 
set actions ground mdp vector specifying action object dom 
intermediate cases allowing degrees parallelism possible 
simplicity presentation focus multiagent case action assignment action unit 
example tactical domain consider simplified version schema illustrated fig 
classes units participate game footman enemy 
footman enemy classes state variable health domain dom health healthy wounded dead 
footman class contains single valued link footman enemy enemy 
transition model footman health depend health enemy footman footman enemy footman enemy dead probability footman wounded die significantly higher 
footman choose attack enemy 
footman associated action footman selects enemy attacking 
consequence model action change link structure enemy linked set footmen enemy footmen footman 
case transition model health enemy may depend number footmen dead action choice attack enemy enemy enemy footmen enemy footmen 
define template reward function 
reward enemy dead enemy 
template describe instance tactical domain 
particular world define instances class links instances 
example world footmen enemies objects footman footman enemy enemy 
footman linked enemy footman enemy enemy footman enemy enemy 
enemy linked footmen enemy footmen enemy footmen footman footman 
template number objects links specific vs world yield defined factored mdp vs shown fig 

approximately solving relational mdps approaches solving mdps 
effective linear programming lp denote states mdp actions 

sn lp variables 
vn vi represents si value state si 
lp formulation minimize si vi subject vi si si vk si 
state relevance weights 
sn objective function set positive weights si 
setting state space exponentially large state joint assignment random variables object exponential number units scenario 
multiagent problem number actions exponential number agents 
lp exponential number variables exponential number constraints 
exact solution linear program infeasible 
address issue assumption value function approximated sum local value subfunctions associated individual objects model 
approximation special case factored linear value function approach 
associate value subfunction vo object 
simply local value function depend state individual object 
example local value subfunction enemy object enemy associate numeric value assignment variable enemy health 
richer approximation associate value function pairs small subsets closely related objects 
world requires small extension basic representation 
omit details due lack space 
function footman defined joint assignments footman health enemy health footman enemy enemy 
represent complete value function world sum local value subfunction individual object world 
example world vs footmen enemies global value function vs health health health health health health health health health health 
scope value subfunction object state variables vo depends 
local subfunctions approximate global value function vo 
linear approximation value function lp approach adapted value function representation 
lp variables local components individual local value functions vo dom 
example lp variable joint assignment health health represent components 
similar lp variables included components 
constraint global state global action vo ro vo transformation effect reducing number free variables lp number objects times number parameters required describe object local value function 
constraint global state action exponentially large number 
guestrin koller parr gkp show certain cases exponentially large lp solved efficiently exactly 
particular compact solution applies mdp factored represented dbn approximate value function decomposed weighted linear combination local basis functions 
assumptions gkp decomposition lp grows exponentially induced tree width graph determined complexity process dynamics locality basis function 
approach applies easily 
structure dbn representing process dynamics highly factored defined local interactions objects 
similarly value functions local involving single objects groups closely related objects 
induced width resulting graph problems quite small allowing techniques gkp applied efficiently 
generalizing value functions approach provides principled way decomposing high dimensional value function certain types domains help address generalization problem local value function objects world help provide value function objects worlds especially worlds different sets objects 
obtain generalization build intuition different objects class behave similarly share transition model reward function 
differ interactions objects local contribution value function similar 
example may reasonable assume different footmen similar long term chance killing enemies 
restrict class value functions requiring objects class share local value subfunction 
formally define class local value subfunction vc class 
assume parameterization value function defined object assumption holds trivially scope vc simply sc simply parameter assignment dom sc 
local value function depend states neighboring objects define parameterization accordingly example parameter possible joint state linked footman enemy pair 
specifically defining separate subfunctions define class subfunction 
contribution footman global value function health health 
similarly footman contribute health health 
class value function defines specific value function world sum class local value functions objects vc 
value function depends set objects world local value functions involve related objects links 
importantly objects class contribute function summation argument function object state specific object neighbors 
state contributions different objects class differ 
footman local value subfunction parameters dead footman lower contribution alive 
finding generalized mdp solutions class level value function easily generalize worlds 
assume single set local class value functions vc approximation wide range worlds 
assuming set value functions act new world replanning described step fig 

simply define world specific value function act 
optimize vc way maximizes value entire set worlds 
formalize intuition assume probability distribution worlds agent encounters 
want find single set class local value functions vc fit distribution worlds 
view task optimizing single meta level mdp nature chooses world rest dynamics determined mdp 
precisely state space 
transition model obvious initial state nature chooses world initial state initial starting distribution states 
remaining evolution done dynamics 
example nature choose number footmen enemies define links yields defined mdp vs 
lp formulation meta mdp allows formalize task finding generalized solution entire class mdps 
specifically wish optimize class level parameters vc single ground mdp entire address problem similar lp solution single world sec 

variables simply parameters local class level value subfunctions vc tc tc dom 
constraints recall object lp formulation constraint state action vector ao 
generalized solution state space union state spaces possible worlds 
constraint set union constraint sets world actions ro ao value function world defined class level eq 

principle additional constraint state 
natural choice state relevance weights constraint eliminated objective function minimize models potential number objects may infinite objective function unbounded 
prevent problem assume goes zero sufficiently fast number objects tends infinity 
understand assumption consider generative process selecting worlds number objects chosen classes links object chosen 
decomposition 
intuitive assumption described formalized 
distribution number objects chosen arbitrarily long bounded exponentially decaying function 
sampling worlds main problem formulation size lp size objective number constraints grows number worlds situations grows exponentially number possible objects may infinite 
practical approach address problem sample reasonable number worlds distribution solve lp worlds 
resulting class value function worlds sampled 
start sampling set worlds 
define lp terms worlds possible worlds 
world lp contain set constraints form eq 

note worlds constraints share variables vc represent class value function 
complete lp variables vc tc tc dom 
minimize vc 
subject vc rc vc marginalization variables world constraints form ones sec 

sampled worlds apply lp decomposition techniques gkp world solve lp efficiently 
generalization algorithm summarized step fig 

solution obtained lp sampled worlds general equal obtained worlds considered simultaneously 
show quality approximations close sufficient number worlds sampled 
specifically polynomial number sampled worlds guarantee high probability quality value function approximation obtained sampling worlds close obtained considering possible worlds 
theorem consider class value functions parameters obtained lp possible worlds minimizing eq 
subject constraints eq 
obtained lp sampled worlds optimal value function meta mdp number sampled worlds polynomial ln error bounded rmax probability rmax maximum object reward 
proof omitted lack space see online version uses techniques developed de van roy analyzing constraint sampling general mdps 
important differences analysis includes error introduced sampling objective case sum subset worlds lp full meta mdp 
issue previously addressed 
second algorithm de van roy relies assumption constraints sampled ideal distribution stationary distribution optimal policy 
unfortunately sampling distribution difficult computing near optimal policy 
analysis world sampled algorithm exploits factored structure model represent constraints exactly avoiding dependency ideal distribution 
learning classes objects definition class value function assumes objects class local value function 
cases objects class play different roles model different impact value 
example capability build status may greater impact 
distinctions type usually known advance learned agent gains experience domain detects regularities 
propose procedure takes exactly approach assume set worlds 
world approximate value function vo computed described sec 

addition object associated set features 
example features may include local information object linked global information world contains addition footmen 
define training data vo 
defined learning problem training data partition objects classes objects class similar value functions 
approaches tackling task 
choose decision tree regression construct tree predicts local value function parameters features 
split tree corresponds feature branch tree defines subset local value functions feature values defined path leaf path average value function set 
regression tree learning algorithm tries construct tree predictive local value function aim construct tree mean leaf close training data assigned leaf 
leaves tend correspond objects local value functions similar 
take leaves tree define subclasses subclass characterized combination feature values specified path corresponding leaf 
algorithm summarized step fig 

note mean subfunction leaf value subfunction corresponding class parameters value subfunction optimized class lp step algorithm 
experimental results evaluated generalization algorithm domains computer network administration 
computer network administration problem implemented algorithm matlab cplex lp solver 
full lp decomposition gkp constraint generation extension proposed memory requirements 
learning subclasses input set training worlds set features 
algorithm compute object value function described sec 

apply regression tree learning vo 
define subclass leaf characterized feature vector associated path 

computing class value function input set sub class definitions template vc 
set training worlds algorithm compute parameters vc optimize lp relative worlds 
acting new world input set local value functions vc 
set sub class definitions world 
algorithm repeat obtain current state 
determine appropriate class features 
define 
coordination graph algorithm gkp compute action maximizes 
take action world 
generalization algorithm 
lower second approach 
experimented multiagent computer network examples various network topologies pair basis functions involve states neighboring machines see 
problems computers underlying mdp states actions 
lp decomposition algorithm uses structure underlying factored model solve problems efficiently 
tested extent value functions shared objects 
fig 
plot value object gave assignment status working instances legs topology 
values cluster classes 
cart learn decision trees class partition 
case learning algorithm partitioned computers subclasses illustrated fig 
server intermediate leaf 
fig 
see server third column highest value broken server cause chain reaction affecting network leaf value column lowest affect computer 
evaluated generalization quality classbased value function comparing performance planning specifically new environment 
topology computed class value function sampled networks computers 
sampled number objects value function parameter value intermediate ll eaf ll eaf intermediate server intermediate ll eaf estimated policy value agent class value function optimal approximate value function expected maximum value ring star legs max norm error value function class learning learnt classes ring star legs network administrator results training data learning classes classes learned legs generalization quality evaluated monte carlo runs steps advantage learning subclasses 
tactical footmen enemies 
new network computed value function factorization class restrictions 
value function parameters different parameters object entire classes optimized particular network 
process repeated sets networks 
results shown fig 
indicate value policy class value function close value replanning suggesting generalize new problems 
computed upper bound expected value optimal policy removing negative effect neighbors status machines 
bound loose approximate policies achieve value close 
wanted determine procedure learning classes yields better approximations ones obtained default classes 
fig 
compares error class value function obtained replanning 
graph suggests learning classes decision trees regression tree procedure obtain better approximation value function 
order evaluate algorithm game implemented methods cplex lp solver 
created tasks evaluate aspects game long term strategic decision making local tactical battle maneuvers 
interface scenarios complex tasks publicly available dags stanford edu 
task designed rmdp model represent system consulting domain expert 
planning policies evaluated actual game 
better visualize results direct reader view videos policies website robotics stanford edu guestrin research generalization 
website contains details rmdp model 
important note policies constructed relative approximate model game evaluated real game 
tactical model goal take opposing enemy force equivalent number units 
time step footman decides enemy attack 
enemies controlled hand built strategy 
modelled footmen enemies having health points decrease units attacked 
simple aggregator represent effect multiple attackers 
encourage coordination footman linked buddy ring structure 
local value functions include terms triples linked variables 
solved model world footmen enemies shown fig 

resulting policy fairly complex demonstrates successful coordination footmen initially footmen focus enemy 
enemy injured footman switches target 
enemy weak footman continues attack tackle different enemy 
policy footmen defeat enemies 
factors generated planning algorithm grow exponentially number units planning larger models infeasible 
fortunately executing policy instantiate current state time step action selection significantly faster 
execute step fig 
algorithm larger scenarios generalize class value function world footmen enemies replanning step approach 
policy continues demonstrate successful coordination footmen beat policy 
number units increases position enemies increasingly important 
currently model consider feature world footmen enemies policy loses close battle 
strategic model goal kill strong enemy 
player starts peasants collect gold wood attempt build requires gold wood 
resources consumed build action 
gold player train footman 
footmen choose attack enemy 
attacked enemy loses health points back may kill footmen 
solved model peasants footmen enemy 
related central footman buddy 
scope local value function included triples related objects 
resulting policy quite interesting peasants gather gold wood build gold build footman 
attacking enemy footman waits second footman built 
attack enemy 
stronger enemy able kill footmen quite weak 
footman trained waiting second attacks weak enemy able kill 
planning large scenarios infeasible action selection performed efficiently 
generalized value function tackle world peasants footmen replanning 
peasants coordinate gather resources 
interestingly attacking footmen policy waits trained attacking 
footmen kill enemy dies 
successfully generalized problem joint state action pairs pairs 
discussion tackled longstanding goal planning research ability generalize plans new environments 
generalization complementary uses tackle new environments minimal replanning 
second allows generalize plans smaller tractable environments significantly larger ones solved directly planning algorithm 
experimental results support fact class value function generalizes new plans class subclass structure discovered learning procedure improves quality approximation 
furthermore successfully demonstrated methods real strategic computer game contains characteristics real world dynamic resource allocation problems 
papers consider generalization problem 
approaches represent value functions general terms usually require hand constructed particular task 
focused reusing solutions isomorphic regions state space 
comparison method exploits similarities objects evolving parallel 
interesting combine types decomposition 
boutilier symbolic value iteration computes order value functions generalize objects 
focuses computing exact value functions generalize different world 
furthermore relies theorem proving tools adds complexity approach 
methods deterministic planning focused generalizing compactly described policies learned domains incrementally build order policy 
closest spirit approach yoon extends approaches stochastic domains 
perform similar procedure discover classes finding structure value function 
approach finds regularities compactly represented value functions policies 
tackle tasks multiagent planning action space exponentially large compact policies exist 
key assumption method interchangeability objects class 
mechanism learning subclasses allows deal cases objects domain vary generalizations successful heterogeneous environments objects different influences dynamics rewards 
additionally efficiency lp solution algorithm depends connectivity underlying problem 
domain strong constant interactions objects robocup reward function depends arbitrarily state objects blocksworld solution algorithm probably efficient 
cases tactical domain generalization scale larger problems 
combine lp decomposition technique constraint sampling address high connectivity issue 
general extending techniques highly connected problems open problem 
successfully applied class value functions new environments replanning domains direct application sufficient obtain solution 
domains generalized value functions provide initial policy refined variety local search methods 
assumed relations change time 
domains blocksworld robocup assumption false 
guestrin show context specific independence allow dynamically changing coordination structures multiagent environments 
similar ideas may allow tackle dynamically changing relational structures 
summary believe class value functions methods significantly applicability mdp models large scale real world tasks 
grateful ron parr useful discussions 
supported dod muri program administered office naval research air force contract darpa task program 
boutilier reiter price 
symbolic dynamic programming order mdps 
ijcai 
de van roy 
constraint sampling linear programming approach approximate dynamic programming 
submitted math 
operations research 
dean kanazawa 
probabilistic temporal reasoning 
aaai 
dietterich 
hierarchical reinforcement learning maxq value function decomposition 
journal artificial intelligence research 
fikes hart nilsson 
learning executing generalized robot plans 

intel 
guestrin koller parr 
multiagent planning factored mdps 
nips 
guestrin venkataraman koller 
context specific multiagent coordination planning factored mdps 
aaai 
hauskrecht meuleau kaelbling dean boutilier 
hierarchical solution markov decision processes macro actions 
uai 
khardon 
learning action strategies planning domains 
artificial intelligence 
koller pfeffer 
probabilistic frame systems 
aaai 
martin geffner 
learning generalized policies planning concept languages 
kr 
parr 
flexible decomposition algorithms weakly coupled markov decision problems 
uai 
schuurmans 
direct value approximation factored mdps 
nips 
schweitzer 
generalized polynomial approximations markovian decision processes 
mathematical analysis applications 
sutton barto 
reinforcement learning 
mit press cambridge ma 
thrun sullivan 
discovering structure multiple learning tasks tc algorithm 
icml 
yoon fern givan 
inductive policy selection order mdps 
uai 
