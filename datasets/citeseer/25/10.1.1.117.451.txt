hierarchical policy gradient algorithms mohammad cs umass edu sridhar mahadevan cs umass edu department computer science university massachusetts amherst amherst ma usa hierarchical reinforcement learning general framework attempts accelerate policy learning large domains 
hand policy gradient reinforcement learning methods received attention means solve problems continuous state spaces 
suffer slow convergence 
combine approaches propose family hierarchical policy gradient algorithms problems continuous state action spaces 
introduce class hierarchical hybrid algorithms group subtasks usually higher levels hierarchy formulated value function rl problems problems 
demonstrate performance proposed algorithms simple taxi fuel problem complex continuous state action ship steering domain 

value function reinforcement learning extensively studied machine learning literature 
weak theoretical guarantees performance methods problems large continuous state spaces 
alternative approach consider class parameterized stochastic policies compute gradient performance function respect parameters improve policy adjusting parameters direction gradient 
approach known policy gradient reinforcement learning baxter bartlett received attention means solve problems continuous state spaces 
main motivations algorithms theoretically guaranteed converge lo cally optimal policies possible incorporate prior knowledge methods appropriate choice parametric form policy 
real world high dimensional tasks performance function parameterized large number parameters methods show poor performance stuck local optima 
algorithms usually slower methods due large variance gradient estimators 
reasons application algorithms problematic real world domains 
possible solution incorporate prior knowledge decompose high dimensional task collection modules smaller manageable state spaces learn modules way solve problem 
hierarchical value function rl methods parr dietterich sutton developed approach attempt scale rl large state spaces 
define family hierarchical policy gradient hpg algorithms scaling methods high dimensional domains 
hpg subtasks involved decision making subtasks child call non primitive subtasks defined problems solution involves computing locally optimal policy 
subtask formulated terms parameterized family policies performance function method estimate gradient performance function routine update policy parameters performance gradient 
accelerate learning hpg algorithms formulating higher level subtasks usually involve smaller manageable state finite action spaces problems lower level subtasks infinite state action spaces problems morimoto doya 
call family algorithms hierarchical hybrid algorithms 
effectiveness proposed algorithms demonstrated simple taxi fuel problem com proceedings twentieth international conference machine learning icml washington dc 
plex continuous state action ship steering task 

hierarchical task decomposition introduce hierarchical task decomposition continuous state action space ship steering problem miller see 
ship starts randomly chosen position orientation turning rate constant speed gate placed fixed position 
km 
gate km 
ship steering domain 
equation gives motion equations ship time constant convergence desired turning rate sec constant speed ship sec sampling interval 
time lag changes desired turning rate actual rate modeling effects real ship inertia resistance water 
sin cos time state ship position orientation actual turning rate 
action desired turning rate ship 
state variables action continuous range shown table 
ship steering problem episodic 
episode goal learning generate sequences actions steer center ship gate minimum amount time 
sides gate placed coordinates 
ship moves bound episode terminates considered failure 
applied flat actor critic algorithms task achieving performance reasonable amount time 
believe failure occurred due reasons state meters meters degrees degrees sec action degrees sec table 
range state action variables ship steering task 
problem hard rl algorithms 
ship turn faster degrees sec state variables change small amount control interval 
need high resolution discretization state space order accurately model state transitions requires large number parameters function approximator problem intractable 
second time lag changes desired turning rate actual turning rate ship position orientation requires controller deal long delays 
successfully applied flat policy gradient algorithm simplified versions problem shown change ship starts fixed position randomly chosen orientation turning rate goal reaching neighborhood fixed point 
indicates high dimensional non linear control problem learned appropriate hierarchical decomposition 
prior knowledge decompose problem levels task graph shown 
high level agent learns select diagonal horizontal vertical subtasks 
low level low level subtask learns sequence turning rates achieve goal 
symmetry map possible subtasks root subtasks low level associated diagonal subtasks associated horizontal vertical subtasks shown 
call diagonal subtask horizontal vertical subtask 
goal initial position initial position goal 
shows simplified versions ship steering task low level subtasks hierarchical decomposition ship steering problem 
continuous action turning rate root horizontal vertical diagonal subtasks subtasks diagonal horizontal vertical subtask subtask primitive action continuous action turning rate 
task graph ship steering problem 
illustrated ship steering task designer system uses domain knowledge recursively decomposes task collection subtasks important solving problem 
decomposition represented directed acyclic graph called task graph shown 
mathematically task graph modeled decomposing task mdp finite set subtask mdps 

subtask mdp models subtask hierarchy root task solving solves entire mdp non primitive subtask defined mdp state space initiation set set terminal states action space transition probability function reward function primitive action primitive subtask decomposition executable terminates immediately execution 
subtask refer non primitive subtasks 
policy subtask model gives hierarchical policy 

hierarchical policy executed stack discipline similar subroutine calls programming languages 

policy gradient formulation decomposing problem set subtasks described section formulate subtask problem 
focus episodic problems assume task root hierarchy episodic 

policy formulation formulate subtask set randomized stationary policies parameterized terms vector denotes probability action state policy corresponding assumption set policies 
state action function bounded bounded second derivatives 
furthermore bounded differentiable bounded derivatives 
time call subtask hierarchy starts initial states terminates terminal states model instantiation subtask episode shown 
model terminal states transit probability reward absorbing state assumption subtask parameterized policy 
initial states 
terminal states 
shows model subtask episodic problem assumption 
subtask termination exists state action stationary policies states 
model define new mdp subtask transition probabilities rewards probability subtask starts state set transition matrices 
result subtask lemma assumptions hold 
state 
lemma equivalent assume mdp recurrent underlying markov chain policy mdp single recurrent class state recurrent state 

performance measure definition define weighted reward go performance measure subtask formulated parameter ized policy assumption holds reward go state sk min sk time state visited 

optimizing weighted reward go order obtain expression gradient mdp defined section 
lemma mdp recurrent 
mdp steady state probability distribution state mean recurrence time 
define qi sk usual action value function 
mdp derive proposition gives expression gradient weighted reward go respect proposition assumptions hold si ai expression gradient proposition estimated renewal cycle tm tm sn sn sn tm time mth visit recurrent state qi sn tm ri sn estimate qi equation obtain procedure update parameter vector approximate gradient direction time step 
sk sk ak ri sk ak cycle consecutive visits recurrent state step size parameter satisfies assumptions 
deterministic nonnegative satisfy 
non increasing exists positive integer positive scalar atp positive integers convergence result iterative procedure equation update parameters 
proposition assumptions hold sequence parameter vectors generated equation 
converges probability 
equation provides unbiased estimate 
systems involving large state space interval visits state large 
consequence estimate large variance 
approaches proposed reduce variance estimations delivering faster convergence 
instance approach replace subset state space containing reset method easily implemented subtask defining 
approach uses discount factor reward go estimation 
methods introduce bias estimate 
approaches derive modified version equation incrementally update parameter vector approximate gradient direction 

hierarchical policy gradient algorithms decomposing task set subtasks described section formulating subtask hierarchy episodic problem illustrated section update equation derive hierarchical policy gradient algorithm hpg maximize weighted reward go subtask hierarchy 
algorithm shows pseudo code algorithm 
lines algorithm internal reward inside subtask speed local learning propagate upper levels hierarchy 
lines replaced policy gradient algorithm optimize weighted reward go baxter bartlett 
algorithm demonstrates family hierarchical policy gradient algorithms maximize weighted go subtask hierarchy 
algorithm hierarchical policy gradient algorithm maximizes weighted reward go subtask hierarchy 
function hpg task state primitive action execute action state observe state reward return terminated choose action policy hpg task state observe result state internal reward zi zi zi gi zi return hpg formulation subtask limitations compact parameterized representation policy limits search policy set typically smaller set possible policies 
gradient optimization algorithms search method policy find solution locally globally optimal 
general family algorithms described converges recursively local optimal policy 
policy learned subtask hierarchy coincides best policies algorithms converge recursively optimal policy 
despite methods proposed reduce variance gradient estimators algorithms algorithms slower methods show simple taxi fuel experiment section 
way accelerate learning hpg algorithms formulate subtasks involving smaller state spaces finite action spaces usually located higher levels hierarchy problems large state spaces infinite action spaces usually located lower levels hierarchy problems 
formulation benefit faster convergence methods power algorithms domains infinite state action spaces time 
call family algorithms hierarchical hybrid algorithms illustrate ship steering experiment 

experimental results section apply hierarchical policy gradient algorithm proposed wellknown taxi fuel problem dietterich compare performance maxq value hierarchical rl algorithm flat learning 
turn complex continuous state action ship steering domain apply hierarchical hybrid algorithm task compare performance flat actor critic algorithms 

taxi fuel problem grid world inhabited taxi shown 
stations marked lue ed 
task episodic 
episode taxi starts randomly chosen location randomly chosen amount fuel ranging units 
passenger stations chosen randomly passenger wishes transported stations chosen randomly 
taxi go passenger location pick passenger go destination location drop passenger 
episode ends passenger deposited destination station taxi goes fuel 
possible states primitive actions domain navigation actions pickup action dropoff action action consumes unit fuel 
action deterministic 
reward action additional reward successfully delivering passenger 
reward taxi attempts execute dropoff pickup actions illegally reward fuel level falls zero 
system performance measured terms average reward step 
domain equivalent maximizing total reward episode 
experiment conducted times results averaged 
taxi blue station green station red station yellow station gas station 
taxi fuel domain 
reward maxq flat learning hierarchical policy gradient number trials 
compares performance hierarchical algorithm proposed maxq flat learning taxi fuel problem 
compares proposed hierarchical policy gradient hpg algorithm maxq dietterich value hierarchical rl algorithm flat learning 
graph shows maxq converges faster hpg flat learning hpg slightly faster flat learning 
hierarchical policy gradient algorithm experiment shown algorithm policy parameter state action pair 
expected hpg algorithm converges performance maxq slower value counterpart 
performance hpg improved better policy formulation sophisticated policy gradient algorithms subtask 
slow convergence methods motivates value policy methods hierarchy study define expressive policies subtask 
address hierarchical hybrid algorithms section leave 

ship steering problem section apply hierarchical hybrid algorithm ship steering task described section compare performance flat algorithms 
flat algorithm section uses equation cmac function approximator dimensional tilings dividing space tiles 
actor critic algorithm uses function approximator actor dimensional tilings size tiles critic 
fifth dimension critic tilings continuous action 
hierarchical hybrid algorithm decompose task task graph 
high level learner explores low dimensional subspace original high dimensional state space 
state variables coordinates ship full range 
actions diagonal horizontal vertical subtasks similar subtasks shown 
state space coarsely discretized states 
value algorithm greedy action selection replacing traces learn sequence diagonal horizontal vertical subtasks achieve goal entire task passing gate 
episode ends ship passes gate moves bound 
new episode starts ship randomly chosen position orientation turning rate 
algorithm set learning rate starts remains unchanged performances low level subtasks reach certain level decreased factor episodes 
low level learner explores local areas high dimensional state space discretization 
high level learner selects low level subtasks low level subtask takes control executes steps shown 
maps ship new coordinate system ship position diagonal subtask horizontal vertical subtask 
sets low level goal position diagonal subtask horizontal vertical subtask 
sets low level boundaries 
generates primitive actions ship reaches neighborhood lowlevel goal circle radius low level goal success moves low level bounds failure 
low level subtasks state variables range coordination variables 
action variable desired turning rate ship continuous variable range degrees sec 
control interval sec times sampling interval sec 
policy gradient learning algorithm lines algorithm update parameters 
addition cmac function approximator dimensional tilings dividing space tiles 
parameter defined tile parameterized policy gaussian wi total number tiles state falls tile 
number success episodes hierarchical hybrid algorithm flat policy gradient algorithm flat actor critic algorithm number episodes 
shows performance hierarchical hybrid flat actor critic algorithms terms number successful trials episodes 
actual action generated mapping value chosen gaussian policy range degrees sec sigmoid function 
addition original reward step define internal rewards low level success failure reward distance current ship orientation angle current position low level goal exp deg gives width reward function 
low level subtask terminates reward propagates high level summation rewards step 
addition reward received low level high level uses reward successfully passing gate 
train system episodes 
episode high level learner controller located root selects low level subtask selected lowlevel subtask executed successfully terminates ship reaches low level goal fails ship goes low level bounds 
control returns high level subtask root 
results averaged simulation runs 
compares performance hierarchical hybrid algorithm flat actor critic algorithms terms number successful trials episodes 
shows despite high resolution function approximators flat algorithms performance worse hierarchical hybrid algorithm 
computation time step hierarchical hybrid algorithm due large number parameters learned 
demonstrates performance system terms average number low level subtask number low level subtask calls episodes number low level subtask number episodes 
shows performance system terms number low level subtask calls 
number primitive steps episodes hierarchical hybrid algorithm flat policy gradient algorithm flat actor critic algorithm number episodes 
shows performance hierarchical hybrid flat actor critic algorithms terms number steps pass gate 
calls 
shows learning learner executes low level subtasks diagonal horizontal vertical subtasks episode 
compares performance hierarchical hybrid flat actor critic algorithms terms average number steps goal averaged episodes 
shows learning takes primitive actions turn actions hierarchical hybrid learner pass gate 
flat algorithms show better performance hierarchical algorithm terms average number steps goal flat algorithms find global optimal policy hierarchical hybrid algorithm converges just recursive optimal solution shows performance episodes worse hierarchical hybrid algorithm 
figures show performance diagonal horizontal vertical subtasks terms number success executions respectively 
demonstrates learned policy sample initial points shown big circles 
upper initial point lower initial point 
low level subtasks chosen agent high level shown small circles 
number success episodes diagonal subtask performance number episodes 
shows performance diagonal subtask terms number successful trials episodes 
number success episodes horizontal vertical subtask performance number episodes 
shows performance horizontal vertical subtask terms number successful trials episodes 

combines advantages hierarchical task decomposition methods describes class hierarchical policy gradient hpg algorithms problems continuous state action spaces 
accelerate learning hpg algorithms proposed hierarchical hybrid algorithms higher level subtasks formulated lower level subtasks problems 
effectiveness algorithms demonstrated applying simple taxi fuel problem continuous state action ship steering domain 
algorithms proposed case task episodic 
formulated algorithms case task continuing include results space reasons 
case root task formulated continuing problem average reward performance function 
policy learned root involves policies children type optimality achieved root depends formulate subtasks hierarchy 
investigated different notions optimality hierarchical average reward reported previous mahadevan mahadevan hpg algorithms continuing root task 

shows learned policy initial points 
proposed algorithms give ability deal large continuous state spaces appropriate control real world problems speed learning crucial 
policy gradient algorithms give opportunity accelerate learning defining expressive set parameterized policies subtask 
results ship steering task indicate order apply methods real world domains expressive representation policies needed 
baxter bartlett 

infinite horizon estimation 
journal artificial intelligence research 
dietterich 

maxq method hierarchical reinforcement learning 
proceedings fifteenth international conference machine learning pp 

mahadevan 

continuoustime hierarchical reinforcement learning 
proceedings eighteenth international conference machine learning pp 

mahadevan 

hierarchically optimal average reward reinforcement learning 
proceedings nineteenth international conference machine learning pp 



actor critic algorithms 
doctoral dissertation mit 


simulation methods markov decision processes 
doctoral dissertation mit 
morimoto doya 

acquisition stand behavior real robot hierarchical reinforcement learning 
robotics autonomous systems 
parr 

hierarchical control learning markov decision processes 
doctoral dissertation university california berkeley 
sutton precup singh 

mdps semi mdps framework temporal abstraction reinforcement learning 
artificial intelligence 
