fast learning algorithm deep belief nets geoffrey hinton simon osindero department computer science university toronto kings college road toronto canada hinton osindero cs toronto edu show complementary priors eliminate explaining away effects inference difficult densely connected belief nets hidden layers 
complementary priors derive fast greedy algorithm learn deep directed belief networks layer time provided top layers form undirected associative memory 
fast greedy algorithm initialize slower learning procedure fine tunes weights contrastive version wake sleep algorithm 
fine tuning network hidden layers forms generative model joint distribution handwritten digit images labels 
generative model gives better digit classification best discriminative learning algorithms 
low dimensional manifolds digits lie modelled long free energy landscape top level associative memory easy explore directed connections display associative memory mind 
learning difficult densely connected directed belief nets hidden layers difficult infer conditional distribution hidden activities data vector 
variational methods simple approximations true conditional distribution approximations may poor especially deepest hidden layer prior assumes independence 
variational learning requires parameters learned learning time scale poorly number parameters increases 
describe model top hidden layers form undirected associative memory see appear neural computation yee teh department computer science national university singapore science drive singapore comp nus edu sg remaining hidden layers form directed acyclic graph converts representations associative memory observable variables pixels image 
hybrid model attractive features 
fast greedy learning algorithm find fairly set parameters quickly deep networks millions parameters hidden layers 

learning algorithm unsupervised applied labeled data learning model generates label data 

fine tuning algorithm learns excellent generative model outperforms discriminative methods mnist database hand written digits 

generative model easy interpret distributed representations deep hidden layers 

inference required forming percept fast accurate 

learning algorithm local adjustments synapse strength depend states presynaptic post synaptic neuron 

communication simple neurons need communicate stochastic binary states 
section introduces idea complementary prior exactly cancels explaining away phenomenon inference difficult directed models 
example directed belief network complementary priors 
section shows equivalence restricted boltzmann machines infinite directed networks tied weights 
section introduces fast greedy learning algorithm constructing multi layer directed networks layer time 
variational bound shows new layer added generative model improves 
greedy algorithm bears resemblance boosting repeated weak learner reweighting data vector ensure step learns new re represents 
weak learner top level units label units top level sensory pathway units units pixel image network model joint distribution digit images digit labels 
training case consists image explicit class label progress shown learning algorithm labels replaced multilayer pathway inputs spectrograms multiple different speakers saying isolated digits 
network learns generate pairs consist image spectrogram digit class 
construct deep directed nets undirected graphical model 
section shows weights produced fast greedy algorithm fine tuned algorithm 
contrastive version wake sleep algorithm hinton 
suffer mode averaging problems cause wake sleep algorithm learn poor recognition weights 
section shows pattern recognition performance network hidden layers weights mnist set handwritten digits 
knowledge geometry provided special preprocessing generalization performance network errors digit official test set 
beats achieved best back propagation nets hand crafted particular application 
slightly better errors reported decoste support vector machines task 
section shows happens mind network running constrained visual input 
network full generative model easy look mind simply generate image high level representations 
consider nets composed simple logistic belief net containing independent rare causes highly anti correlated observe house jumping 
bias earthquake node means absence observation node times 
earthquake node truck node jump node total input means chance 
better explanation observation house jumped odds apply hidden causes active 
wasteful turn hidden causes explain observation probability happening earthquake node turned explains away evidence truck node 
stochastic binary variables ideas generalized models log probability variable additive function states directly connected neighbours see appendix details 
complementary priors phenomenon explaining away illustrated inference difficult directed belief nets 
densely connected networks posterior distribution hidden variables intractable special cases mixture models linear models additive gaussian noise 
markov chain monte carlo methods neal sample posterior typically time consuming 
variational methods neal hinton approximate true posterior tractable distribution improve lower bound log probability training data 
learning guaranteed improve variational bound inference hidden states done incorrectly better find way eliminating explaining away altogether models hidden variables highly correlated effects visible variables 
widely assumed impossible 
logistic belief net neal composed stochastic binary units 
net generate data probability turning unit logistic function states immediate ancestors weights wij directed connections ancestors si exp bi bi bias unit logistic belief net hidden layer prior distribution hidden variables factorial binary states chosen independently model generate data 
non independence posterior distribution created likelihood term coming data 
eliminate explaining away hidden layer extra hidden layers create complementary prior exactly opposite correlations likelihood term 
likelihood term multiplied prior get posterior exactly factorial 
obvious complementary priors exist shows simple example infinite logistic belief net tied weights priors complementary hidden layer see appendix general treatment conditions complementary priors exist 
tied weights construct complementary priors may mere trick making directed models equivalent undirected ones 
shall see leads novel efficient learning algorithm works progressively weights layer weights higher layers 
infinite directed model tied weights generate data infinite directed net starting random configuration infinitely deep hidden layer performing top ancestral pass binary state variable layer chosen bernoulli distribution determined top input coming active parents layer 
respect just directed acyclic belief net 
directed nets sample true posterior distribution hidden layers starting data vector visible units transposed weight matrices infer factorial distributions hidden layer turn 
hidden layer sample factorial posterior computing factorial posterior layer appendix shows procedure gives unbiased samples complementary prior layer ensures posterior distribution really factorial 
sample true posterior compute derivatives log probability data 
generation process converges stationary distribution markov chain need start layer deep compared time takes chain reach equilibrium 
exactly inference procedure wake sleep algorithm hinton models described variational approximation required inference procedure gives unbiased samples 
start computing derivative generative weight ij unit layer unit layer see 
logistic belief net maximum likelihood learning rule single data vector log ij denotes average sampled states probability unit turned visible vector stochastically reconstructed sampled hidden states 
computing posterior distribution second hidden layer sampled binary states hidden layer exactly process reconstructing data sample bernoulli random variable probability learning rule written log ij dependence tion eq 
eq 
conditional unproblematic deriva expectation weights replicated full derivative generative weight obtained summing derivatives generative weights pairs layers log wij 
vertically aligned terms cancel leaving boltzmann machine learning rule eq 

restricted boltzmann machines contrastive divergence learning may immediately obvious infinite directed net equivalent restricted boltzmann machine rbm 
rbm single layer hidden units connected undirected symmetrical connections layer visible units 
generate data rbm start random state layers perform alternating gibbs sampling units layer updated parallel current states units layer repeated system sampling equilibrium distribution 
notice exactly process generating data infinite belief net tied weights 
perform maximum likelihood learning rbm difference correlations 
weight wij visible unit hidden unit measure correlation clamped vi vi vi infinite logistic belief net tied weights 
downward arrows represent generative model 
upward arrows part model 
represent parameters infer samples posterior distribution hidden layer net clamped 
visible units hidden states sampled conditional distribution factorial 
alternating gibbs sampling run markov chain shown reaches stationary distribution measure correlation 
gradient log probability training data log wij learning rule maximum likelihood learning rule infinite logistic belief net tied weights step gibbs sampling corresponds computing exact posterior distribution layer infinite logistic belief net 
maximizing log probability data exactly minimizing kullback leibler divergence kl distribution data equilibrium distribution defined model contrastive divergence learning hinton run markov chain full steps measuring second correlation 
equivalent ignoring derivatives full step consists updating updating vi infinity infinity depicts markov chain uses alternating gibbs sampling 
full step gibbs sampling hidden units top layer updated parallel applying eq 
inputs received current states visible units bottom layer visible units updated parallel current hidden states 
chain initialized setting binary states visible units data vector 
correlations activities visible hidden unit measured update hidden units chain 
difference correlations provides learning signal updating weight connection 
come higher layers infinite net 
sum ignored derivatives derivative log probability posterior distribution layer vn derivative kullback leibler divergence tween posterior distribution layer vn equi distribution defined model 
contrastive divergence learning minimizes difference kullback leibler divergences kl kl ignoring sampling noise difference negative gibbs sampling produce gibbs sampling reduces kullback leibler diver gence equilibrium distribution 
important notice depends current model parameters way changes parameters change ignored contrastive divergence learning 
problem arise training data depend parameters 
empirical investigation relationship maximum likelihood contrastive divergence learning rules carreira hinton 
contrastive divergence learning restricted boltzmann machine efficient practical hinton 
variations real valued units different sampling schemes described teh 
quite successful modeling formation topographic maps welling denoising natural images roth black images biological cells ning 
marks movellan describe way contrastive divergence perform factor analysis welling 
show network logistic binary visible units linear gaussian hidden units rapid document retrieval 
appears efficiency bought high price applied obvious way contrastive divergence learning fails deep multilayer networks different weights layer networks take far long reach conditional equilibrium clamped data vector 
show equivalence rbm infinite directed nets tied weights suggests efficient learning algorithm multilayer networks weights tied 
greedy learning algorithm transforming representations efficient way learn complicated model combine set simpler models learned sequentially 
force model sequence learn different previous models data modified way model learned 
boosting freund model sequence trained re weighted data emphasizes cases preceding models got wrong 
version principal components analysis variance modeled direction removed forcing modeled direction lie orthogonal subspace sanger 
projection pursuit friedman stuetzle data transformed nonlinearly distorting direction data space remove non gaussianity direction 
idea greedy algorithm allow model sequence receive different representation data 
model performs non linear transformation input vectors produces output vectors input model sequence 
shows multilayer generative model top layers interact undirected connections connections directed 
undirected connections top equivalent having infinitely higher layers tied weights 
intra layer connections simplify analysis layers number units 
possible learn sensible optimal values parameters assuming parameters higher layers construct complementary prior 
equivalent assuming weight matrices constrained equal 
task learning assumption reduces task learning rbm difficult approximate solutions rapidly minimizing contrastive divergence 
learned data mapped create higher level data hidden layer 
rbm perfect model original data higher level data modeled perfectly higher level weight matrices 
generally rbm able model original data perfectly generative model better greedy algorithm 
learn assuming weight matrices tied 

freeze commit wt infer hybrid network 
top layers undirected connections form associative memory 
layers directed top generative connections map state associative memory image 
directed bottom recognition connections infer factorial representation layer binary activities layer 
greedy initial learning recognition connections tied generative connections 
factorial approximate posterior distributions states variables hidden layer subsequent changes higher level weights mean inference method longer correct 

keeping higher weight matrices tied learn rbm model higher level data produced wt transform original data 
greedy algorithm changes higher level weight matrices guaranteed improve generative model 
shown neal hinton negative log probability single data vector multilayer generative model bounded variational free energy expected energy approximating distribution minus entropy distribution 
directed model energy configuration log log bound log log log log binary configuration units hidden layer prior probability current model defined weights probability distribution binary configurations hidden layer 
bound equality true posterior distribution 
weight matrices tied factorial distribution produced applying wt data vector true posterior distribution step greedy algorithm log equal bound 
step freezes terms fixed derivative bound derivative log maximizing bound weights higher layers exactly equivalent maximizing log probability dataset occurs probability 
bound tighter possible log fall lower bound increases log fall value step greedy algorithm bound tight point bound increases 
greedy algorithm clearly applied recursively full maximum likelihood boltzmann machine learning algorithm learn set tied weights bottom layer set weights learn weights layer time guarantee decrease log probability data full generative model 
practice replace maximum likelihood boltzmann machine learning algorithm contrastive divergence learning works faster 
contrastive divergence voids guarantee reassuring know extra layers guaranteed improve imperfect models learn layer sufficient patience 
guarantee generative model improved greedily learning layers convenient consider models layers size higherlevel weights initialized values learned weights layer 
greedy algorithm applied layers different sizes 
back fitting algorithm learning weight matrices layer time efficient optimal 
weights higher layers learned weights simple inference procedure optimal lower layers 
sub optimality produced greedy learning relatively innocuous supervised methods boosting 
labels scarce guarantee expected change log probability 
label may provide bits constraint parameters fitting typically problem underfitting 
going back earlier models may cause harm 
unsupervised methods large unlabeled datasets case may high dimensional providing bits constraint generative model 
fitting serious problem alleviated subsequent stage back fitting weights learned revised fit better weights learned 
greedily learning initial values weights layer recognition weights inference generative weights define model retain restriction posterior layer approximated factorial distribution variables layer conditionally independent values variables layer 
variant wake sleep algorithm described hinton 
allow higher level weights influence lower level ones 
pass recognition weights bottom pass stochastically picks state hidden variable 
generative weights directed connections adjusted maximum likelihood learning rule eq 
weights undirected connections top level learned fitting top level rbm posterior distribution penultimate layer 
pass starts state top level associative memory uses top generative connections stochastically activate lower layer turn 
pass top level undirected connections generative directed connections changed 
bottom recognition weights modified 
equivalent sleep phase wake sleep algorithm associative memory allowed settle equilibrium distribution initiating pass 
associative memory initialized pass allowed run iterations alternating gibbs sampling initiating pass contrastive form wake sleep algorithm eliminates need sample equilibrium distribution associative memory 
contrastive form fixes problems sleep phase 
ensures recognition weights learned representations resemble real data helps eliminate problem mode averaging 
particular data vector current recognition weights pick particular mode level ignore different modes equally generating data learning pass try alter recognition weights recover modes sleep phase pure ancestral pass 
pure ancestral pass start prolonged gibbs sampling get equilibrium sample top level associative memory 
top level associa weights longer tied weights computed states variables layer generative weights variables cases network guessed right second guess probability probability best guess 
true classes arranged standard scan order 
tive memory eliminate problem wake phase independent top level units required allow ancestral pass mean variational approximation poor top layer weights 
appendix specifies details algorithm matlab style pseudo code network shown 
simplicity penalty weights momentum learning rate parameters 
training data reduced single case 
performance mnist database training network mnist database handwritten digits contains training images test images 
results different pattern recognition techniques published publicly available database ideal evaluating new pattern recognition methods 
basic version mnist learning task knowledge geometry provided special pre processing enhancement training set unknown fixed random permutation pixels affect learning algorithm 
permutation invariant version task generalization performance network errors official test set 
network shown trained training images divided balanced mini batches containing examples digit class 
weights updated mini batch 
preliminary experiments images handwritten digits usps database showed way model joint distribution digit images labels architecture type images units hidden layer 
test cases network got wrong 
case labeled network guess 
true classes arranged standard scan order 
initial phase training greedy algorithm described section train layer weights separately starting bottom 
layer trained sweeps training set called epochs 
training units visible layer rbm real valued activities 
normalized pixel intensities learning bottom layer weights 
training higher layers weights real valued activities visible units rbm activation probabilities hidden units lower level rbm 
hidden layer rbm stochastic binary values rbm trained 
greedy training took hours layer matlab ghz xeon processor done error rate test set see details network tested 
training top layer weights ones associative memory labels provided part input 
labels represented turning unit softmax group units 
activities group reconstructed activities layer exactly unit allowed active probability picking unit pi exp xi exp xj xi total input received unit curiously learning rules unaffected competition units softmax group synapses need know unit competing unit 
competition affects probability unit turning probability affects learning 
greedy layer layer training network trained different learning rate weight decay epochs algorithm described section 
learning rate momentum weight decay chosen training network times observing performance separate validation set images taken remainder full training set 
epochs algorithm pass followed full iterations alternating gibbs sampling associative memory performing pass 
second epochs iterations performed epochs iterations performed 
time number iterations gibbs sampling raised error validation set decreased noticeably 
network performed best validation set tested error rate 
network trained training images error rate full training set low final error rate initial training set images 
took epochs making total learning time week 
final network error rate errors network shown 
cases network gets correct second best probability best probability shown 
error rate compares favorably error rates achieved feed forward neural networks hidden layers trained optimize discrimination back propagation algorithm see table appearing 
detailed connectivity networks hand crafted attempt different learning rates different layers learning rate momentum set quite conservatively avoid oscillations 
highly learning speed considerably improved careful choice learning parameters possible lead worse solutions 
training set unequal numbers class images assigned randomly mini batches 
check learning significantly improved error rate network left running small learning rate test error displayed epoch 
weeks test error fluctuating epoch number training errors smallest 
particular task best reported error rate stochastic online learning separate squared error output units 
error rates reduced net hidden layer units small initial weights separate cross entropy error function output unit gentle learning john platt personal communication 
identical result achieved net units hidden layer second hidden layer softmax output units regularizer penalizes squared weights amount carefully chosen validation set 
comparison nearest neighbor reported error rate uchicago edu wilder mnist training cases extremely slow 
reduced norm 
standard machine learning technique comes close error rate generative model basic task support vector machine gives error rate decoste 
hard see support vector machines domain specific tricks weight sharing subsampling lecun 
improve performance discriminative neural networks 
obvious reason weight sharing sub sampling reduce error rate generative model currently investigating approach 
improvements achieved averaging opinions multiple networks technique available methods 
substantial reductions error rate achieved supplementing data set slightly transformed versions training data 
pixel translations decoste achieve 
local elastic deformations convolutional neural network simard 
achieve slightly better achieved best hand coded recognition algorithm belongie 
explored distorted data learning generative models types distortion need investigated fine tuning algorithm currently slow 
testing network way test network stochastic pass image fix binary states units lower layer associative memory 
states fixed label units initial real valued activities iterations alternating gibbs sampling activate correct label unit 
method testing gives error rates higher rates reported 
better method fix binary states units lower layer associative memory turn label units turn compute exact free energy resulting component binary vector 
row shows samples generative model particular label clamped 
top level associative memory run iterations alternating gibbs sampling samples 
computation required independent label unit turned teh hinton method computes exact conditional equilibrium distribution labels approximating gibbs sampling previous method doing 
method gives error rates higher ones quoted stochastic decisions pass 
remove noise ways 
simplest pass deterministic probabilities activation place stochastic binary states 
second repeat stochastic pass times average label probabilities label log probabilities repetitions picking best 
types average give identical results results similar deterministic pass method reported results 
looking mind neural network generate samples model perform alternating gibbs sampling top level associative memory markov chain converges equilibrium distribution 
sample distribution input layers generate image single pass generative connections 
clamp label units particular class gibbs sampling see images model class conditional distributions 
shows sequence images class generated allowing iterations gibbs sampling samples 
initialize state top layers providing random binary image input 
shows class conditional state associative memory evolves allowed run freely row shows samples generative model particular label clamped 
top level associative memory initialized pass random binary image pixel probability 
column shows results pass initial high level state 
subsequent columns produced iterations alternating gibbs sampling associative memory 
label clamped 
internal state observed performing pass iterations see associative memory mind 
word mind intended metaphorical 
believe mental state state hypothetical external world high level internal representation constitute veridical perception 
hypothetical world shows 
shown possible learn deep belief network layer time 
obvious way assume higher layers exist learning lower layers compatible simple factorial approximations replace intractable posterior distribution 
approximations need true posterior close factorial possible 
ignoring higher layers assume exist tied weights constrained implement complementary prior true posterior exactly factorial 
equivalent having undirected model learned efficiently contrastive divergence 
viewed constrained variational learning penalty term divergence approximate true posteriors replaced constraint prior variational approximation exact 
layer learned weights weights higher layers 
higher level weights change priors lower layers cease com true posterior distributions lower layers longer factorial transpose generative weights inference longer correct 
variational bound show adapting higher level weights improves generative model 
demonstrate power fast greedy learning algorithm initialize weights slower fine tuning algorithm learns excellent generative model digit images labels 
clear best way fast greedy algorithm 
better omit fine tuning speed greedy algorithm learn ensemble larger deeper networks larger training set 
network parameters cubic millimeters mouse cortex barlow pers 
comm networks complexity fit single voxel high resolution fmri scan 
suggests bigger networks may required compete human shape recognition abilities 
current generative model limited ways lee mumford 
designed images nonbinary values treated probabilities case natural images top feedback perception limited associative memory top layers systematic way dealing perceptual invariances assumes segmentation performed learn sequentially attend informative parts objects discrimination difficult 
illustrate major advantages generative models compared discriminative ones 
generative models learn low level features requiring feedback label learn parameters discriminative models overfitting 
discriminative learning training case constrains parameters bits information required specify label 
generative model training case constrains parameters number bits required specify input 

easy see network learned generating model 

possible interpret non linear distributed representations deep hidden layers generating images 

superior classification performance discriminative learning methods holds domains possible learn generative model 
set domains eroded moore law 
acknowledgments peter dayan zoubin ghahramani yann le cun radford neal terry sejnowski max welling helpful discussions referees greatly improving manuscript 
research supported nserc gatsby charitable foundation cfi 
fellow canadian institute advanced research holds canada research chair machine learning 
belongie malik puzicha 

shape matching object recognition shape contexts 
ieee transactions pattern analysis machine intelligence 
carreira hinton 

contrastive divergence learning 
artificial intelligence statistics 
decoste 

training invariant support vector machines 
machine learning 
freund 

boosting weak learning algorithm majority 
information computation 
friedman stuetzle 

projection pursuit regression 
journal american statistical association 
hinton 

training products experts minimizing contrastive divergence 
neural computation 
hinton dayan frey neal 

wake sleep algorithm self organizing neural networks 
science 
lecun bottou haffner 

gradient learning applied document recognition 
proceedings ieee 
lee mumford 

hierarchical bayesian inference visual cortex 
journal optical society america 
marks movellan 

diffusion networks product experts factor analysis 
proc 
int 
conf 
independent component analysis pages 
hinton 

recognizing handwritten digits hierarchical products experts 
ieee transactions pattern analysis machine intelligence 
neal 

connectionist learning belief networks 
artificial intelligence 
neal hinton 

new view em algorithm justifies incremental sparse variants 
jordan editor learning graphical models pages 
kluwer academic publishers 
ning lecun piano bottou 

automatic developing embryos videos 
ieee transactions image processing 
pearl 

probabilistic inference intelligent systems networks plausible inference 
morgan kaufmann san mateo ca 
roth black 

fields experts framework learning image priors 
ieee conf 
computer vision pattern recognition 
sanger 

optimal unsupervised learning single layer linear feedforward neural 
neural networks 
simard platt 

best practice convolutional neural networks applied visual document analysis 
international conference document analysis icdar ieee computer society los alamitos pages 
teh hinton 

rate coded restricted boltzmann machines face recognition 
advances neural information processing systems volume 
teh welling osindero hinton 

energy models sparse overcomplete representations 
journal machine learning research 
welling hinton osindero 

learning sparse topographic representations products student distributions 
becker obermayer editors advances neural information processing systems pages 
mit press cambridge ma 
welling rosen zvi hinton 

exponential family application information retrieval 
advances neural information processing systems pages 
mit press cambridge ma 
tables version mnist task learning algorithm test error permutation invariant generative model permutation invariant support vector machine degree polynomial kernel permutation invariant backprop cross entropy weight decay permutation invariant backprop cross entropy early stopping permutation invariant backprop squared error line updates permutation invariant nearest neighbor examples norm permutation invariant nearest neighbor examples norm permutation invariant nearest neighbor examples norm permutation invariant nearest neighbor examples norm images backprop extra data cross entropy early stopping elastic deformations convolutional neural net virtual svm images extra data degree polynomial kernel pixel transl 
images shape context features hand coded matching images backprop lenet extra data convolutional neural net affine transformations images backprop lenet convolutional neural net table error rates various learning algorithms mnist digit recognition task 
see text details 
appendix complementary priors general complementarity consider joint distribution observables hidden variables likelihood function corresponding family complementary priors distributions joint distribution leads posteriors exactly leads posterior expressed yj 
functional forms likelihood admit complementary prior 
appendix show family constitutes likelihood functions admitting complementary prior exp yj exp yj log normalisation term 
assertion hold need assume positivity distributions value corresponding family complementary priors assume form exp log yj constant ensure normalisation 
combination functional forms leads expression joint exp yj yj prove assertion need show likelihood function form eq 
admits complementary prior complementarity implies functional form eq 

firstly directly verified eq 
complementary prior likelihood functions eq 

show converse assume complementary prior likelihood function 
notice factorial form posterior simply means joint distribution satisfies set conditional independencies yj yk set conditional independencies exactly satisfied undirected graphical model edge hidden variable observed variable observed variables pearl 
hammersley clifford theorem positivity assumption joint distribution form eq 
forms likelihood function eq 
prior eq 
follow 
complementarity infinite stacks consider subset models form eq 
likelihood 
means sets conditional independencies xi yj condition useful construction infinite stack directed graphical models 
identifying conditional independencies eq 
satisfied complete bipartite undirected graphical model hammersley clifford theorem assuming positivity see form fully characterises joint distributions interest exp likelihood functions take form exp xi yj xi yj xi yj xi log immediately obvious marginal distribution observables eq 
expressed infinite directed model parameters defining conditional distributions layers tied 
intuitive way validating assertion follows 
consider methods draw samples marginal distribution implied eq 

starting arbitrary configuration iteratively perform gibbs sampling alternation distributions eq 

run markov chain long positivity assumptions ensure chain mixes properly eventually obtain unbiased samples joint distribution eq 

imagine unroll sequence gibbs updates space consider parallel update variables constitute states separate layer graph 
unrolled sequence states purely directed structure conditional distributions form eq 
alternation 
equivalence gibbs sampling scheme layers unrolled graph adjacent pairs layers joint distribution eq 

formalize intuition unrolling graph follows 
basic idea construct joint distribution unrolling graph upwards moving away data layer successively deeper hidden layers put defined distribution infinite stack variables 
verify simple marginal conditional properties joint distribution show construction obtained unrolling graph downwards deep layer 
sequence stack variables identified original observed hidden variables interpreted sequence successively deeper layers 
define functions exp fx fy gx fy gy fx dummy variables joint distribution sequence variables assuming order markovian dependency follows gx 
gy 
verify induction distribution marginal distributions fx 
fy 
definition distribution eq 
eqs 

wehave fy fy fx similarly 
see downward conditional distributions hold true gx gy joint distribution stack variables gives unrolled graph downward direction conditional distributions eq 
generate sample downwards pass markov chain mixes 
inference infinite stack directed graphs equivalent inference joint distribution sequence variables 
words simply definition joint distribution eqs 
obtain sample posterior simply sampling 
directly shows inference procedure exact unrolled graph 
pseudo code algorithm matlab style pseudo code implementation algorithm described section back fitting 
method contrastive version wake sleep algorithm hinton 
code outlined assumes network type shown visible inputs label nodes layers hidden units 
applying algorithm perform layer wise greedy training described sections 
algorithm data biases row vectors 
generative model lab top pen hid vis number units layer foo weight matrices names rec recognition biases gen generative biases 
simplicity learning rate 
perform bottom pass get wake positive phase probabilities sample states logistic data rand logistic rand logistic targets rand positive phase statistics contrastive divergence targets perform gibbs sampling iterations top level undirected associative memory initialize loop iter logistic rand softmax logistic rand negative phase statistics contrastive divergence starting gibbs sampling run perform top generative pass get sleep negative phase probabilities sample states logistic rand logistic predictions logistic logistic logistic logistic updates generative parameters data data updates top level associative memory parameters targets updates recognition inference approximation parameters 
