learning act real time dynamic programming lambda andrew barto steven bradtke satinder singh department computer science university massachusetts amherst ma march lambda authors rich yee vijay gullapalli brian jonathan bachrach helping clarify relationships heuristic search control 
rich sutton chris watkins paul werbos ron williams sharing fundamental insights subject numerous discussions rich sutton making aware korf research thoughtful comments manuscript 
grateful dimitri bertsekas steven sullivan independently pointing error earlier version article 
harry klopf insight persistence encouraged interest class learning problems 
research supported barto national science foundation ecs ecs air force office scientific research afb afosr 
learning methods dynamic programming dp receiving increasing attention artificial intelligence 
researchers argued dp provides appropriate basis compiling planning results reactive strategies real time control learning strategies system controlled incompletely known 
introduce algorithm dp call real time dp rtdp embedded system improve performance experience 
rtdp generalizes korf learning real time algorithm problems involving uncertainty 
invoke results theory asynchronous dp prove rtdp achieves optimal behavior different classes problems 
theory asynchronous dp illuminate aspects dp reinforcement learning methods watkins learning algorithm 
secondary aim article provide bridge ai research real time planning learning relevant concepts algorithms control theory 
increasing interest artificial intelligence ai researchers systems embedded environments demanding real time performance narrowing gulf problem solving control engineering 
similarly machine learning techniques suited embedded systems comparable methods adaptive control dynamic systems 
growing number researchers investigating learning systems dynamic programming dp algorithms solving stochastic optimal control problems arguing dp provides appropriate basis compiling planning results reactive strategies real time control learning strategies system controlled incompletely known 
learning algorithms dp employ novel means improving computational efficiency conventional dp algorithms 
werbos watkins proposed incremental versions dp learning algorithms sutton dyna architecture learning planning reacting principles 
key issue addressed dp learning tradeoff short long term performance agent learn improve long term performance may require sacrificing short term performance 
dp learning examples reinforcement learning methods autonomous agents improve skills environments contain explicit teachers 
article introduce learning algorithm dp call real time dynamic programming rtdp embedded problem solving system improve long term performance experience prove results behavior different types problems 
rtdp result recognizing korf learning realtime lrta algorithm closely related form dp known asynchronous dp 
novel observation permits generalize ideas lrta apply real time problem solving tasks involving uncertainty 
particular apply theory asynchronous dp developed bertsekas bertsekas tsitsiklis show rtdp converges optimal solutions applied types real time problem solving tasks involving uncertainty 
theory asynchronous dp motivated suitability asynchronous dp parallel processing adapt theory case performing dp concurrently problem solving control 
extension rtdp called adaptive rtdp applicable information lacking problem structure addition solution 
recognizing theory asynchronous dp relevant learning permits provide new insight watkins learning algorithm dp learning algorithm explored ai researchers 
simulation results comparing performance rtdp adaptive rtdp learning conventional dp term real time usage korf refers problems actions performed hard time constraints 
address details scheduling issues arise algorithms components complex real time systems 
algorithm simulated real time problem solving tasks involving uncertainty 
aim article discuss important issues arise dp learning algorithms particular attention devoted indicating aspects formal justification 
doing attempt clarify links ai research real time planning learning relevant concepts control theory 
discuss selected concepts control theory believe relevant efforts ai develop autonomous systems performing real time uncertainty 
remain issues relevant dp learning ai discuss 
example adopt formalism say best apply problems interest ai 
formalism potentially applicable wide variety specific problems easy specify exactly subproblems complex systems best take advantage methods 
accord dean wellman regard dp reinforcement learning component technology addresses issues important developing sophisticated embedded agents address 
reader familiar contributing lines research provide necessary background section followed section discussion proper relationship concepts ai control theory 
development theoretical material occupies sections class stochastic optimal control problems occupying section conventional dp occupying section 
major parts theoretical development 
part sections concerns problems accurate models available 
describe rtdp convergence properties relationship lrta 
second part section concerns additional complexity case incomplete information accurate model problem lacking 
section brief dp learning algorithms outside theoretical scope article 
section discuss issues practical implementations dp learning algorithms address 
section example problem illustrate rtdp algorithms 
conclude section appraisal significance approach discuss open problems 
background major influence research leading current dp algorithms method samuel modify heuristic evaluation function game checkers 
method updated board evaluations comparing evaluation current board position evaluation board position arise game 
attempting score calculated current board position look calculated terminal board position chain moves probably occur actual play 
samuel 
result process backing board evaluations evaluation function improve ability evaluate long term consequences moves 
version algorithm samuel represented evaluation function weighted sum numerical features adjusted weights error derived comparing evaluations current predicted board positions 
compatibility connectionist learning algorithms approach refined extended sutton heuristically number single agent problem solving tasks barto sutton anderson anderson sutton :10.1.1.132.7760
algorithm implemented neuron connectionist element called adaptive critic element 
sutton called algorithms temporal difference td methods obtained theoretical results convergence :10.1.1.132.7760
proposals klopf sutton barto developed methods models animal learning 
minsky discussed similar ideas context credit assignment problem reinforcement learning systems independently developed ideas related animal behavior christensen korf experimented samuel method updating evaluation function coefficients linear regression holland bucket brigade algorithm assigning credit classifier systems closely related samuel method :10.1.1.79.7413
tesauro td gammon program td method connectionist network improve performance playing backgammon achieved remarkable success 
independently approaches inspired samuel checkers player researchers suggested similar algorithms theory optimal control dp provides important solution methods 
applied control problems dp term introduced bellman consists methods successively approximating optimal evaluation functions decision rules deterministic stochastic problems 
general form dp applies optimization problems costs objects search space compositional structure exploited find object globally minimum cost performing exhaustive search 
kumar kanal discuss dp level generality 
restrict attention dp applies problems objects state sequences generated problem solving control tasks 
dp solves optimization problems solving recurrence relations explicitly searching space state sequences 
backing state evaluations basic step dp procedures solving recurrence relations 
discuss dp algorithms detail section 
dp algorithms avoid exhaustive search state sequence space exhaustive ai standards require repeated generation expansion possible states 
reason dp played significant role ai 
heuristic search algorithms contrast explicitly designed avoid exhaustive way 
dp algorithms relevant learning way heuristic search algorithms systematically update evaluations states effect adjust problem heuristic evaluation function incorporating results repeated shallow searches 
heuristic search algorithms update estimates costs reach states initial state function typically update heuristic evaluation function estimating cost reach goal state function despite fact dp algorithms exhaustive sense described possible arrange computational steps control real time problem solving 
basis rtdp algorithms describe article 
cases convergence optimal evaluation function requires repeated generation expansion states performance improves incrementally necessarily monotonically accomplished 
improvement ultimate convergence optimality central 
perspective taken werbos proposed method similar adaptive critic element framework dp 
called approach heuristic dynamic programming written extensively refs 

related algorithms discussed witten watkins extended sutton td algorithms developed explicitly utilizing theory dp 
term incremental dynamic programming refer class algorithms discussed examples 
williams baird theoretically analysed additional dp algorithms suitable time application 
come ferguson independently proposed method similar adaptive rtdp 
sutton barto williams discussed reinforcement learning perspective dp adaptive control white jordan barto provide additional background extensive current research 
aspects approach apply problems involving continuous time state action spaces restrict attention discrete time problems finite sets states actions relative simplicity closer relationship non numeric problems usually studied ai 
excludes various differential approaches optimization algorithms related connectionist algorithm jacobson mayne jordan jacobs werbos white jordan 
relevance dp planning learning ai articulated sutton dyna architecture 
key idea dyna perform computational steps dp algorithm information obtained state transitions taken exceptions heuristic search literature algorithms proposed er 
algorithms dp backups update heuristic evaluation functions developed independently dp 
system controlled hypothetical state transitions simulated model system 
satisfy time constraints approach interleaves phases acting planning performed hypothetical state transitions 
underlying dp algorithm compiles resulting information efficient form directing course action 
aspect dyna system model refined learning process deriving training information state transitions observed control 
line model refinement executing dp algorithm concurrently generation actions implications planning ai discussed sutton ref 

article introduce fact theory asynchronous dp applicable analysis dp reinforcement learning algorithms 
asynchronous dp algorithms differ conventional dp algorithms proceed systematic exhaustive sweeps problem state set 
bertsekas bertsekas tsitsiklis proved general theorems convergence asynchronous dp applied discretetime stochastic optimal control problems 
motivated suitability asynchronous dp parallel processing relate results real time variants dp article 
best knowledge explicit theory asynchronous dp real time control ferguson 
korf lrta algorithm heuristic search algorithm caches state evaluations search performance improves repeated trials 
evaluations states visited problem solver maintained hash table 
cycle algorithm proceeds expanding current state generating immediate successor states evaluating previously stored evaluations exist hash table initially heuristic evaluation function 
assuming objective find path goal state score computed neighboring state adding evaluation cost edge current state 
minimum resulting scores new evaluation current state stored hash table move lowest scoring neighboring state 
lrta backs state evaluations way samuel algorithm dp 
fact shall see follows slight caveat lrta deterministic specialization asynchronous dp applied line 
korf related real time rta algorithm second smallest score stored 
lrta closely related control dp rta discuss rta 
heuristic search control dynamic systems ai focused problems having relatively little mathematical structure control theorists studied restrictive classes problems developed correspondingly detailed theories 
concepts methods control theory relevant problems interest ai discussed example dean wellman 
section prelude introducing stochastic optimal control framework results cast discuss relationship heuristic search real time heuristic search selected concepts control theory heuristic search system control heuristic search algorithms apply state space search problems defined set states set operators map states states initial state set goal states 
objective find sequence operators maps initial state goal states possibly optimizes measure cost merit solution path 
components constitute model real problem solving puzzle proving theorem planning robot path 
term control literature heuristic search problem solving means process deciding manipulating model problem question 
despite similarities meaning term control control theory refers process manipulating behavior physical system real time supplying appropriate input signals 
ai control specifies formal search process control theory steers behavior physical system time 
models manipulated search algorithms physical systems set immediately arbitrary states suspend activity await controller decisions 
models formalize system control problems called dynamic systems explicit account passage time 
follows control mean control dynamic systems control search 
applications symbolic representation sequence operators final objective heuristic search algorithm 
intent may execute operator sequence generate time sequence actual inputs physical system 
result control engineer form control control method differs substantially methods addressed control theory 
sequence inputs actions produced way heuristic search open loop control policy meaning applied system information system actual behavior control underway execution monitoring feedback 
terms control theory heuristic search control design procedure producing open loop control policy system model policy appropriate initial state 
normal circumstances line design procedure completed control system normal circumstances planning phase problemsolving process strictly precedes execution phase 
open loop control works fine true model determine control policy completely accurate model physical system physical system initial state exactly determined physical system deterministic unmodeled disturbances 
conditions hold problems studied ai true realistic control problems 
uncertainty behavior physical system process modeling system implies closed loop control produce better performance 
control closed loop action depends current observations real system past observations information internal controller 
closed loop control policy called closed loop control rule law strategy rule specifying action function current possibly past information behavior controlled system 
closely corresponds universal plan discussed example chapman ginsberg schoppers :10.1.1.49.6261
control theory closed loop control policy usually specifies action function controlled system current state just current values observable variables distinction significance universal planning discussed chapman 
control closely associated negative feedback counteracts deviations desired system behavior negative feedback control merely special case closed loop control 
uncertainty closed loop control principle competent open loop control 
deterministic system disturbances policy initial state exists open loop policy produces exactly system behavior open loop policy generated running system simulating perfect model control closed loop policy 
true stochastic case unmodeled disturbances outcome random unmodeled events anticipated designing open loop policy 
note game playing systems closed loop control reason opponent kind disturbance 
game player uses opponent actual previous moves determining move 
exactly reasons closed loop control better open loop control single agent problems involving uncertainty 
corollary explains universal closed loop control control engineers system model designing acceptable control policy significantly faithful actual system designing closed loop open loop policies 
open loop control practical alternative expensive impossible monitor controlled system behavior detail sufficient closed loop control 
control theory addresses problem designing adequate closed loop policies line assumption accurate model system controlled available 
line design procedure typically yields computationally efficient method determining action function observed system state 
possible design complete closed loop policy line control problems studied engineers necessary perform additional re design re planning problem instances differing initial state 
changing control objectives hand require policy re design 
design closed loop policies line repeated line design openloop policies 
approach called receding horizon control 
current state open loop policy designed current state playing role initial state 
design procedure terminate time constraints imposed line operation 
done designing finite horizon open loop policy example model searching fixed depth current state 
applying action specified resulting policy remainder policy discarded design process repeated observed state 
despite requiring line design ai corresponds line planning projection prediction system model receding horizon control produces control policy reactive current system state closed loop policy 
view policy involve explicit planning projection planning phase complete fixed amount time retain system reactivity observed system states 
contrast methods design closed loop policies line receding horizon control react line changes control objectives 
optimal control familiar control objective control system output matches output tracks trajectory closely possible face disturbances 
called regulation tracking problems respectively 
optimal control problem hand control objective function controlled system behavior function need defined terms output trajectory 
typical optimal control problem requires controlling system go initial state goal state minimum cost trajectory 
contrast tracking problems desired trajectory part problem specification trajectory part solution optimal control problem 
optimal control problems closely related problems heuristic search algorithms apply 
specialized solution methods exist optimal control problems involving linear systems quadratic cost functions methods calculus variations yield closed form solutions restricted classes problems 
numerical methods applicable problems involving nonlinear systems costs include gradient methods dp 
gradient methods optimal control closely related gradient descent methods studied connectionists error backpropagation algorithm dp methods closely related heuristic search 
heuristic search algorithm dp line procedure designing optimal control policy 
heuristic search algorithm dp produces optimal closed loop policy open loop policy initial state 
real time heuristic search algorithms real time heuristic search defined korf apply state space search problems underlying model extended account passage time 
model dynamic system 
real time heuristic search algorithms apply state space search problems additional properties time unique current state system controlled known searcher controller sequence time intervals constant bounded duration searcher controller commit unique action choice operator system changes state time interval manner depending current state searcher controller action 
factors imply fixed upper bound amount time searcher controller take deciding action action date state information 
traditional heuristic search algorithm design procedure open loop policy real time heuristic search algorithm control procedure accommodate possibility closed loop control 
korf lrta algorithm kind receding horizon control online method designing closed loop policy 
receding horizon control studied control engineers lrta accumulates results local design procedure effectiveness resulting closed loop policy tends improve time 
stores information shallow searches forward current state updating evaluation function control decisions 
updates basic steps dp view lrta result interleaving steps dp actual process control control policy design occurs concurrently control 
approach advantageous control problem large unstructured mathematically complete control design feasible line 
case requires partial closed loop policy policy useful subregion problem state space 
designing partial policy line allows actual experience influence subregion state space design effort concentrated 
design effort expended parts state space visited actual control 
general possible design policy optimal subset states design procedure considers entire state set possible certain conditions required korf convergence theorem lrta 
adaptive control control theorists term adaptive control cases accurate model system controlled available designing policy line 
called control problems incomplete information 
adaptive control algorithms design policies line information control problem accumulates time controller system interact 
distinction adaptive control learning control takes advantage repetitive control experiences information acquired useful long term 
distinction may useful types control problems think utility limited applied kinds problems algorithms consider article 
mean adaptive control article algorithms lrta samuel algorithm learning algorithms adaptive control algorithms assume existence accurate model problem solved 
certainly odd control algorithm learns facto adaptive forced adopt control engineer restricted definition adaptive control 
section describe algorithms properties learning adaptive control algorithms 
markovian decision problems basis theoretical framework class stochastic optimal control problems called markovian decision problems 
simplest class problems general include stochastic versions problems heuristic search algorithms apply time allowing borrow developed control operations research literature 
frameworks include stochastic problems important due uncertainty applications fact presence uncertainty gives closed loop reactive control advantages open loop control 
markovian decision problem operators take form actions inputs dynamic system probabilistically determine successor states 
action primitive theory important understand applications action high level command executes repertoire complex behaviors 
problems practical importance formulated markovian decision problems extensive treatment theory application framework books bertsekas ross 
markovian decision problem defined terms discrete time stochastic dynamic system finite state set ng 
time represented sequence time steps 
section introducing rtdp treat sequence specific instants real time best treat merely sequence 
time step controller observes system current state selects control action simply action executed applied input system 
observed state action selected finite set admissible actions 
controller executes action system state time step state transition probability pij 
assume application action state incurs immediate cost ci necessary refer states actions immediate costs time steps occur st ut ct denote respectively state action immediate cost time step ut st ct cst ut 
discuss significant extension formalism controller observe current state complete certainty 
possibility studied extensively important practice complexities introduces scope article 
closed loop policy specifies action function observed state 
policy denoted controller executes action observes state stationary policy change time 
article term policy mean stationary policy 
policy function called evaluation function cost function corresponding policy assigns state total cost expected accumulate time controller uses policy starting state 
policy state define expected value infinite horizon discounted cost accrue time controller uses policy initial state fl fl factor discount immediate costs expectation assuming controller uses policy refer simply cost state policy immediate cost state policy ci cost state policy expected discounted sum immediate costs incurred starting state theorists study markovian decision problems types evaluation functions function giving average cost time step consider formulations 
objective type markovian decision problem consider find policy minimizes cost state defined equation 
policy achieves objective optimal policy depends fl unique denote lambda lambda lambda 
optimal policy corresponds evaluation function optimal evaluation function optimal cost function denoted lambda control theory simply called control 
term action term commonly ai 
general alternatively regard immediate costs bounded random numbers depending states actions 
case ci denotes expected immediate cost application action state theory discussed remains unchanged 
lambda optimal policy lambda lambda state lambda optimal cost state possible cost state policy 
infinite horizon discounted version markovian decision problem simplest mathematically discounting ensures costs states finite policy optimal policy stationary discount factor fl determines strongly expected costs influence current control decisions 
fl cost state just immediate cost transition state 
equation js ci 
case optimal policy simply selects actions minimize immediate cost state optimal evaluation function just gives minimum immediate costs 
fl increases costs significant determining optimal actions solution methods generally require computation 
fl undiscounted case cost state equation need finite additional assumptions required produce defined decision problems 
consider set assumptions undiscounted case resulting decision problems closely related problems heuristic search applied 
problems bertsekas tsitsiklis call stochastic shortest path problems thinking immediate costs arc lengths graph nodes correspond states absorbing set states set states entered left immediate cost associated applying action states absorbing set zero 
assumptions imply infinite horizon evaluation function policy system absorbing set assigns finite costs state fl 
true finite number immediate costs incurred policy time zero 
additionally discounted case optimal policy stationary 
absorbing set states corresponds set goal states deterministic shortest path problem call goal set 
tasks typically solved heuristic search objective find optimal closed loop policy just optimal path initial state 
ai researchers studying reinforcement learning focus shortest path problems immediate costs zero goal state reached reward delivered controller new trial begins 
special kinds stochastic shortest path problems address issue delayed reinforcement particularly stark form 
rewards correspond negative costs formalism 
discounted case rewards magnitude optimal policy produces shortest path rewarding state 
example stochastic shortest path problem receiving attention identical non rewarding immediate costs positive value zero 
case optimal policy produces finite horizon problems optimal policies generally nonstationary different actions optimal state depending actions remain horizon reached 
shortest path goal state undiscounted case 
problems examples minimum time optimal control problems 
example race track problem illustrate markovian decision framework formalize game called race track described martin gardner simulates automobile racing 
modify game section compare performance various dp learning algorithms considering single car making probabilistic 
race track shape drawn graph starting line finish line consisting designated squares 
square boundary track possible location car 
shows example tracks 
start sequence trials car placed starting line random position moves car attempts move track finish line 
acceleration deceleration simulated follows 
previous move car moved squares horizontally squares vertically move squares vertically squares horizontally difference gamma difference gamma 
means car maintain speed dimension slow speed dimension square move 
car hits track boundary move back random position starting line reduce velocity zero gamma gamma considered zero continue trial 
objective learn control car crosses finish line moves possible 
figures show examples optimal near optimal paths race tracks shown 
addition difficulty discovering faster ways reach finish line easy car gather speed negotiate track curves 
matters worse introduce random factor problem 
probability actual accelerations move zero independently intended accelerations 
gamma probability controller intended actions executed 
think simulating driving track unpredictably slippery braking throttling effect car velocity 
race track problem suggestive robot motion navigation problems intention formulate way best suited design autonomous vehicle realistic sensory motor capabilities 
regard representative example problems requiring learning difficult skills formulate entire task stochastic shortest path problem 
step formulation define dynamic system controlled 
state system time step represented quadruple integers st xt yt xt yt 
integers computational experiments described section means projected path car move intersects track boundary place finish line 
starting line finish line starting line finish line example race tracks 
panel small race track 
panel larger race track 
see table details 
horizontal vertical coordinates car location second integers speeds horizontal vertical directions 
xt xt gamma xt gamma horizontal speed car time step similarly yt yt gamma yt gamma assume gamma gamma 
set admissible actions state set pairs ux uy ux uy set gamma 
ut denote action time closed loop policy assigns admissible action state action time step st gamma gamma gamma gamma gamma gamma equations define state transitions system 
probability gamma controller action reliably executed state time step xt xt xt yt yt yt xt xt yt yt probability system ignores controller action state time step xt xt xt yt yt yt xt xt yt yt assumes straight line joining point xt yt point xt yt lies entirely track intersects finish line 
case car collided track boundary state randomly chosen position starting line 
move takes car finish line treated valid move assume car subsequently stays resulting state new trial begins 
method keeping car track equations define state transition probabilities states admissible actions 
complete formulation stochastic shortest path problem need define set start states set goal states immediate costs associated action state 
set start states consists zero velocity states starting line states coordinates squares making starting line 
set goal states consists states reached time step crossing finish line inside track 
state transition function defined set absorbing 
immediate cost non goal states independently action taken ci non goal states admissible actions immediate cost associated transition goal state zero 
restrict attention policies guaranteed take car finish line need discounting 
policy undiscounted infinite horizon cost state expected number moves car cross finish line state controlled policy optimal policy minimizes cost state policy car expected cross finish line quickly possible starting state 
optimal cost state lambda smallest expected number moves finish line 
total number states depends configuration race track imposed limit car speed potentially infinite 
set states reached set start states policy finite considered state set stochastic shortest path problem 
optimality equation set stage discussing dp provide detail relationship policies evaluation functions 
evaluation function gives cost state policy necessarily select actions lead best successor states evaluated words necessarily greedy policy respect evaluation function 
define greedy policy stochastic case watkins notation plays role learning method described section 
realvalued function states may evaluation function policy guess evaluation function heuristic evaluation function heuristic search arbitrary function 
state action qf ci fl pij qf cost action state evaluated sum immediate cost discounted expected value costs possible successor states action system state transitions deterministic equation simplifies qf ci flf successor state action node child node edge corresponding operator 
deterministic case think qf summary result ply lookahead node edge corresponding operator evaluated stochastic case requires generalization view edges correspond operator having different probability followed 
evaluation function policy qf gives cost generating action state policy 
values policy greedy respect states action satisfying qf minu greedy policy respect action minimizes set values state denote policy greedy respect note policy greedy respect different evaluation functions 
key fact underlying dp methods policies greedy respect evaluation functions optimal policies 
lambda optimal policy evaluation function optimal evaluation function lambda lambda lambda means state lambda satisfies qf lambda lambda min lambda furthermore policy greedy respect lambda optimal policy 
lambda known possible define optimal policy simply defining satisfy equation defining greedy respect lambda due definition values equation generalizes stochastic case fact optimal policy policy best respect lambda determined ply search current state 
deeper search necessary lambda summarizes information search obtain 
letting lambda qf lambda simplify notation related key fact necessary sufficient condition lambda optimal evaluation function state true lambda minu lambda minu ci fl xj pij lambda form bellman optimality equation solved lambda dp algorithm 
set number states simultaneous nonlinear equations 
form equations depends dynamic system immediate costs underlying decision problem 
lambda optimal action state determined follows 
values lambda admissible actions determined equation 
general takes mn computational steps number states number admissible actions state knows state transition probabilities state zero usually deterministic case amount computation deterministic case 
computing values amounts ply lookahead search state requires knowledge system state transition probabilities 
values optimal action determined equation takes gamma comparisons 
computational complexity finding optimal action method dominated complexity finding lambda complexity dp algorithm 
dynamic programming complete accurate model markovian decision problem form knowledge state transition probabilities pij immediate costs ci states actions possible principle solve decision problem line applying various known dp algorithms 
describe versions basic dp algorithm called value iteration 
dp algorithms including called policy iteration learning algorithms scope article briefly discuss policy iteration section 
treat dp referring value iteration noted 
solving markovian decision problems value iteration successive approximation procedure converges optimal evaluation function lambda successive approximation method solving bellman optimality equation basic operation backing estimates optimal state costs 
variations value depending computations organized 
describe version applies backup operations synchronously 
synchronous dynamic programming fk denote estimate lambda available stage dp computation 
stage fk estimated optimal cost state refer simply stage cost state similarly refer fk stage evaluation function may evaluation function policy 
index stages dp computation denote time step control problem solved 
synchronous dp fk defined terms fk follows state fk minu ci fl xj pij fk minu fk initial estimate lambda refer application update equation state backing cost 
backing costs common operation variety search algorithms ai mean backed cost saved 
backed cost saved updating evaluation function 
iteration defined equation synchronous values fk appear right hand side equation 
imagines having separate processor associated state applying equation states means processor backs cost state time old costs states supplied processors 
process updates values fk simultaneously 
alternatively sequential implementation iteration requires temporary storage locations stage costs computed stage costs 
sequential ordering backups irrelevant result 
states largest number admissible actions state iteration consists backing cost state exactly requires mn operations stochastic case mn operations deterministic case 
large state sets typical ai control problems desirable try complete iteration repeat process converges lambda example backgammon states single iteration value iteration case take years mips processor 
fl repeated synchronous iterations produce sequence functions converges optimal evaluation function lambda initial estimate 
cost state need get closer optimal cost iteration maximum error fk lambda states decrease ref 

synchronous dp line versions value iteration discuss generates sequence functions converges lambda fl explicitly generate sequence policies 
stage evaluation function corresponds greedy policy policies explicitly formed 
ideally wait sequence converges lambda form greedy policy corresponding lambda optimal policy 
possible practice value iteration converges asymptotically 
executes value iteration meets test approximate convergence forms policy resulting evaluation function important note function sequence evaluation functions generated value iteration closely approximate lambda order corresponding greedy policy optimal policy 
policy corresponding stage evaluation function may optimal long algorithm converges lambda unaided computations value iteration detect happens 
fact important reason line variants value iteration discuss article advantages line variants 
controller uses policy defined current evaluation function perform optimally evaluation function converges optimal evaluation function 
bertsekas bertsekas tsitsiklis give conditions ensuring convergence synchronous dp stochastic shortest path problems undiscounted case fl 
terminology policy proper implies nonzero probability eventually reaching goal set starting state 
proper policy implies goal set reached eventually state probability 
existence proper policy generalization stochastic case existence path initial state goal set 
synchronous dp converges lambda undiscounted stochastic shortest path problems conditions 
initial cost goal state zero 
proper policy 
policies proper incur infinite cost state 
third condition ensures optimal policy proper rules possibility cost path exists reaches goal set 
condition policy iteration contrast explicitly generates sequence policies converges optimal policy finite number iterations finite number states admissible actions assuming 
policy iteration shortcomings discuss section 
true immediate costs transitions non goal states positive ci non goal states actions deterministic case conditions satisfied solution path state sum immediate costs loop positive 
gauss seidel dynamic programming gauss seidel dp differs synchronous version costs backed state time sequential sweep states computation state costs states 
assume states numbered order sweep proceeds order result iteration gauss seidel dp written follows state fk minu ci fl xj pij minu fk 
synchronous dp order states costs backed influences computation 
gauss seidel dp converges lambda conditions synchronous dp converges 
fl repeated gauss seidel sweeps produce sequence functions converges lambda undiscounted stochastic shortest path problems conditions described ensure convergence synchronous dp ensure convergence gauss seidel dp 
cost backup uses latest costs states gauss seidel dp generally converges faster synchronous dp 
furthermore clear state orderings produce faster convergence depending problem 
example shortest path problems sweeping goal states backwards shortest paths usually leads faster convergence sweeping forward direction 
gauss seidel dp algorithms direct interest article solve example problem described section serves bridge synchronous dp asynchronous form discussed 
assumption positive immediate costs weakened nonnegativity ci exists optimal proper policy 
asynchronous dynamic programming asynchronous dp similar gauss seidel dp back state costs simultaneously 
organized terms systematic successive sweeps state set 
proposed bertsekas developed bertsekas tsitsiklis asynchronous dp suitable multi processor systems communication time delays common clock 
state separate processor dedicated backing cost state generally processor may responsible number states 
times processor backs cost state different processor 
back cost state processor uses costs states available awakens perform backup 
multiprocessor implementations obvious utility speeding dp practical significance algorithms discuss see 
interest asynchronous dp lies fact require state costs backed systematically organized fashion 
full asynchronous model notion discrete computational stages apply processor awaken continuum times notion iteration stage facilitates discussion rtdp section 
forms dp fk denote estimate lambda available stage computation 
stage costs subset states backed synchronously costs remain unchanged states 
subset states costs backed changes stage stage choice subsets determines precise nature algorithm 
sk set states costs backed stage fk computed follows fk minu fk sk fk 
algorithm fk may differ fk state states possibly depending sk 
gauss seidel dp costs states may backed times costs backed 
asynchronous dp includes synchronous gauss seidel algorithms special cases synchronous dp results sk gauss seidel dp results sk consists single state collection sks defined implement successive sweeps entire state set sn gamma fng sn sn 
discounted asynchronous dp converges lambda provided cost state backed infinitely provided state contained infinite number subsets sk 
practice means strategy selecting states cost backups eliminate state possible selection 
undiscounted case fl additional assumptions necessary ensure convergence 
follows result bertsekas tsitsiklis asynchronous dp converges undiscounted stochastic shortest path problems cost state backed infinitely conditions section convergence synchronous dp met initial cost goal state zero proper policy policies proper incur infinite cost state 
important realize single backup state cost dp necessarily improve estimate state optimal cost may fact worse 
appropriate conditions cost state converges optimal cost repeated backups 
gauss seidel dp order states costs backed influence rate convergence problem dependent way 
fact underlies utility various strategies teaching dp learning algorithms supplying experience dictating selected orderings backups lin utgoff clouse whitehead 
dynamic programming real time dp algorithms described line algorithms solving markovian decision problems 
successively approximate optimal evaluation function sequence stages stages related time steps decision problem solved 
consider algorithms controller performs asynchronous dp concurrently actual process control concurrently process executing actions 
concurrent dp control processes interact follows control decisions date information dp computation state sequences generated control influence selection states dp backup operation applied estimated costs stored 
asynchronous version dp appropriate role due flexibility stages defined 
consequence interaction controller automatically uses intermediate results dp computation guide behavior dp computation focus regions state set relevant control revealed system behavior 
algorithm call real time dp rtdp results interaction specific characteristics 
section assume complete accurate model decision problem case sutton discusses relation planning ai 
section discuss adaptive case complete accurate model decision problem available 
model decision problem concurrent execution dp control carried simulation mode model surrogate actual system underlying decision problem 
result novel line dp computation computational advantages conventional line dp due ability focus relevant parts state set 
despite real time computation regard concurrent execution dp control simulation mode form learning 
fact learning accomplished programs samuel tesauro 
learning occurred simulated games learning systems competed 
emphasize real time dp learning algorithms reader aware discussion applies algorithms simulation mode 
describe concurrent execution dp control think time steps discrete time formulation markovian decision problem indices sequence instants real time controller execute control actions 
st state observed time kt total number asynchronous dp stages completed time latest estimate optimal evaluation function available controller select action ut st 
controller executes ut incurs immediate cost cst ut system state changes st 
time action ut selected additional stages asynchronous dp stages completed yield bt denote set states costs backed stages 
note states bt costs backed stages 
real time dp rtdp refers cases concurrently executing dp control processes influence follows 
controller follows policy greedy respect estimate lambda means ut greedy action respect 
ties selecting actions resolved randomly way ensures continuing selection greedy actions 
second execution ut ut cost st backed st bt simplest case bt cost st backed time step generally bt contain states addition st generated type lookahead search 
example bt consist states generated exhaustive search st forward fixed search depth consist states generated search best say rtdp converges associated asynchronous dp computation converges lambda controller takes actions greedy respect current estimate lambda rtdp converges optimal control performance attained conditions described section ensuring asynchronous dp converges lambda apply executed concurrently control 
consequently discounted case condition required convergence rtdp state completely ruled having cost backed 
rtdp backs cost current optimal policy controller continue switch optimal policies rtdp continues select greedy actions 
results nonstationary optimal policy different optimal actions taken state different occasions 
state way achieve sure controller continues visit state 
approaches ensuring 
approach assume done engineering literature markov process resulting policy ergodic 
means nonzero visiting state matter actions executed 
discounted rtdp converges assumption 
assumption unsatisfactory stochastic shortest path problems allow proper subsets states absorbing satisfied trivial stochastic shortest path problem state goal state 
second way ensure state visited infinitely multiple trials 
trial consists time interval nonzero bounded duration rtdp performed 
interval system set new starting state new trial begins obviously method impossible set system state selected start states problems approach possible possible rtdp simulation mode 
trial rtdp initial states trials selected state selected infinitely infinite series trials obviously state visited infinitely start infinite number trials 
simple way accomplish start trial randomly selected state state nonzero selected 
trial rtdp mean rtdp trials initiated state probability start state infinitely infinite series trials 
theorem immediate result noting discounted case trial rtdp gives rise convergent asynchronous dp computation probability method terminating trials theorem discounted markov decision problem defined section initial evaluation function trial rtdp converges probability 
natural trial rtdp undiscounted stochastic shortest path problems trials terminate goal state reached predetermined number time steps 
trial rtdp gives rise convergent asynchronous dp computation probability undiscounted stochastic shortest path problems conditions enumerated section result theorem undiscounted stochastic shortest path problems trial rtdp converges probability conditions initial cost goal state rtdp interrupted trial cost state trial influenced cost starting state trial 
prevents state transitions caused trainer influencing evaluation function 
zero proper policy policies proper incur infinite cost state 
trial rtdp interesting relax requirement yield complete optimal evaluation function complete optimal policy 
consider approach solving undiscounted stochastic shortest path problems designated subset start states trials start 
say state relevant start state optimal policy exist reached state controller uses policy 
suffices find policy optimal restricted relevant states states irrelevant states occur optimal policy 
knew states relevant apply dp just states possibly save considerable amount time space 
clearly possible knowing states relevant requires knowledge optimal policies seeking 
certain conditions continuing back costs irrelevant states trial rtdp converges function equals lambda relevant states controller policy converges policy optimal relevant states 
costs irrelevant states may backed 
memory estimated state costs allocated incrementally trials exhaustive memory requirement conventional dp avoided trial rtdp tends focus computation set relevant states eventually restricts computation set 
conditions possible stated precisely theorem proof appendix theorem undiscounted stochastic shortest path problems trial rtdp initial state trial restricted set start states converges probability lambda set relevant states controller policy converges optimal policy possibly nonstationary set relevant states conditions initial cost goal state zero proper policy immediate costs incurred transitions non goal states positive ci non goal states actions initial costs states non overestimating lambda states condition satisfied simply setting significance theorem gives conditions policy optimal relevant states achieved continuing devote computational effort backing costs irrelevant states 
conditions rtdp yield optimal policy trials allowed time goal state reached possible eliminate requirement proper policy exists 
timing prevents getting stuck fruitless cycles time period extended systematically ensure long optimal paths followed interruption 
state action sets large feasibly apply conventional dp algorithms amount computation saved clearly depend characteristics problem solved branching structure 
rtdp applied line simulation mode evaluation function changes greedy policy shows improvement controller automatically takes advantage improvement 
occur evaluation function close lambda discounted undiscounted problems eventual convergence rtdp depend critically choice states costs backed execution actions cost current state backed judicious selection states accelerate convergence 
sophisticated exploration strategies implemented selecting states prior knowledge information contained current evaluation function 
example trial approach stochastic shortest path problem guided exploration reduce expected trial duration helping controller find goal states 
sense rtdp back costs states current costs accurate estimates optimal costs successor states accurate current costs 
techniques teaching dp learning systems suggesting certain back ups refs 
rely fact order costs states backed influence rate convergence asynchronous dp applied line 
promising approach developed peng williams moore atkeson authors call prioritized sweeping directs application dp backups predecessors states costs change significantly 
exploration objective facilitate finding optimal policy complete model decision problem distinguished exploration designed facilitate learning model decision problem available 
discuss objective exploration section 
rtdp lrta theorem generalization korf convergence theorem lrta 
rtdp extends lrta ways generalizes lrta stochastic problems includes option backing costs states time intervals execution actions 
notation simplest form lrta operates follows determine action ut st controller backs cost st setting ft st minimum values cst gamma actions st st successor action ft gamma current cost costs states remain 
controller inputs minimizing action system observes st repeats process 
note bt lrta kt equals form lrta special case rtdp applied deterministic problem bt 
differs special case way 
rtdp executes action greedy respect ft lrta executes action greedy respect ft gamma 
usually inconsequential difference lrta ft differ ft gamma st st successor 
lrta saves computation requiring minimization time step minimization required perform backup gives greedy action 
general case rtdp backs state cost time interval sense latest estimate lambda select action 
extended form lrta related rtdp 
discussion korf assumes evaluation state may augmented look ahead search 
means costs ft gamma st successor states lrta perform line forward search st depth determined amount time computational resources available 
applies evaluation function ft gamma frontier nodes backs costs st immediate successors 
done roughly setting backed cost state generated forward search minimum costs successors korf procedure 
backed costs successor states update ft gamma st described costs backed costs states generated forward search saved 
despite fact backed costs states computed new evaluation function ft differs old st limits space constraints sense store backed costs states possible especially controller experience multiple trials different starting states 
contrast lrta rtdp save backed costs executing defined stages asynchronous dp 
specifically saving backed costs produced korf procedure corresponds executing number stages asynchronous dp equal depth forward search tree 
stage synchronously backs costs immediate predecessors frontier states current costs frontier states second stage backs costs states immediate predecessors states additional stage dp back cost st completes computation 
procedure apply stochastic case suggests stages asynchronous dp useful 
stages back costs states forward search tree back costs states tree 
example noting general forward search generate graph cycles multiple backups costs states improve information contained 
possibilities basically different instances rtdp converge conditions described theorems 
repeated trials information accumulating developing estimate optimal evaluation function improves control performance 
consequently lrta rtdp learning algorithms suggested name chosen korf 
directly apply adaptive control problems term control theory applies problems complete accurate model system controlled lacking 
section discuss rtdp adaptive control problems 
adaptive control versions value iteration described synchronous gauss seidel asynchronous real time require prior knowledge system underlying markovian decision problem 
require knowledge state transition probabilities pij states actions require knowledge immediate costs ci states actions 
system deterministic means know successor states immediate costs admissible actions state 
finding approximating optimal policy knowledge available known markovian decision problem incomplete information solution methods problems examples adaptive control methods major classes adaptive methods markovian decision problems incomplete information 
bayesian methods rest assumption known priori probability distribution class possible stochastic dynamic systems 
observations accumulate distribution revised bayes rule 
actions selected dp find policy minimizes expected cost set possible systems time 
non bayesian approaches contrast attempt arrive optimal policy asymptotically system pre specified class systems 
actions may optimal basis prior assumptions accumulated observations policy approach optimal policy limit experience accumulates 
kumar surveys large literature classes methods conveys issues sophistication existing theoretical results 
restrict attention non bayesian methods practical large problems 
types non bayesian methods distinguished 
indirect methods explicitly model dynamic system controlled 
system identification algorithms update parameters values determine current system model time control 
typically control decisions assumption current model true model system control theorists call certainty equivalence principle 
direct methods hand form policies explicit system models 
directly estimate policy information system model evaluation markovian decision problems incomplete information problems incomplete state information controller complete knowledge system state time step control 
called partially observable markovian decision problems despite relevance applications scope article 
function policy determined 
indirect direct methods central issue conflict controlling system exploring behavior order discover control better 
called conflict identification control appears indirect methods conflict conducting exploration achieve model convergence objective eventually optimal policy 
direct methods require exploration involve issues 
adaptive optimal control algorithms require mechanisms resolving problems mechanism universally favored 
approaches rigorous theoretical results available reviewed kumar variety heuristic approaches studied barto singh kaelbling moore schmidhuber sutton watkins thrun thrun oller :10.1.1.17.2654
subsections describe non bayesian methods solving markovian decision problems incomplete information 
methods form basis algorithms proved converge optimal policies describe exploration mechanisms rigor developing theory direction 
call method generic indirect method 
system identification algorithm updates system model time step control conventional dp algorithm executed time step current system model 
method computational complexity severely limits utility representative approaches described engineering literature serves point comparative purposes 
describe indirect method simplest modification generic indirect method takes advantage rtdp 
call method adaptive rtdp 
third method describe direct learning method watkins 
briefly describe hybrid direct indirect methods 
generic indirect method indirect adaptive methods markovian decision problems incomplete information estimate unknown state transition probabilities immediate costs history state transitions immediate costs observed controller system interact 
usual approach define state transition terms parameter contained parameter space theta pair states action state transition probability corresponding parameter theta functional dependence known form 
usually assumes lambda theta true parameter pij lambda 
identification task estimate lambda experience 
common approach takes estimate lambda time step parameter having highest probability generating observed history maximum likelihood estimate lambda simplest form approach identification assume unknown parameter list actual transition probabilities 
time step system model consists maximum likelihood estimates denoted unknown probabilities pairs states actions 
observed number times time step action executed system state transition state pj number times action executed state maximum likelihood state probabilities time uij immediate costs ci unknown determined simply memorizing observed infinite number time steps action taken infinitely state system model converges true system 
mentioned nontrivial ensure occurs system controlled 
time step generic indirect method uses non real time dp algorithm determine optimal evaluation function latest system model 
lambda denote optimal evaluation function 
course model correct lambda equal lambda generally case 
certainty equivalence optimal policy time step policy greedy respect lambda lambda lambda lambda denote policy 
time step lambda st certainty equivalence optimal action 
line dp algorithms described determine lambda including asynchronous dp 
sense time step initialize dp algorithm final estimate lambda produced dp algorithm completed previous time step 
small change system model time step means lambda lambda probably differ significantly 
pointed computation required perform dp iteration prohibitive problems large numbers states 
action controller execute time 
certainty equivalence optimal action lambda st appears best observations time consequently pursuing objective control controller execute action 
current model necessarily correct controller pursue identification objective dictates select actions certainty equivalence optimal actions 
easy generate examples current certainty equivalence optimal policy prevents convergence true optimal policy due lack exploration see example kumar 
simplest ways induce exploratory behavior controller randomized policies actions chosen probabilities depend current evaluation function 
action nonzero probability executed current certainty equivalence optimal action having highest probability 
problems immediate cost random function current state action maximum likelihood estimate immediate cost observed average immediate cost state action 
facilitate comparison algorithms simulations described section adopt action selection method boltzmann distribution watkins lin sutton 
method assigns execution probability admissible action current state probability determined rating action utility 
compute rating action st follows qf lambda st transform ratings negative sum probability mass function admissible actions boltzmann distribution time step probability controller executes action st prob gamma tp st gamma positive parameter controlling sharply probabilities peak certainty equivalence optimal action lambda st 
increases probabilities uniform decreases probability executing lambda st approaches probabilities actions approach zero 
acts kind computational temperature simulated annealing decreases time :10.1.1.123.7607:10.1.1.123.7607
controls necessary tradeoff identification control 
zero temperature exploration randomized policy equals certainty equivalence optimal policy infinite temperature attempt control 
simulations described section introduced exploratory behavior method just described generating randomized policies decrease time pre selected minimum value learning progressed 
choice method dictated simplicity desire illustrate algorithms generic possible 
doubt sophisticated exploratory behavior beneficial effects behavior algorithms 
adaptive real time dynamic programming generic indirect method just relies executing non real time dp algorithm convergence time step 
straightforward substitute rtdp resulting indirect method call adaptive rtdp 
method exactly rtdp described section system model updated line system identification method maximum likelihood method equation current system model performing stages rtdp true system model action time step determined randomized policy equation method balances identification control objectives 
adaptive rtdp related number algorithms investigated 
sutton dyna architecture focuses learning methods policy iteration section encompasses algorithms adaptive rtdp discusses ref 

lin discusses methods closely related adaptive rtdp 
engineering literature ferguson describe algorithm similar adaptive rtdp focus markovian decision problems performance measured average cost time step discounted cost discussed 
performing rtdp concurrently system identification adaptive rtdp provides opportunity progress identification influence selection states backup operation applied 
sutton suggested advantageous back costs states confidence accuracy estimated state transition probabilities 
devise various measures confidence estimates direct algorithm states cost backups reliable state transition information confidence measure 
time possible confidence measure direct selection actions controller tends visit regions state space confidence low improve model regions 
strategy produces exploration aids identification conflict control 
kaelbling lin moore schmidhuber sutton thrun thrun oller discuss possibilities :10.1.1.17.2654
learning learning method proposed watkins solving markovian decision problems incomplete information indirect adaptive methods discussed direct method explicit model dynamic system underlying decision problem 
directly estimates optimal values pairs states admissible actions call admissible state action pairs 
recall equation lambda optimal value state action cost generating action state optimal policy 
policy selecting actions greedy respect optimal values optimal policy 
optimal values available optimal policy determined relatively little computation 
watkins proposed family learning methods call learning article simplest case called step learning 
observed learning methods simple idea suggested previously far knew 
observed problems intensively studied years surprising studied earlier 
idea assigning values state action pairs formed basis approach dp seen algorithms learning estimating values watkins dissertation 
depart somewhat presentation view taken watkins sutton barto singh learning method adaptive line control 
emphasize learning relationship asynchronous dp basic learning algorithm line asynchronous dp method unique requiring direct access state transition probabilities decision problem 
describe usual line view learning 
line learning maintaining explicit estimate optimal evaluation function done methods described learning maintains estimates optimal values admissible state action pair 
state action qk estimate lambda available stage computation 
recalling lambda optimal values state equation think values stage implicitly defining fk stage estimate lambda state fk minu qk values define evaluation function way contain information evaluation function 
example actions ranked basis values ranking actions evaluation function requires knowledge state transition probabilities immediate costs 
having direct access state transition probabilities line learning access random function generate samples probabilities 
state action input function returns state probability pij 
call function successor successor 
successor function amounts accurate model system form probabilities learning access probabilities 
shall see line learning role successor function played system 
stage line learning synchronously updates values subset admissible state action pairs leaves unchanged values admissible pairs 
subset admissible state action pairs values updated changes stage stage choice subsets determines precise nature algorithm 
ji denote set admissible stateaction pairs values updated stage state action pair necessary define learning rate parameter determines new qvalue determined old value backed value 
ffk ffk denote learning rate parameter updating value stage qk computed follows qk gamma ffk qk ffk ci successor fk equation 
values admissible state action pairs remain qk qk admissible learning backup mean application equation single admissible state action pair 
value admissible state action pair backed infinitely infinite number stages learning rate parameters ffk decrease stages appropriate way sequence fqk generated line qlearning converges probability lambda admissible pairs 
essentially proved watkins watkins dayan revised proof ref 

appendix describes method meeting required learning rate conditions developed darken moody 
method obtaining results real time learning example problems section 
gain insight line learning relating asynchronous dp 
stage values admissible state action pairs define evaluation function fk equation 
view stage line learning defined equation updating fk fk state fk minu qk evaluation function update correspond stage usual dp algorithms samples successor selected actions determined state action pairs conventional dp backup contrast uses true expected successor costs admissible actions state accurate think line learning asynchronous version asynchronous dp 
asynchronous dp asynchronous level states backup operation state requires minimizing expected costs admissible actions state 
amount computation required determine expected cost admissible action depends number possible successor states action large total number states stochastic problems 
line learning hand asynchronous level admissible state action pairs 
learning backup requires minimizing admissible actions give state stage line learning effect stage asynchronous dp sk special case problem deterministic set admissible state action pairs states sk ffk admissible state action pairs 
order calculate equation fk successor equation require computation proportional number possible successor states 
stochastic case asynchronous dp backup require mn computational steps learning backup requires 
advantage offset increased space complexity learning fact learning backup takes information account backup asynchronous dp asynchronous dp backup comparable learning backups 
computation required learning backup required asynchronous dp backup learning advantageous stages computed quickly despite large number possible successor states real time applications discuss 
real time learning line learning turned line algorithm executing concurrently control 
current system model provides approximate successor function result indirect adaptive method identical adaptive rtdp section stages line learning substitute stages asynchronous dp 
advantages adaptive rtdp number admissible actions large 
term real time learning case originally discussed watkins model system underlying decision problem real system acts successor function 
direct adaptive algorithm backs value single state action pair time step control state action pair consists observed current state action executed 
real time learning compute optimal policy explicit model system underlying decision problem 
specifically assume time step controller observes state st available estimated optimal values produced preceding stages real time learning 
denote estimates qt admissible state action pairs 
controller selects action ut st information manner allows exploration 
executing ut controller receives immediate cost cst ut system state changes st 
qt computed follows qt st ut gamma fft st ut qt st ut fft st ut cst ut st complete minimization avoided follows 
qk backed new value qk smaller fk fk set smaller value 
new value larger fk fk qk fk qk fk explicitly minimizing current values state admissible actions 
case sole greedy action respect fk 
done fk fk 
procedure computes minimization equation explicitly updating values state action pairs sole greedy action value increases 
ft st minu st qt st fft st ut learning rate parameter time step current state action pair 
values admissible state action pairs remain qt qt admissible st ut 
process repeats time step 
far convergence concerned real time learning special case offline learning set state action pairs values backed step stage st ut sequence values generated realtime learning converges true values lambda conditions required convergence line learning 
means admissible action performed state infinitely infinite number control steps 
noteworthy pointed dayan admissible action state real time learning reduces td algorithm investigated sutton :10.1.1.132.7760
define complete adaptive control algorithm making real time learning necessary specify action selected current values 
convergence optimal policy requires kind exploration required indirect methods facilitate system identification discussed 
method selecting action current evaluation function randomized method described equation method leads convergence indirect method leads convergence corresponding direct method real time learning 
learning methods real time learning real system underlying decision problem plays role successor function 
possible define successor function real system system model 
state action pairs experienced control real system provides successor function state action pairs system model provides approximate successor function 
sutton studied approach algorithm called dyna performs basic learning backup actual state transitions hypothetical state transitions simulated system model 
performing learning backup hypothetical state transitions amounts running multiple stages line learning intervals times controller executes actions 
step real time learning performed actual state transition 
obviously possible ways combine direct indirect adaptive methods emphasized sutton discussion general dyna learning architecture 
possible modify basic learning method variety ways order enhance efficiency 
example lin studied method real time learning augmented model line learning action clearly stand preferable current values 
case line learning carried backup values admissible actions promising latest values current state 
watkins describes family learning methods values backed information gained sequences state transitions 
way implement kind extension eligibility trace idea back values state action pairs experienced past magnitudes backups decreasing zero increasing time past :10.1.1.132.7760
sutton td algorithms illustrate idea :10.1.1.132.7760
attempting combinations variations learning methods described scope article 
barto singh dayan lin moore sutton comparative empirical studies adaptive algorithms learning 
methods explicit policy representations dp learning algorithms described non adaptive adaptive cases explicit representation evaluation function function giving values admissible state action pairs 
functions computing action time step policy defined explicitly stored 
number real time learning control methods dp policies evaluation functions stored updated time step control 
methods addressed article methods closely related policy iteration dp algorithm value iteration algorithms discussed section 
policy iteration see bertsekas alternates phases policy evaluation phase evaluation function current policy determined policy improvement phase current policy updated greedy respect current evaluation function 
way evaluate policy executing value iteration algorithms discussed section assumption admissible action state action specified policy evaluated 
alternatively explicit matrix inversion methods 
policy evaluation require repeated minimizing admissible actions require computation practical large state sets 
feasible modified policy policy iteration policy evaluation phase executed completion policy improvement phase 
real time algorithms policy iteration effectively executing asynchronous form modified policy iteration concurrently control 
examples methods appear pole balancing system barto sutton anderson refs 
dyna pi method sutton pi means policy iteration 
barto sutton watkins discuss connection methods policy iteration detail 
article discuss learning algorithms policy iteration theory understood theory learning algorithms asynchronous value iteration 
williams baird valuable contribution theory addressing dp algorithms asynchronous grain finer dp learning 
algorithms include value iteration policy iteration modified policy iteration special cases 
integrating theory scope article 
storing evaluation functions issue great practical importance implementing algorithms described article evaluation functions represented stored theoretical results described assume lookup table representation evaluation functions principle possible number states admissible actions finite assumed article 
applying conventional dp problems involving continuous states actions usual practice discretize ranges continuous state variables lookup table representation cf 
boxes representation michie chambers barto sutton anderson 
leads space complexity exponential number state variables situation prompting bellman coin phrase curse dimensionality 
methods described article asynchronous dp learning circumvent curse dimensionality focusing behavior trial rtdp stochastic shortest path problems designated start states reduce storage requirement memory allocated incrementally trials 
number methods exist making lookup table representation efficient necessary store costs possible states 
hash table methods assumed korf lrta permit efficient storage retrieval costs small subset possible states need stored 
similarly kd tree data structure access state costs explored moore provide efficient storage retrieval costs finite set states dimensional state space :10.1.1.17.2654
theoretical results described article extend methods preserve integrity stored costs assuming hash collisions resolved 
approaches storing evaluation functions function approximation methods parameterized models 
example samuel checkers player evaluation function approximated weighted sum values set features describing checkerboard configurations 
basic backup operation performed weights comments apply storing values admissible state action pairs 
state costs 
weights adjusted reduce discrepancy current cost state backed cost 
approach inspired variety studies parameterized function approximations 
discrepancy supplies error error correction procedure approximates functions training set function samples 
form supervised learning learning examples provides natural way connectionist networks shown example anderson tesauro 
parametric approximations evaluation functions useful generalize training data supply cost estimates states visited important factor large state sets 
fact supervised learning method associated manner representing hypotheses adapted approximating evaluation functions 
includes symbolic methods learning examples 
methods generalize training information derived back operations various dp algorithms 
example chapman kaelbling tan adapt decision tree methods mahadevan connell statistical clustering method 
yee discusses function approximation perspective dp learning algorithms 
despite large number studies principles dp combined generalizing methods approximating evaluation functions theoretical results article automatically extend approaches 
generalization helpful approximating optimal evaluation function detrimental convergence underlying asynchronous dp algorithm pointed watkins illustrated simple example bradtke 
function approximation scheme adequately represent optimal evaluation function trained samples function follow adequate representation result iterative dp algorithm uses approximation scheme stage 
issues arise numerically solving differential equations 
objective problems approximate function solution differential equation boundary conditions absence training examples drawn true solution 
words objective solve approximately differential equation just approximate solution 
interested approximately solving bellman optimality equation easier problem approximating solution available 
extensive literature function approximation methods dp multigrid methods methods splines orthogonal polynomials bellman dreyfus bellman daniel kushner 
literature devoted line algorithms cases complete model decision problem 
adapting techniques literature produce approximation methods rtdp dp learning algorithms challenge research 
best knowledge theoretical results directly address generalizing methods dp learning algorithms 
results sutton dayan concern td methods evaluate policy linear combination complete set linearly independent basis vectors :10.1.1.132.7760
unfortunately results address problem representing evaluation function compactly represented lookup table 
bradtke addresses problem learning values quadratic functions continuous state results restricted linear quadratic regulation problems 
singh yee point discounted case small errors approximating evaluation function function giving values lead worst small decrements performance controller approximate evaluation function basis control :10.1.1.132.7760
result plausible small evaluation errors drastically undermine control performance condition true raise concerns combining dp learning function approximation 
research needed provide better understanding function approximation methods effectively algorithms described article 
illustrations dp learning race track problem described section illustrate compare conventional dp rtdp adaptive rtdp real time learning race tracks shown 
small race track shown panel start states goal states states reachable start states policy 
shown squares car land crossing finish line 
larger race track shown panel start states goal states states reachable start states 
set controller intended actions executed probability 
applied conventional gauss seidel dp race track problem mean gauss seidel value iteration defined section fl initial evaluation function assigning zero cost state 
gauss seidel dp converges conditions special case dp converges conditions section satisfied 
specifically clear proper policy track possible car reach finish line reachable state may hit wall restart improper policy incurs infinite cost state immediate costs non goal states positive 
selected state ordering applying gauss seidel dp concern influence convergence rate selected ordering gauss seidel dp converged approximately half number sweeps synchronous dp 
table summarizes small larger race track problems computational effort required solve gauss seidel dp 
gauss seidel dp considered converged optimal evaluation function maximum cost change states successive sweeps gamma 
estimated number relevant states race track number states reachable start states optimal policy counting states visited executing optimal actions trials 
estimated earliest point dp computation optimal evaluation function approximation corresponding greedy policy optimal policy 
recall optimal policy greedy policy respect evaluation functions 
running test trials sweep policy greedy respect evaluation function produced sweep 
sweep recorded average path length produced test trials 
convergence gauss seidel dp compared averages optimal expected path length obtained dp algorithm noting sweep average path length gamma optimal 
resulting numbers sweeps backups listed table columns labeled number sweeps optimal policy number backups optimal policy 
optimal policies emerged considerably earlier computations optimal evaluation functions important note estimation process part conventional line value iteration algorithms requires considerable amount additional computation resulting numbers backups useful assessing computational requirements real time algorithms allow controllers follow optimal policies comparable numbers backups 
applied rtdp adaptive rtdp real time learning race track problems 
immediate costs positive know lambda non negative states setting initial costs states zero produces initial evaluation function required theorem 
applied real time algorithms trial manner starting trial car placed starting line zero velocity square starting line selected equal probability 
trial ended car reached goal state 
theorem fl rtdp converge optimal evaluation function repeated trials 
rtdp adaptive rtdp back costs states control step restricted attention simplest case back cost current state time step 
case bt obviously algorithms applied simulation mode 
executed runs algorithm different random number seeds run sequence trials evaluation function initialized zero 
policy iteration algorithms address problem explicitly generating sequence improving policies updating policy requires computing corresponding evaluation function generally timeconsuming computation 
small track larger track number reachable states number goal states est 
number relevant states optimum exp path length number sweeps convergence number backups convergence number sweeps optimal policy number backups optimal policy table example race track problems 
results obtained executing dp 
monitor performance algorithm kept track path lengths moves car took going starting line finish line trial run 
record data divided run sequence disjoint epochs epoch sequence consecutive trials 
epoch path length mean average path lengths generated epoch algorithm 
adaptive rtdp real time learning applied conditions incomplete information algorithms induced exploratory behavior randomized policies boltzmann distribution described section 
control tradeoff identification control decreased parameter equation move reached pre selected minimum value initialized run 
parameter values additional simulation details provided appendix shows results rtdp panel adaptive rtdp panel real time learning panel 
central line graph shows epoch path length averaged runs corresponding algorithm 
upper lower lines show sigma standard deviation average sample runs 
average epoch path lengths initial epochs algorithm large show graphs useful note average epoch path lengths epoch rtdp adaptive rtdp real time learning respectively moves 
initial average path lengths large especially real time learning reflects primitive nature exploration strategy 
clear graphs problem rtdp learned faster variance adaptive rtdp real time learning learning rate measured terms number epochs numbers moves table discussed 
surprising differences versions problem complete information panel incomplete information panels 
performances rtdp adaptive rtdp similar despite differences reflects fact maximum likelihood system identification procedure algorithm converged rapidly relevant states due low level stochasticity problem 
graphs show real time learning takes epochs rtdp adaptive rtdp reach similar level performance 
reflects fact backup real time learning takes account information backups rtdp adaptive rtdp disadvantage somewhat offset relative computational simplicity learning backup 
shows real time learning results epochs 
convenient way show policies result algorithms show paths car follow start state sources randomness turned random exploration randomness problem state transition function turned 
right panel paths generated way policies produced algorithm judged effectively converged 
inspected graphs find smallest epoch numbers average epoch path lengths essentially reached asymptotic levels epochs rtdp panel epochs adaptive rtdp panel epochs real time learning panel 
treated appropriate caution effective convergence times useful comparing algorithms 
path shown panel optimal sense produced noiseless conditions policy optimal stochastic problem 
paths panels hand generated optimal policy despite fact move shorter path panel control decisions track suboptimal policies produce higher probability car collide track boundary stochastic conditions 
illustrate amount uncertainty problem increases increasing optimal policies generate paths conservative sense keeping safer distances track boundary maintaining lower velocities 
table provides additional information performance real time algorithms small track 
comparative purposes table includes column gauss seidel dp 
estimated path length effective convergence rtdp adaptive rtdp real time learning executing test trials learning turned policy produced effective convergence algorithm 
turned random exploration algorithms 
row table labeled est 
path length epoch number average epoch path length starting line finish line epoch number average epoch path length starting line finish line epoch number average epoch path length starting line finish line performance real time learning algorithms small track 
panel rtdp 
panel adaptive rtdp 
panel real time learning 
central line graph shows epoch path length averaged runs corresponding algorithm 
upper lower lines show sigma standard deviation epoch path length sample runs 
exploration controlled adaptive rtdp real time learning decreasing move reached pre selected minimum value 
right side panel shows paths car follow noiseless conditions start state effective convergence corresponding algorithm 
rtdp ave time effective convergence sweeps epochs epochs epochs est 
path length effective convergence ave number backups ave number backups epoch states backed times states backed times states backed times table summary learning performance small track real time dp rtdp adaptive real time dp real time learning 
amount computation required gauss seidel dp included comparative purposes 
effective convergence gives average path length test trials rtdp directly comparable gauss seidel dp 
epochs trials rtdp improved control performance point trial took average moves 
rtdp performed average backups reaching level performance half number required gauss seidel dp converge optimal evaluation function 
number backups comparable backups sweeps dp resulting evaluation function defines optimal policy table 
way compare gauss seidel dp rtdp examine backups perform distributed states 
cost state backed sweep gauss seidel dp rtdp focused backups fewer states 
example epochs average run rtdp backed costs states times states times costs states backed average run 
collect path length estimates somewhat smaller average epoch path lengths shown effective convergence graphs produced exploration turned graphs show path lengths produced random exploration turned 
gauss seidel dp averaged costs start states computed optimal evaluation function obtain estimated path length listed table 
statistics rtdp epochs focused states optimal paths 
surprisingly solving problem conditions incomplete information requires backups 
adaptive rtdp took epochs average backups achieve trials averaging moves effective convergence 
real time learning took epochs average backups achieve somewhat level performance see 
examining backups distributed states shows adaptive rtdp considerably focused real time learning 
epochs adaptive rtdp backed states times states times 
hand epochs real time learning backed values states times states times results real time learning reflect inadequacy primitive exploration strategy algorithm 
shows results rtdp adaptive rtdp real time learning larger race track table provides additional information 
results obtained conditions described small track 
shows realtime learning results larger track epochs 
judged rtdp adaptive rtdp real time learning effectively converged epochs respectively 
adaptive rtdp effectively converged faster rtdp terms number epochs partially due fact epochs tended moves backups epochs rtdp 
see achieve slightly suboptimal performance rtdp required computation conventional gauss seidel dp 
average epoch path lengths initial epoch algorithm large show graphs moves respectively rtdp adaptive rtdp real time learning 
large numbers moves especially real time learning reflect primitive nature exploration strategy 
paths shown right panel generated noiseless conditions policies produced effective convergence corresponding algorithms 
path shown panel optimal sense produced noiseless conditions policy optimal stochastic problem 
paths panels hand generated slightly suboptimal policies 
simulations definitive comparisons real time algorithms conventional dp illustrate features 
gauss seidel dp continued back costs states real time algorithms strongly focused subsets states relevant control objectives 
focus increasingly narrow learning continued 
convergence theorem trial rtdp applies simulations rtdp know algorithm eventually focused relevant states states making optimal paths 
rtdp achieved value state backed updated 
epoch number average epoch path length performance real time learning small track epochs 
initial part graph shows data plotted panel different horizontal scale 
rtdp ave time effective convergence sweeps epochs epochs epochs est 
path length effective convergence ave number backups ave number backups epoch states backed times states backed times states backed times table summary learning performance larger track real time dp rtdp adaptive real time dp real time learning 
amount computation required gauss seidel dp included comparative purposes 
epoch number average epoch path length starting line finish line epoch number average epoch path length starting line finish line epoch number average epoch path length starting line finish line performance real time learning algorithms larger track 
panel rtdp 
panel adaptive rtdp 
panel real time learning 
central line graph shows epoch path length averaged runs corresponding algorithm 
upper lower lines show sigma standard deviation epoch path length sample runs 
exploration controlled adaptive rtdp real time learning decreasing move reached pre selected minimum value 
right side panel shows paths car follow noiseless conditions start state effective convergence corresponding algorithm epoch number average epoch path length performance real time learning larger track epochs 
initial part graph shows data plotted panel different horizontal scale 
nearly optimal control performance computation gauss seidel dp small track computation gauss seidel dp larger track 
adaptive rtdp real time learning focused progressively fewer states run generic indirect method comparison inefficient apply problems states race track problems perform complete sweep move 
sharp contrast amount computation required real time algorithms move small limiting factor simulations results described adaptive rtdp real time learning produced exploration strategy decreased randomness selecting actions decreasing move reached pre selected minimum value 
described conducted experiments different minimum values decreasing trials moves 
performance algorithms altered worse changes 
systematic attempt investigate effects various exploration strategies clear performance algorithms highly sensitive exploration introduced controlled 
implementing adaptive rtdp race track problems took advantage knowledge action possible successors state 
allowed avoid performing move divisions required straightforward equation 
possible general conditions incomplete information 
algorithms scale larger problems adequately addressed simulations 
results small larger race track give indication algorithms scale collection problems adequate studying issue 
variability algorithm performance function problem details size state action sets difficult extrapolate performance just problems 
proceeding larger problems hampered large space requirements algorithms continue lookup tables storing evaluation functions 
tesauro td gammon system encouraging data point dp learning conjunction function approximation methods problems larger described continued theoretical research necessary address computational complexity real time dp algorithms 
clear simulations real time dp algorithms confer significant computational advantages conventional line dp algorithms 
concluding discussion race track problem point misleading think application dp learning algorithms problem productive way apply realistic robot navigation tasks 
example learning applied formulation race track problem refines skill racing specific track 
skill transfer tracks due specificity track represented 
realistic applications dp learning robot navigation requires states actions lin mahadevan connell 
discussion conventional dp algorithms limited utility problems large state spaces combinatorial state spaces problems interest ai require fully expanding possible states storing cost state 
heuristic search contrast selectively explores problem state space 
dp algorithms successively approximate optimal evaluation functions relevant learning way heuristic search 
effectively cache permanent data structure results repeated searches forward state 
information improves algorithm proceeds ultimately converging optimal evaluation function determine optimal policies relative ease 
heuristic search algorithms update estimate cost reach states initial state typically update heuristic evaluation function estimating cost reach goal state state 
principles dp relevant learning conventional dp algorithms really learning algorithms operate line 
designed applied problem solving control learning occurs experience accumulates actual simulated attempts problem solving control 
possible execute line dp algorithm concurrently actual simulated control dp algorithm influence influenced ongoing control process 
doing satisfy certain requirements results algorithm call rtdp special case essentially coincides korf lrta algorithm 
general approach follows previous research dp principles problem solving learning refs 

contribution article bring bear dp learning theory asynchronous dp bertsekas tsitsiklis 
suitability asynchronous dp implementation multi processor systems motivated theory novel results 
applying results especially results stochastic shortest path problems rtdp provides new theoretical basis learning algorithms 
convergence theorems asynchronous dp imply rtdp retains competence conventional synchronous gauss seidel dp algorithms extension korf lrta convergence theorem framework provides conditions rtdp avoids exhaustive nature line dp algorithms ultimately yielding optimal behavior 
term simulation mode refer execution rtdp related algorithms simulated control actual control 
dp learning simulation mode illustrated samuel checkers playing system tesauro backgammon playing system illustrations rtdp race track problem 
despite fact dp learning algorithms executed simulation mode line algorithms treat learning algorithms incrementally improve control performance simulated experience solely application computational methods 
algorithms rtdp require accurate model decision problem simulation mode option obvious advantages due large number trials required 
applying rtdp actual control sense time compute satisfactory policy line method actual control 
applied actual control simulation mode rtdp significant advantages conventional dp algorithms 
rtdp responsive demands control selecting states backup operation applied focus computation parts state set control information important improving control performance 
convergence theorem trial rtdp applied stochastic shortest path problems specifies conditions rtdp focuses states optimal paths eventually abandoning states produce policy optimal relevant states continuing back costs states possibly backing costs states 
illustrations race track problem show rtdp obtain near optimal policies problems significantly computation required conventional dp 
compelling fact approach illustrated rtdp form useful approximations optimal evaluation functions problems conventional dp feasibly applied 
mentioned example backgammon single sweep conventional dp take years mips processor 
true despite fact large fraction states backgammon irrelevant normal play 
rtdp closely related monte carlo algorithms achieve computational efficiency automatically allocating computation example unimportant terms sum correspond rare events computational process 
reason asymptotic computational complexity monte carlo methods exceed methods classes problems 
monte carlo methods generally competitive deterministic methods small problems high precision answers required 
research needed fully elucidate correspondences exploit refining dp learning methods understanding computational complexity 
problems large states sets backgammon lookup table method storing evaluation functions restricted attention practical 
research dp learning methods storage schemes 
problems dp learning algorithms focus increasingly small subsets states illustrated simulations race track problem data structures hash tables kd trees allow algorithms perform despite dramatically reduced space requirements 
adapt supervised learning procedures back operation dp learning method provide training information 
methods generalize adequately training data provide efficient means storing evaluation functions 
success achieved methods generalize connectionist networks theory article automatically extend cases 
generalization disrupt convergence asynchronous dp 
additional research needed understand effectively combine function approximation methods asynchronous dp 
addition case accurate model decision problem available devoted considerable attention markovian decision problems incomplete information problems accurate model available 
adopting terminology engineering literature problems require adaptive control methods 
described indirect direct approaches problems 
method called generic indirect method representative majority algorithms described engineering literature applicable markovian decision problems incomplete information 
system identification algorithm adjusts system model line control controller selects actions current estimate optimal evaluation function computed conventional dp algorithm assumption current model accurately models system 
dp algorithm re executed system model updated 
approach theoretically convenient costly apply large problems 
adaptive rtdp results substituting rtdp conventional dp generic indirect method 
means rtdp executed system model generated system identification algorithm 
adaptive rtdp tailored available computational resources adjusting number dp stages executes time step control 
due additional uncertainty case learning necessarily slower non adaptive case measured number backups required 
amount computation required select control action roughly 
means practical apply adaptive rtdp problems larger practical apply methods generic indirect method re execute conventional dp algorithm system model updated 
addition indirect adaptive methods discussed direct adaptive methods 
direct methods form explicit models system underlying decision problem 
described learning algorithm approximates optimal evaluation function forming estimates state transition probabilities 
learning uses sample state transitions generated system model observed actual control 
learning asynchronous dp algorithm operates finer grain asynchronous dp algorithm described section 
basic operation asynchronous dp backing cost state requiring computation proportional number possible successor states basic operation learning backing value state action pair computation depend number possible successor states 
fine grain basic learning backup allows real time learning focus selected actions addition selected states way responsive behavior controlled system 
cost flexibility increased space required store values state action pairs fact learning backup gather information complete dp backup operation 
sophisticated exploration strategies important solving markovian decision problems conditions complete incomplete information 
complete information sophisticated exploration strategy improve control performance decreasing time required reach goal states case rtdp focusing dp stages states information useful improving evaluation function gained 
knowledgeable ordering back ups accelerate convergence asynchronous dp applied line 
information incomplete sophisticated exploration useful reasons 
case exploration strategies address necessity gather information unknown structure system controlled 
exploration case complete information conducted simulation mode kind exploration conducted line 
discussed exploration performed reason conflicts performance objective control short term basis controller execute actions appear best current evaluation function 
sophisticated exploration strategies simulations race track problem attempt article analyse issues pertinent exploration sophisticated exploration strategies play essential role making learning methods practical larger problems 
mention clear easy devise consistent set desiderata exploration strategies 
example researchers argued exploration strategy visit states regions state space information system low quality learn regions visit states regions state space information system high quality back operation uses accurate estimates state transition probabilities visit states having successors costs close optimal costs back operation efficiently propagates cost information 
suggestions sense proper context clear design strategy best incorporates 
encouraging convergence results article compatible wide range exploration strategies 
article assumed states system controlled completely unambiguously observable controller 
assumption critical theory operation algorithms discussed difficult satisfy practice 
example current state robot world vastly different list robot current sensations 
positive side effective closed loop control policies distinguish possible sensations 
exploiting fact requires ability recognize states complex flow sensations 
problem state identification subject research variety disciplines approaches studied guises remains critical factor extending applicability dp learning methods 
widely applicable approach problem take perspective constitutes system state purposes control constitutes system independent control objectives 
framework adopted article dynamic system underlies decision problem misleading suggesting existence single definitive grain delineate events mark passage 
actuality control objectives dictate important flow controller sensations multiple models different levels abstraction needed achieve 
caution recognized algorithms described article find wide application components sophisticated embedded systems 
appendices proof trial rtdp theorem prove theorem extends korf convergence theorem lrta trial rtdp applied undiscounted stochastic shortest path problems 
proof prove theorem special case cost current state backed time interval bt kt see section 
observe proof change bt allowed arbitrary set containing st denote goal set st ut ft respectively denote state action evaluation function time step arbitrary infinite sequence states actions evaluation functions generated trial rtdp starting arbitrary start state 
observe evaluation functions remain non overestimating time ft lambda states true induction ft ft st ft lambda ft st minu cs ft minu cs lambda lambda st equality bellman optimality equation equation 
set states appear infinitely arbitrary sequence nonempty state set finite 
set admissible actions state zero probability causing transition state set actions pij gamma 
states gamma appear finite number times finite time states visited probability action chosen infinite number times state occurs probability transition occur probability exist time st ut st 
know time step rtdp backs cost st st bt 
write back operation follows ft st minu cs ut ut ft gamma ut ft know st ut gamma ut st 
right summation equation zero 
means costs states gamma influence operation rtdp 
rtdp performs asynchronous dp markovian decision problem state set goal states contained immediate costs markovian decision problem positive 
discounting shown asynchronous dp cause costs states grow bound 
contradicts fact cost state overestimate optimal cost finite due existence proper policy 
contains goal state probability 
trial rtdp performs asynchronous dp stochastic shortest path problem state set satisfies conditions convergence theorem asynchronous dp applied undiscounted stochastic shortest path problems bertsekas tsitsiklis proposition 
consequently trial rtdp converges optimal evaluation function stochastic shortest path problem 
know optimal evaluation function problem identical optimal evaluation function original problem restricted states costs states gamma influence costs states time 
furthermore probability contains set states reachable start state optimal policy 
clearly contains start states start state begins infinite number trails 
trial rtdp executes greedy action respect current evaluation function breaks ties way continues execute greedy actions 
know number policies finite trial rtdp converges optimal evaluation function restricted time continues select actions greedy respect optimal evaluation function optimal actions 
probability contains states reachable start state optimal policy time controller rtdp execute optimal actions 
trivial revision argument holds rtdp backs costs states current state time step bt arbitrary subset simulation details discount factor fl set simulations sets bt set rtdp involve parameters 
gauss seidel dp requires specifying state ordering sweeps 
selected ordering concern influence convergence rate 
adaptive rtdp real time learning require exploration training trials implemented equation 
generate data described section decreased parameter successive moves follows tmax tmin fi gamma tmin move number cumulative trials fi tmax tmin 
real time learning additionally requires sequences learning rate parameters fft equation satisfy hypotheses learning convergence theorem 
defined sequences follows 
fft denote learning rate parameter value state action pair backed time step nt number backups performed value time step learning rate fft defined follows fft ff ff initial learning rate 
set ff 
equation implements search converge schedule fft suggested darken moody 
argue schedules achieve performance stochastic optimization tasks 
shown schedule satisfies hypotheses learning convergence theorem 
anderson 
strategy learning multilayer connectionist representations 
technical report tr gte laboratories incorporated waltham ma 
corrected version report published proceedings fourth international workshop machine learning san mateo ca morgan kaufmann 
barto sutton anderson 
neuronlike elements solve difficult learning control problems 
ieee transactions systems man cybernetics 
reprinted anderson rosenfeld neurocomputing foundations research mit press cambridge ma 
barto sutton watkins 
sequential decision problems neural networks 
touretzky editor advances neural information processing systems pages san mateo ca 
morgan kaufmann 
barto sutton watkins 
learning sequential decision making 
gabriel moore editors learning computational neuroscience foundations adaptive networks pages 
mit press cambridge ma 
barto 
reinforcement learning adaptive critic methods 
white editors handbook intelligent control neural fuzzy adaptive approaches pages 
van nostrand reinhold new york 
barto singh 
computational economics reinforcement learning 
touretzky elman sejnowski hinton editors connectionist models proceedings summer school pages 
morgan kaufmann san mateo ca 
bellman dreyfus 
functional approximations dynamic programming 
math tables computation 
bellman 
polynomial approximation new computational technique dynamic programming allocation processes 
mathematical computation 
bellman 
dynamic programming 
princeton university press princeton nj 
bertsekas 
distributed dynamic programming 
ieee transactions automatic control 
bertsekas 
dynamic programming deterministic stochastic models 
prentice hall englewood cliffs nj 
bertsekas tsitsiklis 
parallel distributed computation numerical methods 
prentice hall englewood cliffs nj 
bradtke 
reinforcement learning applied linear quadratic regulation 
advances neural information processing san mateo ca 
morgan kaufmann 
appear 
chapman 
cake 
ai magazine 
chapman kaelbling 
input generalization delayed reinforcement learning algorithm performance comparisons 
proceedings international joint conference artificial intelligence 
christensen korf 
unified theory heuristic evaluation functions application learning 
proceedings fifth national conference artificial intelligence aaai pages san mateo ca 
morgan kaufmann 

theoretical comparison efficiencies classical methods monte carlo method computing component solution set linear algebraic equations 
meyer editor symposium monte carlo methods pages 
wiley new york 
daniel 
splines efficiency dynamic programming 
journal mathematical analysis applications 
darken moody 
note learning rate schedule stochastic optimization 
lippmann moody touretzky editors advances neural information processing systems pages san mateo ca 
morgan kaufmann 
dayan 
navigating temporal difference 
lippmann moody touretzky editors advances neural information processing systems pages san mateo ca 
morgan kaufmann 
dayan 
reinforcing connectionism learning statistical way 
phd thesis university edinburgh 
dayan 
convergence td general 
machine learning 
dean wellman 
planning control 
morgan kaufmann san mateo ca 

contraction mappings theory underlying dynamic programming 
siam review 
gardner 
mathematical games 
scientific american january 

optimality 
artificial intelligence 
ginsberg 
universal planning universally bad idea 
ai magazine 

connectionist problem solving computational aspects biological learning 
birkhauser boston 
hart nilsson raphael 
formal basis heuristic determination minimum cost paths 
ieee transactions systems science cybernetics 
holland 
escaping brittleness possibility general purpose learning algorithms applied rule systems 
michalski carbonell mitchell editors machine learning artificial intelligence approach volume ii pages 
morgan kaufmann san mateo ca 
jacobson mayne 
differential dynamic programming 
elsevier new york 
ferguson 
computationally efficient adaptive control algorithms markov chains 
proceedings th conference decision control pages tampa florida 
jordan jacobs 
learning control unstable system forward modeling 
touretzky editor advances neural information processing systems san mateo ca 
morgan kaufmann 
kaelbling 
learning embedded systems 
mit press cambridge ma 
revised version research tr june 
kirkpatrick gelatt vecchi :10.1.1.123.7607
optimization simulated annealing 
science 
klopf 
brain function adaptive systems theory 
technical report air force cambridge research laboratories bedford ma 
summary appears proceedings international conference systems man cybernetics ieee systems man cybernetics society dallas tx 
klopf 
neuron theory memory learning intelligence 
washington 
korf 
real time heuristic search 
artificial intelligence 
kumar 
survey results stochastic adaptive control 
siam journal control optimization 
kumar kanal 
cdp unifying formulation heuristic search dynamic programming branch bound 
kanal kumar editors search artificial intelligence pages 
springer verlag 
kushner 
numerical methods stochastic control problems continuous time 
springer verlag new york 
kwon pearson 
modified quadratic cost problem feedback stabilization linear system 
ieee transactions automatic control 
le cun 
theoretical framework back propagation 
touretzky hinton sejnowski editors proceedings connectionist models summer school pages 
morgan kaufmann san mateo ca 

real time optimal path planning distributed computing paradigm 
proceedings american control conference boston ma 
long ji lin 
programming robots reinforcement learning teaching 
proceedings ninth national conference artificial intelligence pages cambridge ma 
mit press 
long ji lin 
self improvement reinforcement learning planning teaching 
birnbaum collins editors learning proceedings eighth international workshop pages san mateo ca 
morgan kaufmann 
long ji lin 
self improving reactive agents case studies reinforcement learning frameworks 
animals animats proceedings international conference simulation adaptive behavior pages cambridge ma 
mit press 
long ji lin 
self improving reactive agents reinforcement learning planning teaching 
machine learning 
mahadevan connell 
automatic programming behavior robots reinforcement learning 
artificial intelligence 
mayne 
receding horizon control nonlinear systems 
ieee transactions automatic control 
er heuristic search algorithm modifiable estimate 
artificial intelligence 
michie chambers 
boxes experiment adaptive control 
dale michie editors machine intelligence pages 
oliver boyd 
minsky :10.1.1.17.2654
theory neural analog reinforcement systems application brain model problem 
phd thesis princeton university 
minsky 
steps artificial intelligence 
proceedings institute radio engineers 
reprinted feigenbaum feldman editors computers thought 
mcgraw hill new york 
moore :10.1.1.17.2654
efficient memory learning robot control 
phd thesis university cambridge cambridge uk 
moore 
variable resolution dynamic programming efficiently learning action maps multivariate real valued state spaces 
birnbaum collins editors learning proceedings eighth international workshop pages san mateo ca 
morgan kaufmann 
moore atkeson 
memory reinforcement learning efficient computation prioritized sweeping 
advances neural information processing san mateo ca 
morgan kaufmann 
appear 
peng williams 
efficient learning planning dyna framework 
proceedings second international conference simulation adaptive behavior honolulu hi 
appear 
puterman shin 
modified policy iteration algorithms discounted markov decision problems 
management science 
ross 
stochastic dynamic programming 
academic press new york 
samuel 
studies machine learning game checkers 
ibm journal research development pages 
reprinted feigenbaum feldman editors computers thought mcgraw hill new york 
samuel 
studies machine learning game checkers 
ii progress 
ibm journal research development pages november 
schmidhuber 
adaptive confidence adaptive curiosity 
technical report institut ur informatik technische universit unchen 
unchen germany 
schoppers :10.1.1.49.6261
universal plans reactive robots unpredictable environments 
proceedings tenth international joint conference artificial intelligence pages menlo park ca 
schoppers 
defense reaction plans caches 
ai magazine 
singh yee :10.1.1.132.7760
upper bound loss approximate optimal value functions 
technical report 
submitted technical note machine learning 
sutton 
temporal credit assignment reinforcement learning 
phd thesis university massachusetts amherst ma 
sutton :10.1.1.132.7760
learning predict method temporal differences 
machine learning 
sutton 
integrated architectures learning planning reacting approximating dynamic programming 
proceedings seventh international conference machine learning pages san mateo ca 
morgan kaufmann 
sutton 
planning incremental dynamic programming 
birnbaum collins editors learning proceedings eighth international workshop pages san mateo ca 
morgan kaufmann 
sutton editor 
special issue machine learning reinforcement learning volume 
machine learning 
published reinforcement kluwer academic press boston ma 
sutton barto 
modern theory adaptive networks expectation prediction 
psychological review 
sutton barto 
temporal difference model classical conditioning 
proceedings ninth annual conference cognitive science society hillsdale nj 
erlbaum 
sutton barto 
time derivative models reinforcement 
gabriel moore editors learning computational neuroscience foundations adaptive networks pages 
mit press cambridge ma 
sutton barto williams 
reinforcement learning direct adaptive optimal control 
proceedings american control conference pages boston ma 
tan 
learning cost sensitive internal representation reinforcement learning 
birnbaum collins editors learning proceedings eighth international workshop pages san mateo ca 
morgan kaufmann 
tesauro 
practical issues temporal difference learning 
machine learning 
thrun oller 
active exploration dynamic environments 
moody hanson lippmann editors advances neural information processing systems san mateo ca 
morgan kaufmann 
thrun 
role exploration learning control 
white editors handbook intelligent control neural fuzzy adaptive approaches pages 
van nostrand reinhold new york 
utgoff clouse 
kinds training information evaluation function learning 
proceedings ninth annual conference artificial intelligence pages san mateo ca 
morgan kaufmann 
watkins 
learning delayed rewards 
phd thesis cambridge university cambridge england 
watkins dayan 
learning 
machine learning 
werbos 
regression new tools prediction analysis behavioral sciences 
phd thesis harvard university 
werbos 
advanced forecasting methods global crisis warning models intelligence 
general systems yearbook 
werbos 
applications advances nonlinear sensitivity analysis 
editors system modeling optimization 
springer verlag 
proceedings tenth ifip conference new york 
werbos 
building understanding adaptive systems statistical numerical approach factory automation brain research 
ieee transactions systems man cybernetics 
werbos 
generalization back propagation applications recurrent gas market model 
neural networks 
werbos 
approximate dynamic programming real time control neural modeling 
white editors handbook intelligent control neural fuzzy adaptive approaches pages 
van nostrand reinhold new york 
white jordan 
optimal control foundation intelligent control 
white editors handbook intelligent control neural fuzzy adaptive approaches pages 
van nostrand reinhold new york 
whitehead 
complexity cooperation learning 
birnbaum collins editors learning proceedings eighth international workshop pages san mateo ca 
morgan kaufmann 
williams baird iii 
mathematical analysis actor critic architectures learning optimal controls incremental dynamic programming 
proceedings sixth yale workshop adaptive learning systems pages new haven ct aug 
witten 
adaptive optimal controller discrete time markov environments 
information control 
witten 
exploring modelling controlling discrete sequential environments 
international journal man machine studies 
yee 
abstraction control learning 
technical report department computer science university massachusetts amherst ma 

