international journal computer vision kluwer academic publishers 
manufactured netherlands 
development comparison robust methods estimating fundamental matrix torr murray department engineering science university oxford parks road oxford ox pj uk robots ox ac uk dwm robots ox ac uk received august revised july accepted 
goals 
develop variety robust methods computation fundamental matrix calibration free representation camera motion 
methods drawn principal categories robust estimators viz 
case deletion diagnostics estimators random sampling develops theory required apply non linear orthogonal regression problems 
considerable amount interest focussed application robust estimation computer vision relative merits individual methods unknown leaving potential practitioner guess value 
second goal compare judge methods 
comparative tests carried correspondences generated synthetically statistically controlled fashion feature matching real imagery 
contrast previously reported methods goodness fit synthetic observations judged terms fit observations se terms fit ground truth 
variety error measures examined 
experiments allow statistically satisfying quasi optimal method synthesized shown stable percent outlier contamination may percent outliers 
performance bounds established method variety robust methods estimate standard deviation error covariance matrix parameters examined 
results comparison broad applicability vision algorithms input data corrupted noise gross outliers 
keywords robust methods fundamental matrix matching 
computer vision algorithms assumed squares framework sufficient deal data corrupted noise 
applications visual data noisy contain outliers data gross disagreement postulated model 
outliers inevitably included initial fit distort fitting process fitted parameters arbitrary 
particularly severe veridical data degenerate near degenerate respect model outliers appear break degeneracy 
circumstances deployment robust estimation methods essential 
robust methods continue recover meaningful descriptions statistical population data contain outlying elements belonging different population 
able perform assumptions underlying estimation say noise model wholly satisfied 
earliest draw value methods attention computer vision researchers fischler bolles 
shows table data contains gross outlier point 
fit result applying squares fit result applying squares robust method removed outlier solid line result applying fully robust ransac algorithm data 
data set demonstrate failings na heuristics remove torr murray 
data set outlier comparing squares robust fitting 
outliers 
example discarding point largest residual squares fitting removes point point 
repeated application heuristic convergence results half valid data discarded point remaining inlier completely erroneous fit 
statistical literature reports wide variety robust estimators mosteller tukey cook devlin fischler bolles huber hampel rousseeuw roth torr murray zhang kumar hanson shapiro brady stewart 
aim develop variety methods categories estimators case deletion diagnostics random sampling apply computation fundamental matrix 
turn requires novel extensions robust estimation techniques handle non linear problems involving orthogonal regression 
fundamental matrix provides general compact representation ego motion captured views projective camera requiring knowledge camera calibration faugeras hartley 
computation fundamental matrix outliers typically arise gross errors correspondence mismatches inclusion movement inconsistent majority 
caused features occluding contours shadows independently moving objects 
robust estimation impacts estimation data segmentation 
degeneracies fundamental matrix occur frequently 
second aim compare performance non robust squares methods range robust methods making intra comparisons large controlled data sets data real imagery 
allowed coupling robust techniques arrive empirically optimal statistically satisfying method 
techniques drawn applicability broad sweep computer vision problems troubled outlying data 
recovery motion structure fundamental matrix calibrated analogue essential matrix long history 
aloimonos divided research area epochs 
spent finding problem broadest terms solution 
ascertained epoch saw researchers devising constructive proofs uniqueness solution involving minimum number points longuet higgins tsai huang 
unfortunately minimalist algorithms highly sensitive noise leading erroneous belief recovery structure motion essentially problem qualitative solutions possible 
third epoch directed minimizing effects noise correspondences weng images aloimonos weng 
usually done squares framework concomitant difficulties highlighted 
fourth epoch required emphasis robust estimation provide output solution list data gross disagreement 
previous carried robust estimators context structure motion recovery torr murray kumar hanson zhang 

discussed 
organised follows 
section reviews best practice squares estimation methods fundamental matrix discusses variety error measures 
section describes method comparing non robust robust estimators method generating synthetic data 
theory estimators case deletion diagnostics random sampling developed sections section comments briefly hough transforms problems large parameter spaces 
requirement robust methods estimation standard deviation method achieve robust section 
best estimators intra category competitions compared inter category competition described section 
techniques blended quasi optimal method results demonstrated real imagery section 
section discuss results draw 

linear squares methods 
example application fundamental matrix consider movement set point image projections object undergoes rotation non zero translation views 
motion set homogeneous image points xi viewed image transformed set second image positions related fxi homogeneous image coordinate fundamental matrix faugeras hartley 
matrix degrees freedom ratio parameters significant det 
robust estimation fundamental matrix underlining symbol indicates perfect quantity distinguishing value corrupted noise assumed gaussian 

orthogonal squares regression method ignoring moment problem enforcing rank constraint correspondences system appears archetype solution linear squares regression 
linear refers linearity parameters fi eq 
written just 
errors exist measured coordinates orthogonal squares pearson ordinary squares minimizing sum squares distances shown part fig 

consider fitting hyperplane fp set points rp coordinates zi zi zi zip centroid data origin 
centring standard statistical technique involves shifting coordinate system data points centroid lies origin 
best fitting hyperplane passes centroid data pearson 
assuming noise gaussian elements equal variance hyperplane maximum likelihood estimated minimizing perpendicular sum euclidean distances points plane pearson kendall stuart min zi 
orthogonal ordinary squares distances 
torr murray subject 
constraint ensures estimate invariant transformation rotation translation scaling inhomogeneous coordinates 
example best fitting line dimensional scatter xi yi estimated minimizing axi subject constraint pearson 
reformulate eigen problem measurement matrix rows zi moment matrix eigenvalues increasing order corresponding eigenvectors orthonormal system 
best fitting hyperplane eigenvector corresponding minimum eigenvalue moment matrix 
evident zi sum squares residuals ri case perpendicular distances hyperplane 
fundamental matrix xi yi xi yi xi yi measurement matrix 
nxn nxn xn yn diagonal matrix weights feature correspondence corresponding inverse standard deviation error 
assumed homogeneous 
section estimation iteratively re weighted squares explained 
variances image coordinates different axes say image coordinates weighted dividing respective variances 
estimate minimizes mf subject jf constant diag normalization chosen realize solution equivalence class solutions different scalings 
best numerical stability origin coordinate system placed data centroid 
centring moment matrix achieved subtracting column dimensional vector mean column 
iteratively re weighted squares methods orthogonal squares method fact produce sub optimal estimate residuals minimization ri xi yi xi yi xi yi distributed 
cause outlined different residuals nearly distributed discussed 
require working knowledge solution iteratively re weighted squares minimization required bookstein sampson 
expression ri eq 
known algebraic distance 
geometrical significance example measure perpendicular distance feature quadric variety represented image coordinate space 
variance ri said meaning depends location feature correspondences 
sampson discovered similar property fitting algebraic residuals conics 
point perturbed gaussian noise minimization algebraic distance eigenvector moment matrix sub optimal 
shown kendall stuart best fitting maximum likelihood quadratic curve sum squares perpendicular geometric distances points curve minimum illustrated fig 

furthermore solution invariant euclidean transformations coordinate system 
reason problem arise fitting hyperplane residuals linear measurements algebraic geometric distances coincide fig 

joins different points conic fig 
parallel unique closed form solution unobtainable 
sampson proposed order approximation distance 

line conic fitting minimizing perpendicular geometric distances optimal 
line equivalent minimizing algebraic residual centred data conic quadric surface 
method 
noting expression residuals fits conic fundamental matrix bilinear measurements weng 
adapted sampson method computation fundamental matrix 
estimated computing min wsi zi wsi optimal weight variance residual 
dropping subscripts sampson weng optimal weighting ws gradient partial derivatives rx eq 
rx 
order approximation standard deviation residual 
calculation weights requires value fundamental matrix vice versa iterative method called 
modified method proposed conics sampson exploiting fact fundamental matrix defines quadratic image coordinates 
method computes algebraic fit eigenvalue method re weights algebraic distance sample point gradient computed previous iteration unit weights iteration 
fundamental matrix zero determinant presence noise constraint robust estimation fundamental matrix imposed minimization 
epipolar lines intersect unique epipole 
force estimated fundamental matrix rank iteration replaced nearest rank matrix calculating weights 
approach adopted proceeds follows 
singular value decomposition golub van loan recovered due noise full rank non zero singular values diag 
approximate rank matrix diag reduced rank approximation fundamental matrix optimal weights convert algebraic distance point statistical distance noise space equivalent order approximation geometric distance shown sampson pratt 
weighting breaks epipole numerator denominator approach zero indicating information correspondences closer epipole 
practice remove unstable constraints points pixel estimated epipole excluded iteration calculation 
note kanatani proposed modification sampson distance estimating essential matrix calibrated analogue distances yield identical results fundamental matrix test data continue sampson distance 
method 
luong faugeras examined sampson weighting ws suggested marginally better results obtained distance point epipolar line error minimized 
noted fundamental matrix defines epipolar geometry point corresponding image lie epipolar line fx image 
noisy measurements lie associated epipolar lines exactly 
perpendicular distance point predicted epipolar line image torr murray distance point epipolar line fx second image rx order minimize geometric distance point epipolar line image root mean square image planes point epipolar line 
ensures image receives equal consideration 
distance referred epipolar distance 
assumed errors measured location point gaussian variance coordinate system rotated axis aligned epipolar line distance point true epipolar line gaussian distribution variance equal variance image point locations approximately follows distribution degrees freedom 
method uses iterative re weighted squares estimate solution forced rank 
quantity minimized equivalent weighting algebraic distance 
effectively epipolar weighting dissimilar sampson weighting ws summary squares methods different error terms discussed method uses sum squares algebraic distances ri fxi 
method uses sum squares sampson distances ri ri method uses sum squares epipolar distances ei terms probability distributions corresponds intractable second order approximation distribution third distribution 
show comparisons squares methods torr 
summary error criteria methods similar performance superior cost function non linear gradient descent minimization gill murray 
important result best squares method performs presence outliers 
mean square sampson distance mean square epipolar distance measures accuracy solution measures evaluated comparison robust estimators 
measure justifiable 
epipolar distance merit immediately physically intuitive adoption sampson distance things recommend 
represents sum squares algebraic residuals divided standard deviations standard deviations epipolar distances unknown 
secondly kendall stuart suggested set parameters minimize orthogonal distance point curve surface maximum likelihood solution 
distance turns expensive compute provides order approximation 
thirdly discussed fully torr order approximation distance correspondence space defined manifold defined space approximation significant figures 
shown similarly approximation distance point optimally estimated correspondence hartley sturm 
fourthly value provides maximum likelihood estimate variance error coordinate 
experiment confirmed 
data perturbed noise estimate provided near 
ground truth known tests synthetic data distance noise free points estimated noise corrupted points tends zero fit improves 

method comparing robust estimators considerable appears statistical literature detection outliers context ordinary non orthogonal regression see review little done outlier detection orthogonal regression shapiro brady hyperplane fitting appears exception statistical computer vision literature 
appears large scale comparative studies reported robust estimation general hyper surfaces category estimation fundamental matrix falls minimize geometric algebraic distance 
developed evaluated number robust methods problem 
evaluation places emphasis performance criteria relative efficiency ii breakdown point defined follows 
relative efficiency regression method defined ratio lowest achievable variance estimated parameters cram rao bound kendall stuart actual variance provided method 
empirical measure achieved calculating distance di ei noise free projections synthetic world points provided estimator 
traditionally goodness fit assessed seeing parameters fit observed data 
point wrong criterion aim find set parameters best fit unknown true data 
parameters fundamental matrix primary importance structure corresponding epipolar geometry 
consequently little sense compare solutions directly comparing corresponding parameters fundamental matrices compare difference associated epipolar geometry weighted density matching points 
inadequacy fit observed data assess efficiency presence outliers demonstrated results section 
robust estimation fundamental matrix ii breakdown point estimator smallest proportion outliers may force value estimate outside arbitrary range 
normal squares estimator outlier sufficient arbitrarily alter result breakdown point number points set 
indication breakdown point gained conducting tests varying proportions outliers 
plan comparison involves levels initial simple test weed methods completely ineffective followed detailed testing remaining methods involving evaluation range real synthetic data 
return characterize tests describe generation synthetic data 
data randomly generated region visible positions synthetic camera having intrinsic coordinates equivalent aspect ratio optic centre image centre focal length pixels giving field view giving 
values chosen similar camera capturing real imagery 
projection point position second camera rotation translation motion random different test 
order simulate effects search window commonly employed feature matchers limit range depths correspondences accepted disparity lay pixels 
notion limits depth obtained pure translation max min 
fig 
show typical set point correspondences image motion vectors arising arbitrary random motion 
blob position 
overlaid epipolar lines computed image motion camera intrinsics positions 
initial achieved testing sets synthetic correspondences 
set point correspondences generated accordance synthetic camera motion additional correspondences outliers 
image torr murray 
set synthetically generated correspondences perturbed noise superimposed true epipolar geometry synthetic camera pair 
point perturbed gaussian noise standard deviation 
standard deviation actual noise free projections synthetic world points estimated epipolar lines calculated 
standard deviation exceeded times noise point positions method rejected outright 
methods passed initial test tested exhaustively increasing outlier contamination steps 
outliers mismatches generated random direction image plane minimum maximum allowable disparity pixels position image 
experiment different percentage outliers repeated different data sets size giving correspondences proportion outliers correspondences 
mentioned section assess performance method variance weighted distances di ei noise free ground truth projections synthetic world points tabulated function fraction outliers 

category estimators turn category robust estimators estimators 
description sampson distance measure epipolar distance measure replaced derived earlier 
set correspondences xi suppose wish find maximum likelihood data 
preferred priori value equivalent maximizing pr 
joint probability identical noise distribution assuming noise gaussian zero mean independent datum standard deviation pr exp 
maximizing equivalent minimizing negative logarithm constant terms course proof squares maximum likelihood estimator errors gaussian 
real conditions gaussian assumption poor 
aim estimators huber hampel follow maximum likelihood formulations deriving optimal weighting data non gaussian conditions 
outlying observations weights reduced rejected outright 
estimators minimize sum symmetric function di errors di unique minimum di 
parameters sought minimize di 
form derived particular chosen density function manner shown case gaussian errors 
usually density function chosen weighting di idi squared error reduces effects outliers estimated parameters 
typical weighting scheme statistics literature proposed huber di di di di 
standard deviation error scale known priori maximum likelihood estimate median 
robust estimation fundamental matrix 
variance distance measures measured projections noise free points function percentage outliers estimators huber 
distance measure sampson epipolar distance 
curve derived trials data 
discussion estimation deferred section 
computer vision estimation luong olsen zhang 
estimation epipolar geometry 
statistics literature analytical estimators applied ordinary squares 
done orthogonal regression area principal component analysis devlin 
conducted comparison number robust techniques estimating principal components data eigenvectors 
weightings explored concluded huber weighting weighting due di gave best estimates principal components 
large small di tends weighting inverse residual unity respectively 
numerical calculation estimators problematic best far closed form solutions exist problem appears intractable 
note weights computed estimate residuals turn requires knowledge solution 
huber suggests iterative computational scheme weights held constant values equal iteration whilst set parameters estimated 
huber proves iterations repeated local possibly global minimum objective function reached 
algorithm appendix modification iterative squares method sampson combining sampson distance weighting ws robust weighting estimators 
initial solution obtained orthogonal regression method 
gives results test synthetic data estimators weightings huber assuming known best possible case 
shows results sum squares sampson distance minimized 
graph shows variance sampson distance true noise free correspondences epipolar geometry estimated huber wins estimators similar typically poor outliers data set variance error term excess recall initial gaussian noise variance 
shows results epipolar distance measure minimized 
seen huber provides graceful degradation outliers totally fails outliers 
estimators highly vulnerable poor starting conditions algorithm converges local minimum 
unfortunately linear squares estimator initialization certainly produce poor starting conditions presence gross outliers 
observed variance estimate high compared expect added gaussian noise variance 
fact trials produced lower standard deviation standard deviation error inflated torr murray small proportion estimates totally failed converge 
weighting functions explored notably function mosteller tukey di di kumar hanson 
error function advantage performing locally gaussian small errors tapering constant large errors limiting effect 
tests failed reveal significant improvement error function cases huber performed marginally better 
implementations tried producing poorer results 
estimator routines ordinary squares error presumed variable taken numerical algorithm group library 
performance poorer huber orthogonal regression 
second gradient descent methods tried huber error function staring point 
frequency algorithm local minima led approach rejected outright non robust initialization robust initialization provided exceptionally results seen 
computation estimators highly intractable involving solution non linear equations number correspondences 
huber suggested approach computing iterated squares suitable priori knowledge parameters gross outliers easily identified 

category ii case deletion diagnostics section describes methods influence measures particularly case deletion diagnostics 
basic concept underlying influence simple 
small perturbations introduced aspect problem formulation assessment change outcome analysis 
important issues determination type perturbation scheme secondly particular aspect analysis monitor thirdly method assessment 
case deletion methods particular monitor effect analysis removing data 
instance ask epipolar geometry change deletion correspondences 
different measures influence proposed statistical literature case deletion 
differ particular regression result effect deletion measured standardization comparable observations 
methods discussed influence computed regression process inexpensive relative cost regression 
case ordinary squares interested reader referred rigorous analytical coverage theory methods 
attention orthogonal regression 
suggested arrive influence functions assessing higher order effect principal eigenvalues eigenvectors 
torr murray extended cook orthogonal regression discussed detail 
shapiro brady proposed influence measure monitors effects deletion minimum eigenvalue 
torr murray gave examples variety diagnostics estimation affine instantaneous flow 
section case deletion diagnostic developed orthogonal regression adapted estimate fundamental matrix 

extending cook case orthogonal regression derive formula influence point orthogonal regression derived extending works cook shapiro brady 
early version torr murray 
consider set points lying hyperplane measured values zi perturbation gaussian noise uniform standard deviation matrix rows squares estimate eigenvector moment matrix corresponding minimum eigenvalue 
analogy cook cook developed ordinary squares monitor effect deleting observation estimated parameters exact solutions closed form change solution calculated eigenvector perturbation theory torr golub van loan 
noted examination effects perturbation aspect model covariance matrices minimum eigenvalue shapiro brady interest appears best test eigenvector perturbations directly 
dimensional symmetric moment matrix having eigenvalues increasing order corresponding eigenvectors forming orthonormal system 
matrix perturbed multiplicity data degenerate eigenvector perturbed golub van loan mu uk case deletion ith observation means allows calculation estimate parameters ith element excluded zi uk estimate ith element deleted 
zi uk 
section comments improve estimate form analysis provides intuition nature outliers effect solution 
useful influence scalar quantity 
necessary norm characterize influence norm map vector scalar 
norm defined terms symmetric positive definite matrix give influence measure ti def robust estimation fundamental matrix eq 
ti zi uk zi ul noting ri residual ith observation ti zi uk zi ul 
contours constant ti ellipsoids dimension equal rank centred equivalently 
clearly character ti determined may chosen reflect specific concerns 
norm chosen ti invariant non singular linear transformations data 
suppose matrix torr shown approximated pseudo inverse covariance matrix parameter estimate improved estimate section 
choosing covariance matrix norm measure change solution allows alterations element parameter vector equal weight changes parameters approximate changes error measure furthermore ti defines conic parameter space principal axes determined eigenvalues eigenvectors computed zi eq 
ti mf mf mf mf 
equation leads ti zi uk zi ul torr murray noting ku uk jk influence measure ti zi zi zi 
singular value decomposition golub van loan teukolsky derive computationally simple form eq 

singular value decomposition matrix columns left hand singular vectors matrix columns right hand singular vectors diagonal matrix corresponding singular values diag ascending order smallest singular value 
vik element easy see ti zi vik vik leverage factor defined li def vik leverage factor large orthogonal projection observation zi uk large corresponding eigenvalue small 
leverage gives measure influence point large outliers residual small 
leverage key factor distinguishing ti consideration algebraic residual ri 
outlier ri small ti tend large 
note scale need known calculate relative values ti 
temporarily ignoring scaling measure ti revealing computationally efficient form ti li residual multiplied leverage factor 
statistic ti gives relative influence point regression remove outliers point maximal influence deleted regression recomputed repeating procedure data falls threshold determined 
worked example case deletion outlier detection scheme applied data set fischler bolles 
data centred giving sum squares 
singular value decomposition centred data matrix inserting values eq 
values gives perturbed results table plotted fig 

shown fig 
concentric ellipses corresponding increasing values influence measure ti determined covariance matrix parameter estimate 
parameter space ellipses constant ti ti values inwards ti 
shows clearly point exerting undue influence fit 
table compare ti diagnostic na residual diagnostic diagnostic shapiro brady measures perturbation table 
perturbed results arising deletion point estimated order approximation eq 

point 
ellipses constant ti parameter space concentric 
ellipses determined covariance matrix parameter estimate 
perturbed solutions arisen deletion points plotted 
seen points lie furthest centre ellipses 
smallest eigenvalue 
generalized distance ti correctly identifies point outlying algebraic residual ri perturbation smallest eigenvalue indicate point outlying 
table shows leverage li vi point large 
note simpler expression leverage discriminating 
noted indicating outliers directly leverages give indication points influential regression 
understand diagnostic change eigenvalue fails case note robust estimation fundamental matrix table 
ti measure correctly identifies point outlier algebraic residual ri smallest eigenvalue perturbation 
columns show values leverages li point ri ti li eigenvectors sensitive perturbations eigenvalues especially eigenvalues quite close 
leads speculation diagnostic ti detecting degeneracy small data sets 
shapiro brady overcame problem explicitly recomputing regression point deleted outliers left determine gives minimal sum squared residuals 
approach prohibitively expensive large datasets 

application computing fundamental matrix method derived assumption linear regression 
fundamental matrix gives rise quadratic image coordinates 
order minimize correct measure modification iterative squares method described section 
iteration reweighting data convert algebraic residuals correct statistical distance noise space point maximum influence deleted 
furthermore fundamental matrix projected nearest singular fundamental matrix singular value decomposition described section 
case deletion algorithms successively delete points sum squares residuals lies test 
outline new algorithm appendix 
gives average sum squares distance actual points estimated epipolar geometries influence measures torr shapiro deleting largest residual iteration 
part torr murray 
variance distance measures measured projections noise free points function percentage outliers case deletion methods 
distance measure sampson distance predicted epipolar line 
derived trials data 
uses sampson measure part epipolar distance measure 
large data set performances similar significant statistical difference estimators 
smaller data sets ti give better results just deleting largest residual 
variance lower shapiro residual methods outliers 
example outliers variance noise free points ti shapiro residual methods respectively 
variance methods varies ti gave better performance linear data fitting hyperplanes 
convergence case deletion diagnostics superior convergence estimators solution typically accurate 
estimation process data reweighted iteration case deletion schemes datum considered iteration 
leads increased accuracy expense iterations generally outlier 
disadvantage case deletion schemes require fairly estimate 
improving estimate ti experiments reported adopted techniques increase accuracy estimation process 
improvement estimate iterative methods 
second improvement uses calculated remove bias solution third improvement uses calculated re estimate covariance matrix 
improving estimate 
closed form solution obtained order approximation improved 
golub provides method computing eigenvalues eigenvectors matrix uu operations diagonal 
general analysis gu methods implemented case deletion shapiro brady providing marked improvement result 
non parametric removal bias 
luong 
show linear methods produce biased solution fundamental matrix analogous conic fitting kanatani 
kanatani gives bias linear estimation essential matrix 
kanatani follows parametric approach removal bias assumption gaussian noise 
suggest non parametric approach robust outliers failure gaussian assumptions 
jackknife known extensively studied statistical non parametric technique gain unbiased estimate covariance 
fj jackknife estimate shown bias decreases polynomial function number observations kendall stuart 
original sample data subsamples data formed systematically deleting observation turn 
jackknife fj nf bias parameter 
seen small amount extra computation necessary get bias free estimates quantities calculated 
problem removal bias increase error biased solution lower error 
full analysis bias removal complicated scope 
tests average reduction error fit true points dependent type motion nearer image orthographic conditions bias remove due fact problem linear orthographic case 
average hides fact correspondences near high curvature points fundamental matrix consider quadric manifold space image coordinates greater bias 
non parametric estimation covariance 
experimentation revealed estimation covariance pseudo inverse moment matrix poor 
improved estimate really beneficial large values may gained little extra computational cost jackknife estimate covariance matrix 
category iii random sampling algorithms early example robust algorithm random sample consensus paradigm ransac fischler bolles 
large proportion data may outlying approach opposite conventional smoothing techniques 
data possible obtain initial solution attempting identify outliers small subset data feasible estimate parameters robust estimation fundamental matrix point subsets line correspondences fundamental matrix process repeated times different subsets ensure chance subsets contain data points 
best solution maximizes number points residual threshold 
outliers removed set points identified non outliers may combined give final solution 
initial exploration ransac method estimate epipolar geometry reported torr murray 
estimate fundamental matrix points selected form data matrix 
null space moment matrix dimension barring degeneracy 
defines parameter family exact fits correspondences 
introducing constraint det leads cubic det real solutions 
total number consistent features solution recorded 
order determine feature pair consistent fundamental matrix sampson distance correspondence image compared threshold described section 
ideally possible subsample data considered usually computationally infeasible important question subsample dataset required statistical significance 
fischler bolles rousseeuw proposed slightly different means calculation give broadly similar numbers 
follow approach 
number samples chosen sufficiently high give probability excess subsample selected 
expression probability fraction contaminated data number features sample 
table gives torr murray table 
number subsamples required ensure probability data points selected subsample non outliers 
fraction contaminated data features sample values number subsamples required ensure 
generally better take samples needed samples degenerate 
seen far computationally prohibitive robust algorithm may require fewer repetitions outliers directly linked number proportion outliers 
seen smaller data set needed instantiate model fewer samples required level confidence 
fraction data contaminated unknown usual educated worst case estimate level contamination order determine number samples taken 
updated larger consistent sets worst guess set inliers discovered reduced 
general correspondence sample insufficient spread disparities estimate obtained sample unique 
example degeneracy 
consider correspondences shown fig 

epipolar geometries fit data shown view 
veridical epipolar geometry erroneous solution consistent cubic eq 

clearly result estimated sample consistent correspondences conform underlying motion 
desirable devise scheme determine subsample degenerate 
detection degeneracy ransac subject torr 
ransac originated computer vision years similar highly robust estimator developed independently field statistics rousseeuw median squares lms estimator rousseeuw 
algorithms differ slightly solution giving median selected estimate rousseeuw 
algorithms implemented sampson epipolar distances algebraic distance appendix 
variances distances function percentage outliers fig 
sampson epipolar measures 
lms ransac perform similarly lms giving slightly better performance contamination 
random sampling algorithm min minimize probability randomness described stewart 
lms require priori knowledge variances 
appears assumptions error distribution inappropriate estimation fundamental matrix 
discussed torr 

typical set points selected ransac alongside epipolar geometries exactly fit data true epipolar geometry spurious 
robust estimation fundamental matrix 
variance distance measures measured projections noise free points function percentage outliers ransac lms random sampling method 
distance measure sampson epipolar distance 
curve derived trials data 
results show generally lms gives slightly better result 

note hough transforms study techniques fall categories described proved successful 
worth mentioning hough transform ballard brown long history valuable service computer vision 
parameter space divided cells datum adds vote cell parameter space parameters consistent datum 
voting cells parameter space number votes greater threshold marked representing possible solutions 
hough transform runs problems dimension parameter space high space requirement exponential dimensionality computational expense fast hough transform li rises exponentially dimension parameter space mclauchlan 
dimensionality fundamental matrix coarsest quantization parameter space say cells dimension demand cells 

robust determination standard deviation robust techniques eliminate outliers founded knowledge standard deviation error outliers typically discriminated inliers set inliers di set outliers recall di sampson distance 
section describes robust method estimating standard deviation related characteristics image feature detector matcher 
value unknown case estimated data 
outliers data estimated directly standard deviation residuals non linear squares minimization 
outliers minority estimate variance derived median squared error chosen parameter fit rousseeuw 
known di asymptotically consistent estimator di distributed cumulative distribution function gaussian probability density function 
shown empirically rousseeuw recall number data number parameters correction factor improves estimate standard deviation 
noting estimate di 
torr murray lms algorithm obtain estimate median estimate standard deviation 
final result obtained non linear minimization estimate standard deviation improved em algorithm dempster 
inlier outlier distribution gaussian different parameters em algorithm guaranteed increase likelihood estimated standard deviation data 
standard deviation estimated pair images results filtered time 
gaussian assumptions shown bar shalom fortmann correspondences required ensure chance variance actual value 
image pairs give rise unusually high standard deviations possess independently moving objects detection discussed torr 
random perturbations image correspondences unit standard deviation estimate standard deviation conflation image error error estimator 

comparison robust categories iii graph comparing best method squares category robust categories iii described 
expected squares method non robust giving standard deviation data outliers 
estimator huber error provides inaccurate results figures outliers breaks 
case deletion diagnostics provided accurate estimate standard deviation assumed known experiments 
standard deviation unknown perform badly 
lms algorithm gives best performance error measures 
ransac gave equivalent slightly worse performance standard deviation error term known shown ransac perform outliers roth 
tallies experience ransac motion segmentation torr murray 
earlier noted iterative estimation estimators successful starting estimate 
output random sampling linear regression starting estimate estimation iterative huber algorithm improvement shown range fig 

improvement appears small significant effect computed epipolar geometry shown fig 

part shows epipolar geometry estimated lms method shows estimated iterative improvement result huber estimator 
closer 
variance noise free points estimated best category estimator sampson distance measure epipolar distance measure respectively 
curve derived tests points 
seen random sampling lms give best result case deletion method performs requires exact estimate achieve result limiting practice 
near optimal estimator suggest shown lms method initialize estimator significantly better random sampling method 
robust estimation fundamental matrix 
comparison epipolar geometry estimated lms lms followed huber 
data corrupted fig 
true epipolar geometry seen 

synthetically generated correspondences inliers outliers 
veridical epipolar geometry shown 
part shows epipolar geometry recovered non robust algorithm shows robust lms plus huber combination 
veridical geometry derived uncorrupted data shown earlier fig 

previous experiment highlights sensitivity recovered epipolar geometry fitting differences robust methods 
worth showing difference best non robust best robust methods 
shows correct correspondences mismatches data 
shows veridical epipolar geometry shows recovered iterative square method gives recovered robust symbiotic combination lms huber 
difference obviously substantial 

empirically optimal algorithm key far 
major categories robust estimator random sampling gives best results 
torr murray 
possible improve performance mixing robust methods 
random sampling initialize iterative estimators yields empirically optimal combination 
observations allowed create empirically optimal combination algorithms summarized 
algorithm determine set putative correspondences unguided matching image matching epipolar geometry storing point ordered list correspondences 
correspondences supplied ransac initialized approximate guess standard deviation 
instance calibration example shown initial estimate standard deviation 
lms run algorithm provides updated estimate median running em reduces estimate 
iterated estimator em algorithm gives non linear gradient descent part algorithm 
best estimates handed estimation algorithm refinement 
iterative re weighted squares huber robust weighting function 
practice iterations adequate 
non linear gradient descent algorithm replaces squares algorithm estimation scheme 
non linear minimization conducted method described gill murray 
minimization uses parameterization enforces det condition 
note correspondences included stage 
stages gross outliers effectively removed huber function places ceiling value errors parameters move iterated search marginal outliers inliers 
avoids ransac unduly biasing estimation stages 
note stage iterated step algorithm re assesses putative correspondences epipolar geometry implicit running estimate point correspondence outlier reasonable match point list potential matches inlier algorithm alters match locally minimize di 
empirically optimal algorithm 
generate matches unguided matching generate point feature image ordered list best matching points image similarly image 
random sampling apply estimator best set matches image vice versa 
ransac known lms 

re assess matches re estimate em algorithm 

estimation refine iterative squares incorporating sampson weights modified huber robust weighting iteratively reweighted squares 

re assess matches re estimate em algorithm 

estimation replace iterative squares non linear method parameterization ensures det see luong 

re assess matches re estimate em algorithm 

real images demonstrate performance combination random sampling estimators real imagery results aid corner matching 
step lms algorithm synthetic data ransac method initial estimate certain outliers 
guided feature matching 
shows images calibration grid 
similarity features matching difficult fig 
shows correspondences postulated feature matcher purely intensities 
considerable numbers mismatches 
part shows results ransac huber eliminate outliers computation fundamental matrix associated epipolar geometry guide matching 
part shows estimated epipolar geometry calibration grid initial set matches 
standard deviations number inliers method summarized table example figs 

seen robust estimation fundamental matrix 
view object creates matching difficulties impoverished matcher shown number mismatches 
shows matches consistent epipolar geometry eliminating outliers 
part shows estimated epipolar geometry matches rejected re matched 
torr murray table 
performances best class estimator suggested empirically optimal method real images terms number inliers standard deviation error set 
initial estimate standard inliers 
estimators case deletion ransac optimal method fig 
inliers inliers inliers inliers 
shows matches consistent epipolar geometry eliminating outliers 
huber estimator initialized converged incorrect solution 
case deletion diagnostic despite gross outliers converged reasonable solution 
random sampling performs best followed case deletion provide better estimates estimation poorly initialized 
results poorly initialized estimation case deletion diagnostics compared fig 
table seen standard deviations inliers similar estimators estimator clearly failed eliminate outliers 
result demonstrates goodness fit observed data criterion judge estimator bearing earlier decision reject measure relative efficiency 
football sequence 
figures show images sequence taken football match motion computed matching image corner features shown 
dominant image motion result camera panning 
shows inliers shows outliers lying predominantly independently moving shows results estimator note results fairly major outliers 
reason estimators perform badly case outliers small proportion data 
shows inliers case deletion diagnostic 
data degenerate football supporters lie approximately plane consistent solutions 
different solutions goodness fit different correspondences indicated inliers plane 
observed kumar hanson ransac performs data near degenerate subsequent estimator ransac helps stabilise situation 
walking sequence 
figures images sequence showing person camera camera moves keep view 
robust estimation fundamental matrix 
consecutive images football match camera panning 
inliers mainly crowd texture shown outliers attached independently moving players 
inliers estimator 
inliers case deletion diagnostic 
torr murray 
images sequence showing movement person camera camera moves keep view resulting correspondences corner matching 
combined ransac huber estimator segments set correspondences inliers outliers 
inliers estimator 
inliers case deletion diagnostic 
shows inliers outliers 
inspection seen grossly incorrect correspondences classified outlying show corresponding results 
data near degenerate containing large planes fit stable outliers included 
robust estimation story stability considered 
discussed 

discussion 
actual versus expected performance robust estimation techniques far superior non robust methods squares course imperfect 
synthesized data allows objective measure performance obtained shown combination random sampling huber estimation fig 

types error possible type outlier wrongly classified inlier type ii inlier wrongly classified outlier 
seen outliers correctly classified great type errors 
better worse expected 
estimation standard deviation obviously plays key le determines threshold error considered outlying 
estimating necessary steer scylla higher estimate increase number type errors whilst decreasing number type ii errors vice versa 
expected bounds proportion type error estimated follows 

percentage outliers inliers correctly discovered percentages contamination best robust estimator combining random sampling huber 
robust estimation fundamental matrix gaussian assumptions population lie mean 
confidence interval established error follows inliers identified correctly type errors 
higher percentage type ii errors expected 
epipolar constraint allows disambiguation dimension mismatch happens lie epipolar line identified 
argument minimizing distance similar argument constructed search window size pixels maximum area swept distance epipolar line el 
chance mismatch area random value approximately expect inliers correctly identified 
seen fig 
algorithm comes close attaining values wide range 
em algorithm applied real image data difference inlier outlier standard deviations substantial allowing clear discrimination inliers outliers cases 
images fig 
initial estimate standard deviation application em algorithm standard deviation inliers outliers 
generally ransac algorithm robust initial estimate inlier outlier distributions distinct 

improving efficiency accuracy taubin method taubin proposed generalized eigenvector fit implicit curves surfaces invariant choice coordinate system 
chooses minimize objective function dt ri ri may solving generalized eigenvector system mf rx ry setting equal eigenvector corresponding smallest eigenvalue 
taubin method may torr murray iterated weighting iteration similar manner sampson 
method advantages computational complexity linear method experimentation trials gave lower error 
improvement apparent trials large gains efficiency accuracy sufficient prompt study 

surveyed range robust estimators applied computation fundamental matrix 
extended results obtained statistical literature ordinary regression orthogonal regression computation fundamental matrix linear problem hyperplane fitted geometric distance computing fundamental matrix non linear bilinear problem hyper surface fitted 
intra category comparisons large synthetic data sets best category compared inter category competition 
methods evaluated relative efficiency breakdown point 
random sampling techniques shown provide best solution 
tests lms generally gave better fit ransac circumstances 
outliers lms provide solution occur cases independent motion 
secondly half data fitted multiple solutions half data plane lms fit unstable 
ransac generally robust algorithm lms applied input data fall categories 
estimators satisfying statistical standpoint shown suffer initial estimate poor initialization performed non robust squares 
estimator method initialized robust random sampling combination provided better results random sampling 
small improvements marked effect resulting epipolar geometry 
estimation iterative squares scheme developed 
enforce constraint determinant fundamental matrix zero final step non linear minimization replaces iterative squares parameterization enforces det 
ensures unbiased recovery epipole described luong 

tripartite approach random sampling iterative estimation estimation gradient descent gives satisfactory results 
experiments real imagery showed estimation improved robust estimation provide epipolar constraint matcher 
method functions high degree outlier contamination expense computation time 
outliers cost issue case deletion diagnostics provide efficient way judging relative merit correspondences 
case deletion methods smaller data sets 
considered structural constraints particular visibility constraint 
outliers consistent epipolar geometry appear camera allowing identified outlying 
considered natural extension estimation process motion segmentation explored torr murray torr 
notable comparisons robust estimators computer vision compare findings studies 
meer 
compared estimators lms data smoothing lms halved error giving clearly superior results 
kumar hanson compared estimation random sampling lms ransac error criteria problem pose determination 
reach quote li robust regression technique proven superior situations partly handling forms influential observations 
results force disagree pessimistic view explore reasons difference opinion 
reject random sampling certain cases due rationale 
observations inliers noisy conceivable pose returned consensus algorithm explains significant set observation low leverage quite inlier high leverage outlier 
occur percent data consistent multiple solution lms case median near zero wildly different solutions data 
suggested approach ransac combined estimator typically overcome problem 
suggest ransac reasonable guess standard deviation possibly obtained lms noted final result reasonably insensitive guess zhang 
developed independently 
explored robust estimators estimate fundamental matrix 
little comparative estimators suggest lms estimate generally prefer ransac followed input controlled exclude independent motion noted previously 
method requires somewhat sampling random sampling phase points require 
leads convergence 
furthermore obtained points automatically det obtained points 
useful final result ransac fed directly non linear minimizer 
difference zhang epipolar distance rejected favour sampson robust estimation fundamental matrix measure 
theoretically satisfying closer approximation maximum likelihood estimator trials produces slightly better results 
points linear methods carry bias 
discussed robust nonparametric method removing bias 
carried analysis assessing fit true data set ground truth known 
note test databases larger previous studies giving greater indication reliability result 

postscript degeneracy noted random sampling degeneracy point samples 
fact broader problem long neglected statistics literature 
possible solutions determined true data degenerate degeneracy broken handful rogue outliers 
gives example 
parts show frames resulting point correspondences 
consecutive images buggy rotating turntable 
matches superimposed second image 
show epipolar geometries generated distinct fundamental matrices correspondences consistent fundamental matrix consistent fundamental matrix epipolar geometries obviously differ 
torr murray sequence toy truck rotated turntable 
majority data degenerate figs 
shows epipolar geometries consistent veridical data 
running random sampling generate approximations inliers second 
random sampling performed set solution 
question arises determine solution valid data degenerate 
question problematic outliers data 
account degeneracy arise detect conduct estimation presence degeneracy torr 
appendix estimator algorithm 
initialize weights wi correspondence 

number iterations weight ith constraint multiplying wi calculate orthogonal regression 
project estimated nearest rank matrix singular value decomposition 
calculate algebraic residuals ri 
correspondence dropping subscript calculate weighting ws calculate distance di 
calculate huber di di di di 
case deletion algorithm 
set weights wi correspondence 

inliers weight ith constraint multiplying wi 
calculate orthogonal regression correspondences 
project estimated nearest rank matrix singular value decomposition 
calculate algebraic residuals ri 
calculate influence correspondence ti 
cast correspondence largest ti 
correspondence dropping subscript calculate weighting ws calculate distance di 
random sampling algorithm 
repeat samplings determined table select random sample minimum number data points parameter estimate calculate distance measure di feature ransac calculate number inliers consistent method prescribed section lms estimator calculate median error 

select best solution biggest consistent data set 
case ties select solution lowest standard deviation residuals 

re estimate parameters data identified consistent 
effective possible computationally expensive estimator powell method teukolsky nag may point 
acknowledgments supported gr research studentship uk engineering physical science research council 
authors paul beardsley andrew zisserman steven maybank larry shapiro mike brady reviewers comments 
notes 
points unequal variance element may weighted standard deviation 

kanatani page provides interesting discussion assertion 

fit victims bed stretching 
hartley suggested preconditioning fundamental matrix replaced nearest rank equivalent 

case different axes different variances transform data scaling coordinates column standard deviation order obtain uniform variance 
ballard brown 
computer vision 
prentice hall new jersey 
bar shalom fortmann 
tracking data association 
academic press 
beardsley torr zisserman 
model acquisition extended image sequences 
report department engineering science university oxford 
bookstein 
fitting conic sections scattered data 
computer vision graphics image processing 

sensitivity analysis linear regression 
john wiley new york 
cook 
characterisations empirical influence function detecting influential cases regression 
technometrics 

influence principal component analysis 
biometrika 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
roy 
statist 
soc 
devlin 
robust estimation dispersion matrices principal components 
amer 
stat 
assoc 
faugeras 
seen dimensions uncalibrated stereo rig 
proc 
nd european conference computer vision sandini ed santa margherita ligure italy springer verlag vol 
lncs pp 

fischler bolles 
random sample consensus paradigm model fitting application image analysis automated cartography 
commun 
assoc 
comp 
mach 
gill murray 
algorithms solution nonlinear squares problem 
siam num 
anal 
golub 
modified eigenvalue problems 
siam review 
golub van loan 
matrix computations 
john hopkins university press 
gu 
singular value decomposition 
siam matrix analysis applications 
hampel rousseeuw 
robust statistics approach influence functions 
wiley new york 
hartley 
estimation relative camera positions uncalibrated cameras 
proc 
nd european conference computer vision sandini ed santa margherita ligure italy springer verlag vol 
lncs pp 

robust estimation fundamental matrix hartley 
defence point algorithm 
proc 
th int 
conf 
computer vision boston ma pp 

ieee computer society press los alamitos ca 
hartley sturm 
triangulation 
proc 
arpa image understanding workshop pp 

see proc 
computer analysis images patterns prague vol 
lncs springer verlag pp 

mosteller tukey 
eds 
robust regression 
john wiley sons 
huber 
robust statistics 
john wiley sons 
kanatani 
geometric computation machine vision 
oxford university press 
kanatani 
statistical bias conic fitting renormalization 
ieee trans 
pattern analysis machine intelligence 
kanatani 
statistical optimization geometric computation theory practice 
elsevier science amsterdam 
kendall stuart 
advanced theory statistics 
charles griffin london 
kumar hanson 
robust methods estimating pose sensitivity analysis 
computer vision graphics image processing 
li 
exploring data tables trends shapes 
robust regression mosteller tukey eds john wiley sons pp 

li 
fast hough transforms hierarchical approach 
computer vision graphics image processing 
longuet higgins 
computer algorithm reconstructing scene projections 
nature 
luong 
matrice calibration sur environnement vers plus grande des systemes 
ph thesis paris university 
luong deriche faugeras 
determining fundamental matrix analysis different methods experimental results 
inria technical report inria sophia antipolis 

robust estimators multivariate location scatter 
ann 
stat 
mclauchlan 
describing textured surfaces stereo vision 
ph thesis ai vision research unit university sheffield 
meer mintz rosenfeld 

robust regression methods computer vision review 
international journal computer vision 
mosteller tukey 
data analysis regression 
addison wesley reading ma 
numerical algorithms group 
nag fortran library vol 
olsen 
epipolar line estimation 
proc 
nd european conference computer vision sandini ed santa margherita ligure italy springer verlag vol 
lncs pp 

pearson 
lines planes closest fit systems points space 
philos 
mag 
ser 

pratt 
direct squares fitting algebraic surfaces 
computer graphics 
roth levine 
extracting geometric primitives 
computer vision graphics image processing 
rousseeuw 
robust regression outlier detection 
wiley new york 
torr murray sampson 
fitting conic sections scattered data iterative refinement bookstein algorithm 
computer graphics image processing 
shapiro brady 
rejecting outliers estimating errors orthogonal regression framework 
phil 
trans 
soc 
lond 

aloimonos 
multi frame approach visual motion perception 
international journal computer vision 

applied nonparametric statistical methods 
chapman hall london 
stewart 
new robust estimator computer vision 
ieee trans 
pattern analysis machine intelligence 
taubin 
estimation planar curves surfaces nonplanar space curves defined implicit equations applications edge range image segmentation 
ieee trans 
pattern analysis machine intelligence 

elements statistical computing 
chapman hall new york 
torr 
outlier detection motion segmentation 
phil 
thesis university oxford 
torr murray 
statistical detection non rigid motion 
proc 
rd british machine vision conference leeds hogg ed springer verlag pp 

torr murray 
statistical detection independent movement moving camera 
image vision computing 
torr murray 
outlier detection motion segmentation 
proc 
sensor fusion vi boston ma schenker ed vol 
spie pp 

torr murray 
stochastic motion segmentation 
proc 
rd european conference computer vision stockholm 
ed springer verlag pp 

torr zisserman maybank 
robust detection degeneracy 
proc 
th int 
conf 
computer vision boston ma ieee computer society press los alamitos ca pp 

torr zisserman murray 
motion clustering trilinear constraint views 
europe china workshop geometrical modelling invariants computer vision mohr wu eds springer verlag pp 

torr maybank zisserman 
robust detection degenerate configurations fundamental matrix 
report department engineering science university oxford 
teukolsky press flannery vetterling 
numerical recipes art scientific computing 
cambridge university press cambridge 
tsai huang 
uniqueness estimation threedimensional motion parameters rigid objects curved surfaces 
ieee transactions pattern analysis machine intelligence 
weng huang ahuja 
motion structure perspective views algorithms error analysis error estimation 
ieee transactions pattern analysis machine intelligence 
weng ahuja huang 
optimal motion structure estimation 
ieee trans 
pattern analysis machine intelligence 
zhang deriche faugeras luong 
robust technique matching uncalibrated images recovery unknown epipolar geometry 
ai journal 
