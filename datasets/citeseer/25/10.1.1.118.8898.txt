integrating topics syntax thomas griffiths mark steyvers mit edu uci edu massachusetts institute technology university california irvine cambridge ma irvine ca david blei joshua tenenbaum blei cs berkeley edu mit edu university california berkeley massachusetts institute technology berkeley ca cambridge ma statistical approaches language learning typically focus short range syntactic dependencies long range semantic dependencies words 
generative model uses kinds dependencies simultaneously find syntactic classes semantic topics despite having representation syntax semantics statistical dependency 
model competitive tasks part speech tagging document classification models exclusively short long range dependencies respectively 
word appear sentence reasons serves syntactic function provides semantic content 
words play different roles treated differently human language processing function content words produce different patterns brain activity different developmental trends 
language learner discover syntactic semantic classes words 
cognitive scientists shown unsupervised statistical methods identify syntactic classes extract representation semantic content methods captures interaction function content words recognizes roles distinct 
explore statistical learning prior knowledge syntax semantics discover difference function content words simultaneously organize words syntactic classes semantic topics 
approach relies different kinds dependencies words produced syntactic semantic constraints 
syntactic constraints result relatively short range dependencies spanning words limits sentence 
semantic constraints result long range dependencies different sentences document similar content similar words 
model capture interaction short long range dependencies 
model generative model text hidden markov model hmm determines emit word topic model 
different capacities components model result factorization sentence function words handled hmm content words handled topic model 
component divides words finer groups different criterion function words divided syntactic classes content words divided semantic topics 
model extract clean syntactic semantic classes identify role words play document 
competitive quantitative tasks part speech tagging document classification models specialized detect short long range dependencies respectively 
plan follows 
introduce approach considering general question syntactic semantic generative models combined arguing composite model necessary capture different roles words play document 
define generative model form describe markov chain monte carlo algorithm inference model 
results illustrating quality recovered syntactic classes semantic topics 
combining syntactic semantic generative models probabilistic generative model specifies simple stochastic procedure data generated usually making unobserved random variables express latent structure 
defined procedure inverted statistical inference computing distributions latent variables conditioned dataset 
approach appropriate modeling language words generated latent structure speaker intentions widely statistical natural language processing 
probabilistic models language typically developed capture short range long range dependencies words 
hmms probabilistic context free grammars generate documents purely syntactic relations unobserved word classes bag words models naive bayes topic models generate documents semantic correlations words independent word order :10.1.1.110.4050
considering factors influencing words appear documents models assume words assessed single criterion posterior distribution hmm group nouns play syntactic role vary contexts posterior distribution topic model assign determiners topics bear little semantic content 
major advantage generative models modularity 
generative model text specifies probability distribution words terms probability distributions words different models easily combined 
produce model expresses short long range dependencies words combining models sensitive kind dependency 
form combination chosen carefully 
mixture syntactic semantic models word exhibit short range long range dependencies product models word exhibit short range long range dependencies 
consideration structure language reveals models appropriate 
fact subset words content words exhibit long range semantic dependencies words obey short range syntactic dependencies 
asymmetry captured composite model replace probability distributions words syntactic model semantic model 
allows syntactic model choose emit content word semantic model choose word emit 
composite model explore simple composite model syntactic component hmm semantic component topic model 
graphical model composite shown 
model defined terms sets variables sequence words wn wi words sequence topic assignments zn zi topics sequence classes cn ci classes 
class say ci designated semantic class 
zth topic associated distribution words network neural networks image images object kernel support vector network images image obtained kernel output objects svm output described objects 
trained obtained described neural network trained svm images composite model 
graphical model 
generating phrases 
class associated distribution words document distribution topics transitions classes ci ci follow distribution si document generated procedure 
sample dirichlet prior 
word wi document draw zi draw ci ci draw wi draw wi provides intuitive representation phrases generated composite model 
shows class hmm 
classes simple multinomial distributions words 
third topic model containing topics 
transitions classes shown arrows annotated transition probabilities 
topics semantic class probabilities choose topic hmm transitions semantic class 
phrases generated path model choosing word distribution associated syntactic class topic followed word distribution associated topic semantic class 
sentences syntax different content generated topic distribution different 
generative model acts playing game semantic component provides list topical words shown black slotted templates generated syntactic component shown gray 
inference em algorithm applied graphical model shown treating document distributions topics classes transition probabilities parameters 
em produces poor results topic models parameters local maxima 
consequently focused approximate inference algorithms 
markov chain monte carlo mcmc see perform full bayesian inference model sampling posterior distribution assignments words classes topics 
assume document specific distributions topics drawn dirichlet distribution topic distributions drawn dirichlet distribution rows transition matrix hmm drawn dirichlet distribution class distributions re drawn dirichlet distribution dirichlet distributions symmetric 
gibbs sampling draw iteratively topic assignment zi class assignment ci word wi corpus see 
words class assignments topic assignments hyperparameters zi drawn zi zi wi di zi di zi zi zi ci ci di zi number words document di assigned topic zi zi wi number words assigned topic zi wi counts include words ci exclude case obtained conditional distributions conjugacy dirichlet multinomial distributions integrate parameters 
similarly conditioned variables ci drawn ci wi ci ci ci zi zi ci ci ci ci ci ci ci ci ci ci ci ci ci ci ci ci ci ci ci zi wi ci wi number words assigned class ci wi excluding case ci ci number transitions class ci class ci counts transitions exclude transitions ci 
indicator function value argument true 
increasing order hmm introduces additional terms ci affect sampling 
results tested models brown corpus concatenation brown tasa corpora 
brown corpus consists documents word tokens part speech tags token 
tasa corpus untagged collection educational materials consisting documents word tokens 
words appearing fewer documents replaced asterisk punctuation included 
combined vocabulary size 
dedicated hmm class sentence start markers 
addition running composite model examined special cases model hmm classes start semantic classes equivalent latent dirichlet allocation lda hmm semantic class distribution vary documents simply different hyperparameter classes :10.1.1.110.4050
brown corpus ran samplers lda st nd rd order hmm composite models chains iterations samples lag iterations burn iterations 
brown tasa ran single chain iterations lda rd order hmm composite models 
gaussian metropolis proposal sample hyperparameters draws hyperparameter gibbs sweep 
syntactic classes semantic topics components model sensitive different kinds dependency words 
hmm sensitive short range dependencies constant documents topic model sensitive long range dependencies vary documents 
consequence hmm allocates words vary contexts semantic class differentiated topics 
results algorithm taken th iteration rd order composite model brown tasa shown 
model cleanly separates words play syntactic semantic roles sharp contrast results lda model shown words forced topics 
syntactic categories include prepositions pronouns past tense verbs punctuation 
state hmm shown eighth column emits common nouns majority nouns assigned semantic class 
designation words syntactic semantic depends corpus 
comparison applied rd order composite model topics classes set blood body game heart land water drink ball trees classes picture story alcohol tree farmers government film team image matter bottle farm state lens play blood forest farmers government light water story drugs ball heart trees land state eye matter stories drug game pressure forests crops federal lens molecules poem alcohol team body land farm public image liquid characters people lungs soil food local mirror particles poetry drinking baseball oxygen areas people act eyes gas character person players vessels park farming states glass solid author effects football arteries wildlife wheat national object substance poems player area farms laws objects temperature life body field breathing rain corn department lenses changes poet basketball said time new way see years came day went may part great know number get called kind small go place little take old find upper topics extracted lda model 
lower topics classes composite model 
column represents single topic class words appear order probability topic class 
classes give probability words list terminated words account probability mass nips papers volumes 
full text acknowledgments section excluding section headers 
resulted word tokens 
replaced words appearing fewer papers asterisk leading types 
sampling scheme brown tasa 
selection topics classes th iteration shown 
words convey semantic information setting model algorithm network form part syntax nips consistent words documents leads incorporated syntactic component 
identifying function content words identifying function content words requires information syntactic class semantic context 
machine learning word control innocuous verb important part content 
likewise graph refer indicate content related graph theory 
tagging classes indicate control appears verb noun deciding graph refers requires information content rest document 
factorization words hmm lda components provides simple means assessing role word plays document evaluating posterior probability assignment lda component 
results procedure identify content words sentences nips papers shown 
probabilities evaluated averaging assignments samples take account semantic context document 
result combining long range dependencies model able pick words sentence concern content document 
selecting words high probability image data state membrane chip experts kernel network images gaussian policy synaptic analog expert support neural object mixture value cell neuron gating vector networks objects likelihood function digital hme svm output feature posterior action current synapse architecture kernels input recognition prior reinforcement dendritic neural mixture training views distribution learning potential hardware learning space inputs em classes neuron weight mixtures function weights pixel bayesian optimal conductance function machines visual parameters channels vlsi gate set outputs see model networks show trained algorithm values note obtained system results consider described case models denotes assume problem parameters network units remains need method data represents propose defined approach functions exists describe generated problems suggest shown process algorithms 

topics classes composite model nips corpus 
contrast approach study network activity control single cell parameters input resistance time space constants parameters crucial sic integration 
integrated architecture combines feed forward control error feedback adaptive control neural networks 
words proof convergence require softassign algorithm return doubly stochastic matrix sinkhorn theorem guarantees matrix merely close doubly stochastic reasonable metric 
aim construct portfolio maximal expected return risk level time horizon simultaneously obeying institutional legally required constraints 
left graph standard experiment right training samples 
graph called guest graph called host graph 
function content words nips corpus 
graylevel indicates posterior probability assignment lda component black highest 
boxed word appears function word content word element pair sentences 
words low frequency treated single word type model 
assigned syntactic hmm classes produces templates writing nips papers content words inserted 
example replacing content words model identifies second sentence content words appropriate topic write integrated architecture combines simple probabilistic syntax topic semantics generative models 
marginal probabilities assessed marginal probability data model harmonic mean likelihoods iterations sampling standard method evaluating bayes factors mcmc 
probability takes account complexity models complex models penalized integrating latent space larger regions low probability 
results shown 
lda outperforms hmm brown corpus hmm performs lda larger brown tasa corpus 
composite model provided best account corpora marginal likelihood lda hmm brown composite st nd rd st nd rd marginal likelihood lda brown tasa hmm composite st nd rd st nd rd log marginal probabilities corpus different models 
labels horizontal axis indicate order hmm 
adjusted rand index hmm tags composite top st nd rd st nd rd st nd rd st nd rd brown brown tasa brown brown tasa adjusted rand index frequent words dc hmm composite part speech tagging hmm composite distributional clustering dc 
able whichever kind dependency information predictive 
higher order transition matrix hmm composite model produced little improvement marginal likelihood brown corpus rd order models performed best brown tasa 
part speech tagging part speech tagging identifying syntactic class word standard task computational linguistics 
unsupervised tagging methods lexicon identifies possible classes different words 
simplifies problem words belong single class 
genuinely unsupervised recovery parts speech assess statistical models language learning distributional clustering 
assessed tagging performance brown corpus tagsets 
set consisted brown tags excluding sentence markers leaving total tags 
set collapsed tags high level designations adjective adverb conjunction determiner foreign noun preposition pronoun punctuation verb 
evaluated tagging performance adjusted rand index measure concordance tags class assignments hmm composite models th iteration 
adjusted rand index ranges expectation 
results shown 
models produced class assignments strongly concordant part speech hmm gave slightly better match full tagset composite model gave closer match top level tags 
partly words vary strongly frequency contexts get assigned semantic class composite model misses fine grained distinctions expressed full tagset 
hmm composite model performed better distributional clustering method described form frequent words brown clusters 
compares clustering classes words hmm composite models trained brown 
document classification documents brown corpus classified groups editorial journalism romance fiction 
assessed quality topics recovered lda composite models training naive bayes classifier topic vectors produced models 
computed classification accuracy fold cross validation th iteration single chain 
models perform similarly 
baseline accuracy choosing classes prior 
trained brown lda model gave mean accuracy number parentheses standard error 
st nd rd order composite models gave respectively 
trained brown tasa lda model gave st 
nd rd order composite models gave respectively 
slightly lower accuracy composite model may result having fewer data find correlations sees words allocated semantic component account approximately words corpus 
composite model described captures interaction short longrange dependencies words 
consequence posterior distribution latent variables model picks syntactic classes semantic topics identifies role words play documents 
model competitive part speech tagging classification models specialize short long range dependencies respectively 
clearly model justice depth syntactic semantic structure interaction 
illustrates sensitivity different kinds statistical dependency sufficient stages language acquisition discovering syntactic semantic building blocks form basis learning sophisticated representations 

tasa corpus appears courtesy tom landauer touchstone applied science associates nips corpus provided sam roweis 
supported darpa calo program ntt communication science laboratories 
neville mills lawson 
language different neural different sensitive periods 
cerebral cortex 
brown 
language 
harvard university press cambridge ma 
chater finch 
distributional information powerful cue acquiring syntactic categories 
cognitive science 
landauer dumais 
solution plato problem latent semantic analysis theory acquisition induction representation knowledge 
psychological review 
manning sch tze 
foundations statistical natural language processing 
mit press cambridge ma 
blei ng jordan :10.1.1.110.4050
latent dirichlet allocation 
journal machine learning research 
jurafsky 
better integration semantic predictors statistical language modeling 
proceedings icslp volume pages 
griffiths steyvers 
finding scientific topics 
proceedings national academy science 
gilks richardson spiegelhalter editors 
markov chain monte carlo practice 
chapman hall 
kucera francis 
computational analysis day american english 
brown university press providence ri 
kass 
bayes factors 
journal american statistical association 
hubert arabie 
comparing partitions 
journal classification 
