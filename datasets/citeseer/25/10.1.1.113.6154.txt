arxiv cmp lg jun empirical comparison probability models dependency grammar jason eisner cis department university pennsylvania rd st philadelphia pa usa linc cis upenn edu technical report appendix eisner gives superior experimental results reported talk version details results obtained 
eisner trained probability models small set conjunction free parses derived wall street journal section penn treebank evaluated models held test set novel parsing algorithm 
describes details experiments repeats larger training set sentences 
reported talk extensive training yields greatly improved performance cutting half error rate eisner 
nearly half sentences parsed thirds sentences parsed 
models described original best score obtained generative model attaches words correct parent 
better models explored particular simple variants comprehension model better attachment accuracy model tags words accurately comparable trigram tagger 
tags roughly known advance search error eliminated new model attains attachment accuracy 
find parser collins combined tagger achieves trained tested sentences 
briefly discuss similarities differences collins model pointing strengths noting strengths combined dependency parsing phrase structure parsing 
eisner proposed compared probabilistic models dependency grammar parsing algorithm 
models relative performance interest reflect different independence assumptions syntactic structure 
eisner included empirical comparison models results complete press time 
unfortunately written version included results pilot study small training set 
results larger experiment coling 
purpose technical report describe significantly improved results additions 
organization follows 
conceptual overview reader encouraged read eisner 
detail experimental setup specifies precise probability models experiments explains probabilities estimated describes training test data prepared 
gives experimental results discusses 
offers concluding remarks 
material supported national science foundation graduate fellowship benefited greatly discussions mike collins dan melamed mitch marcus adwait ratnaparkhi 
special due mike collins kindly testing parser dataset 
man corner taught play golf eos dt nn dt nn vbd prp nn vb nn man play corner taught eos bare bones dependency structure described text 
constituent structure subcategorization may highlighted displaying dependencies lexical tree 
precise formulation models dependency structures brief review terminology 
bare bones dependency structure sequence string words 
word annotated tag indicates word syntactic semantic role parent indicates word plays role 
parent word usually pointer link word string said modify 
word string head sentence modifies parent said special symbol eos sentence mark falls just past string word 
addition dependency structure formed parents assigned way links cross form cycles 
word said head contiguous substring formed descendants 
gives example 
tags simple part speech tags kucera francis marcus 
possible articulated tag set order achieve parses precise accurate 
example tag extended includes part speech nn noun indications token heads definite np serves semantic agent 
aside questions smoothing sparse data large tag set changes model 
tag set experiments merely slightly refined version part speech tags kucera francis marcus 
refinements described 
probability models structural choices compares various probability models 
model describes probability distribution space dependency structures word strings 
sentence words wn parser assigns respective tags tn parents 
pn maximize probability pr resulting dependency structure words tags 
parser returns highest probability structure consistent word string model probability dependency structure described notional process generates unique sequence structural choices 
probability occurrence probability making appropriate sequence choices 
choice randomly probability conditioned certain aspects previously choices 
certain models achieve correspondence choice sequences legal dependency structures 
general assigning probability choice sequence model assigns total probability class legal structures consistent sequence 
golf classes size exist model called leaky deficient allocates probability situations legally arise 
classes size exist model called incomplete 
find probabilities individual structures classes 
parser returns highest probability structure arbitrary representative highest probability class 
purposes implementation parse scored probability choice sequence structures class suffer tie broken arbitrarily 
third possibility model may inconsistent classes overlap structure generated way 
consistent models considered stated generated unique sequence structural choices 
note may eliminate model renormalizing probabilities defining total probability allocated formed structures putting pr formed pr pr formed normalized conveniently constant maximizing maximizing pr just formed structures classes similarities models general form models motivated eisner reader referred discussion 
gives overview 
defining precise versions models took care comparison fair having models condition decisions comparable information 
particular models sensitive types probabilistic interactions 
words conditioned parents adjacent siblings dependency tree 
child attached model predicted model generated model probability varies word tag pair word tag pair parent 
probability varies tag closest child parent model sensitive markov dependencies successive children word 

words conditioned adjacent words string 
model attend string local relations word tag pair generated probability conditional word tag pairs immediately precede string 

generating sequence word tag pairs distinguished start state generate distinguished symbol 
generate sequences items sequence children word essentially regarded output markov process sequence words sentence 

backoff smoothing sparse data see performed similarly models 
example model probability child depends identity neighboring sibling consider shortened version sibling tag 
description individual models pseudocode run merely illustrates model dependency structure probability pr generated canonical series probabilistic choices 
straightforward reconstruct pseudocode pr defined structure value parser maximizes dynamic programming described eisner 
play receive left children 
model play considers predecessors nearest farthest taught corner 
decides 
sequence decisions probability sequence coin flips flips independent decision may influenced word selected principle attaches play removes need subject verb agreement increasing chance singular noun attach 
attaches fills subject position play reduces chance noun man attach left 
total probability possible dependency structure probability generating words tags structure markov process times probability achieving exactly right sequence coin flips obtain observed links 
model play decides random wants closest left child 
decides wants closest child children 
coincidence happens just generated markov process 
fill play requirements 
total probability possible dependency structure probability generating words tags structure markov process times probability word priori wants children parent assigned structure 
model play decides random wants left children 
choice markov process exactly model result play decision desired children generated separate markov process generate words tags 
total probability possible dependency structure probability word priori want children structure 
model just model probability play chooses conditioned fact available serve subject previously generated appropriate position markov process 
see difference imagine society rare playful 
model play reject subject grounds man common subject play 
model play quite eager accept subject sentence play position serve subject 
total probability possible dependency structure probability generating words tags structure markov process times probability word select children structure available selected 
probability model compute model multiplied probability markov process coin flips ignoring coin flips 
narrative ignores role may spirit hoping instance play serve parent 
parent preferences 
understanding models example 
notation twi denotes pair wi ti called tagged word parents pi represented indices pi means ith word modifies jth word 
indices closest nd closest 
right children wi denoted kid kid kid similarly indices left children kid kid 
kid denote abuse notation tw kid taken represent twi distinguished value indicates left right child sequence 
model bigram lexical affinities model generates tagged sentence simple trigram markov model follows 
pr 
tw tw bos bos sentence 


choose randomly possible tagged words conditioned 
pr pr pr 
eos eos break sentence don change point model generated sequence tagged words probability pr 
second phase model choose parents conditional tagged words get full dependency structure probability pr pr pr pr 
pair words including eos mark second phase model chooses add link 
order decisions important links chosen independently 
wi decides child wk condition decision closest child wk 
lets model capture certain facts subcategorization 


choose left children wk choice sees wk tag closest child 
number left children far 
downto 
choose take kid conditioned twi twk 
pr pr pr choice twi twk 
chose 
kid 
likewise choose right children wk 
number right children far 

model leaky links chosen independently possible generate illegal dependency structures feature words parents words multiple parents link cycles crossing links 
model selectional preferences phase model generates tagged sentence exactly model word tag chosen local context 
second phase corresponds pseudocode 
word randomly independently chooses highly specific subcategorization frame frames tries link words satisfy frames 
frame word describes parent children word expects corresponds lexicalized version disjunct link grammar sleator temperley supertag probabilistic tree adjoining grammar srinivas joshi 
tagged word tw chooses subcategorization part frame generating markov sequence desired left children markov sequence desired right children tagged words tw expects match heads complements adjuncts 
gets part frame choosing tagged word expects match parent 
generation probabilities conditioned tw 


choose sequence tagged words twk require left children match 
choice sees twk tag closest child 
choose tagged word tw kid match choice tagged words vocabulary conditioned twk tw 
pr pr pr tw twk tw 
tw break left child sequence 
similarly choose sequence tagged words wk require right children match 


choose tagged word tw wk require parent match conditioned wk 
pr pr pr tw wk 
possible choose parents 
pn way parent children word wk satisfy frame wk generated 
model leaky step may fail right words satisfy frames linked crossing links 
case probability mass assigned impossible structure 
note particular tagged word generated times generation tagged sentence words links parent child 
structure legal tagged word generated occasions 
furthermore model incomplete may way carry step 
model recursive generation model idea word generates actual children just way model word generates desired children subcategorization frame 
tagged word tw generate left children tw tw 
markov sequence tagged words conditioned tw right children likewise 
process repeated recursively words yielding tree 
process consists calling generate pair eos eos generate tw defined 

choose tw sequence left children tw tw choice sees tw tag closest child 

choose tw conditional tw tw 
pr pr pr tw tw tw 
tw break left child sequence 
generate tw 
similarly choose sequence tagged words wk require right children match 
example parsing arithmetic expressions word string minimal example admit dependency correspond reflect exactly frame word token wi proved word frame specifies number left right children direction left right parent dependency structure possible step model longer incomplete 
unfortunately shows augmenting frames parental direction hurts model performance substantially 
notation convenience describing actual indices children straightforward 
top generative model leaky incomplete 
addition models originally described eisner report results new bottom model similar model model model collins model realistic selectional preferences simplest regard model variant model word model generates subcategorization frame line ignorance words available fill frame words generated 
better model generate string words model word select sequence real children tokens remaining words cf 
model generating sequence desired children types 
ideally phase model look follows 

select left child sequence word existing words 

choose kid set 
kid choices words left kid plus distinguished symbol 
pr pr pr kid twk 
kid break left child sequence 
similarly select right child sequence word 

parent frame chosen model generate structures crossing links leaky model difficulty description lies estimating pr kid twk tw kid line 
probability choosing particular child special choice particular set available remaining children 
model collins faces similar problem collins addresses essentially backed probability pr kid twi twk 
tokens labeled twi twk sentence probability links 
model backs nuanced way pr kid twi twk tw kid 
tokens labeled twi twk sentence probability links st left child fall left cth left child labeled tw kid 
conditions disregarded collins 
note nuanced backoff model enables model capture probabilistic interactions successive children wk just models 
important interaction outermost child wk 
interaction serves capture facts arity fact wk depending existing child particular type may require forbid additional child 
purposes computation regard model variant model difference line model continue multiply pr probabilities links accepted right probabilities links rejected wrong 
probability model assigns structure probability model select links structure 
difficult give independent justification model lines note interesting variation generation depth order cth child word side word chosen word left sibling tagged words immediately string precede child subtree 
allow model take string local context account way models variation tested experimentally 
implemented parsing framework eisner leads somewhat larger span signatures meaning time space requirements suffer fairly large wholly unreasonable constant factor 
hand model consider untagged versions preceding words extra cost mike collins 
especially problem model assigns probability structure links probability added separately structures links 
probability estimation conditional probabilities required difficult estimate directly represent ratios counts rare events may occurred training data 
section describes estimate probabilities 
backoff strategy general approach decompose probability form product form pr pr pr pr factors product estimated separately conditional probability 
estimate conditional probability may back reduce condition detailed 
example estimate third factor choose reduce condition simply depending independence assumptions believe justified 
estimate pr pr pr respectively 
severe reductions throw away potentially relevant information conditions obtain sentence justified sparse data 
allow dynamic tradeoff sensitive conditions sufficient data conditional probability estimated associated list reductions increasingly severe 
reduction list keeps original condition reductions throw away information 
list reductions 
suppose wish estimate pr severe reduction reduces condition reduction return estimate count count approximation pr training data counts 
additional reductions recursively compute estimate remaining list reductions return estimate count count coarse backed estimate weight additional observations specific context specific context frequently observed largely override coarse estimate collins 
features functions backoff describing factors reductions assume functions tag tw tagged word extract tag 
word tw tagged word extract lowercase version word 
cap tw tagged word extract information capitalization 
cap tw take values lowercase caps letters init tw non punctuation word sentence just letter capitalized 
efficiency bother add numerator denominator compute event count large 
policy may unwise numerator may zero denominator large 
done tag function modified tw special word high frequency closed class word tag tw tw 
equivalent giving special words tags 
cap cases particular unambiguously capitalized mixed case words short tag function maps specific tags general tags example short jjr short jjs 
corpus tags shortened tags 
tiny tag aggressive version short tag groups tags just equivalence classes noun verb noun modifier adverb preposition wh word punctuation 
dist distance word positions represented ranges 
reductions backoff models turn probabilities generated 

models generate string tagged words trigram model 
crucial probability computed factors reductions pr twk twk twk pr cap twk word twk tag twk twk twk pr tag twk twk twk reduction list tag twk tag twk tag twk short tag twk pr word twk tag twk twk twk reduction list tag twk pr cap twk word twk tag twk twk twk reduction list word twk tag twk tag twk notice consider reduction factor specifies estimate pr tk tk tk pr wk tk standard trigram model church 

model estimate pr 
formally problem estimating pr twk twk handled special case pr twk twk twk computation 

models generate twk sequence children direction dir dir left right 
concreteness assume dir right children kid kid 
pr tw kid twk tw kid dir pr cap tw kid word tw kid tag tw kid twk tw kid dir pr tag tw kid twk tw kid dir reduction list twk short tag tw kid dir twk dir tag twk short tag tw kid dir short twk dir pr word tw kid tag tw kid twk tw kid dir reduction list tag tw kid twk dir tag tw kid tag twk dir tag tw kid see pr cap tw kid word tw kid tag tw kid twk tw kid dir reduction list word tw kid tag tw kid tag tw kid version model generate children head desired distance head objective find pr dist kid tw kid twk tw kid dir 
version multiply additional factor pr dist kid tw kid twk tw kid dir reduction list tw kid tag twk tag tw kid tag twk note model ordinarily leaky model adding factor leaky 
note reductions disjunctive condition disjuncts grouped bracket 
disjunction useful possible know part original condition thrown away reduction order overcome sparse data 
disjunctive reduction estimate desired factor pr tag tw kid twk tw kid dir follows count tag tw kid twk dir count tag tw kid tag twk short tag tw kid dir count twk dir count tag twk short tag tw kid dir compute numerator denominator disjunct separately disjunct entire reduction find estimate adding numerators adding denominators 
disjunct greater denominator condition common greater influence estimate collins brooks 

models able decide words positions children st child pr link twi twk tw kid reduction list word twi tag twi word twk tag twk short tag tag twi word twk tag twk short tag word twi tag twi tag twk short tag tw kid word twi tag twi word twk tag twk tag twi tag twk short tag tag twi tag twk tiny tag possible construct version models conditions distance case pr link dist twi twk tw kid reduction list unknown words dist word twi tag twi word twk tag twk short tag dist tag twi word twk tag twk short tag tw kid dist word twi tag twi tag twk short tag tw kid dist word twi tag twi word twk tag twk tag twi word twk tag twk word twi tag twi tag twk dist tag twi tag twk short tag tw kid dist tag twi tag twk tiny tag tw kid system deals unknown words uniform way technique attenuation 
parsing test sentence begins unknown word input attenuated replaced symbol indicative word morphological class 
word ends digit symbol morph num characters symbol morph xx xx uppercase versions word characters word fairly symbol morph short 
capitalization properties original word see retained 
formally suppose input word string attenuated version string unknown words replaced morphological classes 
parser run chooses tags parents maximize pr 
pr constant input maximizing pr pr pr pr pr originally desired 
important system train attenuated symbols morph xx distribution symbols training correspond distribution unknown words words testing 
corpus see happens divided discourse coherent sections articles 
training replace word morphological symbol entire training section appears 
sentences train include attenuated words morph short 
system able learn example tokens unknown short lowercase words short lowercase words appearing article time tend common nouns 
contrast arbitrary tokens short lowercase words prepositions 
description corpus corpus dependency structures derived wall street journal sentences appear penn treebank ii marcus 
simplicity omitted sentences contained conjunction 
allowed postpone questions best handle conjunction constructing dependency representations modifying probability models 
omitted number sentences noticed clear annotator errors 
corpus contained remaining sentences lengths ranged words including punctuation mean median 
corpus structured articles sections sentences mean median 
phrase structure tree penn treebank converted bare bones dependency structure process steps 
instances treebank style flat structure group maximal sequence nnp proper noun siblings npr proper noun phrase constituent 
group maximal sequence cd cardinal number siblings qp quantifier phrase 
group qp constituent 
npr group maximal sequence nn common noun siblings np noun phrase 

automatically correct common types annotator errors discard sentence correction effected automatically 

constituent bottom tree upward heuristics exception tables determine overt non trace contributes head define head head 
overt link head head head parent 
strategy computationally cheaper ideal solution mine sentence statistics individual words morphological classes 
avoid sparse data problems training word replaced happens appear test data 
particular word occurs training data careful train full lexical item merely attenuation full item needed parsing test data 
policy constitute test data fitting model case learning training data time needs model test example 
looks input data answer 

modify certain tags resulting structure informative mark auxiliary verbs adding feature tags 
premodifiers nouns lose ability take complements specifiers mark 
participial postmodifiers nouns lose ability take subjects mark 
distinguish prepositions 
treebank share tag 

modify dependency structure better reflects semantic relations sequence auxiliaries possibly interrupted adverbs point main verb head verb phrase 
experiments discussion evaluation method divided corpus randomly test data sentences training data 
deny advantage training half article testing half chose test data repeatedly choosing sentence random marking entire section test data marked test sentences 
scored models tagged parsed test data 
evaluate tagging percentage words correct tag 
recall somewhat fine grained tag set item penn treebank marcus task correspondingly harder 
evaluate parsing simply percentage words attached correctly correctly selected parents 
single attachment score easier understand precision recall crossing brackets triple parseval black 
lin independently argues attachment score penalizes errors appropriate way parseval metrics single semantically difficult damage number constituents 
models evaluated current parser written lisp model impractical run training set large 
recall model high memory requirements able remember pairs words appeared sentence high time requirements compute probability known word parse takes time 
terminated experiment early test results early sentences appeared far inferior models 
ran versions models 
results shown figures 
baseline model eisner 
word tagged common tag ignoring case 
unknown words treated usual way assigned common tag new words sharing capitalization letters 
tagged word selects parent word tagged choose parent common distance words tagged example determiner takes word parent 
resulting structure may ill formed scored words correct tag correct parent 
trigram tagger model works identically phase models tagger add links run compare tagging accuracy parsers 
versions model vary attitudes frames 
version word generates desired parent 
am grateful joshua goodman directing attention 
broken class correct tag non punc adv prep wh unknown word count tokens baseline model trigram tagger model parent dir model model lex model model distance model 
model model distance true tags model true tags collins auto tags results models increasing order performance words correct tag parent 
small type shows percentage words tags correctly identified 
large type shows percentage words parents correctly identified 
boldfaced column shows scores words punctuation 
remaining columns consider models performance particular kinds words prepositions unknown words seen training data 
version proposed footnote remedies incompleteness model having word frame specify direction word parent 
version frame generated line pseudocode model higher chance success 
versions model attempted pure version word generates sequences left right children 
non lexical version severe reduction pr word tw kid tag tw kid twk tw kid dir estimated pr word tw kid tag tw kid 
statistical relation child parent mediated tags words ignorant words 
version corresponds straw man model eisner 
leaky version generates desired distance child head described 
improves performance somewhat 
model described earlier 
results evaluation basis preliminary experiments eisner expected model win 
model outperform models apparently emerged model turn beaten variations model model third variant model model highest performing model 
non lexical model performs surprisingly percentage points lexical version 
versions different errors 
non lexical version tends favor right branching introduces new factor pr parent dir child parent probability computation estimate reductions tag child tag parent short tag child tiny tag parent class true parent correct tag non punc adv prep wh unknown model parent dir model model lex model model distance model 
model model distance true tags model true tags collins auto tags essentially breakdown middle columns different 
columns shows various parts speech manage recall children 
example percentage words children verbs correctly attached verbs 
attachment errors contagion search err err ratio error baseline model trigram tagger model parent dir model model lex model model distance model 
model model dist true tags model true tags collins auto tags collins percentage sentences attachment errors 
better models thirds sentences usually 
punctuation counted 
rains error sentence probability second error increased 
columns show pr err pr errs err percentages ratio 
final column shows percentage sentences experiment victims search error 
sentences model preferred correct structure structure parser parser consider overly aggressive pruning parse chart typically parser wrong initial guesses tags consider 
providing tags parser lines essentially eliminates search error providing clue performance improves tags provided 
structure strongly lexical version easily led astray right branching structure 
better smoothing low counts help problem 
variants model third far successful 
preferences appear unreliable suspect manually constructed competence grammars traditionally focus subcategorization 
example happens nouns modify words right including nouns words left 
linguistically fact nouns frequency words wish modified nouns side 
unwise think nouns insist attaching rightward expense subcategorization 
third version model interesting property 
pr dependency structure happens exactly tagged word sequence model trigram markov model 
parser maximizes product generative probability considers tree local information markov probability considers string local information 
compared model uses generative probability hybrid better tagging slightly better parsing 
compared model uses markov probability hybrid slightly better tagging course better parsing 
noteworthy result tagging improved adding parser vice versa 
best models model model tagging performance beat pure trigram tagger model 
worse models parsing hurts tagging overriding decisions tagger 
converse true just noted model effectively beats model adding local tagger 
comparison parser experiment compared successful versions different models state art parser collins 
results shown lines likewise figures 
collins parser performs similarly best model model fine grained differences example model advantage unknown words nominal modifiers collins parser better attaching known prepositions wh words 
purposes experiment collins parser trained tested penn treebank sentences system 
converted output parser dependency form automatic tools convert treebank sentences 
enabled score output metrics 
issue making comparison collins parser runs separate tagger black box begins parse tagger highly trained 
comparison fairer ran models mode informed correct treebank tags 
completely determine highly articulated tags system uses see constrain choice tags sufficiently reduce tagging error tag set just 
tagger collins uses ratnaparkhi error treebank tag set putting collins slight disadvantage mitigated somewhat fact collins trains parser slightly erroneous output tagger correct tags 
principal benefit feeding tags model way virtually eliminated quite serious problem search error boosting performance substantially 
discussion comparison somewhat surprising accuracy roughly matches collins original plan eisner value simplicity high performance 
chose dependency grammar simple homogeneous probability models wished answer key design questions probabilistic parsing 
parsers number mechanisms different probability models mildly different interesting way see 
parsers rely heavily associations pairs lexical items possible discern points correspondence 
collins parser parts uses different sort probability model tagging chunking base nps general parsing 
models homogeneous treat base nps specially model treat tagging specially 
base nps collins parser carry roughly load sibling interactions parser 
collins parser base nps help avoid certain errors words john smith may water heater grouped object words attach verb 
double subject errors arise parser trick generating children terminated markov sequence helps capture arity 
verbs learn nominal left children row 
parser attention interdependencies siblings general advantage base np method capture interdependencies difference transitive verbs 

collins parser produces tree labeled nonterminal symbols 
nonterminals probability model require verb subject resulting constituent serve sentential complement 
example thought takes sentential vp complement thought john left mary verb left low probability linking thought subject 
principle accomplished dependency model adding features tags vary constituent structure projected head 
experimented 
necessary allow tags verb left high probability getting subject 
tag link thought 
possibly tree informative output bare bones dependency structure may easier recover semantic relationships additional internal structure 
eisner notes methods easily adapted handle labeled dependencies bare bones dependencies 
links annotated semantic roles symbols 
phrase structure trees encoded labeled dependencies sort collins dependency methods described eisner powerful produce trees 

collins parser sensitive intervening punctuation parent child local configurations adjacency intervening verbs 
solution treating punctuation marks words recognizes single comma may discourage links levels crossing comma 
small differences kind formalizing linguistic intuitions course substantial effects 
effects topic current amply demonstrated 
apparently possible intuitions intended 
best systems considered including collins model remaining errors matters semantic nuance precisely nuances semantic subcategorization difficult pick syntax lexical associations seen training data 
attack errors large sets specialized hacks inordinate amounts annotated training data 
may help treating words atomically 
may wish explore methods generalize affinities individual words classify words phrases groups terms tend create fill semantic roles 
approach bootstrap existing model obtain approximate parses large quantities naturally occurring raw text training 
significance testing particularly test sample consisted sentences words counting punctuation wished test results statistical significance 
necessary careful attachment errors sentence independent 
shows independent 
parsing errors suffer rains phenomenon number errors sentence contagious distribution apparently large difference result just badly parsed sentences attachment errors 
danger finding significance 
reason employed non parametric monte carlo test 
pair models error rates differed considered pairs parses produced models test sentences 
asked pair parses randomly colored parse red parse blue difference red error rate blue error rate reach 
difference due badly parsed sentence answer half time significance level 
pair models computed significance level attachment performance differed making random coloring passes sentences checking random red blue difference strong observed difference models 
column nearly attachment performances significantly different level usually 
models reliably distinguished certain pairs successive lines 
wit adding distance model lines resulted significant improvement level lines significantly different lines 
technique test significance differences tagging performance runs models tags 
significance level comparisons see lines significantly different models tagging performance trigram tagger model model line tag significantly better versions model lines levels respectively versions model distance significantly differ best models lines significantly differ tagging performance parsing performance 
hope helpful ways 
foremost comparative results shed light design probability model parsing 
particular models considered lexical affinities important vs helps parsing string local tree local information vs helps tagging tree local string local information vs helps parsing distance information distance harmful assume words generate preferences vs best condition decisions information available vs 
absolute results quite promising type parsing state art accuracy 
may accurate parser useful part larger system 
striking results obtained simple models 
example special treatment np chunks verbs punctuation 
interesting performance essentially identical trigram tagger trigrams consist parent adjacent children consecutive words 
particular independence assumptions allow words influence tags 
third tried provide details replicate experimental improve design having reinvent 
modest entry relatively new area experimentally comparing statistical nlp methods caraballo charniak chen goodman mooney 
experimental practices described knowledge previously applied comparison parsers evaluation metric dependency attachments proposed lin non parametric methods evaluate statistical significance 
black black 
procedure quantitatively comparing syntactic coverage english grammars 
proceedings february darpa speech natural language workshop 
caraballo charniak sharon caraballo eugene charniak 

new figures merit probabilistic chart parsing 
ms brown university submitted computational linguistics 
chen goodman stanley chen joshua goodman 

empirical study smoothing techniques language modeling 
proceedings th annual meeting association computational linguistics santa cruz july 
cmp lg 
church kenneth church 

stochastic parts program noun phrase parser unrestricted text 
proceedings nd conf 
applied natural language processing austin tx 
association computational linguistics morristown nj 
collins michael collins 

new statistical parser bigram lexical dependencies 
proceedings th acl santa cruz july 
cmp lg 
collins brooks collins brooks 

prepositional phrase attachment model 
proceedings third workshop large corpora 
cmp lg 
eisner jason eisner 

new probabilistic models dependency parsing exploration 
proceedings th international conference computational linguistics coling copenhagen august 
cmp lg 
kucera francis kucera francis 

computational analysis day american english 
providence ri brown university press 
lin dekang lin 

dependency method evaluating broad coverage parsers 
proceedings international joint conference artificial intelligence 
marcus marcus santorini 

building large annotated corpus english penn treebank 
computational linguistics 
marcus marcus kim marcinkiewicz macintyre ferguson katz 

penn treebank annotating predicate argument structure 
proceedings human language technology workshop 
mooney mooney 

comparative experiments disambiguating word senses illustration role bias machine learning 
proceedings conference empirical methods natural language processing university pennsylvania may cmp lg 
ratnaparkhi adwait ratnaparkhi 

maximum entropy model part speech tagging 
conference empirical methods natural language processing 
sleator temperley daniel sleator davy temperley 

parsing english link grammar 
technical report cmu cs 
cs dept carnegie mellon univ cmp lg 
srinivas joshi srinivas aravind joshi 

disambiguation super parts speech supertags parsing 
proceedings coling kyoto japan august 
cmp lg 

