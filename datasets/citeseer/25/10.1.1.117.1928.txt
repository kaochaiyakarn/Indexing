cognitive science 
finding structure time jeffrey elman university california san diego time underlies interesting human behaviors 
question represent time connectionist models important 
approach represent time implicitly effects processing explicitly spatial representation 
current report develops proposal lines described jordan involves recurrent links order provide networks dynamic memory 
approach hidden unit patterns fed back internal representations develop reflect task demands context prior internal states 
set simulations reported range relatively simple problems temporal version xor discovering syntactic semantic features words 
networks able learn interesting internal representations incorporate task demands memory demands approach notion memory inextricably bound task processing 
representations reveal rich structure allows highly context dependent expressing generalizations classes items 
representations suggest method representing lexical categories type token distinction 
jay mcclelland mike jordan mary hare dave rumelhart mike mozer steve david zipser mark stimulating discussions 
mcclelland jordan anonymous reviewers helpful critical comments earlier draft 
supported contract office naval research contract army avionics ft 
requests reprints sent center research language university california san diego ca 
author reached electronic mail elman crl ucsd edu 
page time clearly important cognition 
inextricably bound behaviors language express temporal sequences 
difficult know deal basic problems goal directed behavior planning causation way representing time 
question represent time arise special problem unique parallel processing models parallel nature computation appears odds serial nature temporal events 
traditional serial frameworks representation serial order interaction serial input output higher levels representation presents challenges 
example models motor activity important issue action plan literal specification output sequence plan represents serial order manner fowler kelso kelso jordan rosenbaum 
linguistic theoreticians tended concerned representation processing temporal aspects utterances assuming instance information utterance available simultaneously syntactic tree research natural language parsing suggests problem trivially solved frazier fodor marcus 
elementary facts human activity temporal extent ignored problematic 
parallel distributed processing models processing sequential inputs accomplished ways 
common solution attempt parallels time giving spatial representation 
problems approach ultimately solution 
better approach represent time implicitly explicitly 
represent time effect processing additional dimension input 
describes results pursuing approach particular emphasis problems relevant natural language processing 
approach taken simple results complex unexpected 
solution problem time may interact problems connectionist architectures including problem symbolic representation connectionist representations encode structure 
current approach supports notion outlined van gelder see smolensky elman connectionist representations may functional compositionality syntactically compositional 
section briefly describes problems arise time represented externally spatial dimension 
second section describes approach 
major portion report presents results applying new architecture diverse set problems 
problems range complexity temporal version exclusive function discovery syntactic semantic categories natural language data 
problem time obvious way dealing patterns temporal extent represent time explicitly associating serial order pattern dimensionality pattern page vector 
temporal event represented element pattern vector second temporal event represented second position pattern vector 
entire pattern vector processed parallel model 
approach variety models cottrell munro zipser elman zipser hanson 
drawbacks approach basically uses spatial metaphor time 
requires interface world buffers input 
clear biological systems shift registers 
logical problems system know buffer contents examined 
second shift register imposes rigid limit duration patterns input layer provide longest possible pattern suggests input vectors length 
problems particularly troublesome domains language comparable representations patterns variable length 
true basic units speech phonetic segments sentences 
seriously approach easily distinguish relative temporal position absolute temporal position 
example consider vectors 
vectors appear instances basic pattern displaced space time give temporal interpretation 
geometric interpretation vectors clear patterns fact quite dissimilar spatially distant 
pdp models course trained treat patterns similar 
similarity consequence external teacher similarity structure patterns desired similarity generalize novel patterns 
shortcoming serious interested patterns relative temporal structure preserved face absolute temporal displacements 
representation time richer problems 
follows simple architecture described number desirable temporal properties yielded interesting results 
networks memory spatial representation time described treats time explicit part input 
different possibility allow time represented effect processing 
means giving processing system dynamic properties responsive temporal sequences 
short network memory 

reader may easily convinced comparing locations vectors space 
patterns considered temporally displaced versions basic pattern vectors different 
page hidden output input plan 
architecture jordan 
connections output state units fixed weight 
connections shown 
approach modified way 
suppose network shown augmented input level additional units call context units 
units hidden sense interact exclusively nodes internal network outside world 
imagine sequential input processed clock regulates presentation input network 
processing consist sequence events 
time input units ways accomplished number interesting proposals appeared literature jordan tank hopfield stornetta hogg huberman watrous shastri waibel hinton shikano lang pineda williams zipser 
promising suggested jordan 
jordan described network shown containing recurrent connections associate static pattern plan serially ordered output pattern sequence actions 
recurrent connections allow network hidden units see previous output subsequent behavior shaped previous responses 
recurrent connections give network memory 
receive input sequence 
input single scalar value vector depending nature problem 
context units initially set 
input units context units activate hidden units hidden units feed forward 
activation function bounds values 
output units hidden units input units context units 
simple recurrent network activations copied hidden layer context layer basis fixed weight 
dotted lines represent trainable connections 
page activate output units 
hidden units feed back activate context units 
constitutes forward activation 
depending task may may learning phase time cycle 
output compared teacher input backpropagation error rumelhart hinton williams incrementally adjust connection strengths 
recurrent connections fixed subject adjustment 
time step sequence repeated 
time context units contain values exactly hidden unit values time context units provide network memory 
internal representation time 
feedforward networks employing hidden units learning algorithm hidden units develop internal representations input patterns recode patterns way enables network produce correct output input 
architecture context units remember previous internal state 
hidden units task mapping external input previous internal state desired output 
patterns hidden units saved context hidden units accomplish mapping time develop representations useful encodings temporal properties sequential input 
internal representations develop sensitive temporal context effect time implicit internal states 
note representations temporal context need literal 
represent memory highly task 
consider results applying architecture number problems involve processing inputs naturally sequence 
exclusive exclusive xor function interest learned simple layer network 
requires layers 
xor usually problem involving bit input vectors yielding bit output vectors respectively 
problem translated temporal domain ways 
version involves constructing sequence bit inputs presenting bit inputs bit time time steps followed bit output continuing input output pair chosen random 
sample input 
second bits xor ed produce third fourth fifth xor ed give sixth 
inputs concatenated unbroken sequence 
current version xor problem input consisted sequence bits constructed manner 
input stream network shown input unit hidden units output unit context units bit time 
task network point time predict bit sequence 

little detail order connections context units hidden units 
network connections hidden unit context unit 
implies equal number context hidden units 
upward connections context units hidden units fully distributed context unit activates hidden units 
page input sequence shown bit time correct output corresponding points time shown 
input 
output 
recall actual input hidden layer consists input shown copy hidden unit activations previous cycle 
prediction just input world network previous state continuously passed back cycle 
notice temporal structure sequence possible correctly predict item 
network received bit example chance bit 
network receives second bit possible predict third xor 
fourth bit fifth predictable 
fifth bit sixth predicted 
fact passes bit sequence constructed way network ability predict sequential input closely follows schedule 
seen looking sum squared error output prediction successive points input 
error signal provides useful guide network recognized temporal sequence moments outputs exhibit low error 
contains plot sum error cycle mod 
graph root mean squared error consecutive inputs sequential xor task 
data points averaged trials 
squared error time steps averaged cycles 
error drops points sequence correct prediction possible points error high 
indication network learned temporal structure input able previous context current input predictions input 
network fact attempts xor rule points time fact obscured page averaging error done 
looks output activations apparent nature errors network predicts successive inputs xor previous 
guaranteed successful third bit result correct predictions times 
interesting solution temporal version xor somewhat different static version problem 
network hidden units unit highly activated input sequence series identical elements unit highly activated input elements alternate 
way viewing network developed units sensitive high low frequency inputs 
different solution feedforward networks simultaneously inputs 
suggests problems may change nature cast temporal form 
clear solution easier difficult form important lesson realize solution may different 
simulation prediction task way somewhat analogous autoassociation 
autoassociation useful technique discovering intrinsic structure possessed set patterns 
occurs network transform patterns compact representations generally exploiting redundancies patterns 
finding redundancies interest tell similarity structure data set cf 
cottrell munro zipser elman zipser 
simulation goal find temporal structure xor sequence 
simple autoassociation task simply reproducing input points time trivially solvable require sensitivity sequential patterns 
prediction task useful solution requires network sensitive temporal structure 
structure letter sequences question asked memory capacity network architecture employed sufficient detect complex sequential patterns xor 
xor pattern simple respects 
involves single bit inputs requires memory extends bit back time different input patterns 
challenging inputs require multi bit inputs greater temporal extent larger inventory possible sequences 
variability duration pattern complicate problem 
input sequence devised intended provide just sorts complications 
sequence composed different bit binary vectors 
vectors derived real speech think representing speech sounds dimensions vector corresponding articulatory features 
table shows vector letters 
sequence formed steps 
consonants combined random order obtain letter sequence 
consonant replaced rules ba dii page table vector definitions alphabet consonant vowel interrupted high back voiced initial sequence form gave rise final sequence 
letter represented bit vectors 
sequence semi random consonants occurred randomly consonant identity number vowels regular 
basic network xor simulation expanded provide bit input vectors input units hidden units output units context units 
training regimen involved presenting bit input vector time sequence 
task network predict input 
sequence wrapped pattern 
network trained passes sequence 
tested sequence obeyed regularities created different initial randomization 
error signal part testing phase shown 

graph root mean squared error letter prediction task 
labels indicate correct output prediction point time 
error computed entire output vector 
page target outputs shown parenthesis graph plots corresponding error prediction 
obvious error oscillates markedly points time prediction correct error low points time ability correctly predict evidently quite poor 
precisely error tends high predicting consonants low predicting vowels 
nature sequence behavior sensible 
consonants ordered randomly vowels 
network gotten consonant input predict identity vowel 
knows tokens vowel expect 
vowel sequence way predict consonant 
points time error high 
global error pattern tell story 
remember input patterns patterns network trying predict bit vectors 
error shown sum squared error bits 
examine error bit bit basis graph error bits time steps shown 
striking difference error patterns 
error predicting bit consistently lower error fourth bit points time 

bit corresponds feature consonant fourth bit corresponds feature high 
happens consonants value feature consonant differ high 
network learned vowels follow consonants error vowels low 
learned vowels follow consonant 
interesting corollary network knows soon expect consonant 
network know consonant correctly predict consonant follows 
bit patterns consonant show low error bit patterns high show high error 
behavior requires context units simple feedforward network learn transitional probabilities input learn patterns span inputs 
simulation demonstrates interesting point 
input sequence ways complex xor input 
serial patterns longer duration variable length prediction depends variable mount temporal context input 
graph root mean squared error letter prediction task 
error computed bit representing feature 
graph root mean squared error letter prediction task 
error computed bit representing feature high 
page consists bit bit vector 
reasonably thought extended sequential dependencies patterns exceed temporal processing capacity network 
opposite true 
fact level individual bit patterns enables network partial predictions cases complete prediction possible 
dependent fact input structured course 
lesson extended sequential dependencies may necessarily difficult learn 
dependencies structured structure may learning easier harder 
discovering notion word taken granted learning language involves things learning sounds language morphemes words 
theories acquisition depend crucially primitive types word morpheme categories noun verb phrase berwick weinberg pinker 
rarely asked language learner knows entities exist 
notions assumed innate 
fact considerable debate linguists representations language 
commonplace speak basic units phoneme morpheme word constructs clear uncontroversial definition 
commitment distinct levels representation leaves troubling residue entities appear lie levels 
instance languages sound meaning correspondences lie phoneme morpheme sound symbolism 
concept word straightforward think cf 
greenberg lehman 
english instance consistently definable distinction words apple compounds apple pie phrases library congress man street 
furthermore languages differ dramatically treat words 
languages eskimo called words nearly resemble english speaker call phrases entire sentences 
fundamental concepts linguistic analysis fluidity suggests important role learning exact form concepts remains open important question 
pdp networks representational form representational content learned simultaneously 
representations result flexible graded characteristics noted 
ask notion word maps concept emerge consequence learning sequential structure letter sequences form words sentences word boundaries marked 
imagine version previous task letter sequences form real words words form sentences 
input consist individual letters imagine analogous speech sounds recognizing orthographic input vastly simpler acoustic input 
letters sequence time breaks letters word breaks words different sentences 
sequence created sentence generating program lexicon words 
program generated sentences varying length words 
page sentences concatenated forming stream words 
words broken letter parts yielding letters 
letter word converted bit random vector 
result stream separate bit vectors letter 
vectors input time 
task point time predict letter 
fragment input desired output shown table 
table input output 
program simplified version program described greater detail simulation 
page network input units hidden units output units context units trained complete presentations sequence 
error relatively high point sequence sufficiently random difficult obtain low error memorizing entire sequence required far presentations 
graph error time reveals interesting pattern 
portion error plotted data point marked letter predicted error point time 
notice onset new word error high 
word received error declines sequence increasingly predictable 
error provides clue recurring sequences input correlate highly words 
information categorical 
error reflects statistics occurrence graded 
possible determine sequences constitute words sequences bounded high error criteria boundaries relative 
leads ambiguities case see lead misidentification common sequences incorporate word occur frequently treated quasi unit 
sort behavior observed children early stages language acquisition may treat idioms formulaic phrases fixed lexical items macwhinney 
simulation taken model word acquisition 
listeners clearly able predictions partial input marslen wilson tyler prediction major goal language learner 
furthermore occurrence sounds part identifies word 
environment sounds uttered linguistic context equally critical establishing coherence sound sequence associating meaning 
simulation focuses limited part information available language learner 
simulation simple point information signal serve 
graph root mean squared error letter word prediction task 
page cue boundaries linguistic units learned demonstrates ability simple recurrent networks extract information 
discovering lexical classes word order consider problem arises context word sequences 
order words sentences reflects number constraints 
languages english called fixed word order languages order tightly constrained 
languages free word order languages greater optionality word order order free sense random 
syntactic structure selectional restrictions subcategorization discourse considerations factors join fix order words occur 
sequential order words sentences simple determined single cause 
addition argued generalizations word order accounted solely terms linear order chomsky chomsky 
structure underlies surface strings structure provides insightful basis understanding constraints word order 
undoubtedly true surface order words provide insightful basis generalizations word order true point view listener surface order thing visible audible 
underlying structure cued surface forms structure implicit 
previous simulation saw network able learn temporal structure letter sequences 
order letters simulation small set relatively simple rules 
rules determining word order english hand complex numerous 
traditional accounts word order generally invoke symbolic processing systems express structural relationships 
easily believe qualitative difference nature computation required simulation required predict word order english sentences 
knowledge word order require symbolic representations capacity apparently pdp systems 
furthermore true pointed surface strings may cues structure considerable innate knowledge may required order reconstruct structure surface strings 
interesting question ask network learn aspects underlying structure 
simple sentences 
step somewhat modest experiment undertaken 
sentence generator program construct set short word utterances 
classes nouns verbs chosen listed table 
examples category noticed instances categories verb destroy may included verb tran 
different lexical items 

worst case word constitutes rule 
hopefully networks learn recurring orthographic regularities provide additional general constraints cf 
sejnowski rosenberg 
page table categories lexical items sentence simulation category examples noun hum man woman noun anim cat mouse noun book rock noun dragon monster noun frag glass plate noun food cookie sandwich verb think sleep verb tran see chase verb move break verb percept smell see verb destroy break smash verb ea eat generator program categories sentence templates table table templates sentence generator word words word noun hum verb eat noun food noun hum verb percept noun noun hum verb destroy noun frag noun hum verb noun hum verb tran noun hum noun hum verb noun noun hum verb noun anim verb eat noun food noun anim verb tran noun anim noun anim verb noun noun anim verb noun verb noun verb noun frag noun verb eat noun hum noun verb eat noun anim noun verb eat noun food create random word sentence frames 
sentence frame page filled randomly selecting possible words appropriate category 
word replaced randomly assigned bit vector word represented different bit 
word bit flipped 
extra bits reserved simulations 
encoding scheme guaranteed vector orthogonal vector reflected form class meaning words 
word vectors sentences concatenated input stream bit vectors created 
word vector distinct breaks successive sentences 
fragment input stream shown column table english gloss vector parentheses 
desired output column 
table fragment training sequences sentence simulation input output woman smash smash plate plate cat cat move move man man break break car car boy boy move move girl girl eat eat bread bread dog dog move move mouse mouse mouse mouse move move book book lion simulation network similar simulation input layer output layers contained nodes hidden context layers contained nodes 
task network learn predict order successive words 
training strategy follows 
sequence bit vectors formed input sequence 
word sequence input time order 
task input cycle predict bit vector corresponding word sequence 
word sequence process began break starting page word 
training continued manner network experienced complete passes sequence 
measuring performance network simulation straightforward 
rms error training dropped 
output vectors sparse simulation bits turned network quickly learns turn output units drops error initial random value 
light final error impressive 
recall prediction task non deterministic 
successors predicted absolute certainty 
built error inevitable 
prediction error free true word order random 
sequence words limited number possible successors 
circumstances network learn expected frequency occurrence possible successor words activate output nodes proportional expected frequencies 
suggests testing network performance rms error calculated actual successors output compared expected frequencies occurrence possible successors 
expected values determined empirically training corpus 
word sentence compared sentences point identical 
constitute comparison set 
probability occurrence possible successors determined set 
yields vector word training set 
vector dimensionality output vector representing distinct word turning single bit represents likelihood possible word occurring bit position fractional number equal probability 
testing purposes likelihood vector place actual teacher rms error computed comparison network output 
note appropriate likelihood vectors testing phase 
training done actual successors point force network learn probabilities 
performance evaluated manner rms error training set sd 
remaining minor problem error measure elements likelihood vectors sum represent probabilities activations network need sum 
conceivable network output learns relative frequency occurrence successor words readily approximates exact probabilities 
case shape vectors similar length different 
alternative measure normalizes length differences captures degree shape vectors similar cosine angle 
vectors parallel cosine yield rms error case feel network extracted crucial information 
mean cosine angle network output training items likelihood vectors sd 
measure rms cosine network learned approximate likelihood ratios potential successors 
accomplished 
input representations give information form class prediction 
word vectors orthogonal 
generalizations true classes words learned occurrence statistics composition classes learned 
network extracted generalizations opposed simply memorizing sequence expect see patterns emerge internal representations network develops course learning task 
internal representations page captured pattern hidden unit activations evoked response word context 
recall hidden units activated input units context units 
representations words isolation 
nature internal representations studied way 
learning phase complete passes corpus connection strengths network frozen 
input stream passed network final time learning place 
testing network produced predictions inputs output layer 
ignored 
hidden unit activations word context input saved resulting bit vectors 
word occurs times different contexts 
approximation word prototypical composite representation hidden unit activation patterns produced word contexts averaged yield single bit vector unique words input stream 
section see possible study internal representations words context 
internal representations subject hierarchical clustering analysis 
shows resulting tree tree reflects similarity structure internal representations lexical items 
lexical items similar properties grouped lower tree clusters similar words resemble clusters connected higher tree 
network discovered major categories words 
large category corresponds verbs category corresponds nouns 
verb category broken groups require direct object intransitive direct object optional 
noun category broken major groups animates 
animates divided human non human non human divided large animals small animals 
broken nouns appeared subjects agent active verbs 
network developed internal representations input vectors reflect facts possible sequential ordering inputs 
network able predict precise order words recognizes corpus class inputs viz verbs typically follow inputs viz nouns 
knowledge class behavior quite detailed fact class items precedes chase break smash infers large animals form class 
points emphasized 
category structure appears hierarchical 
large animals members class human animate nouns 
hierarchical interpretation achieved way spatial relations representations organized 
representations near representational space form classes higher level categories correspond larger general regions space 
second true soft implicit 
categories may qualitatively distinct far space may 
tony plate personal communication pointed technique dangerous inasmuch may introduce statistical artifact 
hidden unit activation patterns highly dependent preceding inputs 
preceding inputs uniformly distributed follow precisely occurrence conditions appropriate different categories means mean hidden unit pattern contexts specific item closely resemble mean hidden unit pattern items category 
occur learning consequence averaging vectors occurs prior cluster analysis 
results averaging technique verified clustering individual tokens tokens closer members type tokens types 
page distance eat 
hierarchical cluster diagram hidden unit activation vectors simple sentence prediction task 
labels indicate inputs produced hidden unit vectors inputs context hidden unit vectors averaged multiple contexts 
smell move see think exist break smash sleep transitive chase mouse cat animals dog monster lion dragon woman girl humans man boy car book rock sandwich cookie food bread plate glass intransitive transitive animates verbs nouns page categories share properties distinct boundaries 
category membership cases may marginal unambiguous 
content categories known network 
network information available ground structural information real world 
respect network information available real language learners 
realistic model acquisition imagine utterance provides source information nature lexical categories world provides source 
model embedding linguistic task environment network dual task extracting structural information contained utterance extracting structural information environment 
lexical meaning grow associations types input 
simulation important component meaning context 
representation word closely tied sequence embedded 
incorrect speak hidden unit patterns word representations conventional sense patterns reflect prior context 
view word meaning dependence context demonstrated way 
freeze connections network just trained learning occurs 
imagine novel word network seen assign word bit pattern different trained 
word place word man man occur occur 
new sequence sentences created trained network 
hidden unit activations saved subjected hierarchical clustering analysis sort training data 
resulting tree shown 
internal representation word bears relationship words word man original training set 
new word assigned internal representation consistent network learned learning occurs simulation consistent new word behavior 
way looking certain contexts network expects man 
just way imagine real language learners making cues provided word order intelligent guesses meaning novel words 
simulation designed provide model context effects word recognition behavior consistent findings described experimental literature 
number investigators studied effects sentential context word recognition 
researchers claimed lexical access insensitive context results suggest context sufficiently strong selectively facilitate access related words job 
furthermore individual items typically predictable classes words shoben 
precisely pattern error predicting actual word context remains high network able predict approximate likelihood occurrence classes words 

jay mcclelland suggested humorous entirely accurate metaphor task trying learn language listening radio 
page types tokens structured representations considerable discussion ways pdp networks differ traditional computational models 
apparent difference traditional models involve symbolic representations pdp nets people non subsymbolic fodor pylyshyn smolensky 
difficult complex issue part definition symbol problematic 
symbols things useful contrast pdp vs traditional models regard various functions symbols serve 
traditional pdp networks involve representations symbolic specific sense representations refer things 
traditional systems symbols names 
pdp nets internal representations generally activation patterns set hidden units 
kinds representations task referring important differences 
classical symbols typically refer classes categories pdp nets representations may highly context dependent 
smell move see think exist eat break smash chase mouse cat dog monster lion dragon woman girl boy car book rock sandwich cookie bread plate glass 
hierarchical clustering diagram hidden unit activation vectors simple sentence prediction task addition novel input 
sleep page mean representations capture information category class clear previous simulation mean room representation scheme pick individuals 
property pdp representations serious draw back 
extreme suggests separate representations entity john different context entity occur leading infinite number john drawback suggest aspect pdp networks significantly extends representational power 
distributed representations context representing words consequence simple recurrent networks provides solution thorny problem question represent type token differences sheds insight ways distributed representations represent structure 
order justify claim commenting representational richness provided distributed representations developed hidden units 
localist schemes node stands separate concept 
acquiring new concepts usually requires adding new nodes 
contrast hidden unit patterns simulations reported tended develop distributed representations 
scheme concepts expressed activation patterns fixed number nodes 
node participates representing multiple concepts 
activation pattern entirety meaningful 
activation individual node may uninterpretable isolation may refer feature micro feature 
distributed representations number advantages localist representations benefits 
units analog capable assuming activation states continuous range minimum maximum values principle limit number concepts represented finite set units 
simulations hidden unit patterns double duty 
required represent inputs develop representations serve useful encodings temporal context processing subsequent inputs 
theory analog hidden units capable providing infinite memory 
course reasons practice memory bounded number concepts stored finite 
limited numeric precision machines simulations run activation function repetitively applied memory results exponential decay training regimen may optimal exploiting full capacity networks 
instance simulations reported involved prediction task 
task incorporates feedback training cycle 
pilot poorer performance tasks delay injecting error network 
just representational capacity simple recurrent networks remains open question see servan schreiber cleeremans mcclelland 
having preliminary observations return question context sensitivity representations developed simulations reported 
consider sentence processing simulation 
learning predict words sentence sequences network developed representations reflected aspects words meaning grammatical category 
apparent similarity structure internal representation word structure graphically tree 
advantages discussed length hinton mcclelland rumelhart 
page sense representations clustered context sensitive 
fact recall representations composites hidden unit activation patterns response word averaged different contexts 
hidden unit activation pattern represent boy instance really mean vector activation patterns response boy occurs different contexts 
reason mean vector previous analysis large part practical 
difficult hierarchical clustering patterns difficult display resulting tree graphically 
want know patterns displayed tree way artifactual 
second analysis carried patterns clustered 
tree displayed numerical results indicate tree identical tree shown terminals stand different lexical items branches continue containing specific instances lexical item context 
instance lexical item appears inappropriately branch belonging 
correct think tree showing network discovered sequence inputs types 
types different lexical items shown 
finer grained analysis reveals network distinguishes specific occurrences lexical item tokens 
internal representations various tokens lexical type similar 
gathered single branch tree 
internal representations subtle distinctions example boy context boy 
similar representations various tokens tokens type exactly identical 
interesting sub structure representations various types token 
seen looking shows subtrees corresponding tokens boy girl 
think expansions terminal leaves boy girl 
individual tokens distinguished labels indicate original context 
thing apparent sub trees types boy girl similar 
closer scrutiny see organization exceptions tokens boy occur sentence initial position clustered tokens boy sentence final position clustered 
furthermore pattern occurs patterns representing girl 
sentence final words clustered basis similarities preceding words 
basis clustering sentence initial inputs simply preceded effectively noise prior sentences 
useful expectations sentence initial noun noun prior sentences 
hand imagine discourse structure relating sentences useful information sentence affect representation sentence initial words 
example information disambiguate give referential content sentence initial pronouns 
useful try understand results geometric terms 
hidden unit activation patterns pick points high fixed dimensional space 
space available network internal representations 
network structures space way important relations entities translated spatial relationships 
entities nouns located region space verbs 
similar manner different types lexical items distinguished occupying different regions space tokens type differentiated 
differentiation page 
hierarchical cluster diagram hidden unit activation vectors response occurrences boy girl 
upper case labels indicate actual input lower case labels indicate context 
non random way tokens type elaborated similar elaboration type 
john bears spatial relationship john mary bears mary 
context appealing provides basis establishing generalizations classes items allows tagging individual items context 
result able identify types time tokens 
symbolic systems type token distinctions indexing binding operations networks provide alternative account distinctions indexing binding 
human behaviors unfold time 
try understand behaviors account temporal nature 
current set simulations explores consequences attempting develop representations time distributed task dependent time represented implicitly network dynamics 
approach described employs simple architecture surprisingly powerful 
points worth highlighting 
problems change nature expressed temporal events 
simulation sequential version xor learned 
solution problem involved detection state changes development frequency sensitive hidden units 
casting xor problem temporal terms led different solution typically obtained feed forward simultaneous input networks 
page time varying error signal clue temporal structure 
temporal sequences uniformly structured uniformly predictable 
network successfully learned structure temporal sequence error may vary 
error signal metric structure exists provides potentially useful form feedback system 
increasing sequential dependencies task necessarily result worse performance 
second simulation task complicated increasing dimensionality input vector extending duration sequence making duration sequence variable 
performance remained complications accompanied redundancy provided additional cues task 
network able discover parts complex input predictable making possible maximize performance face partial unpredictability 
representation time memory highly task dependent 
networks depend internal representations available part input previous state 
way internal representations inter mix demands task demands imposed carrying task time 
separate representation time 
simply representation input patterns context output function just happens input patterns sequential 
representation representation time varies task task 
presents somewhat novel view memory 
account memory passive separate subsystem 
properly speak memory sequences memory inextricably bound rest processing mechanism 
representations need flat atomistic unstructured 
sentence task demonstrated sequential inputs may give rise internal representations hierarchical nature 
implicit similarity structure hidden unit activations require priori architectural commitment depth form hierarchy 
importantly distributed representations available space richly structured 
categorial relationships type token distinctions readily apparent 
item may representation representations structured relations representations preserved 
results described preliminary nature 
highly suggestive raise questions answer 
networks properly thought dynamical systems know properties 
instance analyses reported frequent hierarchical clustering techniques order examine similarity structure internal representations 
representations snapshots internal states course processing sequential input 
hierarchical clustering snapshots gives useful information ways internal states network different points time similar dissimilar 
temporal relationship states lost 
know trajectories states look 
sort attractors develop systems 
problem course networks studied high dimensional systems consequently difficult study page traditional techniques 
promising approach currently studied carry principal components analysis hidden unit activation pattern time series construct phase state portraits significant principal components elman 
question interest memory capacity networks 
results reported suggest networks considerable representational power systematic analysis better defined tasks clearly desirable 
experiments currently underway sequences generated finite state automata various types devices relatively understood memory requirements may precisely controlled servan schreiber cleeremans mcclelland 
things feedforward pdp models shown simple networks capable discovering useful interesting internal representations static tasks 
put way rich representations implicit tasks 
interesting human behaviors serial component 
exciting results suggest inductive power pdp approach discover structure representations tasks unfold time 
page berwick weinberg 

grammatical basis linguistic performance 
cambridge ma mit press 
chomsky syntactic structures 
hague 
chomsky 

aspects theory syntax 
cambridge ma mit press 
cottrell munro zipser 

image compression back propagation demonstration extensional programming 
sharkey ed advances cognitive science vol 

chichester england ellis horwood 
elman 

structured representations connectionist models 
crl technical report 
center research language university california san diego 
elman zipser 

discovering hidden structure speech 
journal acoustical society america 
fodor pylyshyn 

connectionism cognitive architecture critical analysis 
pinker mehler eds connections symbols pp 

cambridge mass mit press 
fowler 

timing control speech production 
bloomington indiana university linguistics club 
fowler 

coarticulation theories extrinsic timing control 
journal phonetics 
frazier fodor 

sausage machine new stage parsing model 
cognition 
greenberg 

universals language 
cambridge ma mit press 
spoken word recognition processes gating paradigm 
perception psychophysics 
hanson 

connectionist network learns natural language grammar exposure natural language sentences 
ninth annual conference cognitive science society seattle washington 
lawrence erlbaum associates hillsdale hinton mcclelland rumelhart 

distributed representations 
rumelhart mcclelland eds parallel distributed processing explorations microstructure cognition vol 
pp 

cambridge mass mit press 
jordan 

serial order parallel distributed processing approach 
institute cognitive science report 
university california san diego 
jordan rosenbaum 

action 
technical report department computer science university massachusetts amherst 
kelso 

dynamical theory speech production data theory 
journal phonetics 
page 

problem serial order behavior 
ed cerebral mechanisms behavior 
new york wiley 
lehman 

historical linguistics 
new york holt rinehart winston 


motor control serial ordering speech 
psychological review 
macwhinney 

acquisition 
monographs society research child development 
marcus 

theory syntactic recognition natural language 
cambridge ma mit press 
marslen wilson tyler 

temporal structure spoken language understanding 
cognition 
pineda 

generalization backpropagation recurrent higher order neural networks 
dana anderson ed neural information processing systems 
new york american institute physics 
pinker 


language learnability language 
development 
cambridge ma harvard university press 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart mcclelland eds parallel distributed processing explorations microstructure cognition vol 
pp 

cambridge ma mit press 


interaction knowledge sources spoken word identification 
journal memory language 
kelso 

skilled actions task dynamic approach 
psychological review 
sejnowski rosenberg 

parallel networks learn pronounce english text 
complex systems 
servan schreiber cleeremans mcclelland 

encoding sequential structure simple recurrent networks 
cmu technical report cmu cs 
computer science department carnegie mellon university 
smolensky 

variable binding representation symbolic structures connectionist systems 
technical report cu cs department computer science university colorado 
smolensky 

proper treatment connectionism 
behavioral brain sciences 
stornetta hogg huberman 

dynamical approach temporal pattern processing 
proceedings ieee conference neural information processing systems denver page 

effects context immediate interpretation unambiguous nouns 
journal experimental psychology learning memory cognition 
job 

accessing lexical ambiguity effects context dominance 
psychological research 
tank hopfield 

neural computation concentrating information time 
proceedings ieee international conference neural networks san diego june 
waibel hinton shikano lang 

phoneme recognition time delay neural networks 
atr technical report tr 
japan atr interpreting telephony research laboratories watrous shastri 

learning phonetic features connectionist networks experiment speech recognition 
proceedings ieee international conference neural networks san diego june 
williams zipser 

learning algorithm continually running fully recurrent neural networks 
institute cognitive science report 
university california san diego 
page 
