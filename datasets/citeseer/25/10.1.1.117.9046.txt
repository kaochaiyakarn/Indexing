filtering simulation auxiliary particle filters michael pitt nan nen shephard die approach filtering time 
suggest die outliers die design discrete represent sequentially prior distribution 
ck problems 
key words filtering markov chain monte carlo particle filter sampling importance resampling simulation state sp cc 

includes 
carter kohn article model time series yt 
kitagawa west papers re conditionally independent unobserved sufficient state assumed markovian 
task simulation carry line filtering learn state contemporaneously available information 
estimating compute density probability distribution function lit assume west harrison 

article simulation perform filtering extensive literature 
approach extend particle filter suggested independently various authors 
particular gordon salmond smith non gaussian sta space models 
algorithm extensions metric forms measurement density smoothing problem 
independently proposed transition density state ot 
kitagawa generalized state evolution initialized density 
time series problems 
filtering thought repeated application stage procedure 
current density propagated transition density produce prediction density ot oe df 
discarded best gilks context real time application sequential analysis medical patients 
proposed isard blake context robustly tracking motion visual clutter 
term condensation algorithm 
statistical refinements general class algorithm generically called particle filters econ 
moves filtering density bayes theorem 
hi lat jh carpenter clifford 
doucet 
liu chen written independently article 
idea calling class algorithm particle filters carpenter 

kitagawa term particles 
similar slt jh df 
ideas stronger assumptions blind deconvolution problem liu chen sequential importance sampling algorithms hendry implies data processed single richard kong 
liu 
wong 
sweep 
updating knowledge states re discuss particle filtering literature extend information 
straightforward number directions finite set known discrete points support broader context 
article organized follows 
previous calculations computed exactly 
section analyze statistical basis particle filters support continuous integrals focus weaknesses 
section introduce cally solved numerical methods 
main contribution auxiliary particle filter method 
numerous attempts provide algorithms approximate filtering densities 
important give numerical examples section state section 
pin research department ics imperial college queen gate london sw bz 

mail pitt ic ac ik 
neil official fellow college oxford ox inf 
mail neu ox ac ik 
authors social council financial support projects estimation vi 
simulation ec analysis econometric time european union eu econometric inference uai simulation techniques peter clifford david firth helpful 
comments editor 
editor 
referees initial submission useful 
reported carried ox 

particle filters definition particle filters particle filters class simulation filters re approximate filtering random variable ot 
th di rob yb yt es 
ot wl ability mass rl 
continuous variable american journal american june vol 
theory 
ii pitt shephard auxiliary particle filters approximated discrete random support 
points viewed samples 
assumed equal 
taken large 
require particles increasingly density 
particle filters treat discrete support generated particles true filtering density 
allows pro approximation prediction density simply discrete support particles 
call lf ia empirical prediction density 
mixture densities echoes earlier filtering example sorensen 
combined measurement density produce 
propor tionality oc yt lf ia empirical filtering density approximation true filtering density 
generically particle filters sample density produce new particles th gh wi wet ts thi proc iterated data 
call particle filter normal produces independent identically distributed samples empirical filtering density 
may advantages deliberately inducing negative correlations particles approach explicitly pointed carpenter fur density combined likelihood lat produce posterior 
sample choosing probability drawing ia 
evaluate yt proportionality 
leaves sampling methods draw sampling importance resampling acceptance sampling markov chain monte carlo mcmc 
rest section write prior likelihood abstracting subscripts conditioning arguments briefly describe methods context 
sampling resampling 
importance resampling sir method rub draws 
ar associates ea 
draws weights wj rj rj ei wi weighted sample converge nonrandom sample desired posterior wi 
nonrandom sample converted random sample size resampling weights rr 
requires method suggested particle filter framework 
gordon 
isard blake kitagawa 
understand efficiency sir method useful think sir approximation importance sampler moment ef df jf 
liu suggested variance estimator approximately slowly varying proportional ef jr sir method imprecise rj variable 
happen likelihood highly peaked compared prior 
adaptation 
foregoing sir algorithm samples making blind proposals ar prior ignoring fact know value ther explored earlier pitt shephard 
main feature existing particle filters 
discuss 
say particle filter adapted proposals particle filter take account value number different contexts 
include adapted sir particle filter line tracking problems estimating step ahead den general structure sity yt prediction decomposition joint density observations estimating corresponding distribution function yt useful diagnostic measure fit non gaussian models see shephard smith 



evaluate wj ai jg 
resample ai weights proportional wj produce sample size looks attractive particle filter sampling empirical prediction density la implies evalu way sampling empirical prediction ate densities generate samples 
density think ia prior typically large 
implies adaption generally feasible sir particle filters 
rejection markov chain monte carlo sampling 
exactly remarks hold rejection sampling 
blind rejection sampling particle filter simulate accept probability 
arg id xa 
proposed 

rejection worse high adaption difficult 
typically involve evaluating computationally infeasible 

min adapting expensive 
mid weaknesses particle filter problem degree accuracy efficiently sample show section 
fixed lagged filter 
basics 
auxiliary variable particle filtering higher dimension 

define ot oc ot ia lli oi iii joint density times perform reweighting putting draw ki weights proportional called second stage weights yt ia wi wj ist ei wi hope second stage weights variable original sir method 
discrete distribution produce sample size making proposals high conditional likelihoods reduce costs sampling times particles low likelihoods second stage process 
improves statistical efficiency sampling procedure means reduce value substantially 
measure statistical efficiency procedures argued earlier look minimizing 
compare standard sir sir auxiliary variable 
simplicity set cases 
standard sir particle filter large yt lat df ia yt df ot ia ijr eo ll 
yt la df yt jie yt df calculation sir auxiliary variable particle filter gives jr shows efficiency gain ai ai lc le ml lie vary auxiliary variable particle filter efficient 
depend mildly ot iq typically quite tightly peaked tightly peaked ott compared conditional likelihood 
examples basics 
previous generic scheme usually reduce variability second stage weights 
adaption schemes specific structure time series model allow achieve equal weights 
achieve exactly equal weights 
say fully adapted procedure model 
produce iid samples 
situation particularly interesting 
close assumptions kong ai 
sequential importance sampler 
full adaption practical importance 
remind fully adapted particle filters produce iid samples ac 
due approximation ac finite mixture distribution 
inherent construction class filter 
nonlinear gaussian measurement model 
gaussian measurement case absorption measurement density transition equation particularly convenient 
consider nonlinear transition density ot loc jj oc ot yc ll oc 
qh oc ih iq hh ctt lla ih qh 
ot 
implies stage weights 
yt ex exp gaussian measurement density implies weights equal 
example gaussian autoregressive conditional heteroscedasticity arch model see 

bollerslev 
engle 
nelson observed independent gaussian error 
qt ct 
qt qt pta 
model fully adaptable 
received great deal attention econometric literature attractive multivariate generalizations see diebold harvey ruiz king 
far know likelihood methods exist literature analysis type model various tions number approximations suggested 
tit 
exactly pr qh ex qt wk pr ola 
choose probability proportional wk draw truncated distribution conditional ji iit negative 
weights wk pr ala 
truncated draw pr 
style argument carries ordered probit censored models observe example min 
adaption important types models 
naively implemented particle auxiliary variable filters generally vulnerable tightly peaked measurement densities 
censored model 
min measurement density degenerate yh particle filter degenerate give mass simulation closest simulated pr ah equal iit adaption overcomes problem instantly 
adaption essential problem 
suppose lat gaussian bivariate observe min 
models called disequilibrium models economics 
area includes shephard 
pr ex pr yt lat pr la 
tjt proportional probability iq having minimum exactly iit 
shown exactly wi fo 
prq ll pr ql la 


having selected sample hl yt probability yt prq li iit ol yt iit likewise 
iit probability 
pfol ii angular 
ii solid line 
rt cie mnn eo 
iid ii auxiliary mean dot ed line 

ju 

ai 

mixtures normals 
suppose ot loe gaussian measurement density discrete mixture normals 
perfectly sample working oe oc aj ot lla wi ot lla jt sample selecting index probability proportional wj drawing ot lla yt disadvantage approach complete enumeration storage wi le involves calculations 
approach trivially extended cover case ot lat mixture normals 
mcmc smoothing methods state space models mixtures studied example carter shephard 

time series angles numerical example model 
section compare particle auxiliary particle filter angular time series model bearings model 
consider simple scenario described gordon ai 

observer considered stationary 
origin plane ship assumed gradually accelerate decelerate randomly time 
discretisation system xt 
zt 

eo 

ut var form tat hut 
cates state evolution error arises white noise 
initial state des ship starting positions velocities ni prior state evolution prior states 
model mean ta zt xd 
measured angle wrapped cauchy density mean resultant length 
ol simulated scenario 
efficiency particle filter basic method discussed section setup described gordon ai 

till ered ue ni choose yielding sam 
ut nid 
dispersion wrapped cauchy density 
tual initial starting vector taken 
contrast obvious notation xt zt represent ship horizontal vertical positions time represent corresponding velocities 
state evolution don ai 
wish accurate tight prior initial state 
want variance quantities arising pitt shephard auxiliary particle filters 

ni filtered heavy line kp 


bo md md ni returns 
uk 
pound dey iy 
notice iw ys 
mean 

posterior density small allowing formulate reasonably conclusive evidence relative efficiency auxiliary method standard method 
take diagonal initial variance pi elements diagonal 
illustrates realization model foregoing scenario 
ship moving direction time 
trajectories posterior filtered means particle method auxiliary method cases fairly close true path despite small amount simulation 
monte carlo comparison 
compare methods monte carlo study foregoing scenario 
true filtered mean calculated replication auxiliary method 
replication mean squared error mse particle method component state time evaluated running method different random number seed times recording average resulting squared difference resulting particle filter estimated mean true filtered mean 
replication state component time calculate 
particle mean replication state component time simulation true filtered mean replication state component time log mean squared error component time obtained rep log rep mse ri operation performed auxiliary method deliver corresponding quantity study rep 
allow values set 
shows relative performance methods component state vector time 
component quantity plotted time 
values close indicate methods broadly equivalent performance negative values indicate auxiliary method performs better standard particle filter 
graphs give expected result auxiliary particle filter typically precise difference methods falling increases 
stochastic volatility basic sv model defined section 
construct times compound daily returns dollar pound sterling day 
trading days active trading 
dataset discussed detail pitt 

sir auxiliary quickly particle pitt ci shephard auxiliary particle filters october 
revised 
bat 

dynamic conditional independence models markov chain monte 

iu iu iql oj 



dq kl ti handbook vi 
cds 
mcfadden amsterdam iip 

carpenter clifford dd 
particle filter working university oxford dept statistics 
carter 
gibbs sampling state space models 
diebold 
dynamics rate volatility multivariate latent arch mode applied 

ox object matrix programming 
consultants press 
doucet 

sequential simulation methods bayesian filtering unpublished manuscript university cambridge dept engineering 
fisher 
circular data cambridge cambridge university press 

diagnostics time series analysis unpublished manuscript 
university new south wales australian graduate school management 
gill spie 
mari ov carlo practice london chapman hall 
gordon smith 
novel non gaussian bayesian state estimation ieee 


ruiz 

economic 
time series models arch 


hendry richard 
evaluation dynamic variable models dor 
hull 
white 

options stochastic 

gaussian measurement time 
er 




monte carlo approximations smith 
ts non standard time series general state space unpublished manuscript seminar fur 
models 
sorensen als ch 

blake 
contour 
non gaussian sums automatica 
lion conditional proc european rr tic 
west 
mixture models monte carlo bayesian dd cch 
dynamic models computer 

jacquier 


bayesian analysis west harrison 


fa ting stochastic volatility models discussion 
journal models nd cd new york 
kim 
shephard 
chib 
stoc ii ro ur ad ft uo 
ft uj economic 

king 


volatility uno national stock markets econometrica 




non sta pace modeling non stationary ime series 
tm association 
si 
monte carlo filter smoother non nonlinear state space models 
statistics 


uu 
wong 


bayesian milling data problems statistical association 


simulation estimation models lagged latent variables applied econometrics 

uu 


independent sampling comp rejection sampling sampling statistics 


uu 
chen 
journal american statistical association 
sequential monte carlo methods dynamic journal american statistical association 

simulation inference gaussian limited dependent processes econometrics jo ci 
pitt shephard 
fixed lag auxiliary particle filter deterministic sampling rules unpublished manuscript nu 
time varying covariances factor stochastic volatility approach discussion bayesian statistics cds 
dawid smith oxford oxford university press 
rubin 
comment posterior data augmentation tanner wong american statistical association 

shephard 

partial non gaussian state space tl ne 

dd pin 
non 
