hierarchical topic models nested chinese restaurant process david blei thomas griffiths blei cs berkeley edu mit edu michael jordan joshua tenenbaum jordan cs berkeley edu mit edu university california berkeley massachusetts institute technology berkeley ca cambridge ma address problem learning topic hierarchies data 
model selection problem domain daunting large collection possible trees 
take bayesian approach generating appropriate prior distribution partitions refer nested chinese restaurant process 
nonparametric prior allows arbitrarily large branching factors readily accommodates growing data collections 
build hierarchical topic model combining prior likelihood hierarchical variant latent dirichlet allocation 
illustrate approach simulated data application modeling nips abstracts 
complex probabilistic models increasingly prevalent domains bioinformatics information retrieval vision robotics 
domains create fundamental modeling challenges due open ended nature data sets grow time grow bring new entities new structures fore 
current statistical modeling tools rigid regard particular classical model selection techniques hypothesis testing poorly matched problems data continue accrue unbounded sets structures considered step 
important instance modeling challenges provided problem learning topic hierarchy data 
collection documents contains set words wish discover common usage patterns topics documents organize topics hierarchy 
note terminology document modeling methods describe general 
develop efficient statistical methods constructing hierarchy allow grow change data accumulate 
approach model selection problem specifying generative probabilistic model hierarchical structures bayesian perspective problem learning structures 
hierarchies random variables random variables specified procedurally algorithm constructs hierarchy data available 
probabilistic object underlies approach distribution partitions integers known chinese restaurant process 
show extend chinese restaurant process hierarchy partitions show new process representation prior posterior distributions topic hierarchies 
possible approaches modeling topic hierarchies 
approach node hierarchy associated topic topic distribution words 
document generated choosing path root leaf repeatedly sampling topics path sampling words selected topics 
organization topics hierarchy aims capture breadth usage topics corpus reflecting underlying syntactic semantic notions generality specificity 
approach differs models topic hierarchies built premise distributions associated parents children similar :10.1.1.19.8906
assume constraint example root node may place probability mass function words descendants placing probability mass function words 
model closely resembles hierarchical topic model considered address model selection problem primary focus :10.1.1.33.8915
chinese restaurant processes brief description chinese restaurant process subsequently show process extended hierarchies 
chinese restaurant process chinese restaurant process crp distribution partitions integers obtained imagining process customers sit chinese restaurant infinite number tables 
basic process specified follows 
customer sits table 
mth subsequent customer sits table drawn distribution previously occupied table unoccupied table mi mi number previous customers table parameter 
customers sit seating plan gives partition items 
distribution gives partition structure draws dirichlet process 
crp generalizes dirichlet process allowing variations basic rule eq 
including data dependent choice general functional dependence current partition 
flexibility prove useful setting 
crp represent uncertainty number components mixture model 
mixture component parameter hyperparameter chinese restaurant placing draw tables 
data point generated choosing table eq 
sampling value distribution specified parameter associated table 
data set posterior chinese restaurant distribution partitions observed data 
number mixture components reflected number tables data occupy possible partition particular data sitting table induce posterior parameter table 
posteriors estimated markov chain monte carlo 
applications various kinds mixture models begun appear years examples include gaussian mixture models hidden markov models mixtures experts :10.1.1.19.9008:10.1.1.16.2929
terminology inspired chinese restaurants san francisco infinite seating capacity 
coined jim pitman lester early eighties 
extending crp hierarchies crp amenable mixture modeling establish relationship tables mixture components relationship mixture components data 
models consider data point associated multiple mixture components lie path hierarchy 
develop hierarchical version crp prior models 
nested chinese restaurant process defined imagining scenario 
suppose infinite number infinite table chinese restaurants city 
restaurant determined root restaurant infinite tables card name restaurant 
tables restaurants cards refer restaurants structure repeats infinitely 
restaurant referred exactly restaurants city organized infinitely branched tree 
note restaurant associated level tree root restaurant level restaurants refers level 
tourist arrives city vacation 
evening enters root chinese restaurant selects table eq 

second evening goes restaurant identified night table chooses table eq 

repeats process days 
trip tourist sat restaurants constitute path root restaurant lth level infinite tree described 
tourists take day vacations collection paths describe particular level subtree infinite tree see example tree 
prior model topic hierarchies 
just posterior standard crp express uncertainty possible number components posterior nested crp express uncertainty possible level trees 
hierarchical topic model consider data set composed corpus documents 
document collection words word item vocabulary 
basic assumption words document generated mixture model mixing proportions random document specific 
consider multinomial topic variable associated set distributions words parameter 
topic distributions distribution value basic mixture components model 
document specific mixing proportions associated components denoted vector 
temporarily assuming possible topics corpus assumption soon relax ranges possible values dimensional vector 
document specific mixture distribution ip zi random distribution random 
specify level generative probabilistic process generating document choose vector topic proportions distribution corpus level parameter repeatedly sample words mixture distribution chosen value 
distribution chosen dirichlet distribution obtain latent dirichlet allocation model lda :10.1.1.110.4050
lda twolevel generative process documents associated topic proportions corpus modeled dirichlet distribution topic proportions 
describe hierarchical extension model refer hierarchical lda 
level tree node associated topic distribution generates document follows choose path root tree leaf draw vector topic proportions dimensional dirichlet generate words document mixture topic distributions path root leaf mixing proportions 
model viewed fully generative version hierarchical topic model :10.1.1.33.8915
develop methodology allows learning hierarchical structures parameters combine nested crp 
seen nested crp induces prior possible trees 
place prior topic distributions associated restaurant infinite tree particular assume symmetric dirichlet hyperparameter 
document drawn choosing level path restaurants drawing words topic distributions associated restaurants path 
note documents share topic distribution associated root restaurant 

root restaurant 

level 
draw table restaurant eq 

set restaurant referred table 

draw dimensional topic proportions dir 

word 
draw 
mult 
draw wn topic distribution associated restaurant cz 
generative model graphical model 
node labeled refers collection infinite number level paths drawn nested crp 
cm variables deterministic simply look th level mth path infinite collection paths 
having observed distribution cm defined nested chinese restaurant process conditioned cq suppose corpus documents 
wm posterior essentially transferred deterministic relationship posterior paths consider new document wm 
posterior path depend unobserved posterior paths documents original corpus 
subsequent new documents depend original corpus new documents observed 
note eq 
new document choose previously unvisited restaurant level tree 
peaked posterior essentially selected particular tree new document change hierarchy words provide justification change 
variation crp theme note consider process flattens nested crp standard crp retains idea tourist eats meals 
tourist eats times single restaurant constraint choose table twice 
vacation interesting model provides interesting prior 
particular prior flat lda model document topic distributions potentially infinite total set topic distributions 
examine model section compare crp methods selection bayes factors 
approximate inference gibbs sampling section describe gibbs sampling algorithm sampling posterior nested crp corresponding topic distributions model 
gibbs sampler provides clean method simultaneously exploring parameter space particular topic distributions corpus model space level trees 
variables needed sampling algorithm wm nth word mth document observed variables model cm restaurant corresponding paths tourists triangle square pentagon circle infinite tree chinese restaurants 
solid lines connect restaurant restaurants referred tables 
collected paths tourists describe particular subtree underlying infinite tree 
illustrates sample state space posterior nested crp documents 
graphical model representation hierarchical lda nested crp prior 
separated nested chinese restaurant process topic distributions 
infinite corresponds restaurants 
th topic distribution document zm assignment nth word mth document available topics 
variables model integrated 
gibbs sampler assesses values zm cm 
conceptually divide gibbs sampler parts 
current state crp sample zm variables underlying lda model algorithm developed reproduce :10.1.1.20.4260:10.1.1.110.4050
second values lda hidden variables sample cm variables associated crp prior 
conditional distribution cm topics associated document cm wm cm denote variables documents expression instance bayes rule wm likelihood data particular choice cm cm prior cm implied nested cm crp 
likelihood obtained integrating parameters gives cm wm cm cm cm cm cm number instances word assigned topic indexed cm including current document total vocabulary size denotes standard gamma function 
contains previously unvisited restaurant cm zero 
note cm drawn block 
set possible values cm corresponds union set existing paths tree equal number leaves sample documents document corpus level bars hierarchy described section skewed higher levels 
document words term vocabulary 
correct hierarchy gibbs sampler corpus 
structure leaf error results estimating hierarchies simulated data 
structure refers level hierarchy integer number branches root followed number children branch 
leaf error refers leaves incorrect resulting tree exact 
subsumes errors 
set possible novel paths equal number internal nodes 
set enumerated scored eq 
definition nested crp section 
examples empirical results section describe number examples experiments models described 
experiments sampler burn iterations subsequently took samples iterations apart iterations 
local maxima problem model 
avoid randomly restart sampler times take trajectory highest average posterior likelihood 
illustrate nested crp process feasible learning text hierarchies contrived corpus small vocabulary 
generated corpus word documents level hierarchy vocabulary terms 
corpus topic distributions vocabulary viewed bars grid 
root topic places probability mass bottom bar 
second level topic identified leftmost bar rightmost bar represents second topic 
leftmost topic subtopics rightmost topic subtopic 
illustrates documents sampled model 
illustrates recovered hierarchy gibbs sampling algorithm described section 
estimating hierarchy structures hypothesis testing approaches model selection impractical provide viable method searching large space trees 
compare crp method lda models standard approach implemented simpler flat model described section 
generated corpora word documents lda model 
vocabulary size randomly generated mixture components symmetric dirichlet 
comparison crp prior approximate bayes factor method model selection chooses model maximizes data various appropriate prior :10.1.1.20.4260
lda model bayes factor method slower crp involves multiple runs gibbs sampler speed comparable single run crp gibbs sampler 
furthermore bayes factor method choose appropriate range crp prior free parameter 
shown crp prior effective bayes factor setting 
note crp bayes factor somewhat sensitive choice hyperparameter prior topic dimension chinese process prior true dimension dimension bayes factor true dimension left average dimension crp prior plotted true dimension simulated data true value see overlapping points 
dimension generated corpora vocabulary size 
corpus contains documents words 
right results model selection bayes factors 
distributions 
simulated data hyperparameter known provide fair comparison 
similar experiment generated corpora different hierarchies model symmetric dirichlet prior topic distributions 
corpus documents words vocabulary terms 
reports results sampling resulting posterior trees gibbs sampler section 
cases recover correct structure usually recover structure leaf correct structure 
experiments predicted structure deviated nodes correct structure 
lastly demonstrate applicability real data applied model text data set 
nips abstracts words vocabulary terms estimated level hierarchy illustrated 
model nicely captured function words auxiliary list nuisance practical applications language models require 
level separated words pertaining neuroscience abstracts machine learning abstracts 
delineated important subtopics fields 
results suggest effective tool text applications 
summary nested chinese restaurant process distribution hierarchical partitions 
shown process nonparametric prior hierarchical extension latent dirichlet allocation model 
result flexible general model topic hierarchies naturally accommodates growing data collections 
gibbs sampling procedure model provides simple method simultaneously exploring spaces trees topics 
model natural extensions 
restricted hierarchies fixed depth simplicity straightforward consider model vary document document 
document mixture topics path hierarchy different documents express paths different lengths neurons visual cells cortex synaptic motion response processing cell neuron circuit cells input synapses chip analog vlsi synapse weight digital cmos design recognition speech character word system classification characters phonetic algorithm learning training method new problem control reinforcement learning policy state actions value optimal hidden units layer input output unit vector training topic hierarchy estimated abstracts nips nips 
node contains top words corresponding topic distribution 
represent varying levels specialization 
second current model document associated single path natural consider models documents allowed mix paths 
natural way take advantage syntactic structures paragraphs sentences document 
aldous 
exchangeability related topics 
cole de probabilit de saint flour xiii pages 
springer berlin 
segal koller ormoneit :10.1.1.19.8906
probabilistic abstraction hierarchies 
advances neural information processing systems 
hofmann :10.1.1.33.8915
cluster abstraction model unsupervised learning topic hierarchies text data 
ijcai pages 
ferguson 
bayesian analysis nonparametric problems 
annals statistics 
pitman 
combinatorial stochastic processes 
notes st flour summer school 

neal 
markov chain sampling methods dirichlet process mixture models 
journal computational graphical statistics june 
west muller escobar 
hierarchical priors mixture models application regression density estimation 
aspects uncertainty 
john wiley 
beal ghahramani rasmussen :10.1.1.16.2929
infinite hidden markov model 
advances neural information processing systems 
rasmussen ghahramani :10.1.1.19.9008
infinite mixtures gaussian process experts 
advances neural information processing systems 
blei ng jordan :10.1.1.110.4050
latent dirichlet allocation 
journal machine learning research january 
griffiths steyvers :10.1.1.20.4260:10.1.1.110.4050
probabilistic approach semantic representation 
proceedings th annual conference cognitive science society 
kass raftery :10.1.1.20.4260
bayes factors 
journal american statistical association 
roweis 
nips abstracts 
www cs toronto edu roweis data html 
