distance metric learning large margin nearest neighbor classification kilian weinberger john lawrence saul department computer information science university pennsylvania levine hall walnut street philadelphia pa cis upenn edu show learn distance metric nearest neighbor knn classification semidefinite programming 
metric trained goal nearest neighbors belong class examples different classes separated large margin 
data sets varying size difficulty find metrics trained way lead significant improvements knn classification example achieving test error rate mnist handwritten digits 
support vector machines svms learning problem reduces convex optimization hinge loss 
learning svms framework requires modification extension problems multiway opposed binary classification 
nearest neighbors knn rule oldest simplest methods pattern classification 
yields competitive results certain domains cleverly combined prior knowledge significantly advanced state ofthe art 
knn rule classifies unlabeled example majority label nearest neighbors training set 
performance depends crucially distance metric identify nearest neighbors 
absence prior knowledge knn classifiers simple euclidean distances measure dissimilarities examples represented vector inputs 
euclidean distance metrics capitalize statistical regularities data estimated large training set labeled examples 
ideally distance metric knn classification adapted particular problem solved 
hardly optimal example distance metric face recognition gender identification tasks distances computed fixed size images 
shown researchers knn classification significantly improved distance metric learned labeled examples 
simple global linear transformation input features shown result better knn classification 
builds novel direction success previous approaches 
show learn distance metric knn classification 
metric optimized goal nearest neighbors belong class examples different classes separated large margin 
goal metric learning differs crucial way previous approaches minimize pairwise distances similarly labeled examples :10.1.1.58.3667
objective far difficult achieve leverage full power knn classification accuracy require similarly labeled inputs tightly clustered 
approach largely inspired neighborhood component analysis metric learning energy models 
goals methods quite different 
particular able cast optimization instance semidefinite programming 
optimization propose convex global minimum efficiently computed 
approach parallels learning support vector machines svms notably goal margin maximization convex objective function hinge loss 
light parallels describe approach large margin nearest neighbor classification 
framework viewed logical counterpart svms knn classification replaces linear classification 
framework contrasts classification svms intriguing respect requires modification problems multiway opposed binary classification 
extensions svms multiclass problems typically involve combining results binary classifiers require additional machinery elegant nontrivial 
cases training time scales linearly number classes 
contrast learning problem explicit dependence number classes 
model xi yi denote training set labeled examples inputs xi discrete necessarily binary class labels yi 
binary matrix yij indicate labels yi yj match 
goal learn linear transformation compute squared distances xi xj xi xj 
specifically want learn linear transformation optimizes knn classification distances measured way 
developing useful terminology 
target neighbors addition class label yi input xi specify target neighbors inputs label yi wish minimal distance xi computed eq 

absence prior knowledge target neighbors simply identified nearest neighbors determined euclidean distance share label yi 
done experiments 
ij indicate input xj target neighbor input xi 
binary matrix yij matrix ij fixed change learning 
cost function cost function distance metrics parameterized eq 
competing terms 
term penalizes large distances input target neighbors second term penalizes small distances input inputs share label 
specifically cost function ij xi xj ij xi xj xi xl ij ijl second term max denotes standard hinge loss denotes positive constant typically set cross validation 
note term penalizes large distances inputs target neighbors similarly labeled examples 
large margin second term cost function incorporates idea margin 
particular input xi hinge loss incurred differently labeled inputs distances exceed absolute unit distance distance input xi target neighbors 
cost function favors distance metrics differently labeled inputs maintain large margin distance threaten invade neighborhoods 
learning dynamics induced cost function illustrated fig 
input target neighbors 
parallels svms competing terms eq 
analogous cost function svms 
cost functions term penalizes norm parame margin xi xi target neighbor local neighborhood margin xi xi similarly labeled differently labeled differently labeled schematic illustration input neighborhood xi training left versus training right 
distance metric optimized target neighbors lie smaller radius training ii differently labeled inputs lie outside smaller radius margin unit distance 
arrows indicate gradients distances arising optimization cost function 
ter vector weight vector maximum margin hyperplane linear transformation distance metric incurs hinge loss examples violate condition unit margin 
just hinge loss svms triggered examples near decision boundary hinge loss eq 
triggered differently labeled examples invade neighborhoods 
convex optimization reformulate optimization eq 
instance semidefinite programming 
semidefinite program sdp linear program additional constraint matrix elements linear unknown variables required positive semidefinite 
sdps convex reformulation global minimum eq 
efficiently computed 
obtain equivalent sdp rewrite eq 
xi xj xi xj xi xj matrix parameterizes mahalanobis distance metric induced linear transformation rewriting eq 
sdp terms straightforward term linear hinge loss mimicked introducing slack variables ij pairs differently labeled inputs yij 
resulting sdp minimize ij ij xi xj xi xj ijl ij ijl subject xi xl xi xl xi xj xi xj ijl ijl 
constraint indicates matrix required positive semidefinite 
sdp solved standard online packages general purpose solvers tend scale poorly number constraints 
implemented special purpose solver exploiting fact slack variables ij attain positive values slack variables ij sparse labeled inputs separated resulting pairwise distances incur hinge loss obtain active constraints 
solver combination sub gradient descent matrices mainly verify reached global minimum 
projected updates back positive semidefinite cone step 
alternating projection algorithms provably converge case implementation worked faster generic solvers results evaluated algorithm previous section data sets varying size difficulty 
table compares different data sets 
principal components analysis pca reduce dimensionality image speech text data speed training avoid overfitting 
isolet mnist experimental results averaged runs randomly generated splits data 
isolet mnist pre defined training test splits 
data sets randomly generated splits run 
number target neighbors weighting parameter eq 
set cross validation 
purpose cross validation training sets partitioned training validation sets 
reporting trends discussing individual data sets detail 
compare knn classification error rates mahalanobis versus euclidean distances 
break ties different classes repeatedly reduced neighborhood size ultimately classifying necessary just nearest neighbor 
fig 
summarizes main results 
smallest data set training appears issue mahalanobis distance metrics learned semidefinite programming led significant improvements knn classification training testing 
training error rates reported fig 
leave estimates 
computed test error rates variant knn classification inspired previous energy models 
energy classification test example xt done finding label minimizes cost function eq 

particular hypothetical label yt accumulated squared distances nearest neighbors xt share label training set corresponding term cost function accumulated hinge loss pairs differently labeled examples result labeling xt yt corresponding second term cost function 
test example classified hypothetical label minimized combination terms yt argmin yt tj xt xj ij xi xj xi xl shown fig 
energy classification assignment rule generally led reductions test error rates 
compared results multiclass svms 
data set mnist trained multiclass svms linear rbf kernels fig 
reports results better classifier 
mnist non homogeneous polynomial kernel degree gave best results 
see :10.1.1.21.4023
great speedup achieved solving sdp monitors fraction margin constraints resulting solution starting point actual sdp interest 
matlab implementation currently available www seas upenn edu 
iris wine faces bal isolet news mnist examples train examples test classes input dimensions features pca constraints mil mil bil active constraints cpu time run runs table properties data sets experimental parameters classification 
training error rate mnist news isolet bal faces wine iris knn euclidean distance knn mahalanobis distance energy classification multiclass svm testing error rate training test error rates knn classification euclidean versus mahalanobis distances 
yields lower test error rates smallest data set presumably due training 
energy classification see text generally leads improvement 
results approach state art multiclass svms 
small data sets classes wine iris balance data sets small data sets training examples just classes taken uci machine learning repository data sets size distance metric learned matter seconds 
results fig 
averaged experiments different random splits data set 
results data sets roughly comparable better cases worse neighborhood component analysis nca relevant component analysis rca reported previous 
face recognition face recognition data set contains grayscale images individuals different poses 
downsampled images pixels pca obtain dimensional eigenfaces :10.1.1.12.7580
training test sets created randomly sampling images person training images testing 
task involved way classification essentially recognizing face unseen pose 
fig 
shows improvements due classification 
fig 
illustrates improvements graphically showing nearest neighbors change result learning mahalanobis metric 
algorithm operated low dimensional eigenfaces clarity shows rescaled images 
available www ics uci edu mlearn mlrepository html 
available www uk research att com html test image nearest neighbors training nearest neighbors training images face recognition data base 
top row image correctly recognized knn classification mahalanobis distances euclidean distances 
middle row correct match nearest neighbors mahalanobis distance euclidean distance 
bottom row incorrect match nearest neighbors euclidean distance mahalanobis distance 
spoken letter recognition isolet data set uci machine learning repository examples classes corresponding letters alphabet 
reduced input dimensionality originally projecting data leading principal components account total variance 
data set dietterich bakiri report test error rates nonlinear backpropagation networks output units class nonlinear backpropagation networks bit error correcting code :10.1.1.72.7289
energy classification obtains test error rate 
text categorization newsgroups data set consists posted articles newsgroups roughly articles newsgroup 
version data set removed headers stripped 
tokenized newsgroups rainbow package 
article initially represented weighted word counts common words 
reduced dimensionality projecting data leading principal components 
results fig 
obtained averaging runs splits training test data 
best result data set test error rate improved significantly knn classification euclidean distances 
performed comparably best multiclass svm obtained test error rate linear kernel dimensional inputs 
handwritten digit recognition mnist data set handwritten digits extensively benchmarked :10.1.1.21.4023
original grayscale images reduced dimensionality retaining principal components capture data variance 
energy classification yielded test error rate cutting baseline knn error rate third 
comparable benchmarks exploiting additional prior knowledge include multilayer neural nets svms :10.1.1.21.4023
fig 
shows digits nearest neighbor changed result learning mismatch euclidean distance match distance 
related researchers attempted learn distance metrics labeled examples 
briefly review methods pointing similarities differences 
available people csail mit edu newsgroups available yann lecun com exdb mnist test image nearest neighbor training nearest neighbor training top row examples mnist images nearest neighbor changes training 
middle row nearest neighbor training mahalanobis distance metric 
bottom row nearest neighbor training euclidean distance metric 
xing semidefinite programming learn mahalanobis distance metric clustering 
algorithm aims minimize sum squared distances similarly labeled inputs maintaining lower bound sum distances differently labeled inputs 
similar basis semidefinite programming differs focus local neighborhoods knn classification 
shalev shwartz proposed online learning algorithm learning mahalanobis distance metric 
metric trained goal similarly labeled inputs small pairwise distances bounded differently labeled inputs large pairwise distances bounded 
margin defined difference thresholds induced hinge loss function 
similar basis appeal margins hinge loss functions differs focus local neighborhoods knn classification 
particular seek minimize distance similarly labeled inputs specified neighbors 
goldberger proposed neighborhood component analysis nca distance metric learning algorithm especially designed improve knn classification 
algorithm minimizes probability error stochastic neighborhood assignments gradient descent 
shares essentially goals nca differs construction convex objective function 
chopra proposed framework similarity metric learning metrics parameterized pairs identical convolutional neural nets 
cost function penalizes large distances similarly labeled inputs small distances differently labeled inputs penalties incorporate idea margin 
similar cost function metric parameterized linear transformation convolutional neural net 
way obtain instance semidefinite programming 
relevant component analysis rca constructs mahalanobis distance metric weighted sum class covariance matrices 
similar pca linear discriminant analysis different approach reliance second order statistics 
hastie tibshirani consider schemes locally adaptive distance metrics vary input space 
appeals goal margin maximization differs substantially approach 
particular suggest decision boundaries svms induce locally adaptive distance metric knn classification 
contrast approach similarly named involve training svms 
discussion shown learn mahalanobis distance metrics knn classification semidefinite programming 
framework assumptions structure distribution data scales naturally large number classes 
ongoing focused directions 
working apply classification problems hundreds thousands classes advantages apparent 
second investigating kernel trick perform classification nonlinear feature spaces 
yields highly nonlinear decision boundaries original input space obvious algorithm lead significant improvement 
extending framework learn locally adaptive distance metrics vary input space 
metrics lead flexible powerful classifiers 
belongie malik puzicha 
shape matching object recognition shape contexts 
ieee transactions pattern analysis machine intelligence pami 
chopra lecun 
learning metric discriminatively application face verification 
proceedings ieee conference computer vision pattern recognition cvpr san diego ca 
cover hart 
nearest neighbor pattern classification 
ieee transactions information theory pages 
crammer singer 
algorithmic implementation multiclass kernel vector machines 
journal machine learning research 
dietterich bakiri :10.1.1.72.7289
solving multiclass learning problems error correcting output codes 
journal artificial intelligence research number 
gunopulos peng 
large margin nearest neighbor classifiers 
ieee transactions neural networks 
goldberger roweis hinton 
neighbourhood components analysis 
saul weiss bottou editors advances neural information processing systems pages cambridge ma 
mit press 
hastie tibshirani 
discriminant adaptive nearest neighbor classification 
ieee transactions pattern analysis machine intelligence pami 
lecun jackel bottou cortes denker drucker guyon muller sackinger simard vapnik :10.1.1.21.4023
comparison learning algorithms handwritten digit recognition 
fogelman gallinari editors proceedings international conference artificial neural networks icann pages paris 
mccallum 
bow toolkit statistical language modeling text retrieval classification clustering 
www cs cmu edu mccallum bow 
sch lkopf smola 
learning kernels support vector machines regularization optimization 
mit press cambridge ma 
shalev shwartz singer ng 
online batch learning pseudo metrics 
proceedings st international conference machine learning banff canada 
hertz weinshall pavel 
adjustment learning relevant component analysis 
proceedings seventh european conference computer vision eccv volume pages london uk 
springer verlag 
simard lecun decker 
efficient pattern recognition new transformation distance 
advances neural information processing systems volume pages san mateo ca 
morgan kaufman 
turk pentland :10.1.1.12.7580
eigenfaces recognition 
journal cognitive neuroscience 
vandenberghe boyd 
semidefinite programming 
siam review march 
xing ng jordan russell 
distance metric learning application clustering side information 
dietterich becker ghahramani editors advances neural information processing systems cambridge ma 
mit press 
