appear neural computation 
variational learning nonlinear gaussian belief networks brendan frey geoffrey hinton department computer science university toronto king college road toronto canada april view perceptual tasks vision speech recognition inference problems goal estimate posterior distribution latent variables depth stereo vision sensory input 
flurry research independent component analysis exemplifies importance inferring latent variables input data 
latent variables method linearly related input perception requires nonlinear inferences classification depth estimation 
unifying framework stochastic neural networks nonlinear latent variables 
nonlinear units obtained passing outputs linear gaussian units various nonlinearities 
general variational method maximizes lower bound likelihood training set give results visual feature extraction problems 
show variational method pattern classification compare performance nonlinear networks methods problem handwritten digit recognition 
proposals unsupervised multilayer neural networks contain stochastic generative model learn adjusting parameters maximize likelihood generating observed data 
tractable models kind factor analysis everitt independent component analysis comon jutten herault bell sejnowski amari cichocki yang mackay 
linear generative models factor analysis hidden layer contains fewer units visible layer 
generative model hidden units driven unit variance independent gaussian noise 
hidden units provide top input linear visible units generative weights visible unit level added gaussian noise 
generative weights noise levels visible units tractable compute posterior distribution hidden activities induced observed vector visible activities 
posterior distribution full covariance gaussian mean depends visible activities 
distribution computed straightforward adjust generative weights maximize likelihood observed data gradient method expectation maximization em algorithm rubin thayer 
unfortunately factor analysis ignores statistical structure data contained covariance matrix hidden representations linearly related data unable extract hidden causes data important tasks vision speech recognition 
independent component analysis generative model linear independent noise levels hidden units non gaussian 
difficult compute full posterior distribution hidden units visible vector 
number hidden visible units setting noise levels visible units zero possible collapse posterior distribution hidden units point multiplying visible activities inverse matrix hidden visible generative weights 
maximize likelihood data weights adjusted posterior points high log probability noise models hidden units whilst keeping determinant generative weight matrix small probability density space hidden activities gets concentrated mapped visible space 
unfortunately independent component analysis extracts components linear function data assumes data noise free unable extract hidden causes nonlinearly related observed noisy data 
attempts enhance representational capabilities independent component analysis adding noise visible units olshausen field lewicki sejnowski 
nonlinear generative models appealing approach understanding cortex constructs models sensory data assume uses maximum likelihood learn hierarchical generative model 
tasks vision speech recognition cortex probably requires distributed representations nonlinear function data allow noise level hierarchy 
attempts developing learning algorithms capable constructing generative models successful practice simpler linear models 
hard compute represent posterior probability distribution hidden representations visible vector set weights noise variances 
unsupervised version boltzmann machine hinton sejnowski multilayer generative model learns distributed representations nonlinear function data 
uses symmetrically connected stochastic binary units relatively simple learning rule follows gradient log likelihood data generative model 
unfortunately get gradient necessary perform gibbs sampling hidden activities reach thermal equilibrium data vector clamped visible units 
time consuming problem worse need compute derivatives partition function requires network reach thermal equilibrium visible units 
sampling noise difficulty reaching equilibrium networks large weights learning algorithm painfully slow 
binary stochastic units connected directed acyclic graph get binary sigmoidal belief network pearl neal 
acyclic means aren closed paths edge directions 
may closed paths edge directions ignored 
net input unit weighted sum activities unit parents 
learning easier network boltzmann machine need compute derivative partition function gradient log likelihood involve difference sampled statistics 
importantly longer necessary gibbs sampling converge thermal equilibrium weights adjusted 
analysis em provided neal hinton shown average learning algorithm improves bound log probability data gibbs sampling brief get close equilibrium hinton sallans ghahramani 
attempts avoid gibbs sampling altogether fitting sigmoidal belief network data 
see frey review methods 
rely idea learning improve bound log likelihood data posterior distribution hidden states computed incorrectly 
stochastic helmholtz machine hinton uses separate stochastic recognition network compute quick dirty approximation sample posterior distribution hidden units visible vector 
simple rule learning generative weights recognition weights approximation produced recog nition network poor method learning recognition weights guaranteed improve 
deterministic helmholtz machine dayan restrictive assumptions stochastic version probability distribution approximate full posterior distribution binary hidden states data vector 
assumes approximating distribution written product separate probabilities hidden unit 
assumes approximating product distribution computed deterministic recognition network single bottom pass 
assumption relaxed variational approaches saul jaakkola jordan jaakkola saul jordan eliminate separate recognition model generative weights numerical optimization find set probabilities minimizes asymmetric divergence true posterior distribution 
continuous sigmoidal belief networks real valued data comes real physical processes binary units inappropriate model fail capture approximately linear structure data small ranges 
example small changes position orientation scale object lead linear changes pixel intensities 
way endow linear gaussian networks described representations nonlinear functions data apply smooth sigmoidal squashing function output gaussian passing activity network 
nonlinear squashing function allows unit take variety behaviors ranging nearly gaussian nearly binary 
frey frey shown markov chain monte carlo variational method train small networks units 
smoothness squashing function prevents units placing probability mass single point units able produce activities exactly equal zero 
ability network set activities exactly equal zero important sparse representations units participate explaining input pattern 
piecewise linear belief networks attempt produce sparse distributed representations real valued data hinton ghahramani investigated generative models composed multiple layers rectified linear units 
generative model unit receives top input linear function rectified states layer adds gaussian noise get real valued state xi wi ai xj noise xj xj xj xj ai set indices parents unit output unit sends layer equal state positive equal negative 
networks units set activities units exactly equal zero participate explaining current input pattern 
hinton ghahramani showed gibbs sampling feasible networks multilayer networks rectified linear units learn extract sparse hidden representations nonlinearly related images 
nonlinear gaussian belief networks linear generative models binary sigmoidal belief networks continuous sigmoidal belief networks piecewise linear belief networks viewed networks gaussian units apply various nonlinearities gaussian states 
probability density function variables xn nonlinear gaussian belief network ny ny ai xi ai xj ai set indices parents unit normal density function standard variance gaussian noise unit fj nonlinear function unit example units may step function making binary sigmoidal units cumulative gaussian activation function units may rectification function making real valued units encourage sparse representations 
define wi represents constant bias unit 
generalize variational method developed jaakkola 
networks binary units show successfully applied performing approximate inference learning nonlinear gaussian belief networks 
variational method applied different types nonlinearity network networks kind described binary linear units come pairs output linear unit gated associated binary unit 
variational expectation maximization surprisingly simple variational technique inference learning 
method variables observed postulate simple parametric variational distribution remaining unobserved variables 
variational distribution separate generative distribution 
numerical optimization method conjugate gradients adjust variational parameters bring close true posterior possible 
cost function measures closeness kullback leibler sense bounds log likelihood input pattern 
choice cost function leads efficient generalized expectation maximization learning algorithm described 
cost function bounds log probability set indices observed variables current input pattern set indices unobserved variables current input pattern ng 
variational bound neal hinton hlog hlog log indicates expectation unobserved variables respect 
easily shown unconstrained maximized setting case bound tight 
gives exact probabilistic inference constrained form gives approximate probabilistic inference 
variational distribution consider product gaussians xi xi hare variational parameters 
adjusting parameters obtain axis aligned gaussian approximation true posterior distribution hidden unit inputs 
variational distribution turns expressed terms mean variance output unit 
mi mean output unit input gaussian noise mean variance mi fi dx vi variance output unit input gaussian noise mean variance vi fi mi dx assume easily computed closely approximated case vi bounded give new lower bound 
see app 
functions case linear units binary units rectified units sigmoidal units 
variational bound simplifies nx ai ai log nx log formula concise introduced dummy variational parameters observed variables xi observed value fix 
unit term curly braces measures mean squared error input unit parents ai xj weighted model noise variance larger noise variances particular mean squared prediction error important 
probabilistic inference variational inference consists fixing xi maximizing respect log 
optimization variances performed log domain log allowed go negative 
conjugate gradient method perform optimization techniques steepest descent possibly covariant method amari 
derivatives respect log hare app 
optimization means variances variational distribution represent inference statistics 
learning bound log probability entire training set equal sum bounds individual training patterns 
variational expectation maximization algorithm onf consists iterating steps see hlog simplifies add subtract ai numerator argument hlog log ih xi ai ai wij mj fj xj cross terms produced square vanish expectation 
step perform variational inference respect sets variational parameters corresponding different input patterns 
step respect model parameters 
notice maintaining sufficient statistics scanning training set step necessary store sets variational parameters 
sufficient statistics described app 
speed current step initialize set variational parameters set step pattern 
turns step performed efficiently see app 
details 
values model variances affect values weights maximize maximize respect weights 
pointed jaakkola binary gaussian belief networks quadratic weights singular value decomposition solve weights exactly 
optimal model variances computed directly 
software set unix programs implement variational learning available athttp www cs utoronto ca frey 
software includes linear units binary units rectified units sigmoidal units 
new types unit added easily providing nonlinear function derivatives 
visual feature extraction approximate maximum likelihood estimation latent variable models learn latent structure perceptually significant hinton 
section consider unsupervised feature extraction tasks task compare representations learned variational method applied types representations learned gibbs sampling applied piece wise linear 
hidden units piece wise linear activation function gibbs sampling efficiently learning described hinton ghahramani 
variational learning contains binary hidden units type described jaakkola saul jordan 
section see variational technique compares learning method continuous hidden units generalization variational method binary continuous units compares variational learning binary networks 
continuous bars problem important problem vision modeling surface edges way consistent physical constraints 
goal simpler bars problem dayan zemel learn supervision detect bars orthogonal orientations model constraint image consists bars orientation 
hinton ghahramani continuous form problem 
training image formed choosing vertical horizontal orientation equal probability 
bar orientation turned probability intensity drawn uniformly 
examples training set images sort shown fig 
area tiny white square indicates pixel intensity 
noisy version data unit variance noise added pixel shown fig 
black square indicates negative pixel value 
data sets iterations variational em train layer binary top layer unit rectified units linear visible bottom layer units 
units hidden layers little effect features extracted learning 
resulting weights projecting middle layer units image shown fig 
fig 

surprisingly clearer bar features extracted noisy data 
weights shown top layer binary unit middle layer units tend top layer unit active orientation inactive 
weights look similar rectified unit top binary unit properly represents discrete choice horizontal vertical 
fig 
fig 
show weights learned variational em network hidden units binary 
individual bars properly extracted noise free noisy training data 
log probability bounds trained binary rectified linear nats nats bounds trained binary binary linear nats nats 
trained layer rectified hidden units gibbs sampling 
method learning rate chosen 
sweeps gibbs sampling performed pattern presentation parameters adjusted line 
weights obtained passes data set shown fig 
fig 

features noise free data clear ones extracted variational method 
features noisy data cleaned weight decay hinton ghahramani 
estimate log probability data case gibbs sampling readily provide straight forward way obtain estimates 
experiments variational em gibbs sampling method took roughly time 
line version variational em may faster 
learning variational method gibbs sampling noise free noisy bar patterns 
training examples 
weights learned variational method rectified units 
weights learned variational method binary units 
weights learned gibbs sampling rectified units 
continuous stereo disparity problem vision problem latent variables nonlinearly related input estimation depth stereo pair sensory images 
simplified version problem becker hinton goal learn visual input consists randomly positioned dots dimensional surface placed depths 
experiments blurred dots gaussian functions randomly positioned uniformly continuous interval brightness dot magnitude gaussian drawn uniformly 
left shift right shift applied equal probability obtain second activity pattern 
sensory images containing real values obtained dividing interval pixels assigning pixel net activity pixel 
twelve examples training set pairs images obtained manner shown fig 
images positioned relative shift evident 
noisy version data unit variance gaussian noise added sensor shown fig 

stereo disparity problem difficult bars problem overlap underlying features 
see imagine multi disparity problem sensory images dimensional sensors 
expect depth inference easier case evidence possible directions shift 
imagine stacking sensory images top resulting square image contain blurred diagonal bars oriented right left 
extracting disparity data roughly equivalent extracting bar orientation data previous section 
noisy noise free data sets iterations variational em train layer binary top layer unit rectified middle layer units linear visible bottom layer units 
resulting weights projecting middle layer units sets pixels shown fig 
fig 

cases algorithm extracted features spatially local represent possible depths 
fig 
fig 
show weights learned variational em network hidden units binary 
bounds trained binary rectified linear nats nats bounds trained binary binary linear nats nats 
gibbs sampling method rectified hidden units learning parameters described previous section produced weight patterns shown fig 
fig 

noisy data features extracted gibbs sampling appear slightly cleaner extracted variational em 
learning variational method gibbs sampling noise free noisy stereo disparity patterns 
training examples 
weights learned variational method rectified units 
weights learned variational method binary units 
weights learned gibbs sampling rectified units 
handwriting recognition variational inference learning real valued pattern classification training class data 
ci event pattern comes class posterior class probabilities bayes rule ci cj cj prior probability pattern comes class likelihood class approximated value variational bound obtained generalized step 
section report performances completely automated learning procedures problem recognizing grey level images handwritten digits cedar cdrom database zip codes hull 
delve evaluation platform rasmussen obtain fair comparisons including levels statistical significance 
fig 
shows performances obtained different sizes training set methods nearest neighbors neighborhood class data determined leave cross validation mixture diagonal axis aligned gaussians number gaussians class determined validation set factor analysis number factors class determined validation set hidden layer rectified hidden units linear visible units number hidden units determined validation set 
methods training set split class data set aside validation 
models different complexities number gaussians number hidden units trained remaining data em generalized em convergence 
model gave highest validation set log probability log probability bound trained convergence data corresponding class 
prevent degenerate fitting pixels intensities happen single valued training set variances visible units allowed fall methods 
method classify test pattern 
obtain robust estimates relative performances methods problem handwritten digit recognition trained tested method multiple times disjoint training set test set pairs 
original data set images partitioned set images training set images testing 
training set sizes patterns disjoint training set test set pairs extracted partitions test set images 
estimated error rate knn mdg fa dig class estimated error rates grey level handwritten digit recognition different sizes training set images methods nearest neighbor mixture diagonal axis aligned gaussians factor analysis rectified gaussian belief networks hidden layers 
size training set error rates different methods order 
numbers boxes values percent paired test null hypothesis corresponding methods identical performance dot indicates value 
training set size patterns disjoint training set test set pairs extracted test set images 
fig 
horizontal bar gives estimate expected error rate particular method particular training set size 
methods ordered left right training set size follows nearest neighbors mixture gaussians factor analysis hidden layer hidden layer 
vertical bar gives estimate error standard deviation corresponding estimate expected error rate 
integers boxes lying beneath axis values percent paired test compares performances corresponding methods 
select method list lower left hand corner scan left right 
see number means method performed better method selected statistical significance 
low value indicates difference misclassification rates significant 
precisely value estimate probability obtaining difference performance equal greater observed difference assume methods perform equally null hypothesis 
largest training set size hidden layer performs better nearest neighbors level mixture gaussians level factor analysis level hidden layer level 
performance hidden layer fairly indistinguishable performance factor analysis 
average half hidden units factor analysis indicating provides compact representation input 
interesting compact representation emerged despite fact posterior distribution hidden units approximated axis aligned gaussian distribution factor analysis exact full covariance posterior distribution hidden units 
results visual feature extraction show variational technique extract perceptually significant continuous nonlinear latent structure 
contrast networks continuous hidden variables networks binary hidden variables extract spatially local features data 
similarly linear methods factor analysis independent component analysis fail extract spatially local features zoubin ghahramani personal communication 
results show variational method viable alternative gibbs sampling stochastic neural networks rectified hidden variables 
advantages variational method gibbs sampling include absence learning rate ability compute log probability bound efficiently 
particularly useful pattern classifiers train network class data classify novel pattern picking network gives highest estimate log probability frey 
results show handwritten digit regime training set size perform better nearest neighbors mixture gaussians linear factor analysis method 
variational method may powerful making distribution xi mixture gaussians making entire distribution mixture product form distributions jaakkola jordan grouping small numbers hidden variables full covariance gaussians fit variational inference 
considered types continuous nonlinear unit binary rectified sigmoidal 
variational method easily extended types units units hinton sallans ghahramani long output mean function output variance function computed 
perform gradient step gradients functions respect arguments needed 
focussed attention implementations variational algorithm suited biology believe modifications 
inference algorithm uses bound derivatives computed simple differences passed locally network 
partial step line learning case derivative bound just current input pattern followed 
derivative followed applying delta type rule locally computed differences 
acknowledgments peter dayan zoubin ghahramani tommi jaakkola helpful discussions 
appreciate useful feedback provided michael jordan anonymous reviewer 
gibbs sampling experiments performed software developed zoubin ghahramani seehttp www cs utoronto ca zoubin 
research funded arnold beckman foundation natural science engineering research council canada information technology research center ontario 
step show compute derivatives respect variational parameters 
current set variational parameters including ones fixed current input pattern compute unit current values mean output mi mi output variance vi vi mean net input ni ai bound log probability input pattern computed nx log ni ai nx log perform gradient optimization step derivatives bound respect log computed follows log nj vj log mj cj ij cj wij ni mj log cj vj wij ni cj ij cj set indices children unit app 
gives expressions derivatives nonlinear functions binary units rectified units sigmoidal units 
step produces set variational parameters corresponding training pattern initialize step 
step log probability bound model variances influence optimal weights 
step maximize total bound respect weights 
bound quadratic weights singular value decomposition solve exactly 
fact weights associated input variable decoupled weights network 
value wij affect optimal value wkl consequently solving optimal weights matter solving linear systems system dimensionality equal number parents unit solved gives weights incoming connections unit 
consider input means output means input variances output variances mean net inputs computed training pattern step 
necessary store sets training patterns 
compute sufficient statistics ajk dj bj cij ej accumulated scanning training set step 
sufficient statistics computed solve weights 
system equations weights associated input unit ai cij ai fixed set equations 
singular value decomposition solve set weights 
fact system equations unit dimensionality equal number parents unit including bias 
model variances computed dj ej aj derivatives interesting nonlinear functions appendix give output means variances useful types units including linear units binary units rectified units sigmoidal units 
linear units deal nonlinear units useful include units visible units linear unit output mean variance derivatives binary units obtain stochastic binary unit take log log 
unit output mean variance cumulative gaussian function derivatives rectified units log log rectified unit linear input exceeds outputs 
unit output mean variance derivatives log log sigmoidal units cumulative gaussian squashing function leads closed form expressions output mean derivatives 
closed form expression output variance approximately bounded new function giving upper bound free energy 
output mean variance bound derivatives log log amari 

differential geometrical methods statistics lecture notes statistics vol 

springer new york ny 
amari cichocki yang 
new learning algorithm blind signal separation 
touretzky mozer hasselmo editors advances neural information processing systems 
mit press cambridge ma 
becker hinton 
self organizing neural network discovers surfaces random dot stereograms 
nature 
bell sejnowski 
information maximization approach blind separation blind deconvolution 
neural computation 
comon jutten herault 
blind separation sources 
signal processing 
dayan hinton neal zemel 
helmholtz machine 
neural computation 
dayan zemel 
competition multiple cause models 
neural computation 
everitt 
latent variable models 
chapman hall new york ny 
frey 
continuous sigmoidal belief networks trained slice sampling 
mozer jordan petsche editors advances neural information processing systems 
mit press cambridge ma 
available athttp www cs utoronto ca frey 
frey 
variational inference continuous sigmoidal bayesian networks 
sixth international workshop artificial intelligence statistics 
frey 
graphical models machine learning digital communication 
mit press cambridge ma 
see mit edu book home tcl isbn 
hinton dayan frey neal 
wake sleep algorithm unsupervised neural networks 
science 
hinton ghahramani 
generative models discovering sparse distributed representations 
philosophical transactions royal society london 
hinton sallans ghahramani 
hierarchical community experts 
jordan editor learning inference graphical models 
kluwer academic publishers norwell ma 
hinton sejnowski 
learning relearning boltzmann machines 
rumelhart mcclelland editors parallel distributed processing explorations microstructure cognition volume pages 
mit press cambridge ma 
hull 
database handwritten text recognition research 
ieee transactions pattern analysis machine intelligence 
jaakkola saul jordan 
fast learning bounding likelihoods sigmoid type belief networks 
touretzky mozer hasselmo editors advances neural information processing systems 
mit press 
jaakkola jordan 
approximating posteriors mixture models 
jordan editor learning inference graphical models 
kluwer academic publishers norwell ma 
lewicki sejnowski 
learning nonlinear overcomplete representations efficient coding 
jordan kearns solla editors advances neural information processing systems 
mit press cambridge ma 
mackay 
maximum likelihood covariant algorithms independent component analysis 
unpublished manuscript available wol ra phy cam ac uk mackay 
neal 
connectionist learning belief networks 
artificial intelligence 
neal hinton 
new view em algorithm justifies incremental variants 
unpublished manuscript available internet ftp ftp cs utoronto ca pub radford em ps olshausen field 
emergence simple cell receptive field properties learning sparse code natural images 
nature 
pearl 
probabilistic reasoning intelligent systems 
morgan kaufmann san mateo ca 
rasmussen neal hinton van camp revow ghahramani tibshirani 
delve manual 
university toronto toronto canada 
www cs utoronto ca delve 
rubin thayer 
em algorithms ml factor analysis 
psychometrika 
saul jaakkola jordan 
mean field theory sigmoid belief networks 
journal artificial intelligence research 

