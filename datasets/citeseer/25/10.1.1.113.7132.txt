journal machine learning research submitted published matching words pictures barnard cs arizona edu computer science department university arizona tucson az usa duygulu duygulu edu tr department computer engineering middle east technical university ankara turkey david forsyth daf cs berkeley edu computer science division university california berkeley ca usa de freitas cs ubc ca department computer science university british columbia vancouver canada david blei blei cs berkeley edu computer science division university california berkeley ca usa michael jordan jordan cs berkeley edu computer science division department statistics university california berkeley ca usa editors kandola thomas hofmann tomaso poggio john shawe taylor new approach modeling multi modal data sets focusing specific case segmented images associated text 
learning joint distribution image regions words applications 
consider detail predicting words associated images auto annotation corresponding particular image regions region naming 
auto annotation help organize access large collections images 
region naming model object recognition process translating image regions words translate language 
learning relationships image regions semantic correlates words interesting example multi modal data mining particularly typically hard apply data mining techniques collections images 
develop number models joint distribution image regions words including explicitly learn correspondence regions words 
study multi modal correspondence extensions hofmann hierarchical clustering aspect model translation model adapted statistical machine translation brown multi modal extension mixture latent dirichlet allocation mom lda 
models assessed large collection annotated images real barnard duygulu david forsyth de freitas david blei michael jordan 
barnard duygulu forsyth de freitas blei jordan scenes 
study depth difficult problem measuring performance 
annotation task look prediction performance held data 
alternative measures oriented different types task 
measuring performance correspondence methods harder determine word placed right region image 
annotation performance proxy measure accurate measurement requires hand labeled data occur smaller scale 
show results annotation proxy manually labeled data 

remarkable fact text images separately ambiguous jointly tend probably writers text descriptions images tend leave visually obvious color flowers mention properties difficult infer vision species flower say 
wide variety data sets consist large numbers annotated images 
examples include corel data set museum image collections example www thinker org fam thinker html web archive www archive org collections news photographs web come captions 
typically annotations refer content annotated image specifically comprehensively 
example corel annotations describe specific image content museum collections annotated specific material artist date acquisition contain material 
describe series models link images text various ways 
practical applications large collections images widespread users able browse search collections 
broad range computer vision methods search collections images 
typically images matched features computed entire image image regions 
literature broad review reviews forsyth forsyth ponce 
exception systems identify faces schneiderman kanade naked people fleck pedestrians oren cars schneiderman kanade matching usually directed object semantics 
user studies show large disparity user needs technology supplies enser enser 
hair raising reading example request stock photo library pretty girl doing active setting beach wearing exercise clothes relaxed tee shirt 
feature girl look active happy healthy posed set nice natural looking user studies include studied practice manually operated newspaper photo archive study practice finnish newspaper digital photo archive 
studied requests received national library medicine archive 
literature authors break semantics images requested different ways perspective important points users request images object kinds princess identities princess wales matching words pictures users request images depict things visible picture concepts evoked visible picture queries image histograms texture appearance vanishingly uncommon text associated images extremely useful practice example newspaper index largely captions 
practical applications methods link text images imperfectly automated image annotation numerous organizations manage collections images internal 
typical workflow described studied image archive finnish newspaper 
receive pictures annotate words useful keys retrieving pictures journalists search collection keywords 
annotation difficult uncertain attractive procedure annotated images automatically 
predicting words high posterior probability image 
examples automated annotation appear barnard 
barnard forsyth 
browsing support museums release parts collections web attract visitors giving sense see visited 
typically users know collection wish search don prefer browse frost 
means attractive organize collection way sense visitors supported browsing 
collecting images looked similar similarly annotated start 
fitting probability model appropriate structure yields quite useful clusters described barnard 

auto illustrate commercial image collections supply attractive service casual user searching collection typically difficult expensive 
tool automatically suggest images illustrate blocks text expose value collection making possible casual users get reasonable results cheaply 
auto illustration possible obtain images high probability text barnard barnard forsyth 
annotation correspondence recognition currently words search pictures productive sequence terms jpg jpeg query google 
variety ways words pictures simultaneously 
straightforward search simple conjunction keywords image region features facility provided blobworld carson 
webseer swain uses similar ideas query images web indexes results automatically estimated image features 
include image photograph sketch notably output face finder 
going cascia integrate 
see enser enser enser various image archives roughly procedure 
barnard duygulu forsyth de freitas blei jordan text histogram data indexing la cascia 
experimented image features part query refinement process chen 
srihari text information disambiguate image features particularly face finding applications srihari srihari srihari 
aware general probability models link text images 
model offer usual benefits probability models boolean queries doesn need know exactly right search terms get useful results offer 
predict text images 
ways 
firstly attempt predict annotations entire images information 
refer task annotation 
secondly attempt associate particular words particular image substructures infer correspondence 
data sets contain correspondence information probably difficult create data sets 
normally collection images collection associated words 
seen form classification problem having labeled examples labeled bags examples image positive contains tiger stuff negative doesn maron maron multiple instance learning train classifiers identify particular keywords image data bags 
attempt sort correspondences image structures words directly build classifiers word separately 
satoh kanade cooccurrence models automatically associating faces names video 
closest predicting words regions mori 
occurrence statistics collected words image areas defined fixed grid 
correspondence peculiar feature object recognition 
current theories object recognition reason terms geometric correspondence pose consistency terms template matching classifiers search establish presence suggestive relations templates 
detailed review strategies appears forsyth ponce 
little address object recognition broad scale 
example known recognize thousands different objects data sets practically available 
little said easy hard recognize particular set features 
reasonable hope questions discussed sees object recognition process uses huge data set learn put image structures words correspondence 
explores variety latent variable models auto illustration annotation correspondence 
step represent image information section describes representation adopted 
section describe series models link words image data explicit encoding correspondence words regions 
models appropriate auto illustration annotation 
analogy learning correspondence model associate words image regions learning lexicon suggests possible build process uses little supervisory input 
effect builds model unsupervised methods marks model output 
standard process machine translation literature guide melamed thesis see jurafsky martin manning sch tze 
section describe model uses vector quantized representation image regions yield problem exactly analogous lexicon learning 
simplest model learns correspondence explicitly 
disadvantage representation image regions obtained independent text annotation 
ignores potentially important data matching words pictures image differences important 
sophisticated correspondence models text data simultaneously clustering representations image regions section 
part role compare numerous models meaning different variants type model 
keep track allocated acronym appears bold time 
annotation performance fairly straightforward evaluate automatically large data sets correspondence performance difficult evaluate 
large data sets giving correspondence don exist obliged evaluate correspondence manually means evaluation done large scale 
section describes methods evaluation section gives extensive comparison methods 

input representation preprocessing image segmented normalized cuts shi malik 
segmenter occasional tendency produce small typically unstable regions 
represent largest regions image computing region set features 
features represent roughly major visual properties size represented portion image covered region position represented coordinates region center mass normalized image dimensions color redundantly represented average standard deviation region 
texture represented average variance filter responses 
difference gaussian filters different sigmas oriented filters aligned degree increments 
see shi malik additional details approach texture 
shape represented ratio area perimeter squared moment inertia center mass ratio region area convex hull 
refer region features blob 
claim image features adopted canonical 
chosen computable image region independent recognition hypothesis 
expect better worse behavior available different sets image features 
remains interesting open question construct feature sets offer performance particular vision task depend emerging object hypothesis interesting efficient way 

annotation models classes models joint distribution text blobs show applied annotate images 
multi modal hierarchical aspect models model multi modal extension hofmann hierarchical model text hofmann hofmann puzicha :10.1.1.34.97
model combines aspect model soft clustering barnard duygulu forsyth de freitas blei jordan multi modal extension hierarchical model text 
model 
shown images occurring text generated nodes arranged tree structure 
nodes generate image regions gaussian distribution words multinomial distribution 
cluster associated path leaf root 
nodes close root shared clusters nodes closer leaves shared clusters 
means properly fitted model nodes closer root tend emit items words regions shared large number data elements nodes closer leaves emit items specific small numbers data elements 
potentially vertical structure aspects horizontal structure clusters help model distributions interest 
implementation supports tree topologies including degenerate linear topology branches cluster topologies vertical structure modeling clustering 
experiments consider binary tree linear case comparable number nodes 
extent image cluster generated nodes path 
clusters consideration document modeled sum clusters weighted probability document cluster 
process generating set observations associated document described nw indexes clusters indexes words document indexes image regions document indexes levels 
set observations document set words document set blobs document exponents nw nw nb nb matching words pictures introduced normalize differing numbers words blobs image 
nw denotes number words document nw denotes maximum number words document 
normalization works fine corel data variance nw applies blobs 
choice essentially image word sun comparable words sun clouds duplicating word sun image 
word emission probabilities frequency tables blob emission probabilities gaussian distribution diagonal covariance features regions 
stabilize training translate scale region feature data zero mean unit variance 
model parameters converted original data space training complete 
limit variance gaussian distribution training data space 
similarly word frequency forced small value greater zero vocabulary size goal simply avoid need certain computations log space 
training document specific prior nodes path leaf root vertical weights 
refer model results independent 
barnard 
barnard forsyth experiment allowing cluster dependent level structure 
replaced model 
notice models true generative models joint probability distribution image items described terms specific documents training set 
model powerful search applications 
prediction difficult documents training set 
marginalize training data blei 

alternative estimate mixing weights cluster specific average computed training 
strategy appears suggesting set vertical nodes model document significant mixing weights 
observation led alternative model described barnard 
generative 
model call model gives nw 
model fitting 
models fit expectation maximization algorithm dempster 

update equations similar hofmann puzicha course probability expressions include parts word region occurrences 
model estimate vertical mixing weights document hofmann puzicha 
model estimate vertical mixing weights document cluster 
model update equations vertical mixing weights simpler just need compute cluster dependent average estimating quantities document 
significant memory saving number documents large 
image word prediction 
predict words images assume new document set observed blobs wish compute word vocabulary barnard duygulu forsyth de freitas blei jordan 
case models drop document index vertical weights normally interested applying documents outside training set 
note model replaced 
vertical weights cluster specific average mixing weights labeled ave vert results estimate labeled doc vert 
previous estimated model expensive give better results 
marginalizing training data worked ave vert small data sets expensive compute data scale interest report results 
mixture multi modal latent dirichlet allocation latent dirichlet allocation lda blei generative probabilistic model independent collections data collection modeled randomly generated mixture latent factors 
example text modeling collection words document lda provides model independently generated documents corpus 
generative model lda readily module mixture model 
furthermore lda extendable multi modal data 
particular mixture multi modal lda model mom lda assumes image corresponding words generated process 
choose mixture components multinomial 

conditioned mixture component choose mixture factors dir 

words choose factors zn multinomial 
choose words wn wn zn conditional probability wn mixture component latent factor 

blobs choose factor sm multinomial 
choose blob bm bm sm multivariate gaussian distribution diagonal covariance conditioned factor sm mixture component depicted graphical model 
parameters mom lda dimensional multinomial parameter 
matrix dimensional dirichlet parameter conditioned mixture component 
matching words pictures mixture multi modal latent dirichlet allocation model 
outer plate represents repetition images 
image blobs words 
parameters model depicted simplicity 
matrix cz distribution words conditioned mixture component hidden factor 
matrix matrix cs cs parameters ddimensional multivariate gaussian distribution blobs conditioned mixture component hidden factor 
maximum likelihood estimates dirichlet word multinomials gaussian parameters obtained em algorithm variational step 
image mom lda compute approximate posterior mixture components mixture component approximate posterior dirichlet factors 
parameters perform image word prediction finding corresponding distribution words 
denote approximate posterior mixture components denote corresponding approximate posterior dirichlet 
distribution words image collection blobs 
integral easily computed expectation zth component dir cz cy mom lda similar model derives predictive abilities higher level mixture component underlying lda useful joint model accurately predict words images 
due implicit assumption words blobs exchangeable generated order 
issue described blei jordan authors derive lda model annotated data partial exchangeability 
images generated words subsequently generated images 
resulting model predict words images resorting higher level multinomial mixture components 
barnard duygulu forsyth de freitas blei jordan 
simple correspondence models natural want build models predict words specific image regions image 
simple ways 
simplest vector quantize representations image regions directly exploit analogy statistical lexicon learning 
fact hierarchical aspect models mom lda yield correspondence information 
discrete data translation machine translation lexicon links discrete objects words language discrete objects words language 
come lexicon aligned bitext consists small blocks text languages known correspond meaning 
traditional example hansard canadian parliament speaker remarks french english correspond meaning 
assuming unknown correspondence words coming joint probability distribution linking words languages missing data problem brown 
straightforward create analogous image data 
means vector quantize set features representing image region 
region gets single label blob token 
aligned bitext consisting blobs words image 
construct joint probability table linking word tokens model word opposed instance blob tokens 
current keywords associated image 
data set provide explicit correspondences missing data problem easily dealt application em see duygulu details 
label approach discrete translation 
correspondence hierarchical clustering model hierarchical clustering models model relationships specific image regions words explicitly 
encode correspondence extent cooccurrence advantage having topics collect nodes 
example word tiger occurs orange region items generated shared node far fewer nodes observations 
simple word prediction method single blob 
consider effect regions say cluster membership replacing cluster prior 
label strategies region cluster results 
notice quite replacing set blobs single blob interest insist word region come node come cluster possibly different nodes associated cluster 
get correspondence models differently trained 
matching words pictures 
integrating correspondence hierarchical clustering methods described wholly satisfactory learning correspondence 
vector quantizing image regions independent words ignoring potentially useful training data 
expect methods compelled reveal information trained represent 
relationship clustering correspondence 
example translating english french word sun translated surrounding words computer related word sun remain unchanged brand computer 
similarly gray patch pavement city scene jungle scene 
suggests building explicit correspondence information existing hierarchical clustering models 
building correspondence models involves strengthening relationship words image regions 
linking word emission region emission probabilities mixture weights approach image regions modeled independent model words emitted conditioned regions 
implement strategy having vertical mixture weights regions carries words 
node contributes little region emission image node contribute little word emission 
correspondence implicit calculated true correspondence method 
formally consider words regions handled asymmetrically 
stipulate nw 
notice chosen compute distribution inherited words cluster cluster basis 
denote model dependent 
analogy independent case consider cluster dependent level distributions get drop dependency training set get 
model annotation estimate doc vert compute word posterior 
labeling regions applying predict words particular blob sense independent models words emitted conditioned blobs 
described wish model annotation simply sum contributions blobs 
interestingly gives equations word prediction equivalent cluster linear topology 
barnard duygulu forsyth de freitas blei jordan paired word region emission nodes second method relationship regions words 
assume observed words regions emitted pairs 
refer model correspondence 
models similarly modified get models 
equation evaluates likelihood assuming proposed correspondence 
interested training data correspondence provided need estimate correspondence part training process 
experimented methods doing discussed 
regardless additional step added step model fitting process assignment estimation expectation indicator variables 
methods computing correspondence assume probability word segment correspond estimated probability emitted node 
denote word region correspond case 
choice proposed correspondence shared cluster hypotheses 
doing intuitive sense significantly reduces number correspondence estimates required 
note training longer pure gradient descent log likelihood decrease small amount training process iterates 
true gradient descent need marginalize possible correspondences impractical 
approximate sum obtained maximal close maximal match approximation common literature statistical learning lexicons see example melamed 
experimented possible strategies estimating matches 
report results graph matching jonker 
algorithm gives polynomial time method assign edges bipartite graph vertex connected vertex sum costs associated possible edge minimized 
costs negative log probabilities likelihood maximized 
regardless matching strategy need deal differing numbers words regions 
implementation ensure regions words repeating region collection needed 
ensure sufficient words repeating word collection words regions 
correspondence models null fertility refusal predict correspondence comes variety annoying difficulties 
primary issue choice correspondence model 
map regions words usually impossible numbers different 
described require regions linked words option deciding region corresponds word 
forces models image regions corresponding particular words cope large pool outliers 
problem principle handled appending special matching words pictures word null text data item special image region null image regions data item 
traditional solution machine translation literature tendency single words languages generate word property referred fertility modeled explicitly framework brown melamed 
limited experience models easy fit data sets tendency fit model word generated null image regions image region generates null word 
clearly matter resolved prior model deletion words image regions respectively 
complication probability annotation absent independent annotation annotators mention tigers mention people simple strategy offers benefits directly modeling null words refuse predict annotation annotation highest probability region low probability discourages predictions regions identity moot 
crude doesn mitigate effect outliers fitting process 

evaluation methods annotation models different kinds data 
try annotate images represented original data set example annotating images arriving archive 
second try annotate images collection represented original training data example performing object recognition 
correspondence models difficulties 
issue predict appropriate words particular region 
typically way obtain accurate answer question look picture 
form manual evaluation difficult satisfactory number images 
strict informative test determine annotation performance correspondence model grounds poor annotation performance implies poor correspondence performance crucially contrapositive necessarily true 
measuring annotation performance measure annotation performance comparing words predicted various models words held data 
data sets including image annotations typically omit obviously appropriate words 
purpose compare methods significant problem model cope set missing annotations 
performance comparisons carried automatically substantial scale 
express prediction performance relative predictions obtained empirical word frequency training set 
matching performance empirical density required demonstrate nontrivial learning 
doing substantially better corel data difficult 
annotators typically provide common words example sky water people fewer common words example tiger 
means annotating images say sky water people quite successful strategy 
performance empirical word frequency reduced empirical density flatter 
data set increment performance empirical density sensible indicator 
look word prediction held data rank models measures 
barnard duygulu forsyth de freitas blei jordan measuring quality word posterior distribution models predictive nature estimate kullback leibler kl divergence computed predictive distribution target distribution 
unfortunately target distribution known simply assume actual words predicted uniformly words predicted 
definition error contribution document measure model kl model kl log vocabulary 
smooth add minimum empirical word distribution renormalize 
compute combined measure group images simply average quantity set 
relate conditional log likelihood normally computed held data 
words image consideration observed declare log model kl log observed constant logq 
observed averaged images held set essentially held log likelihood conditioned image regions weighted image contributes roughly regardless number words 
mentioned express performance relative empirical word distribution test data arrange larger values correspond better performance 
specifically report data empirical kl model kl negative model worse prior positive better 
adjustment benefits 
immediately clear doing number positive second reduces variance values computed different sets difficulty set partly reflected empirical kl models predict words 
better fitting model predict better set words need concrete measurement goodness omitted words 
difficulty needs loss function traditional zero loss highly misleading 
conceivable applications certain errors cat tiger offensive car vegetable 
number classes predict large size vocabulary normalize correct incorrect classifications 
specifically compute model ns vocabulary size number actual words image number words predicted correctly number words predicted incorrectly 
score gives value predicting matching words pictures predicting predicting exactly actual word set false positives false negatives 
score predicting exactly complement actual word set 
kl case report difference error empirical word distribution larger values correspond better performance model ns empirical ns number words predicted determined algorithm case case basis 
benefit measure simply counting number correct words fixed number guesses reward estimate words predict 
word prediction scores reported predicting words exceed certain probability threshold 
clear value threshold maximizes performance comparison method training data word frequency value methods word prediction value computed training data reported results 
report results simpler related word prediction measure model pr best words 
keywords sky water sun allow models predict words image 
range score clearly 
measuring correspondence performance measuring performance methods predict specific correspondence regions words difficult images checked hand 
limits size pool means measurements may contain significant noise surprisingly difficult establish stick exact policy regions carry say label people 
region method annotation summing word posteriors regions 
furthermore reasonably expect method predict annotations accurately predict correspondence 
means annotation measures offer plausible proxy 
annotation proxy report results image region word prediction methods 
image methods compute annotations natural way 
second strategy methods region methods compute image annotations region methods 
recall annotation models provide correspondence despite explicitly trained identify suffixes region region cluster 
compute region word posteriors image annotation methods word posteriors necessarily sum requirement models region emits word 
enforce requirement normalizing posteriors region summing 
annotation performance correspondence models assessed marginalizing correspondence model testing model annotation model 
means measures error described apply straightforward fashion 
notice test correspondence component model model performs poorly measure unhelpful correspondence model 
barnard duygulu forsyth de freitas blei jordan manual correspondence scoring corroborate measure score correspondence results hand 
method directly looks correspondence require human judgment 
hand labeled region number images appropriate word vocabulary 
insisted region plausible visual connection chosen words 
word ocean coral judged incorrectly ocean transparent 
difficulties include words landscape valley normally apply larger areas regions pattern arguably designated correct appears scored incorrect pattern recognition isn particularly helpful 
regions linked vocabulary term regions omitted consideration computing scores 
producing labeled data set clearly time consuming error prone process able ground truth modest number images images test sets 
hand labeled set able compute measures image annotation case smaller test set 

experiments experiments images cd corel image data set 
cd images relatively specific topic aircraft cd drew samples cd sets divided training standard held sets 
images remaining cd formed difficult novel held set 
predicting words images difficult reasonably expect success quite generic regions sky water noise 
sample process consideration results samples averaged 
controls input data em initialization 
images segmented cuts shi malik 
excluded words occurred times test set yielded vocabularies order words 
modest selection features segment including size position color oriented energy filters simple shape features 
discrete translation model clusters vector quantization 
linear topologies nodes trees binary trees levels nodes 
mom lda method mixture components latent factors 
annotation results looked performance hierarchical clustering methods function number em iterations train models 
important check fitting especially models truly generative 
take preemptive strategy reduce fitting problems 
train models subset data iterations model starting point training subset data 
repeated times obtain initial point training full data 
preliminary experiments indicated generally slight benefit doing 
experiment tempering training suggested hofmann puzicha stochastic versions em celeux 
matching words pictures number training iterations fitting plots performance terms kl divergence error models function number training iterations 
limit plots iterations verified trend established iterations holds training continues hundreds iterations 
general results indicate fitting big problem 
expect performance training set generally improved number iterations exception 
case inference procedures performance training set reached peak decreased 
methods reasonably fast inference necessarily ad hoc generative 
similar behavior possible intuitively observe behavior models 
held sets possible benefit reached iterations training algorithm cases dropped severely case 
expect performance novel set containing images cd represented training set dropped faster standard held set 
evaluating model novel sets test ability learn properties data generalize different images 
simplicity rest results reported iterations mom lda run convergence 
iterations roughly optimal standard held data sub optimal novel held data iterations give better results 
scoring annotations normalized score effect refusal predict studied behavior normalized score measure score comparing predicted words function minimal probability required predict words 
measure predicting words words gives zero 
expected general behavior go zero peak drop zero 
peak training data empirical word distribution set conservative refuse predict level table 
comparison models different scores provide comprehensive annotation results table prediction score pr model pr empirical pr table normalized score ns model ns empirical ns table kl score model kl empirical kl 
models omitted performance similar models 
bit better training data slightly better held data slightly worse novel set 
surprising parameters 
linear topologies give results inference methods case clusters 
ave vert method sense cluster gives poor results methods equivalent case single cluster 
close study reveals results far consistent measures 
broad view hierarchical clustering methods give surprisingly similar results paired reasonable inference strategy note error estimates provided parenthesis 
surprisingly results linear arrangements nodes roughly comparable 
discussed 
prior kl model kl prior kl model kl barnard duygulu forsyth de freitas blei jordan performance vs iterations data sets model inference strategies ave vert doc vert region cluster region number iterations training data held data novel held data performance vs iterations data sets model inference strategies ave vert doc vert region cluster region number iterations training data held data novel held data prior kl model kl prior kl model kl performance vs iterations data sets model inference strategies ave vert doc vert region cluster region number iterations number iterations training data held data novel held data performance vs iterations data sets model inference strategies ave vert doc vert region cluster region training data held data novel held data annotation performance kl divergence predictive density empirical word occurrence density models function number training iterations 
performance relative empirical word distribution vertical axis extent models better empirical distribution bigger better 
results average runs different training test sets 
performance shown different test sets training set held set held set substantially different character training set 
notice performance training set decreases increasing iterations peak 
due ad hoc inference methods introduced efficiently compute required distribution despite fact model truly generative 
plots show task benefit obtained iterations 
normalized score normalized score matching words pictures normalized score vs refuse predict level model training data held data empirical ave vert doc vert region cluster region refuse predict level normalized score vs refuse predict level model training data held data refuse predict level empirical ave vert doc vert region cluster region normalized score normalized score normalized score vs refuse predict level model training data held data empirical ave vert doc vert region cluster region refuse predict level normalized score vs refuse predict level model discrete translation training data held data empirical discrete trans refuse predict level normalized word prediction performance versus refuse predict level model 
single value required table refuse predict level empirical word distribution gives maximum 
results training held sets empirical distribution close curves essentially top 
refuse predict level probability word emission decreases exponentially left right level recorded axis 
increases decreases number words predicted increases performance increases decreases 
methods illustrated perform significantly better prediction training word frequency refuse predict levels 
barnard duygulu forsyth de freitas blei jordan method training data held data novel data linear doc vert binary ave vert binary doc vert binary region cluster binary region binary ave vert binary doc vert binary region cluster binary region linear doc vert binary ave vert binary doc vert binary region cluster binary region binary ave vert binary doc vert binary region cluster binary region linear region binary ave vert binary doc vert binary region cluster binary region discrete translation mom lda table image annotation performance methods developed text 
methods hierarchical clustering models cluster conditional independence increases dependence word emission blobs integrates strict correspondence discrete translation discrete translation method mom lda latent dirichlet allocation mixture components factors 
values increase annotation word list prediction score measure pr computed empirical word distribution 
average words predict value result held data corresponds predicting opposed empirical distribution 
predicting words images novel cd difficult methods consistently little better empirical distribution task 
errors shown parentheses estimated variance word prediction process different test sets samples set averaged result set 
general range results support key notion word prediction facilitated honoring compositional nature images associated text 
words generally matching words pictures method training data held data novel data linear doc vert binary ave vert binary doc vert binary region cluster binary region binary ave vert binary doc vert binary region cluster binary region linear doc vert binary ave vert binary doc vert binary region cluster binary region binary ave vert binary doc vert binary region cluster binary region linear region binary ave vert binary doc vert binary region cluster binary region discrete translation mom lda table image annotation performance methods developed text 
values increase normalized classification score measure ns computed empirical word distribution 
refuse predict level corresponds roughly predicting percent words increase typical held data results corresponds reaching level guesses compared roughly empirical distribution vocabulary size depending test set 
see table additional details 
associated pieces images entire image 
building representations pieces incorporate word region features cleaner approach word prediction attempting represent images 
set objects smaller set common arrangements objects 
look effect topology important observation methods image clustering reliant having images close training data 
discussed included clustering models exploit context 
results indicate forcing images clusters strong condition doing 
barnard duygulu forsyth de freitas blei jordan method training data held data novel data linear doc vert binary ave vert binary doc vert binary region cluster binary region binary ave vert binary doc vert binary region cluster binary region linear doc vert binary ave vert binary doc vert binary region cluster binary region binary ave vert binary doc vert binary region cluster binary region linear region binary ave vert binary doc vert binary region cluster binary region discrete translation mom lda table image annotation performance methods developed text measured reduction kl divergence computed empirical distribution roughly 
numbers largely comparison intuitive absolute scale readily available 
see table additional details 
clustering improved training set results slightly degraded performance held images cd novel cd clustering significantly reduces performance 
quite understandable light discussion previous paragraph degree degradation especially case held images cd training unexpected 
performance discrete translation worse similar non discrete model linear doc 
consistent belief better simultaneously learn models blobs linkage words 
performance different data sets indicates may overfitting problems 
turn may due fact errors due early quantization artifacts training data 
results mom lda model worth noting 
performance training data worse models annotation results slightly poorer matching words pictures models test data resembles training data competitive models test data novel 
mom lda shows strong resistance overfitting 
mom lda model far fewer clusters models mom lda mixture components lda factors versus mixture components aspect model factors study larger scale mom lda models warranted 
exploiting context important remove ambiguities results indicate need explore subtle approaches doing 
course clustering warranted applications browsing search characterization training set important 
recognition needed deal ambiguity 
think recognition identifying sky image jet having exposed sky jungle scenes clear trying put jet image inappropriate cluster yield poor results 
correspondence results shows region annotations sample images 
result labeled region maximal probability word model 
table provide quantitative correspondence results computed images held sets 
results error measures provided 
region word prediction reasonable predict words region 
process closely studied simple keyword prediction error model pr empirical pr results suggest methods developed learn correspondence fact better task relative performance annotation proxy 
example pr measure linear region scores annotation proxy significantly exceeded performance linear linear doc vert scores 
correspondence measure comparable 
paired word blob emission approach intended effect improving correspondence performance annotation performance disappointed correspondence performance matched methods significantly 
expect need integrate null properly approach 
possible benefits model longer compelled predict words predict second joint probability table may fitted accurately fitting process protected large number outliers caused forcing region correspond word 
currently correspondence annotation linear region linear doc vert appears best choice measures data sets account 

discussion compared variety methods predicting words pictures 
methods predict words predict correspondence words 
practical applications methods 
furthermore offer intriguing way think object recognition 
great deal remains done 
large variety models fitting methods appear natural 
example model conditional distribution image features word gaussian gaussian word 
fitting model form presents practical difficulties precision barnard duygulu forsyth de freitas blei jordan grass sky people water recall precision fish ocean grass snow sunset precision clouds plants leaves closeup close plane ice field beach cat church city coral fence forest ground head hills horizon market reflection river road sand sea shore street temple town recall closeup plants leaves field beach sand ice ground forest plane recall sky water people specific word precision recall discrete translation model 
word task predict images keyword predict 
word predicted maximal word predicted blobs image 
precision total number correct predictions images divided number predictions duplicate predictions count single prediction 
recall total number correct predictions divided number occurrences keyword 
results average held data sets 
relationship precision recall modulated refuse predict level changes curves 
level increases fewer words predicted recall goes words predicted predicted certainty precision goes 
show results words performance show words poor performance 
scales curves poorly predicted words located bottom left corner 
refuse predict level fixed show performance words scatter plot 
figures show quite modest set words performance rest limited 
matching words pictures examples region annotation pair held data 
rows results 
left image row labels water labels due word common training region features 
images lots correct words image annotation words right region poor correspondence 
specifically car image tires labeled tracks belongs 
horse image horse right place 
bottom right image example complete failure 
barnard duygulu forsyth de freitas blei jordan method pr measure linear region binary region cluster binary region binary region cluster binary region linear region binary region cluster binary region binary region cluster binary region linear region discrete translation table correspondence performance measured sets manually annotated images held set pr measure 
values relative performance empirical distribution 
task pr arguably indicative measure corresponds forcing region emit small number words number alternative labels 
ns measure appropriate refuse predict level calibrated different conditions 
note comparison annotation results linear region linear doc vert give results linear doc vert linear region 
initial experiments suggest worthwhile 
attempt model selection methods form suggest synonymous words corel data set contains train locomotive words effectively synonymous features jet plane bird say 
model selection methods search appropriate feature sets 
note particular reason features need independent identity improved model predict bits word index fixed feature set predict bits features conditioned bits 
corel data set relatively simple annotations nouns selected relatively small vocabulary 
data sets contain free text annotations 
simple matter tag nouns throw away text regard result annotation 
possible example wish natural language processing identify candidate annotations appear refer picture 
performance certainly affected correspondence model currently require region generate word words accounted 
require correspondence words subset regions insert null 
slightly modifies formulation em fitting methods 
currently little information effect supervision expect quite small supervisory input lead significant changes model 
missing correspondence information generate symmetries incomplete data log likelihood 
example tiger grass appear way determine annotating small number images break symmetry cause substantial matching words pictures change model 
reasonable measure performance model associated fitting algorithm quantity supervisory input required achieve particular level performance collection 
large scale evaluation correspondence models genuinely difficult 
problem important 
distant recognition systems manage vocabularies large manual checking labeled images unsatisfactory test 
tell system works 
current strategy investigate methods obtain extrapolated estimates correspondence performance proxies applied test sets carefully chosen properties 
key issue entropy labels hard predict second word word data item test collection annotation performance predict correspondence performance 
acknowledgments project part digital libraries initiative sponsored nsf 
specifically material supported national science foundation 
iis 
barnard de freitas receive funding nserc canada duygulu funded turkey 
david blei funded fellowship microsoft 
grateful jitendra malik doron tal normalized cuts software robert wilensky helpful conversations 
enser 
analysis user need image archives 
journal information science 
barnard duygulu forsyth 
clustering art 
ieee conference computer vision pattern recognition pages ii hawaii 
barnard forsyth 
learning semantics words pictures 
international conference computer vision pages ii 
blei jordan 
modeling annotated data 
technical report csd berkeley cs division 
blei ng jordan 
latent dirichlet allocation 
advances neural information processing systems 
brown della pietra della pietra mercer 
mathematics machine translation parameter estimation 
computational linguistics 
carson belongie greenspan malik 
blobworld image segmentation expectation maximization application image querying 
ieee transactions pattern analysis machine intelligence 
celeux diebolt 
stochastic versions em algorithm 
technical report inria march 
barnard duygulu forsyth de freitas blei jordan chen 
multi modal browsing images web documents 
spie document recognition retrieval 
chen bouman dalton 
hierarchical browsing search large image databases 
ieee transactions image processing 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
series methodological 
duygulu de freitas forsyth 
object recognition machine translation learning lexicon fixed image vocabulary 
seventh european conference computer vision pages iv 
enser 
query analysis visual information retrieval context 
journal document text management 
enser 
progress documentation pictorial information retrieval 
journal documentation 
fleck forsyth bregler 
finding naked people 
bernard buxton roberto cipolla editors th european conference computer vision pages ii 
springer 
forsyth 
computer vision tools finding images video sequences 
library trends 
forsyth ponce 
computer vision modern approach 
prentice hall 
frost taylor torres 
browse search patterns digital image database 
information retrieval 
hofmann 
learning representing topic 
hierarchical mixture model word occurrence document databases 
workshop learning text web cmu 
hofmann puzicha 
statistical models occurrence data 
memo massachusetts institute technology 
jonker 
shortest augmenting path algorithm dense sparse linear assignment problems 
computing 
jurafsky martin 
speech language processing natural language processing computational linguistics speech recognition 
prentice hall 

user types queries impact image access systems 
challenges indexing electronic text images 
learned information 
la cascia sethi sclaroff 
combining textual visual cues content image retrieval world wide web 
ieee workshop content access image video libraries 
matching words pictures manning sch tze 
foundations statistical natural language processing 
mit press 
cambridge ma 

user searching challenges indexing practices digital newspaper photo archive 
information retrieval 
maron 
learning ambiguity 
ph dissertation massachusetts institute technology 
maron 
multiple instance learning natural scene classification 
fifteenth international conference machine learning 
melamed 
empirical methods exploiting parallel texts 
mit press cambridge massachusetts 
mori takahashi oka 
image word transformation dividing vector quantizing images words 
international workshop multimedia intelligent storage retrieval management conjunction acm multimedia conference orlando florida 
oren papageorgiou sinha osuna 
pedestrian detection wavelet templates 
computer vision pattern recognition pages 

view picture theoretical image analysis empirical user studies indexing retrieval 
library research 
shin ichi satoh kanade 
name association face name video 
proceedings ieee computer vision pattern recognition cvpr pages june 
schneiderman kanade 
statistical approach object recognition applied faces cars 
ieee conference computer vision pattern recognition page 
ieee 
shi malik 
normalized cuts image segmentation 
ieee transactions pattern analysis machine intelligence 
srihari 
extracting visual information text captions label human faces newspaper photographs 
ph thesis suny buffalo 
srihari 
visual semantics extracting visual information text accompanying pictures 
aaai seattle wa 
srihari chopra venkataraman govindaraju 
collateral text image interpretation 
arpa image understanding workshop monterey ca 
swain frankel athitsos 
webseer image search engine world wide web 
technical report tr computer science department university chicago 

