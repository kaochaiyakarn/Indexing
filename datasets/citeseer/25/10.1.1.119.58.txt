new techniques extracting features protein sequences jason wang ma dennis shasha cathy wu propose new techniques extract features protein sequences 
features inputs bayesian neural network bnn apply bnn classifying protein sequences obtained pir protein database maintained national biomedical research foundation 
evaluate performance proposed approach compare protein built sequence alignment machine learning methods 
experimental results show high precision proposed classi er complementarity bioinformatics tools studied 
keywords bioinformatics biological data mining neural networks feature extraction sequences 
supported part nsf iri iri iis iis pharmaceuticals 
computer information science new jersey institute technology university heights newark nj jason cis njit edu 
contact author phone fax 
science informatics unit department genomics pharmaceuticals summit nj ma com 
institute mathematical sciences new york university mercer street new york ny shasha cs nyu edu 
bioinformatics program protein information resource national biomedical research foundation pir georgetown university medical center reservoir road nw washington dc georgetown edu 
result human genome project related orts dna rna protein data accumulate accelerating rate 
mining biological data extract useful knowledge essential genome processing 
subject gained signi cant attention bioinformatics community 
case study extracting features protein sequences bayesian neural network classify sequences 
problem studied stated formally follows 
unlabeled protein sequence known superfamily want determine belongs 
refer target class set sequences non target class 
general superfamily group proteins share similarity structure function 
unlabeled sequence detected belong infer structure function process important aspects bioinformatics computational biology 
example drug discovery sequence obtained disease determined belongs superfamily may try combination existing drugs treat disease approaches available protein sequence classi cation 
approach compare unlabeled sequence sequences target class sequences non target class alignment tool blast 
assigns class containing sequence best matching second method protein sequence classi cation hidden markov models hmms 
hmm method sam employs machine learning algorithm uses probabilistic graphical models describe time series sequence data 
originally applied speech recognition applied modeling analyzing protein superfamilies 
generalization position speci scoring matrix include insertion deletion states 
hmm built super family 
scores unlabeled sequence respect hmm super family 
score signi cant cut value regarded super family 
approach protein sequence classi cation iteratively build model hidden markov models sam position speci weight matrix psi blast 
unlabeled sequence seed sequence iteratively searched super family hmm position speci weight matrix 
study compare approach blast sam iterative method sam 
hidden markov models choose sam outperforms protein sequence classi cation 
iterative methods choose sam psi blast sensitive homolog detection 
choose blast point comparison represents di erent computing paradigm performing classi cation simply alignment 
interesting nding compared classi cation methods complement combining yields higher precision individually experimental results show 
consistent previous report gave preliminary analysis complementarity approach blast sam 
feature extraction protein data dimensional point view protein sequence contains characters letter amino acid alphabet fa yg 
important issue applying neural networks protein sequence clas si cation encode protein sequences represent protein sequences input neural networks 
sequences may best representation 
input representations easier neural networks recognize un regularities 
input representations crucial success neural network learning 
propose new encoding techniques entail extraction high level fea tures protein sequences 
best high level features relevant 
relevant mean high mutual information features output neural networks mutual information measures average reduction uncertainty output neural networks values features 
way look features capture global similarity local similarity protein sequences 
global similarity refers similarity multiple sequences local similarity refers motifs fre quently occurring substrings sequences 
sections elaborate nd global local similarity ofthe protein sequences 
section presents classi ca tion algorithm employs bayesian neural network originated mackay 
section evaluates performance proposed classi er 
section compares approach protein classi ers 
section concludes 
global similarity protein sequences calculate global similarity sequences adopt gram known tuple method described 
gram encoding method extracts various patterns consecutive amino acid residues protein sequence counts num ber occurrences extracted residue pairs 
instance protein sequence gram amino acid encoding method gives result pv occurs vk indicating vk occurs twice kt nv 
adopt letter exchange group fe represent protein sequence fh kg fd qg fcg fs gg fm vg ff wg 
exchange groups represent conservative replacements evolution 
exchange groups ectively equivalence classes amino acids derived pam 
example protein sequence total number possible patterns gram encoding number di erent letters protein alphabet 
pam blosum amino acid substitution matrices derived blocks database 
represented 
gram exchange group encoding sequence fore 
protein sequence apply gram amino acid encoding gram exchange group encoding sequence 
possible grams total 
grams chosen neural network input features require weight parameters training data 
di cult train neural network phenomenon called curse dimensionality 
di erent methods proposed solve problem careful feature selection scaling input dimensionality 
propose select relevant features grams employing distance measure calculate relevance feature 
feature value 
denote class conditional density functions feature class represents target class class non target class 
denote distance function de ned jp distance measure prefers feature feature 
intuitively means easier distinguish class observing feature feature appears class seldom class vice versa 
feature gram 
denote occurrence number feature sequence denote total number grams len represent length len 
de ne feature value gram respect sequence len example suppose 
value feature vk respect 
protein sequence may short random pairings large ect term distance address feature selection classi cation 
result 
approximate equation respectively mean value standard deviation feature positive negative respectively training dataset 
intuitively equation larger numerator smaller denominator larger distance easier separate class class vice versa 
mean value standard deviation feature set sequences de ned vu nx nx value feature respect sequence total number sequences ng top features grams largest values 
intuitively features occur frequently positive training dataset frequently negative training dataset 
protein sequence training unlabeled test sequence examine features calculate values de ned equation feature values input feature values bayesian neural network sequence compensate possible loss information due ignoring grams linear correlation coe cient lcc values grams respect protein sequence mean value grams positive training dataset calculated input feature value speci cally lcc experimental results show choosing ng yield reasonably performance provided su cient training sequences 
experimented di erent combinations grams top ng features bottom ng features smallest values 
results worse top ng features 
de ned lcc xj xj mean value jth gram positive training dataset feature value jth gram respect de ned equation 
local similarity protein sequences contrast grams occur sequence referred global similarities local similarity protein sequences refers fre quently occurring motifs motif composed substrings occurring local regions sequence 
fs kg positive training dataset 
previously developed sequence mining tool nd regular expression motifs forms motif length len approximately matches mut mutations occur sequences mutation mismatch insertion deletion letter residue len mut occur user speci ed parameters 
segments sequence substrings consecutive letters variable length don care vldc symbol 
length motif number non vldc letters motif 
matching motif sequence vldc symbol motif instantiated arbitrary number residues cost 
example sequence rst instantiated mn second instantiated kwk 
number mutations motif sequence representing cost inserting motif 
tool heuristic works small sample sequences set sequences storing generalized su tree gst 
gst constructed asymptotically time space total length sequences sample 
heuristic traverses gst generate candidate regular expression motifs compares candidate motifs sequences calculate occurrence numbers 
candidate motif sequence determine mut mutations mut jsj time jmj log jsj nd motifs satisfying user speci ed parameter values time mut total length sequences sample average length sequences total number sequences tool practically faster due optimization heuristics implemented speeding traversal gst 
number motifs returned enormous 
useful develop measure evaluate signi cance motifs 
propose minimum description length mdl principle calculate signi cance motif 
mdl principle states best model motif case minimizes sum length bits description model length bits description data positive training sequences case encoded model 
evaluating signi cance motifs adopt information theory fundamental form measure signi cance di erent motifs 
theory takes account probability amino acid motif sequence calculating description length motif sequence 
speci cally shannon showed length bits transmit symbol optimal coding log probability symbol occurs 
probability distribution alphabet fb ng calculate description length string kl alphabet lx log ki case alphabet protein alphabet containing amino acids 
probability distribution orp case calculated examining occurrence frequencies amino acids positive training dataset straightforward way describe encode sequences referred scheme encode sequence sequence separated delimiter 
dlen denote description length sequence dlen aj log aj number occurrences example suppose sequence dlen log log log log log log log log log dlen denote description length fs kg 
description length dlen kx dlen dlen description length delimiter dlen insigni cant ignore dlen kx dlen method encode sequences referred scheme encode regular expression motif say encode sequences speci cally sequence approximately match encode encode scheme 
example illustrate scheme 
consider example encode indicates mutation allowed matching delimiter signal motif 
denote alphabet fa amino acids 
denote probability distribution alphabet 
approximated reciprocal average length motifs 
denotes number motif motif form motif form 
actual number sequences encoded scheme dependent motif 
motif study sequences encoded motif scheme 
calculate description length motif substituting prob ability distribution probability distribution equation 
speci cally jk dlen denote description length bits motif dlen log log instance consider kx log ji dlen log log log log log log log sequences approximately matched motif encoded aid motif 
example consider 
matches mutation representing cost inserting third position rst vldc symbol instantiated mn second vldc symbol instantiated kwk 
rewrite mn ss kwk ss denotes concatenation strings 
encode 
delimiter indicates mutation occurs matching indicates mutation insertion adds letter third position general mutation operations involved positions observed approximate string matching algorithms 
description length encoded denoted dlen calculated easily equation 
suppose sequences ph positive training dataset approximately match motif weight signi cance denoted de ned hx dlen pi dlen intuitively sequences approximately matching mj bits hx dlen pi encode encode sequences larger weight 
nd set regular expression motifs forms positive training dataset motifs satisfy user speci ed parameter values len mut occur 
choose top motifs largest weights 
denote set motifs 
suppose protein sequence training sequence unlabeled test sequence approximately match mut mutations motifs motifs local similarity ls value denoted ls de ned max ls mi ls value input feature value bayesian neural network sequence note max operator maximize discrimination 
gen eral positive sequences large ls values high probabilities small ls values low probabilities 
hand negative sequences small ls values high probabilities large ls values low probabilities 

essentially proposed scheme count amino acids sequence motif 
scheme complete sense di erent sequences may description length number amino acids 
second may ways align motif sequence description length encoded sequence may unique 
consequence weight motif de ned equation may unique case proposed heuristic randomly picks 
approaches nding motifs dif ferent forms calculating signi cance values see 
motifs relatively little ect pir sequence classi cation combination proposed techniques yields high precision experimental results show 
bayesian neural network classi er adopt bayesian neural network bnn originated mackay classify protein sequences 
input features including grams lcc software available wol ra phy cam ac uk pub mackay readme html 
gram gram lcc value ls value bayesian neural network architecture 
feature described section ls feature described section 
protein sequence represented vector real numbers 
bnn hidden layer containing multiple hidden units 
output layer output unit logistic activation function bnn fully connected adjacent layers 
fig 
illustrates example bnn model hidden units 
fx mg denote training dataset including positive negative training sequences 
input feature vector including input feature values binary target value output unit 
equals represents protein sequence target class 
denote input feature vector protein sequence training sequence test sequence 
architecture weights bnn output value uniquely determined input vector logistic activation function output unit output value jx probability represents protein sequence target class likelihood function data model calculated djw tm exp djw djw isthe cross entropy error function djw nx log djw objective function non bayesian neural network training process minimized 
process assumes possible weights equally 
weight decay avoid tting training data poor generalization pq test data adding term objective function weight decay parameter hyperparameter sum squares weights neural network number weights 
objective function minimized penalize neural network weights large magnitudes 
penalizes complex model favors simple model 
precise way specify appropriate value tuned ine 
contrast bayesian neural network hyperparameter interpreted parameter model optimized online bayesian learning process 
adopt bayesian training neural networks described calculate maximize evidence dj 
training process employs iterative procedure iteration involves levels inference 
fig 
illustrates training process bnn 
classifying unlabeled test sequence represented input feature vector output bnn jx probability belongs target class 
probability greater decision boundary assigned target class assigned non target class 
general unlabeled test sequence amino acids takes time calculate gram feature values time calculate lcc feature value time calculate ls feature value total length motifs chosen constant time calculating probability jx 
time complexity approach classifying unlabeled test sequence 
input training data 
choose number hidden units model 
initialize hyperparameter weight parameters 
hyperparameter find probable weights training process level inference 
reestimate hyperparameter second level inference 
converge calculate evidence model third level inference 
try model output chosen model parameters 
training process bayesian neural network 
performance bnn classi er data carried series experiments evaluate performance proposed bnn classi er pentium ii pc running linux operating system 
data experiments obtained international protein sequence database release protein information resource pir maintained national biomedical research foundation pir georgetown university medical center 
database accessible pir georgetown edu currently sequences 
table summarizes data experiments 
positive datasets considered globin kinase ras super families respectively pir protein database 
negative dataset contained protein sequences taken pir protein database lengths ranging residues residues negative sequences belong dataset globin kinase related transforming protein ras transforming protein negative sequences table data experiments 
number sequences minimal length sequences maximal length sequences 
positive superfamilies 
positive negative sequences randomly divided training sequences test sequences size training dataset equaled size test dataset multiplied integer training data tested bnn models di erent numbers hidden units 
hidden units evidence obtained largest cf 
fig 
xed number hidden models hidden units require training time achieving roughly performance 
table summarizes parameters base values experiments 
measure evaluate performance bnn classi er precision pr de ned pr otal number test sequences classi ed correctly otal total number test sequences 
general precision comprehensive measure sense considers true positives false positives true negatives false negatives unclassi ed sequences nd best parameter values proposed bnn classi er 
false positive non target member sequence misclassi ed target member sequence 
false negative refers sequence target class globin superfamily misclassi ed non target member 
results globin superfamily results superfamilies note bnn classi er yield unclassi ed sequence 
contrast classi ers blast sam sam compare yield unclassi ed sequences experimental results show 
similar 
parameter meaning value number grams bnn number motifs bnn len length motifs mut mutation number occur occurrence frequency motifs size ratio table parameters base values proposed bnn classi er 
results rst experiment considered grams evaluated ect performance proposed bnn classi er 
fig 
graphs pr function seen performance improves initially increases 
reason grams precisely represent protein sequences 
large training data insu cient performance degrades 
general larger input features bnn classi er larger training dataset bnn requires 
case positive training sequences negative training sequences 
data yield reasonably performance 
figuring big parameter requires tuning 
worked theory 
second experiment considered motifs studied ect performance bnn classi er 
motifs lengths ranging residues residues 
fig 
graphs function seen motifs uses better performance achieves 
require time matching test sequence motifs 
experimented parameter values len mut occur 
results didn change parameters changed 
time spent test sequence motifs linearly proportional number motifs uses 
pr pr ng impact bnn classi er 
nl ect bnn classi er 
fig 
compares ects various types features introduced 
isolate ects features began type features combinations 
seen features generated global similarities yield better results generated local similarities 
happens pir superfamilies categorized global similarities sequences 
note best performance achieved features 
pr lcc gram ls lcc gram gram lcc ls comparison various types features bnn classi er 
tool underlying techniques bnn bayesian neural networks blast similarity search pairwise alignment sam hidden markov models sam iterative hidden markov models table bioinformatics tools studied 
comparison protein classi ers purpose section compare proposed bnn classi er blast classi er built sequence alignment sam sam classi ers built hidden markov models 
table summarizes studied tools 
parameter values bnn classi er shown table 
bnn classi er grams regular expression motifs 
blast version number 
default values parameters blast 
tool aligned unlabeled test sequence positive training sequences target class globin superfamily negative training sequences non target class shown table tool 
score threshold expectation value blast undetermined unclassi ed 
assigned class containing sequence best matching sam version number 
tool employed program build hmm model positive training sequences 
calculated log odds scores training sequences program 
log odds scores negative real numbers scores positive training sequences generally smaller scores negative train ing sequences 
largest score positive training sequences smallest score negative training sequences recorded 
high ng low min fs ng 
calculated log odds scores unlabeled test sequences program 
score unlabeled test sequence smaller low classi ed member target class globin sequence 
score larger high classi ed member non target class 
score low high unclassi ed undetermined 
sam version number 
tool built hmm target model unlabeled test sequence scored training sequences hmm target model 
lowest score training sequences higher expectation value value hmm target model test sequence undetermined unclassi ed 
assigned test sequence class containing training sequence having lowest value 
target model built iterations 
rst iteration sam blast compare test sequence sequences non redundant protein database maintained national center biotechnology information ncbi chose set close homologs build initial hmm 
blast search test sequence non redundant protein log odds scores opposed values tool value training sequence calculated respect training dataset value test sequence calculated respect test dataset 
kinds value directly comparable 
value hmm target model study 
experimented values results worse 
database loose cut value get pool potential homologs 
hmm obtained previous iteration compared pool potential homologs looser cut value previous iteration nd weaker homologs 
weaker homologs included build new hmm iteration 
process repeated times 
comparing relative performance tools measures addition precision pr de ned previous section specificity sensitivity unclassified unclassified speci city nfp sensitivity npo unclassi ed po unclassi ed un ng fp number false positives fn number false negatives number undetermined positive test sequences un number undetermined negative test sequences ng total number negative test sequences po total number positive test sequences 
note contrast pr specificity sensitivity consider unclassi ed sequences 
add unclassified unclassified measures performance evaluation 
rst experiment studied ect threshold value blast classi er 
fig 
shows impact values performance blast 
seen blast performs 
smaller values speci city blast approach false positives number unclassi ed sequences enormous 
xed threshold value subsequent experiments 
tables summarize results classi cation times seconds studied tools referred basic classi ers superfamilies table percentage precision specificity sensitivity unclassified impact values blast 
bnn blast sam sam combiner precision speci city sensitivity unclassi ed unclassi ed cpu time table comparison studied classi ers globin superfamily 
bnn blast sam sam combiner precision speci city sensitivity unclassi ed unclassi ed cpu time table comparison studied classi ers kinase superfamily 
bnn blast sam sam combiner precision speci city sensitivity unclassi ed unclassi ed cpu time table comparison studied classi ers ras superfamily 
bnn blast sam sam combiner precision speci city sensitivity unclassi ed unclassi ed cpu time table comparison studied classi ers superfamily 
classi cation results percentage test sequences classi ers agreed correct classi ers agreed wrong classi ers disagreed correct classi ers disagreed wrong table complementarity studied tools bnn blast sam sam 
percentages table add 

addition basic classi ers developed ensemble classi ers called combiner employs weighted voter works follows 
basic classi er gives undetermined verdict classi er regarded abstaining verdict counted 
result combiner result majority remaining classi ers 
tie remaining classi ers result combiner result bnn classi er 
see comparison blast sam sam bnn classi er faster yielding fewer unclassi ed sequences 
combiner achieves highest precision sam requires time classi ers 
table shows complementarity studied tools bnn blast sam sam 
see classi ers agree classi cation result result correct probability 
examining sam cpu time classifying kinase sequences shown table higher times spent classifying sequences shown tables 
reason kinase sequence homologs non redundant protein database maintained ncbi 
sam gets homologs kinase sequence homologs sequences time build hmm target model kinase sequence 
bayesian neural network approach classifying protein sequences 
main contributions include development new algorithms extracting global similarity local similarity sequences input features bayesian neural network development new measures evaluating signi cance grams frequently occurring motifs classifying sequences experimental studies compare performance proposed bnn classi er classi ers blast sam sam di erent superfamilies sequences pir protein database 
main ndings include 
studied classi ers bnn blast sam sam complement combining yields better results classi ers individually 
training phase done bnn classi er may take time 
classi er trained runs signi cantly faster blast sam sequence classi cation 
research directions include comparison motifs generated di erent tools combination learning tools neural networks hidden markov models applied sequence classi cation pir prosite protein databases generalization classi ers combination graph matching algorithms analyze sequence structure relationship protein dna sequences 
acknowledgments anonymous reviewers thoughtful comments constructive sug 
dr david mackay sharing bayesian neural network software dr richard hughey providing sam package 
shavlik blattner 
neural network input representations produce accurate consensus sequences dna fragment assemblies 

altschul madden scha er zhang zhang miller lipman 
gapped blast psi blast new generation protein database search programs 
nucleic acids research 
bailey grundy 
classifying proteins family product correlated values 
proceedings third annual international conference computational molecular biology pp 

bairoch 
prosite dictionary sites patterns proteins current status 
nucleic acids research 
barker huang srini xiao yeh er wu 
protein information resource pir 
nucleic acids research 
ben 
distance measures information measures error bounds feature evaluation 
krishnaiah kanal eds classi cation pat tern recognition reduction dimensionality handbook statistics volume north holland publishing north holland pp 

ukkonen 
discovering patterns subfamilies 
proceedings fourth international conference intelligent systems molecular biology pp 

califano 
splash structural pattern localization analysis sequential histograms 
bioinformatics 
url www research ibm com splash 
jones 
feature selection genetic sequence classi cation 
bioinformatics 
craven shavlik 
machine learning approaches gene recognition 
ieee expert 
dash liu 
feature selection classi cation 
intelligent data analysis 
electronic journal url www east elsevier com ida schwartz 
model evolutionary change proteins 
atlas protein sequence structure suppl 

eddy 
pro le hidden markov models 
bioinformatics 
eddy mitchison durbin 
maximum discrimination hidden markov models sequence consensus 
journal computational biology 
grundy bailey 
family pairwise search motif models 
bioinformatics 
hart califano 
systematic automated discovery patterns prosite families 
proceedings fourth annual international conference computational molecular biology 
automated assembly protein blocks database searching 
nucleic acids research 
amino acid substitution matrices protein blocks 
proceedings national academy sciences usa 
hirsh noordewier 
background knowledge improve inductive learn ing dna sequences 
proceedings tenth annual conference arti cial intelligence applications pp 

hughey krogh 
hidden markov models sequence analysis extension analysis basic method 
computer applications biosciences 
hui 
color set size problem applications string matching 
apostolico crochemore galil manber eds combinatorial pattern matching lecture notes computer science volume springer verlag pp 

hughey 
weighting hidden markov models maximum discrim 
bioinformatics 
karplus barrett hughey 
hidden markov models detecting remote protein homologies 
bioinformatics 
krogh brown mian haussler 
hidden markov models computational biology applications protein modeling 
journal molecular biology 
mackay 
evidence framework applied classi cation networks 
neural computation 
park karplus barrett hughey haussler hubbard chothia 
sequence comparisons multiple sequences detect times remote pairwise methods 
journal molecular biology 
rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
rissanen 
modeling shortest data description 
automatica 
shannon 
mathematical theory communication 
bell system technical journal 

method protein sequence classi cation frequency analysis application search functional sites domain localization 
computer applications biosciences 
eddy durbin 
comprehensive database protein domain families seed alignments 
proteins 
wang chirn marr shapiro shasha zhang 
combinatorial pattern discovery scienti data preliminary results 
proceedings acm sigmod international conference management data pp 

wang ma shasha wu 
application neural networks biological data mining case study protein sequence classi cation 
proceedings sixth acm sigkdd international conference knowledge discovery data mining pp 

wang marr shasha shapiro chirn lee 
complementary classi cation approaches protein sequences 
protein engineering 
wang shapiro shasha wang yin 
new techniques dna sequence classi cation 
journal computational biology 
wang shapiro shasha eds 
pattern discovery biomolecular data tools techniques applications 
oxford university press new york 
wu berry fung 
neural networks full scale protein sequence classi cation sequence encoding singular value decomposition 
machine learning 
wu 
neural networks genome informatics 
elsevier science 
wu chang 
protein classi cation arti cial neural system 
protein science 
wu manber 
fast text searching allowing errors 
communications acm 

