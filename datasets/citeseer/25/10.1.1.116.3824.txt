ieee transactions information theory vol 
october quantization robert gray fellow ieee david neuhoff fellow ieee history theory practice quantization dates similar ideas appeared literature long ago 
fundamental role quantization modulation analog digital conversion recognized early development modulation systems especially oliver pierce shannon 
bennett published high resolution analysis quantization exact analysis quantization noise gaussian processes shannon published beginnings rate distortion theory provide theory quantization analog digital conversion data compression 
papers years ago trace history quantization origins decade survey fundamentals theory popular promising techniques quantization 
index terms high resolution theory rate distortion theory source coding quantization 
dictionary random house definition quantization division quantity discrete number small parts assumed integral multiples common quantity 
oldest example quantization rounding analyzed sheppard application estimating densities histograms 
real number rounded nearest integer say resulting quantization error generally define quantizer consisting set intervals cells index set ordinarily collection consecutive integers set reproduction values points levels quantizer defined expressed concisely indicator function 
definition sense assume partition real line 
cells disjoint exhaustive 
general definition reduces rounding manuscript received january revised june 
supported part national science foundation ncr mip 
gray department electrical engineering stanford university stanford ca usa 
neuhoff electrical engineering computer science department university michigan ann arbor mi usa 
publisher item identifier 
invited ieee fig 

nonuniform quantizer example integers generally cells take form called thresholds form increasing sequence 
width cell length function called quantization rule 
simple quantizer reproduction levels depicted fig 
collection intervals bordered thresholds levels interval 
quantizer said uniform roundoff case levels equispaced say apart thresholds midway adjacent levels 
infinite number levels allowed cells width equal separation levels 
finite number levels allowed cells width outermost cells semi infinite 
example uniform quantizer cell width levels fig 

uniform quantizer cell width region input space quantizer level called granular region simply support outside quantizer error unbounded called overload saturation region 
generally support granular region nonuniform quantizer region input space relatively small distance level overload region complement granular region 
concrete small defined half width largest cell finite width 
quality quantizer measured goodness resulting reproduction comparison original 
way accomplishing define distortion measure quantifies cost distortion resulting reproducing consider average distortion measure quality system smaller average distortion meaning higher quality 
common distortion measure squared error shall encounter 
practice average sample average quantizer applied sequence real data theory views data sharing common probability density function pdf corresponding generic random variable average distortion expectation ieee transactions information theory vol 
october fig 

uniform quantizer 
distortion measured squared error mean squared error mse special case shall focus 
desirable average distortion small possible fact negligible average distortion achievable letting cells numerous tiny 
cost terms number bits required describe quantizer output decoder arbitrarily reliable reproduction possible digital storage communication media finite capacity 
simple method quantifying cost communications storage assume quantizer codes input binary representation channel codeword quantizer index specifying reproduction level reconstruction 
possible levels binary representations binary codewords equal length temporary assumption binary vectors need larger integer integer components bits 
definition rate code bits input sample quantizer fixed length binary codewords said fixed rate quantizer levels assumed binary codewords equal length 
restriction weakened 
note logarithms base explicitly specified 
summary goal quantization encode data source characterized probability density function bits possible low rate way reproduction may recovered bits high quality possible small average distortion 
clearly tradeoff primary performance measures average distortion simply distortion abbreviate rate 
tradeoff may quantified operational distortion rate function defined distortion scalar quantizer rate 
alternatively define operational rate distortion function rate fixed rate scalar quantizer distortion inverse far described scalar quantization fixed rate coding technique data sample independently encoded fixed number bits decoded reproduction 
shall see alternative quantization techniques permit better tradeoff distortion rate distortion rate vice versa 
purpose review development techniques theory design performance 
example type technique interested operational distortion rate function defined distortion quantizer type rate 
interested best possible performance quantizers 
preview occasional benchmark comparison informally define class quantizers class quantizers operate scalars vectors scalars vector quantizers fixed variable rate sense binary codeword describing quantizer output length depending input memoryless memory example different sets reproduction levels depending past 
addition restrict attention quantizers change time 
confronted input past history quantizer produce output regardless time 
occasionally term lossy source code simply code alternatives quantizer 
rate defined average number bits source symbol required describe corresponding reproduction symbol 
informally generalize operational distortion rate function providing best performance scalar quantizers defined infimum average distortion quantization techniques rate 
viewed best possible performance quantizers constraints dimension structure complexity 
section ii begins historical tour development theory practice quantization past years period encompassing entire literature subject 
complementary approaches dominate history state theory key papers appeared volume bell systems technical journal 
approach best known readers transactions rate distortion theory source coding fidelity criterion shannon information theoretic approach source coding suggested providing foundations information theory fully developed source coding 
second approach high resolution asymptotic quantization theory origins pcm oliver pierce shannon quantization error spectra bennett 
history state art quantization derives seminal works 
contrast asymptotic theories small important collection results asymptotic nature 
oldest results exact analyses gray neuhoff quantization special cases analysis spectra quantization error uniformly quantized sinusoidal signals bennett derivation power spectral density uniformly quantized gaussian random process 
important results basic optimality conditions iterative descent algorithms quantizer design developed lloyd popularized max 
goal section introduce historical context key ideas quantization originated classical works evolved past years remaining sections survey selectively detail variety results illustrate historical development state field 
section iii basic background material needed remainder including general definition quantizer basic forms optimality criteria descent algorithms 
material introduced introduced section ii 
completeness section iii largely self contained 
section iv reviews development quantization theories compares approaches 
section describes number specific quantization techniques 
review large subject quantization space discuss mention subject 
effort select important doubt missed important due bias misunderstanding ignorance 
apologize reader researchers may neglected 
ii 
history history quantization takes parallel paths causes problems clustering topics 
follow roughly chronological order order paths best 
specifically track design analysis practical quantization techniques paths fixed rate scalar quantization leads directly discussion section predictive transform coding adds linear processing scalar quantization order exploit source redundancy variable rate quantization uses shannon lossless source coding techniques reduce rate 
lossless codes originally called noiseless 
follow early forward looking vector quantization including seminal shannon zador vector quantization appears paradigm analyzing fundamental limits quantizer performance practical coding technique 
surprising amount vector quantization theory developed outside conventional communications signal processing literature 
subsequently review briefly developments mid mid mainly concern emergence vector quantization practical technique 
sketch briefly developments mid 
stated presume squared error distortion measure 
fixed rate scalar quantization pcm origins quantization theory quantization source coding fidelity criterion origins pulse code modulation pcm technique patented reeves years wrote historical perspective appraisal pcm 
predictions surprisingly accurate eventual ubiquity digital speech video 
technique successfully implemented hardware black reported principles implementation bell labs goodall 
pcm subsequently analyzed detail popularized oliver pierce shannon 
pcm digital technique conveying analog information signal principally telephone speech analog channel typically wire atmosphere 
words modulation technique alternative am fm various types pulse modulation 
consists main components sampler including prefilter quantizer fixed rate binary encoder binary pulse modulator 
sampler converts continuous time waveform sequence samples sampling frequency 
sampler ordinarily preceded lowpass filter cutoff frequency filter ideal shannon nyquist shannon whittaker sampling theorem ensures lowpass filtered signal principle perfectly recovered appropriately filtering samples 
quantization samples renders approximation mse recovered waveform approximately sum mse quantizer high frequency power removed lowpass filter 
binary pulse modulator typically uses bits produced quantizer determine amplitude frequency phase sinusoidal carrier waveform 
evolutionary development modulation techniques performance pulse amplitude modulation presence noise improved samples quantized nearest set levels modulating carrier equally spaced levels typical 
introduces quantization error deciding levels transmitted presence noise done reliability mse substantially reduced 
reducing number quantization levels easier decide level transmitted came cost considerable increase mse quantizer 
solution fix value giving acceptably small quantizer mse binary encode levels receiver binary decisions great reliability 
resulting system pcm best resistance noise modulations time 
digital era emerged recognized sampling quantizing encoding part pcm performs analog digital conversion uses extending communication analog channels 
communications field recognized task analog digital conversion source coding factored binary modulation separate task 
ieee transactions information theory vol 
october pcm generally considered just consist sampling quantizing encoding longer includes binary pulse modulation 
quantization information theory literature generally considered form data compression modulation conversion originally viewed data expansion accurately bandwidth expansion 
example speech waveform occupying roughly khz nyquist rate khz 
sampling nyquist rate quantizing bits sample modulating resulting binary pulses amplitude frequency shift keying yield signal occupying roughly khz fold increase bandwidth 
mathematically constitutes compression sense continuous waveform requiring infinite number bits reduced finite number bits practical purposes pcm interpreted compression scheme 
early contribution theory quantization applied rice characteristic function transform method provide exact expressions quantization error moments resulting uniform quantization certain specific inputs including constants sinusoids 
complicated sums bessel functions resembled early analyses nonlinear modulation technique fm left little hope general closed form solutions interesting signals 
general contributions quantization theory came papers oliver pierce shannon bennett 
part analysis pcm communications developed oft quoted result large rate resolution uniform quantizer cell width yields average distortion quantizer levels rate source input range support width natural choice approximation yields familiar form signal noise ratio snr showing large rate snr uniform quantization increases db bit increase rate referred db bit rule formula considered high resolution formula formula applies situation cells average distortion small rate large reproduction produced quantizer quite accurate 
result appeared years earlier albeit somewhat disguised form sheppard treatment 
bennett developed fundamental results quantization theory 
generalized high resolution approximation uniform quantization provide approximation systems preceded uniform quantizer monotonic smooth nonlinearity called compressor say inverse nonlinearity reconstructing signal 
output reproduction input db uniform quantizer 
bennett showed case uniform quantizer integral taken granular range input 
constant assumes maps unit interval bennett pointed nonuniform quantizer implemented result referred bennett integral provides asymptotic approximation quantizer 
useful jump ahead point interpreted lloyd explicitly point constant times quantizer point density function function property region number quantizer levels integrating region gives fraction quantizer reproduction levels region evident normalized prove useful consider unnormalized quantizer point density integrated gives total number levels fraction 
current situation unnormalized density generalize case infinite 
rewriting bennett integral terms point density function yields common form idea quantizer point density function generalize vectors approach sense vector quantizers represented 
bennett demonstrated assumptions high resolution smooth densities quantization error behaved random noise small correlation signal approximately flat white spectrum 
led additive noise model quantizer error properties formula interpreted representing quantizer output sum signal white noise 
model popularized widrow viewpoint avoids fact noise fact dependent signal approximations valid certain conditions 
quantization noise generally perceptually desirable 
motivation randomizing action quantization addition dither signal method introduced roberts means making quantized images look better replacing artifacts resulting deterministic errors random noise 
shall return dithering section seen suitable dithering exact bennett approximations uniform distribution signal independence quantizer noise 
bennett variation rice method derive exact computation spectrum quantizer noise gaussian process gray neuhoff quantization uniformly quantized providing exact computations quantization error spectra 
developed high resolution formula distortion fixed rate scalar quantizer approximations similar bennett bennett 
variational techniques minimize formula formula operational distortion rate function fixed rate scalar quantization large values called formula 
part derivation demonstrated optimal quantizer resulted roughly equal contributions total average distortion quantization cell result called partial distortion theorem rederive bennett integral effect derived optimal compressor function equivalently optimal quantizer point density substituting point density bennett integral fact yields 
example input density gaussian variance fact large rates decreases implies signal noise ratio increases db bit rule 
virtually high resolution formulas obey rule 
constant adds vary source quantizer considered 
formula derived directly bennett integral variational methods lloyd smith apparent knowledge earlier roe 
derived variational methods application lder inequality bennett integral additional benefit demonstrating claimed minimum global 
known time turns gaussian source independent identically distributed samples operational function times larger distortion achievable quantization technique rate 
shannon known 
equivalently induced signal noise ratio db best possible fixed distortion rate bits sample larger achievable best quantizers 
smith re examined pcm 
things gave somewhat cleaner derivations indicated derived earlier 
bennett integral optimal compressor function formula 
lloyd important study quantization main contributions 
necessary sufficient conditions fixed rate quantizer locally optimal conditions satisfied implied small perturbations levels thresholds increase distortion 
optimal quantizer smallest distortion necessarily satisfy conditions called optimality conditions necessary conditions 
simply stated lloyd optimality conditions fixed rate quantizer optimal quantizer partition optimal set reproduction levels set reproduction levels optimal partition 
lloyd derived conditions straightforwardly principles recourse variational concepts derivatives 
case mean squared error condition implies minimum distance nearest neighbor quantization rule choosing closest available reproduction level source sample quantized second condition implies reproduction level corresponding cell conditional expectation centroid source value lies specified cell minimum error estimate source sample 
sources multiple locally optimal quantizers globally optimal 
second optimality conditions lloyd developed iterative descent algorithm designing quantizers source distribution initial collection reproduction levels optimize partition levels minimum distortion mapping gives partition real line intervals optimize set levels partition replacing old levels centroids partition cells 
alternation continued convergence local global optimum 
lloyd referred design algorithm method developed method ii optimality properties 
choose initial smallest reproduction level 
determines cell threshold right turn implies larger reproduction level 
approach alternately produces level threshold 
level chosen initial level reduce distortion algorithm continues 
lloyd provided design examples uniform gaussian laplacian random variables showed results consistent high resolution approximations 
method ii initially gain popularity rediscovered max method easily extends vector quantizers types quantizers structural constraints 
third motivated apparently unaware bennett smith lloyd bennett integral formula concept point density function 
critically important step subsequent generalizations bennett integral vector quantizers 
showed directly situations global optimum local optimum quantizers satisfy optimality conditions asymptotically optimal point density 
ieee transactions information theory vol 
october unfortunately lloyd published archival journal time 
institute mathematical statistics ims meeting appeared print bell laboratories technical memorandum 
result results widely known engineering literature years independently rediscovered 
independent variational derivations lloyd simple derivations 
essential extensions vector quantizers development quantizer optimization procedures 
knowledge mention lloyd ieee literature came fleischer derivation sufficient condition log source density concave order optimal quantizer locally optimal quantizer consequently lloyd method yields globally optimal quantizer 
condition satisfied common densities gaussian laplacian 
zador referred lloyd year earlier ph dissertation discussed 
year bell telephone laboratories technical memorandum goldstein variational methods derive conditions global optimality scalar quantizer terms second order partial derivatives respect quantizer levels thresholds 
provided simple counterintuitive example symmetric density optimal quantizer asymmetric 
added terms representing overload distortion formula bennett integral optimize uniform nonuniform quantizers 
unaware prior bennett optimal compressor characteristic formula 
max published variational proof lloyd optimality properties th power distortion measures rediscovered lloyd method ii numerically investigated design fixed rate quantizers variety input densities 
widrow derived exact formula characteristic function uniformly quantized signal quantizer infinite number levels 
results showed condition characteristic function input signal zero argument greater moments quantized random variable moments signal plus additive signal independent random variable uniformly distributed misinterpreted saying quantized random variable approximated input plus signal independent uniform noise clearly false statement quantizer error deterministic function signal 
bandlimited property characteristic function implies fourier transform theory probability density function infinite support signal transform perfectly bandlimited 
conclude subsection mentioning early appeared mathematical statistical literature hindsight viewed related scalar quantization 
specifically variational techniques consider optimal grouping gaussian data respect average squared error 
developed consider lloyd optimality conditions variational techniques study optimum go go gauge sets acknowledged lloyd 
cox derived similar conditions 
additional early seen relating vector quantization reviewed 
scalar quantization memory recognized early common sources speech images considerable redundancy scalar quantization exploit 
term redundancy commonly early days popular quantization literature 
strictly speaking refers statistical correlation dependence samples sources usually referred memory information theory literature 
current emphasis historical follow traditional language 
disrupting performance scalar quantizers redundancy exploited attain substantially better rate distortion performance 
early approaches combined linear processing scalar quantization preserving simplicity scalar quantization arguments insights improve performance incorporating memory code 
important approaches variety predictive coding transform coding 
shared intuition preprocessing operation intended scalar quantization efficient remove redundancy data 
day common belief data compression equivalent redundancy removal data redundancy compressed 
discussed belief contradicted shannon demonstrated strictly improved performance vector quantizers memoryless sources early toth 
removing redundancy leads improved codes 
predictive quantization appears originate delta modulation patent van commonly cited early cutler patent differential quantization communication signals philips technical report delta modulation 
cutler stated patent object invention improve efficiency communication systems advantage correlation signals systems cited reduction redundancy key reduction quantization noise 
elias provided information theoretic development benefits predictive coding published 
early include 
particular claims bennett style asymptotics highresolution quantization error discussed approximations rigorously derived 
point view squares estimation theory optimally predict data sequence past gray neuhoff quantization fig 

predictive quantizer encoder decoder 
sense minimizing mean squared error resulting error residual innovations sequence uncorrelated minimum possible variance 
permit reconstruction coded system prediction past reconstructed samples true samples 
accomplished placing quantizer inside prediction loop predictor decode signal 
simple predictive quantizer differential modulator dpcm depicted fig 

predictor simply sample quantizer bit system delta modulator 
predictive quantizers considered memory quantization sample depends previous samples feedback loop 
predictive quantizers extensively developed example adaptive versions widely speech video coding number standards 
speech coding form basis itu video coding form basis interframe coding schemes standardized mpeg series 
comprehensive discussions may books survey papers 
decorrelation early motivation predictive quantization common view primary role predictor reduce variance variable scalar quantized 
view stems facts prediction errors source samples quantized quantization error precisely equals scalar quantizer operating prediction errors operational distortion rate function scalar quantization proportional variance precisely scaling random variable quantized factor results scaling density prediction error usually sufficiently similar form source operational distortion rate function smaller original source approximately ratio variance source prediction error quantity called prediction gain 
analyses form usually claim high resolution conditions distribution prediction error approaches error predictions past source samples past 
clear accuracy approximation increases sufficiently rapidly finer resolution ensure difference operational distortion rate functions types prediction errors small relative values decreasing resolution finer 
open question type analysis typically uses bennett formulas asymptotically correct 
results high resolution approximations widely accepted compare experimental results 
assuming give correct answer large rates stationary gaussian source memory distortion optimized dpcm quantizer scalar quantizer factor variance source step prediction error smallest mse prediction sample previous samples 
turns exceeds factor distortion optimal fixed rate scalar quantization exceeds memoryless gaussian source 
appears dpcm job exploiting source memory scalar quantization high resolution assumption 
rigorously shown may apply bennett integral formula directly prediction error analysis feedback quantization systems proved notoriously difficult results limited proofs stability asymptotic stationarity analyses distortion hermite polynomial expansions gaussian processes analyses distortion source wiener process exact solutions nonlinear difference equations describing system descriptions output sequences moments including power spectral densities constant sinusoidal signals finite sums sinusoids rice method results extend quantizers inside feedback loop 
conditions code design resembling lloyd optimality conditions studied feedback quantization conditions optimality conditions lloyd sense necessary conditions quantizer feedback loop yield minimum average distortion subject rate constraint 
return issue consider finite state vector quantizers 
optimality certain causal coding structures somewhat akin predictive feedback quantization 
transform coding second approach exploiting redundancy scalar quantization linear preprocessing 
source samples collected vector say dimension multiplied orthogonal matrix ieee transactions information theory vol 
october fig 

transform code 
orthogonal transform resulting transform coefficients scalar quantized usually different quantizer coefficient 
operation depicted fig 

style code introduced kramer mathews analyzed popularized huang 
kramer mathews simply assumed goal transform decorrelate symbols huang proved decorrelating lead optimal transform code design case gaussian sources high resolution 
transform coding extensively developed coding images video discrete cosine transform dct commonly computational simplicity performance 
dct coding basic approach dominating current image video coding standards including jpeg mpeg 
codes combine uniform scalar quantization transform coefficients efficient lossless coding quantizer indices considered section quantizer 
discussions transform coding images see transform coding widely high fidelity audio coding 
predictive quantizers transform coding approach lent quite bennett high resolution approximations classical analysis huang performance optimized transform codes fixed rate scalar quantizers gaussian sources result demonstrated karhunen lo decorrelating transform optimum application assumptions 
transform karhunen lo transform coefficients uncorrelated independent input vector gaussian 
seminal huang showed high resolution approximation theory provide analytical descriptions optimal performance design algorithms optimizing codes structure 
particular showed high resolution assumptions gaussian sources average distortion best transform code rate optimal scalar quantization factor average variances components source vector covariance matrix 
note reduction distortion larger sources memory correlation covariance matrices sources smaller determinants 
large turns distortion optimized transform coding rate exceeds factor distortion optimal fixed rate scalar quantization exceeds memoryless gaussian source 
dpcm transform coding job exploiting source memory system scalar quantization 
variable rate quantization shannon lossless source coding theory clear assigning equal numbers bits quantization cells wasteful cells unequal probabilities 
number bits produced quantizer average reduced shorter binary codewords assigned higher probability cells 
course means longer codewords need assigned probable cells shannon theory shows general net gain 
leads directly variable rate quantization partition cells codebook levels binary codewords varying lengths assigned cells alternatively levels 
ordinarily set binary codewords chosen satisfy prefix condition member prefix member order insure unique 
precise section may view variable rate quantizer consisting partition codebook lossless binary code assignment binary codewords 
variable rate quantizers rate longer defined logarithm codebook size 
instantaneous rate input number binary symbols binary codeword length binary codeword rate average length binary codewords average taken probability distribution source samples 
operational distortion rate function definition smallest average distortion variable rate quantizers having rate 
weakened constraint expanding allowed set quantizers operational distortion rate function ordinarily smaller fixed rate optimum 
huffman algorithm provides systematic method designing binary codes smallest possible average length set probabilities cells 
codes designed way typically called huffman codes 
unfortunately known expression resulting minimum average length terms probabilities 
shannon lossless source coding theorem implies source quantizer partition find assignment binary codewords prefix set average length uniquely decodable set binary codewords average length gray neuhoff quantization shannon entropy quantizer output probability source sample lies th cell shannon provided simple way attaining performance upper bound quantizer index assign binary codeword length kraft inequality ensures possible simply choosing paths binary tree 
tighter bounds developed 
example gallager shown entropy smaller average length huffman code largest see discussion bounds 
ordinarily smaller shows generally fairly accurate estimate average rate especially high resolution case 
simple formula determining rate huffman code entropy provides useful estimate reasonable simplify variable length quantizer design problem little redefining instantaneous rate quantizer th quantizer level define average rate entropy output 
mentioned underestimates true rate small amount case exceeds 
define operational distortion rate function minimum average distortion variable rate quantizers output entropy quantizer output entropy lower bound actual rate operational distortion rate function may optimistic falls defined average length rate 
quantizer designed provide smallest average distortion subject entropy constraint called entropy constrained scalar quantizer 
variable rate quantization called variable length quantization quantization entropy coding 
critical take pains distinguish quantizers entropy coded quantizers 
usually blur distinction average length entropy measures rate quantizers important particular discussion 
sort blurring measure rate fixed rate quantization 
important note number quantization cells levels play primary role variable rate quantization example levels places source density small little effect distortion rate 
number levels infinite advantage eliminating overload region resulting overload distortion 
potential drawback variable rate quantization necessity dealing variable numbers bits produces 
example bits communicated fixed rate digital channel buffering take buffer overflows underflows account 
drawback potential error propagation bits received decoder error 
basic simple example variable rate quantizer plays fundamental role benchmark comparison uniform scalar quantizer variable length binary lossless code 
possibility applying variable length coding quantization may occurred number people familiar quantization shannon 
earliest papers oliver 
max mind computed entropy nonuniform uniform quantizers designed minimize distortion number levels 
gaussian source results showed variable length coding yield rate reductions bit sample 
high resolution analysis variable rate quantization developed handful papers 
papers widely scattered unpublished situation understood ieee community 
high resolution analysis schutzenberger showed distortion optimized variable rate quantization scalar vector decreases rate just fixed rate quantization 
find multiplicative factors describe nature partitions codebooks best variable rate quantization 
renyi showed uniform scalar quantizer infinitely levels small cell width output entropy approximately differential entropy source variable discovered interesting fact high resolution case mean squared error uniform scalar quantization exceeds distortion achievable quantization scheme whatsoever factor equivalently induced signal noise ratio db best possible fixed distortion rate bit sample larger achievable best quantizers 
gaussian source gains db bit sample best fixed rate scalar quantizer 
interest note compare performance specific quantization scheme unfortunately published journal widely circulated 
unpublished bell telephone laboratories technical memo zador studied variable rate fixed rate quantization 
focus vector quantization described 
point variable rate scalar quantization large rate results showed operational distortion rate function distortion codes rate aware turns formula demonstrating high ieee transactions information theory vol 
october resolution case uniform best type scalar quantizer variable rate coding applied 
papers appeared ieee literature fact transactions variable rate quantization aforementioned 
showed numerical evaluation uniform scalar quantization variable rate coding attains performance db bit sample best possible gaussian source 
second gish pierce demonstrated analytically empirically 
specifically derived generally fact high resolution nonuniform scalar quantizer output entropy unnormalized point density quantizer 
approximations bennett integral rederive show highresolution case uniform scalar quantizers achieve operational distortion rate function variable rate quantization 
comparing called shannon lower bound showed sources db bit sample best possible performance quantization system whatsoever earlier 
results showed performance attainable source distribution just gaussian case checked 
generalized results squared error distortion nondecreasing functions magnitude error 
known proof fact high resolution case entropy successive outputs uniformly scalar quantized stationary source memory generalization vectors show rate large uniform scalar quantization variable length coding successive quantizer outputs block entropy coding achieves performance db bit sample sources memory 
accomplished comparing shannon lower bounds 
important result widely appreciated rediscovered ziv showed similar result holds small rates 
note uniform scalar quantizers quite simple lossless code capable approaching th order entropy quantized source quite complicated 
addition gish pierce observed coding vectors performance improved quantizer cells cube implicitly uniform scalar quantizers noted hexagonal cell superior dimensions originally demonstrated toth newman 
uniform quantization asymptotically best entropy constrained quantization lower rates nonuniform quantization better series papers explored algorithms designing 
wood provided numerical descent algorithm designing scalar quantizer showed predicted gish pierce performance slightly superior uniform scalar quantizer followed lossless code 
dealing vector quantization technique discussed berger described lloyd conditions optimality entropy constrained scalar quantizer squared error distortion 
formulated optimization unconstrained lagrangian minimization developed iterative algorithm design entropy constrained scalar quantizers 
showed gish pierce demonstration approximate optimality uniform scalar quantization variable rate quantization holds approximately rate large holds exactly exponential densities provided levels placed centroids 
netravali introduced fixed point algorithm goal minimizing average distortion scalar quantizer entropy constraint 
approach taken noll 
berger refined approach entropy constrained quantizer design 
variable rate quantization extended dpcm transform coding high resolution analysis shows gains relative fixed rate quantization applied direct scalar quantizing 
note variable rate quantization analysis dpcm suffers flaws fixed rate quantization analysis dpcm 
numerous extensions bennett style asymptotic approximations approximation characterizations properties optimal high resolution quantization fixed variable rate quantization squared error error moments appeared 
excellent summary early contained elias 
close section important practical observation 
current jpeg related standards viewed combination transform coding variable length quantization 
worth pointing standard resembles differs models considered far 
previously stated transform coefficients separately quantized possibly different uniform quantizers bin lengths quantizers determined customizable quantization table 
typically produces quantized transformed image zeros 
lossless variable length code scans image zig zag peano fashion producing sequence zeros indices corresponding nonzero values huffman coded arithmetic coded 
procedure effect coding transform coefficients largest magnitude ones important reconstruction 
early transform coders typically coded say coefficients ignored rest 
essence method adopted standards selectively coded important coefficients having largest magnitude simply gray neuhoff quantization lowest frequency coefficients 
coding step hindsight viewed simple way locating significant coefficients turn described accurately 
implicit significance map early version idea essential wavelet coders 
beginnings vector quantization described previous subsections early produced steady stream advances design analysis practical quantization techniques principally scalar predictive transform quantization quantizer performance improving decades progressed 
hand roughly time parallel series developments concerned fundamental limits quantization practical quantization issues 
speak primarily remarkable shannon important zador important contributors 
dealt called vector quantization vq block multidimensional quantization just scalar quantization components vector say successive source samples quantized simultaneously 
characterized dimensional partition dimensional codebook containing dimensional points reproduction codewords codevectors assignment binary codewords cells partition equivalently codevectors 
immediate advantage vector quantization provides model general quantization scheme operating vectors structural constraints 
clearly includes transform coding special case considered include predictive quantization operating locally vector 
lack structural constraints general model amenable analysis optimization 
early decades vector quantization served primarily paradigm exploring fundamental performance limits evident practical coding technique 
shannon source coding theory classic shannon sketched idea rate source minimum bit rate required reconstruct source degree accuracy measured fidelity criterion mean squared error 
sketch fully developed sources additive measures distortion block source codes called vector quantizers 
shannon showed coding rate distortion achievable vector quantizers kind equal function subsequently called shannon distortion rate function determined statistics source measure distortion 
shannon described solution equivalent problem minimizing rate subject distortion constraint answer function ha subsequently called shannon rate distortion function inverse accordingly theory called rate distortion theory cf 

elaborate shannon theory note immediately extend quantizer notation distortion rate definitions operational distortion rate functions define smallest distortion possible dimensional fixed rate vector quantizer achieves rate 
distortion dimensional vectors defined numerical average distortions respective components 
rate times average number bits describe dimensional source vector 
dimension explicit notation allowing vary omit 
furthermore shannon channel coding lossless source coding theories consider best possible performance codes dimensions assuming data blocked vectors arbitrary size define operational distortion rate function operational rate distortion functions defined similarly 
finite dimension function depend definition rate log reproduction size average binary codeword length quantizer output entropy 
turns affected choice 
definitions rate 
source shannon distortion rate function defined minimum average distortion conditional distributions mutual information emphasize scalar variables 
principal result coding theorem source coding fidelity criterion shannon showed vq dimension rate yield smaller average distortion dimension possibly large exists vq rate greater distortion nearly illustrative example shannon distortion rate function gaussian source variance variance source 
equivalently shannon rate distortion function known represents best possible performance quantization scheme whatsoever formulas previously comparing performance scalar quantizers best quantization schemes 
example comparing sees earlier statement operational distortion rate function scalar quantization times larger notice shows source exponential rate decay distortion rate demonstrated high resolution arguments high rates extends rates 
usually case sources 
shannon approach subsequently generalized sources memory cf 

general ieee transactions information theory vol 
october definitions distortion rate rate distortion functions resemble operational distortion rate functions infima th order functions 
example th order distortion rate function stationary random process defined infimum average distortion conditional probability distributions average mutual information distortion rate function process sources previously called sources 
rate distortion functions defined similarly 
source coding theorem shows appropriate conditions sources memory rates words shannon distortion rate function represents asymptotically achievable lower bound performance vq dimension 
positive coding theorem demonstrating shannon distortion rate function fact achievable allows codes arbitrarily large dimension complexity difficult prove existence codes rests law large numbers suggesting large dimensions required codes consequently large demands complexity memory delay 
shannon results zador gish pierce provide benchmarks comparison quantizers 
shannon results provide interesting contrast early results quantizer performance 
specifically early quantization theory derived limits scalar quantizer performance assumption high resolution showed bounds achievable suitable choice quantizer 
shannon hand fixed finite rate considered asymptotic limits dimension vector quantizer allowed arbitrarily large 
asymptotics high resolution fixed dimension generally viewed quantization theory fixed rate high dimension generally considered source coding theory information theory 
prior quantization viewed primarily pcm form analog conversion digital modulation shannon source coding theory generally viewed mathematical approach data compression 
explicitly apply shannon source coding theory problem analog conversion combined digital transmission appear explicit comparisons quantizer performance shannon rate distortion function 
distinct variation shannon approach introduced english literature kolmogorov described results russian information theorists inspired shannon treatment coding respect fidelity criterion 
kolmogorov considered notions rate respect fidelity criterion second notion shannon mutual information minimized subject constraint average distortion case measured squared error 
similar minimization mutual information requirement maximum distortion input reproduction exceed specified level kolmogorov referred functions entropy random object name subsequently considered apply maximum distortion constrained shannon function called rate distortion function constrained average distortion 
note maximum distortion respect distortion measure incorporated average distortion formulation considers new distortion measure defined 
shannon rate distortion function information theoretic definition 
quantization corresponding operational definitions 
operational epsilon entropy entropy random variable defined smallest entropy quantized output reproduction input probability effectively variable rate definition lossless coding required achieve bit rate near entropy 
alternatively define operational epsilon entropy smallest number reproduction codevectors inputs probability codevector 
quantity clearly infinite random object finite support 
shannon case definitions dimensional vectors limiting behavior studied 
results regarding convergence limits equality informationtheoretic operational notions epsilon entropy 
theory concerned approximating epsilon entropy small epsilon entropy extends function approximation theory slight change removing notion probability 
epsilon entropy log smallest number balls radius required cover compact metric space function space see discussion various notions epsilon entropy 
mention epsilon entropy close mathematical connection rate distortion theory 
emphasis codes minimize average maximum distortion 
earliest vector quantization outside shannon sketch rate distortion theory earliest definite vector quantization flavor appeared mathematical statistical literature 
important remarkable considered problem equivalent dimensional generalization scalar quantization squared error distortion measure 
gray neuhoff quantization suppose mass density defined euclidean space 
finite partition euclidean space disjoint bodies cells collection vectors associated cell partition 
partition collection vectors minimizes sum moments inertia cells associated vectors 
problem formally equivalent fixed rate dimensional vector quantizer squared error distortion measure probability density derived consider lloyd optimality conditions centroid nearest neighbor mapping fundamental principles variational techniques proved existence solution described iterative descent algorithm finding partition vector collection 
derivation applies immediately finite dimensional space lloyd extends immediately vector quantization dimension 
aware problems local optima stated generally unique solution 
mention quantization appears state vector quantization problem provide necessary conditions solution yield design algorithm 
toth described specific application problem dimensions source uniform density bounded support region quantization asymptotically large number points 
earlier inequality showed optimal twodimensional quantizer assumptions tessellated support region hexagons 
evaluation performance genuinely multidimensional quantizer 
bell laboratories technical memorandum newman appearance english 
particularly important point simple case independent uniform random variables redundancy remove performance achievable quantizing vectors hexagonal lattice encoding partition strictly better achievable uniform scalar quantization viewed dimensional quantizer square encoding lattice 
high resolution approximations vector quantization published schutzenberger upper lower bounds distortion dimensional variable rate vector quantizers form unfortunately upper lower bounds diverge increases 
zador large advance high resolution methods show large rates operational distortion rate function fixed rate quantization form term independent source dimensional source density term depends source 
generalized formula vector case 
formula obviously matches shannon distortion rate function dimension rate large case approximations zador formula advantage applicable dimension shannon theory applicable large hand shannon theory applicable rate high resolution theory applicable large rates 
theories complementary 
zador explicitly extended lloyd optimality properties vectors distortion measures integer powers euclidean norm generalizing results dimensions higher specifically consider descent design algorithms 
unfortunately results zador thesis published little known outside bell laboratories gersho important described 
zador dissertation dealt analysis vector quantization asymptotic formula correct 
left subsequent unpublished memo derive correct formula 
curiously reports formula thesis memo 
high resolution methods showed large rates operational distortion rate function variable rate vector quantization form term independent source dimension normalized differential entropy source 
completed schutzenberger begun 
mid optimality properties described lloyd zador design algorithm lloyd rediscovered statistical clustering literature 
similar algorithms introduced ball hall macqueen means algorithm 
algorithms developed statistical clustering applications selection finite collection templates represent large collection data mse sense fixed rate vq mse distortion measure quantization terminology cf 
anderberg diday simon hartigan 
macqueen incremental incorporation successive samples training set design codes vector mapped reproduction level representing cluster level cluster replaced adjusted centroid 
simultaneous updates centroids lloyd 
ieee transactions information theory vol 
october unfortunately early results propagate diverse groups working similar problems 
zador extensions lloyd results little known outside bell laboratories 
virtually unknown quantization community 
clustering community effectively vector quantizer design algorithms context statistical clustering little known time quantization community generally appreciated lloyd algorithm fact clustering algorithm 
part lack interest due fact appeared strong motivation consider quantization vectors scalars 
motivation came result shannon landmark source coding fidelity criterion 
implementable vector quantizers mentioned evident earliest studies vector quantization practical technique 
obvious encoding procedure brute force nearest neighbor encoding compare source vector quantized reproduction vectors codebook 
vq dimension rate codevectors number computations required grows exponentially dimension rate product gets quickly hand 
example roughly codevectors 
codevectors need stored consumes costly resources 
proof shannon source coding theorem relies dimension large suggesting large dimension needed attain performance 
point note development channel codes shannon theory suggested large dimension common circa consider channel codes dimensions order 
doubt appeared similarly large dimensions needed effective quantization 
clearly implementation vq dimensions question 
hand channel codes era large dimension performance bch codes highly structured encoding decoding need done brute force 
discussion surprising vq intended practical technique reproduction codebook highly structured order reduce complexity encoding decoding 
specifically speak fixed rate vector quantizer introduced dunn multidimensional gaussian vectors 
argued code effectively permutation code earlier slepian channel coding reproduction codebook contains codevectors permutations 
leads quantizer reduced fairly large complexity 
dunn compared numerical computations performance scheme shannon rate distortion function 
mentioned earlier comparison 
berger jelinek wolf berger introduced lower complexity encoding algorithms permutation codes berger showed large dimensions operational function permutation codes approximately equal optimal variable rate scalar quantizers 
attain performance scalar quantization permutation codes advantage avoiding buffering error propagation problems variable rate quantization 
notwithstanding skepticism feasibility brute force unstructured vector quantization serious studies began appear mid independent results reported describing applications clustering algorithms usually means problems vector quantization 
clustering ideas design vector quantizer low rate speech 
hilbert clustering algorithms joint image compression image classification 
papers appear applications direct vector quantization speech image coding applications 
chen algorithm equivalent dimensional lloyd algorithm design dimensional vector quantizers 
vector extension lloyd method applied linear predictive coded lpc speech parameters weighted quadratic distortion measure parameter vectors closely related itakura saito spectral distortion measure 
collin clustering ideas design dimensional vector quantizers speech coding 
westin esposito considered clustering algorithms design vector quantizers squared error magnitude error distortion measures 
important quantization doubt gersho asymptotically optimal block quantization 
popularized high resolution theory potential performance gains vector quantization provided new simplified variations proofs zador results vector extensions gish pierce results squared error distortion introduced lattice vector quantization means achieving asymptotically optimal quantizer point density entropy constrained vector quantization random vector bounded support 
simple derivations combined vector quantizer point density approximations lder jensen inequalities generalizing scalar quantizer technique introduced 
step development conjecture regarding asymptotically optimal quantizer cell shapes zador constants conjecture borne gersho name considered length section iv 
portions extended nondecreasing functions norms 
gersho stimulated renewed interest theory design direct vector quantizers demonstrated contrary common impression large dimensions required significant gains achieved scalar quantization quantizing vectors modest dimension gray neuhoff quantization result codes competitive predictive transform codes applications 
linde gray explicitly extended lloyd algorithm vector quantizer design 
seen clustering approach vector quantizer design originated years earlier linde introduced direct extension original lloyd optimal pcm design algorithm extended general distortion measures previously considered including input weighted quadratic distortion useful speech coding succeeded algorithm point referred lbg algorithm splitting method designing quantizer scratch developed designs quantizer words means doubles codebook size adding new codevector near existing codevector runs lloyd algorithm 
numerical examples quantizer design complemented gersho high resolution results lloyd complemented shown modest dimensions modest rates significant gains scalar quantization achieved direct vector quantization modest complexity 
year developed tree structured vector quantizer dimensional lpc vectors greatly reduced encoder complexity exponential growth codebook size linear growth searching sequence small codebooks single large codebook 
result bits lpc speech coder intelligible quality comparable scalar quantized lpc speech coders times rate 
see 
year debray spectral distance measure optimize predictors dpcm thorough study vector quantization image compression published yamada fujita 
hindsight surprising effectiveness low dimensional vq explained fact shannon theory large dimension needed attain performance arbitrarily close ideal 
channel coding rates capacity ideal performance means zero error probability large dimension needed codes approach 
quantizing rate ideal performance means distortion equal zero really point making difference actual ideal performance arbitrarily small 
example come db require terribly large dimension 
return section iv estimates required dimension 
followed active period facets quantization theory design 
results developed early decade grouped march special issue quantization transactions published bell laboratories technical memos lloyd newman zador berger extension optimality properties entropy constrained scalar quantization distortion measures extensive comparison minimum entropy quantizers fixed rate permutation codes generalizations fleischer conditions uniqueness local optima results asymptotic behavior lloyd algorithm training sequence size theory means consistency pollard seminal papers lattice quantization conway sloane rigorous developments bennett theory vector quantizers th power distortion measures wise demonstration stochastic stability general class feedback quantizers including historic class predictive quantizers delta modulators adaptive generalizations study convergence rate lloyd algorithm demonstration garey johnson lloyd max optimization np hard 
middle tutorial articles vector quantization appeared greatly increased accessibility subject :10.1.1.116.3824
mid middle late wide variety vector quantizer design algorithms developed tested speech images video signal sources 
quantizer design algorithms developed alternatives lloyd algorithm include simulated annealing deterministic annealing pairwise nearest neighbor origins earlier clustering techniques stochastic relaxation selforganizing feature maps neural nets 
variety quantization techniques introduced constraining structure vector quantization better balance complexity performance methods applied real signals especially speech images random sources permitted comparison theoretical high resolution shannon bounds 
literature begins grow large cite works possible interest techniques considered section mention examples leave discussion section discussed depth section fast search algorithms developed unstructured reproduction codebooks faster searches reproduction codebooks constrained simple structure example subset points regular lattice lattice vector quantizer 
additional structure imposed faster searches virtually loss performance fisher pyramid vq takes advantage asymptotic equipartition property choose structured support region quantizer 
tree structured vq uses tree structured reproduction codebook matched tree structured search algorithm 
tree structured vq far memory provided multistage residual vq 
variety product vector quantizers cartesian product reproduction codebook rapidly searched 
examples include polar vector quantizers mean removed vector quantizers shape gain vector quantizers 
trellis encoders trellis coded quantizers viterbi algorithm encoder matched reproduction codebook trellis structure 
hierarchical ieee transactions information theory vol 
october table lookup vector quantizers provide fixed rate vector quantizers minimal computational complexity 
early quantization techniques results applications original form reprint collection quantization abut ieee reprint collection vector quantization 
close section brief discussion specific works deal optimizing variable rate scalar quantizers additional structure problem leads general formulation optimal quantization section 
extended berger necessary conditions optimality entropy constrained scalar quantizer general distortion measures described design algorithms similar berger iterative algorithm second fixed point algorithm considered natural extension lloyd method fixed rate variable rate vector quantization 
chou developed generalized lloyd algorithm entropy constrained vector quantization generalized berger lagrangian formulation scalar quantization fixedpoint design algorithm vectors 
optimality properties minimizing lagrangian distortion derived rate average length entropy 
lloyd optimal decoder remained unchanged lossless code easily seen optimal lossless code encoded vectors formulation shows optimal encoder simultaneously consider distortion rate resulting encoder 
words quantizers variable rate encoder minimizes sum squared error weighted bit rate squared error 
approach entropy constrained scalar quantization described 
place mention gish pierce result rate high optimal entropy constrained scalar vector quantization provide roughly bit improvement uniform scalar quantization block entropy coding 
berger showed permutation codes achieved roughly performance fixed rate vector quantizer 
ziv showed subtractive dithering allowed dithered uniform quantization followed block lossless encoding bit worse optimal entropy constrained vector quantizer block size rate high 
subtractive dithering discussed adds random dither signal input removes decompressed output 
previously discussed results eliminate usefulness fixed rate quantizers may simpler avoid difficulties associated codes 
results suggest uniform quantization lossless coding candidate benchmark performance comparison 
known operational distortion rate function variable rate quantization dithering better dithering 
decade seen continuing activity developing high resolution theory design algorithms variety quantization structures applying principles theory optimizing signal processing communication systems incorporating quantizers 
arrival place close historical tour results current decade sketched remaining sections 
difficult resist pointing lloyd algorithm rediscovered statistical literature name principal points distinguished traditional means assumption absolutely continuous distribution empirical distribution formulation included vq formulation general distribution 
unfortunately works reflect awareness rich quantization literature 
quantizers today uniform scalar combined prediction transforms 
niche applications true vector quantizers including lattices constrained code structures exhibit advantages including coding speech residuals code excited linear predictive speech coding systems microsoft streaming video 
vector quantization scalar quantization usually applied digital signals signals finely quantized converter 
case quantization vector scalar truly represents compression reduces number bits required describe signal reduces bandwidth required transmit signal description analog link 
modern video coding schemes incorporate lagrangian distortion viewpoint accomplishing rate control predictive quantization general sense motion compensation uniform quantizers optimized lossless coding transform coefficients coding cf 

iii 
quantization basics encoding rate distortion optimality section presents self contained manner basics memoryless quantization vector quantizers operate independently successive vectors 
brevity omit memoryless qualifier rest section 
key characteristic quantizer dimension positive integer 
input dimensional vector alphabet 
alphabets interest rate distortion theory virtually alphabets encountered quantization realvalued vector spaces case alphabet called support source distribution 
quantizer scalar vector 
case quantizer consists components lossy encoder index set arbitrary countable set usually taken collection consecutive integers reproduction decoder reproduction alphabet lossless encoder invertible mapping probability collection variable length binary vectors satisfies prefix condition 
alternatively lossy encoder specified partition reproduction decoder specified reproduction codebook gray neuhoff quantization points codevectors codewords lossless encoder described binary codebook containing binary channel codewords 
quantization rule function equivalently dimensional quantizer applying lossy lossless encoders followed corresponding decoders sequence dimensional input vectors extracted data encoded 
unique way vector extraction design performance quantizer usually depend significantly specific method 
data naturally forms sequence scalar valued samples speech vector extraction done parsing data successive tuples adjacent samples example possibilities extract samples followed odd samples samples 
subsampling useful multiresolution reconstruction interpolative vector quantization 
types data may canonical extraction method 
example stereo speech dimensional vectors consist just left samples just right samples half left followed right example grayscale imagery dimensional vectors come parsing image rectangular blocks pixels tiling polytopes hexagons shapes aimed advantage eye insensitivity noise diagonals comparison horizontal vertical lines 
vectors come regular parsing 
image color pixel value represented dimensional vector dimensional vectors extracted ways 
data sequence color images digital video extraction possibilities increase immensely 
generic domains memoryless quantization theory analysis design proceed 
call random vector domain input data source quantized described fixed value alphabet probability distribution quantizer dimensional 
case specific vector dimension contents allowed vary dimensional speech parameter vectors line spectral pairs reflection coefficients coded 
second call random process domain input data characterized discrete parameter random process countable collection usually infinite random variables different ways extracting vectors component variables may considered compared including different choices dimension indicated general ways 
concreteness provides opportunity key points random process domain interest section focus exclusively example video community longstanding debate progressive versus interlaced scanning different extraction methods 
canonical case data naturally forms onedimensional scalar valued sequence successive tuples adjacent samples extracted quantization 
assume random process stationary specific exception 
stationary models easily defined include processes exhibit distinct local global stationarity properties speech images models composite hidden markov mixture sources 
random vector domain firstorder stationarity assumption individual components vector need identically distributed 
domain presume quantizer operates dimensional random vector usually assumed absolutely continuous described probability density function pdf densities usually assumed finite variance order avoid technical difficulties 
memoryless quantizers described referred vanilla vector quantizers block source codes 
alternative quantizer memory 
memory incorporated variety ways separately lossy encoder example different mappings conditional past lossless encoder index produced quantizer coded conditionally previous indices 
shall return vector quantizers memory section primary emphasis remain memoryless quantizers 
occasionally term code generic substitute quantizer 
instantaneous rate quantizer applied particular input normalized length channel codeword number bits source symbol sent describe reproduction 
important special case binary codewords length case quantizer referred fixed length fixed rate 
measure quality reproduction assume existence nonnegative distortion measure assigns distortion cost reproduction input ideally distortion measure easy compute useful analysis perceptually meaningful sense small large distortion means poor perceived quality 
single distortion measure accomplishes goals common squared error distortion satisfies 
lack perceptual meaningfulness useful indicator perceptual quality importantly generalized class distortion measures proved useful perceptual coding input weighted quadratic distortion measures form positive definite matrix depends input cf 

theory design techniques ieee transactions information theory vol 
october considered extend measures discussed 
assume assumption involves genuine loss generality allows consider lossless code code inputs exists considerable literature various distortion measures including norms differences convex nondecreasing functions norms differences 
rarely application real systems emphasis mse comments generalizations input weighted quadratic distortion measures 
performance quantizer applied source characterized normalized rate normalized average distortion quantizer described rate distortion pair goal compression system design optimize rate distortion tradeoff 
fixed rate quantizers constrain optimization allowing code assign fewer bits inputs benefit provide simpler codes avoid necessity buffering order match variable rate codewords possibly digital channel 
optimal rate distortion tradeoff fixed dimension formalized ways optimizing distortion constrained rate optimizing rate constrained distortion unconstrained optimization lagrange approach 
approaches lead respectively operational distortion rate function operational rate distortion function operational lagrangian weighted distortion rate function nonnegative number 
small value leads low distortion high rate solution large value leads low rate high distortion solution 
note bracketed term considered modified lagrangian distortion smallest average lagrangian distortion 
formalizations optimal performance uses essentially equivalent distortion rate rate distortion functions duals distortion rate pair convex hull curves corresponds lagrangian value note constrains problem fixed rate codes lagrangian approach reduces distortion rate approach longer depends code considered just binary indexing formal definitions quantizer optimality easily yield optimality conditions direct vector extensions variations lloyd conditions 
conditions common flavor components code fixed third component specific form code optimal 
resulting optimality properties summarized 
proofs simple require calculus variations differentiation 
proofs may 
fixed lossy encoder regardless lossless encoder optimal reproduction decoder output minimizing conditional expectation distortion output input encoder produced index vectors called lloyd centroids 
note optimal decoder output encoder output simply optimal estimate input vector sense minimizing conditional average distortion 
distortion squared error reproduction decoder simply conditional expectation encoded centroid distortion measure input weighted squared error centroid fixed lossy encoder regardless reproduction decoder optimal lossless encoder optimal lossless code discrete source huffman code lossy encoded source 
fixed reproduction decoder lossless code lagrangian parameter optimal lossy encoder minimum distortion nearest neighbor encoder modified lagrangian distortion measure code constrained fixed rate second property irrelevant third property reduces familiar minimum distortion encoding respect asin original formulation lloyd implicit shannon 
resulting partition called voronoi partition 
general variable rate case minimum distance respect distortion measure encoder suboptimal gray neuhoff quantization optimal rule takes account distortion codeword length 
simply cascading minimum mse vector quantizer lossless code suboptimal 
general case instantaneous rate considered optimal encoding goal trade distortion rate optimal fashion 
cases encoder viewed mechanism controlling output decoder minimize total lagrangian distortion 
optimality conditions imply descent algorithm code design initial code optimize encoder components optimize reproduction decoder remaining components optimize lossless coder remaining components 
denote transformation resulting operations 
iteration decrease leave unchanged average lagrangian distortion 
iterate convergence improvement falls beneath threshold 
algorithm extension variation algorithm optimal scalar quantizer design introduced fixed rate scalar quantization lloyd 
algorithm fixed point algorithm converges code code fixed point respect generalized lloyd algorithm applies distribution including parametric models empirical distributions formed training sets real data 
obvious means choosing best design algorithm sweep values provide choice rate distortion pairs 
mention lloyd style iterative algorithms design structured forms quantization 
example codes constrained fixed rate algorithm means clustering finding fixed number representative points yield minimum average distortion minimum distortion mapping assumed 
mentioned section variety clustering algorithms exist design vector quantizers solve clustering problems 
convincingly yielded significant benefits lloyd algorithm variations terms trading rate distortion proved faster slower 
algorithms simulated deterministic annealing experimentally better job avoiding local optima finding globally optimal distortion rate pairs basic lloyd algorithm repeated applications lloyd algorithm different initial conditions proved effective avoiding local optima 
focus lloyd algorithm simplicity proven merit designing codes wealth results regarding convergence properties 
centroid property optimal reproduction decoders interesting implications special case squared error distortion measure follows easily quantizer output considered unbiased estimator input 
component quantizer output orthogonal component quantizer error 
example known fact minimum mean squared error estimate unknown observation causes estimate orthogonal error 
view previous property implies quantizer error uncorrelated quantizer output assumed quantizer input 
implies energy variance quantized signal original signal 
shows quantizer error uncorrelated input 
fact correlation minus mean squared error 
instructive consider extreme points tradeoff distortion zero rate 
suppose case rate affect lagrangian distortion mse counts 
source discrete optimize case forcing zero distortion lossless code 
case shannon lossless coding theorem implies rate measured average instantaneous codelength rate measured entropy simply entropy vector 
terms lagrangian formulation conversely suppose case distortion costs negligible amount rate costs enormous amount optimal attained zero rate simply tolerating distortion suffer 
distortion zero rate code minimized centroid unconditional distribution simply mean mse case 
lagrangian formulation extreme points global optima albeit second useless practice 
far focused random vector domain considered optimality quantizers fixed dimension 
practice source coding theory dimension may parameter choice interest consider optima depend 
accordingly focus random process domain assuming source onedimensional scalar valued stationary random process 
situation various operational optima explicitly note dimension denotes operational distortion rate function dimension rate similarly denote operational rate distortion lagrange functions 
optimal performance quantizers rate equal defined ieee transactions information theory vol 
october similar definitions hold rate versus distortion lagrangian viewpoints 
stationarity shown cf 
lemma operational distortion rate function subadditive sense positive integers shows generally decreasing trend increases 
known equal shown subadditivity implies cf 
high dimensional quantizers quantizer 
note hold special cases fixed rate quantizers variable rate quantizers 
important point squared error distortion measures specifically represents performance achieved exactly degenerate situations source distribution discrete continuous 
course infimum definition quantizers performance arbitrarily close 
conclude quantizers truly optimal 
essential understand word optimal random process domain context specific constraint class quantizers dimensional fixed rate vq entropy constrained uniform scalar quantization pyramid coding dimension name random 
desirable optimality loses bit lustre considers fact optimal code class suboptimal code 
evident importance lloyd style optimality principles lies ultimately ability guide optimization quantizers specific constraints classes 
iv 
high resolution quantization theory section presents overview high resolution theory compares results shannon rate distortion theory 
simplicity adopt squared error distortion measure late section extensions distortion measures discussed 
styles high resolution theory developments informal simple approximations rigorous limiting formulas rigorously derived 
proceed informal style results rigorous approach summarized 
presume random vector domain fixed dimension described previous section stated 
asymptotic distortion mentioned earlier elementary result high resolution theory approximation mean squared error uniform scalar quantizer step size derive 
consider level uniform quantizer levels quantizer applied continuous random variable probability density small overload distortion ignored mean squared error mse distortion may approximated follows approximation derives ignoring overload distortion 
source density entirely contained granular region quantizer approximation needed 
second approximation derives observing density may approximated constant small interval 
usually mean value theorem integration assumes density continuous measurable function approximately continuous sufficiently small approximation valid discontinuous densities 
third approximation derives recognizing definition riemann integral approximately equal integral approximation derives ignoring overload region 
mentioned earlier sections situations variable rate quantization infinite number levels permitted 
cases support uniform scalar quantizer contains source density overload distortion ignore important mention sense approximated small small saying assert difference small 
discussed context rigorous framework high resolution theory shown ordinary conditions ratio tends decreases 
generally mention high resolution approximations discussed hold ratio tending sense 
assumptions simple approximations deriving guise derivation subsequent high resolution formulas nonuniform vector variable rate quantizers 
said principal 
small cell type supposition gives theory high resolution name 
gray neuhoff quantization uniform quantization cells size shape levels center cell outermost cells ignored 
cell size key performance determining gross characteristic 
advanced vector quantization cells may differ size shape codevectors need centers cells 
consequently gross characterizations needed 
point density inertial profile 
point density vector quantizer direct extension point density introduced section ii 
nonnegative usually smooth function integrated region determines approximate fraction codevectors contained region 
fixed rate coding point density usually normalized number codevectors total integral 
coding number codevectors key performance determining parameter may infinite point density usually left unnormalized 
consider fixed rate coding presume normalized stated 
clearly inverse relationship point density volume cells number codevectors cells denotes cell containing density describes discrete set points unique way define specific quantizer 
point density intended high level gross characterization model target quantizer 
describes codevectors way probability density describes set data points say exactly located roughly characterizes distribution 
quantizers different numbers codevectors compared basis point density ideal point density quantizers aspire achieve exactly may approximate 
times concrete definition point density specific quantizer needed 
cases specific point density quantizer function captures fine detail quantizer partition contrast usual notion point density gross characterization 
example mention fixed rate quantization ideal point density usually smooth function closely related source density may say quantizer point density approximately set high probability relative source density 
scalar quantizer implemented proportional derivative compressor function applied input 
notion point density doubt recognizable earliest contributors bennett mentioned earlier explicitly introduced lloyd 
nonuniform scalar quantization vector quantization additional issue codevector placement cells case cell shape 
effect point placement cell shape exhibited approximation contribution small cell codevector mse dimensional vector quantizer normalized moment inertia cell point defined normalizing volume independent size cell 
normalizing dimension yields kind invariance dimension write clear context 
normalized moment inertia resulting contribution smaller sphere cells codevectors center cells sharply pointed vertices displaced codevectors 
cases points farther contribute substantially normalized moment inertia especially dimension large 
quantizers uniform scalar lattice quantizers cells exception outermost cells shape placement codevectors cells 
quantizers cell shape codevector placement varies position 
cases useful characterize variation cell normalized moment inertia nonnegative usually smooth function called inertial profile 
point densities define equal want high level gross characterization model quantizer 
called specific inertial profile quantizer piecewise constant function captures fine details cell normalized moment inertia 
returning expressed effect cell size obviously term inverse relationship point density cell volume yields shows point density locally influences distortion 
summing cells recognizing sum approximation integral yields approximation distortion vector quantizer scalar quantizers points middle cells reduces bennett restated terms point densities lloyd 
general formula called bennett integral 
ieee transactions information theory vol 
october extension bennett integral vector quantizers gersho quantizers congruent cells concept inertial profile needed vector quantizers varying cell shapes codevector placements na neuhoff 
bennett integral expected approximation conditions cells small approximated constant cell 
large cells small 
ordinarily requires large 
ii specific point density quantizer approximately equals high probability set iii specific inertial profile approximately equals high probability set iv adjacent cells similar volumes 
condition rules quantizers scalar cells alternating lengths point density quantizer points interval width assuming simplicity source density uniform itis easy compute bennett integral equals may obtain correct distortion separately applying bennett integral union intervals length union intervals length problem bennett integral linear point density 
accurate cell size change slowly occasionally 
bennett integral linear inertial profile necessary assume adjacent cells similar shapes normally expect case situations bennett integral applied 
examples vector extension bennett integral 
approximating source density constant quantization cell key step derivations assuming effect quantization add noise uniformly distributed 
range noise values match size shape cell 
cells size shape quantization noise obviously correlated vector quantized 
hand uniform scalar lattice vector quantizers error approximately uncorrelated 
general result mentioned section iii correlation input quantization error approximately equal mse quantizer codevectors approximately centroids 
performance best dimensional fixed rate quantizers having bennett integral distortion hope find formula operational distortion rate function dimensional fixed rate vector quantization choosing key characteristics point density inertial profile minimize 
unfortunately known find best inertial profile 
known functions allowable inertial profiles 
gersho widely accepted conjecture rate large cells dimensional quantizer rate minimum nearly minimum mse approximately congruent basic tessellating dimensional cell shape case optimum inertial profile constant bennett integral minimized variational techniques lder inequality resulting optimal point density approximation operational function large normalized moment inertia dimensional tessellating polytopes term depending source distribution 
dividing variance invariant scaling source 
refer respectively gersho constant dimension zador factor dimensional fixed rate quantization zador gersho function dimensional fixed rate quantization 
zador role described 
reduces formula 
form may straightforwardly deduce cells smaller higher probability larger cells contribute roughly distortion approximately partial distortion theorem deduced scalar quantization 
number properties known mention just 
gersho constant known respectively interval regular hexagon 
known monotonically nonincreasing shown form subadditive sequence property strong imply infimum equals limit tends infinity 
long presumed directly shown tend increases zamir feder limit normalized moment inertia dimensional spheres tends infinity 
previously assertion tend depended gersho conjecture 
zador factor tends smaller source densities compact lighter tails uniform dependence source variables 
fortunately high resolution theory need rely solely gersho conjecture zador dissertation subsequent memo showed large rate form independent cell exists partition cells entirely translations rotations voronoi cell lattice tessellations generated lattices 
gersho conjectured admissible sense voronoi partition centroids tessellation coincide tessellation 
essential 
gray neuhoff quantization source distribution 
gersho conjecture really just conjecture deriving key result zador showed random vector uniformly distributed unit cube form large effectively defines 
case prove general result showing quantizer high rate better partition hierarchically constructed partitioning small equally sized cubes subdividing partition quantizer best uniform distribution cube number cells cube depends source density cube 
words local structure asymptotically optimal quantizer optimum quantizer uniform distribution 
light gersho conjecture true 
high rates 
may obtain asymptotically optimal quantizer uniform distribution tessellating statement proven cf 
toth see 
known best lattice tessellation body centered cubic lattice generated truncated octahedron 
proven best tessellation suspect summary gersho conjecture known true false best quantizers uniform source periodic tessellation cell shapes alternate periodic fashion hexagons surface soccer ball 
cells period tessellation volumes may apply bennett integral holds replaced average normalized moment inertia cells period 
cells unequal volumes example discussing condition iv bennett integral mse average distortions computed bennett integral separately union cells type definition needed 
structure optimal quantizers aperiodic 
asymptotically find quantizer periodic structure essentially aperiodic 
open question dimensions best tessellation lattice 
dimensions best known tessellation lattice 
tessellations better best known lattices dimensions eriksson 
shall proceed assuming gersho conjecture correct knowledge case analyses wrong factor larger probably larger case converge discussed 
performance best dimensional variable rate quantizers extensions high resolution theory variable rate quantization bennett integral approximations originally due gish pierce entropy output quantizer 
approximations derived approximations derive bennett integral stated earlier scalar quantizers 
approximation says quantizers small cells unnormalized point density holds equally vector quantizers interpreted vector scalar variable 
mentioned unnormalized point density variable rate quantization number codevectors primary characteristic may infinite 
example add levels way negligible impact distortion entropy 
proceed bennett integral entropy approximation find operational distortion rate function variable rate dimensional memoryless vq 
wish consider somewhat general case 
just gish pierce quite interesting examining best possible performance scalar quantization block entropy coding consider operational distortion rate function vector quantization block entropy coding 
specifically seek defined infimum distortions quantizer rate lossy encoder dimensional memoryless lossless encoder simultaneously codes block successive quantization indices variable length prefix code 
effect code dimensional memoryless vq 
refer dimensional memoryless quantizer variable length coding th order entropy coding 
code conventional memoryless variable rate vector quantizer 
convenient fixed length coding means previous section 
finding high resolution approximations values able compare advantages increasing dimension quantizer increasing order entropy coder 
find assume source produces sequence identical necessarily independent dimensional random vectors density straightforward generalization shows high resolution conditions rate hand distortion code may approximated bennett integral substituted normalized point density fixed rate vector quantization find choosing inertial profile point density minimize bennett integral subject constraint rate right hand side known find best inertial profile gersho conjecture suggests rate large cells best rate constrained quantizers congruent shall assume ieee transactions information theory vol 
october inertial profile best variable rate quantizers approximately case variational techniques simply jensen inequality show best point density uniform support source density 
words quantizer cells size tessellation 
fact yields term depending source distribution 
dividing variance invariant scale 
call th order zador entropy factor zador gersho function variable rate coding 
fixed rate coding special case variable length coding equal 
directly verified jensen inequality 
case scalar quantization optimality uniform point density operational function gish pierce 
zador considered case showed form large constant independent source density larger constant fixed rate quantization 
gersho argument find form 
fixed rate quantization shall proceed assumption gersho conjecture correct case wrong analyses factor probably just little larger case converge fixed rate quantization arbitrary dimension restrict attention random process domain source assumed dimensional stationary random process 
seek high resolution approximation operational distortion rate function represents best possible performance fixed rate memoryless quantizer 
mentioned section iii stationary sources limit highresolution approximation yields fact large zador gersho function 
operational function derived zador showed unknown factors converged derivation due gersho 
notice limiting case doubt constant previously mentioned subadditive smallest large 
similarly stationary sources shown sequence subadditive smallest large 
expression zador gersho function benefits increasing dimension fixed rate quantization continuing random process domain stationary sources generally decreasing natures directly quantify benefits increasing dimension fixed rate quantization 
course cost increasing dimension increase complexity 
example decreases limit decibels represents db decrease mse 
gaussian source decreases limit represents additional db gain 
total high dimensional quantization gains db scalar quantization gaussian source 
gauss markov source correlation coefficient decreases limit gain db yielding total high dimensional vq gain db scalar quantization 
db bit rule gain stated decibels translated reduction rate bits sample dividing 
hand important understand specific characteristics vector quantizers improve dimension 
motivated prior explanations offer 
wish compare optimal quantizer dimension optimal dimensional quantizer simplify discussion assume multiple quantizers differing dimensions characteristics fairly compared comparing product vq implicitly formed times succession 
specifically product quantizer quantization rule successive tuples reproduction codebook consisting concatenations possible sequences codevectors reproduction codebook subscripts attached needed associate appropriate features appropriate quantizer 
distortion rate product quantizer easily seen dimensional vq 
shortcomings optimal dimensional quantizer relative optimal high dimensional quantizer may identified product quantizer particular gray neuhoff quantization suboptimal point density inertial profile find 
simplify discussion assume fixed rate scalar quantizer large rate levels middle cells point density cells product quantizer dimensional rectangles formed cartesian products cells scalar quantizer 
scalar cells width dimensional cube formed rectangle formed cube 
widths cells approximately determined point density inertial profile determined specifically rectangular nature product cells obtains derive respectively facts volume rectangle product side lengths normalized moment inertia rectangle cube times ratio arithmetic mean square side lengths geometric mean side lengths determined scalar point density 
note diagonal quadrant product cells cubes minimum value 
diagonal cells usually rectangular consequently larger 
quantify suboptimality product quantizer principal feature factor ratio distortions kind loss terms reflect loss due inertial profile point density part bennett integral depend cell shape loss ratio distortion product quantizer hypothetical quantizer point density optimal inertial profile na neuhoff considered ratio product code distortion optimal dimensional vq arbitrary just large 
point density loss ratio distortion hypothetical quantizer point density product quantizer constant optimal inertial profile hypothetical quantizer optimal point density constant inertial profile 
substituting fact large finds cell shape loss factored product space filling loss ratio normalized moment inertia cube highdimensional sphere loss factor cells cell shape loss larger space filling loss 
proceed consider source stationary memoryless consider choose scalar point density order minimize hand choosing uniform set onedimensional density small causes product cells region dimensional density small cubes consequently smallest possible value 
causes product point density poorly matched source density result large 
hand choosing causes product quantizer approximately optimal point density step uses fact large 
choice causes infinite 
best point density implicitly compromise 
region small uniform causes gray defined inverse vector quantizer advantage 
space filling loss called cubic loss 
dimension added subscript places dimension needs emphasized 
fact product quantizers optimal point density overlooked 
implies distortion decrease ieee transactions information theory vol 
october fig 

losses optimal dimensional quantization relative optimal high dimensional quantization gaussian source 
bottom curve point density loss point density loss plus loss top curve total loss 
space filling losses estimates 
product quantizer optimum point density 
generates product quantizer cells region largest cubic explains loss 
example gaussian source optimal choice scalar quantizer causes product quantizer db loss db point density loss 
sum db equals called shape loss determined shape density uniform density need compromise scalar point densities leading best product cell shapes best point density similar 
uniform source density shape loss 
summary source comparison high dimensional quantization shortcomings scalar quantization fixed rate coding db space filling loss lack sufficient degrees freedom simultaneously attain inertial profile small point density small 
hand surprising newcomers vector quantization gains scalar quantizers sources secondly gain just recovery space filling loss 
similar comparison dimensional high dimensional vq comparing product quantizer formed uses dimensional vq optimal dimensional quantizer large results increases space filling loss decreases degrees freedom compromise needed dimensional point density minimizes gives optimal point density 
result point density shape losses decrease zero space filling loss 
gaussian source losses plotted fig 

sources memory scalar quantization engenders additional loss due inability exploit dependence source samples 
specifically dependence correlation source samples product point density match ideal point density approximately 
see definition memory loss 
factor point density losses terms due quantizer inability exploit memory 
memory loss dimensional quantization decreases increases 
value memory loss close unity negligible viewed kind effective memory correlation length source 
closely related decorrelation independence length process smallest value source samples approximately uncorrelated separated variable rate quantization arbitrary quantizer dimension entropy coding order continue random process domain stationary sources 
find best possible performance vector quantizers block entropy coding possible choices dimension lossy encoder order entropy coder examine high resolution approximation shows mentioned previously subadditive choosing large small possible small stationary sources known th order differential entropy monotonically nonincreasing choosing large small possible small interestingly shown gersho credits thomas 
follows immediately best possible performance vector quantizers block entropy coding operational distortion rate function fixed rate quantizers 
words entropy coding permit performance better highdimensional fixed rate quantization 
re examine situation bit carefully 
may summarize various high resolution approximations gray neuhoff quantization fig 

ih ih yv gauss markov source correlation coefficient 
operational distortion rate functions convention refers fixed rate coding refers th order entropy coding note tend decrease increase 
subadditive 
nonincreasing 
illustration fig 
plots decibels versus gauss markov source correlation coefficient consider decreases improves increasing 
hand fixed decreases increasing monotonically nonincreasing dimensional quantization high order entropy coding suffers dimensional space filling loss 
hand fixed decreases subadditive high dimensional quantization suffers loss relative best possible performance matter order absence entropy coder 
see attain performance close large space filling loss approximately combination large approximately 
regarding scalar quantization yields representing db loss may acceptable situations 
acceptable needs increased 
unfortunately evident fig 
space filling loss decreases slowly increasing regarding second note considerable freedom 
extreme cases large fixed rate high dimensional quantization large scalar quantization entropy coding 
fact uniform scalar quantization suffice second case 
alternatively may choose moderate values roughly speaking approximately equal effective memory length source plus value needed memoryless source 
effect source considerable memory memory exploited lossy encoder large lossless encoder large moderate values 
cases potential reductions due increasing tend larger potential reductions space filling loss 
example gauss markov source fig 
decreases db increases infinity decreased db point view lossy encoder benefit entropy coding reduces dimension required lossy encoder 
similarly point view lossless encoder benefit increasing dimension vector quantizer decreases order required lossless encoder 
stated way benefits entropy coding decrease increasing quantizer dimension benefits increasing quantizer dimension decrease increasing entropy coding order 
summary cf 
optimal performance attainable high dimensional lossy encoder entropy coding 
performance db best attainable uniform scalar quantizer high order entropy coding 
extreme approaches quite complex practical systems tend compromises moderate quantizer dimension entropy coding order 
fixed rate quantization important understand specific characteristics variable rate quantizers cause perform way 
consequently take look variable rate quantization time point view point density inertial profile high dimensional product quantizer induced optimal low dimensional variable rate quantizer 
situation simpler fixed rate quantization 
mentioned earlier rate large optimal dimensional quantizer uniform point density partition codebook formed tessellating suppose small large multiple structure optimal quantizers sees optimal dimensional quantizer times yields dimensional quantizer having uniform point density optimal dimensional quantizer differing mainly inertial profile equals constant optimal dimensional quantizer equals loss due dimensional quantization space filling loss explains gish pierce scalar quantizers 
emphasize point density memory loss sources memory 
effect entropy code eliminated need shape point density result need compromise cell shapes 
compare structure fixed rate variable rate approaches dimension large 
ieee transactions information theory vol 
october hand optimal quantizers type constant inertial profile hand markedly different point densities optimal quantizer point density optimal variable rate quantizer point density uniform disparate point densities fact yield distortion 
answer provided asymptotic equipartition property aep key fact information theory rests 
stationary ergodic source continuous random variables aep says dimension large dimensional probability density approximately constant set small probability 
specifically shows set typical sequences differential entropy rate source 
follows immediately aep fact point density optimal fixed rate quantizer approximately uniform zero 
optimal variable rate quantizer point density uniform see cells ignored negligible probability cells probability consequently assigned codewords equal length 
approaches lead quantizers identical uniform point density fixed length codewords differ complement set negligible probability 
worthwhile emphasizing discussion section restricted attention quantizers memoryless lossy encoders fixed rate memoryless block lossless encoders 
lossy lossless encoders form dpcm finite state predictive address vector vq lempel ziv arithmetic lossless coding believe easily analyzed case studied shows effects increasing memory lossy lossless encoders 
distortion measures far commonly assumed distortion measure squared error scalars defined vectors defined distortion normalized variety general distortion measures considered literature simplicity tractability squared error long central role 
intuitively average squared error average energy power quantization noise 
common extension distortion measures scalars th power distortion example roe generalized max formulation distortion measures form 
gish pierce considered general distortion measure form monotone increasing function magnitude argument added property property monotone 
distortion measures widely magnitude error th power studies primarily simple computation comparison squared error multiplications 
scalar distortion measures various generalizations vectors 
dimension fixed needs distortion measure say defined dimension allowed vary requires family distortion measures collection called fidelity criterion source coding theory 
commonly assumed fidelity criterion additive single letter sense equivalently additive distortion measures particularly useful proving source coding theorems normalized distortion converge appropriate conditions dimension grows large ergodic theorem 
assume generally distortion measure sense subadditive ergodic theorem lead positive negative coding theorems 
example subadditive distortion measure levenshtein distance counts number insertions deletions number changes takes convert sequence 
originally developed studying errorcorrecting codes levenshtein distance rediscovered computer science community edit distance fixed dimension observe distortion measure written norm differs slightly previous definition subadditive assumed normalized 
previous definition applied equivalent definition 
gray neuhoff quantization idea extended power norm notation norm choose distortion measure referred simply th power distortion additive 
zador defined general th power distortion measure distortion measure form includes th power distortion narrow sense additive distortion measures form weighted average distortions nonnegative 
variation norm norm defined proposed candidate perceptually meaningful norm 
quantizer design algorithms exist case date highresolution quantization theory rate distortion theory developed distortion measure cf 

high resolution theory usually considers fixed dimension additivity family distortion measures required 
high resolution theory tended concentrate difference distortion measures distortion measures form usual euclidean difference usually assumed nice properties monotonic norm argument 
th power distortion measures types fall category 
basic results high resolution theory extended family distortion measures locally quadratic sense provided distortion measure approximately taylor series expansion positive definite weighting matrix depends output 
form ensured assuming distortion measure continuous partial derivatives third order matrix defined dimensional matrix th element positive definite 
basic idea distortion measure introduced gardner rao model perceptual distortion measure speech matrix referred sensitivity matrix requirement existence derivatives third order positive definite added necessary analysis 
examples distortion measures meeting conditions time domain form itakura saito distortion form input weighted quadratic distortion measure form 
case input weighting matrix related partial derivative matrix positive definiteness assures derivative conditions transferred distortion measures satisfying assumptions image distortion measures fisher nill 
bennett integral extended type distortion approximations variable rate operational distortion rate functions developed 
fixed rate case result modified inertial profile assumed limit natural extension gersho conjecture distortion measures consideration implies squared error case optimal inertial profile assumed constant case yield bound minimizing example lder inequality yields optimal point density operational distortion rate function analogous generalizes zador factor distortion measure 
shown bounded moment inertia sphere 
similarly variable rate case ieee transactions information theory vol 
october optimal inertial profile optimal point density results reduce previous results special case squared error distortion measure note particular optimal point density entropy constrained case general uniform density 
parallel results shannon lower bounds function developed family distortion measures linder zamir results multidimensional lattice codes similar distortion measures developed linder zamir zeger 
rigorous approaches high resolution theory years high resolution analyses styles 
informal analyses distortion obtain bennett integral generally ignore overload distortion estimate granular distortion approximating density constant quantization cell 
contrast rigorous analyses generally focus sequences finer quantizers demonstrate limit overload distortion negligible comparison granular distortion ratio granular distortion function parameter tends constant 
informal analyses generally lead basic results rigorous ones clear approximations percentage errors decrease zero quantizers finer 
rigorous derivations provide explicit conditions assumption negligible overload distortion valid 
analyses informal rigorous provide corrections overload distortion give examples overload distortion asymptotically ignored estimated 
similar comments apply informal versus rigorous analyses asymptotic entropy 
review development rigorous theory 
analyses informal rigorous explicitly assume source finite range probability distribution bounded support overload distortion ignored 
cases source really finite range 
example speech images source samples infinite range measurement device finite range 
cases truncation measurement device creates implicit overload distortion affected design quantizer 
little sense choose quantizer fine granular distortion significantly implicit overload distortion 
means upper limit quantizers need considered consequently question small source density approximated constant cells 
analyses explicitly assume source density finite support merely assert overload distortion ignored 
view differs explicit assumption finite support approaches ignore overload distortion 
assuming finite support arguably mathematically honest 
earliest quantizer distortion analyses appear open literature assumed finite range density approximately constant cells assumption 
papers avoided taylor series expansion source density 
example lloyd approach show ignoring overload distortion approximation error formula means tends zero multiplied roe wood taylor series 
overload distortion explicitly considered optimized cell size uniform scalar quantization explicit formula overload distortion granular distortion formula added overload distortion term 
earliest rigorous analysis contained schutzenberger showed dimensional variable rate quantization th power distortion source finite differential entropy depending source dimension dimensional quantizer finitely infinitely cells output entropy distortion exists sequence quantizers increasing output entropies distortion essence results show unfortunately schutzenberger notes ratio tends infinity dimension increases 
indicates problem demonstrating upper bound constructs sequence quantizers cubic cells equal size bounds distortion cell proportional diameter th power 
bound distortion moment inertia cell times maximum value density tend infinity 
papers appeared issue acta math 
acad 
sci 


renyi gave effect rigorous derivation uniform quantizer infinitely levels 
specifically showed provided source distribution absolutely continuous finite denotes uniform quantizer step size denotes quantity approaches zero goes explores happens distribution absolutely continuous 
lloyd gave fairly rigorous analysis distortion include category ignored overload distortion 
gray neuhoff quantization second toth showed twodimensional random vector uniformly distributed unit square mean squared error point quantizer bounded hexagon result independently simpler fashion newman 
clearly lower bound asymptotically achievable lattice hexagonal cells 
follows ratio hexagon tends gersho conjecture holds dimension 
zador thesis rigorous 
mentioned earlier contains principal results 
fixed rate quantization th power distortion measures form source uniformly distributed unit cube shows lemma operational distortion rate function multiplied approaches limit basic idea zador attributes hammersley positive integers divide unit cube subcubes sides length clearly best code codevectors code constructed best code points subcube 
follows operational function source uniformly distributed subcube second relation follows fact sub source just scaling original source 
multiplying sides yields see increasing number codevectors increase somewhat elaborate argument shows approximately true sufficiently large result limit 
see selfsimilarity uniform density divisible similar plays key role argument 
notice shapes cells point density enter 
zador addresses nonuniform densities 
denoting theorem shows dimensional source density satisfies positive part established constructing codes approximately manner chooses sufficiently large support cube large overload distortion contributes little subdivides cube equally sized subcubes places subcube set codevectors optimal uniform distribution subcube abuse notation slightly denote distortion dimensional quantizers codevectors 
number codevectors subcube carefully chosen point density subcube approximates optimal point density original source distribution 
shows distortion code multiplied approximately best codes follows easily see construction creates codes essentially optimal point density cell shape 
describe converse 
zador bell labs memorandum main results weaker conditions 
distortion measure th power general sense includes special cases narrow sense th power euclidean norm considered schutzenberger 
requirement source density marginals property bounded sufficiently large magnitude 
pure tail condition opposed finite moment condition thesis constrains tail peak density 
note longer requires finite 
indicated earlier zador memorandum derives asymptotic form operational distortion rate function variable rate quantization 
words finishes thesis schutzenberger started apparently unaware 
specifically shows constant larger assuming conditions fixed rate result plus additional requirement bounded set containing points gish pierce discovered uniform asymptotically best type scalar quantizer variable rate coding informal rigorous derivations appear transactions 
specifically showed rigorously uniform scalar quantization infinitely cells width distortion output entropy behave follows rigorous formula respectively 
result required density continuous finitely points satisfy tail condition similar zador condition behavior points discontinuity 
outlined rigorous proof scalar case details offered complete proof surprisingly long gish pierce informally derive date provided rigorous derivation 
ieee transactions information theory vol 
october elias rigorous analysis scalar quantization giving asymptotic bounds distortion scalar quantizers defined measure distortion th root average th power cell widths 
companion considers similar bounds performance vector quantizers analogous average cell size distortion measure 
csisz rigorous generalization higher dimensional quantizers 
interest special case principal result theorem consider dimensional source sequence dimensional quantizers countably infinite number cells volume maximum cell diameters tends zero 
certain conditions including condition quantizer finite output entropy output entropy satisfies clearly result applies quantizers generated lattices generally tessellations 
applies quantizers finitely cells sources compact support 
apply quantizers finitely cells sources infinite support deal overload region quantizers 
obtained results indicating rapidly distortion fixed rate lattice quantizers approach rate dimension increase difference distortion measures 
authors studied uniform scalar quantization variable rate coding extended results th power distortion measures 
contribution gallagher studied asymptotic properties uniform scalar quantization 
denoting cell width minimizes distortion cell uniform scalar quantizers denoting resulting minimum error showed source riemann integrable density length shortest interval probability 
support finite finite implies decreases formula rigorous finite case chosen optimally 
support infinite gaussian density decreases rate slower resulting signal noise ratio versus rate curve separates line slope db bit 
consequently ratio operational distortion rate functions uniform nonuniform scalar quantizers increases bound rate increases uniform quantization asymptotically bad 
showed converge exhibited densities inequality strict 
cases formula invalidated heavy tails density 
asymptotic form described 
formal theory advanced papers wise 
demonstrated zador fixed rate result distortion assuming contained generalization random vectors probability densities distributions absolutely continuous continuous 
gave rigorous approach derivation bennett integral scalar quantization 
pointed linder gap proof concerning convergence riemann sums increasing support riemann integral linder fixed correct derivation weaker assumptions 
claimed similar result restrictive conditions suffered sort problems 
subsequent derived result vector quantizers lies bennett integral zador formula 
specifically showed sequence quantizers asymptotically optimal probability density th power distortion source density asymptotically optimal point density hand bennett integral consequently arbitrary 
hand zador result gersho generalization bennett integral essence assumed quantizers optimal cell shapes 
linder zeger rigorously derived asymptotic distortion quantizers generated tessellations showing quantizer formed tessellating basic cell shape scaled positive number average narrow sense th power distortion satisfying combined csisz result show fairly weak conditions finite differential entropy finite output entropy output entropy distortion asymptotically related gersho derived informally 
generalization bennett integral fixed rate vector quantizers arbitrary cell shapes accomplished na neuhoff informal rigorous derivations 
rigorous derivations shown sequence quantizers parameterized number codevectors specific point density specific inertial profile converging probability model point density model inertial profile gray neuhoff quantization respectively converges bennett integral distortion th power couple additional conditions required including implicitly tail condition 
uniform scalar quantization finitely levels oldest elementary form quantization asymptotic form optimal step size resulting mean squared error gaussian densities infinite support 
specifically hui neuhoff gaussian density variance result independently eriksson 
shown overload distortion asymptotically negligible time proved source infinite support 
follows noise ratio increases shows concretely uniform scalar quantization asymptotically bad 
hui neuhoff considered non gaussian sources provided fairly general characterization asymptotic form turned overload distortion asymptotically negligible tail parameter equals case generalized gaussian densities 
cases accurate approximations 
densities ratio overload granular distortion densities tails heavy granular distortion negligible comparison overload distortion 
related result asymptotic form optimal scaling factor lattice quantizers gaussian source 
conclude subsection mentioning gaps rigorous high resolution theory 
course proof gersho conjecture dimensions higher 
open question best tessellation dimensions lattice 
apparently difficult questions 
rigorous derivations extension higher dimensional quantizers finitely levels overload distortion dealt 
likewise rigorous derivations higher dimensional generalization case point density constant 
assuming gersho conjecture correct rigorous derivation zador gersho formulas lines informal derivations start bennett integral 
mention tail conditions rigorous results difficult check 
simpler ones needed 
discussed section ii convincing rigorous asymptotic analyses operational distortion rate function dpcm 
comparing high resolution theory shannon rate distortion theory interesting compare contrast principal theories quantization shall number different domains 
applicability sources shannon rate distortion theory applies fundamentally infinite sequences random variables sources modeled random processes 
results derive frequencies events repeat expressed law large numbers weak law ergodic theorem 
applies sources stationary strict sense weaker sense asymptotic mean stationarity cf 

originally derived ergodic sources extended sources 
contrast high resolution theory applies fundamentally finite dimensional random vectors 
stationary asymptotically stationary sources limits yields results random processes 
example operational distortion rate function equal way see 
rate distortion theory result relevant finite dimensional random vectors operational distortion rate functions fixed quantization strictly bounded th order shannon distortion rate function 
theories extended continuous time random processes 
high resolution results somewhat sketchy 
applied higher dimensional sources images video 
developed gaussian sources context squared error distortion surprising view tractability squared error gaussianity 
applicability distortion measures shannon rate distortion theory applies primarily additive distortion measures distortion measures form normalized version results subadditive distortion measures distortion measures 
high resolution theory results th power difference distortion measures mentioned previously results extended distortion measures 
event theories fully developed squared error distortion measure especially gaussian sources 
addition theories require finite moment condition specific distortion measure 
distortion simply variance source finite 
generally addition discussed previously rigorous high resolution theory results require tail conditions source density example complementarity theories complementary sense shannon rate distortion theory prescribes best possible performance quantizers rate ieee transactions information theory vol 
october asymptotically large dimension high resolution theory prescribes best possible performance codes dimension asymptotically large rate 
fixed rate codes similarly variable rate codes large large large large dimension rate large give result rates convergence useful know large respectively high resolution rate distortion theory formulas accurate 
rule thumb high resolution theory fairly accurate rates greater equal sufficiently accurate rates useful comparing different sources codes 
example fig 
shows signal noise ratios quantizers produced conventional design algorithms predictions thereof zador gersho function gaussian sources markov correlation coefficient apparent data accuracy zador gersho function approximation increases dimension 
convergence rate tends infinity studied 
roughly speaking results show memoryless sources convergence rate unfortunately theory enable predict large dimension order specified percentage may high resolution theory comparing variable rate case example gaussian source fig 
shows yields distortions db predicted dimensions respectively 
sources memory dimension needs larger roughly effective memory length 
may conclude shannon distortion rate function approximation applicable moderate large dimensions quantitative relationships squared error distortion zador gersho function precisely equal known shannon lower bound shannon distortion rate function 
follows rate large lower bound similarly shannon lower bound th order shannon distortion rate function equals follows may thought distortion fictional quantizer having distortion optimal dimensional variable rate quantizer order entropy coding cells normalized moment inertia high dimensional sphere known approaches increases fig 

signal noise ratios optimal vq dots predictions thereof zador gersho formula straight lines 
gaussian 
gauss markov correlation coefficient 
entirely consistent fact approaches increases 
relationships various distortion rate functions summarized 
inequalities marked tight dimension increases marked tight increases 
applicability quantizer types rate distortion theory finds performance best quantizers type stationary sources 
say suboptimal structured dimension constrained quantizers mentioned earlier quantizers dimension distortion bounded th order shannon distortion rate function 
contrast high resolution theory analyze optimize performance number families structured quantizers transform lattice product polar stage directly dimension constrained quantizers 
analyses typically bennett integral 
gray neuhoff quantization ability analyze structured dimension constrained quantizers true forte high resolution theory 
performance versus complexity assessing performance versus complexity major goal quantization theory 
hand rate distortion theory specifies fundamental limits performance regard complexity 
hand high resolution theory analyze performance families quantizers complexity reducing structure learn complexity relates performance 
hui neuhoff combined high resolution theory turing complexity theory show asymptotically optimal quantization implemented complexity increasing polynomially rate 
computability order shannon distortion rate functions computed analytically squared error magnitude error source gaussian laplacian discrete sources cf 

sources computed blahut algorithm 
case squared error computed simpler algorithms 
sources memory complete analytical formulas th order distortion rate functions known gaussian sources 
cases blahut algorithm compute computational complexity overwhelming small 
due difficulty computing lower bounds shannon distortion rate function developed reasonably general cases yield distortion rate function exactly region small distortion cf 

important upper bound derives fact respect squared error gaussian source largest shannon distortion rate function th order limit source covariance function 
compute zador gersho function needs find fixed variable rate cases respectively 
known bounds values lower bound normalized moment inertia sphere dimension bound 
upper bound developed zador derive currently best known tessellations cf 

zador factors computed straightforwardly sources 
cases simple closedform expressions gaussian laplacian gamma densities 
cases numerical integration 
upper bounds 
authors knowledge sources memory simple expressions zador factors gaussian sources depend covariance matrix 
underlying principles rate distortion theory deep elegant theory law large numbers key information theoretic property derives aep 
high resolution theory simpler elegant theory geometric characterizations integral approximations fine partitions 
siblings lossless source coding channel coding sibling branches information theory law large numbers asymptotic equipartition property 
siblings high resolution theory include error probability analyses digital modulation channel coding minimum distance high signal noise ratio assumption average power analyses additive gaussian channel continuous approximation 
code design philosophy theory ordinarily considered constructive leads design philosophy 
rate distortion theory shows high probability high dimensional quantizer constructed randomly choosing codevectors output distribution test channel achieves shannon function 
construction technique leaves desired dimension codes large codes constructed completely impractical 
hand aep indicates codevectors roughly uniformly distributed typical set leads design philosophy code codevectors uniformly distributed set 
special case squared error distortion gaussian source variance output distribution gaussian variance typical set thin shell near surface sphere radius code codevectors uniformly distributed shell 
interior volume highdimensional sphere negligible equally valid codevectors uniformly distributed sphere 
sources codevectors uniformly distributed subset shell 
high resolution theory indicates large rate arbitrary dimension quantization cells spherical possible preferably shaped normalized moment inertia codevectors distributed optimal point density high resolution theory yields clear design philosophy 
scalar case philosophy directly construct quantizer designing nonlinearity derivative extracting resulting reconstruction levels thresholds obtain approximately optimal point quantizer 
mentioned rediscovered times 
unfortunately higher dimensions implement optimal point density creating large 
direct way construct optimal vector quantizers high resolution philosophy 
dimension rate large philosophies merge output distribution achieves shannon distortion rate function converges source density optimal point density 
small moderate values specifies better distribution points rate distortion philosophy uniformly distributing codevectors typical set 
example ieee transactions information theory vol 
october gaussian case indicates point density gaussian hill somewhat larger variance source density 
design philosophy useful 
low rates say bit sample choice look rate distortion theory 
moderate high rates appears high resolution design philosophy better choice 
see consider gaussian source target rate dimensional quantizer points uniformly distributed spherical support region 
ideal code suggested rate distortion theory 
obtains lower bound distortion assuming source vectors outside support region quantized closest point surface sphere assuming cells support region dimensional spheres 
case moderate large rates say rate choosing diameter support region minimize lower bound dimension larger order resulting signal noise ratio db predicted shannon function 
similar results reported 
hand mentioned earlier quantizer dimension achieve distortion 
clear ability come fairly close moderately large dimension due rate distortion theory design philosophy aep spherical codes 
due fact codes small moderate dimension appropriately tapered point densities suggested high resolution theory 
interesting note high resolution theory contains analyses shannon random coding approach 
example zador thesis gives upper bound distortion randomly generated vector quantizer 
nature error process theories say distribution quantization errors 
generally speaking rate distortion theory say comes assuming error distribution caused quantizer performance close similar caused test channel comes close achieving shannon distortion rate function 
reasonable shannon random coding argument shows test channel randomly generate high dimensional codevectors leads high probability code distortion close example may sort argument deduce quantization error high dimensional quantizer approximately white gaussian source memoryless distortion squared error rate large cf 
shows gaussian histograms quantization error vq dimensions example gaussian source memory squared error distortion rate distortion theory shows simple relation spectra source spectra error produced optimal high dimensional quantizer cf 

high resolution theory long tradition analyzing error process bennett focusing distribution error spectrum correlation input 
bennett showed high resolution case power spectral density quantizer error uniform quantization approximately white uniformly distributed provided assumptions high resolution theory met joint density sample pairs smooth 
see sec 

bennett exact expressions power spectral density uniformly quantized gaussian process 
snyder derived conditions quantization error white terms joint characteristic functions pairs samples dimensional analogs widrow condition 
zador highresolution expressions characteristic function error produced randomly chosen vector quantizers 
lee neuhoff high resolution expressions density error produced fairly general deterministic scalar vector quantizers terms point density shape profile function conveys cell shape information inertial profile 
side benefit expressions indicate deduced point density cell shapes quantizer histogram lengths errors 
zamir feder showed error produced optimal lattice quantizer infinitely small cells asymptotically white sense components uncorrelated zero means identical variances 
showed gaussian dimension increases 
basic ideas dimension increases lattices nearly spherical cells uniform distribution high dimensional sphere approximately gaussian cf 

optimal high dimensional high rate vq expected nearly spherical cells aep implies cells size reach rate distortion theory high rate high dimensional codes cause quantization error approximately white gaussian 
successive approximation vector quantizers operate successive approximation progressive fashion low rate coarse quantization followed sequence finer finer quantizations add rate 
tree structured multistage hierarchical quantizers discussed section examples 
methods design progressive indexing codebooks yamada 

successive approximation useful situations decoder needs produce rough approximations data bits receives subsequently refine approximation bits received 
successive approximation quantizers structured way simpler unstructured ones 
examples just cited known performance low complexity progressive nature 
important question performance successive refinement quantizer better quantization step 
hand rate distortion theory analysis shown situations successive approximation done loss optimality 
gray neuhoff quantization hand high resolution analyses stage vq quantified loss particular codes case shown ways modifying quantizer eliminate loss 
theories say successive refinement 
quantization techniques section presents overview quantization techniques mainly vector introduced goal attaining rate distortion performance better attainable scalar techniques direct scalar quantization dpcm transform coding inordinately large complexity brute force vector quantization methods 
recall dimension source vector fixed say goal attain performance close optimal performance expressed fixed rate case usually general case variable rate codes permitted 
case stationary source dimension chosen arbitrarily variable rate cases goal attain performance close case quantizers suboptimal quantizers various dimensions memory blurs notion dimension considered 
liked carefully categorized ordered ranked presentation various methods 
literature variety techniques quite large number competing ways categorize techniques complexity difficult thing quantify special cases fixed variable rate fixed dimension theoretical quantitative comparison 
consequently needed sorting wheat chaff determining methods give best performance versus complexity tradeoff situations gaining understanding certain complexity reducing approaches better 
attempted choose reasonable set techniques ordering discussion 
possible comments techniques 
cases include 
brief discussion complexity 
roughly speaking aspects arithmetic computational complexity number arithmetic operations sample performed encoding decoding storage memory space complexity amount auxiliary storage example codebooks required encoding decoding 
trying combine sense keep separate track associated costs vary implementation venue pc unix platform generic dsp chip specially designed vlsi chip venues storage low cost tempted ignore 
techniques benefit sufficiently increased memory unit cost trivial obtain best performance complexity tradeoff memory usage increased marginal gain cost ratio increases small point total cost memory may 
result think quantizer characterized tuple arithmetic complexity storage complexity added usual rate distortion reminder dimensional fixed rate vq codebook containing codevectors brute force encoding finds closest codevector computing distortion codevector 
words uses optimal lossy encoder codebook creating voronoi partition 
case squared error requires computing approximately operations sample storing approximately vector components 
example codebook rate bits pixel bpp vector dimension codevectors impractical number say real time video coding 
exponential explosion complexity memory cause serious problems modest dimension rate general codes completely impractical high resolution high dimension extremes 
brute force variable rate scheme rate complex typically involving greater number codevectors lagrangian distortion computation entropy coding scheme 
high complexity brute force techniques motivates reduced complexity techniques discussed section 
simple measures arithmetic complexity storage need number qualifications 
decide encoding decoding complexities need counted separately summed important 
example record play situations decoder low complexity 
having particular application mind focus sum encoder decoder complexities 
techniques possible trade computations storage precomputed tables 
cases quantizer characterized single curve 
cases set precomputed tables heart method 
issue cost memory accesses 
operations usually expensive arithmetic operations 
methods job reducing arithmetic operations cost memory accesses significant 
techniques attain smaller values distortion need higher precision arithmetic storage usually accounted assessments complexity may significance 
example study vq codebook storage shown routine cases needs store codevector components bits component rate quantizer 
study assess required arithmetic precision guess need little larger storage plus bit arithmetic suffice 
variable rate coding raises additional issues costs associated buffering storing accessing variable length codewords decoder having parse binary sequences variable length codewords 
assessing complexity quantization technique interesting compare complexity invested lossy ieee transactions information theory vol 
october encoder decoder versus lossless encoder decoder 
recall performance theoretically attained simple lossy encoder uniform scalar quantizer sophisticated lossless encoder vice versa high dimensional fixed rate vq 
quantizer considered low complexity encoders low complexity 
discussion follows focus mainly quantization techniques lossless encoder conceptually quantitatively simple 
wish mention indexing problem may considered lie lossless lossy encoder 
certain fixed rate techniques lattice quantization pyramid vq scalar vector quantization fairly easy find cell source vector lies cells associated set indices simply integers number cells converting identity cell sequence bits nontrivial 
referred indexing problem 
mention additional issues 
vq techniques implementation complexities prohibitive sufficiently codevectors designing inordinately complex requires inordinate amount training data 
second issue applications desirable output encoder progressively decodable sense rough reproduction bits receives improved bits received 
quantizers said progressive embedded 
true progressive decoder designed encoder example compute expected value source vector bits received far 
progressive code intermediate distortions achieved intermediate rates relatively usually quantizers designed specific rate restarting scratch time decoder receives new bit group bits uses simple method update current reproduction 
desirable applications encoding progressive 
designed mind turns number reduced complexity vq approaches address issues 
easier design progressive 
fast searches unstructured codebooks techniques developed speeding full minimum distortion search arbitrary codebook containing dimensional codevectors example generated lloyd algorithm 
contrast codebooks considered called unstructured 
asa group techniques substantial amounts additional memory order significantly reduce arithmetic complexity 
variety techniques mentioned sec 

number fast search techniques similar spirit euclidean distances pairs codevectors precomputed stored table 
source vector quantize initial codevector chosen 
codevectors distance greater eliminated consideration closer eliminated successively compared closer replaces process continues 
way set potential codevectors gradually narrowed 
techniques category different ways narrowing search may 
number fast search techniques coarse low complexity technique 
called coarse typically larger cells voronoi regions codebook searched 
coarse involves scalar quantization type tree structuring binary quantizers called trees 
associated coarse cell bucket containing indices codevector nearest codevector source vector cell 
buckets determined advance saved tables 
encode source vector applies finds index cell contained performs full search corresponding bucket closest codevector techniques type may 
coarse dimensional example length source vector may quantized bucket codevectors having similar lengths searched closest codevector 
class techniques previous low complexity smaller cells voronoi cells finer 
case buckets associated fine cells contain just codevector codevector closest codevector point fine cell 
indices codevectors fine cell stored precomputed table 
relatively fine cells buckets containing codevector member bucket chosen index placed table entry fine cell 
quantization proceeds applying fine index fine cell lies address table containing codevectors outputs index codeword due fact bucket contains codevector techniques may perfect full search 
quantitative analysis increased distortion case lattice quantizer 
fast search methods include partial distortion method transform subspace domain approach 
consideration methods leads question fine cells 
experience best tradeoffs come cells finer coarser explanation coarsely gray neuhoff quantization determine codevector bucket closest efficient fast search method full search 
dividing coarse cells finer ones way doing just 
question arises fast search techniques worth effort perform full search short methods fine cells 
experience usually worth effort full search suffering small increase mse achieve significant reduction arithmetic complexity storage 
case stationary sources dimension subject choice amount arithmetic complexity storage gets better performance doing suboptimal search higher dimensional codebook full search lower dimensional 
fast search methods fine improved optimizing codebook 
cell partition corresponding induced followed table lookup union number fine cells 
question best partition cells union number fine cells 
codevectors centroids cells 
techniques exploited 
technique worth particular mention called hierarchical table lookup vq 
case unstructured codebook searched fine turn searched finer 
specifically uses high rate scalar quantizer times 
level applies dimensional vq pairs scalar quantizer outputs 
level applies dimensional vq pairs outputs dimensional quantizers 
method hierarchical 
quantizers implemented entirely table lookup method eliminates arithmetic complexity memory accesses 
successfully video coding 
structured quantizers turn quantizers structured partitions reproduction codebooks turn lend fast searching techniques cases greatly reduced storage 
techniques discussed 
lattice quantizers lattice quantization viewed vector generalization uniform scalar quantization 
constrains reproduction codebook subset regular lattice lattice set vectors form integers linearly independent usually nondegenerate 
resulting voronoi partition tessellation cells overlapping overload region having shape size orientation 
lattice quantization proposed gersho near optimality high resolution variable rate quantization near optimality high resolution fixed rate quantization uniformly distributed sources 
assume gersho conjecture holds best lattice quantizer approximately best tessellation 
especially important fact highly structured nature led algorithms implementing lossy encoders low arithmetic storage complexity 
find integers associated closest lattice point 
conway sloane reported best known lattices dimensions fast quantizing decoding algorithms 
important dimensional lattices root lattices barnes wall lattice dimension leech lattice dimensions 
give best sphere packings coverings respective dimensions 
eriksson improved lattices dimensions low complexity algorithms lossy encoder issues affect performance complexity lattice quantizers 
variable rate coding scale lattice obtain desired distortion rate implement algorithm mapping variable length binary codewords 
potentially add complexity 
fixed rate coding rate lattice scaled subset lattice points identified codevectors 
induces support region 
source finite support lattice quantizer ordinarily chosen support 
scaling factor lattice subset usually chosen resulting quantizer support region large probability 
case low complexity method needed assigning binary sequences chosen codevectors indexing 
conway sloane method important case support shape enlarged cell 
sources infinite support gaussian difficult question quantize source vector lying outside support region 
example scale lies just inside boundary support region quantize scaled vector usual way 
unfortunately simple method find closest codevector increases overload distortion substantially quantization rule 
date apparently low complexity method substantially increase overload distortion 
high resolution theory applies immediately lattice vq entire lattice considered codebook 
theory difficult usually case bounded portion lattice codebook separately consider granular overload distortion 
variety ways considering tradeoffs involved cf 

case essence lattice code uniform point density nicely shaped cells low normalized moment inertia 
fixed rate coding uniform sources sources bounded support 
discussed earlier sources unbounded support gaussian require large dimensions achieve performance close ieee transactions information theory vol 
october fig 

shape gain vq 
product quantizers product quantizer uses reproduction codebook cartesian product lower dimensional reproduction codebooks 
example application scalar quantizer successive samples viewed product quantizer operating dimensional vector product structure searching easier special case sequence scalar quantizers search need comprised independent searches 
products vector quantizers possible 
typically product quantizer applied original vector samples functions features extracted vector 
complexities product quantizer arithmetic storage encoding decoding sums component quantizers 
ordinarily complexities unstructured quantizer number codevectors complexities equal product components product quantizer 
shape gain vector quantizer example product quantizer 
uses product reproduction codebook consisting gain codebook positive scalars shape codebook unit norm dimensional vectors reproduction vector defined easy see minimum squared error reproduction codeword input vector encoding algorithm choose index maximizes correlation chosen choose index minimizing sequential rule gives minimum reproduction codeword explicitly normalizing input vector computationally expensive 
encoder decoder depicted fig 

potential advantage system separating features able scalar quantizer gain feature lower rate codebook shape feature higher dimension search complexity 
major issue arises total rate constraint best divide bits codebooks 
example rate allocation problem arises product codebooks said shortly 
important notice product quantizer mean independent quantizers component 
shape gain vq optimal lossy encoder general view coordinate time 
separate independent quantization components provides low complexity generally suboptimal encoder 
case shape gain vq optimal lossy encoder happily simple sequential operation gain quantizer scalar selection quantization levels depends result quantizer shape quantizer 
similar ideas mean removed vq mean gain shape vq :10.1.1.116.3824
general formulation product codes chan gersho 
includes number schemes dependent quantization tree structured multistage quantization discussed 
fischer pyramid vq kind shape gain vq 
case codevectors shape codebook constrained lie surface dimensional pyramid set vectors components magnitudes summing 
pyramid vq suited laplacian sources 
efficient method indexing shape codevectors needed suitable method included pyramid vq 
dimensional shape gain product quantizers usually called polar quantizers extensively developed 
dimensional source vector represented polar coordinates basic scheme codebook consists cartesian product nonuniform scalar codebook magnitude uniform scalar codebook phase 
early versions polar quantization independent quantization magnitude phase information versions better method described allowed phase quantizers resolution depends outcome magnitude quantizer 
polar quantizers gray neuhoff quantization called unrestricted 
high resolution analysis study rate distortion performance quantizers 
things analyses find optimal point density magnitude quantizer optimal bit allocation magnitude phase 
originally methods developed specifically polar quantizers 
shown bennett integral applied analyze polar quantization straightforward way 
turns gaussian source optimized conventional polar quantization gains db direct scalar quantization optimized unrestricted polar quantization gains db 
asymptotically square cells optimal dimensional point density loses db relative optimal dimensional vector quantization db product quantizers set features deemed natural decomposing vector 
famous example seen revisit transform coding 
transform coding goal section mainly discuss techniques scalar quantization dpcm transform coding discuss relationships techniques wish discuss bit allocation problem 
traditional transform coding viewed product quantizer operating transform coefficients resulting linear transform original vector 
mentioned traditional high resolution fixed rate analysis high resolution entropy constrained analysis separate lossless coding quantized transform coefficient 
asymptotic low resolution analysis performed 
actual implementations scalar quantizers combined block lossless code lossless code allowed effectively operate entire block quantized coefficients usually combining run length coding huffman arithmetic coding 
result usual high resolution analyses directly applicable 
high resolution theory shows karhunen lo transform optimal gaussian sources asymptotic low resolution analysis likewise dominant transform years discrete cosine transform dct current image video coding standards 
primary competition standards comes discrete wavelet transforms considered shortly 
reason dct lower complexity 
unstructured transform karhunen lo requires approximately operations sample small compared arithmetic complexity unstructured vq large compared approximately operations sample dct 
motivation dct sense approximates behavior karhunen lo transform certain sources 
final motivation frequency decomposition done dct mimics extent done human visual system may quantize dct coefficients perception account 
delve large literature transforms observe bit allocation important issue high resolution approximations variety allocation algorithms fixed slope considered 
method involves operating quantizers points operational distortion rate curves equal slopes 
survey methods see ch 

combinatorial optimization method 
final comment traditional transform coding code considered suboptimal dimensional quantizer constrained structure transform product code 
gains having low complexity transform codes remain popular compression systems balance performance complexity 
subband wavelet pyramid quantization subband codes wavelet codes pyramid codes intimately related cousins transform code 
oldest methods far quantization concerned pyramid code burt quite different fischer pyramid vq :10.1.1.54.299:10.1.1.54.299
burt pyramid constructed image forming gaussian pyramid successively lowpass filtering downsampling forming laplacian pyramid replaces layer gaussian pyramid residual image formed subtracting prediction layer lower resolution layers 
resulting pyramid images quantized scalar quantizers 
approximation layer reconstructed inverse quantizers reproduction decoders upsampling combining reconstructed layer lower resolution reconstructed layers 
note descends pyramid easily combines new bits layer bits produce higher resolution spatially amplitude 
pyramid code viewed original multiresolution codes 
viewed transform code entire original structure viewed linear transform original image observe number pixels roughly doubled 
subband codes decompose image separate images bank linear filters performing linear transformation data prior quantizing 
traditional subband coding filters equal roughly equal bandwidth 
wavelet codes viewed subband codes logarithmically varying bandwidths equal bandwidths filters satisfy certain properties 
subband codes late wavelet codes early field produced major contenders best speech image compression systems 
literature scope article survey far concerned transforms filters basis functions lossless coding quantization quantization 
content mention highlights 
interested reader referred book vetterli wavelets subband coding 
ieee transactions information theory vol 
october subband coding introduced context speech coding 
extension subband filtering vetterli subband filtering applied image coding woods 
early wavelet coding techniques emphasized scalar lattice vector quantization vector quantization techniques applied wavelet coefficients including tree encoding residual vector quantization methods 
major breakthrough performance complexity came provided extremely efficient embedded representation scalar quantized wavelet coefficients called embedded zerotree wavelet coding 
done jpeg primitive way zerotree approach led code sent bits transform coefficients largest magnitude sent subsequent bits describing significant coefficients greater accuracy bits originally significant coefficients significant accuracy improved 
zerotree approach extended vector quantization slight improvement comes significant cost added complexity 
rate distortion ideas optimize rate distortion tradeoffs wavelet packets minimizing lagrangian distortion code trees bit assignments 
competitive schemes demonstrated separate scalar quantization individual subbands coupled sophisticated lossless coding algorithm called stack run coding provide performance nearly 
best wavelet codes tend smart lossless codes lossless codes effectively code large vectors 
wavelet advocates may credit decomposition gains compression theory suggests fact vector entropy coding large vectors feasible 
scalar vector quantization permutation vector quantization fischer pyramid vector quantizer scalar vector quantization attempts match performance optimal entropy constrained scalar quantizer low complexity fixed rate structured vector quantizer 
derivative technique called block constrained quantization simpler easier describe 
reproduction codebook subset fold product scalar codebook 
variable length binary codewords associated scalar levels target rate dimensional codebook contains sequences quantization levels sum lengths binary codewords associated levels minimum distortion codevector dynamic programming 
alternatively essentially optimal search performed low complexity knapsack packing lagrangian approach 
output encoder sequence binary codewords corresponding codevector plus padded bits total equal simplest method requires approximately operations sample storage approximately numbers number scalar quantization levels 
original scalar vector method differs rational lengths binary codewords assigned scalar quantizer levels dynamic programming find best codevector resulting codevectors encoded kind lexicographic encoding 
gaussian sources methods attain snr db order db goal db larger high resolution analysis 
scalar vector method extends sources memory combining transform coding decorrelating approximately decorrelating transform 
tree structured quantization original simplest form dimensional tree structured vector quantizer fixed rate quantizer say rate encoding guided balanced fixed depth binary tree depth codevector associated terminal nodes leaves dimensional associated internal nodes 
quantization source vector proceeds tree structured search finding nodes stemming root node closer finding nodes stemming node closer terminal node codevector 
binary encoding codevector consists sequence binary decisions lead 
decoding done table lookup unstructured vq 
successive approximation scalar quantization yields embedded code naturally progressive structure 
method encoding requires storing tree codevectors demanding approximately twice storage unstructured codebook 
encoding requires distortion calculations tremendous decrease required full search unstructured codebook 
case squared error distortion storing computing distortion internal node may store normal hyperplane bisecting nodes stemming determine side hyperplane lies comparing inner product normal threshold stored 
reduces arithmetic complexity storage roughly half approximately operations sample vectors 
reductions storage possible described 
usual necessarily optimal greedy method designing balanced design stemming root node lloyd algorithm training set 
design stemming say left running lloyd algorithm training vectors mapped left 
scalar case tree implements quantizer optimal quantizer 
tree structuring loses design algorithm necessarily generate best possible quantizers 
multidimensional case expect greedy algorithm produce best unstructured gray neuhoff quantization vq best possible 
pretty 
observed highresolution case cells resulting mixture cubes cubes cut half cut half smaller cubes formed 
gauss gauss markov sources performances moderate high rates designed greedy algorithm fairly predicted bennett integral assuming point density optimum cells equal mixture cubes cubes cut half 
sort analysis indicates primary weakness shapes cells produces 
specifically loss relative optimal dimensional fixed rate vq ranges db db large dimensions 
part loss ratio normalized moment inertia cube best dimensional cell shape approaches db large remainder db due caused cubes cut pieces 
investigating nature cells 
experience performance complexity account competitive vq method 
example assert fast search methods find quite possibly different dimension dominates sense 
fast search approaches tree structured 
searching tree codebook matched size character way 
notable exception hierarchical table lookup vq attains considerably smaller arithmetic complexity attainable expense higher storage 
competitive terms throughput tree structured search amenable pipelining 
generalized unbalanced trees variable depth opposed fixed depth discussed larger branching factors variable branching factors 
recalled goodness original means gains substantial low resolution case variable rate coding source complex structure usual greedy algorithm exploit 
tree structured quantizer analogous classification regression tree unbalanced designed algorithms gardening metaphor growing pruning 
known cart algorithm breiman friedman olshen stone variation cart designing bears initials algorithm 
method balanced unbalanced tree leaves needed grown pruned 
grow balanced tree splitting nodes level tree splitting node time splitting node largest contribution distortion greedy fashion maximize decrease distortion increase rate 
grown tree pruned removing fig 

stage vq 
descendants internal node making leaf 
increase average distortion decrease rate 
select pruning node offers best tradeoff terms increase distortion decrease bits 
shown quite general measures distortion pruning done optimal fashion optimal subtrees decreasing rate nested see 
high rate case pruning removes leaves corresponding cells cubes cut half leaving mainly cubic cells 
wish emphasize variable rate quantization desired pruning done optimize tradeoff distortion leaf entropy 
flurry theory algorithms vector quantizers form recursive partitioning 
see example nobel olshen 
tree growing pruning see 
multistage vector quantization multistage multistep cascade residual vector quantization introduced juang gray form tree structured quantization reduced arithmetic complexity storage 
having separate reproduction codebook branch tree single codebook branches common length coding residual error accumulated point coding input vector directly 
words quantization error residual previous stage quantized usual way stage reproduction formed summing previous reproduction newly quantized residual 
example stage quantizer depicted fig 

rate multistage quantizer sum rates stages distortion simply stage 
easily seen error just stage 
multistage quantizer direct sum reproduction codebook sense contains codevectors formed summing codevectors reproduction codebooks stage 
may view kind product code sense reproduction codebook determined cartesian product stage codebooks 
product quantization complexities arithmetic storage encoding decoding sum stage quantizers plus small amount computing residuals encoder sums decoder 
contrast conventional single stage quantizer rate dimension complexities equal product stage quantizers 
total rate sum stage rates problem arises 
stage quantization fixed rate unstructured dimensional vq stages ieee transactions information theory vol 
october usually happens choosing stages rate leads best performance versus complexity tradeoff 
case complexities approximately square root single stage quantizer 
restrict attention case stages fixed rate vector quantizers dimension reason need dimension fixed rate similarity whatsoever 
words multistage quantization different kinds quantizers stages different dimensions different structures dpcm wavelet coding 
example structuring stage quantizers leads performance substantial reductions complexity 
course multistage structuring leads suboptimal vq dimension 
particular direct sum form codebook usually optimal algorithm described residual stage quantized find closest codevector direct sum codebook 
usual greedy design method uses lloyd algorithm design stage usual way design second stage minimize distortion operating errors general design optimal multistage vq greedy search 
stage vq designed way fairly 
high resolution analysis stage vq bennett integral second stage 
order apply bennett integral necessary find form probability density quantization error produced stage 
motivated asymptotic error density analysis vector quantization 
multistage quantizers improved number ways 
sophisticated greedy encoding algorithms take advantage direct sum nature codebook optimal nearly optimal searches great deal increased complexity 
sophisticated design algorithms greedy benefits 
multistage quantizers developed 
way improving multistage vq adapt stage outcome previous 
scheme introduced lee neuhoff motivated observation stage quantizer high rate say gersho conjecture stage cells approximately shape polytope normalized moment inertia source density approximately constant 
implies conditional distribution residual source vector lies th cell differs th scaling rotation cell differs just scaling rotation 
stage dependent scaling rotation done prior second stage quantization conditional distribution residual cells second stage designed distribution having compromise case stage vq 
distribution essentially uniform support region shaped second stage uniform tesselation 
net effect quantizer inherits optimal point density stage optimal cell shapes second 
high resolution case cell conditioned stage vq works essentially optimal single stage vq complexity 
direct implementation cell conditioned stage vq requires storing scale factor rotation stage cell operate stage residual quantization second stage 
inverses applied subsequently 
stage cells nearly spherical rotations gain small amount typically db may omitted 
best known lattice close best known may lattice vq second stage reduces complexity 
schemes sort developed low moderate rates gibson pan fischer 
cell conditioned stage quantizers viewed having piecewise constant point density sort proposed earlier means circumventing fact optimal vector quantizers implemented 
approach developed 
scheme adapting stage previous called codebook sharing introduced chan gersho 
approach stage finite set reproduction codebooks quantize residual depending sequence outcomes previous stages 
codebook shared subset possible sequences outcomes previous stages 
method lies conventional multistage vq stage codebook shared sequences outcomes previous stages effect different codebook sequence outcomes previous stages 
chan gersho introduced lloyd style iterative design algorithm designing shared codebooks showed controlling number rate codebooks optimize multistage vq constraint storage method effect audio coding 
larger scheme things multistage vq codebook sharing fit broad family generalized product codes introduced 
feedback vector quantization just scalar quantizers vector quantizer predictive simply replace scalars vectors predictive quantization structure depicted fig 

alternatively encoder decoder share finite set states quantizer custom designed state 
encoder decoder able track state absence channel errors state determinable knowledge initial state combined binary codewords transmitted decoder 
result finite state version predictive second stage uniformly refines stage cells point density approximately stage 
gray neuhoff quantization fig 

finite state vector quantizer 
quantizer referred finite state vector quantizer depicted fig 

little theory developed finite state quantizers variety design methods exist lloyd optimal decoder extends natural way finite state vector quantizers optimal reproduction decoder conditional expectation input vector binary codeword state :10.1.1.116.3824:10.1.1.116.3824
optimal lossy encoder easily described state chosen way ensures behavior just greedy fashion minimizes current squared error 
look ahead allowed tree trellis search pick long term minimum distortion path considered subsection 
predictive finite state vector quantizers typically memory lossy encoder memoryless lossless code independently applied successive binary codeword 
course lossless code depend state conditional previous binary codeword 
memoryless vq combined conditional lossless code conditioned previous binary codeword designed conditional entropy constraint 
simple approach works code binary path codevector source vector relative binary path previous source vector usually similar 
kind lossless coding 
address vector quantization introduced feng see way introduce memory lossy encoder vector quantizer goal attaining higher dimensional performance lower dimensional complexity 
approach addition usual reproduction codebook address codebook containing permissible sequences indices codevectors address codebook plays role outer code concatenated channel code trellis trellis encoded quantization discussed limits allowable sequences codewords inner code case way address vector quantization exploit property certain sequences codevectors probable ones contained dpcm memory lossy encoder seriously complicates theory codes explains little 
tree trellis encoded quantization channel coding inspired source coding quantization structures 
channel coding matured earlier dual nature channel source coding suggests channel code turned source code reversing order encoder decoder 
role reversal natural codes eased search requirements imposition tree trellis structure 
tree structured vector quantizers earlier systems imposed tree structure sequence symbols single vector symbols 
channel coding case encoder convolutional code input symbols shifted shift register output symbols formed linear combinations field shift register contents shifted 
sequences output symbols produced fashion depicted tree structure node tree corresponded state shift register final oldest symbol branches connecting nodes determined symbol enter shift register labeled corresponding output output symbol resulting branch taken 
goal channel decoder take sequence tree branch labels corrupted noise find minimum distance valid sequence branch labels 
accomplished tree search algorithm fano stack algorithm 
shift register finite tree redundant new nodes correspond previously seen states tree diagram merged tree trellis searched dynamic programming algorithm viterbi algorithm cf 

early algorithms tree decoding channel codes inverted form tree encoding algorithms sources jelinek anderson 
trellis channel decoding algorithms modified trellis encoding algorithms sources viterbi 
linear encoders sufficed channel coding nonlinear decoders required source coding application variety design algorithms developed designing decoder populate ieee transactions information theory vol 
october trellis searched encoder 
observe reproduction decoder finite state vq decoder trellis encoding system finite state encoder replaced minimum distortion search decoder trellis implied finite state vq decoder optimal encoding sequence inputs 
tree trellis encoded quantizers considered vq large blocklength reproduction codebook constrained possible outputs nonlinear filter finite state quantizer vector quantizer smaller dimension 
structures produce long codewords trellis structure successive reproduction symbols label branches trellis encoder just minimum distortion trellis search algorithm viterbi algorithm 
trellis coded quantization trellis coded quantization scalar vector improves traditional systems labeling trellis branches entire subsets individual reproduction levels 
primary gain resulting reduction encoder complexity level performance 
original trellis encoding systems motivated convolutional channel codes viterbi decoders trellis coded quantization motivated enormously successful coded modulation approach channel coding narrowband channels 
combinations coding wavelet coefficients yielded excellent performance image coding applications winning jpeg contest position serious contender new standard 
gaussian quantizers shannon showed gaussian source worst rate distortion function source variance showing gaussian source extremum source coding sense 
long assumed eventually proved provided robust approach quantization sense exist vector quantizers designed gaussian source average distortion provide worse distortion applied source variance 
provided approach robust vector quantization having code optimal actual source perform worse gaussian source designed 
extended extremal properties rate distortion functions sources memory showed code designed gaussian source yield essentially performance applied process covariance structure 
results essentially shannon theory viewed primarily interest high dimensional quantizers 
different approach gaussian quantizer arbitrary source popat zeger took advantage central limit theorem known structure optimal scalar quantizer gaussian random variable code general process filtering produce approximately gaussian density scalar quantizing result inverse filtering recover original 
robust quantization gaussian quantizers described robust minimax average sense vector quantizer suitably designed gaussian source yield worse average distortion source class sources second order properties 
alternative formulation robust quantization obtained dealing average distortion done places maximum distortion requirement quantizer design 
quantizer considered robust bounds maximum distortion class sources 
morris developed theory robust quantization provide conditions uniform quantizer optimum minimax sense 
viewed variation epsilon entropy goal minimize maximum distortion 
results line may 
minimax results aimed scalar quantization results apply rate dimension 
universal quantization minimax approaches provide means designing fixed rate quantizer source unknown partially known statistics quantizer designed perform worse fixed value distortion sources collection 
alternative approach greedy try design code yields nearly optimal performance regardless source collection coded 
idea universal quantization 
universal quantization universal source coding origins approach universal lossless compression developed rice plaunt dubbed rice machine idea lossless coder distinct sources running multiple lossless codes parallel choosing producing fewest bits period time sending small amount overhead inform decoder code encoder 
classic lossy universal source codes ziv proved existence universal lossy codes certain assumptions source statistics source codebook alphabets 
multiple codebook idea extend shannon source coding theorem stationary sources ergodic decomposition interpret source universal coding problem family ergodic sources 
idea easily described provides means constructing universal codes 
suppose collection dimensional codebooks codevectors designed different type local behavior 
example different codebooks image coder edges textures gradients 
union codebook contains codevectors codes total codevectors 
example equal rate rate universal code gray neuhoff quantization bits symbol small dimension moderately large 
mean necessary large dimensional vq vq product vq image coding square dimension applications vq dimension say different codes resulting rate small increase original rate original rate say universal code theory complicated ordinary code practice mean codes smaller dimension efficient separate codebooks distinct short term behavior 
subsequently variety notions fixed rate universal codes considered compared codes variable rate developed 
early development block source codes universal quantization early days viewed method developing theory practical code design algorithm 
rice machine proved practicality importance simple multiple codebook scheme handling composite sources 
works assumed encoder decoder possess copies codebooks 
zeger bist linder considered systems codebooks designed encoder coded transmitted decoder commonly done codebook replenishment 
review history universal source coding early may 
better performance tradeoffs achieved allowing rate distortion vary chou formulated universal coding problem entropy constrained vector quantization problem family sources provided existence proofs lloyd style design algorithms collection codebooks subject lagrangian distortion measure yielding fixed slope optimization fixed distortion fixed rate 
clustering codebooks originally due chou 
high resolution quantization theory study rates convergence blocklength optimal performance yielding results consistent earlier convergence results developed means linder 
fixed slope universal quantizer approach developed code structures design algorithms yang 
different approach closely resembles traditional adaptive codebook replenishment developed zhang yang wei liu 
approach dubbed gold washing involve training created removed codevectors data received auxiliary random process way tracked decoder side information 
dithering dithered quantization introduced roberts means randomizing effects uniform quantization minimize visual artifacts 
developed images limb speech jayant rabiner 
intuitively goal cause reconstruction error look additive white noise 
turns type dithering intuition true 
dithered quantizer quantizing input signal directly signal random process independent signal called dither process 
dither process usually assumed 
approaches dithering 
roberts considered subtractive dithering final reconstruction formed obvious problem need decoder possess copy dither signal 
dithering forms reproduction principal theoretical property dithering developed showed quantizer error uniformly distributed independent original input signal quantizer overload characteristic function satisfies conditions satisfied example dither signal uniform probability density function follows jayant rabiner snyder see condition implies sequence quantization errors independent 
case uniform dither remains far widely studied literature 
subtractive dither result nice mathematically promises behaved quantization noise quantization error 
impractical applications reasons 
receiver usually perfect analog link transmitter original signal sent analog form pseudorandom deterministic sequence transmitter receiver proposed roberts 
case mathematical guarantee quantization error noise properties hold genuinely random dither 
second subtractive dither signal resembles sample function memoryless random process complicated implement requiring storage dither signal high precision arithmetic perfect synchronization 
result interest study behavior quantization noise simple dithered quantizer 
subtractive dither dither capable making reconstruction error independent input signal claims contrary literature 
proper choice dithering function conditional moments reproduction error independent input signal 
practically important 
example perceived quantization noise energy constant input signal fades high intensity low intensity exhibit strongly behavior 
properties dither ieee transactions information theory vol 
october originally developed unpublished wright subsequently extended refined variety proofs :10.1.1.116.3824
necessary sufficient conditions characteristic function known ensure th moment quantization noise conditional depend sufficient condition dither signal consists sum independent uniformly distributed random variables unfortunately conditional independence moments comes expense loss fidelity 
example quantizer noise power mean squared error means power dither signal directly added quantizer error order form mean squared error 
addition role whitening quantization noise making noise moments independent input dithering played role proofs universal quantization results information theory 
example ziv showed high resolution theory uniform scalar quantization combined dithering vector lossless coding yield performance bit symbol rate distortion function 
extensions lattice quantization variations result developed zamir feder 
quantization noisy channels separation theorem information theory states nearly optimal communication information source noisy channel accomplished separately quantizing source coding source channel coding error control coding resulting encoded source reliable transmission noisy channel 
coding functions designed separately knowledge 
result point point communications limiting result sense large large complexity permitted 
wishes perform near shannon limit moderate delay multiuser situations necessary consider joint source channel codes codes jointly consider quantization reliable communication 
may necessary combine source channel codes simply jointly design 
variety code structures design methods considered purpose involve issues channel coding focus 
mention schemes viewed quantizers modified noisy channel schemes involve explicit channel codes 
general discussions 
approach designing quantizers noisy channels replace distortion measure respect quantizer optimized expected distortion noisy channel 
simple modification distortion measure allows channel statistics included optimal quantizer design formulation 
method referred channel optimized quantization quantization scalar vector trellis 
approach introduced kurtenbach scalar quantizers 
shannon source coding theorem trellis encoders distortion measure proved lloyd style design algorithm encoders provided 
lloyd algorithm vector quantizers modified distortion measure introduced studied 
method applied tree structured vq 
combined maximum likelihood detector improve performance permit progressive transmission noisy channel 
simulated annealing design quantizers 
approach joint source channel coding quantizer structure explicitly involving typical channel coding techniques design scalar vector quantizer source regard channel code resulting indices way ensures small large hamming distance channel codewords corresponds small large distortion resulting reproduction codewords essentially forcing topology channel codewords correspond resulting reproduction codewords 
codes called index assignments 
specific index assignment methods considered 
jayant introduced iterative search algorithm designing index assignments scalar quantizers extended vector quantization zeger gersho dubbed approach pseudo gray coding 
index assignment algorithms include 
binary symmetric channels certain special sources quantizers analytical results obtained 
example shown index assignment minimizes error uniform scalar quantizer channel natural binary assignment 
result remained relatively unknown generalized 
source channel codes considered key issue determination quantization rate total number channel symbols source symbol held fixed 
example quantization rate increased quantization noise decreases noise increases ability channel code protect bits reduced 
clearly optimal choice quantization rate 
issue determination rate distortion decreases optimal system total number channel uses source symbol increases 
issues addressed papers zeger hochwald zeger exponential formulas produced high resolution quantization theory exponential bounds channel coding error probability 
gray neuhoff quantization variety approaches joint source channel coding including codes channel encoder structure optimized source special decoder matched source unequal error protection better protect important lower resolution reproduction indices jointly optimized combinations source channel codes combinations channel optimized quantizers source optimized channel codes leave literature involve heavy dose channel coding ideas 
quantizing noisy sources parallel problem quantizing noisy channel quantizing noisy source 
problem seen trying compress dirty source clean reproduction doing estimation original source quantized version noise corrupted version 
underlying statistics known estimated training sequence treated quantization problem modified distortion measure distortion observation unseen original reconstruction encoded decoded conditional expectation usefulness modified distortion source coding noisy sources seen tsybakov fine obtain information theoretic bounds quantization source coding noisy sources 
berger explicitly modified distortion study shannon source coding theorems noise corrupted sources 
wolf ziv modified distortion measure squared error distortion prove optimal quantizer modified distortion decomposed cascade minimum mean squared error estimator followed optimal quantizer estimated original source 
result subsequently extended general class distortion measures include input weighted quadratic distortion gray generalized lloyd algorithm design 
related results approaches treatment rate distortion theory modified indirect distortion measures occam filters natarajan 
multiple description quantization topic closely related quantization noisy channels multiple description quantization 
problem usually formulated source coding quantization problem network easily described terms packet communications 
simplest case suppose packets information rate transmitted describe reproduction single random vector encoder receive packet wishes provide best reconstruction possible bit rate receives 
viewed network problem receiver seeing channel receiver seeing second channel third seeing channels goal optimal reconstruction total received bitrate 
clearly better having packet result reproduction distortion near shannon distortion rate function simultaneously having packets yield reproduction distortion near optimistic performance general possible 
problem tackled information theory community wolf wyner ziv developed achievable rate regions lower bounds performance 
results extended ahlswede el gamal cover zhang berger 
vaishampayan lloyd algorithm design fixed rate entropy constrained scalar quantizers multiple description problem 
highresolution quantization ideas evaluate achievable performance vaishampayan linder zamir zeger 
alternative approach multiple description quantization transform coding considered 
applications treated interesting variations applications quantization successfully analyzed designed tools described 
examples included time space patience plentiful include mismatch results quantizers designed distribution applied quantizers designed provide inputs classification detection estimation systems quantizers multiuser systems simple networks quantizers implicit finite precision arithmetic modern form roundoff error quantization analog digital digital analog converters modulators 
doubtless failed mention list suffices demonstrate rich theoretical applied fields quantization half century active development 
acknowledgment authors gratefully acknowledge helpful comments corrections suggestions colleagues students reviewers 
particular assistance gersho girod kashyap linder moo shtarkov vetterli zeger 
wise notes optimal quantization proc 
int 
conf 
communications june vol 
pp 

abut vector quantization ieee reprint collection 
piscataway nj ieee press 
collin block encoding application data compression pcm speech proc 
canadian communications conf 
montreal que canada pp 



debray spectral distance measure applied optimum design dpcm coders predictors proc 
ieee int 
conf 
acoustics speech signal processing icassp denver pp 

eriksson optimization lattices quantization ieee trans 
inform 
theory vol 
pp 
sept 
appears lattice quantization part dept inform 
ieee transactions information theory vol 
october theory chalmers univ technol goteborg sweden rep oct 
ahlswede rate distortion region multiple descriptions excess rate ieee trans 
inform 
theory vol 
pp 
nov 
ahmed natarajan rao discrete cosine transform ieee trans 
comput vol 
pp 

useful approximation optimum quantization ieee trans 
commun vol 
com pp 
june 
anderberg cluster analysis applications 
san diego ca academic 
anderson tree encoding speech ieee trans 
inform 
theory vol 
pp 

anderson jelinek cycle algorithm source coding fidelity criterion ieee trans 
inform 
theory vol 
pp 
jan 
mathieu daubechies image coding vector quantization wavelet transform domain proc 
ieee int 
conf 
acoustics speech signal processing icassp albuquerque nm apr pp 

mathieu image coding lattice vector quantization wavelet coefficients proc 
ieee int 
conf 
acoustics speech signal processing icassp toronto ont canada may vol 
pp 

mathieu daubechies image coding wavelet transform ieee trans 
image processing vol 
pp 
apr 
aravind gersho low rate image coding finite state vector quantization proc 
int 
conf 
acoustics speech signal processing icassp tokyo japan pp 

image compression vector quantization finite memory opt 
eng vol 
pp 
july 
quantization error predictive coders ieee trans 
commun vol 
com pp 
apr 
glu gray design predictive trellis waveform coders generalized lloyd algorithm ieee trans 
commun vol 
com pp 
nov 
design joint source channel trellis waveform coders ieee trans 
inform 
theory vol 
pp 
nov 
baker gray image compression nonadaptive spatial vector quantization conf 
rec 
th asilomar conf 
circuits systems computers asilomar ca nov pp 

differential vector quantization achromatic imagery proc 
int 
picture coding symp mar pp 

balakrishnan pearlman lu variable rate treestructured vector quantizers ieee trans 
inform 
theory vol 
pp 
july 
block constrained methods fixed rate entropy constrained quantization ph dissertation univ michigan ann arbor jan 
neuhoff new methods fixed rate quantization proc 
conf 
information sciences systems princeton nj mar pp 

unpublished notes 
block constrained quantization asymptotic analysis di macs ser 
discr 
math 
theoretical comput 
sci vol 
pp 

new fixed rate quantization scheme arithmetic coding proc 
ieee int 
symp 
information theory san antonio tx jan 
block constrained methods fixed rate entropy coded scalar quantization ieee trans 
inform 
theory submitted publication 
ball data analysis social sciences details proc 
fall joint computing conf 
washington dc spartan pp 

sol mathieu pyramidal lattice vector quantization multiscale image coding ieee trans 
image processing vol 
pp 
july 
barnes new multiple path search technique residual vector quantizers proc 
data compression conf 
snowbird ut pp 

barnes frost vector quantizers direct sum codebooks ieee trans 
inform 
theory vol 
pp 
mar 
barnes rizvi advances residual vector quantization review ieee trans 
image processing vol 
pp 
feb 
barnes tran leung statistics fixedpoint roundoff error ieee trans 
acoust speech signal processing vol 
assp pp 
june 
barnes sloane optimal lattice quantizer dimensions siam alg 
discr 
methods vol 
pp 
mar 
bartlett linder lugosi minimax distortion redundancy empirical quantizer design ieee trans 
inform 
theory vol 
pp 
sept 
bath robust memoryless quantization minimum signal distortion ieee trans 
inform 
theory vol 
pp 


vaishampayan asymptotic performance multiple description codes ieee trans 
inform 
theory vol 
pp 
mar 
bei gray improvement minimum distortion encoding algorithm vector quantization ieee trans 
commun vol 
com pp 
oct 
simulation vector trellis encoding systems ieee trans 
commun vol 
com pp 
mar 
bello lincoln gish statistical delta modulation proc 
ieee vol 
pp 
mar 
ben david performance vector quantizer channel errors signal proc 
vi theories applications proc 
eusipco pp 

bennett spectra quantized signals bell syst 
tech 
vol 
pp 
july 
bentley multidimensional binary search trees associative searching commun 
assoc 
comput 
mach pp 
sept 
berger rate distortion theory sources alphabet memory inform 
contr vol 
pp 

rate distortion theory 
englewood cliffs nj prentice hall 
optimum quantizers permutation codes ieee trans 
inform 
theory vol 
pp 
nov 
minimum entropy quantizers permutation codes ieee trans 
inform 
theory vol 
pp 
mar 
berger jelinek wolf permutation codes sources ieee trans 
inform 
theory vol 
pp 
jan 
image video compression standards 
boston ma kluwer 
black pulse code modulation bell lab 
rec vol 
pp 
july 
blahut computation channel capacity rate distortion functions ieee trans 
inform 
theory vol 
pp 
july 
breiman friedman olshen stone classification regression trees 
belmont ca wadsworth 
dither thesis elec 
eng 
dept univ utah salt lake city ut aug 
bruce optimum quantization stationary signals ieee int 
conv 
rec pt 
pp 

random quantization dimensions ieee trans 
inform 
theory vol 
pp 
mar 
note optimal multidimensional ieee trans 
inform 
theory vol 
mar 
results asymptotic performance quantizers ieee trans 
inform 
theory vol 
pp 
mar 
note absolute epsilon entropy ieee trans 
inform 
theory vol 
pp 
jan 
gallagher jr note optimum quantization ieee trans 
inform 
theory vol 
pp 
may 
quantization schemes bivariate gaussian random variables ieee trans 
inform 
theory vol 
pp 
sept 
dimensional quantization bivariate circularly symmetric densities ieee trans 
inform 
theory vol 
pp 
nov 
properties uniform step size quantizers ieee trans 
inform 
theory vol 
pp 
sept 
wise multidimensional asymptotic quantization theory th power distortion measures ieee trans 
inform 
theory vol 
pp 
mar 
buhmann hnel vector quantization complexity costs ieee trans 
inform 
theory vol 
pp 
july 
burt adelson laplacian pyramid compact image code ieee trans :10.1.1.54.299
commun vol 
com pp 
apr 
gray neuhoff quantization gray gray jr optimal quantizations coefficient vectors lpc speech joint meet 
acoustical society america acoustical society japan honolulu hi dec 
gray jr gray optimal quantizations coefficient vectors lpc speech proc 
ieee int 
conf 
acoustics speech signal processing icassp washington dc apr pp 

speech coding vector quantization ieee trans 
acoust speech signal processing vol 
assp pp 
oct 
simple class asymptotically optimal quantizers ieee trans 
inform 
theory vol 
pp 
sept 
candy benjamin structure quantization noise sigma delta modulation ieee trans 
commun vol 
com pp 
sept 
candy eds 
oversampling delta sigma data converters 
new york ieee press 
variations theme gallager image text compression storer ed 
boston ma kluwer pp 

westin esposito optimum quantization minimum distortion proc 
int conf pp 

chou hierarchical vector quantization perceptually weighted block transforms proc 
compression conf 
snowbird ut 
los alamitos ca ieee comp 
soc 
press pp 

applications rate distortion theory bandwidth compression ph dissertation elec 
eng 
dept univ california los angeles 
low rate voice compression system abstracts papers ieee int 
symp 
information theory oct 

chan 
po complexity reduction technique image vector quantization ieee trans 
image processing vol 
pp 
july 

chan gersho high fidelity audio transform coding vector quantization proc 
ieee int 
conf 
acoustics speech signal processing icassp albuquerque nm apr vol 
pp 

constrained storage vector quantization high fidelity audio transform coding proc 
ieee int 
conf 
speech signal processing icassp toronto ont canada may pp 

enhanced multistage vector quantization joint codebook design ieee trans 
commun vol 
pp 
nov 
generalized product code vector quantization family efficient techniques signal compression digital signal processing vol 
pp 


chan gupta gersho enhanced multistage vector quantization joint codebook design ieee trans 
commun vol 
pp 
nov 

chan siu search optimal searching sequence vq encoding ieee trans 
commun vol 
pp 
dec 
chang gray gradient algorithms designing predictive vector quantizers ieee trans 
acoust speech signal processing vol 
assp pp 
aug 
chang may gray hierarchical vector quantizers table lookup encoders proc 
ieee int 
conf 
communications june vol 
pp 

chen dimensional optimum quantizers proc 
ieee int 
conf 
acoustics speech signal processing icassp ct pp 


cheng gersho shoham fast search algorithms vector quantization pattern matching proc 
ieee int 
conf 
acoust ics speech signal processing icassp san diego ca mar pp 


cheng gersho fast codebook search algorithm nearest neighbor pattern matching proc 
ieee int 
conf 
acoustics speech signal processing icassp tokyo japan apr vol 
pp 

chou code clustering weighted universal vq applications proc 
ieee int 
symp 
information theory budapest hungary 
distortion vector quantizers trained vectors decreases optimum ia proc 
ieee int 
symp 
information theory trondheim norway 
chou gray vector quantization approach universal noiseless coding quantization ieee trans 
inform 
theory vol 
pp 
july 
chou gray entropy constrained vector quantization ieee trans 
acoust speech signal processing vol 
pp 
jan 
optimal pruning applications tree structured source coding modeling ieee trans 
inform 
theory vol 
pp 
mar chou conditional entropy constrained vector quantization linear predictive coefficients proc 
int 
conf 
acoustics speech signal processing pp 

chow berger failure successive refinement symmetric gaussian mixtures ieee trans 
inform 
theory vol 
pp 
jan 
model power spectral density quantization noise ieee trans 
acoust speech signal processing vol 
assp pp 
aug 
clarke transform coding images 
orlando fl academic 
distortion pulse count modulation system trans vol 
pp 

pcm distortion analysis elec 
eng pp 
nov 
cohn ladner theory practice vector quantizers trained small training sets ieee trans 
pattern anal 
machine intell vol 
pp 
jan 
coifman wickerhauser 
entropy algorithms best basis selection ieee trans 
inform 
theory vol 
pp 
mar 
conway sloane voronoi regions lattices second moments polytopes quantization ieee trans 
inform 
theory vol 
pp 
mar 
fast quantizing decoding algorithms lattice quantizers codes ieee trans 
inform 
theory vol 
pp 
mar 
fast encoding method lattice codes quantizers ieee trans 
inform 
theory vol 
pp 
nov 
sphere packings lattices groups 
new york springer verlag 
gray vetterli vector quantization image subbands survey ieee trans 
image processing vol 
pp 
feb 
gray olshen training sequence size vector quantizer performance proc 
th annu 
asilomar conf 
signals systems computers pacific grove ca nov pp 

tree structured vector quantization significance map wavelet image coding proc 
ieee data compression conf 
dcc storer cohn eds 
los alamitos ca ieee comp 
soc 
press mar 
cover thomas elements information theory 
chichester wiley 
cox note grouping amer 
statist 
assoc vol 

horwitz palermo palermo minimization mean squared error data transmitted group codes ieee trans 
inform 
theory vol 
pp 
jan 
webber flanagan digital coding speech sub bands bell syst 
tech 
vol 
pp 
oct 
csisz generalized entropy quantization problems proc 
th prague conf pp 

csisz rner information theory coding theorems discrete memoryless systems 
new york academic 
gersho vector predictive coding speech kb ieee trans 
commun vol 
com pp 
july 
cutler differential quantization communication signals patent july 
problem optimum stratification 
vol 
pp 

problem optimum stratification ii 
vol 
pp 

reeves th anniversary pulse code modulation ieee spectrum pp 
may 
jayant algorithm assigning binary indices codevectors multidimensional quantizers proc 
ieee transactions information theory vol 
october ieee int 
conf 
communications june pp 

joint source channel coding variable length codes proc 
ieee data compression conf storer cohn eds 
los alamitos ca computer soc 
press mar pp 

davis hellman tree coding fidelity criterion ieee trans 
inform 
theory vol 
pp 
july 
information rates data compression ieee session 
davisson gray eds data compression vol 
benchmark papers electrical engineering computer science 
pa hutchinson ross 
davisson leon garcia neuhoff new results coding stationary sources ieee trans 
inform 
theory vol 
pp 
mar 
davisson direct proof coding theorem discrete sources memory ieee trans 
inform 
theory vol 
pp 
may 
delta modulation method pcm transmission unit code philips res 
vol 

french patent aug 
devore jawerth lucier image compression wavelet transform coding ieee trans 
inform 
theory vol 
pp 
mar 
devroye gy rfi lugosi probabilistic theory pattern recognition 
new york springer 
dick berger jelinek tree encoding gaussian sources ieee trans 
inform 
theory vol 
pp 
may 
diday simon clustering analysis digital pattern recognition fu ed 
new york springer verlag 
tsybakov information transmission additional noise ire trans 
inform 
theory vol 
pp 

dunham gray joint source noisy channel trellis encoding ieee trans 
inform 
theory vol 
pp 
july 
ostendorf dunham gray algorithm design labeled transition finite state vector quantizers ieee trans 
commun vol 
com pp 
jan 
dunn performance class dimensional quantizers gaussian source proc 
columbia symp 
signal transmission processing columbia univ new york pp 
reprinted data compression benchmark papers electrical engineering computer science vol 
davisson gray eds 
pa hutchinson ross 
chou gray variable rate source coding theorems stationary sources ieee trans 
inform 
theory vol 
pp 
nov 
el gamal cover achievable rates multiple descriptions ieee trans 
inform 
theory vol 
pp 
nov 
el gamal wei simulated annealing design codes ieee trans 
inform 
theory vol 
pp 
jan 
elias predictive coding ph dissertation harvard univ cambridge ma 
predictive coding ii ire trans 
inform 
theory vol 
pp 
mar 
bounds performance optimum quantizers ieee trans 
inform 
theory vol 
pp 
mar 
bounds asymptotes performance multivariate quantizers ann 
math 
statist vol 
pp 

gray unified approach encoding clean noisy sources means waveform autoregressive vector quantization ieee trans 
inform 
theory vol 
pp 
july 
equitz new vector quantization clustering algorithm ieee trans 
acoust speech signal processing vol 
pp 
oct 
equitz cover successive refinement information ieee trans 
inform 
theory vol 
pp 
mar 
result delay information transmission abstracts ieee int 
symp 
information theory italy june 
eriksson lattice quantization part ii rep dept inform 
theory chalmers univ technol goteborg sweden oct 
glu fisher image quality measures performance ieee trans 
commun vol 
pp 
dec 
glu forney jr lattice trellis quantization lattice trellis bounded codebooks high rate theory memoryless sources ieee trans 
inform 
theory vol 
pp 
jan 
study vector quantization noisy channels ieee trans 
inform 
theory vol 
pp 
july 
performance complexity channel optimized vector quantizers speech recognition coding new advances trends berlin germany springer pp 

lin performance entropy constrained block transform quantizers ieee trans 
inform 
theory vol 
pp 
sept 
optimal quantizer performance class non gaussian memoryless sources ieee trans 
inform 
theory vol 
pp 
may 
rate distortion performance dpcm schemes ieee trans 
inform 
theory vol 
pp 
may 
vaishampayan optimal quantizer design noisy channels approach combined source channel coding ieee trans 
inform 
theory vol 
pp 
nov 
toth der auf der und im raum 
berlin germany springer verlag 
sur la representation une population par un nombre fini elements acta math 
acad 
sci 
hung vol 
pp 

feng dynamic address vector quantization rgb color images proc 
inst 
elec 
eng part commun 
speech vision vol 
pp 
aug 
fine properties optimal digital system applications ieee trans 
inform 
theory vol 
pp 
oct 
optimum mean square quantization noisy input ieee trans 
inform 
theory vol 
pp 
apr 
response particular nonlinear system feedback random processes ieee trans 
inform 
theory vol 
pp 
mar 
fischer pyramid vector quantizer ieee trans 
inform 
theory vol 
pp 
july 
geometric source coding vector quantization ieee trans 
inform 
theory vol 
pp 
july 
fischer wang trellis coded vector quantization ieee trans 
inform 
theory vol 
pp 
nov 
fischer wang entropy constrained trellis coded quantization ieee trans 
inform 
theory vol 
pp 
mar 
fix rate distortion functions continuous alphabet memoryless sources ph dissertation univ michigan ann arbor 
flanagan frost read nelson vector quantization codebook generation simulated annealing proc 
int 
conf 
acoustics speech signal processing glasgow scotland may pp 

fleischer sufficient conditions achieving minimum distortion quantizer ieee int 
conv 
rec pp 

principal points biometrika vol 
pp 

cluster analysis multivariate data efficiency vs interpretability classification biometrics vol 

forney jr viterbi algorithm proc 
ieee vol 
pp 
mar 
foster gray finite state vector quantization abstracts ieee int :10.1.1.116.3824
symp 
information theory les arcs france june 
foster gray ostendorf dunham finite state vector quantization waveform coding ieee trans 
inform 
theory vol 
pp 
may 
friedman baskett algorithm finding nearest neighbors ieee trans 
comput vol 
pp 
oct 
frost barnes xu design performance residual quantizers proc 
data compression conf storer reif eds 
los alamitos ca ieee comp 
soc 
press apr pp 

slepian optimal finite state digital transmission systems ieee trans 
inform 
theory vol 
pp 
mar 
gabor recursive source coding 
new york springer verlag 
gray neuhoff quantization gallager information theory reliable communication 
new york wiley 
variations theme huffman ieee trans 
inform 
theory vol 
pp 
nov 
gallagher jr discrete spectral phase coding ieee trans 
inform 
theory vol 
pp 
sept 
quantizing schemes discrete fourier transform random time series ieee trans 
inform 
theory vol 
pp 
mar 
gallagher properties minimum mean squared error block quantizers ieee trans 
inform 
theory vol 
pp 
jan 
gao chen belzer comparison iv leech lattices image subband quantization proc 
ieee data compression conf storer cohn eds 
los alamitos ca ieee comp 
soc 
press mar pp 

gardner rao theoretical analysis high rate vector quantization lpc parameters ieee trans 
speech audio processing vol 
pp 
sept 
garey johnson complexity generalized lloyd max problem ieee trans 
inform 
theory vol 
pp 
mar 
de garrido lu pearlman conditional vector quantization frame difference subband signals proc 
ieee int 
conf 
image processing austin tx pt 
pp 

analysis delayed delta modulation ieee trans 
inform 
theory vol 
pp 
july 
analysis adaptive differential pcm stationary gauss markov input ieee trans 
inform 
theory vol 
pp 
may 
gersho stochastic stability delta modulation bell syst 
tech 
vol 
pp 
apr 
principles quantization ieee trans 
circuits syst vol 
cas pp 
july 
asymptotically optimal block quantization ieee trans 
inform 
theory vol 
pp 
july 
optimal nonlinear interpolative vector quantization ieee trans 
commun vol 
pp 
sept 
gersho vector quantization pattern matching technique speech coding ieee commun 
mag vol 
pp 
dec 
gersho gray vector quantization signal compression 
boston ma kluwer 
gersho image coding vector quantization proc 
int 
conf 
acoustics speech signal processing paris france apr vol 
pp 

gibson adaptive prediction speech differential encoding systems proc 
ieee vol 
pp 
apr 
gibson lattice quantization adv 
electron 
electron phys vol 
pp 

collected papers digital audio bit rate reduction 
new york audio eng 
soc 
girod rate constrained motion estimation visual communication image processing proc 
spie katsaggelos ed sept vol 
pp 

girod gray vetterli image video coding part past image multidimensional signal processing signal proc 
mag chellappa girod munson jr vetterli eds mar pp 

gish optimum quantization random sequences ph dissertation harvard univ cambridge ma mar 
gish pierce asymptotically efficient quantizing ieee trans 
inform 
theory vol 
pp 
sept 
analog source digitization comparison theory practice ieee trans 
inform 
theory vol 
pp 
apr 
goldberg sun image sequence coding vector quantization ieee trans 
commun vol 
com pp 
july 
goldstein quantization noise bell telephone lab 
tech 
memo oct 
gonzales woods digital image processing 
reading ma addison wesley 
goodall telephony pulse code modulation bell syst 
tech 
vol 
pp 
july 
goodman simulated annealing design transmission codes analogue sources electron 
lett vol 
pp 
may 
goyal optimal multiple description transform coding gaussian vectors proc 
data compression conf storer cohn eds 
los alamitos ca comp 
soc 
press mar apr pp 

gray information rates autoregressive processes ieee trans 
inform 
theory vol 
pp 
mar 
new class lower bounds information rates stationary sources conditional rate distortion functions ieee trans 
inform 
theory vol 
pp 
july 
vector quantization ieee assp mag vol 
pp 
apr 
oversampled sigma delta modulation ieee trans 
commun vol 
com pp 
apr 
quantization noise spectra ieee trans 
inform 
theory vol 
pp 
nov 
source coding theory 
boston ma kluwer 
entropy information theory 
new york springer verlag 
combined compression segmentation images proc 
int 
workshop mobile multimedia communication seoul korea sept oct 
gray matsuyama gray jr source coding speech compression proc 
int 
conf 
los angeles ca nov vol 
xiv pp 

gray source coding theorems ergodic assumption ieee trans 
inform 
theory vol 
pp 
july 
gray gray jr asymptotically optimal quantizers ieee trans 
inform 
theory vol 
pp 
feb 
gray gray jr optimal speech compression proc 
th asilomar conf 
circuits systems computers pacific grove ca 
gray multiple local optima vector quantizers ieee trans 
inform 
theory vol 
pp 
nov 
gray linde vector quantizers predictive quantizers gauss markov sources ieee trans 
commun vol 
com pp 
feb 
gray park andrews tiling shapes image vector quantization proc 
rd int 
conf 
advances commun 
control systems iii victoria bc canada sept 
gray jr dithered quantizers ieee trans :10.1.1.116.3824
inform 
theory vol 
pp 
may 
gray wyner source coding simple networks bell syst 
tech 
vol 
pp 
nov 
guan kamel equal average hyperplane partitioning method vector quantization image data patt 
recogn 
lett vol 
pp 
oct 
hall ball isodata novel method data analysis pattern classification stanford res 
inst menlo park ca tech 
rep 
hahn mathews distortion limited vector quantization proc 
data compression conf dcc 
los alamitos ca ieee comp 
soc 
press pp 

hagen robust vector quantization linear mappings block codes proc 
ieee int 
symp 
information theory san antonio tx jan 
design methods vq linear mappings block codes proc 
ieee int 
symp 
information theory trondheim norway june 
hang haskell interpolative vector quantization color images ieee trans 
commun vol 
pp 


hang woods predictive vector quantization images ieee trans 
commun vol 
com pp 
nov 
messerschmitt predictive vector quantization proc 
int 
conf 
acoustics speech signal processing san diego ca mar vol 
pp 

harrison experiments linear prediction television bell syst 
tech 
vol 
pp 
july 
hartigan clustering algorithms 
new york wiley 
haskell computation bounding rate distortion functions ieee trans 
inform 
theory vol 
pp 
sept 
hayashi differential pulse code modulation wiener process ieee trans 
commun vol 
com pp 
june 
differential pulse code modulation stationary gaussian inputs ieee trans 
commun vol 
com pp 
aug 
hilbert cluster compression algorithm joint clustering data ieee transactions information theory vol 
october compression concept jet propulsion lab pasadena ca publication dec 

ho gersho variable rate multi stage vector quantization image coding proc 
ieee int 
conf 
acoustics speech signal processing icassp pp 

hochwald zeger tradeoff source channel coding ieee trans 
inform 
theory vol 
pp 
sept 
hsieh lu chang fast codebook generation algorithm vector quantization images patt 
recogn 
lett vol 
pp 

hsieh chang lossless compression vq index search order coding ieee trans 
image processing vol 
pp 
nov 
huang quantization correlated random variables ph dissertation school engi yale univ new haven ct 

huang block quantization correlated gaussian random variables ieee trans 
commun vol 
com pp 
sept 
huang chen fast encoding algorithm vq encoding electron 
lett vol 
pp 
sept 
huang optimum binary code mit res 
lab 
electron quart 
progr 
rep pp 
july 
huffman method construction minimum redundancy codes proc 
ire vol 
pp 
sept 
hui lyons neuhoff reduced storage vq secondary quantization ieee trans 
image processing vol 
pp 
apr 
hui neuhoff asymptotic analysis optimum uniform scalar quantizers generalized gaussian distributions proc 
ieee int 
symp 
information theory trondheim norway june 
overload distortion negligible uniform scalar quantization proc 
ieee int 
symp 
information theory ulm germany july 
asymptotic analysis optimal fixed rate uniform scalar quantization ieee trans 
inform 
theory submitted publication 
complexity scalar quantization proc 
ieee int 
symp 
information theory whistler bc canada sept 
itakura maximum prediction residual principle applied speech recognition ieee trans 
acoust speech signal processing vol 
assp pp 
feb 
itakura saito analysis synthesis telephony maximum likelihood method proc 
th int 
congr 
acoustics tokyo japan aug pp 

statistical method estimation speech spectral density formant frequencies electron 
commun 
japan vol 
pp 

calculated quantizing noise single integration coders bell syst 
tech 
vol 
pp 
sept 
jain fundamentals digital image processing 
englewood cliffs nj prentice hall 
pcm systems ieee trans 
commun vol 
com pp 
jan 
multidimensional group analysis botany vol 
pp 

jayant digital coding speech waveforms pcm dpcm dm quantizers proc 
ieee vol 
pp 
may jayant noll digital coding waveforms principles applications speech video 
englewood cliffs nj prentice hall 
jayant rabiner application dither quantization speech signals bell syst 
tech 
vol 
pp 
july aug 
jelinek evaluation rate distortion functions low distortions proc 
ieee lett vol 
pp 
nov 
tree encoding memoryless time discrete sources fidelity criterion ieee trans 
inform 
theory vol 
pp 
sept 
jelinek anderson tree encoding information sources ieee trans 
inform 
theory vol 
pp 
jan 
jeong gibson uniform piecewise uniform lattice vector quantization memoryless gaussian laplacian sources ieee trans 
inform 
theory vol 
pp 
may 
image coding uniform piecewise uniform vector quantizers ieee trans 
inform 
theory vol 
pp 
may 
johnston transform coding audio signals perceptual noise criteria ieee select 
areas commun vol 
pp 
feb 
joshi new mmse encoding algorithm vector quantization proc 
ieee int 
conf 
acoust 
speech signal processing icassp toronto ont canada pp 


juang gray jr multiple stage vector quantization speech coding proc 
int 
conf 
speech signal processing icassp paris france apr vol 
pp 

new results robust quantization ieee trans 
commun pp 
aug 

baker sullivan 
chiu recursive optimal pruning applications tree structured vector quantizers ieee trans 
image processing vol 
pp 
apr 
generalization davisson universal variable rate coding theorem ieee trans 
inform 
theory vol 
pp 
nov 
block coding ergodic source relative zero valued fidelity criterion ieee trans 
inform 
theory vol 
pp 
july 
unified approach weak universal source coding ieee trans 
inform 
theory vol 
pp 
nov 
exponential rate convergence lloyd method ieee trans 
inform 
theory vol 
pp 
mar 
stochastic stability feedback quantization schemes ieee trans 
inform 
theory vol 
pp 
mar 
history source coding inform 
theory soc 
vol 
pp 

survey theory source coding fidelity criterion ieee trans 
inform 
theory vol 
pp 
sept 
dunham type stochastic stability class encoding schemes ieee trans 
inform 
theory vol 
pp 
nov 
new results optimal entropy constrained quantization ieee trans 
inform 
theory vol 
pp 
sept 
kim side match overlap match vector quantizers images ieee trans 
image processing vol 
pp 
apr 
hadamard transform tool index assignment ieee trans 
inform 
theory vol 
pp 
july 
kolmogorov shannon theory information transmission case continuous signals ieee trans 
inform 
theory vol 
pp 
sept 
construction optimum vector quantizers simulated annealing trans 
inst 
electron inform 
commun 
eng 
vol 
pp 
jan 
kohonen self organization associative memory rd ed 
berlin germany springer verlag 
lev hierarchical coding discrete sources probl 

inform vol 
pp 
july sept 
estimation mean error discrete scheme probl 

inform vol 
pp 
july sept 
koski statistics error predictive coding stationary ornstein uhlenbeck processes ieee trans 
inform 
theory vol 
pp 
may 
koski 
persson quantizer distortion upper bound exponential entropy ieee trans 
inform 
theory vol 
pp 
july 
chung smith subband image coding entropy constrained residual vector quantization inform 
processing manag vol 
pp 

conditional entropy constrained residual vq application image coding ieee trans 
image processing vol 
pp 
feb 
smith barnes image coding entropy constrained residual vector quantization ieee trans 
image processing vol 
pp 
oct 
necessary conditions optimality variable rate residual vector quantizers ieee trans 
inform 
theory vol 
pp 
nov 
kramer mathews linear coding transmitting set correlated signals ire trans 
inform 
theory vol 
pp 
sept 
statistics television signals bell syst 
tech 
vol 
pp 
july 
gray neuhoff quantization krishnamurthy melton chen neural networks vector quantization speech images ieee select 
areas commun vol 
pp 
oct 
piecewise uniform vector quantizers ieee trans 
inform 
theory vol 
pp 
sept 
construction vector quantizers noisy channels electron 
eng 
japan vol 
pp 
translated vol 
pp 
jan 
kurtenbach ieee trans 
commun 
technol vol 
com pp 
apr 
structured fixed rate vector quantizer derived variable length scalar quantizer 
memoryless sources 
ii 
vector sources ieee trans 
inform 
theory vol 
pp 
may 
role mismatch rate distortion theory ieee trans 
inform 
theory vol 
pp 
jan 

lee 
chen fast closest codeword search algorithm vector quantization proc 
inst 
elec 
eng vis 
image signal processing vol 
pp 
june 
fast search algorithm vector quantization mean pyramids codewords ieee trans 
commun vol 
pp 
feb apr 
lee asymptotic quantization error cell conditioned vector quantization ph dissertation univ michigan ann arbor dec 
lee neuhoff conditionally corrected stage vector quantization conf 
information sciences systems princeton nj mar pp 

asymptotic analysis stage vector quantization ieee int 
symp 
information theory budapest hungary june 
asymptotic distribution errors scalar vector quantizers ieee trans 
inform 
theory vol 
pp 
mar 
lee neuhoff paliwal cell conditioned vector quantization speech proc 
ieee int 
conf 
acoustics speech signal processing icassp toronto ont may vol 
pp 

levenshtein binary codes capable correcting deletions insertions reversals sov 
phys dokl vol 
pp 

lewis knowles image compression wavelet transform ieee trans 
image processing vol 
pp 
apr 
li gray asymptotic performance vector quantizers perceptual distortion measure ieee int 
symp 
information theory ulm germany june full submitted publication 
preprint available online stanford edu gray compression html 
limb design dithered waveforms quantized visual signals bell syst 
tech 
vol 
pp 
sept 
linde gray algorithm vector quantizer design ieee trans 
commun vol 
com pp 
jan 
linde gray fake process approach data compression ieee trans 
commun vol 
com pp 
june 
linder asymptotically optimal quantization probl 
contr 
inform 
theory vol 
pp 

linder lugosi zeger rates convergence source coding theorem empirical quantizer design universal lossy source coding ieee trans 
inform 
theory vol 
pp 
nov 
linder zamir asymptotic tightness shannon lower bound ieee trans 
inform 
theory vol 
pp 
nov 
high resolution source coding distortion measures rate distortion function proc 
ieee int 
symp 
information theory ulm germany june 
submitted publication ieee trans 
inform 
theory 
linder zamir zeger multiple description rate region high resolution source coding proc 
data compression conf storer cohn eds 
los alamitos ca comp 
soc 
press mar apr 
high resolution source coding distortion measures multidimensional ieee trans 
inform 
theory submitted publication 
linder zeger asymptotic entropy constrained performance tessellating universal randomized lattice quantization ieee trans 
inform 
theory vol 
pp 
mar 
evaluation epsilon entropy random variables small epsilon probl 
inform 
vol 
pp 
translated probl 

inform vol 
pp 

quantization dither theoretical survey audio eng 
soc vol 
pp 
may 
liu yang zhang fixed slope universal sequential algorithm lossy source coding gold washing mechanism proc 
rd annu 
allerton conf 
communication control computing monticello il urbana champaign il oct pp 

lloyd squares quantization pcm unpublished bell lab 
tech 
note portions institute mathematical statistics meet atlantic city nj sept 
ieee trans 
inform 
theory special issue quantization vol 
pp 
mar 
rate versus fidelity binary source bell syst 
tech 
vol 
pp 
mar 
lo cham searching algorithm efficient vq encoding images proc 
inst 
elec 
eng vis 
image signal processing vol 
pp 
oct 
gray high resolution quantization theory vector quantizer advantage ieee trans 
inform 
theory vol 
pp 
sept 
lowry millar binary search trees vector quantization proc 
ieee int 
conf 
acoustics speech signal processing dallas tx pp 

lugosi nobel consistency data driven histogram methods density estimation classification ann 
statist vol 
pp 

measuring comparison vol 
pp 
polish 
luttrell self supervised training hierarchical vector quantizers ii int 
conf 
artificial neural networks london iee conf 
publ 
pp 

lyons fundamental limits low rate transform codes ph dissertation univ michigan ann arbor 
lyons neuhoff coding theorem low rate transform codes proc 
ieee int 
symp 
information theory san antonio tx jan 
strongly weakly universal source coding proc 
conf 
information science systems baltimore md johns hopkins univ pp 

macqueen methods classification analysis multivariate observations proc 
th berkeley symp 
mathematical statistics probability vol 
pp 

makhoul gish vector quantization speech coding proc 
ieee vol 
pp 
nov 
entropy constrained trellis coded quantization ieee trans 
commun vol 
pp 
jan 
fischer trellis coded quantization memoryless gauss markov sources ieee trans 
commun vol 
pp 
jan 
fischer gibson predictive trellis coded quantization speech ieee trans 
acoust speech signal processing vol 
pp 
jan 
delta modulation wiener process ieee trans 
commun vol 
com pp 
nov 
mathews vector quantization images vi distortion measure proc 
int 
conf 
image processing washington dc oct vol 
pp 

vector quantization vi distortion measure ieee signal processing lett vol 
pp 

max quantizing minimum distortion ire trans 
inform 
theory vol 
pp 
mar 
mcdonald signal noise idle channel performance dpcm systems particular application voice signals bell syst 
tech 
vol 
pp 
sept 
mclaughlin neuhoff ashley optimal binary index assignments class equiprobable scalar vector quantizers ieee trans 
inform 
theory vol 
pp 
nov 
hes zeger binary lattice vector quantization linear block codes affine index assignments ieee trans 
inform 
theory vol 
pp 
jan 
optimum quantizer algorithm real time block quantizing proc 
ieee int 
conf 
acoustics speech signal pp 

ieee transactions information theory vol 
october miller rose combined source channel vector quantization deterministic annealing ieee trans 
commun vol 
pp 
feb apr 
issues related fixed rate pruned tree structured vector quantizers ieee trans 
inform 
theory vol 
pp 

neuhoff theory lattice fine coarse vector quantization ieee trans 
inform 
theory vol 
pp 
july 
time memory tradeoffs vector quantizer codebook searching decision trees ieee trans 
speech audio processing vol 
pp 
oct 
neuhoff stark fine coarse vector quantization ieee trans 
signal processing vol 
pp 
july 
moo neuhoff asymptotic analysis fixed rate lattice vector quantization proc 
int 
symp 
information theory applications victoria bc canada sept pp 

uniform polar quantization revisited published proc 
ieee int 
symp 
information theory cambridge ma aug 
morris robust quantization discretetime signals independent samples ieee trans 
commun vol 
com pp 

fast vector quantization algorithm proc 
symp 
information theory applications inspec 
fast vector quantization algorithm adaptive searching technique abstracts ieee int 
symp 
information theory san diego ca jan 
murakami asai vector quantizer video signals electron 
lett vol 
pp 
nov 
na neuhoff bennett integral vector quantizers ieee trans 
inform 
theory vol 
pp 
july 
pearlman tree coding image subbands ieee trans 
image processing vol 
pp 
apr 
pour neuhoff mismatched dpcm encoding autoregressive processes ieee trans 
inform 
theory vol 
pp 
mar 
continuity stationary state distribution dpcm ieee trans 
inform 
theory vol 
pp 
mar 
convergence projection method autoregressive process matched dpcm code ieee trans 
inform 
theory vol 
pp 
nov 
natarajan filtering random noise deterministic signals data compression ieee trans 
signal processing vol 
nov 
feng image compression address vector quantization ieee trans 
commun vol 
pp 
dec 
king image coding vector quantization review ieee trans 
commun vol 
pp 
aug 
roy choo interframe hierarchical address vector quantization ieee trans 
select 
areas commun vol 
pp 
june 
netravali haskell digital pictures representation compression 
new york plenum nd ed 

netravali limb picture coding review proc 
ieee vol 
pp 
mar 
netravali optimal quantizer design fixedpoint algorithm bell syst 
tech 
vol 
pp 
nov 
neuhoff source coding strategies simple quantizers vs simple noiseless codes proc 
conf 
information sciences systems mar vol 
pp 

vector quantizers outperform scalar quantizers stationary memoryless sources ieee int 
symp 
information theory whistler bc canada sept 
asymptotic distribution errors vector quantization ieee trans 
inform 
theory vol 
pp 
mar 
polar quantization revisited proc 
ieee int 
symp 
information theory ulm germany july 
neuhoff gilbert causal source codes ieee trans 
inform 
theory vol 
pp 
sept 
neuhoff gray davisson fixed rate universal block source coding fidelity criterion ieee trans 
inform 
theory vol 
pp 
sept 
neuhoff lee performance tree structured vector quantization proc 
ieee int 
conf 
acoustics speech signal processing icassp toronto ont canada may vol 
pp 

neuhoff tree searched vector quantization noiseless coding proc 
conf 
information science systems princeton nj mar pp 

newman hexagon theorem bell lab 
tech 
memo published special issue quantization ieee trans 
inform 
theory vol 
pp 
mar 
nill visual model weighted cosine transform image compression quality assessment ieee trans 
commun vol 
com pp 
june 
nill objective image quality measure derived digital image power spectra opt 
eng vol 
pp 
apr 
nobel vanishing distortion shrinking cells ieee trans 
inform 
theory vol 
pp 
july 
recursive partitioning reduce distortion ieee trans 
inform 
theory vol 
pp 
july 
nobel olshen termination continuity greedy growing tree structured vector quantizers ieee trans 
inform 
theory vol 
pp 
jan 
noll bounds quantizer performance low bit rate region ieee trans 
commun vol 
com pp 
feb 
gray mean gain shape vector quantization proc :10.1.1.116.3824
ieee int 
conf 
acoustics speech signal processing minneapolis mn apr pp 

gray unbalanced tree growing algorithms practical image compression proc 
ieee int 
conf 
acoustics speech signal processing icassp toronto ont canada pp 

oliver pierce shannon philosophy pcm proc 
ire vol 
pp 
nov 
efficient coding bell syst 
tech 
vol 
pp 
july 
neal jr bound signal quantizing noise ratios digital encoding systems proc 
ieee vol 
pp 
mar 
signal quantization noise ratio differential pcm ieee trans 
commun vol 
com pp 
aug 
entropy coding speech television differential pcm systems ieee trans 
inform 
theory vol 
pp 
nov 
orchard fast nearest neighbor search algorithm proc 
ieee int 
conf 
acoustics speech signal processing icassp toronto ont canada pp 

orchard bouman color quantization images ieee trans 
signal processing vol 
pp 
dec 
source coding problem channels receivers bell syst 
tech 
vol 
pp 
dec 
paliwal effect ordering codebook efficiency partial distance search algorithm vector quantization ieee trans 
commun vol 
pp 
may 
pan fischer vector quantization lattice vector quantization speech lpc coefficients proc 
ieee int 
conf 
acoust ics speech signal processing icassp adelaide australia pt 

stage vector quantization lattice vector quantization ieee trans 
inform 
theory vol 
pp 
jan 
quantizing distortion pulse count modulation nonuniform spacing levels proc 
ire vol 
pp 
jan 
pearlman polar quantization complex gaussian random variable ieee trans 
commun vol 
com pp 
june 
pearlman gray source coding discrete fourier transform ieee trans 
inform 
theory vol 
pp 
nov 
pennebaker mitchell jpeg image compression standard 
new york van nostrand reinhold 
pin 
quantization stationary nonstationary gaussian sources voronoi constellations proc 
ieee int 
symp 
information theory ulm germany july 
coding speech lsp parameters noiseless coding proc 
ieee int 
conf 
acoustics speech signal processing icassp albuquerque nm pp 

optimal detection discrete markov sources discrete memoryless channels applications combined source channel coding ieee trans 
inform 
theory vol 
pp 
jan 
gray neuhoff quantization unified approach treestructured multistage vector quantization noisy channels ieee trans 
inform 
theory vol 
pp 
may 
transmission distortion source function encoding block length bell syst 
tech 
vol 
pp 

causal sliding block encoders feedback ieee trans 
inform 
theory vol 
pp 
mar 
poggi fast algorithm full search vq encoding electron 
lett vol 
pp 
june 
generalized cost measure address predictive vector quantization ieee trans 
image processing vol 
pp 
jan 
poggi olshen pruned tree structured vector quantization medical images segmentation improved prediction ieee trans 
image processing vol 
pp 
jan 
pollard quantization method means ieee trans 
inform 
theory vol 
pp 
mar 
popat zeger robust quantization memoryless sources dispersive fir filters ieee trans 
commun vol 
pp 
nov 
posner epsilon entropy data compression ann 
math 
statist vol 
pp 

posner rumsey jr epsilon entropy stochastic processes ann 
math 
statist vol 
pp 

pratt image transmission techniques 
new york academic 
ra kim fast mean distance ordered partial codebook search algorithm image vector quantization ieee trans 
circuits syst 
ii vol 
pp 
sept 
jones digital image compression techniques vol 
tt tutorial texts optical engineering 
bellingham wa spie opt 
eng 
press 
paliwal optimized tree algorithm fast vector quantization speech proc 
euro 
signal processing conf 
grenoble france pp 

efficient approximation elimination algorithm fast nearest neighbor search spherical distance coordinate formulation proc 
euro 
signal processing conf 
barcelona spain sept 
ramchandran vetterli best wavelet packet bases sense ieee trans 
image processing vol 
pp 
apr 
ran combined vq dct coding images noiseless coding proc 
ieee int 
conf 
acoustics speech signal processing albuquerque nm pp 

rao yip discrete cosine transform 
san diego ca academic 
read christiansen flanagan method computing dft vector quantized data proc 
ieee int 
conf 
acoustics speech signal processing icassp glasgow scotland may pp 

gray burg multirate voice digitizer vector quantization ieee trans 
commun vol 
com pp 
apr 
reeves french patent oct 
nyi dimension entropy probability distributions acta math 
acad 
sci 
vol 
pp 

rice mathematical analysis random noise bell syst 
tech 
vol 
pp 
vol 
pp 
reprinted selected papers noise stochastic processes wax wax eds 
new york dover pp 

rice plaunt rice machine television data compression jet propulsion lab pasadena ca tech 
rep sept 
adaptive variable length coding efficient compression spacecraft television data ieee trans 
commun vol 
com pp 
dec 
successive refinement information characterization achievable rates ieee trans 
inform 
theory vol 
pp 
jan 
optimal bit allocation generalized algorithm ieee trans 
inform 
theory vol 
pp 
mar 
gray greedy tree growing algorithm design variable rate vector quantizers ieee trans 
signal processing vol 
pp 
nov 
ladner wang atlas index assignment progressive transmission full search vector quantization ieee trans 
image processing vol 
pp 
may 
rizvi cheng entropy constrained predictive residual vector quantization opt 
eng vol 
pp 
jan 
roberts picture coding pseudo random noise ire trans 
inform 
theory vol 
pp 
feb 
roe quantizing minimum distortion ieee trans 
inform 
theory vol 
pp 
oct 
rose mapping approach rate distortion computation analysis ieee trans 
inform 
theory vol 
pp 
nov 
rose gurewitz fox deterministic annealing approach clustering pattern recogn 
lett vol 
pp 
sept 
vector quantization deterministic annealing ieee trans 
inform 
theory vol 
pp 
july 
constrained clustering optimization method ieee trans 
pattern anal 
machine intell vol 
pp 
aug 

analysis digital errors nonlinear pcm systems ieee trans 
commun vol 
com pp 
jan 
sabin gray product code vector quantizers speech waveform coding conf 
rec 
globecom dec pp 

sabin gray product code vector quantizers waveform voice coding ieee trans 
acoust speech signal processing vol 
assp pp 
june 
global convergence empirical consistency generalized lloyd algorithm ieee trans 
inform 
theory vol 
pp 
mar 
source encoding presence random disturbance ieee trans 
inform 
theory vol 
pp 
jan 
rate distortion function gaussian process weighted square error criterion ieee trans 
inform 
theory vol 
pp 
may 
rate distortion function class sources inform 
contr vol 
pp 
aug 
addendum rate distortion function gaussian process weighted square error criterion ieee trans 
inform 
theory vol 
pp 
sept 
worst sources robust codes difference distortion measures ieee trans 
inform 
theory vol 
pp 
may 
said pearlman new fast efficient image codec set partitioning hierarchical trees ieee trans 
circuits syst 
video technol vol 
pp 
june 
data compression 
san francisco ca morgan kaufmann 
gibson rost algorithm uniform vector quantizer design ieee trans 
inform 
theory vol 
pp 
nov 
gray unbalanced nonbinary tree structured vector quantization proc 
th asilomar conf 
signals systems computers pacific grove ca oct nov pp 

dither signals effects quantization noise ieee trans 
commun vol 
com pp 
dec 
schutzenberger quantization finite dimensional messages inform 
contr vol 
pp 

girod vector quantization entropy coding image subbands ieee trans 
image processing vol 
pp 
oct 
shannon mathematical theory communication bell syst 
tech 
vol 
pp 

coding theorems discrete source fidelity criterion ire nat 
conv 
rec pt 
pp 

shapiro embedded image coding wavelet coefficients ieee trans 
signal processing vol 
pp 
dec 
topics statistical quantization syst 
theory lab stanford electron 
lab stanford univ stanford ca tech 
rep may 
sheppard calculation probable values frequency constants data arranged equidistant divisions scale proc 
london math 
soc vol 
pt 
pp 

shields neuhoff davisson distortion rate function sources ann 
probab vol 
pp 

shoham gersho efficient bit allocation arbitrary set quantizers ieee trans 
acoust speech signal processing vol 
ieee transactions information theory vol 
october pp 
sept 
group transmission frequency division channels pulse code modulation method pp 
translation pp 
slepian class binary signaling alphabets bell syst 
tech 
vol 
pp 

delta modulation bell syst 
tech 
vol 
pp 

smith instantaneous quantized signals bell syst 
tech 
vol 
pp 

efficient nearest neighbor search method ieee trans 
commun vol 
com pp 
july 
fast mmse encoding algorithm vector quantization ieee trans 
commun vol 
pp 
june 
snyder necessary sufficient condition quantization errors uniform white ieee trans 
acoust speech signal processing vol 
assp pp 
oct 
sriram image coding wavelet transforms entropy constrained trellis coded quantization ieee trans 
image processing vol 
pp 
june 
steinberg simulation random processes theory ieee trans 
inform 
theory vol 
pp 
jan 
sur la division des en parties bull 
acad 

sci 
iii vol 
iv pp 

stewart gray linde design trellis waveform coders ieee trans 
commun vol 
pp 
apr 
optimum adaptive differential pulse code modulation ph dissertation 
inst 
brooklyn brooklyn ny 
uniform spherical coordinate quantization spherically symmetric sources ieee trans 
commun vol 
com pp 
june 
ed quantization benchmark papers electrical engineering computer science vol 

new york van nostrand reinhold 
asymptotic performance dirichlet rotated polar quantizers ieee trans 
inform 
theory vol 
pp 
july 
vector quantizer laplace source ieee trans 
inform 
theory vol 
pp 
sept 
unrestricted multistage vector quantizers ieee trans 
inform 
theory vol 
pp 
may 
ku asymptotic performance unrestricted polar quantizers ieee trans 
inform 
theory vol 
pp 
mar 
thomas optimal circularly symmetric quantizers franklin inst 
vol 
pp 

multidimensional spherical coordinates quantization ieee trans 
inform 
theory vol 
pp 
july 
design quantizers histograms ieee trans 
commun vol 
com pp 

ta vector quantization images competitive networks proc 
nd conf 
neural networks pp 

tai lai lin fast nearest neighbor searching algorithms image vector quantization ieee trans 
commun vol 
pp 
dec 
tan yao evaluation rate distortion functions class independent identically distributed sources absolute magnitude criterion ieee trans 
inform 
theory vol 
pp 
jan 
tank hopfield simple neural optimization networks converter signal decision circuit linear programming circuit ieee trans 
circuits syst vol 
cas pp 
may 
li principal points self consistent points elliptical distributions ann 
statist vol 
pp 

optimal threshold level selection quantizing data jpl space programs summary vol 
iv pp 
calif inst 
technol pasadena ca oct 
asymptotic results optimum equally spaced quantization gaussian data jpl space programs summary vol 
iv pp 
calif inst 
technol pasadena ca oct 
barnes roundoff error statistics continuous range multiplier coefficients ieee trans 
circuits syst vol 
cas pp 
jan 
torres improvement codebook search vector quantization ieee trans 
commun vol 
pp 
feb apr 
clark reconstruction error waveform transmission ieee trans 
inform 
theory vol 
pp 
apr 
optimal bit allocation algorithm quantizing random vector probl 
inform 
vol 
pp 
july sept translated russian 
sufficient conditions uniqueness locally optimal quantizer class convex error weighting functions ieee trans 
inform 
theory vol 
pp 
mar 
tsai chen stack run image coding ieee trans 
circuits syst 
video technol vol 
pp 
oct 
channel coding multilevel phase signals ieee trans 
inform 
theory vol 
pp 
jan 
trellis coded modulation redundant signal sets parts ii ieee commun 
mag vol 
pp 
feb 
gersho simulated annealing codebook design proc 
ieee int 
conf 
acoustics speech signal processing icassp new york apr pp 

vaishampayan design multiple description scalar quantizers ieee trans 
inform 
theory vol 
pp 
may 
design entropy constrained multiple description scalar quantizers ieee trans 
inform 
theory vol 
pp 
jan 
vaishampayan 
asymptotic analysis multiple description quantizers ieee trans 
inform 
theory vol 
pp 
jan 
van de weg quantization noise single integration delta modulation system digit code phillips res 
rep vol 
pp 
aug 
dither digital audio audio eng 
soc vol 
pp 
dec 
resolution significant bit digital systems dither audio eng 
soc vol 
pp 
nov correction ibid 
van der weber construction evaluation trellis coded quantizers memoryless sources ieee trans 
inform 
theory vol 
pp 
may 
vetterli multi dimensional sub band coding theory algorithms signal processing vol 
pp 
apr 
vetterli wavelets subband coding 
englewood cliffs nj prentice hall 
vidal algorithm finding nearest neighbors approximately constant average time complexity patt 
recogn 
lett vol 
pp 

chou efficient algorithm hierarchical compression video proc 
int 
conf 
image processing austin tx nov 
los alamitos ca ieee comp 
soc 
press vol 
iii pp 

viterbi trellis encoding memoryless discretetime sources fidelity criterion ieee trans 
inform 
theory vol 
pp 
may 
theory transmission processing information 
new york 
translation feinstein 
moscow ussr 
varaiya optimal causal coding decoding problems ieee trans 
inform 
theory vol 
pp 
nov 
wang trellis coded vector quantization ieee trans 
commun vol 
pp 
aug 
wang ladner codebook organization enhance maximum posteriori detection progressive transmission vector quantized images noisy channels ieee trans 
image processing vol 
pp 
jan 
ward hierarchical grouping optimize objective function amer 
statist 
assoc vol 
pp 
mar 
watson statistics spheres 
new york wiley 
optimal bit allocation algorithm sub band coding proc 
ieee int 
conf 
acoustics speech signal processing icassp pp 

woods subband coding images vector quantization ieee trans 
commun vol 
pp 
june 
widrow study rough amplitude quantization means nyquist sampling theory ire trans 
circuit theory vol 
ct pp 

gray neuhoff quantization statistical analysis amplitude quantized sampled data systems trans 
pt 
ii appl 
ind vol 
pp 

wilson magnitude phase quantization independent gaussian variates ieee trans 
commun vol 
com pp 
nov 
wilson trellis encoding continuous amplitude memoryless sources ieee trans 
inform 
theory vol 
pp 
mar 
carpenter fast search methods vector lookup vector quantization electron 
lett vol 
pp 
dec 
transform picture coding proc 
ieee vol 
pp 
july 
structure real time source coders bell syst 
tech 
vol 
pp 
jul aug indirect rate distortion problems ieee trans 
inform 
theory vol 
pp 
sept 
wolf wyner ziv source coding multiple descriptions bell syst 
tech 
vol 
pp 
oct 
wolf ziv transmission noisy information noisy receiver minimum distortion ieee trans 
inform 
theory vol 
pp 
july 
wong 
juang gray jr bit vector quantization lpc ieee trans 
acoust speech signal processing vol 
assp pp 
oct 
wood optimal quantization ieee trans 
inform 
theory vol 
pp 
mar 
woods ed subband image coding 
boston ma kluwer 
woods neil subband coding images ieee trans 
acoust speech signal processing vol 
assp pp 
oct 
wright unpublished 

wu index allocation vector quantization noisy channels electron 
lett vol 
pp 
july 
wu fallside design connectionist vector quantizers comp 
speech language vol 
pp 

source coding vector quantization codebook excited neural networks comp 
speech language vol 
pp 

wu globally optimum bit allocation proc 
data compression conf 
snowbird ut pp 

wu guan acceleration lbg algorithm ieee trans 
commun vol 
pp 
feb apr 
wyner communication analog data gaussian source noisy channel bell syst 
tech 
vol 
pp 
may june 
results shannon theory ieee trans 
inform 
theory vol 
pp 
jan 
wyner ziv bounds rate distortion function stationary sources memory ieee trans 
inform 
theory vol 
pp 
sept 
yamada fujita vector quantization video signals proc 
annu 
conf 

yamada vector quantizer design video signals trans vol 
pp 

recursive vector quantization monochrome video signals ieice trans vol 
pp 
feb 
yamada gray asymptotic performance block quantizers difference distortion measure ieee trans 
inform 
theory vol 
pp 
jan 
yamaguchi huang optimum fixed length binary code mit res 
lab 
electron quart 
progr 
rep pp 
july 
optimum binary code mit res 
lab 
electron quart 
progr 
rep pp 
july 
yamamoto source coding theory cascade branching communication systems ieee trans 
inform 
theory vol 
pp 
may 
yang zhang berger fixed slope universal lossy data compression ieee trans 
inform 
theory vol 
pp 
sept 
yao tan comments generalized shannon lower bound stationary finite alphabet sources memory ieee trans 
inform 
theory vol 
pp 
nov 
absolute error rate distortion functions sources constrained magnitudes ieee trans 
inform 
theory vol 
pp 
july 
zador development evaluation procedures quantizing multivariate distributions ph dissertation stanford univ stanford univ dept statist 
tech 
rep topics asymptotic quantization continuous random variables bell lab 
tech 
memo 
asymptotic quantization error continuous signals quantization dimension ieee trans 
inform 
theory vol 
pp 
mar revised version 
zamir feder lattice quantization noise ieee trans 
inform 
theory vol 
pp 
july 
information rates pre post filtered dithered quantizers ieee trans 
inform 
theory vol 
pp 
sept 
zeger bist linder universal source coding codebook transmission ieee trans 
vol 
pp 
feb 
zeger gersho stochastic relaxation algorithm improved vector design electron 
lett vol 
pp 
july 
pseudo gray coding ieee trans 
commun vol 
pp 
may 
zeger average number facets cell tree structured vector quantizer partitions ieee trans 
inform 
theory vol 
pp 
sept 
zeger asymptotic bounds optimal noisy channel quantization random coding ieee trans 
inform 
theory vol 
pp 
nov 
zeger gersho globally optimal vector quantizer design stochastic relaxation ieee trans 
signal processing vol 
pp 
feb 
comparison delta pulse code modulation ericsson vol 
pp 

zhang berger new results binary multiple descriptions ieee trans 
inform 
theory vol 
pp 
july 
zhang wei line universal lossy data compression algorithm continuous codebook refinement 
basic results ieee trans 
inform 
theory vol 
pp 
may 
zhang yang line universal lossy data compression algorithm continuous codebook refinement 
ii 
optimality source models ieee trans 
inform 
theory vol 
pp 
may 
zhang 
yang wei redundancy source coding fidelity criterion part known statistics ieee trans 
inform 
theory vol 
pp 
jan 
ziv coding sources unknown statistics part ii distortion relative fidelity criterion ieee trans 
inform 
theory vol 
pp 
may 
universal quantization ieee trans 
inform 
theory vol 
pp 
may 
minimal entropy probl 

inform pp 

lange yu 
shtarkov fixed rate lattice coding sources difference fidelity criterion probl 
redundancy inform syst vol 
pp 

coding sequence independent continuously distributed random values quantizing probl 
redundancy comp 
networks vol 
pp 

