proceedings ieee workshop statistical computational theories vision fort collins june 
published web 
learning low level vision william freeman egon pasztor show learning method low level vision problems 
set markov network patches image underlying scene 
factorization approximation allows easily learn parameters markov network synthetic examples image scene pairs ciently propagate image information 
monte carlo simulations justify approximation 
apply super resolution problem estimating high frequency details low resolution image showing results 
motion estimation problem show resolution aperture problem lling arising application probabilistic machinery 
distributed merl tr www merl com reports tr index html 
related technical reports please see www merl com reports tr index html www merl com reports tr index html 

merl mitsubishi electric res 
lab 
broadway cambridge ma freeman pasztor merl com 
copyright mitsubishi electric information technology center america rights reserved 
seek machinery learning low level vision problems 
problems include motion analysis inferring shape albedo photograph estimating colors 
problems image data want estimate underlying scene 
scene quantities estimated projected object velocities surface shapes re patterns colors missing high frequency details 
low level vision problems typically constrained bayesian regularization techniques fundamental 
progress example di culties remain working complex real images 
typically prior probabilities constraints learned 
general machinery learning solution low level vision problems applications 
second research theme learn statistics natural images 
researchers related statistics properties human visual system statistical methods biologically plausible image representations analyse synthesize realistic image textures :10.1.1.42.1445
methods may help understand early stages representation processing unfortunately don address visual system interpret images estimate underlying scene 
want combine themes scene estimation statistical learning 
study statistical properties synthetically generated labelled visual world images scenes learn infer scenes images 
prior probabilities rich ones learned training data 
researchers applied related learning approaches low level vision problems restricted linear models weak applications 
gather full statistics local regions images scenes 
ask visual system interpret image models probability local scene generated local image probability local scene neighbors 
rst probabilities allow initial scene estimates second allow estimates propagate 
resulting method applies bayesian machinery graphical models low level vision :10.1.1.114.4996
inspired weiss pointed speed advantage bayesian methods conventional relaxation methods propagating local measurement information :10.1.1.54.282
method uses approximations markov assumption factorization approximation sampling 
general machinery apply various problems 
show examples joint gaussian processes compare factorization approximation true answer estimating missing image details estimating motion 
markov network image data seek estimate underlying scene omit vector symbols notational simplicity 
take approach 
rst calculate posterior probability xjy bayes rule xjy cp yjx 
analysis ignore normalization constant yjx likelihood prior 
common loss functions best scene estimate mean minimum mean squared error mmse mode maximum posteriori map posterior probability 
general di cult compute approximations 
rst approximations markov assumption 
divide image scene patches assign node markov network patch :10.1.1.114.4996
connect scene patch corresponding image data patch nearest neighbors see fig 

allow connections scene image nodes di erent resolutions orientations 
connections indicate statistical dependencies 
network implies knowing scene position provides information image link gives information nearby scenes images links nearby scene neighbors 
call problems property low level vision problems 
markov network vision problems 
observations underlying scene explanations connections nodes graphical model indicate statistical dependencies 
con guration nodes explanation factorization method sect 

network description simpli es posterior probability allow estimating local operations :10.1.1.5.1518
solving markov network involves learning phase parameters network connections learned training data inference phase scene corresponding particular image data estimated 
markov assumption nding posterior probability distribution grid structured markov network computationally expensive 
variety approximations proposed :10.1.1.114.4996
network loops markov assumption allows posterior probability factorize useful way :10.1.1.114.4996:10.1.1.54.282
second approximation solve loopy markov network loops learning inference 
sect 
rst derive factorization rules networks loops 
sect 
study validity factorization approximation applied networks loops 
factorization di erent ways factorize posterior probability markov network :10.1.1.114.4996:10.1.1.54.282
non loopy network give answer 
simple valid factorization believe new resulting interpretable messages passed nodes 
iteration scene estimate xj node optimal observations node heard 
derive message passing algorithm rst considering optimal estimate nding messages need passed allow computation 
messages passed scene node heard observation yj 
mmse estimate mean corresponding posterior cp xj cis normalization constant 
call local likelihood xj local prior 
iteration scene node receives messages connecting scene nodes say fig 

heard observations yj scene nodes heard observations previous iteration 
mmse estimate xj mean posterior probability data heard cp yj xj 
call yj region likelihood likelihood region image data scene node 
exploit markov structure 
network loops yj conditionally independent xj link xj independent paths yj 
see messages scene nodes pass scene node region likelihoods node compute appropriate posterior probability region likelihoods passed neighbors local likelihood local prior 
passed messages region likelihoods 
knew calculate region likelihood pass neighbor iteration algorithm complete 
message node pass node 
pass region likelihood observations node heard node yj kjx 
know region likelihood node xj yj 
local likelihood times messages incoming nodes node sent message 
want xj yj 
multiply probability near far recognize markov assumption lets add conditioning wehave yj yj yj marginalize xj nd desired region likelihood yj 
message node passes node summarizing argument iteration mmse estimate node xj xjp xj xj xj runs scene node neighbors node region likelihood xj observations node communicates node calculate lk xk lk lk previous iteration 
initial lk 
learn network parameters measure xj directly synthetic training data 
discrete probability representation replace integral signs sums discrete states random variables 
map estimator mmse arguments hold changes xj xj eq 
eq 
loops markov network contains loops di erent region likelihoods arriving node guaranteed conditionally independent factorization may hold 
learning inference require computationally intensive methods 
various exact approximate methods choose :10.1.1.114.4996
results somewhat non standard approximation apply factorized learning propagation rules loopy network 
note appeal doing 
learn network parameters xj measuring statistics node local neighbors 
scene inference image data involves local computations eq 

applied factorized propagation rules inference networks loops obtained results 
weiss provides theoretical arguments works loop networks 
want know approximation accurate larger networks 
estimating network parameters learning propagation inference 
example joint gaussian processes study questions gaussian random processes 
describe joint probabilities image scene vectors multi dimensional gaussian 
possible design covariance matrix respects statistical dependencies de ned markov network 
solve problem ways exactly conditioning joint gaussian set observations nding posterior probability factorization approximation 
learn conditional probabilities needed propagation marginalizing joint gaussian 
experience real images shows convergence iterations performed numerical experiments relatively small grids nodes 
randomized covariance matrices constrained invariant 
synthetic image data drawn true joint gaussian simulating seeing example world 
shows example showing typical convergence results 
dotted red curves true marginalized posterior node 
isolated gray curves priors darker curves show belief di erent iterations overlaid true posterior red curve 
experiments iterations typical applications runs posterior probability standard deviations true posterior mean std 
dev 
average factorization approximation gives solution true posterior loopy networks jointly gaussian processes 
result encourages test method real scenes 
showing factorization approximation performance 
probability representations continuing applications need choose representation probabilities eqs 

discrete representation allocates discrete set symbols images scenes 
propagation inference fast eq 
reduces vector matrix operations 
learning phase poses problem 
needs measure image scene scene neighbor histograms 
practise symbols measure histograms training data symbol alphabet coarse image 
argue continuous representation 
represent probabilities mixtures gaussians images scenes vectors real values 
approach allows images scenes learning phase feasible em 
propagation di cult mixtures multiplied 
requires pruning merging step avoid runaway gaussians product mixtures making inference slow 
developed hybrid approach call sampled inference 
continuous representation learning phase allows real world image data learn required probabilities 
discrete representation inference allows fast message propagation 
inspired sample algorithms :10.1.1.37.1434
third approximations 
image observation node form line suspects collection scene values accounts observation 
search training data usually closest observations image node 
corresponding scenes training data forms group suspects focus computation scenes explain local image 
discretization conditional probabilities propagation eq 
linking matrices 
ratio mixtures gaussians xj xk xk evaluated outer product xj scenes xk scenes 
evaluating ratio valid scenes avoids singularity problems computing conditional 
method reduced propagation times super resolution problem hours minutes quality reduction 
image requires minutes initialize linking matrices 
studying speed approximations 
gather candidate scenes node sampling mixture yj xj evaluated observation yj example super resolution super resolution problem input image low resolution image 
scene estimated higher resolution image 
visually appealing solution problem allow image data treated resolution independent manner 
applications include digital lm photographs video ntsc format hdtv image compression 
rst task may impossible high resolution data 
see edges low resolution image know remain sharp resolution level 
successes texture synthesis methods expect handle textured areas :10.1.1.42.1445
bayesian method making prior probability 
contrast markov network build prior large amounts training data achieves better results 
non bayesian methods fractal image representation fig 
gathers training data image selecting nearest neighbor training data misses important spatial constraints fig 

apply markov network follows 
image start small image linearly interpolated twice pixel resolution 
scene estimated high frequency image needs add create true image higher pixel resolution 
downsampling anti aliasing training images construct training set blurred sharp image pairs 
ease modeling burden bandpass lter blurred image local contrast normalize desired output image believe relationship contrast independent 
undo normalization estimation 
extracted center aligned pixel patches images scenes image patches overlapped 
applying pca training set summarize color patch image scene vector 
image scene pair samples cluster gaussian mixtures local probabilities assuming spatial translation invariance 
infer high frequency scene image training samples closest image data node patch 
corresponding scenes suspects node 
evaluated xj xk xk scene points xj xk points form unique linking matrix scene neighbors 
propagated probabilities eq 

visually pleasing reconstructions maximum likelihood reconstruction omitting conservative local prior eq 

shows relevant full frequency images trained simple world random variations scene elements depicted 
superresolution examples algorithm converges iterations 
applies algorithm low resolution input image pixels zooming twice showing results properly re ect training data 
shows higher resolution test image 
resulting twice zoomed estimate 
little di erence training set taken time test image set unrelated pictures 
results look better competing methods cubic spline interpolation adobe photoshop fractal expansion 
sub sampled input 
frequency components follow 
true high 
nearest neighbor scene patch corresponding nearest image patch training data 
high freq 
scenes render low res image giving 
markov network inference map solution iteration message passing 
iterations 
note line continuations iterations 
note stable spatially consistent solution iterations 
noise rectangles generic actual input mag 
cubic spline train noise train train generic super resolution example 
blurred subsampled dimension yield low resolution input 
cubic spline interpolation full resolution adobe photoshop loses sharp edges 
recursively zoomed factors markov network trained images di erent worlds random noise colored rectangles generic collection photographs 
estimated high resolution images respectively re ect statistics training world 
input magni ed cubic spline fractal training set generic training set actual full resolution low resolution input image 
cubic spline zoom adobe photoshop 
zooming luminance public domain fractal image compression routine set maximum image delity chrominance components zoomed cubic spline avoid color artifacts 
blurry serious artifacts 
markov network reconstruction training set images taken person 
best possible fair training set image 
markov network training set generic photographs person fewer people 
markov network results show synthesis hair eye details artifacts looks slightly better see brow furrow 
edges textures sharp plausible 
true full resolution image 
example motion estimation show breadth technique apply problem motion estimation 
scene estimated projected velocities moving objects 
image data successive image frames 
application developed sampled inference discrete representation learning inference :10.1.1.54.299:10.1.1.5.1518
applied related message passing scheme multi resolution quad tree network estimate motion gaussian probabilities 
network contain loops structure generated artifacts quad tree boundaries arti cial statistical boundaries model 
show algorithm working simple test cases generated synthetic world moving blobs random intensities shapes 
wrote tree structured vector quantizer code pixel frame blocks image data pyramid level codes level likewise scene patches 
training approximately examples irregularly shaped moving blobs contrast background randomized values 
histograms measured relevant local statistics eqs 

shows iterations inference algorithm eqs 
converges estimate underlying scene velocities 
local probabilities learned yjx lead gure ground segmentation aperture problem constraint propagation lling see 
resulting inferred velocities correct accuracy vector quantized representation 
frames image data gaussian pyramid vector quantized 
optical ow scene information vector quantized 
large arrow added show small vectors orientation 
discussion sampled inference propagation scheme similarities relaxation labelling update vector beliefs linking matrix connections neighbors 
message update algorithm quite di erent relaxation labelling algorithm heuristically derived algorithm behavior belief propagation converges quickly relaxation labelling convergence typically slow 
local probabilities method learns yjx powerful 
embody particular vision algorithm 
probabilities learned way di erent problems lead di erent propagation behavior depending problems 
super resolution image contours extend direction contour fig 

motion estimation lling travels perpendicularly probable scene code fig 
rst iterations bayesian belief propagation 
note initial motion estimates occur edges 
due aperture problem initial estimates agree 
filling motion estimate occurs 
cues gure ground determination may include edge curvature information lower resolution levels 
included implicitly learned probabilities 
ground undetermined region low edge curvature 
velocities lled agree 
velocities lled agree correct velocity direction shown fig 

direction object contour fig 

cases behavior appropriate problem learned general machinery 
summary applied bayesian learning method problem scene estimation 
treat patches images scenes nodes markov network 
synthetic training examples learn statistical relationship local images scenes neighboring scenes 
factorization approximation lets learn network parameters simply propagate local information local update rules 
monte carlo simulations show random draws gaussian processes estimated posterior accurately approximates true posterior markov network 
local probabilities learned training data form heart vision algorithm local prior local likelihood joint probability neighboring scenes 
identify messages passed nodes scheme region likelihoods 
local probabilities learn di erent propagation behavior appropriate algorithm 
technique sampled inference computes posterior probability group candidate scenes satisfy image data 
practise speeds algorithm sacri cing quality 
main approach propagate local estimates nd best global solution long tradition computational vision 
apply machine learning techniques learn potentially rich details governing initial estimates propagation 
applied super resolution method gives results believe state art 
applied motion estimation method resolves aperture problem appropriately lls motion gure 
technique may apply related vision problems line drawing interpretation distinguishing shading re 
adelson blake tenenbaum viola weiss helpful discussions 
barrow tenenbaum 
computational vision 
proc 
ieee 
bell 
independent components natural scenes edge lters 
vision research 
berger 
statistical decision theory bayesian analysis 
springer 
besag 
spatial interaction statistical analysis lattice systems discussion 
royal statist 
soc 

bishop 
neural networks pattern recognition 
oxford 
viola 
texture recognition non parametric multi scale statistical model 
proc 
ieee computer vision pattern recognition 
freeman pasztor :10.1.1.54.299:10.1.1.5.1518
learning estimate scenes images 
kearns solla cohn editors adv 
neural information processing systems volume cambridge ma 
mit press 
frey 
graphical models machine learning digital communication 
mit press 
geiger girosi 
parallel deterministic algorithms mrf surface reconstruction 
ieee pattern analysis machine intelligence may 
geman geman 
stochastic relaxation gibbs distribution bayesian restoration images 
ieee pattern analysis machine intelligence 
heeger bergen 
pyramid texture analysis synthesis 
acm sig graph pages 
computer graphics proceedings annual conference series 
horn 
robot vision 
mit press 
poggio 
synthesizing color algorithm examples 
science 
isard blake 
contour tracking stochastic propagation conditional density 
proc 
european conf 
computer vision pages 

digital image processing 
springer verlag 
jordan editor 
learning graphical models 
mit press 
kersten toole knill anderson 
associative learning scene parameters images 
applied optics 
kittler 
relaxation labelling algorithms review 
image vision computing 
knill richards editors 
perception bayesian inference 
cambridge univ press 
landy movshon editors 
computational models visual processing 
mit press cambridge ma 
karl willsky 
cient multiscale regularization applications computation optical ow 
ieee trans 
image processing 
mackay neal 
error correcting codes sparse matrices 
cryptography coding lncs 
olshausen field 
emergence simple cell receptive eld properties learning sparse code natural images 
nature 
pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
pentland horowitz 
practical approach fractal image compression 
watson editor digital images human vision 
mit press 
poggio torre koch 
computational vision regularization theory 
nature 

mars quadtree fractal image coder decoder 
ucsd edu fractals 
rosenfeld hummel zucker 
scene labeling relaxation operations 
ieee trans 
systems man cybern 
schultz stevenson 
approach image expansion improved de nition 
ieee trans 
image processing 
simoncelli 
statistical models images compression restoration synthesis 
st asilomar conf 
sig sys 
computers paci grove ca 
szeliski 
bayesian modeling uncertainty low level vision 
kluwer academic publishers boston 
weiss :10.1.1.54.282
interpreting images propagating bayesian beliefs 
adv 
neural information processing systems volume pages 
weiss 
belief propagation revision networks loops 
technical report ai lab memo mit cambridge ma 
zhu mumford 
prior learning gibbs reaction di usion 
ieee pattern analysis machine intelligence 

