searching web arvind arasu junghoo cho hector garcia molina andreas paepcke sriram raghavan stanford university offer overview current web search engine design 
introducing generic search engine architecture examine engine component turn 
cover crawling local web page storage indexing link analysis boosting search performance 
common design implementation techniques components 
presentation draw literature experimental search engine testbed 
emphasis introducing fundamental concepts results performance analyses conducted compare different designs 
categories subject descriptors information storage retrieval line information services web services information storage retrieval systems software general terms algorithms design performance additional key words phrases authorities crawling hits indexing information retrieval link analysis pagerank search engine 
plentiful content world wide web useful millions 
simply browse web entry points yahoo 
information seekers search engine web activity 
case users submit query typically list keywords receive list web pages may relevant typically pages contain keywords 
discuss challenges building search engines describe useful techniques 
search engines known information retrieval ir algorithms techniques salton faloutsos 
ir algorithms developed relatively small coherent collections newspaper articles book catalogs physical library 
web hand massive coherent changes authors address computer science department stanford university stanford ca email cho hector paepcke cs stanford edu 
permission digital hard copy part personal classroom granted fee provided copies distributed profit commercial advantage copyright notice title publication date appear notice copying permission acm copy republish post servers redistribute lists requires prior specific permission fee 
acm acm transactions internet technology vol 
august pages 
searching web rapidly spread geographically distributed computers 
requires new techniques extensions old ones deal gathering information making index structures scalable efficiently updateable improving ability search engines discriminate 
item ability discriminate possible exploit linkage web pages better identify truly relevant pages 
question web huge challenging 
studies estimated size web bar yossef lawrence giles bharat broder report slightly different numbers agree pages available 
average size web page bytes just textual data amounts tens terabytes 
growth rate web dramatic 
lawrence giles size web doubled years growth rate projected continue years 
aside newly created pages existing pages continuously updated pitkow pirolli wills douglis cho garcia molina :10.1.1.38.7544
example study half pages months cho garcia molina pages changed daily :10.1.1.38.7544
com domain pages changed daily half life pages days days half pages gone urls longer valid 
cho garcia molina report poisson process model web page changes 
section show results improve search engine quality 
addition size rapid change interlinked nature web sets apart collections 
studies aim understand web linkage structured structure modeled broder barabasi albert albert huberman adamic cho garcia molina :10.1.1.38.7544
study example suggests link structure web somewhat bow tie broder 
pages constitute strongly connected core center bow tie 
form tie loops pages reached core vice versa 
loop consists pages reach core reached 
remaining nodes reach core reached core 
describe search engine techniques useful understand web search engine typically put 
shows engine schematically 
engine relies crawler module provide operation shown left 
crawlers small programs browse web search engine behalf similarly human user follows links reach different pages 
programs starting set urls pages retrieve web 
crawlers extract urls appearing retrieved pages give information crawler control module 
module determines links visit feeds links visit back acm transactions internet technology vol 
august 
arasu fig 

general search engine architecture 
crawlers 
functionality crawler control module may implemented crawlers 
crawlers pass retrieved pages page repository 
crawlers continue visiting web local resources storage exhausted 
basic algorithm modified variations give search engines different levels coverage topic bias 
example crawlers engine biased visit sites possible leaving pages buried deeply site 
crawlers engines specialize sites specific domain governmental pages 
crawl control module responsible directing crawling operation 
search engine complete crawling cycle crawl control module may informed indexes created earlier crawl 
crawl control module may example previous crawl link graph structure index decide links crawlers explore links ignore 
crawl control may feedback usage patterns guide crawling process connection query engine crawl control module 
section examines crawling operations detail 
indexer module extracts words page records url word occurred 
result generally large lookup table provide urls point pages word occurs text index 
table course limited acm transactions internet technology vol 
august 
searching web pages covered crawling process 
mentioned earlier text indexing web poses special difficulties due size rapid rate change 
addition quantitative challenges web calls special common kinds indexes 
example indexing module may create structure index reflects links pages 
indexes appropriate traditional text collections contain links 
collection analysis module responsible creating variety indexes 
utility index created collection analysis module 
example utility indexes may provide access pages length pages certain importance pages number images 
collection analysis module may text structure indexes creating utility indexes 
section examines indexing detail 
crawling indexing run search engines store pages retrieve web 
page repository represents possibly temporary collection 
search engines maintain cache pages visited time required build index 
cache allows serve result pages quickly addition providing basic search facilities 
systems internet archive aimed maintain large number pages permanent archival purposes 
storage scale requires special consideration 
section examines storage related issues 
query engine module responsible receiving filling search requests users 
engine relies heavily indexes page repository 
due web size fact users typically enter keywords result sets usually large 
ranking module task sorting results results near top user looking 
query module special interest traditional information retrieval ir techniques run selectivity problems applied modification web searching traditional techniques rely measuring similarity query texts texts collection documents 
tiny queries vast collections typical web search engines prevent similarity approaches filtering sufficient numbers irrelevant pages search results 
section introduces search algorithms take advantage web interlinked nature 
deployed conjunction traditional ir techniques algorithms significantly improve retrieval precision web search scenarios 
rest article describe detail search engine components 
illustrate specific challenges arise case techniques developed 
intended provide complete survey techniques 
matter fact examples illustration drawn mainly know best 
acm transactions internet technology vol 
august 
arasu addition research universities open laboratories dot com companies worked search engines 
unfortunately techniques dot especially resulting performance private walls disclosed patents language lawyers comprehend appreciate 
believe overview problems techniques provide 

crawling web pages crawler module retrieves pages web analysis indexing module 
discussed crawler module typically starts initial set urls 
roughly places queue urls retrieved kept prioritized 
queue crawler gets url order downloads page extracts urls downloaded page puts new urls queue 
process repeated crawler decides 
enormous size change rate web issues arise including pages crawler download 
cases crawler download pages web 
comprehensive search engine currently indexes small fraction entire web lawrence giles bharat broder 
fact important crawler carefully select pages visit important pages prioritizing urls queue properly fraction web visited kept date meaningful 
crawler refresh pages 
crawler downloaded significant number pages start revisiting downloaded pages order detect changes refresh downloaded collection 
web pages changing different rates cho garcia molina wills crawler needs carefully decide page revisit page skip decision may significantly impact freshness downloaded collection :10.1.1.38.7544
example certain page rarely changes crawler may want revisit page order visit frequently changing ones 
load visited web sites minimized 
crawler collects pages web consumes resources belonging organizations koster 
example crawler downloads page site site needs retrieve page file system consuming disk cpu resources 
retrieval page needs transferred network resource shared multiple organizations 
crawler minimize impact resources robots exclusion protocol 
administrators web site particular network may complain completely block access crawler 
acm transactions internet technology vol 
august 
searching web crawling process parallelized 
due enormous size web crawlers run multiple machines download pages parallel brin page cho garcia molina 
parallelization necessary order download large number pages reasonable amount time 
clearly parallel crawlers coordinated properly different crawlers visit web site multiple times adopted crawling policy strictly enforced 
coordination incur significant communication overhead limiting number simultaneous crawlers 
rest section discuss issues page selection page refresh detail 
discuss load parallelization issues mainly research done topics 
page selection argued crawler may want download important pages downloaded collection high quality 
questions need addressed meaning importance crawler operates crawler guesses pages visit 
discuss questions turn illustrate possible techniques 
importance metrics 
web page define importance page ways metrics combined discussed 
interest driven 
goal obtain pages interest particular user set users 
important pages match interest users 
particular way define notion call driving query 
query importance page defined textual similarity salton formally compute textual similarity viewing document dimensional vector 
term vector represents ith word vocabulary 
appear document zero 
appear set represent significance word 
common way compute significance multiply number times ith word appears document inverse document frequency idf ofthe ith word 
idf factor divided number times word appears entire collection case entire web 
define similarity cosine product vectors salton 
assuming query represents user interest metric shows relevant refer particular importance metric 
acm transactions internet technology vol 
august 
arasu note idf terms similarity computation importance page computed local information idf terms need global information 
crawling process seen entire collection estimate idf factors pages crawled idf terms computed time 
refer estimated importance page different actual importance computed entire web crawled 
chakrabarti presents interest driven approach hierarchy topics 
interest defined topic crawler tries guess topic pages crawled analyzing link structure leads candidate pages 
popularity driven 
page importance depends popular page instance way define popularity page backlink count 
term backlink links point page 
web page backlinks set links pages point backlinks popularity metric importance value number links appear entire web 
ib refer importance metric 
intuitively page linked pages important seldom referenced 
type citation count bibliometrics evaluate impact published papers 
web ib useful ranking query results giving users pages general interest 
note evaluating ib requires counting backlinks entire web 
crawler may estimate value ib number links seen far 
estimate may inaccurate early crawl 
section define similar sophisticated metric called pagerank ir popularity measure 
location driven 
il importance page function location contents 
url leads il function example urls com may deemed useful urls endings urls containing string home may interest urls 
location metric considers urls fewer slashes useful slashes 
location driven metrics considered special case interest driven ones list separately easy evaluate 
particular location metrics mentioned local evaluated simply looking url acm transactions internet technology vol 
august 
searching web stated earlier importance metrics combined various ways 
example may define metric ic ib il constants query combines similarity metric backlink metric location metric 
crawler models 
goal design crawler possible visits high importance pages lower ranked ones certain importance metric 
course crawler estimated importance values ib available 
estimates crawler guess high importance pages fetch 
example may define quality metric crawler ways crawl model crawler starts initial page stops visiting pages 
fixed number determined number pages crawler download crawl 
point perfect crawler visited pages page highest importance value highest 
call pages hot pages 
pages visited real crawler contain pages rank higher equal 
note need know exact rank pages order obtain value clearly estimation may possible download pages obtain global image web 
section restrict entire web pages stanford domain estimate ranks pages assumption 
define performance crawler cs performance ideal crawler course 
crawler manages visit pages entirely random may revisit pages performance total number pages web 
page visited hot page probability expected number desired pages crawler stops 
crawl threshold assume crawler visits pages 
importance target page importance higher considered hot 
assume total number hot pages assume know ranks pages obtain value performance crawler st percentage hot pages visited crawler stops 
ideal crawler performance ideal crawler performance 
purely random crawler revisits pages expected visit hot pages stops 
performance random crawler visits pages performance expected 
acm transactions internet technology vol 
august 
arasu ordering metrics 
crawler keeps queue urls seen crawl queue select url visit 
ordering metric crawler selection selects url ordering value highest urls queue 
ordering metric information seen remembered space limited crawler 
ordering metric designed importance metric mind 
instance searching high ib pages sense ib ordering metric page points 
sense consider ir pagerank metric section importance metric simpler citation count 
section show may case 
location metrics directly ordering url directly gives il value 
similarity metrics harder devise ordering metric seen 
may able text anchors url predictor text contain 
possible ordering metric query anchor text url diligenti proposes approach just anchor text text page near pages considered 
case study 
illustrate possible download important pages significantly earlier adopt appropriate ordering metric show results experiment conducted 
keep experiment manageable defined entire web stanford university web pages downloaded stanford webbase crawler 
assumed pages outside stanford zero importance value links pages outside stanford pages outside stanford count page importance computations 
note pages downloaded crawler reachable stanford homepage 
assumption measured experimentally performance various ordering metrics importance metric ib show result 
graph assumed crawl model threshold threshold value 
pages backlinks considered hot measured hot pages downloaded crawler visited stanford pages 
definition total number hot pages stanford pages 
horizontal axis represents percentage pages downloaded stanford domain vertical axis shows percentage hot pages downloaded 
experiment crawler started stanford homepage www stanford edu different experimental conditions selected page visit ordering metric ir pagerank ib backlink links breadth acm transactions internet technology vol 
august 
hot pages crawled searching web ordering metric pagerank backlink breadth random pages crawled fig 

performance various ordering metrics ib 
breadth 
straight line graph shows expected performance random crawler 
graph clearly see appropriate ordering metric significantly improve performance crawler 
example crawler ib backlink ordering metric crawler downloaded hot pages visited entire web 
significant improvement compared random crawler breadth crawler downloaded hot pages point 
interesting result experiment pagerank ordering metric ir shows better performance backlink ordering metric ib importance metric ib 
due inheritance property pagerank metric help avoid downloading locally popular pages globally popular locally unpopular pages 
additional experiments cho described studied metrics observe right ordering metric significantly improve crawler performance 
page refresh crawler selected downloaded important pages periodically refresh downloaded pages pages maintained date 
clearly exist multiple ways update pages different strategies result different freshness pages 
example consider strategies uniform refresh policy crawler revisits pages frequency regardless change 
proportional refresh policy crawler revisits page proportionally changes 
precisely assume acm transactions internet technology vol 
august 
arasu change frequency page crawler revisit frequency frequency ratio example page changes times page crawler revisits times 
note crawler needs estimate page order implement policy 
estimation change history page crawler collect cho garcia molina :10.1.1.38.7544
example crawler visited downloaded page day month detected changes crawler may reasonably estimate change days 
detailed discussion estimation see cho garcia molina 
strategies give higher freshness come better strategy 
answer questions need understand web pages change time mean freshness pages 
sections go possible answers questions compare various refresh strategies 
freshness metric 
intuitively consider collection pages collection date pages 
instance consider collections containing web pages 
maintains pages date average maintains date pages consider notion age pages obsolete consider collection current ifa refreshed day ago refreshed year ago 
intuitive notion definitions freshness age useful 
incidentally coffman slightly different definition freshness leads results analogous 
discussion refer pages web crawler monitors real world pages local copies local pages 
freshness local collection pages 
define freshness collection follows 
definition 
freshness local page time date time 
date mean content local page equals real world counterpart 
freshness local collection time 
acm transactions internet technology vol 
august 
freshness fraction local collection date 
instance local pages date zero local pages date 
age capture old collection define metric age follows definition 
age local page time date time modification time 
age local collection 
age tells average age local collection 
instance real world pages changed day ago refreshed day 
obviously freshness age local collection may change time 
instance freshness point time point time 
possible fluctuation compute average freshness long period time value representative freshness collection 
definition 
define time average freshness page ei time average freshness collection ei lim ei dt lim searching web dt 
time average age defined similarly 
mathematically time average freshness age may exist take limit time 
definition approximates fact take average freshness long period time 
see time average exist pages change reasonable process crawler periodically revisits page 
refresh strategy 
comparing page refresh strategies important note crawlers limited resources download update limited number pages certain period 
example search engines report crawlers typically download pages second 
crawler call webbase crawler typically runs rate pages acm transactions internet technology vol 
august 
arasu day element modification time fig 

database pages different change frequencies 
second 
depending page refresh strategy limited page download resource allocated different pages different ways 
example proportional refresh policy allocate download resource proportionally page change rate 
illustrate issues consider simple example 
suppose crawler maintains collection pages 
page changes times day changes day 
goal maximize freshness database averaged time 
illustrate simple model 
page day split intervals changes interval 
know exactly page changes interval 
page changes day know precisely 
crawler tiny assume refresh page day 
page refresh 
crawler refresh refresh 
answer question need compare freshness changes pick page 
page changes middle day refresh right change remain date remaining half day 
refreshing page get day benefit freshness increase 
probability changes middle day expected benefit refreshing day day bythe reasoning refresh middle interval remain date remaining half interval day probability 
expected benefit day day crude estimation see effective select refresh 
course practice know sure pages change interval 
furthermore may want worry age data 
example visit age grow indefinitely 
cho garcia molina studied realistic scenario poisson process model 
particular mathematically prove uniform policy superior equal proportional number pages change frequencies refresh rates freshness age metrics page changes follow poisson processes 
detailed proof see cho garcia molina 
acm transactions internet technology vol 
august 
searching web fig 

change frequency vs refresh frequency freshness optimization 
cho garcia molina show obtain optimal refresh policy better uniform assuming page changes follow poisson process change frequencies static change time 
illustrate show refresh frequencies maximize freshness value simple scenario 
scenario crawler maintains pages change rates 
times day respectively crawler download pages day 
graph shows needed refresh frequency page vertical axis function change frequency horizontal axis order maximize freshness page collection 
instance optimal revisit frequency page changes day times day 
notice graph monotonically increase change frequency need refresh pages pages change 
pages change frequency times day refreshed ones change frequency times day 
certain page changes maintain date resource constraint fact better focus resource pages keep track 
particular page scenario cho garcia molina prove shape graph distribution change frequencies poisson process model 
optimal graph collection pages exactly graph scaled constant factor 
matter scenario pages change frequently relative available resources penalized visited frequently 
obtain optimal refresh policy age metric described cho garcia molina 
section discussed challenges crawler encounters downloads large collections pages web 
particular studied crawler select refresh pages retrieves maintains 
acm transactions internet technology vol 
august 
arasu course open issues 
example clear crawler web site negotiate agree right crawling policy crawler interfere primary operation site downloading pages site 
existing crawler parallelization ad hoc quite preliminary believe issue needs carefully studied 
information web hidden search interface query submitted form filled 
current crawlers generate queries fill forms visit dynamic content 
problem get worse time sites generate web pages databases 

storage page repository scalable storage system managing large collections web pages 
shown repository needs perform basic functions 
provide interface crawler store pages 
second provide efficient access api indexer collection analysis modules retrieve pages 
rest section key issues techniques repository 
challenges repository manages large collection data objects web pages 
sense conceptually quite similar systems store manage data objects file systems database systems 
web repository provide lot functionality systems provide transactions logging directory structure targeted address key challenges scalability possible seamlessly distribute repository cluster computers disks order cope size web see section 
dual access modes repository support different access modes equally efficiently 
random access quickly retrieve specific web page page unique identifier 
streaming access receive entire collection significant subset stream pages 
random access query engine serve cached copies user 
streaming access indexer analysis modules process analyze pages bulk 
large bulk updates web changes rapidly see section repository needs handle high rate modifications 
new versions web pages received crawler space occupied old versions reclaimed space compaction reorganization 
repositories maintain temporal history web pages storing multiple versions page 
consider 
acm transactions internet technology vol 
august 
addition excessive conflicts update process applications accessing pages avoided 
obsolete pages file data systems objects explicitly deleted longer needed 
web page removed web site repository notified 
repository mechanism detecting removing obsolete pages 
designing distributed web repository consider generic web repository designed function cluster interconnected storage nodes 
key issues affect characteristics performance repository page distribution nodes physical page organization node update strategy 
searching web page distribution policies 
pages assigned nodes number different policies 
example uniform distribution policy nodes treated identically 
page assigned nodes system independent identifier 
nodes store portions collection proportionate storage capacities 
contrast hash distribution policy allocation pages nodes depends page identifiers 
case page identifier hashed yield node identifier page allocated corresponding node 
various policies possible 
hirai qualitative quantitative comparisons uniform hash distribution policies context web repository 
physical page organization methods 
single node possible operations executed page addition insertion high speed streaming random page access 
physical page organization node key factor determines node supports operations 
options page organization 
example organization treats disk disks set hash buckets small fit memory 
pages assigned hash buckets depending page identifier 
page additions common log structured organization may advantageous 
case entire disk treated large contiguous log incoming pages appended 
random access supported separate tree index maps page identifiers physical locations disk 
devise hybrid hashed log organization storage divided large sequential extents opposed buckets fit memory 
pages hashed extents extent organized log structured file 
hirai compare strategies detail table summarizes relative performance 
log scheme works random access requests expected 
acm transactions internet technology vol 
august 
arasu table comparing page organization methods log structured hash hashed log streaming performance random access performance page addition update strategies 
updates generated crawler design update strategy web repository depends characteristics crawler 
particular ways crawler may structured batch mode steady crawler batch mode crawler executed periodically say month allowed crawl certain amount time targeted set pages crawled stopped 
crawler repository receives updates certain number days month 
contrast steady crawler runs pause continuously supplying updates new pages repository 
partial complete crawls batch mode crawler may configured perform complete crawl time run specific set pages sites 
case pages new crawl completely replace old collection pages existing repository 
second case new collection created applying updates partial crawl existing collection 
note distinction partial complete crawls sense steady crawlers 
depending factors repository choose implement place update shadowing 
place updates pages received crawler directly integrated repository existing collection possibly replacing older versions 
shadowing pages crawl stored separately existing collection updates applied separate step 
shown read nodes contain existing collection service random streaming access requests 
update nodes store set pages retrieved latest crawl 
attractive characteristic shadowing complete separation updates read accesses 
single storage node handle page addition page retrieval concurrently 
avoids conflicts leading improved performance simpler implementation 
downside delay time page retrieved crawler time page available access shadowing may decrease collection freshness 
experience batch mode crawler generating complete crawls match shadowing repository 
analogously steady crawler better match repository uses place updates 
furthermore shadowing greater negative impact freshness steady crawler batch mode crawler additional details see cho garcia molina 
acm transactions internet technology vol 
august 
crawler node manager update nodes lan read nodes fig 

webbase repository architecture 
searching web indexer analyst stanford webbase repository illustrate repository factors fit briefly describe stanford webbase repository choices 
web base repository distributed storage system works conjunction stanford webcrawler 
repository operates cluster storage nodes connected high speed communication network see 
repository employs node manager monitor individual storage nodes collect status information free space load fragmentation number outstanding access requests 
node manager uses information control operations repository schedule update access requests node 
page repository assigned unique identifier derived computing signature checksum cyclic redundancy check url associated page 
url multiple text string representations 
example www stanford edu www stanford edu represent web page give rise different signatures 
avoid problem url normalized yield canonical representation hirai :10.1.1.24.8162
identifier computed signature normalized url 
stanford webcrawler batch mode crawler webbase repository employs shadowing technique 
configured different page organization methods distribution policies read nodes update nodes 
update nodes tuned optimal page addition performance read nodes optimal read performance 
page repository important component web search architecture 
support different access patterns query engine random access indexer modules streaming access efficiently 
acm transactions internet technology vol 
august 
arasu employ update strategy tuned characteristics crawler 
section focused basic functionality required web repository 
number features useful specific applications 
suggest possible enhancements multiple media types far assumed web repositories store plain text html pages 
growth content web increasingly important store index search image audio video collections 
advanced streaming discussion streaming access assumed streaming order important 
sufficient building basic indexes including text structure indexes 
complicated indexes ability retrieve specific subset pages edu domain specified order increasing order citation rank may useful 

indexing indexer collection analysis modules build variety indexes collected pages 
indexer module builds basic indexes text content index structure link index 
indexes pages repository collection analysis module builds variety useful indexes 
short description type index concentrating structure follows link index build link index crawled portion web modeled graph nodes edges 
node graph web page directed edge node node represents hypertext link page points page index link structure scalable efficient representation graph 
common structural information search algorithms brin page kleinberg neighborhood information page retrieve set pages pointed outward links set pages pointing incoming links :10.1.1.120.3875
disk adjacency list representations aho original web graph inverted web graph efficiently provide access neighborhood information 
structural properties web graph easily derived basic information stored adjacency lists 
example notion sibling pages basis retrieving pages related page see section 
sibling information easily derived pair adjacency list structures described 
small graphs hundreds thousands nodes represented efficiently variety known data structures aho 
doing graph nodes inverted web graph direction hypertext links reversed 
acm transactions internet technology vol 
august 
searching web edges engineering challenge 
bharat describe connectivity server system scalably deliver linkage information pages retrieved indexed altavista search engine 
text index link techniques enhance quality relevance search results text retrieval searching pages containing keywords continues primary method identifying pages relevant query 
indexes support retrieval implemented access methods traditionally search text document collections 
examples include suffix arrays manber myers inverted files inverted indexes salton witten signature files faloutsos christodoulakis 
inverted indexes traditionally index structure choice web 
discuss inverted indexes detail section 
utility indexes number type utility indexes built collection analysis module depends features query engine type information ranking module 
example query engine allows searches restricted specific site domain www stanford edu benefit site index maps domain name list pages belonging domain 
similarly neighborhood information link index iterative algorithm see section easily compute store pagerank associated page repository brin page 
index query time aid ranking search results 
rest section focus attention text indexes 
particular address problem quickly efficiently building inverted indexes web scale collections 
structure inverted index inverted index collection web pages consists set inverted lists word index term 
inverted list term sorted list locations term appears collection 
simplest case location consists page identifier position term page 
search algorithms additional information occurrence terms web page 
example terms occurring boldface tags section headings tags anchor text weighted differently ranking algorithms 
accommodate additional payload field added location entries 
payload field encodes additional information needs maintained term occurrence 
index term corresponding location refer pair posting addition inverted lists text indexes maintain known lexicon 
lexicon lists terms occurring index term level statistics total number documents term occurs ranking algorithms salton witten 
acm transactions internet technology vol 
august 
arasu challenges conceptually building inverted index involves processing page extract postings sorting postings index terms locations writing sorted postings collection inverted lists disk 
relatively small static collections environments traditionally targeted information retrieval ir systems index building times critical 
dealing web scale collections naive index building schemes unmanageable require huge resources days complete 
measure comparison traditional ir systems page webbase repository section represents publicly indexable web larger gb large trec collection hawking craswell benchmark large ir systems 
addition content web changes rapidly see section periodic crawling rebuilding index necessary maintain freshness index necessary incremental index update techniques perform poorly confronted huge wholesale changes commonly observed successive crawls web melnik 
storage formats inverted index carefully designed 
small compressed index improves query performance allowing large portions index cached memory 
tradeoff performance gain corresponding decompression overhead query time moffat bell anh moffat witten 
achieving right balance extremely challenging dealing web scale collections 
index partitioning building web scale inverted index requires highly scalable distributed text indexing architecture 
environment basic strategies partitioning inverted index collection nodes martin ribeiro neto barbosa tomasic garcia molina 
local inverted file ifl organization ribeiro neto barbosa node responsible disjoint subset pages collection 
search query broadcast nodes returns disjoint lists page identifiers containing search terms 
global inverted file organization ribeiro neto barbosa partitions index terms query server stores inverted lists subset terms collection 
example system query servers store inverted lists index terms characters ranges store inverted lists remaining index terms 
search query asks pages containing term process involves acm transactions internet technology vol 
august 
fig 

webbase indexing architecture 
melnik describe certain important characteristics strategy resilience node failures reduced network load organization ideal web search environment 
performance studies tomasic garcia molina indicate organization uses system resources effectively provides query throughput cases 
webbase text indexing system experience building text index webbase repository serves illustrate problems building massive index 
index built facilitate tests different solutions able obtain experimental results show tradeoffs 
rest section provide overview webbase index techniques utilized 
system overview 
indexing system operates architecture consisting collection nodes connected local area network 
identify types nodes system 
distributors part webbase repository section store collection web pages indexed 
indexers execute core index building engine 
final inverted index partitioned query servers strategy discussed section 
discuss statistician section 
searching web acm transactions internet technology vol 
august 
arasu page stream loading processing rat dog stripping tokenizing rat dog dog cat rat dog sorting cat dog dog dog rat rat memory memory memory fig 

different phases generating sorted runs 
flushing webbase indexing system builds inverted index stages 
stage distributor node runs distributor process disseminates pages indexers streaming access mode provided repository 
indexer receives mutually disjoint subset pages associated identifiers 
indexers parse extract postings pages sort postings memory flush intermediate structures sorted runs disk 
second stage intermediate structures merged create inverted files associated lexicons 
inverted file lexicon pair generated merging subset sorted runs 
pair transferred query servers depending degree index replication 
parallelizing index builder 
core indexing engine index builder process executes indexer 
demonstrate process effectively parallelized structuring software pipeline 
input index builder sequence web pages associated identifiers 
output index builder set sorted runs containing postings extracted subset pages 
process generating sorted runs logically split phases illustrated 
refer phases loading processing flushing 
loading phase number pages read input stream stored memory 
processing phase involves steps 
pages parsed tokenized individual terms stored set postings memory buffer 
second step postings sorted place term location 
flushing phase sorted postings memory buffer saved disk sorted run 
phases executed repeatedly entire input stream pages consumed 
loading processing flushing tend disjoint sets system resources 
processing obviously cpu intensive flushing primarily exerts secondary storage loading done directly network separate disk 
indexing performance improved executing phases concurrently see 
execution order loading processing flushing fixed phases form software pipeline 
acm transactions internet technology vol 
august 
disk indexing time thread thread thread period goal pipelining technique design execution schedule different indexing phases result minimal running time 
problem differs typical job scheduling problem chakrabarti muthukrishnan vary sizes incoming jobs loading phase choose number pages load 
consider index builder uses executions pipeline process entire collection pages generate sorted runs 
execution pipeline refer sequence phases loading processing flushing transform set pages sorted run 
bi buffer sizes executions 
sum bi fixed amount input represents total size postings extracted pages 
melnik show indexer single resource type single cpu single disk single network connection receive pages optimal speed pipeline achieved buffer sizes identical executions pipeline bn addition show bottleneck analysis derive expression optimal value terms various system parameters 
melnik extend model incorporate indexers multiple cpus disks 
experiments conducted number experiments single index builder verify model measure effective performance gains achieved parallelized index builder 
highlights importance theoretical analysis aid choosing right flushing processing loading optimal resources resources wasted resources fig 

multithreaded execution index builder 
searching web acm transactions internet technology vol 
august 
arasu time generate sorted runs pages hours loading bottleneck observed optimum predicted optimum total postings buffer size mbytes fig 

optimal buffer size 
processing bottleneck buffer size 
predicted optimum size differs slightly observed optimum difference running times sizes minutes page collection 
shows pipelining impacts time taken process generate sorted runs variety input sizes 
note small collections pages performance gain pipelining noticeable substantial 
small collections require pipeline executions time dominated time required start load buffers shut flush buffers 
experiments showed general large collections sequential index builder slower pipelined index builder 
efficient global statistics collection 
mentioned section term level statistics rank search results query 
example commonly statistics inverse document frequency idf 
idf term defined log df total number pages collection df number pages contain occurrence salton 
distributed indexing system indexes built stored collection machines gathering global collection wide term level statistics minimum overhead important issue viles french 
authors suggest computing global statistics query time 
require extra round communication query servers exchange local statistics 
communication adversely impacts term level refers fact gathered statistic describes single terms higher level entities pages web sites 
local statistics mean statistics query server deduce portion index stored node 
acm transactions internet technology vol 
august 
time generate sorted runs hours pipelining mb total buffer size pipelining mb total buffer size number pages indexed fig 

performance gain pipelining 
searching web query processing performance especially large collections spread servers 
statistics collection webbase avoid query time overhead webbase indexing system stores statistics part index creation 
dedicated server known statistician see computing statistics 
local information indexers sent statistician pages processed 
statistician computes global statistics broadcasts back indexers 
global statistics integrated lexicons merging phase sorted runs merged create inverted files see section 
techniques minimize overhead statistics collection 
avoiding explicit statistics avoid additional local data sent statistician available memory 
identified phases occurs flushing sorted runs written disk merging sorted runs merged form inverted lists 
leads strategies fl described 
local aggregation fl postings occur partially sorted order meaning multiple postings term pass memory groups 
groups condensed term local aggregated information pairs sent statistician 
example indexer buffer individual postings term cat single pair cat sent statistician 
technique greatly reduces communication overhead collecting statistics 
strategy sending local information merging 
summaries term aggregated inverted lists created memory sent statistician 
statistician receives parallel sorted streams acm transactions internet technology vol 
august 
arasu cat dog rat cat dog indexers inverted lists cat dog rat cat dog aggregate statistician cat dog rat fig 

strategy 
cat dog rat cat dog indexers lexicon term local aggregate information values indexer merges streams term aggregating term 
resulting global statistics sent back indexers sorted term order 
approach entirely stream require inmemory disk data structures statistician indexer store intermediate results 
progress indexer synchronized statistician turn causes indexers synchronized 
result slowest indexer group bottleneck holding back progress faster indexers 
illustrates strategy computing value df total number documents containing term collection 
shows indexers associated set postings 
indexer aggregates postings sends local statistics pair cat indicates indexer seen documents containing word cat statistician 
fl strategy sending local information flushing 
sorted runs flushed disk postings summarized summaries sent statistician 
sorted runs accessed sequentially processing statistician receives streams summaries globally unsorted order 
compute statistics unsorted streams statistician keeps memory hash table terms related statistics updates statistics summaries term received 
processing phase statistician sorts statistics memory sends back indexers 
illustrates fl strategy collecting page frequency statistics 
melnik analyze experiments compare relative overhead strategies different collection relative overhead strategy time full index creation statistics collection strategy time full index creation statistics collection acm transactions internet technology vol 
august 
cat dog cat cat rat rat dog cat dog dog indexers sorted runs cat dog cat rat dog cat dog cat 
dog 
rat 
hash table statistician cat dog rat hash table statistician processing processing fig 

fl strategy 
searching web cat dog rat cat dog indexers lexicon sizes 
studies show relative overheads strategies acceptably small page collection exhibit sublinear growth increase collection size 
indicates centralized statistics collection feasible large collections 
table ii summarizes characteristics fl strategies 
fundamental issue indexing web pages compared indexing traditional applications systems scale 
representing node graphs need represent graphs millions nodes billions edges 
inverting gigabyte collections documents need build inverted indexes millions pages hundreds gigabytes 
requires careful rethinking redesign traditional indexing architectures data structures achieve massive scalability 
section provided overview different indexes normally web search service 
discussed fundamental structure content indexes utility indexes pagerank index fit search architecture 
context illustrated techniques developed part stanford webbase project achieve high scalability building inverted indexes 
number challenges need addressed 
techniques incremental update inverted indexes handle massive rate change web content developed 
new indexes ranking measures invented techniques allow measures computed massive distributed collections need developed 
spectrum increasing importance personalization ability build indexes measures smaller scale customized individuals small groups acm transactions internet technology vol 
august 
arasu table ii 
comparing strategies phase statistician load memory usage parallelism merging fl flushing users limited resources important 
example haveliwala discusses techniques efficiently evaluating pagerank modestly equipped machines 

ranking link analysis shown query engine collects search terms user retrieves pages relevant 
mentioned section main reasons traditional information retrieval ir techniques may effective ranking query results 
web large great variation amount quality type information web pages 
pages contain search terms may poor quality relevant 
second web pages sufficiently self descriptive ir techniques examine contents page may 
cited example illustrate issue search search engines kleinberg :10.1.1.120.3875
homepages principal search engines contain text search engine web pages frequently manipulated adding misleading terms ranked higher search engine spamming 
techniques base decisions content pages easy manipulate 
link structure web contains important implied information help filtering ranking web pages 
particular link page page considered recommendation page author new algorithms proposed exploit link structure keyword searching tasks automatically building yahoo hierarchy identifying communities web 
qualitative performance algorithms generally better ir algorithms information just contents pages 
possible influence link structure web locally quite hard global level 
link analysis algorithms global level relatively robust spamming 
rest section describes interesting link techniques pagerank hits 
link techniques tasks page classification identifying online communities briefly discussed 
pagerank page define global ranking scheme called pagerank tries capture notion importance page 
instance yahoo 
homepage intuitively important homepage acm transactions internet technology vol 
august 
stanford database group 
difference reflected number pages point pages pages point yahoo homepage stanford database group homepage 
rank page defined number pages web point rank results search query 
citation ranking scheme ib section 
citation ranking especially spamming quite easy artificially create lot pages point desired page 
pagerank extends basic citation idea consideration importance pages point page 
page receives importance yahoo points unknown page points 
citation ranking contrast distinguish cases 
note definition pagerank recursive importance page depends influences importance pages 
simple pagerank 
simple definition pagerank captures intuition discuss computational aspects describing practical variant 
pages web denoted 
denote number forward outgoing links page denote set pages point page assume web pages form strongly connected graph page reached page 
section discuss relax assumption 
simple pagerank page denoted 
searching web division captures intuition pages point page evenly distribute rank boost pages point 
language linear algebra golub loan written vector elements matrix ij page points page 
pagerank vector eigenvector matrix corresponding eigenvalue 
graph strongly connected shown eigenvalue eigenvector uniquely defined suitable normalization performed vector nonnegative 
random surfer model 
definition simple pagerank lends interpretation random walks called random surfer model page 
imagine person web randomly clicking links pages visited 
random surfing equivalent random walk underlying link graph 
random walk problem studied combinatorial problem motwani raghavan 
shown pagerank vector proportional acm transactions internet technology vol 
august 
arasu stationary probability distribution random walk 
pagerank page proportional frequency random surfer visit 
computing pagerank 
indicated section pagerank computation equivalent computing principal eigenvector matrix defined 
simplest methods computing principal eigenvector matrix called power iteration 
power iteration arbitrary initial vector multiplied repeatedly matrix golub loan converges principal eigenvector 
power iteration pagerank computation random vector 
pagerank vector fig 

simple pagerank modified pagerank 
goto illustrate shows pagerank simple graph 
easy verify assignment ranks satisfies definition pagerank 
instance node rank outgoing links 
half rank flows node half node 
node incoming links rank received node 
node receives plus node plus node total 
note node high rank incoming links 
node high rank visits visit 
note ranks nodes add 
power iteration guaranteed converge graph aperiodic 
strongly connected directed graph aperiodic greatest common divisor lengths closed walks 
practice web aperiodic 
acm transactions internet technology vol 
august 
practical pagerank 
simple pagerank defined link graph strongly connected 
web far strongly connected see section 
particular related problems arise real web rank sinks rank leaks 
strongly internally connected cluster pages web graph links point outwards forms rank sink 
individual page outlinks constitutes rank leak 
technically rank leak special case rank sink rank leak causes different kind problem 
case rank sink nodes sink receive zero rank means distinguish importance nodes 
example suppose remove link making nodes sink 
random surfer visiting graph eventually get stuck nodes nodes rank nodes rank 
hand rank reaching rank leak lost forever 
instance remove node links associated node leak 
leak causes ranks eventually converge 
random surfer eventually reach node seen 
page suggest eliminating problems ways 
remove leak nodes degree 
second order solve problem sinks introduce decay factor pagerank definition 
modified definition fraction rank page distributed nodes points 
remaining rank distributed equally pages web 
modified pagerank searching web total number nodes graph 
note simple pagerank section special case occurs 
random surfer model modification models surfer occasionally getting bored making jump random page web random link current page 
decay factor dictates surfer gets bored 
shows modified pagerank graph link removed 
nodes higher ranks nodes indicating surfers tend 
nodes nonzero ranks 
computational issues 
order power iteration practical necessary converge pagerank leak nodes get pagerank 
alternative assume leak nodes links back pages point 
way leak nodes reachable high rank pages higher rank leak nodes reachable unimportant pages 
acm transactions internet technology vol 
august 
arasu iterations 
theoretically convergence power iteration matrix depends eigenvalue gap defined difference modulus largest eigenvalues matrix 
page claim case power iteration converges reasonably fast iterations 
note interested relative ordering pages induced pagerank rank pages actual pagerank values 
terminate power iteration ordering pages reasonably stable 
experiments haveliwala indicate ordering induced pagerank converges faster actual pagerank 
pagerank keyword searching 
brin page describe prototype search engine developed stanford called google subsequently search engine google com 
google uses ir techniques pagerank answer keyword queries 
query google computes ir score pages contain query terms 
ir score combined pagerank pages determine final rank query 
top results search google java 
java sun com source java technology internet com java www sun com java java technology www java pro com java pro www microsoft com java default htm home page microsoft technologies java hits section describe important link search algorithm hits hypertext induced topic search 
algorithm proposed kleinberg 
contrast pagerank technique assigns global rank page hits algorithm query dependent ranking technique 
producing single ranking score hits algorithm produces authority hub scores 
authority pages relevant particular query 
instance stanford university homepage authority page query stanford university page discusses weather stanford 
hub pages pages necessarily authorities point authority pages 
instance page searchenginewatch com hub page google uses text techniques enhance quality results 
example anchor text considered part page pointed 
link page anchor stanford university points page text stanford university considered part may weighted heavily actual text acm transactions internet technology vol 
august 
query search engine points authorities homepages search engines 
reasons interested hub pages 
hub pages hits algorithm compute authority pages 
second hub pages useful set pages returned user response query chakrabarti 
hub authority pages related 
hub pages ones point authority pages 
conversely expect page authority pointed hubs 
exists mutually reinforcing relationship hubs authorities authority page page pointed hubs hubs pages point authorities 
intuition leads hits algorithm 
hits algorithm 
basic idea hits algorithm identify small subgraph web apply link analysis subgraph locate authorities hubs query 
subgraph chosen depends user query 
selection small subgraph typically pages focuses link analysis relevant part web reduces amount phase 
subgraph selection analysis done query time important complete quickly 
identifying focused subgraph 
focused subgraph generated forming root set random set pages containing query string expanding root set include pages neighborhood text index section construct root set 
algorithm computing focused subgraph follows set pages contain query terms text index page include pages points include maximum pages point graph induced focused subgraph searching web algorithm takes input query string parameters parameter limits size root set parameter limits number pages added focused subgraph 
control limits influence extremely popular page www yahoo com interestingly mutually reinforcing relationships identified exploited web tasks see instance brin 
kleinberg altavista construct root set absence local text index 
acm transactions internet technology vol 
august 
arasu appear root set 
expanded set rich authorities authority pointed page root set 
likewise lot hubs included link analysis 
link analysis phase hits algorithm uses mutually reinforcing property identify hubs authorities expanded set 
note phase oblivious query derive 
pages focused subgraph denoted denote set pages point page denote set pages page points 
link analysis algorithm produces authority score ai hub score hi page set authority scores hub scores initialized arbitrary values 
algorithm iterative performs kinds operations step called operation authority score page updated sum hub scores pages pointing 
step hub score page updated sum authority scores pages points 
step ai hj step hi aj steps capture intuition authority pointed hubs hub points authorities 
note incidentally page hub authority 
hits algorithm just computes scores page hub score authority score 
algorithm iteratively repeats steps normalization hub authority scores converge initialize ai hi arbitrary values repeat convergence apply operation apply operation normalize ia ih example hub authority calculation shown 
example authority score node obtained adding hub scores nodes point dividing value heuristics eliminate links focused subgraph 
reader referred kleinberg details 
acm transactions internet technology vol 
august 
fig 

hits algorithm 
searching web hub authority values interesting mathematical properties just case pagerank 
adjacency matrix focused 
th entry equals page points page 
vector authority scores vector hub scores 
operations expressed ah respectively 
simple substitution shows final converged hub authority scores satisfy aa ah constants added account normalizing step 
vector authority scores vector hub scores eigenvectors matrices aa respectively 
link analysis algorithm simple power iteration multiplies vectors aa respectively explicitly materializing 
vectors principal eigenvectors aa respectively 
just case pagerank rate convergence hub authority scores depends eigenvalue gap 
order hubs authority pages induced hub scores authority scores converges faster iterations actual values 
interestingly idea hubs identify authorities close analog hyperlinked research communities bibliometrics rousseau garfield 
kleinberg argues bibliometrics authority typically acknowledges existence authorities 
situation different case web 
scarcely expect homepage toyota point homepage honda 
pages hubs point honda toyota homepages hub pages identify honda toyota homepages authorities topic say automobile companies acm transactions internet technology vol 
august 
arasu conclude discussion hits algorithm sample result query java borrowed kleinberg 
authorities www com java sun com javasoft homepage www com faq html java developer ncsu uiuc edu srp java html java book pages sunsite unc edu html comp lang java faq link techniques looked different link algorithms supporting keyword querying 
similar link techniques proposed tasks web 
briefly mention 
identifying communities 
web lot online communities set pages created people sharing common interest 
instance set pages devoted database research forming database research community 
thousands communities web ranging exotic mundane 
interesting problem identify study nature evolution online communities communities useful information resources people matching interests study sheds light sociological evolution web 
gibson observe communities characterized central core hub authority pages 
kumar note dense nearly bipartite graph induced edges hubs authorities contain bipartite core completely connected bipartite graph 
discovered online communities enumerating bipartite cores process call trawling 
finding related pages 
problem keyword searching users formulate queries keywords may difficulty coming right words describe interests 
search engines support different kind query called find related query query example 
kind query users give instance typically web page kind information seek 
search engine retrieves web pages similar page 
kind query supported instance google netscape related service 
find related query answered traditional ir techniques text similarity 
web exploit link structure 
basic idea cocitation 
page points pages related 
note hits acm transactions internet technology vol 
august 
searching web algorithm implicitly uses cocitation information hub authorities 
kleinberg suggests simple modification hits algorithm support find related query 
dean henzinger propose algorithms find related query companion algorithm extends kleinberg idea cocitation algorithm uses simple cocitation count determine related pages 
find link algorithms perform better netscape service companion algorithm better 
classification resource compilation 
search portals yahoo 
altavista hierarchical classification subset web pages 
users browse hierarchy locate information interest 
hierarchies typically manually compiled 
problem automatically classifying documents example classifications studied ir 
chakrabarti extended ir techniques include hyperlink information 
basic idea words document categories neighboring pages classification 
shown experimentally technique enhances accuracy classification 
related problem studied automatic resource compilation chakrabarti 
emphasis case identify high quality pages particular category topic 
algorithms suggested variations hits algorithm content information addition links 
directions link structure web contains lot useful information harnessed support keyword searching information retrieval tasks web 
discussed interesting link analysis techniques section 
pagerank global ranking scheme rank search results 
hits algorithm identifies search query set authority pages set hub pages 
complete performance evaluation link techniques scope 
interested reader referred amento comparison performance different link ranking techniques 
interesting research directions explored 
promising direction study sources information query logs click streams improving web searching 
important research direction study sophisticated text analysis techniques latent semantic indexing dumais explore enhancements hyperlinked setting 

searching world wide web successfully basis information tasks today 
search engines increasingly relied extract just right information vast number web acm transactions internet technology vol 
august 
arasu pages 
engines asked accomplish task minimal input users usually just keywords 
show engines put 
main functional blocks typical architecture 
crawlers travel web retrieving pages 
pages stored locally indexed analyzed 
query engine retrieves urls relevant user queries 
ranking module attempts sort returned urls promising results user 
simple set mechanisms requires substantial underlying design effort 
design implementation complexity stems web vast scale 
explained crawlers limited resources bandwidth storage heuristics ensure desirable pages visited search engine knowledge existing web pages stays current 
shown large scale storage web pages search engines organized match search engine crawling strategies 
local web page repositories enable users access pages randomly entire collection streamed 
indexing process studied extensively smaller homogeneous collections requires new thinking applied millions web pages search engines examine 
discussed indexing parallelized needed statistics computed indexing process 
fortunately interlinked nature web offers special opportunities enhancing search engine performance 
introduced notion pagerank variant traditional citation count 
web link graph analyzed number links pointing page taken indicator page value 
hits algorithm hubs authorities technique takes advantage web linkage 
algorithm classifies web pages primarily function major information sources particular topics authority pages pages primarily point readers authority pages hubs 
pagerank hits boost search selectivity identifying important pages link analysis 
substantial amount remains accomplished search engines keep expanding web 
new media images video pose new challenges search storage 
offer current search engine technologies point upcoming new directions 
acknowledgments gene golub referees useful suggestions improved 
acm transactions internet technology vol 
august 
searching web aho hopcroft ullman 
data structures algorithms 
addison wesley reading ma 
albert barabasi jeong 
diameter world wide web 
nature sept 
amento terveen hill 
authority mean quality 
predicting expert quality ratings web documents 
proceedings rd annual international acm sigir conference research development information retrieval 
acm press new york ny 
anh moffat 
compressed inverted files reduced decoding overheads 
proceedings st annual international acm sigir conference research development information retrieval sigir melbourne australia aug croft moffat van rijsbergen wilkinson zobel chairs 
acm press new york ny 
bar yossef berg chien 
approximating aggregate queries web pages random walks 
proceedings th international conference large data bases 
barabasi 
albert 
emergence scaling random networks 
science oct 
bharat broder 
mirror mirror web study host pairs replicated content 
proceedings eighth international conference world wide web 
bharat broder henzinger kumar venkatasubramanian 
connectivity server fast access linkage information web 
comput 
netw 
isdn syst 

brin 
extracting patterns relations world wide web 
proceedings sixth international conference extending database technology valencia spain mar 
schek ramos alonso eds 
brin page 
anatomy large scale hypertextual web search engine 
comput 
netw 
isdn syst 

broder kumar maghoul raghavan rajagopalan stata tomkins wiener 
graph structure web experiments models 
proceedings ninth international conference world wide web 
chakrabarti dom gibson kumar raghavan rajagopalan tomkins 
spectral filtering resource discovery 
proceedings acm sigir workshop hypertext information retrieval web melbourne australia 
acm press new york ny 
chakrabarti dom indyk 
enhanced hypertext categorization hyperlinks 
sigmod rec 

chakrabarti dom raghavan rajagopalan gibson kleinberg 
automatic resource compilation analyzing hyperlink structure associated text 
proceedings seventh international conference world wide web www brisbane australia apr ellis eds 
elsevier sci 
pub 
amsterdam netherlands 
chakrabarti muthukrishnan 
resource scheduling parallel database scientific applications 
proceedings th annual acm symposium parallel algorithms architectures spaa padua italy june blelloch chair 
acm press new york ny 
chakrabarti van den berg dom 
focused crawling new approach topic specific web resource discovery 
proceedings eighth international conference world wide web 
cho garcia molina 
estimating frequency change 
submitted publication 
acm transactions internet technology vol 
august 
arasu cho garcia molina 
evolution web implications incremental crawler 
proceedings th international conference large data bases 
cho garcia molina 
synchronizing database improve freshness 
proceedings acm sigmod conference management data sigmod dallas tx may 
acm press new york ny 
cho garcia molina page 
efficient crawling url ordering 
comput 
netw 
isdn syst 

coffman liu weber 
optimal robot scheduling web search engines 
tech 
rep inria rennes france 
dean henzinger 
finding related pages world wide web 
proceedings eighth international conference world wide web 
diligenti coetzee lawrence giles gori 
focused crawling context graphs 
proceedings th international conference large data bases 
douglis feldmann krishnamurthy 
rate change metrics live study world wide web 
proceedings usenix symposium internetworking technologies systems 
usenix assoc berkeley ca 
dumais furnas landauer deerwester harshman 
latent semantic analysis improve access textual information 
proceedings acm conference human factors computing systems chi washington dc may hare ed 
acm press new york ny 
rousseau 

elsevier science new york ny 
faloutsos 
access methods text 
acm comput 
surv 
mar 
faloutsos christodoulakis 
signature files access method documents analytical performance evaluation 
acm trans 
inf 
syst 
oct 
garfield 
citation analysis tool journal evaluation 
science 
gibson kleinberg raghavan 
inferring web communities link topology 
proceedings th acm conference hypertext hypermedia links objects time space structure hypermedia systems hypertext pittsburgh pa june chair 
acm press new york ny 
golub van loan 
matrix computations 
nd ed 
johns hopkins university press baltimore md haveliwala 
efficient computation pagerank 
tech 
rep 
computer systems laboratory stanford university stanford ca 
stanford edu pub 
hawking craswell thistlewaite 
overview trec large collection track 
proceedings th conference text retrieval trec 
hirai raghavan garcia molina paepcke 
webbase repository web pages 
proceedings ninth international conference world wide web 

huberman adamic 
growth dynamics world wide web 
nature sept 
kleinberg 
authoritative sources hyperlinked environment 
acm nov 
koster 
robots web trick treat 
connexions apr 
kumar raghavan rajagopalan tomkins 
trawling web emerging cyber communities 
proceedings eighth international conference world wide web 
lawrence giles 
searching world wide web 
science 
lawrence giles 
accessibility information web 
nature 
manber myers 
suffix arrays new method line string searches 
siam comput 
oct 
acm transactions internet technology vol 
august 
macleod martin nordin 
design distributed full text retrieval system 
proceedings acm conference research development information retrieval sigir dei pisa italy sept ed 
acm press new york ny 
melnik raghavan yang garcia molina 
building distributed full text index web 
tech 
rep wp stanford digital library project 
computer systems laboratory stanford university stanford ca 
www stanford edu cgi bin get wp 
melnik raghavan yang garcia molina 
building distributed full text index web 
proceedings tenth international conference world wide web 
moffat bell 
situ generation compressed inverted files 
am 
soc 
inf 
sci 
aug 
motwani raghavan 
randomized algorithms 
cambridge university press new york ny 
page brin motwani winograd 
pagerank citation ranking bringing order web 
tech 
rep 
computer systems laboratory stanford university stanford ca 

citation influence journal aggregates scientific publications theory application literature physics 
inf 
process 
manage 

pitkow pirolli 
life death electronic frontier 
proceedings acm conference human factors computing systems chi atlanta ga mar pemberton ed 
acm press new york ny 
ribeiro neto barbosa 
query performance tightly coupled distributed digital libraries 
proceedings third acm conference digital libraries dl pittsburgh pa june witten shipman eds 
acm press new york ny 
robots exclusion protocol 

robots exclusion protocol 
info webcrawler com mak projects robots exclusion html 
salton ed 

automatic text processing 
addison wesley series computer science 
addison wesley longman publ 
reading ma 
tomasic garcia molina 
performance inverted indices distributed text document retrieval systems 
proceedings nd international conference parallel distributed systems dec 

viles french 
dissemination collection wide information distributed information retrieval system 
proceedings th annual international acm sigir conference research development information retrieval sigir seattle wa july fox ingwersen fidel eds 
acm press new york ny 
wills 
better understanding web resources server responses improved caching 
proceedings eighth international conference world wide web 
witten moffat bell 
managing gigabytes compressing indexing documents images 
nd ed 
morgan kaufmann publishers san francisco ca 
received november revised march accepted march searching web acm transactions internet technology vol 
august 
