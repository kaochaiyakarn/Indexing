journal japanese society artificial intelligence september 
japanese translation naoki abe 
short boosting yoav freund robert schapire labs research shannon laboratory park avenue florham park nj usa yoav schapire www research att com schapire yoav research att com boosting general method improving accuracy learning algorithm 
short overview introduces boosting algorithm adaboost explains underlying theory boosting including explanation boosting suffer overfitting boosting relationship support vector machines 
examples applications boosting described 
horse racing gambler hoping maximize winnings decides create computer program accurately predict winner horse race usual information number races won horse betting odds horse 
create program asks highly successful expert gambler explain betting strategy 
surprisingly expert unable articulate grand set rules selecting horse 
hand data specific set races expert trouble coming rule thumb set races bet horse won races bet horse favored odds 
rule thumb obviously rough inaccurate unreasonable expect provide predictions little bit better random guessing 
furthermore repeatedly asking expert opinion different collections races gambler able extract rules thumb 
order rules thumb maximum advantage problems faced gambler choose collections races expert extract rules thumb expert useful 
second collected rules thumb combined single highly accurate prediction rule 
boosting refers general provably effective method producing accurate prediction rule combining rough moderately inaccurate rules thumb manner similar suggested 
short overviews boosting focusing especially adaboost algorithm undergone intense theoretical study empirical testing 
introducing adaboost describe basic underlying theory boosting including explanation tends overfit 
describe experiments applications boosting 
background boosting roots theoretical framework studying machine learning called pac learning model due valiant see kearns vazirani model 
kearns valiant pose question weak learning algorithm performs just slightly better random guessing pac model boosted arbitrarily accurate strong learning algorithm 
schapire came provable polynomial time boosting algorithm 
year freund developed efficient boosting algorithm optimal certain sense suffered certain practical drawbacks 
experiments early boosting algorithms carried drucker schapire simard ocr task 
adaboost adaboost algorithm introduced freund schapire solved practical difficulties earlier boosting algorithms focus 
pseudocode adaboost fig 

algorithm takes input training set belongs domain instance space label label set assume discuss extensions multiclass case 
adaboost calls weak base learning algorithm repeatedly series rounds main ideas algorithm maintain distribution set weights training set 
weight distribution training example round denoted initially weights set equally round weights incorrectly classified examples increased weak learner forced focus hard examples training set 
weak learner job find weak hypothesis appropriate distribution goodness weak hypothesis measured error notice error measured respect distribution weak learner trained 
practice weak learner may algorithm weights training examples 
alternatively possible subset training examples sampled unweighted resampled examples train weak learner 
relating back horse racing example instances correspond descriptions horse races horses running odds track records horse initialize train weak learner distribution get weak hypothesis error choose update normalization factor chosen distribution 
output final hypothesis boosting algorithm adaboost 
labels give outcomes winners race 
weak hypotheses rules thumb provided expert gambler subcollections examines chosen distribution weak hypothesis received adaboost chooses parameter 
intuitively measures importance assigned note assume loss generality gets larger gets smaller 
distribution updated rule shown 
effect rule increase weight examples misclassified decrease weight correctly classified examples 
weight tends concentrate hard examples 
final hypothesis weighted majority vote weak hypotheses weight assigned schapire singer show adaboost analysis extended handle weak hypotheses output real valued confidence rated predictions 
instance weak hypothesis outputs prediction sign predicted label magnitude gives measure confidence prediction 
focus case binary valued weak hypothesis predictions 
error cumulative distribution rounds margin error curves margin distribution graph boosting letter dataset reported schapire 
left training test error curves lower upper curves respectively combined classifier function number rounds boosting 
horizontal lines indicate test error rate base classifier test error final combined classifier 
right cumulative distribution margins training examples iterations indicated short dashed long dashed hidden solid curves respectively 
analyzing training error basic theoretical property adaboost concerns ability reduce training error 
write error hypothesis guesses instance class random error rate binary problems measures better random predictions 
freund schapire prove training error fraction mistakes training set final hypothesis weak hypothesis slightly better random training error drops exponentially fast 
similar property enjoyed previous boosting algorithms 
previous algorithms required lower bound known priori boosting begins 
practice knowledge bound difficult obtain 
adaboost hand adaptive adapts error rates individual weak hypotheses 
basis name ada short adaptive bound eq 
combined bounds generalization error prove adaboost boosting algorithm sense efficiently convert weak learning algorithm generate hypothesis weak edge distribution strong learning algorithm generate hypothesis arbitrarily low error rate sufficient data 
boosting stumps boosting stumps boosting boosting comparison versus boosting stumps boosting set benchmark problems reported freund schapire 
point scatterplot shows test error rate competing algorithms single benchmark 
coordinate point gives test error rate percent benchmark coordinate gives error rate boosting stumps left plot boosting right plot 
error rates averaged multiple runs 
generalization error freund schapire showed bound generalization error final hypothesis terms training error sample size vc dimension weak hypothesis space number boosting rounds 
vc dimension standard measure complexity space hypotheses 
see instance blumer 
specifically techniques baum haussler show generalization error high probability denotes empirical probability training sample 
bound suggests boost ing overfit run rounds large 
fact happen 
early experiments authors observed empirically boosting overfit run thousands rounds :10.1.1.49.2457
observed adaboost continue drive generalization error long training error reached zero clearly contradicting spirit bound 
instance left side fig 
shows training test curves running boosting top quinlan decision tree learning algorithm letter dataset 
response empirical findings schapire bartlett gave alternative analysis terms margins training examples 
margin error number classes adaboost sleeping experts rocchio naive bayes prtfidf error number classes adaboost sleeping experts rocchio naive bayes prtfidf comparison error rates adaboost text categorization methods naive bayes probabilistic tf idf rocchio sleeping experts reported schapire singer 
algorithms tested text corpora reuters newswire articles left ap newswire headlines right varying numbers class labels indicated axis 
example defined number positive correctly classifies example 
magnitude margin interpreted measure confidence prediction 
schapire proved larger margins training set translate superior upper bound generalization error 
specifically generalization error high probability 
note bound entirely independent number rounds boosting 
addition schapire proved boosting particularly aggressive reducing margin quantifiable sense concentrates examples smallest margins positive negative 
boosting effect margins seen empirically instance right side fig 
shows cumulative distribution margins training examples letter dataset 
case training error reaches zero boosting continues increase margins training examples effecting corresponding drop test error 
attempts successful insights gleaned theory margins authors 
behavior adaboost understood game theoretic setting explored freund schapire see grove schuurmans breiman 
particular boosting viewed repeated play certain game adaboost shown special case general algorithm playing repeated games approximately solving game 
shows boosting closely related linear programming online learning 
relation support vector machines margin theory points strong connection boosting support vector machines vapnik 
clarify connection suppose weak hypotheses want combine interested choosing coefficients reasonable approach suggested analysis adaboost generalization error choose coefficients bound eq 
minimized 
particular suppose term zero concentrate second term effectively attempting maximize minimum margin training example 
idea precise denote vector weak hypothesis predictions associated example call instance vector vector coefficients call weight vector 
notation definition margin eq 
write goal maximizing minimum margin boosting norms denominator defined range simply equal comparison explicit goal support vector machines maximize minimal margin form described eq 
norms euclidean svm norm instance vector weight vector adaboost uses norm instance vector norm weight vector 
described manner svm adaboost similar 
important differences different norms result different margins 
difference norms may significant considers low dimensional spaces 
boosting svm dimension usually high millions 
case difference norms result large differences course adaboost explicitly attempt maximize minimal margin 
schapire analysis suggests algorithm try margins training examples large possible sense regard maximum minimal margin algorithm illustrative approximation adaboost 
fact algorithms explicitly attempt maximize minimal margin experimentally successful adaboost 
margin values 
especially relevant variables sparse 
instance suppose weak hypotheses range label examples computed majority vote weak hypotheses 
case shown number relevant weak hypotheses small fraction total number weak hypotheses margin associated adaboost larger associated support vector machines 
computation requirements different 
computation involved maximizing margin mathematical programming maximizing mathematical expression set inequalities 
difference methods regard svm corresponds quadratic programming adaboost corresponds linear programming 
fact noted deep relationship adaboost linear programming connects adaboost game theory online learning 
different approach search efficiently high dimensional space 
quadratic programming computationally demanding linear programming 
important computational difference svm boosting algorithms 
part reason effectiveness svm adaboost find linear classifiers extremely high dimensional spaces spaces infinite dimension 
problem overfitting addressed maximizing margin computational problem associated operating high dimensional spaces remains 
support vector machines deal problem method kernels allow algorithms perform low dimensional calculations mathematically equivalent inner products high dimensional virtual space 
boosting approach employ greedy search perspective weak learner oracle finding coordinates non negligible correlation label reweighting examples changes distribution respect correlation measured guiding weak learner find different correlated coordinates 
actual involved applying svm adaboost specific classification problems selecting appropriate kernel function case weak learning algorithm 
kernels weak learning algorithms different resulting learning algorithms usually operate different spaces classifiers generate extremely different 
multiclass classification far considered binary classification problems goal distinguish possible classes 
real world learning problems multiclass possible classes 
methods extending ada boost multiclass case 
straightforward generalization called adaboost adequate weak learner strong achieve reasonably high accuracy hard distributions created adaboost 
method fails weak learner achieve accuracy run hard distributions 
sample examples largest weight ocr task reported freund schapire 
examples chosen rounds boosting top line rounds middle rounds bottom 
underneath image line form label example labels get highest second highest vote combined hypothesis point run algorithm corresponding normalized scores 
case sophisticated methods developed 
generally reducing multiclass problem larger binary problem 
schapire singer algorithm adaboost mh works creating set binary problems example possible label form example correct label labels freund schapire algorithm adaboost special case schapire singer adaboost algorithm creates binary problems example correct label incorrect label form example correct label methods require additional effort design weak learning algorithm 
different technique incorporates dietterich bakiri method error correcting output codes achieves similar provable bounds adaboost mh adaboost weak learner handle simple binary labeled data 
schapire singer give method combining boosting error correcting output codes 
experiments applications practically adaboost advantages 
fast simple easy program 
parameters tune number round 
requires prior knowledge weak learner flexibly combined method finding weak hypotheses 
comes set theoretical guarantees sufficient data weak learner reliably provide moderately accurate weak hypotheses 
shift mind set learning system designer trying design learning algorithm accurate entire space focus finding weak learning algorithms need better random 
hand caveats certainly order 
actual performance boosting particular problem clearly dependent data weak learner 
consistent theory boosting fail perform insufficient data overly complex weak hypotheses weak hypotheses weak 
boosting especially susceptible noise 
adaboost tested empirically researchers including 
instance freund schapire tested adaboost set uci benchmark datasets weak learning algorithm algorithm finds best decision stump single test decision tree 
results experiments shown fig 

seen boosting weak decision stumps usually give results boosting generally gives decision tree algorithm significant improvement performance 
set experiments schapire singer boosting text categorization tasks 
weak hypotheses test presence absence word phrase 
results experiments comparing adaboost methods shown fig 

nearly experiments performance measures tested boosting performed significantly better methods tested 
boosting applied text filtering ranking problems classification problems arising natural language processing 
generalization adaboost schapire singer provides interpretation boosting gradient descent method 
potential function algorithm associate cost example current margin 
potential function operation adaboost interpreted coordinate wise gradient descent space linear classifiers weak hypotheses 
insight design algorithms learning popular classification rules 
cohen singer showed apply boosting learn rule lists similar generated systems ripper irep rules 
freund mason showed apply boosting learn generalization decision trees called alternating trees nice property adaboost ability identify outliers examples mislabeled training data inherently ambiguous hard categorize 
adaboost focuses weight hardest examples examples highest weight turn outliers 
example phenomenon seen fig 
taken ocr experiment conducted freund schapire 
number outliers large emphasis placed hard examples detrimental performance adaboost 
demonstrated convincingly dietterich 
friedman suggested variant adaboost called gentle adaboost puts emphasis outliers 
freund suggested algorithm called brownboost takes radical approach de emphasizes outliers clear hard classify correctly 
algorithm adaptive version freund boost majority algorithm 
schapire drifting games reveal interesting new relationships boosting brownian motion repeated games raising new open problems directions research 
steven abney robert schapire yoram singer 
boosting applied tagging pp attachment 
proceedings joint sigdat conference empirical methods natural language processing large corpora 
peter bartlett 
sample complexity pattern classification neural networks size weights important size network 
ieee transactions information theory march 
eric bauer ron kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning appear 
eric baum david haussler 
size net gives valid generalization 
neural computation 
blumer andrzej ehrenfeucht david haussler manfred warmuth 
learnability vapnik chervonenkis dimension 
journal association computing machinery october 
bernhard boser isabelle guyon vladimir vapnik 
training algorithm optimal margin classifiers 
proceedings fifth annual acm workshop computational learning theory pages 
leo breiman 
arcing edge 
technical report statistics department university california berkeley 
leo breiman 
prediction games arcing classifiers 
technical report statistics department university california berkeley 
leo breiman 
arcing classifiers 
annals statistics 
william cohen 
fast effective rule induction 
proceedings twelfth international conference machine learning pages 
william cohen yoram singer 
simple fast effective rule learner 
proceedings sixteenth national conference artificial intelligence 
corinna cortes vladimir vapnik 
support vector networks 
machine learning september 
thomas dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
machine learning appear 
thomas dietterich bakiri 
solving multiclass learning problems errorcorrecting output codes 
journal artificial intelligence research january 
harris drucker corinna cortes 
boosting decision trees 
advances neural information processing systems pages 
harris drucker robert schapire patrice simard 
boosting performance neural networks 
international journal pattern recognition artificial intelligence 
yoav freund 
boosting weak learning algorithm majority 
information computation 
yoav freund 
adaptive version boost majority algorithm 
proceedings twelfth annual conference computational learning theory 
yoav freund raj iyer robert schapire yoram singer 
efficient boosting algorithm combining preferences 
machine learning proceedings fifteenth international conference 
yoav freund mason 
alternating decision tree learning algorithm 
machine learning proceedings sixteenth international conference 
appear 
yoav freund robert schapire 
experiments new boosting algorithm 
machine learning proceedings thirteenth international conference pages 
yoav freund robert schapire 
game theory line prediction boosting 
proceedings ninth annual conference computational learning theory pages 
yoav freund robert schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences august 
yoav freund robert schapire 
adaptive game playing multiplicative weights 
games economic behavior appear 
jerome friedman trevor hastie robert tibshirani 
additive logistic regression statistical view boosting 
technical report 
johannes rnkranz gerhard widmer 
incremental reduced error pruning 
machine learning proceedings eleventh international conference pages 
adam grove dale schuurmans 
boosting limit maximizing margin learned ensembles 
proceedings fifteenth national conference artificial intelligence 
satoshi shirai 
decision trees construct practical parser 
machine learning 
jeffrey jackson mark craven 
learning sparse perceptrons 
advances neural information processing systems pages 
michael kearns leslie valiant 
learning boolean formulae finite automata hard factoring 
technical report tr harvard university aiken computation laboratory august 
michael kearns leslie valiant 
cryptographic limitations learning boolean formulae finite automata 
journal association computing machinery january 
michael kearns umesh vazirani 
computational learning theory 
mit press 
richard maclin david opitz 
empirical evaluation bagging boosting 
proceedings fourteenth national conference artificial intelligence pages 
mason peter bartlett jonathan baxter 
direct optimization margins improves generalization combined classifiers 
technical report systems engineering australian national university 
merz murphy 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
quinlan 
bagging boosting 
proceedings thirteenth national conference artificial intelligence pages 
ross quinlan 
programs machine learning 
morgan kaufmann 
robert schapire 
strength weak learnability 
machine learning 
robert schapire 
output codes boost multiclass learning problems 
machine learning proceedings fourteenth international conference pages 
robert schapire 
drifting games 
proceedings twelfth annual conference computational learning theory 
robert schapire yoav freund peter bartlett wee sun lee 
boosting margin new explanation effectiveness voting methods 
annals statistics october 
robert schapire yoram singer 
improved boosting algorithms confidence rated predictions 
proceedings eleventh annual conference computational learning theory pages 
appear machine learning 
robert schapire yoram singer 
boostexter boosting system text categorization 
machine learning appear 
robert schapire yoram singer amit singhal 
boosting rocchio applied text filtering 
sigir proceedings st annual international conference research development information retrieval 
holger schwenk bengio 
training methods adaptive boosting neural networks 
advances neural information processing systems pages 
valiant 
theory learnable 
communications acm november 
vladimir vapnik 
nature statistical learning theory 
springer 

