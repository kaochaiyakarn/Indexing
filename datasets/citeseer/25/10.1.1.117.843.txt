unified resource scheduling framework heterogeneous computing environments ammar viktor prasanna department ee systems university southern california los angeles ca ph ammar prasanna usc edu major challenge metacomputing systems computational grids effectively shared resources compute cycles memory communication network data repositories optimize desired global objectives 
develop unified framework resource scheduling metacomputing systems tasks various requirements submitted participant sites 
goal minimize execution time collection application tasks 
model application task represented directed acyclic graph dag 
task consists subtasks resource requirements specified subtask level 
framework general accommodates emerging service qos advance resource reservations 
scheduling algorithms consider compute resources data repositories advance reservations 
shown advantageous schedule system resources unified manner scheduling type resource separately 
algorithms improvement separated approach respect completion time 

improvements communication capability geographically distributed systems attractive diverse set resources solve challenging applications 
heterogeneous computing hc systems called metacomputing systems computational grids 
research projects underway supported darpa ito quorum program naval postgraduate school subcontract number 
raghavendra aerospace box los angeles ca ph raghu aero org including example mshn globus legion users select employ resources different domains seamless manner execute applications 
general metacomputing systems compute resources different capabilities display devices data repositories interconnected heterogeneous local wide area networks 
variety tools services developed users submit execute applications metacomputing system 
major challenge metacomputing systems effectively available resources 
metacomputing environment applications submitted various user sites share system resources 
resources include compute resources communication resources network bandwidth data repositories file servers 
programs executing environment typically consist subtasks communicate cooperate form single application 
users submit jobs sites metacomputing system sending tasks quality service qos requirements 
task scheduling distributed system classic problem detailed classification see 
works scheduling tasks metacomputing systems 
scheduling independent jobs meta tasks considered 
application tasks represented directed acyclic graphs dags dynamic scheduling algorithms devised 
include hybrid generational algorithm 
static algorithms scheduling dags metacomputing systems described 
previous algorithms focus compute cycles main resource 
previous dags scheduling algorithms assume subtask receives input data predecessor subtasks 
scheduling decisions machine performance subtasks cost receiving input data predecessor subtasks 
metacomputing applications need resources data repositories addition compute resources 
example data intensive computing applications access high volume data distributed data repositories databases archival storage systems 
execution time applications data movement 
applications computationally demanding communication intensive 
achieve high performance applications scheduling decisions required resources 
assigning task machine gives best execution time may result poor performance due cost retrieving required input data data repositories 
impact accessing data servers scheduling decisions considered context developing apples agent digital sky survey analysis application 
apples selects run statistical analysis amount required data data servers 
primary motivation optimize performance particular application 
develop unified framework resource scheduling metacomputing systems 
framework considers compute resources resources communication network data repositories 
incorporates emerging concept advance reservations system resources reserved advance specific time intervals 
framework various requirements submitted participant sites 
application task consists subtasks represented dag 
resource requirements specified subtask level 
subtask input data data items predecessors data sets data repositories 
subtask ready execution predecessors completed received input data needed execution 
framework allow input data sets replicated data set accessed data repositories 
additionally task submitted qos requirements needed compute cycles memory communication bandwidth maximum completion time priority framework sources input data execution times subtasks various machines availability considered simultaneously minimize completion time 
unified framework allows factors taken account resource scheduling illustrate ideas heuristic algorithms resource scheduling problem compute resources data repositories advance reservations 
resources available schedule subtasks certain time intervals reserved users times 
qos requirements deadlines priorities included algorithms 
objective resource scheduling algorithms minimize completion time submitted tasks 
research part mshn project collaborative effort dod naval postgraduate school academia nps usc purdue university industry 
mshn management system heterogeneous networks designing implementing resource management system rms distributed heterogeneous shared environments 
mshn assumes heterogeneity resources processes qos requirements 
processes may different priorities deadlines compute characteristics 
goal schedule shared compute network resources qos requirements satisfied 
scheduling algorithms derivatives may included scheduling advisor component mshn 
organized follows 
section introduce unified resource scheduling framework 
section heuristic algorithms solving general resource scheduling problem considers data repositories advance reservations system resources 
simulation results section demonstrate performance algorithms 
section gives research directions 

scheduling framework 
application model metacomputing system considering application tasks compete computational resources communication network data repositories 
application task consists set communicating subtasks 
data dependencies subtasks assumed known represented directed acyclic graph dag set subtasks application executed represented represents data dependencies communication subtasks 
indicates communication subtask represents amount data sent shows example application tasks 
example task consists subtasks task consists subtasks 
framework qos requirements specified task 
requirements include needed compute cycles memory communication bandwidth maximum completion time model subtask input data data items predecessors data sets data repositories 
subtask input data data items data sets retrieved execution 
task task 
example application tasks subtask completion generated output data may forwarded successor subtasks written back data repositories 
applications subtask may contain 
example adaptive signal processing asp applications typically composed sequence computation stages subtasks 
stage consists number identical sub subtasks fft qr decompositions 
stage repeatedly receives input previous stage performs computations sends output stage 

system model metacomputing system consists heterogeneous machines file servers data repositories assume estimate execution time subtask machine available compile time 
estimated execution times matrix gives estimated computation time subtask machine subtask executed machine set infinity 
system resources may available time intervals due advance reservations 
available time intervals machine available time intervals data repository matrices give message transfer time byte communication latency machines respectively 
matrices specify message transfer time byte communication latency tween data repositories machines respectively 
gives amount input data sets needed data repositories subtask systems multiple copies data sets data repository provide required data sets subtask 

problem statement goal minimize execution time collection applications compete system resources 
strategy optimizing performance collection tasks opposed single application taken smartnet mshn 
hand emphasis projects apples optimize performance individual application cooperate applications sharing resources 
multiple users share resources optimizing performance individual application may dramatically affect completion time applications 
formally state resource scheduling problem 
metacomputing system machines data repositories advance reserved times system resources application tasks application represented dag communication latencies transfer rates various resources matrices subtasks execution times various machines ma trix amount input data sets needed data repositories subtask find schedule schedule determines subtask start time duration resources needed execute subtask 
subject constraints subtask subtask subtask subtask subtask 
application dag example sec 
table 
subtask execution times subtask execute predecessors completed items received predecessors input data sets retrieved data repositories preserve advance resource reservations subtask execute machine time subtask access data repository time 

separated scheduling vs unified scheduling scheduling methods exist literature scheduling application dags compute network resources 
consider data repositories 
inclusion data repositories obtain schedules compute resources data repositories independently table 
transfer costs time units data unit subtask amount input data repository data set choices units units units unit units table 
input requirements subtasks 
separated scheduling machines 
separated scheduling data repositories 
unified scheduling combine schedules 
section show simple example separated approach efficient respect completion time 
shows dag representation application task subtasks 
example assume fully connected system machines data repositories file servers 
subtask execution times time units table 
table gives cost time units transferring data unit data repository machine 
assume subtask needs input data set retrieved data repositories table 
example simple list scheduling algorithm called baseline algorithm 
algorithm described 
baseline algorithm fast static algorithm mapping dags hc environments 
partitions subtasks dag blocks levels algorithm similar level partitioning algorithm described section 
subtasks ordered subtasks block come fore subtasks block block sorted descending order number descendents subtask ties broken arbitrarily 
subtasks considered mapping order 
subtask mapped machine gives minimum completion time particular subtask 
original algorithm account data repositories implemented modified version algorithm 
modified version algorithm chooses data repository gives best retrieving time input data set 
schedule separated approach scheduling machines shown 
completion time schedule time units 
case map application subtasks machines resources system 
subtask choose data repository gives best retrieving delivery time input data set previously mapped machine subtask order minimize completion time 
completion time schedule separated approach scheduling data repositories time units shown 
case map application subtasks data repositories system resources 
subtask choose machine gives best completion time subtask previously mapped data repository get required data set subtask 
shows schedule unified approach 
completion time unified scheduling time units 
unified approach map subtask machine data repository time order minimize completion time 
previous example shows clearly scheduling separated approach efficient respect completion time 
advance reservations separated scheduling lead poor utilization resources type resource available available 

resource scheduling algorithms subtasks section develop static compile time heuristic algorithms scheduling tasks metacomputing system compute resources data repositories advance reservations 
resources available schedule subtasks certain time intervals reserved users times 
framework incorporates notion qos algorithms consider qos 
currently working extending scheduling algorithms task task 
combined dag tasks fig 
consider qos requirements deadlines priorities security 
state art systems assume central scheduler set static application tasks schedule 
static applications complete set task scheduled known priori 
tasks sites sent central scheduler determine schedule subtask global objective achieved 
information submitted tasks status various resources communicated central scheduler 
centralized scheduler appropriate decisions achieve better utilization resources 
scheduling metacomputing systems schedule compute resources known npcomplete 
method known list scheduling algorithm 
list scheduling subtasks dag placed list priority assigned subtask 
subtask scheduled predecessors scheduled 
ready subtasks considered scheduling order priorities 
section develop modified versions list scheduling algorithm generalized task scheduling problem advance resource reservations 
heuristic algorithms list scheduling types level level scheduling greedy approach 
briefly describe types algorithms 
level level level level task task 
level partitioning combined dag fig 

level level scheduling framework application tasks represented dags node subtask edges predecessors represent control flow 
subtask computation cost data items communicated predecessor subtasks data sets repositories 
subtask ready execution predecessors completed received input data needed execution 
facilitate discussion scheduling algorithms hypothetical node created linked zero communication time links root nodes submitted dags obtain combined dag 
dummy node zero computation time 
shows combined dag tasks 
minimizing maximum time complete combined dag achieves global objective 
level level heuristic partition combined dag levels subtasks 
level contains independent subtasks dependencies subtasks level 
subtasks level executed parallel ready 
level contains dummy node 
level contains subtasks incident edges subtasks predecessors 
subtasks level level successors 
subtask predecessors levels level 
shows levels combined dag fig 

combined dag example level level scheduling algorithm combine submitted dags dag 
level partitioning combined dag 
level set set subtasks level 
empty find subtasks machine gives minimum completion time subtask data repository get input data set 
min finish choose subtask minimum completion time 
max finish choose subtask maximum completion time 
schedule subtask update remove machine data repository 
pseudo code level level scheduling algorithms levels 
scheduler considers subtasks level time 
subtasks particular level subtask minimum completion time scheduled min finish algorithm subtask maximum completion time scheduled max finish algorithm 
advance reservations compute resources data repositories handled choosing fit time interval optimize completion time subtask 
idea min finish algorithm algorithm min min algorithm smartnet step attempt minimize finish time subtask ready set 
hand idea max finish algorithm max min algorithm smartnet minimize worst case finishing time critical subtasks giving opportunity mapped best resources 
pseudo code level level scheduling algorithms shown 

greedy approach subtasks specific level combined dag belong different independent tasks scheduling level level creating dependency various tasks 
completion times levels different tasks vary widely level level scheduling algorithms may perform 
idea greedy heuristics min finish max finish consider subtasks levels ready execute determining schedule 
advance execution different tasks different amounts attempt achieve global objective provide response times short tasks time 
consider minimum finish time maximum finish time ready subtasks determining order subtasks schedule 
greedy algorithms min finish max finish algorithm similar min finish max finish respectively 
differ respect set 
greedy algorithms set may contain subtasks levels 
initially contains subtasks level applications 
mapping subtask algorithms check successors ready considered scheduling add set 
subtask considered schedul ing predecessors scheduled 

results discussion set generalized resource scheduling problem considered clear variation list scheduling perform best 
intuition scheduling subtasks considering resource types result bounded suboptimal solutions 
order evaluate effectiveness scheduling algorithms discussed sections developed software simulator calculates completion time 
input parameters simulator fixed values range values minimum maximum value 
time number subtasks min finish max finish min finish max finish baseline 
simulation results machines data repositories varying number subtasks time millisecond number machines data repositories min finish max finish min finish max finish baseline 
simulation results subtasks varying number machines data repositories subtask execution times communication latencies communication transfer rates data items amounts data sets amounts specified simulator range values 
actual values parameters choosen randomly simulator specified ranges 
fixed input parameters number machines number data repositories number data items total number subtasks 
assume task needs input data set data repositories 
data set replicated may data repositories 
compute resource data repository slots blocked simulation indicate advance reservations 
compare scheduling algorithms separated version baseline algorithm discussed section 
simulation results shown figures 
scheduling algorithms compared varying number subtasks machines data repositories 
shows similar comparison varying number machines data repositories subtasks 
preliminary results show heuristic algorithms similar performance relatively uniform task costs 
simulation results clearly show advantageous schedule system resources unified manner scheduling type resource separately 
scheduling algorithms baseline algorithm separated approach 

represents best knowledge step unified framework resource scheduling emerging constraints important metacomputing 
considered requirement advance reservations compute resources data repositories 
investigating question advance reservations impact task completion times 
scheduling soon want reserve resource subtask avoid waiting resource blocking different subtask 
currently working extending scheduling algorithms consider qos requirements deadlines priorities security 
mapping qos specified task level subtasks framework 
plan develop scheduling algorithms dynamic environments mentioned resource requirements 
dynamic environment application tasks arrive real time non deterministic manner 
system resources may removed new resources may added run time 
dynamic scheduling algorithms real time information require feedback system 
adam chandy dickson comparison list schedules parallel processing systems comm 
acm dec 
armstrong hensgen kidd relative performance various mapping algorithm independent sizable variance run time predictions th heterogeneous computing workshop hcw pp 
march 
berman wolski scheduling perspective application th ieee international symposium high performance distributed computing august 
berman high performance schedulers grid blueprint new computing infrastructure foster kesselman ed morgan kaufmann publishers san francisco ca pp 

braun taxonomy describing matching scheduling heuristics mixed machines heterogeneous computing systems workshop advances parallel distributed systems west lafayette oct 
taxonomy scheduling general purpose distributed computing systems ieee trans 
software engineering feb 
fernandez allocating modules processors distributed system ieee trans 
software engineering se nov 
foster kesselman ed grid blueprint new computing infrastructure morgan kaufmann publishers san francisco ca 
freund carter watson keith generational scheduling heterogeneous computing systems int conf 
parallel distributed processing techniques applications pdpta pp 
aug 
freund campbell hensgen keith kidd lima moore rust siegel scheduling resources multi user heterogeneous computing environments th heterogeneous computing workshop hcw pp 
march 
freund kidd hensgen moore smartnet scheduling framework heterogeneous computing international symposium parallel architectures algorithms networks beijing china june 
freund siegel heterogeneous processing ieee computer june 
globus web page 
www globus org 
ibarra kim heuristic algorithms scheduling independent tasks non identical processors journal acm april 
iverson dynamic competitive scheduling multiple dags distributed heterogeneous environment th heterogeneous computing workshop hcw pp 
march 
iverson parallelizing existing applications distributed heterogeneous environment th workshop hcw pp 
apr 
prasanna wang heterogeneous computing challenges opportunities ieee computer june 
potter scott dynamic task mapping algorithms distributed environment th heterogeneous computing workshop hcw pp 
apr 
legion web page 
legion virginia edu 
maheswaran siegel dynamic matching scheduling algorithm heterogeneous computing systems th workshop hcw pp 
march 
moore baru rajasekar wan data intensive computing grid blueprint new computing infrastructure foster kesselman ed morgan kaufmann publishers san francisco ca pp 

mshn web page 
www mshn org 
shirazi wang pathak analysis evaluation heuristic methods static task scheduling journal parallel distributed computing 
watson flann freund genetic simulated annealing scheduling data dependent tasks heterogeneous environment th heterogeneous computing workshop hcw pp 
apr 
lee compile time interconnection constrained heterogeneous processor architectures ieee trans parallel distributed systems feb 
catlett metacomputing communications acm june 
lee wang howard jay siegel roychowdhury anthony task matching scheduling heterogeneous computing environments genetic algorithm approach journal parallel distributed computing nov 
biographies ammar ph candidate department electrical engineering systems university southern california los angeles california usa 
main research interest task scheduling heterogeneous environments 
received degree computer engineering kuwait university degree computer engineering university southern california 
member ieee ieee computer society acm 
viktor prasanna prasanna kumar professor department electrical engineering systems university southern california los angeles 
received electronics engineering bangalore university school automation indian institute science 
obtained ph computer science pennsylvania state university 
research interests include parallel computation computer architecture vlsi computations high performance computing signal image processing vision 
dr prasanna published extensively consulted industries areas 
widely known pioneering reconfigurable architectures high performance computing signal image processing image understanding 
served organizing committees international meetings vlsi computations parallel computation high performance computing 
serves editorial boards journal parallel distributed computing ieee transactions computers 
founding chair ieee computer society technical committee parallel processing 
fellow ieee 
raghavendra senior engineering specialist computer science research department aerospace 
received ph degree computer science university california los angeles 
september december faculty electrical engineering systems department university southern california los angeles 
january july boeing chair professor computer engineering school electrical engineering computer science washington state university 
received presidential young investigator award ieee fellow 
subject area editor journal parallel distributed computing editor chief special issues new journal called cluster computing baltzer science publishers program committee member networks related international conferences 
