selection relevant features examples machine learning avrim blum avrim cs cmu edu school computer science carnegie mellon university pittsburgh pennsylvania pat langley langley isle org institute study learning expertise court palo alto california survey review machine learning methods handling data sets containing large amounts irrelevant information 
focus key issues problem selecting relevant features problem selecting relevant examples 
describe advances topics empirical theoretical machine learning general framework compare di erent methods 
close challenges area 
appear special issue arti cial intelligence relevance greiner subramanian eds 
intelligent systems laboratory daimler benz research center page mill road palo alto ca 
selecting relevant features examples page 
machine learning aims address larger complex tasks problem focusing relevant information potentially overwhelming quantity data increasingly important 
instance data mining corporate scienti records involves dealing features examples internet world wide web volume low quality information easy access learning system 
similar issues arise personalization ltering systems information retrieval electronic mail netnews 
address speci aspects focusing task received signi cant attention ai literature problem focusing relevant features representing data problem selecting relevant examples drive learning process 
review topics presenting general frameworks compare contrast di erent approaches 
problem focusing relevant features 
section relate important notions relevance task describe general goals feature selection algorithms 
report methods developed problem characterizing embedded lter wrapper approaches compare explicit feature selection techniques weighting schemes 
turn section problem focusing relevant examples describing methods ltering labeled unlabeled data 
conclude section open problems challenges empirical theoretical fronts 
proceeding clarify scope survey focuses methods results computational learning theory experimental machine learning 
substantial feature selection elds pattern recognition statistics data selection elds statistics information theory philosophy science 
space cover areas readers aware similarities approaches discuss 

problem irrelevant features conceptual level divide task concept learning subtasks deciding features describing concept deciding features 
view selection relevant features elimination irrelevant ones central problems machine learning induction algorithms incorporate approach addressing 
practical level induction algorithms scale domains irrelevant features 
speci cally goal thenumber training examples needed reach desired level accuracy called sample complexity grow slowly number features needed achieve performance 
instance uncommon text classi cation task represent examples attributes expectation small fraction crucial lewis lewis 
years growing amount ofwork machine learning experimental theoretical nature focused developing algorithms desirable properties 
induction algorithms di er considerably emphasis focusing relevant features 
extreme lies simple nearest neighbor method classi es test instances retrieving nearest stored training example available attributes distance computations 
cover hart showed approach excellent asymptotic accuracy little thought reveals presence irrelevant attributes considerably slow rate learning 
fact langley iba analysis simple nearest neighbor indicates number training examples needed reach page selecting relevant features examples accuracy similar pac notion sample complexity grows exponentially number irrelevant attributes conjunctive target concepts 
experimental studies nearest neighbor aha langley sage consistent discouraging 
extreme lie induction methods explicitly attempt select relevant features reject irrelevant ones 
techniques learning logical descriptions constitute simplest example approach sophisticated methods identifying relevant attributes augment improve induction method including nearest neighbor 
theoretical experimental results methods encouraging 
instance theoretical results show focusing small subset features algorithm signi cantly reduce consideration corresponding reduction sample size su cient guarantee generalization blumer 
somewhat middle extremes feature weighting methods explicitly select subsets features aim achieve scaling behavior 
structure remainder section follows 
describing important formal notions relevance context supervised learning 
addition introducing terminology de nitions help illustrate general goals feature selection algorithms 
turn discussing methods problem characterizing embedded lter wrapper approaches relation selection scheme basic induction algorithm 
decomposition part re ects historical trends helps comparing approaches may di erent seen belong category certain ways similar motivations 
compare explicit feature selection techniques weighting schemes tackle problem somewhat di erent perspective 
de nitions relevance number di erent de nitions machine learning literature means features relevant 
reason variety generally depends question relevant 
point di erent de nitions may appropriate depending goals 
describe important de nitions relevance discuss signi cance 
doing hope illustrate issues involved variety motivations approaches taken literature 
concreteness consider setting features attributes describe examples feature domain fi 
instance feature may boolean red discrete multiple values color continuous wavelength 
example point instance space fn 
learning algorithm set training data data point example paired associated label classi cation boolean multiple valued continuous 
learning algorithm sees xed sample helpful postulate additional quantities done pac learning model see kearns vazirani probability distribution instance space target function examples labels 
model sample having produced repeatedly selecting examples labeling function target function may deterministic probabilistic case example probability distribution labels just single label 
note distribution model integrity constraints data 
instance suppose representing decimal digit boolean features feature digit greater equal model having assign examples probability zero target function de ned examples 
selecting relevant features examples page setup simplest notion relevance notion relevant target concept 
de nition relevant target xi relevant target concept exists pair examples instance space di er assignment xi 
way stating de nition feature xi relevant exists example instance space value xi ects classi cation target concept 
notice notion drawback learning algorithm access sample necessarily determine feature xi relevant 
worse encoding features redundant feature repeated twice may possible see examples di er feature examples probability zero hand de nition choice theoretical analyses learning algorithms notion relevance prove convergence properties algorithm algorithm 
de nition useful situations target function real object learning algorithm actively query inputs choosing learning algorithm trying reverse engineer piece hardware just convenient ction 
remedy drawbacks de nition john kohavi eger de ne notions termed relevance respect distribution nice interpretation notion relevance respect sample 
de nition strongly relevant sample distribution xi strongly relevant sample exist examples di er assignment xi di erent labels di erent distributions labels appear multiple times 
similarly xi strongly relevant target distribution exist examples having non zero probability di er assignment xi satisfy 
words just de nition required non zero probability 
de nition weakly relevant sample distribution xi weakly relevant sample target distribution remove subset features xi strongly relevant 
notions relevance useful viewpoint learning algorithm attempting decide features keep features strongly relevant generally important matter sense removing strongly relevant feature adds ambiguity sample 
features weakly relevant important depending features ignored 
practice may wish adjust de nitions account statistical variations 
instance special case de nition feature xi weakly relevant correlated target function xi strongly relevant features removed nite sample want account statistical signi cance 
somewhat di erent vein de nitions cases caring exactly features relevant relevance measure 
want relevance say complicated function requiring algorithm explicitly select subset features just want perform quantity low 
purpose notion relevance complexity measure respect sample data set concepts useful page selecting relevant features examples de nition relevance complexity measure sample data set concepts number features relevant de nition concept error fewest relevant features 
words asking smallest number features needed achieve optimal performance concept reason specifying concept class may feature asa person social security number highly relevant point contained useless respect sorts concepts consideration 
additional robustness de nition modi ed allow concepts nearly minimal error produces smaller relevant set 
notions relevance independent speci learning algorithm 
guarantee just feature relevant necessarily useful algorithm vice versa 
caruana freitag explicit notion term incremental usefulness simply call usefulness de nition incremental usefulness sample data algorithm feature set feature xi incrementally useful respect accuracy hypothesis produces feature better accuracy achieved just feature set notion especially natural feature selection algorithms search space feature subsets incrementally adding removing features current set instance follow general framework described section 
de nitions clear consider concepts expressed disjunctions features suppose learning algorithm sees examples relevant features de nition depend true target concept consistent target disjunction include rst feature 
de nitions say strongly relevant rest weakly relevant note weakly relevant strongly relevant removing 
de nition wewould say simply relevant features number features relevant smallest consistent disjunction 
notion incremental usefulness de nition depends learning algorithm presumably feature set third feature useful features 
revisit question de nition related section discuss simple speci algorithm 
variety extensions tothe de nitions 
instance consider relevant linear combinations features just relevant individual features 
case analogy de nition ask lowest dimensional space projecting examples space preserves existence function class 
notion relevance natural statistical approaches learning 
methods principal component analysis commonly heuristics nding lowdimensional subspaces 
selecting relevant features examples page 
state space feature subsets speci es attributes induction 
note states space case involving features partially ordered state children right including attribute dark circles parents 
feature selection heuristic search turn discussing feature selection algorithms generally algorithms dealing data sets contain large numbers irrelevant attributes 
paradigm viewing approaches especially perform explicit feature selection heuristic search state search space specifying subset possible features 
view characterize feature selection method terms stance basic issues determine nature heuristic search process 
determine starting point points space turn uences direction search operators generate successor states 
depicts natural partial ordering space child having exactly feature parents 
suggests start successively add attributes start attributes successively remove 
approach called forward selection known backward elimination 
variations partial ordering devijver kittler report operator adds features takes away genetic operators crossover produce somewhat di erent types connectivity 
second decision involves organization search 
clearly exhaustive space impractical exist possible subsets attributes 
realistic approach relies greedy method traverse space 
point search considers local changes current set attributes selects iterates 
instance hill climbing approach known stepwise selection elimination considers adding removing features decision point lets retract earlier decision keeping explicit track search path 
options consider states generated operators select best simply choose rst state improves accuracy current set 
replace greedy scheme sophisticated methods best rst search expensive tractable domains 
third issue concerns strategy evaluate alternative subsets attributes 
commonly metric involves attribute ability discriminate classes occur training data 
page selecting relevant features examples induction algorithms incorporate criterion information theory directly measure accuracy training set separate evaluation set 
broader issue concerns feature selection strategy interacts basic induction algorithm discuss shortly detail 
decide criterion halting search 
example adding removing attributes alternatives improves estimate classi cation accuracy continue revise feature set long accuracy degrade continue generating candidate sets reaching search space select best 
simple halting criterion combination values selected attributes maps single class value assumes noise free training data 
robust alternative simply orders features relevancy score uses system parameter determine break point 
note design decisions induction algorithm carries feature selection 
provide useful dimensions describing techniques developed address problem refer repeatedly 
concrete revisit scenario section considering concepts expressible disjunction boolean features simple strategy known greedy set cover algorithm disjunction zero features outputs negative example 
features negative example safe add hypothesis choose inclusion current hypothesis increases number correctly classi ed positive examples breaking ties arbitrarily 
repeat safe features increase number correctly classi ed positives halt 
respect framework algorithm begins leftmost point incrementally moves rightward evaluates subsets performance training set nite penalty misclassifying negative examples halts take step strictly improves evaluated performance 
data points listed section algorithm rst put halt 
hard see exists disjunction consistent training set method nd 
fact number features selected method log jsj times larger number relevant features de nition johnson haussler 
algorithm illustrate relationships de nitions previous section 
instance incrementally useful features algorithm de nition weakly relevant de nition converse necessarily true 
fact data consistent disjunction strongly relevant features de nition may ignored algorithm due algorithm conservative nature ignores feature may cause misclassify negative example 
hand data consistent disjunction strongly relevant features incrementally useful eventually placed algorithm hypothesis algorithm may relevant feature strongly relevant due evaluation criterion 
review speci feature selection methods wehave grouped classes embed selection basic induction algorithm feature selection lter features passed induction treat feature selection wrapper induction process 

hard see follows fact exist feature add captures fraction misclassi ed positive examples 
direction nding smallest disjunction consistent set data np hard garey johnson polynomial time algorithm nd disjunctions log times larger smallest place np quasi polynomial time lund yannakakis 
selecting relevant features examples page embedded approaches feature selection methods inducing logical descriptions provide clearest example feature selection methods embedded basic induction algorithm 
fact algorithms inducing logical conjunctions mitchell vere winston greedy set cover algorithm little add remove features concept description response prediction errors new instances 
methods partial ordering describes space hypotheses algorithms typically ordering organize search concept descriptions 
theoretical results learning pure conjunctive pure disjunctive concepts encouraging 
mentioned greedy set cover approach logarithmic factor larger smallest possible 
fact warmuth personal communication notes achieve slightly better bounds pac setting halting earlier training examples misclassi ed 
resulting hypothesis guaranteed fairly small sample complexity grows logarithmically number irrelevant features 
results apply directly settings target concept conjunction disjunction list functions produced algorithm 
situations form include learning intersections halfspaces constant dimensional spaces blumer algorithms learning dnf formulas log time uniform distribution 
results greedy set cover method distribution free worst case pazzani report average case analysis simpler methods conjunctive learning imply logarithmic growth certain product distributions 
similar operations adding removing features form core methods inducing complex logical concepts methods involve routines combining features richer descriptions 
example recursive partitioning methods induction quinlan id cart breiman carry greedy search space decision trees stage evaluation function select attribute best ability classes 
partition training data attribute repeat process subset extending tree downward discrimination possible 
hellerstein extended techniques greedy set cover recursive fashion apply complex functions term dnf formulas alternation decision lists 
blum describes methods set attributes unbounded long individual example satis es reasonably small number model dealing text documents instance may contain small number possible words dictionary 
cases feature selection process clearly embedded complex algorithm 
separate conquer methods learning decision lists michalski clark niblett pagallo haussler embed feature selection similar manner 
techniques evaluation function select feature helps distinguish class add resulting test single conjunctive rule repeat process rule excludes members classes remove members rule covers repeat process remaining training cases 
clearly partitioning separate conquer methods explicitly select features inclusion branch rule preference features appear relevant irrelevant 
reason expect scale domains involve irrelevant features 
theoretical results exist methods experimental studies langley sage suggest decision tree methods scale linearly number irrelevant features certain target concepts logical conjunctions 
studies show targets concepts exhibit page selecting relevant features examples exponential growth nearest neighbor 
experiments almuallim dietterich kira rendell show substantial decreases accuracy sample size irrelevant features introduced selected boolean target concepts 
standard explanation ect involves reliance algorithms greedy selection attributes discriminate classes 
domains little interaction relevant attributes conjunctive concepts 
presence attribute interactions lead relevant feature isolation look discriminating irrelevant cause signi cant problems scheme 
parity concepts constitute extreme example situation arises target concepts 
researchers attempted remedy problems replacing greedy search techniques norton success 
course extensive search carries signi cant increase computational cost 
responded selectively de ning new features combinations existing ones greedy search letting take larger steps matheus rendell pagallo haussler 
approach directly evaluated terms ability handle large numbers irrelevant features experimentally theoretically 
filter approaches feature selection second general approach feature selection introduces separate process purpose occurs basic induction step 
reason john kohavi eger termed lter methods lter irrelevant attributes induction occurs 
preprocessing step uses general characteristics training set select features exclude 
ltering methods independent induction algorithm output combined method 
simplest ltering scheme evaluate feature individually correlation target function information measure select features highest value 
best choice determined testing holdout set 
method commonly text categorization tasks lewis lewis combination naive bayes nearest neighbor classi cation scheme achieved empirical success 
kira rendell relief algorithm follows general paradigm incorporates complex feature evaluation function 
system uses id induce decision tree training data selected features 
kononenko reports extensions method handle general types features 
almuallim dietterich describe ltering approach feature selection involves greater degree search feature space 
focus algorithm looks minimal combinations attributes perfectly discriminate classes 
method begins looking feature isolation turns pairs features triples forth halting nds combination generates pure partitions training set instances di erent classes 
focus passes original training examples described selected features algorithm decision tree induction 
comparative studies regular decision tree method showed number training cases random boolean target concepts focus una ected irrelevant attributes decision tree accuracy degraded signi cantly 
schlimmer describes related method 
note problem disappear increasing sample size 
embedded selection methods rely greedy search distinguish relevant irrelevant features early search process entire instance space available 
selecting relevant features examples page table 
characterization lter approaches feature selection terms heuristic search space feature sets 
authors system starting search halting induction point control criterion algorithm almuallim focus breadth consistency dec tree cardie greedy consistency near 
neigh 
koller sahami greedy threshold tree bayes kira rendell relief ordering threshold dec tree kubat etal 
greedy consistency naive bayes schlimmer systematic consistency singh provan greedy info 
gain bayes net carries systematic search avoid revisiting states space feature sets starting empty set adding features nds combination consistent training data 
focus relief follow feature selection decision tree construction course induction methods 
instance cardie uses ltering preprocessor nearest neighbor retrieval kubat pfurtscheller lter features naive bayesian classi er 
interestingly decision tree method relies embedded selection scheme lter produce reduced set attributes 
singh provan information theoretic metrics lter features inclusion bayesian network koller sahami employed cross entropy measure designed nd markov features naive bayes decision tree induction 
somewhat di erent vein greiner grove kogan issue consider settings helpful tutor lters conditionally attributes 
table characterizes lter methods terms dimensions described earlier section induction algorithm takes advantage reduced feature set 
typical results show improvement embedded selection methods 
experiments focused natural domains contain unknown number irrelevant features researchers almuallim dietterich kira rendell studied experimentally ect introducing features 
class lter methods constructs higher order features original ones orders terms variance explain selects best features 
statistical technique principal components analysis best known example approach generates linear combinations features vectors orthogonal original space 
empirically principal components successfully reduced dimensionality variety learning tasks 
blum kannan describe theoretical guarantees methods form target function intersection halfspaces examples chosen su ciently benign distribution 
related method independent component analysis comon incorporates similar ideas insists new features independent orthogonal 
wrapper approaches feature selection third generic approach feature selection occurs outside basic induction method uses method subroutine postprocessor 
reason john 
refer wrapper approaches see kohavi john issue 
typical wrapper algorithm searches space feature subsets see embedded lter methods evaluates page selecting relevant features examples alternative sets running induction algorithm training data estimated accuracy resulting classi er metric 
wrapper scheme long history literature statistics pattern recognition devijver kittler problem feature selection long active research topic machine learning relatively 
general argument wrapper approaches induction method feature subset provide better estimate accuracy separate measure may entirely di erent inductive bias 
example john 
argue favor wrapper method improve behavior decision tree induction 
reports experimental comparisons forward selection backward elimination impact di erent search control techniques 
john similar comparative studies including ect wrappers versus lters 
caruana freitag report third set empirical studies focusing decision trees explore variations wrapper methods 
major disadvantage wrapper methods lter methods computational cost results calling induction algorithm feature set considered 
cost led researchers invent ingenious techniques speeding evaluation process 
particular caruana freitag describe scheme caching decision trees lets algorithms search larger spaces reasonable time 
moore lee describe alternative scheme speeds feature selection reducing percentage training cases evaluation 
certainly wrapper framework focused decision tree induction 
expect methods nearest neighbor default take account attributes bene feature selection wrappers algorithms incorporate embedded schemes 
expectation led substantial body wrapper methods nearest neighbor case learning 
consider approach behavior detail 
langley sage algorithm combines wrapper idea simple nearest neighbor method assigns new instances class nearest case stored memory learning 
feature selection process effectively alters distance metric decisions account features judged relevant ignoring 
carries backward elimination search space feature sets starting features iteratively removing leads greatest improvement estimated accuracy 
system continues process estimated accuracy declines 
characterize wrapper method evaluation metric involves running nearest neighbor training data measure accuracy alternative feature sets 
particular system uses leave cross validation estimate accuracy feature set novel test cases 
approach may computationally expensive uses insight moore lee tractable 
leave technique estimates accuracy training cases holding case turn constructing classi er remaining cases seeing classi er correctly predicts case averaging results cases 
nearest neighbor simply stores training cases memory implement leave successively removing case remaining ones classify 
scheme expensive estimating accuracy training set 

natural metric involves running induction algorithm entire training data set features measuring accuracy learned structure training data 
john argue convincingly cross validation method provides better measure expected accuracy novel test cases 

kohavi incorporated idea technique inducing decision tables similarities 
selecting relevant features examples page table 
characterization wrapper approaches feature selection terms heuristic search space feature sets 
authors system starting search halting induction point control criterion algorithm aha bankert beam random comparison better near 
neigh 
caruana freitag cap comparison greedy dec tree comparison comparison better tree bayes john kohavi pfleger comparison greedy better dec tree langley sage greedy worse near 
neigh 
langley sage sel 
bayes greedy worse naive bayes moore lee race comparison greedy better near 
neigh 
singh provan greedy worse bayes net skalak random mutation times near 
neigh 
townsend weber kibler comparison better near 
neigh 
langley sage designed number experiments evaluate system 
results synthetic domains suggest features irrelevant learns high accuracy classi ers fewer instances simple nearest neighbor 
ect absent uci data sets suggesting holte nding accuracy level decision trees due highly correlated features cause di culty nearest neighbor completely irrelevant ones 
fare signi cantly better classifying chess games predicting word semantic class giving evidence domains contain irrelevant features 
researchers developed wrapper methods nearest neighbor 
instance aha bankert report technique system starts randomly selected subset features includes option beam search greedy decisions 
report impressive improvements cloud classi cation task involves numeric features 
skalak feature selection nearest neighbor starts random feature set replaces greedy search random hill climbing continues speci ed number cycles 
research wrapper methods focused classi cation moore lee townsend weber kibler combine idea nearest neighbor numeric prediction 
emphasized advantages feature selection induction methods highly sensitive irrelevant features 
langley sage shown naive bayesian classi er sensitive redundant features bene basic approach earlier 
singh provan extended idea learning complex bayesian networks 
suggests techniques feature selection improve induction algorithms variety situations presence irrelevant attributes 
caruana freitag argue methods feature selection focus nding attributes useful performance sense de nition necessarily nding relevant ones 
table characterizes orts wrapper methods terms dimensions discussed earlier induction method case direct search process 
table shows diversity techniques researchers developed heavy reliance experimental comparison variant methods 
unfortunately experiments directly study algorithms ability deal increasing numbers irrelevant features theoretical results available 
page selecting relevant features examples feature weighting methods far discussed algorithms explicitly attempt select relevant subset features 
approach especially embedded algorithms apply weighting function features ect assigning degrees perceived relevance 
separated explicit feature selection approach motivations uses methods tend di erent 
explicit feature selection generally natural result intended understood humans fed algorithm 
weighting schemes tend easier implement line incremental settings generally purely motivated ciency considerations 
weighting schemes characterized terms heuristic search viewed explicit feature selection methods 
weight space lacks partial ordering feature sets approaches feature weighting rely quite di erent forms search 
instance common techniques involve form gradient descent successive passes training instances lead iterative changes weights 
best known attribute weighting method perceptron updating rule minsky papert adds subtracts weights linear threshold unit response errors training instances 
mean squares algorithm widrow ho linear units backpropagation rumelhart hinton williams generalization multilayer neural networks additive changes set weights order reduce error training set 
baluja pomerleau issue discuss neural network approach domains degree feature relevance vary time 
perceptron weighting techniques di culty settings dominated truly irrelevant features instance see kivinen warmuth auer issue 
response littlestone developed winnow algorithm updates weights multiplicative manner additively perceptron rule 
littlestone showed line stream data consistent disjunction features winnow log mistakes 
ectively uses notion relevance de nition 
behavior degrades logarithmically number features irrelevant target concept 
generally winnow achieves logarithmic degradation concept classes conjunctions dnf formulas linear threshold functions separation positive negative examples 
concreteness version winnow algorithm disjunction learning scenario discussed sections proof littlestone theorem winnow algorithm simple version 
initialize weights wn features 
example xn output output 

algorithm mistake algorithm predicts negative positive example xi equal double value wi 
algorithm predicts positive negative example xi equal cut value wi half 

go 
embedded weighting schemes neural network avor aha reports error driven method embedded nearest neighbor learner modi es distance metric altering weights 
selecting relevant features examples page theorem winnow lg mistakes sequence examples consistent disjunction features 
proof 
rst bound number mistakes positive examples 
mistake positive example double weights target function relevant weights mistake negative example halve de nition disjunction 
furthermore weights doubled lg times weights doubled 
winnow lg mistakes positive examples 
bound number mistakes negative examples 
total weight summed features initially mistake positive example increases total weight doubling 
hand mistake negative example decreases total weight halving 
total weight drops zero 
number mistakes negative examples twice number positive examples plus lg 
adding bound number mistakes positive examples yields theorem 
general algorithms developed littlestone warmuth vovk littlestone long warmuth cesa bianchi 

kivinen warmuth describe relations approaches additive updating methods mean squares algorithm 
fact multiplicative updating schemes similar kind multiplicative probability updates occur bayesian methods results provide bounds performance bayesian updating probabilistic assumptions approach met 
experimental tests winnow related multiplicative methods natural domains revealed behavior armstrong blum studies synthetic data show scale domains thousands irrelevant features littlestone 
generally weighting methods cast ways merging advice di erent knowledge sources may generated learning 
light weighting process plays interesting dual role respect lter methods discussed earlier 
filter approaches pass output set selected features black box learning algorithm weighting approaches take input classi ers generated black box learning algorithms determine best way predictions 
hand direct analogs lter wrapper approaches exist determining weights 
stan ll ting describe lter methods conditional probability distributions weight attributes nearest neighbor 
daelemans 
di erent weighting scheme normalizes features information theoretic metric scores produced relief kira rendell 
kohavi langley yun adapted wrapper method search discretized weight space explored way feature sets 
approaches shows improvement features reports comparisons simple selection attributes 

problem irrelevant examples just attributes useful may examples better aid learning process 
suggests second broad type relevance concerns examples brie consider techniques selection 
assumed presence benevolent tutor gives informative instances near misses provides ideal training sequences winston 
robust approach involves letting learner select focus training cases 
page selecting relevant features examples researchers proposed reasons selecting examples learning 
learning algorithm computationally intensive case su cient training data available sense learn examples purposes computational ciency 
reason cost labeling high labels obtained experts unlabeled examples available easy generate 
third reason example selection increase rate learning focusing attention informative examples aiding search space hypotheses 
distinguish examples relevant viewpoint ones relevant viewpoint algorithm 
emphasizes information measures purpose 
feature selection schemes separate example selection methods embed selection process learning algorithm lter examples passing induction process wrap example selection successive calls learning technique 
refer dimension organize section distinction methods select relevant examples labeled training instances ones select unlabeled instances 
selecting labeled data rst generic approach assumes set labeled training data available learning system examples equally useful 
noted embed process example selection basic learning algorithm simple induction schemes take approach 
instance perceptron algorithm edited nearest neighbor methods incremental conjunctive methods learn example current hypothesis misclassi es 
embedded methods called conservative algorithms ignore examples hypothesis correct 
assumes training data test data taken single xed distribution guarantee high probability data training relevant success criteria testing blumer 
learning progresses learner knowledge certain parts input space increases examples understood portion space useful 
instance conservative algorithm error rate ignore training cases achieves error ignore data 
pac model learning algorithms need roughly double number examples seen order halve error rate schapire freund blumer 
conservative algorithms number examples learning proportional error rate number new examples algorithm time wishes halve error rate remains roughly constant 
number examples achieve error rate really just logarithmic linear 
result holds conservative algorithms embed example selection process learning explicit example selection achieve similar ects induction methods 
particular schapire describes wrapper method called boosting takes generic learning algorithm adjusts distribution removing training data algorithm behavior 
basic idea learning progresses booster samples input distribution keep accuracy learner near random guessing 
result learning process focuses currently hard data 
schapire shown boosting lets achieve logarithmic examples described quite general conditions freund improved 
littlestone shown variant naive bayes learns errors deal better irrelevant features standard version updates statistics example 
shows exist interactions problems feature selection example selection 
selecting relevant features examples page technique 
experimental front drucker 
shown boosting improve accuracy neural network methods tasks involving optical character recognition 
approach especially appropriate techniques backpropagation training expensive prediction 
class wrapper methods example selection originated experimental study induction 
quinlan reports windowing technique designed reduce time needed construct decision large training sets 
windowing selects random sample training data induce initial decision tree uses tree classify remaining examples 
misclassi ed cases method selects random set augment original sample constructs new decision tree forth repeating process tree correctly classi es training data 
quinlan reports windowing led substantial reduction processing time large collection chess endgames catlett describes wrapper method called designed larger training sets 
john langley report simpler wrappers determine proper size randomly selected training sample 
lewis catlett describe lter approach selection labeled data techniques machine learning literature embedded wrapper methods 
imagine simple techniques cleaning training data say removing inconsistent examples identical class methods widely 
pass sampling training data constitute ltering research sampling boosting windowing 
selecting unlabeled data learner select data labeled 
useful scenarios unlabeled data plentiful labeling process expensive 
generic approach problem induction algorithm maintains set hypotheses consistent training data called query committee seung 
unlabeled instance method selects hypotheses random consistent set di erent predictions requests label instance 
basic idea informative relevant examples pass test hypotheses classify way 
unfortunately obtain theoretical results query committee requires stronger constraints space hypotheses boosting 
speci cally method requires ability sample random consistent hypotheses quite di cult major topic algorithmic research sinclair jerrum dyer frieze kannan lovasz simonovits 
larger body algorithms generate examples choosing heading membership query algorithms theoretical community empirical community 
common technique algorithms sort take example slightly alter feature values determine ect classi cation 
instance take examples di erent labels walk determine point desired classi cation changes turn determine relevant features tying earlier discussion 
class methods ectively designs critical experiments distinguish competing hypotheses letting eliminate competitors reduce complexity task 
mitchell suggested information theoretic approach example selection sammut 
boosting clear empirical uses originally developed theoretical goal showing weak learning implies strong learning pac model 
words algorithm perform somewhat better guessing distribution hard core function learned boost performance produce high quality predictions 
page selecting relevant features examples banerji gross formal methods demonstrated advantage empirically 
active learning continued tradition instance cohn ghahramani jordan report successful results system selects examples designed reduce learner variance 
parallel theoretical researchers angluin angluin rivest schapire jackson shown ability generate queries greatly enlarges types concept classes guarantee polynomial time learning 
queries experimentation emphasized simple classi cation learning orts addressed complex learning tasks 
example knobe knobe grammar induction system query oracle legality candidate strings distinguish competing hypotheses kulkarni simon coast design critical experiments distinguish competing hypotheses scienti domains 
shen simon gil explored uses experimentation learning action models planning tasks 
learning systems incorporate strategies exploring portions instance space encountered obtain representative information domain 
example scott markovitch adapt idea unsupervised learning situations methods reinforcement learning include bias exploring unfamiliar parts state space lin 
approaches considerably increase learning rates random presentations 
selecting querying unlabeled data embedded methods angluin 
blum 
describe theoretical results wrapper query method applied algorithm 
speci cally show membership queries available algorithm polynomial mistake bound learning reasonable concept class converted automated way number mistakes plus queries logarithmic dependence number irrelevant features 
basic idea gradually grow set features known relevant algorithm mistake queries determine mistake results missing relevant feature place new relevant feature set 

challenges relevance research despite activity associated progress methods selecting relevant features examples remain directions machine learning improve study important problems 
outline research challenges theoretical empirical learning communities 
theoretical challenges claim sense central open theoretical problems machine learning revolve questions nding relevant features 
instance consider known question polynomial time algorithms guarantee learning polynomial size dnf formulas pac uniform distribution models 
consider similar question polynomial size decision trees learnable model 
questions include open problem special case exist polynomial time algorithm learning class boolean functions log relevant features pac uniform distribution models 
special case function log relevant features de nition written truth table having entries small decision tree small dnf representation note learning problem trivial knew priori log variables selecting relevant features examples page relevant 
hand problem appears quite di cult special case 
instance algorithm solve problem need unusual sense class proven impossible learn statistical query model kearns blum 
issues nding relevant features core classes hard 
practical matter unclear experimentally test proposed algorithm problem distribution target functions 
fact functions random truth tables class generally easy 
allow easier experimental testing algorithms problem speci distribution target functions quite hard uniform random examples convenience number relevant features log select random disjoint sets ng size log input compute parity bits indexed contain odd number ones 
majority function bits indexed contain ones zeroes output exclusive results 
second theoretical challenge develop algorithms focusing ability apply complex target classes decision lists parity functions general linear threshold functions 
greatly extend class problems exist positive results line settings 
framework example selection important direction connect membership query models advantage generally algorithmic assume arbitrary points input space may probed ltering unlabeled instances apply xed data stream available require solving computationally hard subproblem 
challenge theoretically analyze ways example selection aid feature selection process 
empirical challenges considerable remains empirical front urgent needs studies challenging data sets 
instance domains date involved features 
exceptions aha bankert study cloud classi cation attributes koller sahami information retrieval attributes typical experiments dealt far fewer features 
langley sage results nearest neighbor method suggest widely uci data sets completely irrelevant attributes 
hindsight natural diagnostic domains experts tend ask relevant features ignore ones 
believe real world domains nd data sets substantial fraction irrelevant attributes want test adequately ideas feature selection 
experiments synthetic data important roles play study feature selection methods 
data sets systematically vary factors interest relevant irrelevant attributes holding factors constant 
way directly measure sample complexity algorithms function factors showing ability scale domains irrelevant features 
distinguish synthetic data systematic experiments reliance isolated arti cial data sets monks problems useful 

fact class easy learn algorithm active membership queries examples choosing 
algorithm learns larger class decision trees membership queries exact leaning model algorithm jackson learns larger class general dnf formulas membership queries respect uniform distribution 

instance classi cation example positive rst bits number ones making parity bits ones zeros majority function xor quantities 
page selecting relevant features examples challenging domains features higher proportion irrelevant ones require sophisticated methods feature selection 
increases ciency increase number states examined constant factor improvements eliminate problems caused exponential growth number feature sets 
viewing problems terms heuristic search suggests places look solutions 
general invent selecting initial feature set start search formulate search control methods take advantage structure space feature sets devise improved frameworks evaluating usefulness alternative feature sets design better halting criteria improve ciency sacri cing accuracy 
research area compare carefully behavior feature selection attribute weighting schemes 
presumably approach advantages leaving open question best answered experiment preferably informed experiments designed test speci hypotheses approaches relevance 
generally feature selection example selection tasks intimately related need studies designed help understand quantify relationship 
empirical example selection gross cohn dealt low dimensional spaces approach clearly holds greater potential domains involving irrelevant features 
resolving basic issues sort promises keep eld machine learning occupied years come 
research supported part 
ccr national science foundation sloan foundation research fellowship 
ce naval research 
researchers active area feature example selection contributed directly indirectly ideas 
referees editors issue helpful comments suggestions 
aha 

study instance algorithms supervised learning tasks mathematical empirical psychological evaluations 
doctoral dissertation department information computer science university california irvine 
aha bankert 

comparative evaluation sequential feature selection algorithms 
fisher 
lenz eds arti cial intelligence statistics new york springer verlag 
almuallim dietterich 

learning irrelevant features 
proceedings ninth national conference onarti cial intelligence pp 

san jose ca aaai press 
angluin 

learning regular sets queries counterexamples 
information computation 
angluin hellerstein karpinski 

learning read formulas queries 
journal acm 
armstrong freitag joachims mitchell 

webwatcher learning apprentice world wide web 
aaai spring symposium information gathering heterogeneous distributed environments 
blum 

learning boolean functions nite attribute space 
machine learning 
blum 

empirical support winnow weighted majority algorithms results calendar scheduling domain 
proceedings twelfth international conference machine learning pp 

lake tahoe ca morgan kaufmann 
selecting relevant features examples page blum furst jackson kearns mansour rudich 

weakly learning dnf characterizing statistical query learning fourier analysis 
proceedings th annual acm symposium theory computing pp 
blum hellerstein littlestone 

learning presence nitely nitely irrelevant attributes 
journal computer system sciences 
blum kannan 

learning intersection halfspaces uniform distribution 
proceedings th annual ieee symposium foundations computer science pp 

ieee 
blumer ehrenfeucht haussler warmuth 

occam razor 
information processing letters 
blumer ehrenfeucht haussler warmuth 

learnability vapnik chervonenkis dimension 
journal acm 
breiman friedman olshen stone 

classi cation regression trees 
belmont ca wadsworth 


exact learning monotone theory 
proceedings ieee symposium foundations computer science pp 

ieee palo alto ca 
cardie 

decision trees improve case learning 
proceedings tenth international conference machine learning pp 

amherst ma morgan kaufmann 
caruana freitag 

greedy attribute selection 
proceedings eleventh international conference machine learning pp 

new brunswick nj morgan kaufmann 
caruana freitag 

useful relevance 
working notes aaai fall symposium relevance pp 

new orleans la aaai press 
catlett 

choosing attributes ciently 
proceedings ninth international conference machine learning pp 

aberdeen scotland morgan kaufmann 
cesa bianchi freund helmbold haussler schapire warmuth 

expert advice 
proceedings annual acm symposium theory computing pp 

clark niblett 

cn induction algorithm 
machine learning 
cohn ghahramani jordan 

active learning statistical models 
journal arti cial intelligence research 
comon 

independent component analysis new concept 
signal processing 
cover hart 

nearest neighbor pattern classi cation 
ieee transactions information theory 
daelemans 

acquisition stress data oriented approach 
computational linguistics 
devijver kittler 

pattern recognition statistical approach 
new york prentice hall 
hellerstein 

pac learning irrelevant attributes 
proceedings ieee symposium foundations computer science pp 

ieee 


evaluation feature selection methods application computer security technical report cse 
davis university california department computer science 
page selecting relevant features examples drucker schapire simard 

improving performance neural networks boosting algorithm 
moody hanson lippmann eds advances neural information processing systems vol 

san francisco morgan kaufmann 
drucker cortes jackel lecun vapnik 

boosting machine learning algorithms 
proceedings eleventh international conference machine learning pp 

new brunswick nj morgan kaufmann 
dyer frieze kannan 

random polynomial time algorithm approximating volume convex bodies 
proceedings annual acm symposium theory computing pp 

freund 

boosting weak learning algorithm majority 
proceedings third annual workshop computational learning theory pp 

san francisco morgan kaufmann 
freund 

improved boosting algorithm implications learning complexity 
proceedings fifth annual acm workshop computational learning theory pp 

acm press 
garey johnson 

computers intractability guide theory np completeness 
san francisco freeman 
gil 

cient domain independent experimentation 
proceedings tenth international conference machine learning pp 

amherst ma morgan kaufmann 
gross 

concept acquisition attribute evolution experiment selection 
doctoral dissertation school computer science carnegie mellon university pittsburgh pa haussler 

quantifying inductive bias concept learning 
proceedings fifth national conference onarti cial intelligence pp 

philadelphia aaai press 
holte 

simple classi cation rules perform commonly domains 
machine learning 
jackson 

cient membership query algorithm learning dnf respect uniform distribution 
proceedings ieee symposium foundations computer science 
ieee 
john kohavi eger 

irrelevant features subset selection problem 
proceedings eleventh international conference machine learning pp 

new brunswick nj morgan kaufmann 
john langley 

static vs dynamic sampling data mining 
proceedings second international conference knowledge discovery data mining pp 

portland aaai press 


principal component analysis 
new york springer verlag 
johnson 

approximation algorithms combinatorial problems 
journal computer system sciences 
kearns vazirani 

computational learning theory 
cambridge ma mit press 
kira rendell 

practical approach feature selection 
proceedings ninth international conference machine learning pp 

aberdeen scotland morgan kaufmann 
kivinen warmuth 

additive versus exponentiated gradient updates linear prediction 
proceedings th annual acm symposium theory computing pp 

new york acm press 
selecting relevant features examples page kohavi 

power decision tables 
proceedings eighth european conference machine learning 
kohavi langley yun 

utility feature weighting nearest neighbor algorithms 
proceedings ninth european conference machine learning 
prague springer verlag 
knobe knobe 

method inferring context free grammars 
information control 
koller sahami 

optimal feature selection 
proceedings thirteenth international conference machine learning 
bari italy morgan kaufmann 
kononenko 

estimating attributes analysis extensions relief 
proceedings seventh european conference machine learning 
kubat pfurtscheller 

discovering patterns eeg signals comparative study methods 
proceedings european conference machine learning pp 

vienna springer verlag 
kulkarni simon 

experimentation machine discovery 
langley eds computational models scienti discovery theory formation 
san francisco morgan kaufmann 
langley iba 

average case analysis nearest neighbor algorithm 
proceedings thirteenth international joint conference onarti cial intelligence pp 

chambery france 
langley sage 

oblivious decision trees cases 
working notes aaai workshop case reasoning pp 

seattle wa aaai press 
langley sage 

induction selective bayesian classi ers 
proceedings tenth conference arti cial intelligence pp 

seattle wa morgan kaufmann 
langley sage 

scaling domains irrelevant features 
greiner ed computational learning theory natural learning systems vol 

cambridge ma mit press 
lewis 

representation learning information retrieval 
doctoral dissertation department computer science university massachusetts amherst 
available technical report um cs 
lewis 

feature selection feature extraction text categorization 
natural language workshop pp 

san morgan kaufmann 
lewis catlett 

heterogeneous uncertainty sampling 
proceedings eleventh international conference machine learning pp 

new brunswick nj morgan kaufmann 
lin 

self improving reactive agents reinforcement learning planning teaching 
machine learning 
littlestone 

learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 
littlestone long warmuth 

line learning linear functions 
proceedings third annual acm symposium theory computing pp 

new orleans acm press 
littlestone 

relative 
mozer jordan petsche eds advances neural information processing systems vol 

denver mit press 
page selecting relevant features examples littlestone warmuth 

weighted majority algorithm 
information computation 
lovasz simonovits 

randomized complexity diameter 
proceedings ieee symposium foundations computer science pp 

ieee 
lund yannakakis 

hardness approximating minimization problems 
proceedings annual acm symposium theory computing pp 

matheus rendell 

constructive induction decision trees 
proceedings eleventh international joint conference arti cial intelligence pp 

detroit mi morgan kaufmann 
michalski 

pattern recognition rule guided inductive inference 
ieee transactions pattern analysis machine intelligence 
minsky papert 

perceptrons computational geometry 
mit press 
mitchell 

generalization search 
arti cial intelligence 
reprinted shavlik dietterich eds 
readings machine learning 
san francisco ca morgan kaufmann 
moore lee 

cient algorithms minimizing cross validation error 
proceedings eleventh international conference machine learning pp 

new brunswick nj morgan kaufmann 
norton 

generating better decision trees 
proceedings eleventh international conference arti cial intelligence pp 

detroit mi morgan kaufmann 
pazzani 

framework average case analysis conjunctive learning algorithms 
machine learning 
pagallo haussler boolean feature discovery empirical learning 
machine learning 
quinlan 

learning cient classi cation procedures application chess games 
michalski carbonell mitchell eds machine learning arti cial intelligence approach 
san francisco ca morgan kaufmann 
quinlan 

programs machine learning 
san francisco morgan kaufmann 


computational approach theory revision 
langley eds computational models scienti discovery theory formation 
san francisco ca morgan kaufmann 
rivest schapire 

inference nite automata homing sequences 
information computation 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart mcclelland eds parallel distributed processing explorations microstructure vol 

cambridge ma mit press 
sammut banerji 

learning concepts asking questions 
michalski carbonell mitchell eds machine learning arti cial intelligence approach vol 

san francisco ca morgan kaufmann 
schapire 

strength weak learnability 
machine learning 
schlimmer 

ciently inducing determinations complete cient search algorithm uses optimal pruning 
proceedings tenth international conference machine learning pp 

amherst ma morgan kaufmann 
selecting relevant features examples page scott 

representation generation exploratory learning system 
fisher pazzani langley eds concept formation knowledge experience unsupervised learning 
san francisco ca morgan kaufmann 
seung opper sompolinsky 

query committee 
proceedings fifth annual workshop computational learning theory pp 

new york acm press 
shen simon 

rule creation rule learning environmental exploration 
proceedings eleventh international joint conference arti cial intelligence pp 

detroit mi morgan kaufmann 
sinclair jerrum 

approximate counting uniform generation rapidly mixing markov chains 
information computation 
singh provan 

comparison induction algorithms selective non selective bayesian classi ers 
proceedings twelfth international conference machine learning pp 

lake tahoe ca morgan kaufmann 
singh provan 

cient learning selective bayesian network classi ers 
proceedings thirteenth international conference machine learning 
bari italy morgan kaufmann 
skalak 

prototype feature selection sampling random mutation hill climbing algorithms 
proceedings eleventh international conference machine learning pp 

new brunswick nj morgan kaufmann 
stan ll 

memory reasoning applied english pronunciation 
proceedings sixth national conference onarti cial intelligence pp 

seattle wa aaai press 
ting 

discretization continuous valued attributes instance learning technical report 
sydney university sydney basser department computer science 
townsend weber kibler 

instance prediction continuous values 
working notes aaai workshop case reasoning pp 

seattle wa aaai press 


learning dnf uniform distribution polynomial time 
proceedings third annual workshop computational learning theory pp 

morgan kaufmann 
vere 

induction concepts predicate calculus 
proceedings fourth international joint conference onarti cial intelligence pp 

ussr morgan kaufmann 
vovk 

aggregating strategies 
proceedings third annual workshop computational learning theory pp 

morgan kaufmann 
widrow ho 

adaptive switching circuits 
ire convention record pp 

winston 

learning structural descriptions examples 
winston ed psychology computer vision 
new york mcgraw hill 
