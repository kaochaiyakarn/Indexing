dissertation andreas stolcke approved chair date university california berkeley date date date bayesian learning probabilistic language models copyright andreas stolcke bayesian learning probabilistic language models andreas stolcke general topic thesis probabilistic modeling language particular natural language 
probabilistic language modeling characterizes strings phonemes words certain domain terms probability distribution possible strings domain 
probabilistic language modeling applied wide range problems years traditional uses speech recognition applications biological sequence modeling 
main contribution thesis particular approach learning problem probabilistic language models known bayesian model merging 
approach characterize follows 
models built batch mode incrementally samples incorporating individual samples working model uniform small number simple operators works gradually transform instance model generalized model abstracts data 
instance parts model coexist generalized ones depending degree similarity observed samples allowing model adapt non uniform coverage sample space 
generalization process driven controlled uniform probabilistic metric bayesian posterior probability model integrating criteria goodness fit respect data notion model simplicity occam razor 
bayesian model merging framework instantiated different classes probabilistic models hidden markov models hmms stochastic context free grammars scfgs simple proba attribute grammars 
theoretical background various applications case studies including induction multiple pronunciation word models speech recogni tion hmms data driven learning syntactic structures scfgs learning simple sentence meaning associations examples 
apart language learning issues number related computational problems involving proba context free grammars discussed 
version earley parser solves standard problems associated scfgs efficiently including computation sentence probabilities sentence prefix probabilities finding parses estimation grammar parameters 
describe algorithm computes gram statistics scfg solving linear systems derived grammar 
method effective tool transform part probabilistic knowledge structured language model unstructured low level form applications speech decoding 
show problem just instance larger class related ones average sentence length derivation entropy solvable computational technique 
introductory chapter tries unified view various model types algorithms literature issues model learning estimation 
prof jerome feldman dissertation chair acknowledgments life berkeley icsi marked wonderful wealth friends colleagues collaborators acquaintances 
hard justice ways people probably forget mention enriched years extraordinary place try 
jerry feldman advisor patiently guides students letting wander ways true 
steve omohundro collaborator friend endless source ideas insight enthusiasm 
extended academic support group taught know berkeley place approximate order appearance john canny bob wilensky stuart russell george lakoff chuck fillmore zadeh jitendra malik john ousterhout david culler 
peter cheeseman wray buntine introduced powers bayes rule just right moment willing source advice may book shelves soon 
nelson morgan adopted group continues valuable source advice real data rap 
ahmad wu terry regier ben gomes friends collaborators making fun years 
icsi ai realization groups bertrand brian kingsbury chris bregler chuck wooters dan jurafsky david bailey david stoutamire eric gary herv bourlard jeff bilmes joachim diederich jonathan segal shastri srini narayanan steve greenberg susan weber konig 
apart utterly fun crowd crucial obtaining results 
icsi staff past developed perfection providing unobtrusive support afternoon tea cookies 
special go friendship help feeling quite literally home 
rest berkeley ai cogsci crowd adele goldberg francesca barrientos jane edwards marti hearst mike braverman mike nj peter norvig short advice encouragement humor rhythm blue notes 
kathryn crabtree indispensable steering possible clear university regulations requirements 
academic advisors munich erwin christian freksa brauer theo ways laid foundations encouraged try luck berkeley 
fernando pereira ran internet indispensable valuable insights bibliographic 
foster parents bill donna everybody wcpc feel welcome 
david blake berkeley student par excellence important source information american berkeley culture 
parents support encouragement understanding count say possible 
susanne patrick oliver wasn 
pages dedicated 
ii contents list figures vii list tables viii overview structural learning probabilistic grammars probabilistic finite state models miniature language learning task miscellaneous topics bibliographical note foundations preliminaries probabilistic language models interpretation probabilities example gram models probabilistic grammars random string generators multinomial distributions parameter estimation likelihood cross entropy grammars hidden variables mixture grammars expectation maximization viterbi derivations approximate em hidden markov models stochastic context free grammars levels learning model merging parameter estimation model merging curve fitting example knowing bayesian model inference need inductive bias posterior probabilities bayesian model merging minimum description length structure vs parameter priors description length priors iii contents iv posteriors grammar structures hidden markov models overview hidden markov models definitions hmm estimation viterbi approximation hmm merging likelihood hmm merging example priors hidden markov models smaller hmms preferred 
algorithm implementation issues efficient sample incorporation computing candidate merges model evaluation viterbi paths global prior weighting search issues related non probabilistic finite state models bayesian approaches state splitting algorithms probabilistic approaches evaluation case studies finite state language induction phonetic word models labeled speech multiple pronunciation word models speech recognition research stochastic context free grammars overview stochastic context free grammars definitions scfg estimation viterbi parses scfg merging sample incorporation merging operators example bracketed samples scfg priors search strategies miscellaneous related bayesian grammar learning enumeration merging chunking approaches cook grammatical inference hill climbing evaluation formal language benchmarks natural language syntax sample ordering contents summary discussion probabilistic attribute grammars probabilistic attribute grammars definitions example pag estimation pag merging sample incorporation nonterminal merging chunking feature operators efficient search feature operations pag priors experiments examples syntactic constraints imposed attributes limitations extensions expressive feature constraints hierarchical features trade offs context free feature descriptions summary efficient parsing stochastic context free grammars overview earley parsing probabilistic earley parsing stochastic context free grammars earley paths probabilities forward inner probabilities computing forward inner probabilities coping recursion example null productions existence complexity issues summary extensions viterbi parses rule probability estimation parsing bracketed inputs robust parsing implementation issues prediction completion efficient parsing large sparse grammars discussion relation finite state models online pruning relation probabilistic lr parsing related contents vi simple typology scfg algorithms summary appendix lr item probabilities conditional forward probabilities grams stochastic context free grammars background motivation algorithm normal form scfgs probabilities expectations computing expectations computing prefix suffix probabilities grams containing string boundaries efficiency complexity issues consistency scfgs experiments summary appendix related problems expected string length derivation entropy expected number nonterminal occurrences grammar types directions formal characterization learning dynamics noisy samples informative search heuristics biases induction model specialization new applications new types probabilistic models bibliography list figures hidden markov models word task simple hmm 
simple stochastic context free grammar generating palindromes approximation curve best merging segment models 
dimensional symmetrical dirichlet prior 
sequence models obtained merging samples case study hmm generating test language case study results induction runs 
case study bw derived hmm structures fail generalization 
case study redundant bw derived hmm structure case study generalization depending global prior weight 
case study ii hmm generating test language case study ii results induction runs 
case study ii bw derived hmm structures fail generalization 
case study ii redundant bw derived hmm structure case study ii generalization depending global prior weight 
initial hmm constructed samples word 
merged hmm constructed samples word 
number hmm states function number samples incorporated incremental merging 
hybrid mlp hmm training merging procedure speech understanding system wooters 
geometric poisson length distributions identical distribution mean 
example grammars langley 
ways generating substring nonterminal vii list tables results timit trials model building methods 
test grammars cook 

example non probabilistic earley parsing 
earley chart constructed parse robust parsing simple grammar table 
tentative typology scfg algorithms prevailing directionality sparseness cfg 
corpora language model statistics 
speech recognition accuracy various language models 
viii chapter overview general topic thesis probabilistic modeling language particular natural language 
probabilistic language modeling tries characterize strings phonemes words certain domain terms probability distribution possible strings domain 
language mathematical framework probability theory formalize questions language string observed 
language string derivations analyses string 
languages similar different 
language described way corpus samples description data language 
problems numerous applications success probabilistic approach reflected large number diversity domains probabilistic language models applied advantage years 
include established applications speech recognition understanding new ones biological sequence modeling 
prerequisite applications learning problem sample corpus assumed representative domain find adequate probabilistic description 
main topic thesis particular approach learning problem probabilistic language models characteristic features models built batch mode incrementally samples incorporating individual samples working model uniform small number simple operators works gradually transform instance model generalized model abstracts data 
chapter 
instance parts model coexist generalized ones depending degree similarity observed samples allowing model adapt non uniform coverage sample space 
generalization process driven controlled uniform probabilistic metric bayesian posterior probability model integrating criteria goodness fit respect data notion model simplicity occam razor 
approach quite general nature scope comparable say mitchell version spaces mitchell needs instantiated concrete domains study utility practicality 
different types probabilistic models hidden markov models hmms stochastic context free grammars scfgs simple probabilistic attribute grammars 
chapter presents basic concepts mathematical formalisms underlying probabilistic language models bayesian learning introduces approach learning general terms 
chapter hmms chapter scfgs chapter attribute grammars describe particular versions learning approach various types languages models 
unfortunately chapters chapter entirely self contained form natural progression ideas formalisms 
chapters address various computational problems aside learning arise connection probabilistic context free language models 
chapter deals probabilistic parsing chapter gives algorithm approximating context free grammars simpler gram models 
chapters nearly self contained need read particular order respect preceding chapters 
research 
chapter discusses general open issues arising gives outlook virtually probabilistic grammar types algorithms described chapters implemented integrated object oriented framework clos 
result purports flexible extensible environment experimentation probabilistic language models 
documentation system available separately stolcke 
background 
remainder general highlights historical structural learning probabilistic grammars probabilistic language models grammars firmly established number areas time automatic speech recognition major applications 
important factor probabilistic nature weighted predictions data person plural stylistic uniformity reflect fact done collaboration 
bibliographic authored publications chapter 
chapter 
conditioned evidence seen past framework probability theory consistent mathematical basis 
fundamental feature truly useful probabilistic models adapt able effective algorithms tuning model previously observed data optimize predictions new data assuming old new data obey statistics 
probabilistic finite state models example point probabilistic finite state models known hidden markov models hmms routinely speech recognition model phone sequences making words recognized 
top part shows simple word model phonetic realization word corresponds path network states transitions probabilities indicated 
network structure training corpus data modeled standard algorithms optimizing estimating probability parameters hmm fit data 
fundamental problem obtain suitable model structure place 
basic idea construct initial model networks observed data shown bottom part gradually transform compact general form process called model merging 
see fundamental tension optimizing fit model observed data goal generalizing new data 
bayesian notions prior posterior model probabilities formalize conflicting goals derive combined criterion allows finding compromise 
chapter describes approach structural learning hmms discusses issues methods recurring chapters 
miniature language learning task additional motivation came seemingly simple task proposed feldman 
construct machine learner generalize usage examples natural language fragment novel instances arbitrary natural language 
shows essential elements miniature language learning problem informally known task 
goal learn language exposure pairs corresponding dimensional pictures natural language descriptions 
syntax semantics language intentionally limited problem manageable 
purpose proposal highlight certain fundamental problems traditional cognitive science theories including issue dependence underlying conceptual system grounding meaning categorization perception explored ongoing research regier feldman 
purposes simpler subproblem interdisciplinary task pairs sentences associated idealized semantics order logic formulae construct adequate formal description relation language 
chapter 
start ae start ae ae ae hidden markov models word generalized word model derived model merging 
initial model derived observed data prior model merging 
symbol represents glottal 
chapters viewed study bridge gap formal syn tax semantics learning problem probabilistic methods originally developed constrained finite state models mentioned earlier 
relax finite state constraint syntactic part learning problem move probabilistic context free models chapter 
result extended model merging algorithm compares favorably alternative computational context free learning approaches proposed literature coping significant portion syntactic structures task 
final extension model merging approach designed allow combined learning miniature language syntax semantics chapter 
basic idea capture correspondence language elements meaning attaching probabilistic attributes nonterminals likewise probabilistic context free grammar 
approach turns impose strong restrictions learning expressive power model sufficient learn basic version chapter 
circle square triangle left square task learner exposed picture sentence pairs shown fixed arbitrary natural language generalize able determine truth novel picture sentence pairs 
baseline version task uses simple static binary spatial relations correspondingly limited syntax shown 
various variants extend scope task feldman 

chapter 
language 
resulting system full justice original problem provides useful example possibilities limitations model merging approach 
miscellaneous topics second part thesis discuss various computational issues related directly part learning problem probabilistic language models 
focus stochastic context free grammars scfgs spite shortcoming represent current state art computational approaches probabilistic natural language processing 
historically algorithms developed meet specific needs part implementation learning algorithms described part result tie ins ongoing project speech understanding 
jurafsky 
chapter describes parser scfgs solves standard tasks probabilistic grammars single elegant efficient framework probabilistic generalization earley algorithm regular cfgs 
particular show fairly straightforward extensions earley parser allow computation sentence probabilities parses estimation grammar parameters 
algorithm special interest allows efficient incremental computation probabilities possible words prefixes sentences generated scfg 
chapter solves related problem probabilistic context free description language compute low level statistics type probability word previous words statistics known gram probabilities essential number standard language modeling applications easily draw directly higher level models scfgs computational reasons 
algorithm useful tool transfering higher level lower level representation 
appendix chapter surveys number related computational tasks scfgs computing average sentence length solvable manner similar gram problem 
bibliographical note various portions thesis published previously listed chapter stolcke omohundro stolcke omohundro wooters stolcke chapter stolcke omohundro chapter stolcke chapter stolcke segal jurafsky 
remaining errors course sole responsibility author 
chapter foundations preliminaries chapter review basic definitions concepts chapters 
introduce classes probabilistic language models subject thesis relevant standard algorithms 
range covered strictly necessary rest thesis order point generalizations connections various approaches specialized literature 
probabilistic language models probabilistic language model probabilistic grammar generalizes classical notion grammar acceptor strings probabilistic domain 
specifically probabilistic language generated grammar probability distribution strings written random variable ranging strings denotes language question 
notation conditional distribution think particular probability conditional probability drawing string previously fixed language 
suggests language drawn probabilistically pool possible languages crucial ingredient approach developed 
domain probability finite strings fixed discrete alphabet probabilistic grammar model description distribution 
language generated model writing probabilities distributions usually collapse includes standard models probabilistic attribute grammars deferred chapter 
chapter 
foundations distinction language distribution grammar description simply write interpretation probabilities probabilities assigned strings model interpreted long term relative frequencies strings assuming samples 
frequentist prob introduce probabilities entities correspond outcomes repeatable experiments models 
cases plausible think probabilities degrees belief inter probabilities 
classic cox shows interpretations compatible observe calculus simple assumptions beliefs compositional properties 
unification frequentist treatment probabilities fundamental bayesian approach described 
allows probabilities common currency relating observations beliefs underlying explanations observations 
example gram models simple example illustrates notions introduces useful standard tool 
gram model defines probabilityof string string length string 
th symbol string product parameters gram model probabilities special delimiter marking 
convenient allow prefix take value denote left string 
context clear string stands 
useful compare definition expression true probability distribution strings arrived repeated condi see gram models represent exactly distributions symbol independent string just immediately preceding symbols including position left string boundary 
chapter 
foundations number parameters gram models grows exponentially cases bigram models trigram models practical importance 
bigram trigram models popular various applications especially speech decoding ney approximate true distributions language elements characters words known violate independence assumption embodied 
essentially truncated version true joint probability grams sense natural choice approximation appropriate extent symbol occurrences tend independent distance occurrences increases 
course important cases natural language assumption blatantly wrong 
example distribution lexical elements natural languages constrained phrase structures relate words essentially arbitrary distances 
main motivation moving minimum stochastic context free models main subjects thesis 
probabilistic grammars random string generators abstractly associate probabilistic language corresponding random string generator device generates strings stochastically distribution probabilistic grammar usually describes making generator concrete 
example grams generator output string symbols left right choosing symbol probability lookup table indexed choice causes generator 
tuple previous symbols 
possible probability string equivalent joint probability sequence roles dice die corresponding conditional distribution certain gram 
roughly sense gram models represent brute force approach probabilistic language modeling dice 
die faces possible symbol 
linguistically interesting due inability describe complex structures non local dependencies language elements 
applications give surprisingly results easy improve sophisticated models 
reason processed efficiently come method choice tasks speech decoding 
chapter revisit gram models describe algorithm deriving complex probabilistic models estimating directly data 
multinomial distributions language models considered thesis described generating strings sequence generalized die tosses 
useful review basic properties multinomial distributions class distributions describing outcomes generalized dice 
actual number account left contexts containing marker 
chapter 
foundations parameters multinomial distribution multinomial short dimension specified tuple probability possible outcomes independent samples drawn probability depends counts outcome 
multinomial distribution describes write 
distribution unordered samples sequences outcomes permutations considered joint event 
ordered samples distribution simply multinomial coefficient dropped 
usual scenario thesis outcomes samples wants draw case difference constant safely ignored 
mainly ordered sample scenario described simpler formula 
formulas summarize means expectations variances covariances respec tively multinomial 
parameter estimation cov var fundamental problem probabilistic language modeling inference parameter values vectors gram grammar available data 
parameters fully describe choice model parameter estimation subsumes learning problem grammars class 
possible parameterize aspects grammar see useful draw distinction parameters continuous type grams structural aspects model 
language statistics estimator parameter putative value set samples actual value simply function computes formalized notions bias consistency 
estimator unbiased expected value data drawn distribution specified quantify bias data increases 
intuitive requirement close estimator converges true amount chapter 
foundations example multinomials unbiased consistent estimator component probabilities observed averages zero bias desirable attested extensive literature modify estimates gram similar models avoid estimates values specifically linguistic domains 
case outcome observed 
domains realistic sample sizes expects counts remain zero underlying parameters wants bias estimates away zero 
see church gale survey comparison various methods doing 
reason introducing bias reduce variance estimator geman 
likelihood cross entropy data fixed view function likelihood function 
large class estimators called maximum likelihood ml estimators defined maxima likelihood functions 
example simple estimator multinomials happens ml estimator setting parameters data 
empirical averages maximizes probability observed intuitively high likelihood means model fits data 
model allocates fixed probability mass unity space possible strings sequences strings 
maximize probability observed strings unobserved ones little probability possible constraints model 
fact model class allows assigning probabilities samples relative frequencies best 
alternative measure fit closeness model distribution concept entropy 
relative entropy known kullback leibler distance distributions defined written log log second sum familiar entropy term distributions appear 
call term cross entropy log distribution sum entropy giving literature terms relative entropy cross entropy synonymously refer probable reason minimization see 
find useful intuitive distinction done think cross chapter 
foundations shown follows equality distributions identical 
iff justifies thinking distributions distributions models 
computing pseudo distance scenarios known model 
example relative exactly presumes knowledge full distributions typical entropy define estimator model parameters estimated value minimizes compute distribution samples drawn 
known directly 
note minimization purposes term relevant unknown constant remains fixed varied 
leaves expected value log true distribution sample corpus log minimized estimated averaging see estimated cross entropy proportional factor log likelihood 
ml estimators effectively minimum relative entropy estimators 
grammars hidden variables grammars considered generate samples combination sequence choices give rise sample uniquely determined gram grammars 
uniquely identify sequence choices leading generation complete string inspecting grams occurring string left right order 
knowing grams compute probabilities probability string products 
complete sequence generator events multinomial samples generate string called derivation 
gram models derivation strings string 
grammars generate strings derivation called ambiguous 
derivation probability product probabilities multinomial outcomes making derivation 
general string probability sum derivation probabilities derivations generating string derives think derivation parts thereof conveniently decomposed unobserved hidden random variable 
relative entropy symmetric obey triangle inequality 
chapter 
foundations mixture grammars simple example grammars generate strings subsidiary component grammars 
step derivation choosing submodels generate string single mixture probability chosen submodel generates string distribution represents 
total probability weighted mixture component distributions 
type model obviously generalized number components including non finite mixtures 
sense probabilistic version union formal languages 
key feature context string generated possible derivations identity observable 
mixing models known language modeling literature interpolation bahl 
term mixture estimated independent training data em algorithm described 
expectation maximization hidden variables ambiguity grammars create additional problem parameter estimation observed outcomes multinomials grammar ambiguous longer simply counted estimators 
cases estimation face hidden variables solved general technique called expectation maximization dempster em 
intuitively objective maximized terms available proceeds guessing values unobserved hidden variables step maximizes likelihood parameters guessed values known observables step 
reestimation may affect guessing process iterates steps procedure converges set parameter values 
guessed values typically expectations unknown statistics needed compute likelihoods name step 
shown em iteration likelihood non decreasing converging local maximum parameter space 
leaves open local likelihood maximization practice question return 
term deleted interpolation refers holding method estimating mixture proportions 
simplified characterization 
full generality maximizes expectation log likelihood function step consists determining function log fixed current parameters hidden variables observed variables 
standard distributions particular exponential family expected log likelihood log likelihood assuming sufficient statistics take expected values 
example multinomial log log log chapter 
foundations em mixtures brief illustration em method consider case simple mixture model redner walker 
step compute expected values counts number times submodel generate string 
samples independent done summing fractional expected counts posterior probabilities samples coming mixture components iff submodel chosen generate submodel generating computed bayes law component case 
posterior probability obtain intuitive result expected attribution sample proportional submodel explains sample measured likelihood weighted prior probability choosing viterbi derivations approximate em submodels interested single derivation string model argmax called viterbi derivation 
finding viterbi derivation canonical way disambiguate string probabilistic grammar sense best explanation viterbi important viterbi derivations approximating full probability string obtained summing derivations max definition maximum approximate term achieved choosing viterbi derivation 
viterbi approximation frequently simplifying em algorithm 
maximizing respect true expectations hidden variables approximates expectations counting viterbi derivation occured probability 
example estimate mixture proportion viterbi replace number times submodel posteriori probable 
submodels ambiguous leaves open possibilities counts probable joint derivations component choice derivation component simply probable component choice full submodel likelihoods summing derivation submodels 
better approximation may practical compute 
chapter 
foundations simple hmm 
state names appear circles outputs emitting states 
hidden markov models hidden markov models hmms probabilistic counterpart nondeterministic finite au nfas hopcroft ullman 
nfas hmms finite number states including initial final ones 
states connected transitions emit output symbols finite discrete alphabet 
hmms defined emit output symbols transitions states usual nfas 
variants equivalent converted 
nfas hmms generate accept strings output alphabet nondeterministic walks initial final states 
addition hmms assign probabilities strings generate computed probabilities individual transitions emissions 
sequence states path leading initial final state constitutes hmm derivation sense introduced 
hmms get name fact underlying state sequence result markov process hidden observation corresponding non unique sequence emissions 
discussing order hmms state dependencies immediate predecessor higher order hmms states depending limited number predecessors 
higher order hmms may transformed equivalent order ones 
hmm modeling unbounded strings case final states 
certain reachability conditions model settles unique stationary distribution states outputs cover thomas 
reason model limited discrete outputs 
continuous output distributions commonly modeling speech example 
restrict finite string generation discrete outputs 
formal definition hmms deferred chapter 
illustrate basic notions way example 
model generates strings regular language probabilities labeled 
exceptions state outputs probability transitions occur state final state 
transition emission probability probabilistic model assigns probabilities strings possible paths model 
path sequence states initial final state hmm version derivation chapter 
foundations simple stochastic context free grammar generating palindromes general sense introduced earlier 
model derivations generating sum joint probabilities random walks example hmm string generated single path model conditional probabilities product individual transition emission probabilities 
fact conditional current state reflects markovian character generation process 
stochastic context free grammars stochastic context free grammars scfgs natural extension context free grammars cfgs probabilistic realm 
simple example leave formal presentation chapter 
context free annotated conditional chosen possible productions left hand side nonterminal expanded 
shows scfg generating palindromes alphabet derivation case usual tree structure parse tree arising nonterminal expansions probability product probabilities rules involves 
generated probability corresponding derivation string grammar 
notice notion context freeness extended include probabilistic conditional independence expansion nonterminal surrounding context 
turns crucial keep computational properties scfgs reasonable major drawback scfgs model say natural language 
return problem various points chapters 
chapter 
foundations levels learning model merging parameter estimation learning probabilistic language models data involves choice model best represents distribution generated samples 
identify levels choices involved choice model class gram finite state context free mixture 
choice model structure states nonterminals productions choice parameters gram probabilities transition probabilities far briefly addressed maximum likelihood em estimators 
fact established methods language models discussed thesis 
main contribution thesis choice model structure 
level currently left human designer learning system 
fundamental problem model design briefly discussed chapter 
boundaries inherently fuzzy due subsumption representations 
example finite state model presence absence transitions represented part model structure level continuous transition probability parameter level probability zero effectively represents absence transition 
representation correspond different learning approaches learn learner discrete decisions learning typically accomplished gradual adjustment continuous parameters em gradient ascent 
similarly faced choice different model classes level possible devise generalized class subsumes alternatives question 
example class probabilistic context free models contains finite state models special case 
hope structure learning method level pick right type model 
practice quite straightforward model structure doesn fall neatly target classes 
model merging approach structural learning problem probabilistic grammars pursued thesis conceptually general method advocated omohundro called model merging 
key ingredients method old algorithms disparate areas dealing various incarnations data modeling task 
omohundro argues domain independent principles lend model merging approach generality efficiency certain degree cognitive plausibility 
basic idea model domain constructed submodels 
small amount data available submodels consist data points similarity generalization 
chapter 
foundations data available submodels complex families may constructed 
elementary induction step successively merge pairs simple submodels form complex submodels 
new submodel combined data merged submodels may reliably chosen complex model space 
approach allows arbitrarily complex models constructed overfitting 
resulting models adapt representational power structure training data 
search submodels merge guided attempt sacrifice little sample likelihood possible result merging process 
search done efficiently greedy search strategy likelihood computations done locally submodel don require global recomputation model update 
curve fitting example idea illustrated task geometrical data modeling 
consider problem modeling curve plane combination straight line segments 
likelihood case corresponds mean square error curve point nearest segment point 
merging step case consists replacing segments single segment 
choose pair merged segment increases error 
shows approximations generated strategy 
excellent job identifying essentially linear portions curve puts boundaries component models corners 
shown repeated merges take place data available segment 
allow reliably fit submodels complex linear segments bezier curves 
possible reliably induce representation uses linear segments portions higher order curves 
models potentially parameters subject overfitting learned directly going merging steps 
model merging obvious converse iterative model splitting 
curve example top approach start single segment repeatedly split 
approach decisions early misses corners curve 
clearly domain dependent experience modeling approaches splitting tend fit structure domain merging 
model splitting approaches grammatical models proposed various authors discussed chapter 
knowing crucial factor model merging stopping criterion 
merging step potential generalization step model space stopping criterion effectively determines generalization observed data takes place 
criteria theoretical heuristic practical nature 
represents non exhaustive list candidates precisely mean squared error proportional log probability data assumption curve generates points normally distributed 
chapter 
foundations error error error error error approximation curve best merging segment models 
top row shows endpoints chosen algorithm various levels allowed error 
bottom row shows corresponding approximation curve 
chapter 
foundations global model characteristics target model known global characterization terms size merging stopped soon model reached matches criteria 
reason global criteria size resource limitations resulting model fit 
error tolerance model fit data described terms loss measure squared error negative log likelihood 
merging proceeds maximum error exceeded 
cross validation limiting error training data course guarantee similarly limited error new data 
standard techniques control generalization error applied cross validation separate data set merging stops model error validation data increases 
thresholding heuristics imposing fixed bounds error log alternatively difference measures result merging control overgeneralization 
empirically case merging step target model produces loss differentials large compared incurred merging steps leading desired model 
bayesian approach model merging developed thesis fit single categories 
viewed formalizing trade criteria 
bayesian model inference need inductive bias model merging framework learning sample data means generalizing 
simple maximum likelihood approach result generalization class models usually rich allow simple duplication sample data 
different traditional parameter estimation approaches fixed model structure constrains maximum likelihood solution leading generalization 
goal model merging course determine right model structure data having specified priori 
intuitive justification general models lower likelihood ml models simpler sense 
particular ml models model merging grow amount data supplied size unbounded sample space finite 
simplicity complexity important known form inductive bias mitchell 
known occam razor william occam dictum multiply things necessity explanations empirical fact assumptions preferred 
crude form simplicity metric embodied model merging process submodels merged total model size suitably defined typically decreases 
really needed principled approach trade inductive bias simpler models fit model observed data 
seen useful fit chapter 
foundations data perfectly hand maximizing simplicity lead models bear relation actual distribution generating data 
need formal notion model complexity fit data trade quantified 
section formalizations bayesian inference inference minimum description length 
approaches turn essentially equivalent provide complementary conceptualizations particular form probabilistic inductive bias 
principles discussed means unique probabilistic scenario 
example exactly matching model data resulting need inductive bias demonstrated mitchell case discrete classifier induction 
posterior probabilities express priori preferences regarding alternative models simplicity probabilistic terms bayesian notion prior probability 
assume exists distribution independent data assigns model priori data probability 
clearly probabilistic form bias presumably gives models higher prior probability 
data endeavor find model maximizes posterior probability bayes law expresses posterior data fixed map maximizes resulting model know map maximum posteriori probability model 
familiar likelihood 
form bayesian model inference generalization maximum likelihood ml estimation method prior maximization purposes 
bayesian model merging combined usual likelihood term easy modify likelihood model merging strategy accommodate prior probabilities 
basic changes need merging step maximize model posterior model likelihood 
stopping criterion increase model posterior possible choice possible merging steps 
bayesian model merging variant carries greedy search model space topology implicitly defined merging operators 
goal search find local maximum posterior probability 
induction strategy note search strategy really forms bias modified chapter 
foundations explicit probabilistic bias expressed prior implicit heuristic bias part choice topology search space search bias 
merging operator defined models space reachable bias significant due local sequential nature posterior probability maximization 
return question relax search bias constrained search methods section 
remaining section chapter common mathematical tools needed instantiating bayesian model merging approach domain probabilistic grammars 
chapters describe instantiation 
minimum description length bayesian inference posterior probabilities alternative formulation terms information theoretic concepts 
dualism formulations useful deeper understanding underlying principles construction prior distributions see section 
maximization implicit bayesian model inference equivalent minimizing information theory tells negative logarithm probability discrete event optimal code word length communicating instance minimize average code length log log log representative message 
accordingly terms equation interpreted message description lengths 
specifically log description length model prior distribution 
negative log joint probability interpreted total description length log corresponds description data model code lengths model data 
inference estimation minimum description length mdl rissanen wallace freeman equivalent useful alternative conceptualization posterior probability maximization 
picture somewhat complex distributions continuous spaces involved 
corresponding mdl formulation consider optimal granularity discrete encoding 
conveniently avoid complication formal description lengths devise priors discrete objects grammar structures 
chapter 
foundations structure vs parameter priors grammatical model described stages 
model structure topology specified set states nonterminals transitions productions 
depending type model 
elements represent discrete choices derivations grammar non zero probability 

conditional structure model continuous parameters specified 
typically multinomial parameters 
write parameter part model prior written describe decomposition model structure part framework leaves room choice discussed earlier may choose structure specification unconstrained allowing probability parameters take non zero values effectively pushing structure specification parameter choice 
examples discussed chapters 
priors multinomial parameters continuous parameters grammar types dealt thesis multi nomial distributions convenient discuss standard priors type distribution point 
multinomial represents discrete finite probabilistic choice event 
number choices multinomial probability parameters associated choice parameters free 
standard prior multinomials dirichlet distribution normalizing constant dimensional beta function parameters prior intuitive interpretation see 
prior weights determine bias embodied prior prior expectation total prior weight 
important reason dirichlet prior case multinomial parameters cheeseman cooper herskovits buntine mathematical 
chapter 
foundations conjugate prior functional form likelihood function multinomial 
likelihood sample multinomial total observed outcomes equation 
means prior likelihood combine bayes law give expression posterior density form furthermore convenient integral product closed form solution 
integral compute posterior model structure detailed sections 
get intuition effect dirichlet prior helpful look dimensional case 
free parameter say identify probability heads biased coin flip probability tails 
assume priori reason prefer outcome prior distribution symmetrical value 
symmetry entails choice equal case depicted various values prior effect adding virtual samples likelihood expression resulting map estimate resulting prior distribution map estimate biased extremes parameter space prior uniform map estimate identical ml estimate 
shows effect varying amounts data total number samples posterior distribution 
data posterior identical prior illustrated increases posterior peaks ml parameter setting 
description length priors mdl framework useful designing priors discrete structural aspects grammars 
prefix free coding scheme models assigns code length induce prior distribution models 

take advantage fact design natural priors domains 
idea extensively grammar types 
chapter 
foundations alpha alpha alpha alpha alpha alpha dimensional symmetrical dirichlet prior 
prior distributions various prior weights 
posterior distributions various amounts data proportion 
chapter 
foundations posteriors grammar structures model bayesian approach simplest form computes posterior probability fully specified compares alternative models basis 
goal find single model best represents data approach amounts joint maximization posterior parameters model structure alternatively may want infer single grammar view representative class grammars obtained varying parameters posterior distribution example new data arrives probability assessed weighted average parameter settings 
motivated bayes optimal solution transductive inference summing possible parameter settings possible model structures consist choosing single structure approximation full bayesian solution averaging part full model space 
optimize approximation choose model structure maximizes associated posterior weight equation reasoning suggests changing objective maximizing joint posterior probabilityof structure parameters maximizing posterior probability model structure 
desired quantity obtained integrating nuisance variable unfortunately usually way compute integral exactly closed form probabilities vary practice resort viterbi approximations quantities involved described specifically model type chapters 
sum possible derivations generate respective chapter hidden markov models overview hidden markov models hmms popular method modeling stochastic sequences underlying finite state structure 
uses area cryptanalysis model choice speech recognition rabiner juang 
applications include part speech tagging kupiec protein classification alignment haussler baldi 
hmms seen probabilistic generalizations non deterministic finite state automata interest point view formal language induction 
modeling applications feasible specify hmms hand 
hmm needs partly estimated available sample data 
applications mentioned crucially involve learning adjusting hmm data 
standard hmm estimation techniques assume knowledge model size structure topology proceed optimize continuous model parameters known statistical techniques 
section defines hmm formalism gives overview standard estimation methods 
contrast traditional hmm estimation baum welch technique baum model merging method adjusts model topology data 
merging operator hmms simple realized collapsing model states transitions emissions 
resulting algorithm described section 
result implementation applications merging algorithm number cial efficiency improvements approximations heuristics developed 
discussed section 
model merging hmms related ideas appeared literature cases considerable time 
section discusses links related compares various approaches 
bayesian hmm merging algorithm evaluated experimentally artificial realistic chapter 
hidden markov models applications section 
compare structure induction capabilities baum welch method find produces models better generalization compact 
particular applications area phonetic word modeling speech recognition show hmm merging effective efficient tool practice 
section summarizes chapter points point continuations line research 
hidden markov models definitions define formally hmms introduced section 
definition discrete output order hidden markov model specified set states output alphabet initial state final state set probability parameters 
transition probabilities emission probabilities output specify probability state follows specify probability symbol emitted state structure topology hmm mean states outputs subset transitions hmm topology specifies subset potential transitions emissions guaranteed subset emissions zero probability leaves remaining probabilities unspecified 
superscripts states emissions output sequence 

words denote discrete time indices generation initial state occurs state sequence final state complete state sequence 
occur emit symbols 
convenience assume possible try keep notation consistent bourlard morgan 
chapter 
hidden markov models definition hmm said generate string state sequence path non zero probability outputs non zero probability probability path relative product transition emission probabilities 
probability paths generate string hmm computed sum probabilities conditions probability parameters hmm necessary ensure defined forms proper distribution set finite strings 
transition probabilities state sum unity emission probabilities state sum unity states reachable path non zero probability path non zero probability 
dead states capture total probability mass contributing total distribution formed ness conditions satisfied obtaining hmms standard estimation algorithms model merging immediate concern 
hmm estimation baum welch estimation method hmms baum assumes certain topology adjusts parameters maximize model likelihood samples 
structure minimally specified probabilities assume non zero values method potentially find hmm structures setting subset parameters zero close zero pruning justified 
fundamental problem hmm estimation state variables directly observable 
observe sequences states addition outputs estimation probability parameters straightforward 
collect sufficient statistics number transitions state number outputs state chapter 
hidden markov models set model parameters maximum likelihood values missing state observations just special case hidden variables discussed section 
consequently em method instantiated case replace unknown transition output frequencies expected values current model estimate sample output sequences 
sample sequence compute posterior probability path generating passes state time done efficient dynamic programming algorithm know forward backward algorithm see rabiner juang 
straightforward compute posterior expectations model parameters maximized respect expectations unknown values re estimating parameters affects expectations fixed point reached 
recomputed parameters estimated em general baum welch method fool proof uses amounts hill climbing procedure guaranteed find local likelihood maximum result baum welch estimation may turn sub optimal 
particular results depend initial values chosen model parameters 
examples phenomenon seen section 
viterbi approximation frequently approximation hmm estimation proceed sample comes single path model paths assumed zero negligible probability 
viterbi path viterbi maximizes summand equation argmax denote th state neglecting paths sums resulting approximated estimates convenience 
statistics re estimating emissions chapter 
hidden markov models kronecker delta 
mention viterbi approximation turns useful efficient implementation hmm induction algorithm described sections 
hmm merging describe application model merging hmms 
merging process described illustrated simple likelihood merging strategy 
discuss priors hmms give modified algorithm resulting posterior probabilities 
implementation details filled section 
likelihood hmm merging model merging method requires major elements 
method construct initial model data 

way merge submodels 

error measure compare goodness various candidates merging limit general ization 

strategy pick merging operators search model space 
elements translated hmm domain follows 
initial hmm constructed disjunction observed samples 
sample represented dedicated hmm states entire model generates observed strings 

merging step combines states gives combined state emission transition probabilities weighted averages corresponding distributions states merged 

simplest error negative logarithm model likelihood 
show generalized bayesian posterior model probability criterion provides principled basis limiting generalization 

default search strategy greedy best search step states give best score evaluation function chosen strategies discussed section 
issue incorporation samples scheduled relative merging 
time assume samples incorporated merging starts practical alternative section 
chapter 
hidden markov models obtain initial model data construct hmm produces exactly input strings 
start state strings string represented unique path state sample symbol 
probability entering paths start state uniformly distributed 
path unique transition state probability 
emission probabilities state produce corresponding symbol 
initial model resulting procedure property assigns sample probability equal relative frequency maximum likelihood model data generally true initial models model merging methodology 
sense initial hmm specific model compatible data modulo weak equivalence hmms 
merging operation repeatedly applied pairs hmm states preserves ability generate samples accounted 
new unobserved strings may generated merged hmm 
turn means greater number possibly infinity strings opposed just sample strings 
algorithm generalizes sample data 
drop likelihood relative training samples measure generalization occurs 
trying minimize change likelihood algorithm performs repeated conservative generalizations certain threshold reached 
see trade model likelihood generalization recast bayesian terms replacing simple likelihood thresholding scheme maximization posterior model probability 
example consider regular language procedure algorithm constructs initial model depicted 
samples drawn strings starting point perform merging steps incurring drop model likelihood 
states merged followed 
merging states entails changes model old states removed new merged state added 
old states called parent states 
transitions old states redirected new state 
transition probabilities adjusted maximize likelihood 
new state assigned union emissions old states emission probabilities adjusted maximize likelihood 
example convention numbering merged state smaller indices parents 
symmetrical sequences merges identical result 
arbitrarily chosen 
chapter 
hidden markov models log log log log log sequence models obtained merging samples transitions special annotations probability 
output symbols appear respective states carry implicit probability 
model log likelihood base 
chapter 
hidden markov models returning example chose merge states 
step decreases log likelihood smallest decrease achieved potential merges 
states merged penalty 
resulting hmm minimal model generating target language prevents merging obtain hmm turns merging remaining states reduces likelihood drastically previous generalization step decimal orders magnitude 
preliminary answer set threshold small allow desirable generalizations 
satisfactory answer provided bayesian methods described 
note data may justify generalization model data driven character central aspects model merging 
domain specific justification model merging case hmms applies 
seen example structure generating hmm recovered appropriate sequence state merges initial model provided available data covers generating model emission transition exercised 
informally initial model obtained unrolling paths generating samples target model 
iterative merging process attempt undo unrolling tracing search model space back generating model 
course best heuristic guaranteed find appropriate sequence merges critically may result model weakly equivalent generating model 
priors hidden markov models previous discussion clear choice prior distribution important term drives generalization 
take approach priors subject experimentation empirical comparison ability lead useful generalization 
choice prior represents intermediate level global choice model formalism hmms case choice particular instance model class specific hmm structure parameters 
model merging approach ideally replaces usually poorly constrained choice low level parameters robust choice prior parameters 
long doesn assign zero probability correct model choice prior eventually overwhelmed sufficient amount data 
practice ability find correct model may limited search strategy case merging process 
hmms special kind parameterized graph structure 
unsurprisingly aspects priors discussed section bayesian approaches induction graph models domains bayesian networks cooper herskovits buntine decision trees buntine 
chapter 
hidden markov models structural vs parameter priors discussed section hmm may specified combination structure continuous parameters 
hmms structure topology set states transitions emissions 
transitions emissions represent discrete choices paths outputs non zero probability hmm 
approach compose prior structure parameters hmm product independent priors transition emission multinomial possibly global factor 
implicit independence assumption parameters different states clearly simplification shouldn introduce systematic bias particular model structure 
greatly simplify computation updating global posteriors various model variants detailed section 
global prior model product prior global aspects model structure including number states prior contribution structure associated state parameters transition emission probabilities associated state prior noted global factor assumed unbiased ignored maximization 
parameter priors hmms hmm transitions emission probabilities conceptually multinomials state apply dirichlet prior discussed section 
parameters exactly depends structure vs parameter trade 
narrow parameter priors natural application dirichlet prior prior distribution set multinomial parameters hmm structure relative equation parameters state transitions emissions contribute factor transition probabilities state ranging states follow emission probabilities state ranging outputs emitted prior weights transitions emissions respectively chosen introduce bias uniform assignment parameters 
chapter 
hidden markov models broad parameter priors preceding version parameters constrained choice model structure indicated earlier may parameters range potential transitions states model emissions elements output alphabet 
dirichlet priors equation states interesting aspect approach emission prior weights chosen non symmetrical prior means adjusted match empirical fraction symbol occurrences data 
empirical bayes approach similar setting prior class probability means buntine 
working assumption transitions emissions priori independent 
principle possible combination broad narrow parameter priors full exploration possibilities remains done 
structure priors hmms case broad parameter priors choice transitions emissions subsumed choice parameters 
structural component left open case number states example add explicit bias smaller number states setting constant 
see state priors produce tendency reducing number states result bayesian occam factors gull 
case narrow parameter priors need specify prior probability mass distributed possible model topologies number states 
practical reasons desirable specification described product individual state distributions 
leads approach 
transitions assume state average certain number outgoing transitions don reason prefer possible target states priori potential transition assessed prior probability existence prior probability prior expected number emissions state 
resulting structural contribution prior state similarly possible emission represents number transitions state number emissions 
mdl terms structural prior corresponds hmm coding scheme encoded log bits emission log bits 
potential transitions emissions missing take log log respectively 
experiments reported narrow parameters priors combined simple mdl structure priors 
details relevant sections 
chapter 
hidden markov models description length priors mdl framework discussed section derive simple priors hmm structures various coding schemes 
example natural way encode transitions emissions hmm simply enumerate 
encoded log bits possible transitions plus special marker allows encode missing transitions explicitly 
total description length transitions state similarly emissions coded log bits 
resulting prior log property small differences number states matter little compared differences total number transitions emissions 
seen section preferred criterion maximization posterior structure requires integrating parameters section give solution computation relies approximation sample likelihoods viterbi paths 
smaller hmms preferred 
intuitively want hmm induction algorithm prefer smaller models larger ones things equal 
interpreted special case occam razor scientific maxim simpler explanations preferred complex explanations required explain data 
notions model size explanation complexity goodness explanation quantified principle modified include trade criteria simplicity data fit 
precisely bayesian approach optimizing product compromise simplicity embodied prior fit data high model likelihood 
hmm priors discussed previous section lead preference smaller simpler models 
answers general phenomenon occam factors bayesian inference related specific way hmms partition data purposes explaining 
discuss turn 
occam factors consider scenario 
asked predictions regarding upcoming election involving number candidates 
model political process 
identify models respective proponents try evaluate basic idea encoding transitions emissions enumeration various sophisticated variants 
example base enumeration transitions canonical ordering states log log bits required 
bit integer coding scheme described cover thomas mdl inference quinlan rivest 
reasonable bayesian inference procedure sensitive minor difference prior little data 
goal simply suggest priors reasonable qualitative properties time computationally convenient 
chapter 
hidden markov models bayesian principles 
predicts candidates probability candidate outcome hand gives chance win realistic chance probability turns winner 
posterior credibility 
marginalize discrete parameter space predictions 
data winning 
assuming priori preference conclude posteriori 
result course just confirms intuition prophet predictions specific true credible predictions general 
ratio allowable range model parameters posterior priori known occam factor gull 
discrete case ranges just respective numbers possible parameter settings versus example 
continuous model parameters occam factor penalizes models parameters larger range parameter space higher dimensionality 
bayesian approach avoids picking model largest number free parameters leads overfitting data 
effective amount data state prior implementing approximation full computation structure posterior hmms dictated equation experimenting crude heuristic simply compared likelihoods alternative models evaluated map parameter settings 
prior dirichlet broad type discussed section 
result structural prior favors smaller configurations completely missing 
surprisingly produced preference smaller models 
intuitive reason combination phenomena particular hmms 
true general map point migrates maximum likelihood setting amount data increases 
case hmms effective amount data state increases states merged 
words number states hmm shrinks total amount data remains constant state get see data average 
merging right states cause states data available allowing likelihood come closer maximum value 
similar principles apply model merging applications model effectively partitions data purpose chapter 
hidden markov models algorithm choosing set priors prior parameters conceptually straightforward modify simple likelihood algorithm section accommodate bayesian approach 
best hmm merging algorithm takes generic form 
best merging batch version build initial maximum likelihood model dataset 
loop merges candidate model 
compute set candidate states model 
compute merged posterior probability 
merge maximizes 


return induced model 
formulation model stand model structure parameters suggested section just model structure discussing implementation results assume explicitly stated 
note computational details fleshed 
important implementation strategies described section 
number potential merges step biggest factor total amount computation performed algorithm 
reduced domain specific constraints section generally samples version merging algorithm feasible small amounts data 
grows linearly total length alternative approach process samples incrementally start merging small amount new data incorporated 
keeps number states number candidates small 
learning successful model growing eventually reach configuration accounts new samples point new merges required 
shows size profile incremental merging applications 
incremental character appropriate scenarios data inherently incomplete line learning algorithm needed continuously updates working model 
best merging line version empty model 

loop explaining 
chapter 
hidden markov models get new samples loop incorporate current model 
compute set merges candidate states model 
candidate compute merged model 
merge maximizes 


break loop 
posterior probability data exhausted break loop return induced model 
incremental merging principle produce results worse batch version evaluation step doesn data disposal 
didn find significant disadvantage practice 
optimize number samples incorporated step batch size speed 
requires balancing gains due smaller model size constant overhead execution step best value depend data merging possible iteration samples time choices 
careful start merging extremely small models resulting incorporating short samples 
priors discussed earlier contain logarithmic terms approach singularities log case produce poor results usually leading extreme merging 
easily prevented incorporating larger number samples say going merging step 
modifications simple best search strategy discussed section 
implementation issues section elaborate implementation various steps generic hmm merging algorithm section 
efficient sample incorporation simplest case step creates dedicated state instance symbol samples states chained transitions probability sample generated state sequence reached transition probability total number samples 
state connects probability 
states emit corresponding output symbol probability 
chapter 
hidden markov models way repeated samples lead multiple paths model generating sample string 
total probability string initial model relative frequency string follows initial model constitutes maximum likelihood model data note corresponding states equivalent paths merged loss model likelihood 
generally merging loop initial passes 
trivial optimization stage avoid initial multiplicity paths check new sample accounted existing path 
transition probability updated 
idea merging samples existing model pursued lines thomason granum 
extension viterbi algorithm new sample aligned existing model states recruiting new states necessary 
alignment couldn effect possible merges wouldn able generate loops reduce initial number states model saving computation subsequent steps 
computing candidate merges general case examine possible pairs states current model 
quadratic cost number states explains importance various strategies keep number states initial model small 
explored various application specific strategies narrow set worthwhile candidates 
example cost merge usually dominated cost merging output distributions states involved index states emission characteristics consider pairs states similar outputs 
constraint removed merging possibilities exhausted 
resulting strategy merging outputs states followed general merging speeds algorithm generally heuristic incremental merging prevent premature merges assessed differently light new data 
hard knowledge target model structure constrain search 
example word models speech recognition usually allowed generate arbitrary repetitions subsequences see section 
merges creating loops excepting self loops eliminated case 
model evaluation viterbi paths find posterior probability potential model need evaluate structural prior depending goal maximization find maximum posterior probability map estimates model parameters evaluate integral equation 
map estimation hmm parameters done baum welch iterative reestimation em method dirichlet prior account reestimation step 
optimization unimplemented 
chapter 
hidden markov models require em iteration candidate model time proportional number samples incorporated model 
section 
evaluation hand obvious exact solution discussed cases problem greatly simplified viterbi approximation assumption probability sample due primarily single generation path hmm section 
likelihood computation exact model likelihood relative dataset product individual sample probabilities equation 
length sample denotes path hmm state sequence 
contribution viterbi approximation implies replacing inner summations terms largest max terms expression conveniently grouped states leading form total counts transitions emissions occurring viterbi paths associated samples notation collection viterbi counts associated state expressed concisely map parameter estimation estimate approximate map parameter settings viterbi path counts maximum likelihood estimates modified include virtual samples provided dirichlet priors prior proportions associated dirichlet distributions transitions emissions respectively equation 
assumed uniform simplicity need 
chapter 
hidden markov models note summations entire set possible transitions emissions corresponds broad parameter prior 
summations restricted transitions emissions current model structure narrow parameter priors 
structure posterior evaluation implement model comparison posterior probabilities hmm structures section need approximate integral apply usual viterbi approximation assume addition viterbi paths change varies 
approximation grossly inaccurate broad parameter priors reasonable narrow priors determines viterbi path 
importantly expect approximation introduce systematic error bias evaluation metric particular model structure especially models compared small structural differences 
viterbi integral approximation written viterbi path associated integral rewritten parameters split parts state integrals second expression evaluated closed form instantiating generic formula dirichlet priors 
optimistic viterbi path updating far viterbi approximation allowed decompose likelihood estimation posterior evaluation problems form allows computation parts organized states 
take full advantage fact need way update viterbi counts merged model 
approach taken update viterbi counts associated state efficiently merging 
particular want avoid having incorporated samples optimistically assuming merging preserves viterbi paths 
initial model creation viterbi counts initialized corresponding sample state created 
initial states shared identical samples initial counts set reflect multiplicity samples 
subsequently merging states corresponding counts simply added recorded counts new state 
example chapter 
hidden markov models current model merged state assigned count correct samples viterbi paths transitions retain paths merged model simply replacing merged states samples change paths include path preservation assumption strictly true holds time merges chosen collapse states similar transition emission probabilities 
assumption easily tested counts corrected training data time time 
incremental model building scenario new samples available large number incorporated interleaved merging want store data seen past 
case exponentially decaying average viterbi counts kept 
effect incorrect viterbi counts eventually fade away replaced date counts obtained form parsing data current model 
incremental model evaluation techniques described previous sections evaluation model variant due merging possible amortized time naive implementation 
evaluating specific candidates merging compute contributions posterior probability state current model 
prior usually depend total number states model set current number minus computations accounting prospective merge 
total computation contributions number states transitions precisely transitions emissions merged states transitions merged states 
potential merge determine parts model affects total number hmm elements affected priors considered likelihood computations old quantities updated subtracting terms corresponding old model elements adding terms merged hmm 
computation addition multiplication logarithms simplicity accuracy 
state shared amortized time candidate order note worst case cost realized hmms sparse usual 
number transitions emissions state bounded constant computation require constant time 
evaluation involves computing multidimensional beta functions products gamma functions transition emission 
addition subtraction scheme incremental computation 
practice may worth implementation effort absolute computational expense small 
chapter 
hidden markov models global prior weighting explained previously merging strategy trades generalization fit data 
gener alization driven maximizing prior contribution data fit virtue maximizing likelihood 
practice convenient single parameter controls balance factors controls generalization 
logarithmic version bayes law obtain log log quantity maximized 
obtain global control parameter generalization modify include prior weight log log algorithm merging earlier 
global prior weight intuitive interpretation reciprocal data multiplier absolute constant scale expression irrelevant maximization multiply get log log log log corresponds posterior data repeated times 
words lowering prior weight pretend data observed decreasing tendency merging generalize data 
refer actual number samples multiplied effective sample size 
quantity quinlan rivest model representativeness data 
equivalent multiplier global prior weighting extremely useful practice 
value trial error amount data starting small value increasing successively cross validating inspecting results 
stage result merging initial model stage avoiding duplication 
global generalization control particularly helpful counter acting potential shortcomings incremental merging 
incremental merging decisions subset data especially important prevent overgeneralization early stages 
adjust depending number samples processed maintain minimum effective sample size incremental merging reducing tendency samples 
principle implies gradual increase samples incorporated 
application reported section 
search issues section described basic best search strategies batch versus incremental sample processing 
orthogonal choice various extensions search method help overcome chapter 
hidden markov models local posterior probability maxima space hmm structures constructed successive merging operations 
far common problem practice stopping criterion triggered early single merging step decreases posterior model probability additional related steps eventually increase 
happens vast majority cases step right direction 
straightforward solution problem add lookahead best strategy 
stopping criterion modified trigger fixed number steps produced improvement merging proceeds best path 
due lookahead depth entail exponential increase computation full tree search 
additional cost performed looking ahead vain merging sequence 
cost amortized samples incremental merging batch size 
best merging lookahead method choice applications lookaheads 
experimented beam search strategies 
set working models kept time limited number say top scoring ones difference score current best model 
inner loop search algorithm current models modified possible merges pool generated best ones beam criterion retained 
including models pool get effect lookahead 
duplication results fact different sequences merges lead final hmm structure 
remove gratuitous duplicates beam attach list disallowed merges model propagated model successors generated merging 
multiple successors model list extended successors produce identical results simply permuting merge sequence 
resulting beam search version algorithm produce superior results data requires aligning long substrings states quality alignment evaluated coordinated merging steps 
hand beam search considerably expensive best search may worth marginal improvement 
results section obtained best search lookahead 
improved search strategies heuristics merging remain important problem research 
related ideas approach bayesian hmm induction new similar forms vast literatures grammar induction statistical inference 
chapter 
hidden markov models non probabilistic finite state models basic level concept state merging implicit notion state equivalence classes pervasively automata theory hopcroft ullman 
applied induction non probabilistic automata angluin smith 
field non probabilistic automata induction tomita simple hill climbing procedure combined goodness measure positive negative samples search space possible models 
strategy obviously similar spirit best search method uses probabilistic goodness criterion positive samples 
incremental version merging algorithm samples incorporated prelim model structure time similar spirit detail automata learning algorithm proposed feldman induces finite state models positive lexicographically ordered samples 
bayesian approaches bayesian approach grammatical inference goes back horning procedure proposed finding grammar highest posterior probability data enumeration candidate models order decreasing prior probability 
procedure proven converge maximum posterior probability grammar finite number steps impractical applied induction context free grammars 
horning approach applied enumerable grammatical domain reason believe simple enumerative approach feasible restricted applications 
hmm merging approach seen attempt bayesian strategy workable operating data driven manner sacrificing optimality result 
state splitting algorithms enumerative search finding best model structure bell 
find optimal text compression models number number states clearly state feasible practical approach 
suggest state merging splitting ways constructing model structure dynamically data dismissed inefficient purposes 
similar significant conceptual differences compression oriented approaches 
evaluation functions invariably entropy likelihood formalized notion trade model fernando pereira pointing 
amazing overlap apparently mutual knowledge text compression field probabilistic computational linguistics 
example problem smoothing estimates solutions mixtures bahl back models katz perfect analogs various strategies building code spaces compression models 
bell 
attribute state merging idea evans 
chapter 
hidden markov models complexity data fit 
second finite state models investigate act encoder decoders text deterministic current state input symbol determine unique state follows string unique derivation 
constrains model space allows states identified string suffixes basis algorithms 
models states supposed encode continuous text 
minor difference view sentence special symbol final state simply dedicated emitting special symbol 
bell 
suggest state efficient induction technique adaptively finding finite state model structure 
approach states successively duplicated differentiated preceding context move promises help prediction symbol 
ron 
give reformulation formal analysis idea terms information theoretic evaluation function 
interestingly bell 
show state splitting strategy confines power finite state model finite context model 
models type finite bound preceding symbols uniquely determine symbol 
words state models derived kind essentially gram models variable bounded context 
restriction applies equally algorithm ron 

contrast consider hmm depicted benchmark model 
describes language context needed correct prediction final symbol unbounded 
model difficulty simple best merging 
major advantage splitting approach guaranteed find appropriate model data target language fact finite context 
probabilistic approaches probabilistic approach hmm structure induction similar described son granum 
basic idea incrementally build model structure incorporating new samples extended form viterbi alignment 
new samples aligned existing model maximize likelihood allowing states inserted deleted alignment purposes 
procedure limited hmms left right ordering states particular loops allowed 
sense approach seen approximation bayesian hmm merging special class models 
approximation case twofold likelihood posterior maximized likelihood single sample entire data set considered 
haussler 
apply hmms trained baum welch method problem protein primary structure alignment 
model structures fixed linear form subject limited modification heuristic inserts states stretches model deletes states shrinks model estimated probabilities 
somewhat surprisingly brown 
construction class gram chapter 
hidden markov models models language modeling viewed special case hmm merging 
class gram grammar easily represented hmm state class 
transition probabilities represent conditional probabilities classes emission probabilities correspond word distributions class higher order hmms required 
incremental word clustering algorithm brown instance hmm merging albeit entirely likelihoods 
evaluation evaluated hmm merging algorithm experimentally series applications 
evaluation essential number reasons simple priors algorithm give general direction little specific guidance may misleading practical cases finite data 
appropriateness priors optimality bayesian infer ence procedure ideal form various approximations simplifications incorporated implementation jeopardize result 
real problems associated data shown hmm merging practical method terms results regarding computational requirements 
proceed stages 
simple formal languages artificially generated training samples provide proof concept approach 
second turn real albeit abstracted data derived timit speech database 
give brief account hmm merging embedded operational speech understanding system provide multiple pronunciation models word recognition 
case studies finite state language induction methodology group tests performed merging algorithm objective twofold wanted assess empirically basic soundness merging heuristic best search strategy compare structure finding abilities traditional baum welch method 
chose number relatively simple regular languages produced stochastic versions generated artificial corpora submitted samples induction methods 
probability furthermore aware realized scheme brown 
efficient recomputation likelihoods merging essentially subtracting old terms adding new ones 
hmm drawings section produced ad hoc algorithm optimizes layout best search heuristic quality metric bayesian principles whatsoever involved 
apologize time hand edit problematic results believe quality sufficient expository purposes 
chapter 
hidden markov models distribution target language generated assigning uniform probabilities choice points hmm topology 
induced models compared variety techniques 
simple obtained computing log likelihood test set 
proportional negative empirical estimate cross entropy reaches minimum distributions identical 
evaluate hmm topology induced baum welch training resulting models pruned transitions emissions probability close zero deleted 
resulting structure compared target model generated merging 
pruning criterion transition emission expected count training set 
specifically check resulting model generates exactly discrete language target model 
done empirically arbitrarily high accuracy simple monte carlo experiment 
target model generate reasonably large number samples parsed hmm evaluation 
samples parsed indicate induction process produced model sufficiently general 
interpreted overfitting training data 
conversely generate samples hmm question check parsed target model 
induction 
cases inspected resulting hmm structures gain intuition possible ways things go wrong 
examples 
note outcome baum welch algorithm may experience vary greatly initial parameter settings 
set initial transition emission probabilities randomly uniform distribution repeated baum welch experiment times 
merging hand deterministic group runs single merging experiment performed comparison purposes 
source variation baum welch method fixed total number model parame ters 
case hmms fully parameterized number states set possible emissions number parameters simply characterized number states hmm 
experiments reported tried variants number states equal target model minimal number states language hand second additional states provided 
nature training samples varied 
standard random sample target distribution experimented minimal selection representative samples chosen characteristic hmm topology question 
selection heuristic characterized follows list strings generated target model pick minimal subset order decreasing probability transition emission target model exercised looping exemplified sample traverses loop twice minimal convention exclude initial final states counts 
chapter 
hidden markov models training samples rule produces listed fact intuitively representative respective target models 
priors merging strategy uniform strategy associated parameter settings merging experiments 
straightforward description length prior hmm topologies section narrow dirichlet prior parameters section drive generalization 
total dirichlet prior weight multinomial held constant biases probabilities non uniform spite target models 
objective function maximization posterior hmm structure discussed section 
merging proceeded incremental strategy described section batch size techniques discussed earlier 
specifically incremental merging constrained states identical emissions followed unconstrained batch merging phase 
global prior weight adjusted keep effective sample size constant 
accordance rationale section gradually increases prior weight incremental merging phase preventing early overgeneralization 
search strategy best steps lookahead 
case study test task learn regular language generated hmm 
turns key difficulty case find dependency symbols separated arbitrarily long sequences intervening non distinguishing symbols 
minimal training sample model consisted strings alternatively sample random strings 
results merging baum welch runs summarized series plots 
plots left column refer minimal training sample runs right column standard regular describe finite state languages stands repetitions string denotes repetitions stands repetitions disjunction set union operator 
test model inspired finite state models similar characteristics subject investigations human language learning capabilities cleeremans chapter 
hidden markov models start case study hmm generating test language chapter 
hidden markov models test set likelihood bw 
bw 
generalization bw 
bw 
data fit bw 
bw 
test set likelihood bw 
bw 
generalization bw 
bw 
data fit bw 
bw 
minimal training sample random training samples case study results induction runs 
chapter 
hidden markov models shows corresponding data random string sample runs 
plot shows quantitative measure induced models performance axis represents various experiments 
case left data point left vertical bar represents single merging run followed data points repeated baum welch runs minimal number states bw data points baum welch runs states bw 
top row plots log likelihood sample test set higher value lower relative entropy kullback leibler distance distribution strings generated target model embodied induced model 
obtain comparable numbers probabilities merged model estimated baum welch set ml estimates ignoring parameter prior merging 
second row plots shows results parsing samples discretized induced model topologies number samples successfully parsed 
score means induced model specific 
third row plots shows converse parsing experiment random samples generated induced model parsed target model 
note samples runs 
score indicates induced model overly general 
note terms general specific loose sense includes cases models comparable set theoretic sense 
particular model general specific target model 
evaluating structural properties model consider success overfit 
models invariably log likelihood close optimal 
log likelihood deceptive may appear close optimal model structure represents poor generalization 
critical longer samples indicative generalization small probability contribute little average log likelihood 
primary reason devising parsing experiments additional evaluation criterion 
results merging procedure able find target model structure types training sets 
left data points plots taken benchmarks evaluating performance baum welch method data 
quality baum welch induced model structures vary wildly choice initial conditions 
minimal sample runs resulted perfect model structures working states states necessary 
random training sample success rate improved respectively 
observed baum welch derived models correspond missing correlation initial final symbols 
models typically generate subset leads samples generated rejected target model cf 
bottom plots 
chapter 
hidden markov models start start case study bw derived hmm structures fail generalization 
chapter 
hidden markov models start case study redundant bw derived hmm structure baum welch studies instructive inspect hmm topologies baum welch estimator 
shows models states trained minimal samples exhibiting overgeneralization demonstrating overfitting overgeneralization 
hmm generates hmm generates modeling repetition generalizing loop single state states model distinction redundantly allocated states generate 
precious states wasted estimation minimal number states case successful discretized structure invariably target model expected probabilities depend training sample 
successful induction states hand leads models definition contain redundant states 
redundancy necessarily simple duplication states target model structure 
convoluted structures induced random samples 
merging studies investigated merging algorithm behaves non optimal values global prior weight explained earlier value implicit number effective samples parameter maintained constant experiments robust roughly order magnitude 
took resulting value adjusted upward downward order chapter 
hidden markov models start start start case study generalization depending global prior weight 
chapter 
hidden markov models start case study ii hmm generating test language magnitude produce overfitted models respectively 
series models minimal sample shown 
structural generalization takes place sample set simply represented concise manner 
wide range target hmm derived different probability parameters 
increase produces model structure longer distinguishes argue overgeneralization natural data 
case study ii second test language generated hmm depicted 
minimal training sample contains strings training sample consisted randomly drawn strings 
presents results graphical form measures arrangement previous case study 
note ranges axes differ 
similar previous experiment merging procedure successful finding target model baum welch estimator produced inconsistent results highly dependent initial parameter settings 
furthermore baum welch success rates reverse switching minimal random sample respectively 
disturbing reveals sensitivity number states model precise statistics sample data 
typically form 
chapter 
hidden markov models test set likelihood bw 
bw 
generalization bw 
bw 
data fit bw 
bw 
test set likelihood bw 
bw 
generalization bw 
bw 
data fit bw 
bw 
minimal training sample random training samples case study ii results induction runs 
chapter 
hidden markov models start start case study ii bw derived hmm structures fail generalization 
baum welch studies previous case study looked various model structures baum welch estimation 
examples section training random samples 
shows structure overly general generates hmm partly time exhibits peculiar case overfitting excludes strings form 
cases happened training set 
accurate model structures states baum welch method tended convoluted 
shows case point 
merging studies repeated experiment examining levels generalization merging algorithm value global prior weight increased orders magnitude 
shows progression models 
pattern similar case study 
resulting models range simple merged representation samples plausible overgeneralization training data 
target model obtained values extremes 
chapter 
hidden markov models case study ii redundant bw derived hmm structure start chapter 
hidden markov models start start start case study ii generalization depending global prior weight 
chapter 
hidden markov models discussion tempting try find pattern performance baum welch estimator terms parameters model size sample size type go intended scope study 
regarding model size expect smaller minimal models produce better coverage target language tendency states available produce close fit data 
observable plots bottom rows figures runs left half typically produce higher number rejected samples 
conversely expects greater tendency overfitting training runs minimal number states 
plots middle rows figures confirm expectation right show greater number rejected strings target language indicating insufficient generalization 
conceivable language exists model size lead compromise generalization data fit produce reliable structure estimation 
problem way predict optimal size 
successful model merging approach relies suitable parameter choices mainly global prior weight number effective samples 
prime advantage merging regard parameters robust sample size distribution mechanics algorithm straightforward experiment 
furthermore appears overgeneralization excessive merging tends produce plausible models obvious caveat tentative limited scope investigation matter human judgment 
phonetic word models labeled speech timit database second evaluation stage looking sizeable collection real world data suitable hmm modeling 
timit texas instruments mit database collection hand labeled speech samples compiled purpose training speaker independent phonetic recognition systems garofolo 
contains acoustic data segmented words aligned discrete labels alphabet phones 
purposes ignored continuous acoustic data viewed database simply collection string samples discrete alphabet 
goal construct probabilistic model word database representing phonetic structure accurately possible maximizing probabilities observed pronunciations 
fraction total available data test set set aside evaluating induced models rest induce estimate probabilistic model word 
comparing performance models generated various methods relevant properties model size processing time arrive reasonably objective comparison various methods 
course ultimate section simple heuristic scales model sizes linearly length samples 
heuristic works particular application crucially relies models loop free wouldn apply generally 
chapter 
hidden markov models test actual system handles acoustic data considerably involved task 
section describe system hmm merging process brought bear 
full timit dataset consists phonetic samples words 
keep task somewhat manageable eliminate large number words samples allow meaningful structural model induction subset data consisting words intermediate frequency 
arbitrarily included words occurring times dataset 
left working dataset words comprising total samples 
word total available various training methods remaining total left evaluation 
qualitative evaluation preliminary evaluating possibility incorporating hmm merging ongoing speech understanding project gary researcher icsi extensive experience acoustic phonetic modeling experimented algorithm timit database 
inspected large number resulting models phonetic plausibility generally appeared sound generated structures close conventional linguistic wisdom 
get idea kinds models hmm merging produces data useful examine example 
shows hmm constructed samples word 
comparison shows initial hmm constructed samples merging identical paths collapsed 
plots number states obtained line incremental merging function number incorporated samples 
numbers states merging stage plotted adjacent datapoint giving rise spikes 
merging starts incorporated samples states 
initially merging occurs additional sample samples parsed hmm require merging 
striking feature induced hmm second syllable contain number alternative pronunciations anchored central consonant common variants 
structural property mirrored branching sections hmm 
number pronunciation second syllable share optional tcl sequence 
notice state initial final exactly output symbol 
constraint imposed due particular task mind resulting hmms 
speech recognition system word models intended implicitly assumes hmm state represents unique phone 
restriction easily enforced algorithm filtering merge candidates pairs states identical emissions 
single output constraint limit representational power hmms multi output state split single output states 
affect structural prior union training test portions original timit distribution 
chapter 
hidden markov models start ao tcl ao tcl aa ix aa ax aa ax ao tcl aw tcl ao en ao ax ao ah ao tcl aa ix ao epi en ao ax nx ao ax ao tcl ao ih aa tcl ao ix aa en ao en ao ix ao tcl ix initial hmm constructed samples word 
probabilities omitted graph 
due repetitions data hmm distinct paths 
ax en ix ih ix ih ix chapter 
hidden markov models start aw ao aa tcl ax ih ah epi nx ix merged hmm constructed samples word 
merging constrained keep emission state unique 
en chapter 
hidden markov models 
states 
samples number hmm states function number samples incorporated incremental merging 
spike represents states added model sample partly merged existing hmm structure 
hmm 
single output hmm emission carries prior probability various structural priors multinomials discussed section 
incidentally constraint speed algorithm significantly candidates efficiently eliminated evaluated rejected 
advantage far outweighs larger size derived models 
second constraint enforced particular domain possibly 
resulting hmms meant phonetic word models sense allow loops models 
rare circumstances merging algorithm tempted introduce loops peculiar repetitive pattern sample banana 
prior knowledge regard simply rule candidate merges introduce loops 
hmm merging derive models allophonic variation data explicitly representing mapping individual phonemes realizations 
contrast approaches induces rules pronunciations individual phonemes contexts decision tree induction concatenated networks representing word pronunciations chen riley 
detailed comparison approaches desirable far hasn carried 
simply approaches combined generating customary hmm modeling speech recognition introduce self loops states model varying durations 
contradict said self loops introduced lower representational level devices state replication 
systematically added merged hmm hmm alignment continuous speech data 
chapter 
hidden markov models sequences induced phoneme models adding directly observed pronunciations purpose smoothing 
quantitative evaluation hmm construction methods tested compared timit data 
maximum likelihood ml model hmm union unique samples probabilities corresponding observed relative frequencies 
essentially result building initial model hmm merging procedure merging takes place 
baum welch estimation hmm fixed size structure submitted baum welch em estimation probability parameters 
course finding right size structure exactly learning problem hand 
wanted evaluate structure finding abilities baum welch procedure set number states fixed multiple maximum sample length word randomly initialized possible emissions non zero probabilities 
em procedure converges transitions emissions probability close zero pruned leaving hmm structure evaluated 
model sizes multiples sample length tried 
standard hmm merging loop suppression see 
simple description length prior section log log emissions narrow dirichlet prior transition emission probabilities log transitions log cases 
global weighting factors structure prior evaluated 
hmm merging single explained 
prior multiple output hmms emission parameter prior exist case structural prior contribution emission log log simple minded way comparing various methods apply training portion data compare generalization test data 
measure generalization customary negative log probability empirical cross entropy assign test samples 
method achieves lowest cross entropy win comparison 
problem immediately poses significant chance test samples zero probability induced hmms 
tempted evaluate number test samples covered model comparison meaningless model assigns low probability possible strings trivially win comparison 
general approach usually taken situation recipe prevents vanishing probabilities new unseen samples 
great approaches common parameter smoothing back schemes suitable comparison task hand 
chapter 
hidden markov models method chosen defined correspond probabilistic model represents proper distribution strings standard back models second model consulted returns unbiased respect methods compared extent possible 
probability zero yield consistent probabilities combined discounting proba bilities ensure total sums unity katz 
discounting scheme various smoothing approaches adding fixed number virtual dirichlet samples parameter estimates tend specific model inherently problematic comparing different model building methods 
overcome problems chose mixture models approach described sec tion 
target models evaluated combined simple back model guarantees non zero probabilities bigram grammar smoothed parameters 
back grammar identical structure target models 
discrete back schemes target back consulted probability assign sample weighted averaged mixture proportion 
comparing model induction methods induce structure 
built mixture model component model parameters mixture proportions estimated em procedure generic mixture distributions 
get meaningful estimates mixture proportions hmm structure induced subset training data full training data estimate parameters including mixture weights 
holding training data mixture model approach similar deleted interpolation method jelinek mercer 
main difference component parameters estimated jointly mixture proportions 
experiments half training data structure induction phase adding half em estimation phase 
ensure back model receives non zero prior probability estimate mixture proportions simple symmetrical dirichlet prior results discussion 
hmm merging evaluated variants single output constraint 
version settings structure prior weight tried 
similarly baum welch training preset number states fully parameterized hmm set times longest sample length 
comparison purposes included performance maximum likelihood hmm grammar kind mixture models evaluate model types 
table summarizes results experiments 
difference traced different goals deleted interpolation main goal gauge reliability parameter estimates want assess different structures 
chapter 
hidden markov models log ml perplexity significance states transitions emissions training time log ml perplexity significance states transitions emissions training time bg bw bw bw log perplexity significance states transitions emissions training time table results timit trials model building methods 
training methods identified keys bg bigram grammar ml maximum likelihood hmm bw baum welch trained hmm merged hmm single output merged hmm 
log test samples 
perplexity average number phones follow context word computed exponential phone cross entropy 
significance refers level test pairing log probabilities test samples best score merging 
number states emissions listed applicable 
training times listed represent total time minutes seconds took induce hmm structure subsequently em train mixture models sparcstation 
chapter 
hidden markov models table includes useful summary statistics model sizes obtained time took compute models 
figures obviously rough measure computational demands comparison suffers fact implementation methods may certainly optimized idiosyncratic ways 
figures give approximate idea expect realistic application induction methods involved 
important general experiments merged models obtained baum welch training dumb approaches bigram grammar ml hmm essentially list observed samples 
conclude pays try generalize data bayesian approach baum welch hmm suitable size 
difference scores simplest approach bigram best scoring merging quite small phone perplexities ranging 
surprising specialized nature small size sample corpus 
unfortunately leaves little room significant differences comparing alternate methods 
advantage best model merging result unconstrained outputs significant compared best baum welch size factor result 
small differences log probabilities probably irrelevant resulting hmms embedded speech recognition system 
biggest advantage merging approach application compactness resulting models 
merged models considerably smaller comparable baum welch hmms 
important standard algorithms operating hmms typically scale linearly number transitions quadratically number states 
advantage production baum welch grow quadratically number states structure induction phase requires fully parameterized hmms 
scaling clearly visible run times observed 
haven done word word comparison hmm structures derived merging baum welch summary model sizes confirm earlier finding section baum welch training needs certain redundancy model real estate effective finding fitting models 
smaller size factors give poor fits sufficiently large hmms tend overfit training data 
choice prior weights hmm merging section controls model size indirect way larger values lead generalization smaller hmms 
best results value set previous experience representative data 
effectively done cross validation procedure generalization successively increased starting small due nature merging algorithm done incrementally outcome merging small submitted merging larger value increases reduce generalization cross validation data 
chapter 
hidden markov models multiple pronunciation word models speech recognition part dissertation research wooters hmm merging extensively context berkeley restaurant project 
medium vocabulary speaker independent continuous speech understanding system functions consultant finding restaurants city berkeley california jurafsky 
application merging algorithm run strings phone labels obtained viterbi aligning word models sample speech timit labels phone alphabet 
result new word models obtained viterbi alignment leading improved labelings procedure iterated improvement recognition performance labelings observed 
word models bootstrapped list pronunciations variety databases 
goal iteration repeated alignments tailor word models task specific data hand generalize possible 
added complication hmms discrete outputs applicable acoustic speech data 
approach developed bourlard morgan hmms combined acoustic feature densities represented multi layer perceptron mlp 
neural network maps frame acoustic features phone alphabet 
network outputs likelihoods hmm states relative acoustic emissions computed required viterbi alignment standard hmm algorithms 
emission probabilities subject change due changes word models reestimated iteration 
mlp bootstrapped weights obtained training pre labeled timit acoustic data 
depicts combined iteration mlp training word model merging viterbi alignment 
viewed instance generalized em algorithm emission probabilities represented mlp hmm structure transition probabilities optimized separately 
separation result different technologies 
system hmm merging possible practical multiple pronunciation word models confined single pronunciation models 
note setting restricted hmm produce acoustic emission non zero probability due continuous nature domain emission distribution represented mlp inherently non vanishing 
assess effectiveness recognition performance multiple pronunciation system compared identical system phone sequence word generated commercial text speech system 
comparison multiple pronunciation modeling derived hmm merging reduce word level error rate 
time error rate level semantic interpretations dropped 
experiments needed identify precisely aspects multiple pronunciation approach account improvement word model building techniques lead significantly different results context 
preliminary results show hmm merging chapter 
hidden markov models training data train mlp forward pass probabilities train mlp labels forward pass probabilities probabilities forward pass timit mlp forced viterbi alignment forced viterbi alignment forced viterbi alignment sentence strings labels pronunciations estimate word models hmm merging word models sentence strings pronunciations estimate word models hmm merging word models sentence strings labels pronunciations task independent word models hybrid mlp hmm training merging procedure speech understanding system wooters 
chapter 
hidden markov models practical effective embedded realistic speech system 
details construction word models discussion ancillary issues graphical hmm representation pronunciation common words corpus wooters 
research evaluations indicate hmm merging approach promising new way induce probabilistic finite state models data 
compares favorably standard baum welch method especially prior constraints hmm topology 
implementation algorithm applications speech domain shown feasible practice 
experimentation range plausible priors new application specific ones time consuming barely scratched surface area 
experience far priors discussed section particular choice prior type parameters greatly affect course best search possibly decision 
words merging heuristic effect likelihood determining factors choice merges 
fact change informative priors 
likewise haven pursued merging hmms non discrete outputs 
example hmms mixtures gaussians emission densities extensively gauvain lee speech modeling 
merging algorithm applicable models provided prior densities straightforward cheeseman 
efficient implementation merging operator may bigger problem wants avoid having explicitly compute merged density merge consideration 
major shortcomings current merging strategy inability back merging step turns overgeneralization light new data 
solution problem addition complementary state splitting operator lines bell 
ron 

evaluation functions approaches entropy equivalent maximum likelihood means bayesian generalization adding suitable prior hard 
mentioned section standard splitting explore full power finite state models combining merging splitting circumvent limitation 
major difficulty evaluating splits opposed merges requires elaborate statistics simple viterbi counts splitting decisions occurrences states path 
chapter stochastic context free grammars overview chapter look model merging applied probabilistic version context free grammars 
stochastic context free grammar scfg formalism generalization hmm just non probabilistic cfgs thought extension finite state grammars 
non scfgs mainstream approach language modeling 
today models finite state simple gram approaches dominate 
reason standard algorithms probabilistic finite state models hmms generalized versions scfgs computationally demanding intractable practice see section 
important problem scfgs may worse modeling aspect language simple finite state models surprisingly job capturing short distance lexical opposed phrase structural contingencies words 
direct consequence conditional independence assumptions embodied scfgs prompted investigation mildly context sensitive grammars probabilistic versions resnik schabes 
come greater computational price 
shown probabilistic cfgs useful applied carefully right domain 
lari young discuss various applications estimated scfgs phonetic modeling 
jurafsky 
show scfg built hand crafted rules probabilities estimated corpus improve speech recognition performance standard gram language models directly coupling scfg speech decoder scfg effectively smoothing device improve estimates gram probabilities sparse data 
algorithms form basis approaches described second part thesis chapter chapter respectively 
bare cfgs aren widely computational linguistics form basis backbone today feature unification grammar formalisms lfg kaplan bresnan gpsg gazdar construction grammar fillmore 
chapter 
stochastic context free grammars scfgs applied successfully modeling biological structures notably secondary structures various types rna sakakibara underwood 
biological sequences relatively small alphabets precisely formalized combinatorial properties comparatively ideal applications formal grammars messy natural language domain 
short leave open question context free models appropriate useful general focus learning problem bayesian model merging framework developed earlier 
section reviews basic scfg formalism gives overview standard algorithms relate learning problem 
section describes model merging algorithm scfgs 
surprising concepts techniques introduced chapter reappear similar form 
major difference additional merging operator called chunking syntagmatic merging 
section relates bayesian scfg merging method various probabilistic non probabilistic approaches literature 
section evaluate algorithm experimentally various formal quasi natural language target grammars 
section summarizes discusses possible extensions approaches problems current method 
stochastic context free grammars definitions definition stochastic context free grammar scfg consists set nonterminal symbols set terminal symbols alphabet nonterminal start set productions rules production probabilities productions form right hand side rhs 
called left hand side lhs production progress addresses structure proteins kevin thompson personal communication 
constitutes subjective judgment computational linguist point view 
molecular biologists probably disagree claim language comparatively simpler domain 
chapter 
stochastic context free grammars notation latin capital letters denote nonterminal symbols 
latin lowercase letters terminal symbols 
strings mixed nonterminal terminal symbols written lowercase greek letters 
empty string denoted 
scfg exactly standard context free grammar productions assigned continuous probability parameters 
interpretation scfg rhs chosen indicated probability nonterminal expanded 
condition top derivation probability conditioned due interpretation get formedness sum productions lhs unfortunately simple local consistency scfg sufficient necessarily scfg represent proper distribution strings 
complication come play learning scfgs corpora important dealing predetermined scfgs parsing purposes 
defer discussion technicality chapters sections 
hmms probabilistic grammars conditional probability string scfg sum probabilities derivations string 
formalized follows 
string definition sentential form nonterminals terminals sentential form produced replacing nonterminal production left derivation sequence sentential forms derived single rule application predecessor step replaced nonterminal left nonterminal sentential form notation 
write derivation sentential forms involved 
probability derivation inductively defined 
step production probability string summation derivations yield chapter 
stochastic context free grammars definition derivation string probabilities completely analogous hmms notion derivation somewhat complex due branching character cfg 
left derivations isomorphic parse trees usual way probability derivation just product rule probabilities involved generation 
restriction left derivations avoids counting derivation multiple times computing total string probability 
usual say string ambiguous derivation non zero probability 
string may probability zero derived due missing productions derivations zero probability rule probability zero 
computing string probabilities difference rule zero probability 
difference relevant considering possible parameterizations scfgs corresponding prior distributions section 
scfg estimation estimating rule probabilities scfg problem unobservable derivations overcome just hmms probabilistic grammars hidden derivation processes 
standard solution em algorithm see section 
sufficient statistics estimating rule probabilities counts times rule derivations generating corpus denoted production 
maximum likelihood ml estimates obtained instantiating equation em procedure go expected counts string samples set current rule probabilities computed step 
rules reestimated formula step steps iterated converge local likelihood maximum 
presence parameter priors estimates modified accordingly 
example dirichlet prior nonterminal maximum posterior map estimate obtained applying equation rule counts 
map estimates em procedure finds parameters locally maximize posterior probabilities just likelihood 
hmms dynamic programming guise chart parsing compute expected counts relative efficiency 
dynamic programming scheme scfgs known inside outside algorithm baker 
originally formulated scfgs chomsky normal form cnf generalized cope arbitrary scfgs 
rule estimation part probabilistic earley parsing framework described chapter section 
complexity inside outside computation length input 
estimation algorithm quite bit expensive forward backward algorithm hmms 
complexity ways reduce discussed chapter 
chapter 
stochastic context free grammars viterbi parses derivation string viterbi parse 
case hmms simplify probability computations involved model merging algorithm keeping track statistics generated viterbi parses 
viterbi parses computed dynamic programming 
chart filled bottom computing substring sentence nonterminal partial parse highest probability 
chart completed maximum probability parse traced back root entry 
version method earley style chart parsing described chapter 
scfg merging describe ingredients extending model merging approach scfgs 
issues addressed cf 
section 
incorporate samples initial current model 

merging operators 

evaluate quality model 

search model space chosen operators evaluation function 
start presenting sample incorporation merging operators regard evaluation function walk example provide mechanics process 
priors scfgs discussed combined standard likelihood function scfgs equation give posterior probability evaluate models 
scfg merging algorithm completed specifying search strategies maximization posterior probability model space 
sample incorporation merging operators initial rules goal sample incorporation model merging produce initial incremental model accounts samples observed far typically dedicating simple submodels new sample 
scfgs accomplished dedicated top level productions designed generate sample 
specifically incorporate sample add productions current grammar create new nonterminals chapter 
stochastic context free grammars reason simply adding single production keep grammar special format simplifies definition merging operators 
initial rules created merging fall categories nonterminal productions non zero number nonterminals rhs 
terminal lexical productions single terminal rhs 
lhs nonterminals productions called 
refer type cfg lack better term call type rule format simple normal form snf 
general cfg null productions converted snf possibly shadowing terminals keeping structure derivations intact 
snf weaker constraint chomsky normal form cnf imposes binary branching structure derivations 
keeping count manipulating grammar productions keep track usage counts rule grammar 
statistics viterbi parses samples grammar built 
bayesian setting counts informative mere probabilities 
seen section case hmms viterbi counts estimate probabilities compute approximate posterior probabilities grammar structure effectively integrating rule probabilities treating nuisance parameters viterbi counts obtained efficiently parsing re parsing sample updating rule counts merging operation assumption viterbi parses preserved merging operators 
part strategy collapse multiple identical samples incorporation step assign corresponding counts initial productions 
notated parentheses productions 
counts fold occurrence sample create rules associated chapter 
stochastic context free grammars notation counts probabilities distinguished parentheses brackets respectively 
indicates rule probability sample strings 
denotes fold occurrence means probability relative distribution 
nonterminal merging distinction applied listing sets rewriting finite state grammar cfg states turn nonterminals left right linear productions 
natural generalize state merging operation hmms nonterminals scfgs 
nonterminal merging operator replaces existing nonterminals single new nonterminal 
merge grammars 
rhs occurrences replaced shorthand operation 
twofold effect merge note may lead merging entire productions case count merged production 

union productions lhss associated new nonterminal chapter 
stochastic context free grammars merge assume previously subjected substitution step 
lhs merging lead entire productions merged 
new count 
analogy hmm state merging carries 
hmms merging operation may generate loop implementing inductive leap model generates finite number strings allows infinitely strings 
true merging create recursion productions occur lhs rhs respectively production merge merging may introduce recursion intermediate nonterminals 
special case recursion dealt specially 
nonterminal merging creates rule deleted grammar 
rationale rules contribute derivations chains replaced single affecting structure derivation 
implies counts associated rules safely forgotten convenience snf format stems fact nonterminal merging applied directly initial grammar 
created specifically purpose gratuitous complication merging operator 
nonterminal chunking nonterminal merging create context free productions usual embedding struc ture additional operator needed 
natural extension combined merging enable possible cfg structures generated ordered sequence create new nonterminal expands replace occurrences rhss 
call operation chunking write chunk chunk 
simple direct recursion scfgs roughly correspond self loop hmm state 
general loops involve intermediate states course 
chapter 
stochastic context free grammars count newly created production sum counts productions substitutions occurred 
chunking viewed different kind merging operation 
identifying alternative submodels sublanguages generated nonterminals chunking hypothesizes submodels form unit virtue occurrence 
ways creating new units description correspond exactly distinction paradigmatic syntagmatic relations linguistic elements de saussure 
explicit terminology paradigmatic merging standard nonterminal merging operation syntagmatic merging chunking operation 
keep short names merging chunking brevity highlight similarity hmm state scfg nonterminal merging 
enable grammar generate new strings scfgs probability result operation 
merging isolated chunking step involves generalization strings generated 
nonterminal created chunking may feed subsequent merging steps producing generalizations wouldn possible precisely purpose introducing second operator 
principle chunking merging create cfg structure snf subject search involved finding right sequence operations assuming samples available 
seen follows target grammar need sample corpus covers rule grammar production generate corpus 
building initial grammar flat productions described section recreate target grammar merging chunking bottom obtain rules generating grammar 
specifically compare derivations incorporated samples derivations target grammar generated merging exactly nonterminals map target nonterminal chunking exactly nonterminals form phrasal units target grammar 
example simple formal language example illustrate interaction sample incorporation merging operators 
target grammar generates language suppose samples counts written snf implicit restriction occurrence part cfg framework 
result discontinuous syntactic units directly represented 
chapter 
stochastic context free grammars actual frequencies occurrence samples important determining result learning likelihood function grammar depends 
list actual log likelihood values ml parameter estimates step give idea data fit component eventual evaluation function 
initial grammar adopt convention names formed appending number uppercase version corresponding terminal symbol 
denote nonterminals created reserved start nonterminal 
gives initial grammar 

log likelihood 
merging steps collapse expanding terminal leave likelihood unchanged chunking operation chunk preserves likelihood shortens existing rules 
shorter description generally result higher prior probability grammar 
new merged chapter 
stochastic context free grammars log likelihood 
chunking operation chunk produces new immediately merged log likelihood 
chunking chunk subsequent merging step merge grammar final form log likelihood 
hmm example instructive consider additional merging steps lead various overly general grammars merge log likelihood merge log likelihood merge log likelihood confirms right time merging characterized large drop likelihood 
see posterior probability maximum point reasonable prior 
bracketed samples major source added difficulty learning scfgs opposed finite state models comes added uncertainty phrase structure bracketing observed samples 
chunking operation created precisely account part learning process 
chapter 
stochastic context free grammars sample strings enriched convey phrase structure information 
bracketed samples ordinary samples substrings enclosed balanced non overlapping pairs parentheses 
bracketing need complete 
example partially bracketed sample example language section known access completely bracketed samples equivalent unlabeled derivation trees learning non probabilistic cfgs possible tractable applying techniques borrowed finite state model induction sakakibara 
pereira schabes shown providing partial bracketing information help induction properly structured scfgs standard estimation approach 
raises question bracketed samples incorporated merging algorithm described far 
possible simple extension sample incorporation procedure described 
creating single top level production account new sample algorithm creates collection productions nonterminals mirror bracketing observed 
sample added grammar productions plus lexical productions created 
pair parentheses generates intermediate nonterminal 
merging chunking applied resulting grammar 
provided sample bracketing complete contains brackets phrase boundaries target grammar chunking unnecessary 
merging principle produce target grammar case provided samples productions 
scfg priors choosing prior distributions scfgs extend various approaches previously hmms 
model decomposed discrete structure collection continuous parameters choice narrow broad parameter priors depending identity non zero rule probabilities part section 
notice broad parameter priors problematic set possible rules parameters priori limited hmms 
length rhss grows number potential rules set nonterminals grows exponentially 
narrow parameter priors inherently simpler natural scfgs 
prior combination priors experiments reported 
parameter product dirichlet distributions section nonterminal 
dirichlet allocates prior probability possible expansions single nonterminal 
total prior chapter 
stochastic context free grammars prior probability geometric poisson production length geometric poisson length distributions identical distribution mean 
weights equal constant 
symmetrical expansions nonterminal priori equally 
convenient simplification nonterminals priori independent expansions 
simple description length prior 
variant nonterminal production encoded string nonterminals marker symbol requiring log lexical productions need log encode 
bits 
simple prior effect letting prior length productions form geometric distribution unnatural especially natural language grammars 
second version description length prior devised corresponds production lengths drawn poisson distribution length prior mean 
shows geometric poisson prior length distributions mean length 
minimum length production case zero encoding production length takes log nonterminals 
case total description length dl prior assigned log bits plus log bits grammar productions computed dl chapter 
stochastic context free grammars criterion model evaluation posterior model structure discussed section 
approximated viterbi method described hmms section 
advantage posterior decomposed product terms grammar nonterminal 
changes posterior computed efficiently recomputing just terms pertaining nonterminals affected merging chunking operation 
search strategies question search efficiently sequences merging operators pressing case scfgs 
main reason new operator chunking creates complex topology search space 
addition evaluation chunking step directly comparable merging chunking generalizing effect grammar affect prior contribution posterior 
experimented search strategies scfg learning discussed 
clearly sophisticated ones possible await study 
best search approach merging hmms 
operator types application instances pooled purpose comparison step locally best chosen 
combined simple linear look ahead extension described section help overcome local maxima 
simple approach fails chunking typically followed merging steps produce improvement 
look ahead feature doesn help chunks get way chunking step right successive merging choices 
multi level best search possible solution problem search procedure aware different nature operators constraining way interact 
empirically simple extension best paradigm generally scfgs 
basic idea search operates distinct levels associated merging chunking respectively 
search merging level consists best sequence merging steps look ahead 
search second level chooses locally best chunking step proceeds search level 
clearly approach generalized number search levels 
notice approach chunking steps evaluated trying exhaustive sequence merges possible choice 
entail overhead quite significant small cases 
beam search beam search locality search relaxed considering pool relatively models simultaneously single best search 
section remarked beam search hmms rarely give worthwhile improvements best approach 
chapter 
stochastic context free grammars beam search produce significantly improved search results scfgs precisely interaction different search operators impact proper evaluation choices 
beam width sufficiently large effects combinations merging chunking assessed correctly types operators treated specially 
adding multi level approach result improvements efficiency results hasn investigated 
concreteness give brief account beam search algorithm especially open ended nature search absence goal state different standard beam search algorithms literature 
beam list nodes ordered descending evaluation score 
case node corresponds model grammar evaluation function posterior probability 
nodes expanded unexpanded depending successor nodes generated search operators 
node expanded descendants inserted beam unexpanded nodes 
single step beam search consists expanding unexpanded nodes current beam available operators 
scope beam search determined parameters beam depth maximum total number nodes beam beam width gives number unexpanded nodes allowed beam 
expansion beam low scoring nodes truncated beam exceed width depth 
alternatively may limit nodes beam scoring certain tolerance current best node 
combination conditions delimiting elements beam known beam criterion 
search terminates unexpanded nodes satisfy beam criterion expanded nodes remain beam 
best scoring node beam returned result search 
beam search described generalization level best search introduced section beam search depth width equivalent best search steps lookahead 
search grammar spaces raises question equivalence models determined efficiently 
necessary avoid duplicate models crowding worthwhile contenders beam 
duplicates generated pervasively operators merging applied different order yield identical results 
address problem scfgs similar types models pronged approach 
efficient methods computing posteriors models necessarily computing full models applied incremental evaluation strategies described section 
model different posterior 
secondly necessary compute hash function cfg structure pseudo random number depends structure productions order names nonterminals 
grammars yield hash code considered identical purposes beam search 
leaves small probability model mistakenly discarded 
hash function optimal probability current implementation 
chapter 
stochastic context free grammars miscellaneous restricted chunking observed unrestricted chunking produce arbitrary cfg structures 
practical purposes set potential chunks needs restricted avoid generating large number hypothesis search step 
specifically restrictions optionally imposed 

null productions 
result proposing empty chunks 

unit chain productions 
result singleton chunks 

sequence nonterminals length exceeding needs occur twice grammar candidate chunking 

chunk replaced occurs opposed choosing subset occurrences 
known sort global constraint restrictions place grammars inferred form intermediate grammar hypothesis depends occurring samples dynamics search process 
chunking undone occasionally chunking operation nonterminals created superfluous retrospect occurrence nonterminal remains grammar result productions merging 
case beneficial undo chunking operation step call 
note final outcome cases achieved choosing chunk place provides trivial convenient way recover chunks temporarily advantageous 
efficient sample incorporation simple extension batch merging procedure incremental line version discussed hmm case section applied unchanged scfgs 
includes prior weighting factor control generalization prevent overgeneralization early rounds incremental merging section 
incremental merging default method experiments reported noted 
result incremental nonterminal merging sequence nonterminals previously subject chunking reappear 
case chunking operation re applied previously allocated lhs nonterminal replacing re occurring sequence 
special form chunking operator known 
various additional strategies possible order reduce number new nonterminals created sample incorporation reducing subsequent merging 
drawback adapted cook 
noticed similarity approaches see discussion section chapter 
stochastic context free grammars approaches reuse previously hypothesized grammar structures possibly preventing algorithm considering better alternatives 

avoid duplicate samples incorporate duplicate samples appropriately adjusted counts 
trivial optimization harm 

try parsing samples resorting ordinary creation new productions 
new sample parsed successfully counts old productions updated reflect new sample 
method subsumes strategy 
see section ways efficiently handle parsing bracketed samples needed method applied structured samples 

save initial merging reuse existing possible 
precludes creation grammars ambiguity level lexical productions 

try parse new sample string fragments existing rules add top level production link fragments start symbol 
subsumes strategy strategy 
section describes approach parsing ungrammatical samples fragments 
noted strategy obtaining results reported 
related bayesian grammar learning enumeration mentioned horning early proponent bayesian version grammar inference enumeration principle general applied theory type probabilistic grammar 
horning focus probabilistic cfgs formal device enumerate grammars assign prior probabilities grammar generating grammar grammar grammar 
expected enumeration practical simplest target grammars horning theoretically important point posterior probabilities formalization simplicity vs data fit trade 
merging chunking approaches idea combining merging chunking hill climbing style search procedure induce cfg structures developed independently researchers 
list aware 
sample ambiguous counts updated derivation respective probabilities viterbi derivation 
case likelihood sample underestimated viterbi computation posterior probability 
updating viterbi derivation favor creation unambiguous grammar structures detailed comparisons done issue 
chapter 
stochastic context free grammars cook 
exhibit procedure similar somewhat different set operators 
approach aimed probabilistic scfgs uses conceptually quite different evaluation function discussed detail illustrates fundamental feature bayesian philosophy adopted 
langley discusses non probabilistic cfg induction approach merging chunking operators described turn wolff 
langley cfg learner alternates merging chunking 
incremental learning strategy described adding lines straightforward 
evaluation function non probabilistic incorporates heuristics control data fit bias grammar simplicity measured total length production rhss 
comparison bayesian criterion highlights considerable conceptual practical simplification gained probabilities universal currency evaluation metric 
approach derived minimal extension hmm merging approach scfgs see section origins state merging concept 
related various induction methods non probabilistic cfgs rely structured parse tree skeleton samples form tree equivalence classes correspond nonterminals cfg fass sakakibara 
seen merging sufficient induction operator fully bracketed samples provided 
cook grammatical inference hill climbing cook 
hill climbing search procedure scfgs shares features ideas 
best approach evaluation metric aims balance complexity grammar discrepancy relative target distribution 
crucial difference relative frequencies samples serving approximation true target distribution 
discrepancy grammar samples evaluated metric combines elements standard relative entropy ad hoc measure string complexity 
complexity grammar likewise measured mix rule entropy rule complexity 
discrepancy complexity combined weighted sum weighting factor set empirically induction procedure apparently quite robust respect exact value parameter 
see conceptual difference bayesian approach consider introductory example section 
samples observed relative frequencies evidence generalization target grammar generates samples observed fold frequencies hypothesis bayesian learner refrain generalization due fold increased log absence additional samples eugene charniak pointing known accessible deserves 
exact rationale measures entirely clear complexities strings determined independently underlying model inconsistent standard information theoretic mdl approach 
chapter 
stochastic context free grammars likelihood loss case 
algorithms just relative frequencies samples hand indifferent change absolute frequencies 
bayesian approach gives intuitive interpretation weighting factor useful practice globally balance complexity discrepancy terms section 
cook 
propose larger set operators partly overlaps merging chunking operations 
chunking known name substitution merging directly available similar effects obtained operation called disjunction creates new nonterminals expand number existing nonterminals 
special operations removing productions subsumed redundant 
emulated merging explicit testing elimination redundant productions useful algorithm shortcuts combinations induction steps macro operators search parlance 
cook 
evaluate algorithm number benchmark grammars reexamined bayesian algorithm 
evaluation merging algorithm scfgs evaluated number experiments 
fall naturally broad categories simple formal languages various grammar fragments modeling aspects natural language syntax 
formal language benchmarks group test grammars extracted article cook 
discussed section 
represent examples simple context free formal languages typically textbooks subject 
main advantage set grammars results compared 
algorithms cook similar underlying intuitions structural grammar induction expect similar results important verify expectation 
experimental setup replicate examples cook procedure 
samples unchanged sample probabilities converted counts total number experiment 
incorporated initial grammars merged batch mode 
snf grammar format introduce subtle inductive bias cook original experiments 
merging procedure constrained maintain correspondence terminals effectively making terminals redundant letting function true terminals terms arbitrary productions possible 
chunking single nonterminals allowed 
notice repeated merging chunking effectively eliminate redundant productions example section 
chapter 
stochastic context free grammars language sample 
grammar search parentheses bf bf bf bf addition strings bs bs shape grammar bs basic english am woman man table test grammars cook 

bs sample number refers sample types tokens 
cook samples typically represent highest probability strings sample grammar relative frequencies matching probabilities 
sample counts scaled total experiments 
grammar grammar identical cook 
save space grammars written cook notation snf experiments 
search simplest search method needed obtain result best bf beam search width bs 
shape grammar refers cfg lee fu model dimensional chromosome pictures samples 
basic english sentences language teaching textbook richards gibson 
priors standard ones symmetrical dirichlet distribution set rule probabilities simple description length prior grammar structure 
extra global weighting prior applied 
results grammars merging algorithm matched ones cook cases 
table gives summary languages samples grammars 
exact match results somewhat remarkable evaluation function algorithm differs significantly cook see discussion section 
search operators procedure different similar intuitions 
indicate languages question fairly robust respect reasonable formalization trade data fit grammar complexity 
interestingly number samples chosen important matching cook result 
doubling chapter 
stochastic context free grammars number caused resulting grammars general cases giving significantly samples produce general results 
discussed evaluation function fact depend actual number samples just relative frequencies conclude cook evaluation function tuned roughly equivalent bayesian posterior sample count 
palindromes simple palindrome language benchmark language investigations cfg learning algorithms 
cook 
discuss proven relatively difficult briefly language reach search operators noting require chunking operator blindly replace occurrences chunk 
learn grammar start chunking sequences typically occur non center position misleading strictly greedy learning strategy 
pereira schabes language show standard scfg estimation poorly finding grammar right form started random initial parameters 
results estimation improved considerably bracketed samples resulting grammar gives incorrect parse structure strings 
algorithm fails find grammar language best search due limitations chunking operation cited cook 
result improved applying powerful beam search beam width 
perfect grammar language production marked 
production redundant strictly required derivation samples eliminated simple viterbi reestimation step search process 
viterbi step meant correct counts inaccurate due optimistic update procedure merging 
result significant show simple merging operators considerably powerful combined sophisticated search techniques expected 
merging 
incidentally palindrome language easy learn bracketed samples concreteness training setup easier palindrome language center symbol fifth table 
samples number identical removal center marker chapter 
stochastic context free grammars natural language syntax section show examples applying bayesian scfg merging algorithm simple examples natural language syntax 
strong caveat order point experiments involve actual natural corpus data 
artificial corpora generated grammars supposed model various aspects natural languages idealized form subject inherent constraints context free grammars 
main goal experiments test ability algorithm recover specific typical grammatical structures purely distributional evidence stochastically generated corpora 
applying algorithm actual data involves additional problems discussed 
indicated section prior production lengths corresponding poisson distribution 
prior mean length set 
lexical categories constituency grammar serve baseline experiments 
generates simple sentences transitive intransitive verbs predications involving prepositional phrases 
indicated productions lhs equal probability 
np vp np det vp vt np vc pp vi pp np det vt touches covers vc vi rolls bounces circle square triangle corresponding corpus contains randomly generated sentences including repetitions 
total number distinct sentences generated grammar 
samples illustrate range allowed constructions circle covers square square triangle circle bounces corpus input incremental best merging 
algorithm produced eventual result grammar having processed samples corpus 
chapter 
stochastic context free grammars result grammar weakly equivalent generates sentence target identical lexical rules 
sentence level productions structured np vc np np vi np vt np np det algorithm successful grouping terminals appropriate lexical categories identifying pervasive np constituents 
log posterior probability grammar slightly target grammar 
phrase structure extensive beam search width finds alternative grammar exhibiting deeper np vp vp vi np vt vc np det structure turns somewhat higher log posterior probability target 
chunking vc prefered standard np allows merging vp expansions respectively 
interestingly importance prior production lengths evident experiment 
poisson prior length distribution flat productions grammar yield higher posterior probability target grammar 
recursive embedding constituents get pp constituents distributional evidence grammar enriched pps pps embedded nps 
changed productions np vp pp comma np vi pp vc np np det det pp comma extends range sentence samples square square circle triangle circle touches triangle triangle circle rolls square triangle square square bounces induced grammars section notated nonterminal labels chosen target grammar possible enhance readability 
actual nonterminal names course irrelevant algorithm 
chapter 
stochastic context free grammars processing random samples grammar generates unbounded number sentences phrase structure rules 
np vc pp np vi np vt np pp comma np vi pp vc np pp np np det np pp induced rule structure differs aspects target 
firstly vp phrase structure inferred 
surprising vp realizations appears single rule generalization greater succinctness gained positing vp constituent 
secondly embedding pp np realized recursion left recursive np rule tail recursive pp rule 
redundancy leads slightly suboptimal score compared target grammar 
grammar clearly weakly equivalent target 
agreement pervasive phenomenon natural languages agreement lexical items constituents forced share features number gender case marking purposes construe phonologically motivated alternation english determiners versus thee case np internal agreement 
minimal extension baseline grammar nonterminals differ baseline shown np det det det det thee circle square triangle arc merging random samples productions fact generalized correctly remainder grammar identical learned baseline corpus 
noted context free grammars particularly suited description agreement phenomena especially agreeing constituents separated material affected agreement 
cfg description case replicate syntactic categories smallest enclosing constituent level 
standard solution problem factor agreement separate description usually feature formalism 
chapter 
stochastic context free grammars problem demonstrated minimal extension baseline grammar incorporate singular plural nps enforce number agreement main verb subject np 
np sg vp sg np pl vp pl np np sg np pl np sg det sg np pl det pl vp sg vi sg vt sg np vc sg pp vp pl vi pl vt pl np vc pl pp pp np det pl circles squares triangles sg circle square triangle vc sg vc pl vi sg bounces rolls vi pl bounce roll vt sg covers touches vt pl cover touch grammar generates sentences including circle bounces circles bounce square triangles squares triangles extensions scope language comes considerable price categories np vp vt vc vi duplicated encode number feature 
subjecting sentence sample merging algorithm exposes additional unexpected problem 
grammar learned weakly equivalent target grammar particular contains lexical categories target grammar 
phrase structure rules quite different group verbs prepositions subject object nps 
np sg vi sg np pl vi pl np sg np pl np sg vt sg np pl vt pl np sg vc sg np pl vc pl np sg det sg np pl det pl chapter 
stochastic context free grammars grammar marginally lower posterior probability target grammar 
reason fairly wide beam search consistently finds phrase structure traditional subtle inherent representational problems agreement phenomena cfgs 
agreement mediated duplication non merging identical nonterminal symbols smallest phrase level enclosing agreeing elements advantageous group agreeing non agreeing parts syntax 
strategy minimizes number extra nonterminals required 
fundamental chunking alternatives example verbs subject vs object regain equal description length generalizations 
localized search prefer subject verb grouping 
presumably actual natural language learner access cues favor chunking verbs objects 
simulated samples contain partial bracketings square triangle triangles circles necessary add partial bracketing samples partially bracketed samples chunking remaining ones appropriate way best scoring decision 
grammar learned sentence corpus modified samples explicit vp bracketing 
expected leads traditional phrase structure including necessary nonterminal duplication account agreement 
np sg vp sg np pl vp pl vp sg vi sg vt sg np sg vt sg np pl vc sg pp vp pl vi pl vt pl np sg vt pl np pl vc pl pp pp np sg np pl np sg det sg np pl det pl incidentally reason productions np np sg np pl chunking replaces occurrences chunk 
result operations chunk np sg chunk np pl interfere productions handle agreement rejected 
chapter 
stochastic context free grammars np vp vp np np art noun art ap noun ap ap art ate saw heard noun cat dog big old sample ordering example grammars langley 
np vp vp verb np np art noun art noun rc rc rel vp verb saw heard noun cat dog mouse art rel conclude pseudo natural language examples langley hill climbing learner non probabilistic cfgs studied see section 
point ordering samples incremental processing considerable effect outcome speed learning process 
grammars shown exhibit types recursion 
grammar contains np rule arbitrary number adjectives old big cat heard big old big dog grammar similar allows embedded relative clauses adjectives 
yields sentences mouse heard dog heard mouse heard cat purpose experiment uniform added original non probabilistic cfg productions generate random samples sample respectively 
samples conditions random order order increasing sentence length number words 
samples processed incremental merging chunking 
sample ordered sentence length resulted grammar contained redundant rules removed reestimation 
final grammar np vi np vt np np det det np np np det cat order samples length remained random 
chapter 
stochastic context free grammars dog big old vi ate vt heard saw essentially phrase structure target grammar recursion noun phrases implemented differently 
hand unordered samples algorithm produced grammar contained redundant rules 
grammar removing rules restarting search pruning redundant rules 
ordered samples samples required creation new rules merging steps necessary produce final grammar seconds total cpu time 
unordered samples hand lead new productions samples merges took seconds 
sample outcome similar 
ordering samples length resulted grammar acceptable rendering original 
np vp vp np np det np rc rc rel vp det cat dog mouse rel heard saw comparison experiment unordered list samples aborted due excessively long run time 
speed accuracy 
reasons ordering samples length incremental merging leads improved similar samples eventually lead single generalization closer reducing time takes hypothesize accept generalization 
result number samples corresponding states fully merged model structure minimized reducing search space 
beam search data produced grammar better target grammar weakly equivalent produced essentially phrase structure shorter rules 
chapter 
stochastic context free grammars rules learned smaller samples tend useful structuring larger samples way 
analyses previous samples effectively guide search merges longer ones 
interesting related result non probabilistic language induction strictly cally ordered sample presentation learning problem certain classes grammar provably easier feldman 
summary discussion artificial examples show scfg learning purely distributional evidence generic bayesian simplicity bias correctly identify common linguistic structures 
shows limited evidence misleading especially comes finding right phrase structures 
hardly surprising lot cues human judgments linguistic structure come sources semantic referents syntactic elements phonological cues morphological markers 
morgan 
brief experiment applied algorithm sentence corpus collected speech system jurafsky 
algorithm produced plausible lexical categories large number chunks corresponding frequent phrases collocations 
generalization achieved near required sufficient coverage new data 
problems partly addressed simple preprocessing steps tagging lexical items standard probabilistic approaches achieve reasonable performance data sort kupiec 
non traditional phrase structuring may problem sentence level generalization main goal application 
bracketing may induced separately bracketing models trained structured data brill 
plan investigate hybrid strategies 
chapter probabilistic attribute grammars chapter describes extension stochastic context free grammars chapter allows probabilistic modeling simple kinds features attributes attached nonterminals grammar 
example attributes represent limited semantics sentences constituents 
call extension probabilistic attribute grammars particular formulation chosen possible extension minimal ways 
learning problem associated follows 
samples pag language pairs sentence strings attribute value sets 
attributes values implicitly describe root nonterminal sentence 
types grammars goal find pag generates samples possibly generalizations extracted samples 
probabilistic models quality evaluated quantities likelihoods prior probabilities usual 
start defining pag extension scfgs section 
parallel extension scfg merging algorithm infer simple section 
experiments drawn language learning scenario introduced chapter described section 
due simplicity current pag formalism little illustration standard probabilistic models learning merging principle generalized new types grammars 
section discusses limitations problems current approach points possible remedies 
section summarizes main points 
probabilistic attribute grammars attribute grammars familiar extension non known compiler theory aho 
attributes registers slots attached nonterminal nodes context free parse tree hold values various purposes typically pertaining semantics chapter 
probabilistic attribute grammars language 
context free productions augmented attribute specifications determine nonterminals attributes computed function nonterminals attribute values terminals 
linguistic formalisms context free grammars employ related concept typically known name features attributes attachments nonterminals grammar values specifications usually restricted form directed acyclic graphs unification equations shieber 
attribute grammar model simplified variant traditional cfg extensions probabilistic component added yield usual notions sample probability likelihood terms attribute feature interchangeably 
definitions model characterized follows 
definition probabilistic attribute grammar pag stochastic context free grammar fol lowing additional provisions 
nonterminal scfg derivation associated finite set features set 
feature function assigns element finite set feature values nonterminals defined 
traditional precedents mentioned denote fact feature nonterminal value extended context free production specifies feature values attached lhs nonterminal terms constant values features rhs nonterminals 
production feature specifications 
specification determines value value feature feature equations associated probabilities written certain ordering constraints typically imposed guarantee attribute values computed effectively efficiently 
chapter 
probabilistic attribute grammars feature specifications certain lhs feature mutually exclusive assign lhs feature chooses probabilistically feature values rhs features 
probabilities involving lhs sum unity 
notation important context able distinguish nonterminal type nonterminal instance derivation node parse tree 
attributes specifications refer nonterminal instances 
simplify notation rely nonterminal names appear context free production possible ambiguity 
sufficient superscripts parentheses distinguish multiple occurrences nonterminal type 
equates value node value second node 
definition suggests straightforward derivation model 
derivation proceeds phases sentence string generated scfg usual top fashion second probabilistic feature assignment takes place bottom consulting feature equations probabilities determined scfg derivation 
features assigned root nonterminal definition features sentence 
definition pag scfg set context free productions set feature equations 
string set feature value assignments pairwise distinct 
definition 
feature derivation derivation choosing exactly feature equation lhs feature expansions making derivation said yield feature assignment feature value pairs subset obtained propagating values rhs nonterminals lhs nonterminals obvious way implied feature equations chosen probability feature derivation string derivations product probabilities equations chosen 
feature probability string derivation sum yields chapter 
probabilistic attribute grammars string feature probability ranges context free derivations yielding string feature assignment pair decomposing stochastic derivation process way convenient definition associated joint probabilities rules interesting uses attributes 
section discusses alternative computationally complex definitions 
example simple pag easily written language 
consider sentences containing single unary binary predicate circle moves circle square square touches circle triangle covers square semantics sentences encoded attributes tr trajector lm landmark rel relation attribute values correspond meanings denoting words circle square touch samples including attributes circle moves tr circle rel move circle square tr circle lm square rel square touches circle tr square lm circle rel touch triangle covers square tr triangle lm square rel cover pag generating similar samples standard scfg backbone types rules lexical productions assign semantic constants denoting words nonterminal productions pass feature values root node 
semantic ambiguities language feature equations carry probability omitted 
scfg probabilities shown arbitrary depending intended distribution 
pag 
np vp tr np lm vp rel vp np det np vp vi vp vi vc pp mind notion meaning blatantly simplistic english centric point illustrate workings chapter 
probabilistic attribute grammars vp pp vp pp vt np vp np vp vt det circle circle square square 
vc vi moves vi move falls vi fall vt touches vt touch covers vt cover 
pp np pp np pp 
feature names top level ones chosen arbitrarily 
particular requirement lhs feature names coincide rhs feature names convenient feature equations readable 
simple lexical ambiguities modeled feature assignments non unit probabilities nonsense square triangle pag estimation standard grammar formalism standard estimation algorithm sets pag probability parameters set samples grammar rules 
iterative likelihood maximization scheme em techniques applied 
focus merging structure learning parameter estimation approaches give details algorithm 
basic idea estimate scfg part grammar separately scfg derivation compute occurrence feature values chapter 
probabilistic attribute grammars adjacent parse tree nodes way similar belief propagation algorithm pearl conditioning top level feature assignments choice scfg rules 
conditional distribution feature values adjacent nodes known likelihood maximized choosing feature equation probability proportional number times features value 
scheme principle capable finding full kind shown starting randomly initialized feature probabilities prone local maxima highly dependent initial conditions parameters 
expected problem observed increasingly deep grammars intermediate features hidden variables part derivation 
problems parameter estimation approach structural learning previ ously observed hmms section scfgs pereira schabes show severely pag formalism 
gives motivation investigation merging learning approaches 
pag merging sections describe usual ingredients necessary specify merging learning algorithm pag formalism 
surprisingly build corresponding components scfg merging algorithm 
sample incorporation step learning algorithm creation ad hoc rules allow new samples generated 
newly created top level productions generate sample strings augment feature value assignments generate observed attribute value pairs 
sample circle square tr circle lm square rel incorporated creating productions circle square tr circle lm square rel circle circle square square extend earlier notation associate counts productions individual feature equations 
counts feature equations indicate equation feature derivations compute likelihoods probability estimates 
section 
chapter 
probabilistic attribute grammars notice features attached lexical productions created imply knowing semantic contributions word 
features attached sentence corresponding rote learning sentence meaning associations 
nonterminal merging chunking nonterminal merging chunking operators introduced chapter applied pag rules modifications 
merging cfg part productions operates usual renaming nonterminals possibly merging productions 
feature equations affected renaming feature equations identical result nonterminal merging associated counts added usual 
way example suppose merge andn starting productions np det np np det np result merge nis np det np np notice feature andf automatically merged accomplished separate operator 
nonterminal chunking operates may require creation new ad hoc features attached newly created nonterminal order preserve features chunked nonterminals 
easily seen example det tr lm rel applying chunk np preserved creating corresponding feature np np np tr np lm np rel np det np chapter 
probabilistic attribute grammars feature operators new operators necessary perform generalizations involving feature equations 
feature merging obvious step merging feature names similar way nonterminals merged 
consider grammar det tr lm rel circle circle square square merged single denoted det tr lm rel circle circle square square feature equation counts preserved step counts added equations merge result renaming features 
feature operation feature attribution postulates certain feature value assigned lhs nonterminal due rhs nonterminal feature 
operation written 
illustrate consider rules created sample subsequent merging lexical categories 
circle square tr circle lm square rel square touches triangle tr circle lm square rel circle circle square square triangle triangle touches touches feature newly created existing feature effectively reused subsequently merging 
chapter 
probabilistic attribute grammars attributing square new productions circle square tr circle lm square rel square touches triangle tr square lm triangle rel square square square square choice feature value attribute nonterminal occurrence rhs nondeterministic heuristics practice see 
efficient search feature operations combining operators convenient conceptualize operators described separate induction steps 
practice efficient combine operators due typical interactions 
experiments reported done existing level best search strategy extensive search method 
feature attribution merging operators particular produce improved posterior probability score conjunction certain nonterminal merging operations 
consider rules np det np det np circle square square circle simply applying syntactic merging operation merge result np det np np circle square square circle cuts probability samples productions half 
alternatively precede merging operation appropriate feature operations circle followed gives grammar np det np chapter 
probabilistic attribute grammars square square circle square preserves likelihood original grammar 
general strategy combining feature operations syntactic merging steps preserve determinism feature equations possible constraint samples generated 
algorithm implementing strategy striking similarity standard feature unification 
think feature operations example unifying constant newly created feature unifying features 
standard unification resolve feature value conflicts splitting probabilities 
feature attribution method efficiently find feature attributions feature attributions considered independently potential search steps 
heuristic suggest promising candidates correlation mutual information occurrences nonterminals feature values rhss productions feature equations respectively 
specifically operation considered nonterminal feature value close perfect correlation alternatively large positive mutual information 
applying operator type deal nondeterminism arising multiple occurrences nonterminals production rhss 
instance value attributed 
full search alternative attributions discover yield feature merging opportunities preserve model likelihood chose appropriate alternative basis 
processing samples incremental mode simple effective approach delay buffer processing samples lead productions give rise ambiguity feature attributions 
specifically processing samples distinct feature values done deterministically 
ambiguities held back samples implicitly resolved merging existing productions 
example circle square tr circle lm square rel circle circle tr circle lm circle rel operations circle circle unambiguous production second 
second production set aside eventually attribution steps results chapter 
probabilistic attribute grammars circle square tr circle lm square rel circle circle circle circle square square square square nonterminal merging operation merge circle square triggers merging 
tr lm rel circle circle square square nonterminal merging step renders original productions identical means associated feature equations reconciled 
instances circle implicitly attributed second feature part combined merging operators discussed previous section 
pag priors prior probability distributions constructed familiar techniques 
probabilities features equations involving fixed values rhs features represent multinomial parameter tuples lhs feature accordingly covered dirichlet prior distribution 
feature equations priors description length 
equation involving constant feature value comes code length cost log bits 
equations features rhs encoded log set features attached nonterminal grammar need encoded uniquely length production rhs 
log bits rhs nonterminal question description length prior favors feature merging operations achieve compact feature specifications virtue collapsing feature names equations 
alternatively feature specifications priori equally rely solely prior context free part grammar bias smaller rules 
feature equations exist independently context free productions implicitly bias feature descriptions complexity 
furthermore redundant strictly possible proper prior probability mass sum sort decay cut large grammars 
comparing grammars limited roughly equal size uniform prior approximation 
chapter 
probabilistic attribute grammars feature equations lead unnecessary splitting feature probabilities likelihood component posterior probability 
experiments examples mentioned chapter motivation pag extension scfgs came partly miniature language learning task proposed feldman 

natural test formalism problem 
description length bias feature equations 
minimal fragment english associated generating grammar example grammar section 
version grammar nouns prepositions verbs category generate random sentence sample 
alternative productions equal probability 
generated sentences processed incremental best search induction operators described 
result grammar weakly equivalent target 
includes accurate feature attributions various terminal symbols appropriate feature passing instantiate top level features 
structural difference grammars flatter phrase structure induced grammar np vi tr np rel vi np vc np tr np lm np np vt np tr np lm np rel vt np det np lexical productions feature assignments vi vt vt det example grammar 
base scfg case discussed section beam search finds deeper phrase structure somewhat higher posterior probability np vp tr np lm vp rel vp vp vi usual rename arbitrary nonterminal feature names generated implementation close target grammar result intelligible 
chapter 
probabilistic attribute grammars vp vi np vp np vp vc vt vt notice non traditional vc chunk case happens match attribute passing rules 
vc allows collapsing cfg productions feature equations vp expansions copula transitive sentences 
syntactic constraints imposed attributes possible find cases unstructured feature assignments supplied part samples implicitly favor phrase structure exhibiting primitive form semantically induced syntactic bias 
due simplicity pag formalism interactions pervasive think 
purely syntactic contingencies tend interfere render semantically preferred alternative dominant regardless attributes 
interestingly example semantic bias entirely likelihood data fit component posterior probability phenomenon dependent bias embodied prior distribution 
start set sample sentences exhibiting types sentence phrase structure 
brevity give sample sentence example go structure associated set samples 
circle touches square circle square square circle assume initial merging chunking resulted traditional np pp phrase categories 
baseline grammar example follows samples corresponding rules listed convenience 
np vt np circle touches square tr np rel vt lm np np vi pp circle square tr np rel pp lm pp pp vi np square circle tr np rel pp lm pp chapter 
probabilistic attribute grammars consider sequence steps necessary arrive phrase structure 
chunks replaced independently chunk vt np vp chunk vi np vp resulting grammar np vp vp vt np tr np vp vt rel vp vp np lm vp np vp vp vi pp tr np vp pp rel vp vp pp lm vp final merging step merge vp vp vp gives familiar result np vp vp vt np tr np vp vt rel vp vp np lm vp vp vi pp vp pp vp pp sequence steps straightforward possible 
alternative sequence uses types operators yields grammar symmetrical equally preferable syntactic grounds assuming distributional evidence favors 
alternative sequence steps starts baseline grammar chunking wrong constituents chunk np vt xp chunk pp vi xp 
xp np xp np vt tr xp xp vt rel xp xp np lm np xp np vp pp vi tr np xp pp rel xp pp lm xp merge new phrase categories merge xp xp xp 
resulting scfg productions number nonterminals productions lengths original solution receive prior probability prior explicitly favors certain types phrase structures right branching left branching 
feature equations obtained quite different case specification collapse equations 
xp np tr xp tr np rel xp lm np lm xp chapter 
probabilistic attribute grammars result probabilities split exact probabilities depend sample statistics lowering grammar likelihood 
alternative syntactic structure preferable due attached semantic features 
limitations extensions pag framework introduced chapter number obvious shortcomings mainly result desire keep probabilistic model simple various techniques familiar earlier models combinations multinomials associated priors viterbi approximations simple merging operators mention important limitations possible extensions remedy 
expressive feature constraints derivation probabilities defined carefully chosen conditional independence order computationally tractable 
marginal probability context free aspect derivation computed independent feature part grammar simply standard scfg rules 
particular feature specifications rule syntactic structure derived non zero probability 
feature derivation written product conditional probabilities lhs features rhs features 
reason feature dependencies put consistent total order 
computational comes price reduced expressiveness especially compare way features traditional non probabilistic grammars 
example rules natural account agreement phenomena relations rhs features np vp np number vp number np det np number number vp vp number number number assigned lexical productions feature equation production highly intuitive effectively require stating marginal probability joint event value assignments opposed conditional value 
strict bottom feature format relaxed notion attributed feature specifications aho 
chapter 
probabilistic attribute grammars theory markov networks pearl tells marginal probabilities assigned locally consistent fashion 
partial solution particular problem imposition global total ordering agreement features entire system constraints expressible product conditional probabilities 
ordering linguistic notion phrase head accomplish rhs features depend rhs features associated distinguished head constituent 
case convenient global bottom ordering feature constraints probabilities string component derivation longer independent featural aspects grammar 
systems unordered constraints probabilistic interpretation known concepts statistical physics 
directly defining probabilities local feature assignments define energy function expresses badness assignment arbitrary positive number 
energy function defined local components product local contributions rule 
rule translated term gives low energy np number vp number 
total energy generate probabilities boltzmann distribution complete feature assignment normalizing constant integral numerator parameter temperature controls peakedness distribution 
formulation elegant intuitively appealing obviates traditional concept derivation 
carries heavy computational price simply obtaining probability string feature assignment pair various alternative grammars generally requires stochastic simulations order compute constant 
posterior probabilities models evaluated monte carlo techniques neal approach inefficient generate test search strategies explored far 
hand formulation suggests investigating learning paradigms stochastic optimization simulated annealing boltzmann machine learning geman geman hinton sejnowski 
hierarchical features chapter saw merging algorithm quite capable inducing recursive syntactic rules 
pag formalism matching recursive structure feature descriptions constrained flat feature structures 
result semantics sentences simple embedded relative clauses circle square left triangle 
configurations equally probable regardless energy probability mass concentrate lowest energy configuration 
chapter 
probabilistic attribute grammars adequately described learned 
obvious solution representational problem hierarchical feature structures structures number linguistic theories grammar shieber 
raises new problems concerning proper probabilistic formulation 
example set hierarchical feature specifications finite set feature names infinite raising question appropriate prior distribution space 
new varied merging operators introduced match increased expressiveness formalism leading new intricacies search procedure 
pursuing extension full generality standard structures worthwhile subject research 
trade offs context free feature descriptions returning issue generalized feature constraints fundamental question evidence hidden feature constraints learned 
seen section agreement example represented learned posterior probabilities merging operators clear context free productions inadequate formalism phenomena 
ideally want move description np sg vp sg np pl vp pl np vp np number vp number general cases context free rules features provide alternative models distributional facts 
cases description length criterion decide better formulation 
incidentally nonterminals may expressed features gazdar shieber eliminating need separate descriptions mechanisms 
basis unified description length metric allows fair comparisons alternative modeling solutions 
summary chapter examined minimal extension stochastic context free grammars incor simple probabilistic attributes features 
saw pair feature oriented operators feature merging attribution existing scfg operators induce simple grammars formalism applied approach rudimentary form semantics miniature language restricted version embedding restricted level semantics flattened features learned operators described chapter 
chapter 
probabilistic attribute grammars domain 
interestingly cases simple semantics help disambiguate syntactic phrase structure learning purely distributional facts reflected model likelihoods semantics hierarchical structure 
addition new operators highlighted need complex heuristics equivalent macro operators order efficiently search space grammars 
pointed severe limitations formulation require substantial extensions order account interesting linguistic phenomena 
chapter efficient parsing stochastic context free grammars far discussed stochastic context free grammar mainly point view learning 
standard task preexisting scfg various problems computation linguistics applications requiring probabilistic processing 
literature scfgs selection parses ambiguous inputs fujisaki guide rule choice efficiently parsing jones eisner compute island probabilities non linear parsing 
speech recognition probabilistic context free grammars play central role integrating low level word models higher level language models ney non finite state acoustic phonotactic modeling lari young 
context free grammars combined scoring functions strictly probabilistic nakagawa context sensitive semantic probabilities magerman marcus magerman weir jones eisner briscoe carroll 
perfect model natural language stochastic context free grammars scfgs superior non sound theoretical basis ranking pruning applications listed involve potentially standard tasks compiled jelinek lafferty 

probability string 
single parse derivation generated grammar 

probability occurs prefix string generated prefix probability 
phrases problem terms context free probabilistic grammars generalize obvious ways classes models 
chapter 
efficient parsing stochastic context free grammars 
parameters rule probabilities chosen maximize probability training set strings 
incremental model merging algorithm scfgs chapter requires efficient operation 
traditional grammar parameter estimation essentially typically post processing step model merging grammar structure learned 
algorithm described chapter compute solutions problems single framework number additional advantages previously isolated solutions 
originally developed solely general efficient tool model merging algorithm 
realized solves task efficient elegant fashion greatly expanding usefulness described 
probabilistic parsers generalization bottom chart parsing cyk algorithm 
partial parses assembled just non probabilistic parsing modulo possible pruning probabilities substring probabilities known inside probabilities computed straightforward way 
cyk chart parser underlies standard solutions problems baker jelinek 
jelinek lafferty solution problem direct extension cyk parsing algorithm terms similarities computation inside probabilities 
algorithm computations tasks proceed incrementally parser scans input left right particular prefix probabilities available soon prefix seen updated incrementally extended 
tasks require reverse pass parse table constructed input 
incremental left right computation prefix probabilities particularly important necessary condition scfgs replacement finite state language models ap plications speech decoding 
pointed jelinek lafferty knowing probabilities arbitrary prefixes conditional probabilities word transition probabilities viterbi style decoder incrementally compute cost function stack decoder bahl 
enables probabilistic prediction possible follow words application prefix probabilities play central role extraction gram probabilities scfgs problem subject chapter 
efficient incremental computation saves time common prefix strings shared 
key features algorithm top parsing method non probabilistic cfgs developed earley 
earley algorithm appealing runs best known efficiency number special classes grammars 
particular earley parsing efficient bottom methods cases top prediction rule potential parses substrings 
worst case computational expense algorithm complete input incrementally new word known specialized algorithms substantially better known grammar classes 
chapter 
efficient parsing stochastic context free grammars earley parser deals context free rule format seamless way requiring conversions chomsky normal form cnf assumed 
advantage probabilistic earley parser extended take advantage partially bracketed input return partial parses ungrammatical input 
extension removes common objections top predictive opposed bottom parsing approaches magerman weir 
overview remainder chapter proceeds follows 
section briefly reviews workings earley parser regard probabilities 
section describes parser needs extended compute sentence prefix probabilities 
section deals modifications solving viterbi training tasks processing partially bracketed inputs finding partial parses 
section discusses miscellaneous issues relates literature subject 
section summarize draw 
get idea probabilistic earley parsing sufficient read sections 
section deals crucial technicality sections fill details add optional features 
assume reader familiar basics context free grammar theory aho ullman chapter 
jelinek 
provide tutorial covering standard algorithms tasks mentioned 
notation input string denoted length individual input symbols identified 
input alphabet denoted substrings identified indices starting positions variables reserved integers referring positions input strings 
chapter latin capital letters denote nonterminal symbols 
latin lowercase letters terminal symbols 
strings mixed nonterminal terminal symbols written lowercase greek letters 
empty string denoted 
earley parsing earley parser essentially generator builds left derivations strings set context free productions 
parsing functionality arises generator keeps track possible derivations consistent input string certain point 
input revealed set possible derivations corresponds parse expand new choices introduced shrink result resolved ambiguities 
describing parser appropriate convenient generation terminology 
parser keeps set states position input describing pending derivations 
earley states known items lr parsing see aho ullman section section 
chapter 
efficient parsing stochastic context free grammars state sets form earley chart 
state form nonterminal grammar strings nonterminals terminals indices input string 
states derived productions grammar 
state defined relative corresponding production semantics current position input processed far 
states describing parser state position collectively called state set note state set input symbols set describes parser state input processed set states input symbols processed 
contains position nonterminal expanded starting input generates substring starting position 
expansion proceeded production expanded right hand side position indicated dot 
dot refers current position 
rhs state dot right entire rhs called complete state indicates left hand side lhs nonterminal fully expanded 
description earley parsing omits optional feature earley states lookahead string 
earley algorithm allows adjustable amount lookahead parsing order process lr grammars deterministically obtain computational complexity specialized lr parsers possible 
addition lookahead orthogonal extension probabilistic grammars include 
operation parser defined terms operations consult current set states current input symbol add new states chart 
strongly suggestive state transitions finite state models language parsing analogy explored probabilistic formulation 
types transitions operate follows 
prediction state nonterminal rhs rules expanding add states index implicit earley 
include clarity 
chapter 
efficient parsing stochastic context free grammars state produced called predicted state 
prediction corresponds nonterminal left derivation 
scanning state terminal symbol matches current input add state move dot current symbol 
state produced scanning called scanned state 
scanning ensures terminals produced derivation match input string 
completion complete state set state right dot add move dot current nonterminal 
state produced completion called completed state 
completion corresponds nonterminal expansion started matching prediction step 
crucial insight working earley algorithm prediction completion feed finite number states possibly produced 
recursive prediction completion terminate eventually parser proceed input scanning 
complete description need specify initial final states 
parser starts sentence nonterminal note empty left hand side 
processing symbol parser verifies produced possibly length input intermediate stage state set remains empty states previous stage permit scanning parse aborted impossible prefix detected 
states empty lhs useful contexts shown section 
collectively refer dummy states 
dummy states enter chart result initialization opposed derived grammar productions 
note difference complete completed states complete states dot right entire rhs result completion step completion produces states complete 
chapter 
efficient parsing stochastic context free grammars easy see earley parser operations correct sense chain transitions predictions scanning steps completions corresponds possible partial derivation 
intuitively true parser performs transitions exhaustively complete finds possible derivations 
formal proofs properties literature aho ullman 
relationship earley transitions derivations stated formally section 
parse trees sentences reconstructed chart contents 
illustrate section discussing viterbi parses 
table gives example earley parsing form trace transitions performed implementation 
earley parser deal type context free rule format null productions replace nonterminal empty string 
productions require special attention algorithm description complicated necessary 
sections assume null productions dealt summarize necessary changes section 
chose simply preprocess grammar eliminate null productions process described 
probabilistic earley parsing stochastic context free grammars stochastic context free grammar scfg extends standard context free formalism adding probabilities production probability rule usually written conditional probability production chosen expansion 
notation extent hides fact probabilities rules nonterminal lhs sum unity 
context freeness probabilistic setting translates conditional independence rule choices 
result complete derivations joint probabilities simply products rule probabilities involved 
probabilities interest mentioned section defined formally 
definition quantities defined relative scfg nonterminal string alphabet 
probability partial derivation inductively defined definition expanded generalized version section 
chapter 
efficient parsing stochastic context free grammars np vp vp pp np det vt vi vp np pp np det vt vi circle square triangle touches circle touches square scanned scanned scanned scanned scanned predicted det circle vt touches det triangle np vp completed completed completed completed completed np det np det np det vp vt np np det np det det predicted np vp predicted predicted vp np vt circle predicted np det circle vp np square vp np vt det square triangle vp vi pp triangle vt touches vi state set table example non probabilistic earley parsing 
example grammar tiny fragment english 
earley parser processing sentence circle touches triangle 
chapter 
efficient parsing stochastic context free grammars string terminals nonterminals production derived replacing occurrence 
string probability derivations sum probabilities left producing sentence probability 
definition probability prefix probability having prefix particular 
string probability start symbol assigned grammar 
sum probabilities sentence strings assume probabilities scfg proper consistent defined booth thompson grammar contains useless nonterminals ones appear derivation 
restrictions ensure nonterminals define probability measures strings section 
proper distribution earley paths probabilities formal definitions conditions order define probabilities associated parser operation scfg need concept path partial derivation executed earley parser 
definition unconstrained earley path simply path sequence earley states linked prediction scanning completion 
purpose definition allow scanning operate generation mode states terminals right dot scanned just matching input 
completed states predecessor state defined complete state state set contributing completion 
path said constrained generate string immediately left dot sequence form string scanned states terminals path complete state matches dot moved rhs 
say path starts nonterminal state predicted state lhs 
left derivation step replaces nonterminal furthest left partially 
order expansion irrelevant definition due multiplicative combination production probabilities 
restrict summation left derivations avoid counting duplicates left derivations play important role 
chapter 
efficient parsing stochastic context free grammars length path defined number scanned states 
note definition path length somewhat counter intuitive motivated fact scanned states correspond directly input symbols 
length path length input string generates 
constrained path contains sequence states state set derived repeated prediction starting initial state followed single state set produced scanning symbol followed sequence states produced completion followed sequence predicted states followed state scanning second symbol 
significance earley paths correspondence left derivations 
allow talk probabilities derivations strings prefixes terms actions performed earley parser 
derivation imply left derivation 
lemma earley parser generates state partial derivation deriving prefix input 
partial derivation nonterminal path unique earley starting vice versa sequence prediction steps corresponds productions applied derivation generates terminal prefix string derived 
invariant underlying correctness completeness earley algorithm proved induction length derivation aho ullman theorem 
slightly stronger statement mapping derivations paths 
follows verifying left derivation choice nonterminal substitution corresponds exactly possible prediction step violated 
established paths correspond derivations convenient associate derivation probabilities directly paths 
uniqueness condition irrelevant correctness standard earley parser justifies probabilistic counting paths lieu derivations 
definition probability path product rule probabilities predicted states occurring 
lemma paths starting nonterminal gives probability partial derivation represented 
particular string probability probabilities paths starting complete constrained sum chapter 
efficient parsing stochastic context free grammars sentence probability initial state constrained prefix probability sum probabilities complete paths starting sum probabilities paths starting initial state constrained scanned state 
note summing paths starting initial state summation paths starting definition initial state 
follows directly definitions derivation probability string probability path probability correspondence paths derivations established lemma 
follows start nonterminal 
obtain prefix probability need sum probabilities complete derivations generate prefix 
constrained paths scanned states represent exactly beginnings derivations 
grammar assumed consistent useless nonterminals partial derivations completed probability 
sum constrained incomplete paths sought sum complete derivations generating prefix 
forward inner probabilities string prefix probabilities result summing derivation probabilities goal compute sums efficiently advantage earley control structure 
accomplished attaching probabilistic quantities earley state follows 
terminology derived analogous similar quantities commonly literature hidden markov models hmms rabiner juang baker 
definition definitions relative implied input string forward probability sum probabilities constrained paths length state inner probability sum probabilities paths length start state generate input symbols 
helps interpret quantities terms unconstrained earley parser operates generator emitting recognizing strings 
tracking possible derivations generator traces single earley path randomly determined choosing prediction steps associated rule probabilities 
notice scanning completion steps deterministic rules chosen 
intuitively forward probability probability earley generator produc ing prefix input position passing state position due left recursion productions state may appear times path occurrence counted total really expected number occurrences state state chapter 
efficient parsing stochastic context free grammars set having said refer simply probability sake brevity keep analogy hmm terminology generalization 
note scanned states probability definition scanned state occur path 
inner probabilities hand represent probability generating substring input nonterminal particular production 
inner probabilities conditional presence nonterminal expansion starting position forward probabilities include generation history starting initial state 
inner probabilities defined correspond closely quantities name baker 
sum states lhs exactly baker inner probability lemma essentially restatement lemma terms forward inner probabilities 
shows obtain sentence string probabilities interested provided forward inner probabilities computed effectively 
lemma earley chart constructed parser input string provided probability nonterminal generates substring possible left derivation grammar computed sum sum inner probabilities complete states lhs start index 
particular string probability prefix probability sum forward probabilities scanned states 
computed computed restriction preceded possible prefix necessary earley parser position pursue derivations consistent input position constitutes main distinguishing feature earley parsing compared strict bottom computation standard inside probability computation baker 
inside probabilities positions nonterminals computed regardless possible prefixes 
technical complication noticed wright computation probabilistic lr parser tables 
relation lr parsing discussed section 
incidentally similar interpretation forward probabilities required hmms non emitting states 
definitions forward inner probabilities coincide final state 
chapter 
efficient parsing stochastic context free grammars computing forward inner probabilities forward inner probabilities subsume prefix string probabilities straightforward compute run earley algorithm 
fact weren left recursive unit productions computation trivial 
purpose exposition ignore technical complications introduced productions moment return picture clear 
run parser forward inner probabilities attached state updated incrementally new states created types transition 
probabilities set unity initial state 
consistent interpretation initial state derived dummy production alternatives exist 
parse proceeds usual probabilistic computations detailed 
probabil ities associated new states computed sums various combinations old probabilities 
new states generated prediction scanning completion certain probabilities accumulated corresponding multiple paths leading state 
state generated multiple times previous probability associated incremented new contribution just computed 
states probability contributions generated order long summation state finished probability enters computation successor state 
section suggests way implement incremental summation 
notation intuitive abbreviations describe earley transitions succinctly 
avoid unwieldy notation adopt convention 
expression means computed incrementally sum various terms computed order accumulated yield value transitions denoted predecessor states left successor states right 
forward inner probabilities states notated brackets state shorthand prediction probabilistic 
new probabilities computed productions note forward probability accumulated 
notation suggests simple implementation obviously borrowed programming language chapter 
efficient parsing stochastic context free grammars rationale 
choosing production 
value just special case definition 
scanning probabilistic sum path probabilities leading times probability states terminal matching input position rationale 
scanning involve new choices terminal selected part production prediction 
completion probabilistic note 
rationale 
update old forward inner probabilities respectively probabilities expanding paths factored 
exactly paths summarized inner probability coping recursion standard earley algorithm probability computations described previous section sufficient weren problem recursion prediction completion steps 
non probabilistic earley algorithm recursing soon predictions completions yield states contained current state set 
computation probabilities mean truncating probabilities resulting repeated summing contributions 
different parsing scenarios scanning step may modify probabilities 
example input symbols attached likelihoods integrated multiplying symbol scanned 
way possible perform efficient earley parsing integrated joint probability computation directly weighted lattices input symbols 
chapter 
efficient parsing stochastic context free grammars prediction loops example consider simple left recursive scfg 
non probabilistically prediction loop position producing states leave forward probabilities corresponding just infinity possible paths 
correct forward probabilities obtained sum infinitely terms accounting possible paths length 
sums corresponds choice production choice second production 
didn care finite computation resulting geometric series computed letting prediction loop summation continue indefinitely 
fortunately repeated prediction steps including due left recursion productions collapsed single modified prediction step corresponding sums computed closed form 
purpose need probabilistic version known parsing concept left corner heart prefix probability algorithm jelinek lafferty 
definition definitions relative scfg 
nonterminals said left corner relation rhs starting exists production iff probabilistic left corner relation matrix probabilities defined total probability choosing production left corner probabilistic relation replaced version set theoretic iff closure operations reduce traditional discrete counterparts choice terminology 
chapter 
efficient parsing stochastic context free grammars relation defined reflexive transitive closure nonterminal 
iff probabilistic reflexive transitive left corner relation matrix probability sums defined series alternatively defined recurrence relation denotes kronecker delta defined unity zero 
recurrence conveniently written matrix notation closed form solution derived existence proof section 
appendix shows speed computation inverting reduced version matrix significance matrix earley algorithm elements sums probabilities potentially infinitely prediction paths leading state predicted state number intermediate states 
computed grammar table lookup modified prediction step 
prediction probabilistic transitive productions non zero 
note new path linking probabilities covers case single step prediction factor updated forward probability accounts sum defined reflexive closure 
chapter 
efficient parsing stochastic context free grammars completion loops prediction completion step earley algorithm may imply infinite summation lead infinite loop computed naively 
unit productions give rise cyclic completions 
problem best explained studying example 
consider grammar input string grammar generates cycle prediction earley chart contains states 
scanning completion truncation enter infinite loop 
completed yielding complete state allows completed leading factors result left corner sum 
complete state non probabilistic earley parser just prediction lead truncated probabilities 
sum probabilities needs computed arrive correct result contains infinitely terms possible loop production 
loop adds factor forward inner probabilities 
summations completed states turn approach taken compute exact probabilities cyclic completions analogous left recursive predictions 
main difference unit productions left corners form underlying transitive relation 
proceeding convince case worry 
authors refer unit productions chain productions 
chapter 
efficient parsing stochastic context free grammars lemma completion cycle 
case productions involved unit productions 
proof 
completion chains true start indices states monotonically input index states nonterminals expanded substring state complete expansion started previous increasing position 
follows current position dot refers input current position 
assumption grammar contains nonterminals generate formally define relation nonterminals mediated unit productions analogous left corner relation 
definition definitions relative scfg 
nonterminals said unit production relation production rhs 
probabilistic unit production relation relation nonterminal defined reflexive transitive closure iff iff 
transitive unit sums defined series exists matrix probabilities matrix probability matrix inversion compute relation closed form null productions earley transitions see section 
chapter 
efficient parsing stochastic context free grammars existence shown section 
modified completion loop probabilistic earley parser matrix collapse unit completions single step 
note iterative completion non unit productions 
completion probabilistic transitive example consider grammar non zero unit production 
highly ambiguous grammar generates strings number possible binary parse trees number terminals 
states involved parsing string listed table forward inner probabilities 
example illustrates parser deals left recursion merging alternative sub parses completion 
grammar single nonterminal left corner matrix transitive closure rank consequently example trace shows factor introduced forward probability terms number prediction steps 
sample string parsed total string probability computed parse having probability values final state 
values scanned states sets prefix probabilities respectively null productions null productions introduce complications relatively straightforward parser operation described far due specifically probabilistic aspects parsing 
chapter 
efficient parsing stochastic context free grammars state set predicted state set scanned completed predicted state set scanned completed predicted state set scanned completed table earley chart constructed parse grammar depicted 
columns right list forward inner probabilities respectively state 
columns separates old factors new ones equations 
addition indicates multiple derivations state 
chapter 
efficient parsing stochastic context free grammars section summarizes necessary modifications process null productions correctly previous description baseline 
treatment null productions follows non probabilistic formulation graham 
original earley 
computing expansion probabilities main problem null productions allow multiple prediction completion cycles scanning steps null productions matched input symbols 
strategy collapse predictions completions due chains null productions regular prediction completion steps way recursive predictions completions handled section 
prerequisite approach precompute nonterminals probability expands empty string 
note recursive problem may null production expand nonterminal 
computation abbreviation cast system non linear equations follows 
way example productions semantics context free rules imply expand rhss productions expands translating probabilities obtain equation words production contributes term rule probability multiplied product variables corresponding rhs nonterminals rhs contains terminal case production contributes possibly lead 
resulting non linear system solved iterative approximation 
variable initialized desired level accuracy attained 
convergence guaranteed repeatedly updated substituting equation right hand sides values monotonically increasing bounded true values producing nonterminals procedure degenerates simple backward substitution 
obviously system solved grammar 

grammars cyclic dependencies probability seen precomputed inner probability expansion empty string sums probabilities earley paths derive justification way probabilities modified prediction completion steps described 
chapter 
efficient parsing stochastic context free grammars prediction null productions prediction mediated left corner relation 
occurring right dot production generate states reachable way relation 
extended presence null productions 
specifically reachability criterion left corner iff non zero probabilityof expanding contribution production left corner probability old prediction procedure modified steps 
replace old takes account null productions sketched 
resulting reflexive transitive closure generate predictions 
relation compute second predicting left corner production add states dot positions rhs nonterminal expand say 
call procedure spontaneous dot shifting accounts precisely derivations expand rhs prefix consuming input symbols 
forward inner probabilities states created state multiplied factors account implied expansions 
factor just product dot position 
completion null productions modification completion step follows similar pattern 
unit production re lation extended allow unit production chains due null productions 
rule effectively act unit production links rhs expand 
contribution unit production relation resulting revised matrix compute closure usual 
second modification instance spontaneous dot shifting 
completing state moving dot get additional states added obtained moving dot nonterminals non zero expansion probability 
prediction forward inner probabilities multiplied corresponding expansion probabilities 
eliminating null productions added complications consider simply eliminating productions preprocessing step 
straightforward analogous corresponding procedure non chapter 
efficient parsing stochastic context free grammars probabilistic cfgs aho ullman algorithm 
main difference updating rule probabilities expansion probabilities needed 

delete null productions start symbol case grammar produces non zero probability 
scale remaining production probabilities sum unity 

original rule contains nonterminal create variant rule set rule probability new rule rule exists sum probabilities 
decrement old rule probability amount 
iterate steps occurrences null able nonterminal rhs 
crucial step procedure addition variants original productions simulate null productions deleting corresponding nonterminals rhs 
spontaneous dot shifting described previous sections effectively performs operation fly rules prediction completion 
existence section defined probabilistic left corner unit production matrices respectively collapse recursions prediction completion steps 
shown matrices obtained result matrix inversions 
appendix give proof existence inverses assured grammar defined senses 
terminology taken booth thompson 
definition scfg alphabet start symbol say proper iff nonterminals rule probabilities sum unity consistent iff defines probability distribution finite strings induced rule probabilities definition 
unfortunately terminology literature uniform 
example jelinek lafferty term proper mean defined 
state mistakenly sufficient condition 
booth thompson show write scfg satisfies generates derivations terminate probability give necessary sufficient conditions 
chapter 
efficient parsing stochastic context free grammars useless nonterminals iff nonterminals appear derivation string 
useful translate consistency process oriented terms 
view scfg non zero probability stochastic string step consists simultaneously replacing nonterminals sentential form right hand sides productions randomly drawn rule probabilities 
booth thompson show grammar consistent probability stochastic rewriting symbol start leaves nonterminals remaining steps goes loosely speaking rewriting terminate finite number steps probability grammar inconsistent 
observe property holds nonterminals grammar useless terminals 
nonterminal admitted infinite derivations non zero probability derivations assumption reachable non zero probability 
series converge prove existence sufficient show corresponding geometric lemma proper consistent scfg useless nonterminals powers relation unit production relation converge zero proof 
entry left corner matrix succeeding left corner similarly entry left corner probability generating imme th power left corner intermediate nonterminals 
certainly generating probability probability entire derivation starting terminates steps derivation bounded couldn terminate expanding left symbol terminal opposed nonterminal 
probability tends entry unit production matrix similar argument applies length derivation long takes terminate initial unit production chain 
lemma proper consistent scfg useless nonterminals series defined converge finite non negative values 
proof 
turn implies series non negative elements respectively 
largest eigenvalue spectral radius 
converging implies magnitude converges similarly elements non negative result adding multiplying interestingly scfg may inconsistent converging left corner unit production matrices consistency stronger constraint 
example chapter 
efficient parsing stochastic context free grammars inconsistent choice left corner relation single number case defined 
informally speaking cases left fringe derivation guaranteed result terminal finitely steps remainder derivation nonterminals generated faster rate replaced terminals 
scfg consistency play important role chapter computation various global quantities gram expectation derivation entropy scfg discussed 
complexity issues probabilistic extension earley parser preserves original control structure aspects major exception collapsing cyclic predictions unit completions steps efficient 
apply complexity analysis earley essentially unchanged 
repeat highlights proof outlines 
analyze dependence size grammar compare result known algorithms scfgs 
key factor upper bounding time space complexity earley algorithm maximal number states created state set input position 
earley state combines production dot position start index 
productions dot positions combine give number equals sum lengths productions roughly total size grammar 
fully parameterized cnf grammars number dotted rules number nonterminals 
fully parameterized cnf grammar triple nonterminals forms production input 
non zero probability 
cases start index contributes factor length scaling input length determine complexity terms note prediction scanning state processed exactly performing operations depend size grammar take completion worst case obtained states result completing predecessors dot positions left possible previous positions 
total time taken time entire string 
completion step dominates computation time gives total run earley identifies important classes context free grammars algorithm runs faster special modifications 
grammars bounded ambiguity result completions combine fixed number previous states combining completions correspond coalescing multiple parses substrings 
completion step takes time give total cfgs processed deterministically correct choice rule determined history parser bounded lookahead input lr grammars result chapter 
efficient parsing stochastic context free grammars earley chart contains fixed maximum number states position 
realize benefit deterministic parsing generally needs lookahead feature earley original version discussed 
prediction scanning completion take constant time terms case time complexity linear 
parser line computation prefix probabilities critical know incremental time complexity processing input symbol word 
analysis get worst case incremental time deterministic grammars 
case refers length prefix incremental processing generally slowed input incorporated chart 
created 
space complexity terms bounded ambiguity grammars constant time state sets elements get time space lri jelinek lafferty algorithms advantage better results known grammar classes 
scaling grammar size bounds inside outside baker try give precise characterization case sparse grammars section gives hints implement algorithm efficiently grammars 
fully parameterized grammars cnf verify scaling algorithm terms number nonterminals compare lri algorithms run time mentioned number states position prediction summation forward probabilities equation implemented efficiently follows 
cnf grammar 
compute sum referring nonterminal right dot done single pass current state set time nonterminals 
multiplying vector matrix get vector time 
equation obtained multiplying rule probability element result vector sums indexed vector total time step 
scanning involves shifting dot states represent terminal productions 
completion update probabilities states result summing predecessors 
note cyclic completions cnf grammars 
implement summations efficiently sum probabilities inner states refer lhs nonterminal single matrix inversion compute left corner unit matrices pass 
accomplished time 
cost amortized subsequent uses parser 
space requirements algorithms discussed proportional number parame ters get algorithm 
dependence number nonterminals lri chapter 
efficient parsing stochastic context free grammars summary summarize modified probabilistic earley algorithm works executing steps input position 
apply single prediction step incomplete states current state set 
transitive predictions subsumed consulting left corner matrix forward probabilities computed multiplying old rule probabilities 
inner probabilities initialized respective rule probabilities 
single scanning step applied states terminals right dot yield initial elements state set 
set remains empty scanned states parse aborted 
forward inner probabilities remain unchanged scanning 
sum forward probabilities successfully scanned states gives current prefix probability 
apply iterative completion highest start index breadth states correspond ing unit productions 
unit production cycles subsumed consulting matrix forward inner probabilities updated multiplying old forward inner probabilities inner probabilities completed expansions 
probabilities nonterminals generate particular substrings input computed sums inner probabilities processing entire string way sentence probability read final state 
extensions section discusses extensions earley algorithm go simple parsing computation prefix string probabilities 
extension quite straightforward supported original earley chart structure leads view part single unified algorithm solving tasks mentioned 
viterbi parses definition viterbi parse string grammar left derivation assigns maximal probability possible derivations definition viterbi parse computation straightforward generalizations corresponding notion hidden markov models rabiner juang computes viterbi chapter 
efficient parsing stochastic context free grammars path state sequence hmm 
precisely approach earley parser fact derivation corresponds path 
path probabilities recursively multiplied completion steps inner probabilities accumulators 
summation probabilities occurs alternative sub parses lead single state 
computation viterbi parse parts forward pass state keep track maximal path probability leading predecessor states associated maximum probability path 
final state reached maximum probability parse recovered tracing back path best predecessor states 
modifications probabilistic earley parser implement forward phase viterbi computation 
state computes additional probability viterbi probability 
propagated way inner probabilities summation replaced maximization maximum products contribute state completed position predecessor associated maximum recorded viterbi path predecessor predecessor state inferred 
completion step uses original recursion collapsing unit production loops 
loops simply avoided lower path probability 
collapsing unit production completions avoided maintain continuous chain predecessors backtracing parse construction 
final state reached recursive procedure recover parse tree associated prediction step need modified viterbi computation 
viterbi parse 
procedure takes earley state input produces viterbi parse substring output 
input state complete result partial parse tree children missing root node 
viterbi parse 
return parse tree root labeled children 

ends terminal call procedure recursively obtain parse tree viterbi parse adjoin leaf node labeled right child root return 
chapter 
efficient parsing stochastic context free grammars 
ends nonterminal 
find viterbi predecessor state current state 
call procedure recursively compute viterbi parse adjoin right child root return 
rule probability estimation viterbi parse rule probabilities scfg iteratively estimated em expectation maximization algorithm dempster 
estimation procedure finds set parameters represent local maximum grammar likelihood function product string probabilities sample corpus samples assumed distributed identically independently 
steps algorithm briefly characterized follows 
step compute expectations grammar rule corpus grammar parameters rule probabilities 
grammar likelihood current step reset parameters maximize likelihood relative expected rule counts step 
procedure iterated parameter values likelihood converge 
shown round algorithm produces likelihood high previous em algorithm guaranteed find local maximum likelihood function 
em generalization known baum welch algorithm hmm estimation baum original formulation case scfgs due baker 
scfgs step involves computing expected number times production applied generating training corpus 
step consists simple normalization counts yield new production probabilities 
section examine computation production count expectations required step 
crucial notion introduced baker purpose outer probability nonterminal joint probability nonterminal generated prefix suffix terminals 
essentially method earley framework extending definition outer probabilities apply arbitrary earley states 
alternatively maximum posterior estimate may generated combining expected rule counts dirichlet prior production probabilities discussed section 
chapter 
efficient parsing stochastic context free grammars definition string probabilities paths start initial state generate prefix pass generate suffix final state 
outer probability earley state sum starting state outer probabilities complement inner probabilities refer precisely parts complete paths generating covered corresponding inner probability potentially confusing aspect definition choice production part outer probability associated state fact definition part rhs states sharing identical yields prefix suffix state passing position independent 
case forward expectation probabilities number probability earley parser operating string generator intuitively states set unit production cycles lead paths state fitting description 
ignore technicality terminology 
term motivated fact reduces outer probability defined baker dot final position 
computing expected production counts going details computing outer probabilities briefly describe obtaining expected rule counts needed step grammar estimation 
string alternatively complete earley path generating production path 
derivation denote expected number uses production expected number times prediction derives number occurrences predicted states derives summation predicted states 
sum probabilities paths passing 
inner outer probabilities defined chapter 
efficient parsing stochastic context free grammars expected count obtained precisely product corresponding expected usage count rule computed sum computed completing forward backward passes backward pass scanning chart predicted states 
computing outer probabilities outer probabilities computed tracing complete paths final state start state single backward pass earley chart 
completion scanning steps need traced back 
reverse scanning leaves outer probabilities unchanged similar inner forward probabilities forward pass operation concern reverse completion 
describe reverse transitions notation forward counterparts state written outer inner probabilities 
reverse completion pairs states chart 
inner probability 
rationale 
relative missing probability expanding filled probability surrounding probability surrounding plus choice rule production expansion partial lhs note computation inner probabilities computed forward pass 
particular way defined turns convenient production probabilities needs computation 
forward pass simple reverse completion terminate presence cyclic unit productions 
version collapses chains productions 
reverse completion transitive chapter 
efficient parsing stochastic context free grammars pairs states non zero 
chart unit summation carried state second summation applied choice unit production 
rationale 
equivalent times accounting infinity increments surroundings occur derived cyclic productions 
note computation unchanged includes infinity cyclically generated subtrees appropriate 
parsing bracketed inputs estimation procedure described em estimators general guaranteed find locally optimal parameter estimates 
unfortunately case unconstrained scfg estimation local maxima real problem success dependent chance initial conditions lari young 
pereira schabes showed partially bracketed input samples alleviate problem certain cases 
bracketing information constrains parse inputs parameter estimates steering clear suboptimal solutions 
second advantage bracketed inputs potentially allow efficient processing space potential derivations equivalently earley paths reduced 
interesting see parser incorporate partial bracketing information 
typically big problem case earley algorithm particularly simple elegant solution 
consider grammar partially bracketed input grammar parentheses indicate phrase boundaries candidate parse consistent parse constituent spanning second third fourth 
supplied bracketing nested course need complete bracketing potentially ways parsing substring 
earley parser deal efficiently partial bracketing information follows 
partially bracketed input processed usual left right 
bracketed portion encountered parser invokes recursively substring delimited pair parentheses encountered 
precisely recursive parser instance gets see substring input 
chapter 
efficient parsing stochastic context free grammars chart disjoint parent instance 
states parent chart explicitly passed see 
conversely finished parent access states returned explicitly child instance 
restriction prevents parsing constituents cross left phrase boundary second restriction prevents violation right phrase boundary 
chart child initialized incomplete states parent state set start position substring 
child returns parent complete states state set 
parent adds returned states state set position immediately substring input completion procedure 
recursive parser invocation completion step replaces usual prediction scanning completion cycle entire bracketed substring 
child returns parent continues processing regular input symbols bracketed substrings 
needless say child parser instance may call recursive instances deal nested bracketings 
recursion scheme efficient explicitly rejects parse inconsistent bracketing 
considers parses consistent bracketing continuing top information standard earley parser 
processing bracketed strings requires modification computation probabilities 
proba bilities passed parent child part states processed 
recursive control structure simply constrains set earley paths considered parser affecting probabil ities indirectly 
example ambiguous strings may lower inner probabilities derivations inconsistent bracketing 
forward pass directly affected bracketing 
viterbi procedure sec tion reverse completion pass section examine states chart 
automatically constrained bracketing 
complexity assess complexity benefit bracketing extend analysis section making recursive structure algorithm 
standard parsing scheme time complexity input length recursive scheme bracketed substring takes time number constituents substring may input symbols nested constituents 
total number bracketings input string upper bound total time preclude shared chart implementation level course long protocol adhered 
chapter 
efficient parsing stochastic context free grammars fully bracketed input string grammar rule derivation reflected corre sponding bracketing 
bounded maximal production length grammar time complexity simply robust parsing applications ungrammatical input dealt way 
traditionally seen drawback top parsing algorithms earley sacrifice robustness ability find partial parses ungrammatical input efficiency gained top prediction magerman weir 
approach problem build robustness grammar 
simplest case add top level productions expand nonterminal including unknown word category 
grammar cause earley parser find partial parses substrings effectively behaving bottom parser constructing chart left right fashion 
refined variations possible top level productions model phrasal categories sentence fragments follow 
probabilistic information pruning version earley parser section effect compromise robust expectation driven parsing 
alternative method making earley parsing robust modify parser accept arbitrary input find chosen subset possible substring parses 
simple extension earley algorithm probabilistic 
probabilistic version produce likelihoods partial parses 
potential advantage grammar modifying approach modified various criteria partial parses allow runtime 
extension robust parsing require changes way earley parser operates chart chart seeded extra states starting 
computation performed result modification essentially equivalent cyk bottom parser advantage single parsing engine standard robust parsing 
seeding chart standard earley parsing parser expects find exactly instance nonterminal generating entire input 
expectation reflected fact chart initialized dummy start state chapter 
efficient parsing stochastic context free grammars robust parsing want identify nonterminals possibly generate substring input 
accomplished placing dummy states positions nonterminals earley chart prior start normal operation 
practice dummy states need added nonterminals expansion start current input symbol 
technique discussed section immediate effect extra states predictions generated completions follow finishing processing th state set chart contain states indicating nonterminal generates substring 

table illustrates robust parsing process example grammar table probabilities extra states handled follows 
initial dummy states initialized forward probability zero 
ensure forward probabilities extra states remain zero don interfere computation prefix probabilities regular earley states 
inner probabilities dummy states initialized unity just start state processed usual way 
inner probabilities substring nonterminal pair read complete dummy states 
viterbi probabilities viterbi back pointers processed unchanged 
applying viterbi parse procedure section complete dummy states yields viterbi parses substring nonterminal pairs 
assembling partial parses consulting chart individual substring nonterminal pairs may useful obtain list complete partial parses input 
complete partial parse sequence nonterminals generate input 
example grammar table input circle touches square complete partial parses np vt pp det vt np 
input grammatical exactly complete partial parses 
note may exist complete partial parse input contains unknown symbols 
preprocessing step line parsing may create new account new input symbols 
earley algorithm minimally extended generate list partial parses 
needed device assembles abutting nonterminals partial parses left right 
carried product normal completion process concept variable state 
variable state special kind dummy state rhs number nonterminals chapter 
efficient parsing stochastic context free grammars state set predicted 
np vp np det state set scanned 
det completed 
det max det np det predicted 
state set scanned circle 
circle completed 
np max np det circle np vp predicted 
vp vi pp vp vt np state set scanned touches 
vt touches completed 
vt max vt touches vp vt np predicted 
pp np state set state set state set state set 
det 
np 
det 
np vt 
det vt 
state set scanned 
completed 
max pp np predicted 
np vp np det state set scanned 
det completed 
det max det np det predicted 
state set scanned square 
square completed 
pp np max pp np np det square np vp state set state set state set np vt 
det vt 
table robust parsing simple grammar table 
np vt det 
det vt det 
np vt pp 
det vt pp 
np vt np 
det vt np 
np vt det 
det vt det 
state sets generated parsing ungrammatical string circle touches square 
dummy states empty lhs represent partial parses 
states representing maximal partial parses marked 
predictions don lead completions omitted save space 
trace variable state completions resulting list complete partial parses input 
chapter 
efficient parsing stochastic context free grammars left dot variable right dot written question mark usual state means nonterminals generated substring 
variable indicates continuation nonterminal sequence allowed 
variable semantics taken account prediction completion 
variable state generates predictions nonterminals having effect nonterminal specific dummy states previous section 
completion variable state combines complete states yield new variable states nonterminal 
complete inserted dot variable retained dot allow completions 
implemented trivial modification standard completion step 
inner probabilities viterbi probabilities viterbi back pointers processed usual 
net effect processing variable states complete partial parses read final state set chart right hand sides variable states discarding variable 
inner probability variable state reflects combined likelihood partial parse input 
viterbi probability variable state joint maximum achieved parses substrings 
different derivations complete partial parse may split input string different places 
viterbi parse procedure applied variable state recover split 
table shows trace variable state completions enumerating partial parses example earlier 
total number complete partial parses exponential length input 
may desirable compute subset applying application specific filter criterion 
criterion interested maximal complete partial parses 
complete partial parse called maximal subsequence nonterminals replaced nonterminal yield complete partial parse 
example case circle touches square maximal parse np vt pp 
turns filter maximal partial parses easy implement earley framework 
maximal parses contain nonterminals part larger partial parse 
completion mark states contributed larger constituent identify unmarked states ones corresponding maximal parses 
chart table maximal states labeled 
completing variable states simply skip completions due non maximal states 
list complete partial parses obtained chart contain precisely maximal ones 
chapter 
efficient parsing stochastic context free grammars implementation issues section briefly discusses experience gained implementing probabilistic earley parser 
implementation mainly straightforward standard techniques context free grammars graham 
aspects unique due addition probabilities 
prediction due collapsing transitive predictions step implemented efficient straightforward manner 
explained section perform single pass current state set nonterminals identifying occurring right dots add states corresponding productions reachable left corner 
indicated equation contributions forward probabilities new states summed paths lead relation state 
summation equation eliminated values old states nonterminal summed multiplied quantities summed nonterminals result multiplied rule probability give forward probability predicted state 
completion prediction completion step involves iteration 
complete state derived completion potentially feed completions 
important detail ensure contributions state summed proceeding state input completion steps 
approach problem insert complete states prioritized queue 
queue orders states start indices highest 
states corresponding expansion completed lead completion earlier expansions 
start index entries managed queue ensuring directed dependency graph formed states traversed breadth order 
completion pass implemented follows 
initially complete states previous scanning step inserted queue 
states removed front queue complete states 
new states produced complete ones added queue 
process iterates states remain queue 
computation probabilities includes chains unit productions states derived productions need queued ensures iteration terminates 
similar queuing scheme start index order reversed reverse completion step needed computation outer probabilities section 
chapter 
efficient parsing stochastic context free grammars efficient parsing large sparse grammars moderate sized application specific natural language grammar taken system jurafsky opportunity optimize implementation algorithm 
relate lessons learned process 
speeding matrix inversions matrix prediction completion steps matrix defined geometric series derived indexed nonterminals grammar 
matrix probabilities left corner relation unit production relation 
derived scfg rules application fixed grammar time taken precomputation left corner unit may crucial occurs line 
cases cost minimal rule probabilities iteratively reestimated 
matrix sparse matrix inversion prohibitive large numbers nonterminals empirically matrices rank bounded number non zero entries row independent inverted time full matrix size require time cases grammar relatively small number nonterminals productions involving nonterminals left corner rhs unit production 
nonterminals non zero contributions higher powers matrix reduce cost matrix inversion needed compute fact substantially subset entries elements indexed nonterminals non empty row example left corner computation obtained deleting rows columns indexed nonterminals productions starting nonterminals 
identity matrix set nonterminals computed inverse denotes matrix multiplication left operand augmented zero elements match dimensions right operand speedups obtained technique substantial 
grammar nonterminals nonterminal productions left corner matrix computed seconds including final multiply addition 
inversion full matrix took minutes seconds 
figures meaningful absolute values 
measurements obtained sun sparcstation chapter 
efficient parsing stochastic context free grammars efficient prediction discussed section worst case run time fully parameterized cnf grammars dominated completion step 
necessarily true sparse grammars 
experiments showed computation dominated generation earley states prediction steps 
worthwhile minimize total number predicted states generated parser 
predicted states affect derivation lead subsequent scanning input symbol constrain relevant predictions 
compute extended left corner relation indicating terminals appear left corners nonterminals 
boolean matrix rows indexed nonterminals columns indexed terminals 
computed product non zero entry iff production nonterminal starts terminals 
old left corner relation 
prediction step ignore incoming states rhs nonterminal dot current input left corner eliminate remaining predictions lhs produce current input left corner 
filtering steps fast involve table lookup 
test corpus technique cut number generated predictions speeded parsing factor 
corpus consisted sentence average length words 
top prediction generated states parsed rate milliseconds sentence 
bottom filtered prediction states generated resulting milliseconds sentence 
trivial optimization earley parsers precompute entire prediction step doesn depend input may eliminate substantial portion total predictions sentence 
bottom filtering technique lost edge scanning precomputed predicted states turned slower computing zeroth state set filtered input 
discussion relation finite state models exposition earley algorithm probabilistic extension concepts terminology algorithms probabilistic finite state models particular hidden markov models rabiner juang 
concepts carry suitably generalized notably forward probabilities 
prefix probabilities computed forward probabilities earley parser just hmms earley states summarize past history way states finite state model 
important differences 
number states hmm clos implementation generic sparse matrices particularly optimized task 
prediction step accounted roughly predictions test corpus 
chapter 
efficient parsing stochastic context free grammars remains fixed number possible earley states grows linearly length input due start index 
incidentally hmm concept backward probabilities useful analog earley parsing 
tempting define conditional probability generator produces remaining string currently state alas ill defined quantity generation suffix depends completion just current state 
solution baker adopted modified form outer probabilities backward probabilities 
outer probabilities follow hierarchical structure derivation sequential structure imposed left right processing 
fortunately outer probability computation just supported earley chart forward inner probabilities 
online pruning finite state parsing especially speech decoding forward probabilities pruning partial parses having seen entire input 
pruning formally straightforward earley parsers state set rank states values remove states small probabilities compared current best candidate simply rank exceed limit 
notice omit certain parses underestimate forward inner probabilities derivations remain 
pruning procedures evaluated empirically invariably sacrifice completeness case viterbi algorithm optimality result 
earley line pruning awaits study reason believe earley framework inherent advantages strategies bottom information including called top parsers 
context free forward probabilities include available probabilistic information subject assumptions implicit scfg formalism available input prefix usual inside probabilities take account nonterminal prior probabilities result top relation start state 
top constraints necessarily mean sacrificing robustness discussed section 
contrary earley style parsing set carefully designed estimated fault tolerant top level productions possible probabilities better advantage robust parsing 
approach subject ongoing tight coupling framework system jurafsky see 
relation probabilistic lr parsing major alternative context free parsing paradigms earley lr parsing aho ullman 
comparison approaches probabilistic non probabilistic aspects interesting provides useful insights 
remarks assume familiarity approaches 
sketch fundamental relations important tradeoffs closest thing hmm backward probability probably suffix probability chapter 
efficient parsing stochastic context free grammars frameworks 
earley parser lr parsing uses dotted productions items keep track progress derivations input processed 
start indices part lr items may term item refer lr items earley states start indices 
earley parser constructs sets possible items fly possible partial derivations 
lr parser hand access complete list sets possible items computed runtime simply follows transitions sets 
item sets known states lr parser 
grammar suitable lr parsing transitions performed deterministically considering input contents shift reduce stack 
generalized lr parsing extension allows parallel tracking multiple state transitions stack actions graph structured stack tomita 
probabilistic lr parsing wright lr items augmented certain conditional probabilities 
specifically probability associated lr item terminology normalized forward probability denominator probability current prefix 
lr item probabilities conditioned forward probabilities compute conditional probabilities words sum items having right dot extra required item corresponds reduce state dot final position 
notice definition independent start index corresponding earley state 
ensure item probabilities correct independent input position item sets constructed probabilities unique set 
may impossible probabilities take infinitely values general depend history parse 
solution wright collapse items probabilities small tolerance identical 
threshold simplify number technical problems left corner probabilities computed iterated prediction resulting changes probabilities smaller subject approximations probabilistic lr parser compute prefix probabilities multiplying successive conditional probabilities words sees 
alternative computation lr transition probabilities scfg estimate probabilities directly traces parses training corpus 
due imprecise relationship lr probabilities scfg probabilities entirely clear model estimated corresponds particular scfg usual sense 
briscoe carroll turn earley parsers lr parsers built various amounts lookahead operation parser deterministic efficient 
case zero lookahead lr considered correspondence lr parsers lookahead earley parsers discussed literature earley aho ullman 
helpful compare closely related finite state concept states lr parser correspond sets earley states similar way states deterministic fsa correspond sets states equivalent non deterministic fsa standard subset construction 
identity expression item probabilities wright proved induction steps performed compute shown appendix 
unfortunately wright presents computation giving precise definition numbers probabilities 
clear numerical properties approximation errors accumulate longer parses 
chapter 
efficient parsing stochastic context free grammars incongruity advantage lr parser probabilistic model right show lr probabilities extended capture non context free contingencies 
problem capturing complex distributional constraints natural language clearly important scope chapter 
simply possible define interesting non standard probabilities terms earley parser actions better model non context free phenomena 
apart considerations choice lr methods earley parsing typical space time tradeoff 
earley parser runs linear time space complexity lr parser grammars appropriate lr class constant factors involved favor lr parser compiled transition action table 
size lr parser tables exponential size grammar due number potential item subsets 
furthermore generalized lr method dealing non deterministic grammars tomita runtime arbitrary inputs may grow exponentially 
bottom line application needs evaluated pros cons approaches find best solution 
theoretical point view earley approach inherent appeal general exact solution computation various scfg probabilities 
related literature earley probabilistic parsers sparse presumably precedent set inside outside algorithm naturally formulated bottom algorithm 
schabes shows earley algorithm augmented computation inner outer probabilities way 
algorithm fully general restricted sentences bounded ambiguity provisions handling unit production cycles 
schabes focuses grammar estimation generalized inside outside probabilities mentions potential computing prefix probabilities 
magerman marcus interested primarily scoring functions guide parser efficiently promising parses 
earley style top prediction suggest worthwhile parses compute precise probabilities 
nakagawa non probabilistic earley parser augmented word match scoring 
truly probabilistic algorithms similar viterbi version described find parse optimizes accumulated matching scores regard rule probabilities 
prediction completion loops come play precise inner forward probabilities computed 
dan jurafsky personal communication written earley parser berkeley restaurant project speech understanding system computed forward probabilities restricted grammars left corner unit production recursion 
parser uses methods described provide argue precise probabilities inappropriate natural language parsing 
chapter 
efficient parsing stochastic context free grammars exact scfg prefix word probabilities tightly coupled speech decoder jurafsky 
essential idea probabilistic formulation earley algorithm collapsing recursive predictions unit completion chains replacing lookups precomputed matrices 
idea arises formulation need compute probability sums infinite series 
graham 
non probabilistic version technique create highly optimized earley parser general cfgs implements prediction completion operations boolean matrices 
matrix inversion method dealing left recursive prediction borrowed lri algorithm jelinek lafferty computing prefix probabilities scfgs cnf 
idea second time deal similar recursion arising unit productions completion step 
suspect proved earley computation forward probabilities applied cnf grammar performs computation sense isomorphic lri algorithm 
case believe parser oriented view afforded earley framework intuitive solution prefix probability problem added advantage restricted cnf grammars 
kupiec proposed version inside outside algorithm allows operate non cnf grammars 
interestingly kupiec algorithm generalization finite state models recursive transition networks 
probabilistic essentially hmms allow nonterminals output symbols 
dotted productions appearing earley states exactly equivalent states rtn derived cfg 
simple typology scfg algorithms various known algorithms probabilistic cfgs share similarities vary similar dimensions 
dimension quantities entered parser chart defined bottom cyk fashion left right constraints inherent part definition 
point variation sparseness trade 
set nonterminals wanted list possible cfg rules nonterminals list infinite due arbitrary length right hand sides productions 
problem example training cfg starting complete ignorance structure rules 
workaround restrict rule format usually cnf list possible productions 
algorithms assume cnf usually formulated terms fully parameterized grammar triples form possible rule cases may specialized handle sparse grammars efficiently 
non zero probability extreme algorithms accept unrestricted cfg meant sparse grammars set theoretic sense possible productions probability connection ghr algorithm pointed fernando pereira 
exploration link lead extension algorithm handle productions described section 
method uses transitive reflexive closure left corner relation chose symbol 
chose symbol chapter point difference 
course cyk style parser operate left right right left reordering computation chart entries 
chapter 
efficient parsing stochastic context free grammars full cnf sparse cfg bottom inside outside stochastic baker kupiec left right lri probabilistic jelinek lafferty earley table tentative typology scfg algorithms prevailing directionality sparseness cfg 
zero 
appears algorithms tend naturally formulated terms stochastic process opposed static specifications string probabilities 
illustrate points algorithms discussed section arranged grid depicted table 
summary earley parser stochastic context free grammars appealing combination advantages existing methods 
earley control structure run best known complexity number special grammar classes worse standard bottom probabilistic chart parsers fully parameterized scfgs 
bottom parsers computes accurate prefix probabilities incrementally scanning input usual substring inside probabilities 
chart constructed parsing supports viterbi parse extraction baum welch type rule probability estimation way backward pass parser chart 
input comes partial bracketing indicate phrase structure information easily incorporated restrict allowable parses 
simple extension earley chart allows finding partial parses ungrammatical input 
computation probabilities conceptually simple follows directly earley parsing framework drawing heavily analogy finite state language models 
require rewriting grammar normal form 
algorithm fills gap existing array algorithms scfgs efficiently combining functionalities advantages previous approaches 
appendix lr item probabilities conditional forward ties section interpretation lr item probabilities defined wright section terms forward probabilities earley parser 
give proof correctness interpretation 
notice ideal lr probabilities attached items weren identification items close probabilities keep lr state list finite 
chapter 
efficient parsing stochastic context free grammars show probability lr item dot rhs 
want item regardless position start index 
note equal position input symbol processed reduce action parser effectively resets reduced nonterminal 
computation lr item sets begins initial item agreeing 
definition operation constructing item sets closure item items corresponding available productions added set 
operation recursive corresponds obviously earley prediction step 
way values propagated follows exactly way forward probabilities handled prediction 
left corner relation compute closure probabilities exactly wright suggests truncated recursion 
closure prediction isomorphic prefix relative items change remains valid step 
successor set kernel items constructed existing closed set corresponds earley scanning completion 
specifically current item item placed reachable scanning terminal reducing completing nonterminal 
stand terminal nonterminal treat cases jointly 
new item probability computed understood scaling total probability items matching unity 
substituting get chapter 
efficient parsing stochastic context free grammars position current input 
abbreviation sum inner probabilities pertaining completed terminal nonterminal 
steps derivation need justification 
computing forward probabilities just earley completion step see equation 
get observe set contains possible kernel items having processed prefix definition lr parsing method 
sum represents possible partial derivations generating prefix chapter grams stochastic context free grammars chapter introduced gram models simplest popular types probabilistic grammars 
particularly special cases bigram trigram models proven extremely useful tasks automated speech recognition part speech tagging word sense disambiguation 
obvious linguistic deficiencies cause number practical difficulties applications 
lack linguistically motivated structure entails large number freely adjustable parameters illustration structure parameter tradeoff discussed section 
result large corpora needed reliable estimation gram models requiring additional sophisticated smoothing techniques avoid known problems maximum likelihood estimators church gale 
lack linguistic motivation gram practically incomprehensible humans impossible extend maintain brute force reestimation 
stochastic context free grammars scfgs problems degree complementary grams 
scfgs fewer parameters reasonably trained smaller corpora capture linguistic generalizations easily understood written extended linguists 
example new word number known possible syntactic categories added usually straightforward adding lexical productions scfg 
structure grammar entails occurrence statistics words 
chapter describes technique computing gram grammar existing scfg attempt get best worlds 
section illustrate basic problem solved provide number motivations solution 
section describes mathematics underlying computation section addresses questions complexity efficient implementation 
section discusses issue gram distribution derived scfg defined 
chapter 
grams stochastic context free grammars section reports experiment demonstrating applications gram scfg computation practical feasibility 
section summarizes algorithm general points 
section briefly touch miscellaneous problems solutions close formal ties gram algorithm 
background motivation defined section gram grammar set probabilities giving probability follows word string possible combination vocabulary language 
word vocabulary bigram grammar approximately free parameters trigram grammar 
example smaller scale illustrates problem 
consider simple scfg rule probabilities brackets book close open language generated grammar contains words 
including markers sentence ning bigram grammar contain probabilities free parameters probabilities sum 
trigram grammar come parameters 
scfg probabilities free parameters 
standard deviation maximum likelihood estimator multinomial parameters equation samples 
expect estimates scfg parameters roughly times reliable bigram model 
number reason discrepancy course structure scfg discrete hyper parameter considerable potential variation fixed 
point structure comprehensible humans cases constrained prior knowledge reducing estimation problem remaining probabilities 
problem estimating scfg parameters data solved standard techniques usually likelihood maximization em simplifying assumption bigrams scfg productions exercised equally 
clearly true source systematic error productions higher scfg get closer terminals 
chapter 
grams stochastic context free grammars algorithm cf 
sections 
absence human expertise grammar induction methods chapter chapter may 
arguments scfgs principle adequate probabilistic models natural languages due conditional independence assumptions embody magerman marcus jones eisner briscoe carroll 
main criticisms production probabilities independent expansion context noun phrase realized subject object position lexical occurrences lexical syntactical contingencies easily represented resulting poor probabilistic estimates phenomena 
shortcomings partly remedied scfgs specific semantically oriented categories rules jurafsky 
goal grams computation constrained scfg useful results interpolated raw gram estimates smoothing 
experiment approach reported 
hand sophisticated language models give better results grams important applications speech recognition 
standard speech decoding technique frame synchronous dynamic programming ney order markov assumption satisfied bigrams models hidden markov models complex models incorporating non local higher order constraints including scfgs 
standard approach simple language models generate preliminary set candidate hypotheses 
hypotheses represented word lattices best lists schwartz chow re evaluated additional criteria afford costly due constrained outcomes 
type setting techniques developed compile probabilistic knowledge encoded elaborate language models gram estimates improve quality hypotheses generated decoder 
comparing directly estimated reliable grams compiled language models potentially useful method evaluating models question 
purpose chapter assume computing grams scfgs practical theoretical interest concentrate computational aspects problem 
noted alternative unrelated methods addressing problem large parameter space gram models 
example brown 
describe approach grouping words classes reducing number conditional probabilities model 
dagan 
explore similarities words interpolate bigram estimates involving words similar syntagmatic distributions 
technique compiling higher level grammatical models lower level entirely new zue 
report building word pair grammar elaborate language models achieve coverage random generation sentences 
essentially propose solution extending approach probabilistic realm 
need obtaining gram estimates scfgs originated speech understanding system mentioned thesis jurafsky 
chapter describes gram algorithm specifically scfgs 
methods described easily adapted simpler hmm case 
chapter 
grams stochastic context free grammars previous solution problem estimate gram probabilities scfg counting randomly generated artificial samples 
algorithm normal form scfgs parts thesis get need normalize grammar chomsky normal form cnf 
cfg cnf productions form cfg structure converted weakly equivalent cnf grammar hopcroft ullman case scfgs probabilities assigned string probabilities remain unchanged 
furthermore parses original grammar reconstructed corresponding cnf parses 
short loss generality assume scfgs question cnf 
algorithm described fact generalizes general canonical form graham format case bigrams modified directly arbitrary scfgs 
cnf form convenient keep exposition simple assume scfgs cnf 
probabilities expectations key insight solution gram probabilities obtained associated expected frequencies grams grams stands expected count occurrences substring sentence proof 
write expectation grams recursively terms order conditional gram probabilities compute immediately gram grammar language generated 
substrings lengths scfg preservation string probabilities trivial grammar null unit productions 
cases algorithm similar section update probabilities 
counts appearing expectations special notation distinction observed expected values 
chapter 
grams stochastic context free grammars ways generating substring nonterminal string notation extending notation previous chapters suffix means generates prefix 
probabilities associated events 
computing expectations denotes non terminal generates goal compute substring expectations grammar 
formalisms scfgs recursive rule structure suggest divide conquer algorithm follows recursive structure grammar 
generalize problem considering occurrences solution sought start symbol grammar 
strings generated arbitrary nonterminal special case expected number possibly overlapping consider possible ways nonterminal generate string substring denoted associated probabilities 
production distinguish main cases assuming grammar cnf 
string question length happens production production adds exactly expectation non terminal productions say generated recursive expansion right hand side 
production subcases 

generate complete see 
likewise generate 
generate suffix expectation cases expectation generating resulting single occurrence 
prefix substring total sum partial expectations 
total expectations chapter 
grams stochastic context free grammars cases substring completely generated recursively respectively 
expectation third case sum possible split points string compute total expectation weighted rule probabilities nonterminal rule cases 
gives sum choices production important special case bigrams summation simplifies quite bit terminal productions ruled splitting prefix suffix allows possibility unigrams equation simplifies recursive specification quantities recursion necessarily bottom quantities right side equation may depend fortunately recurrence linear string find solution solving linear system formed equations type 
notice need compute 
alas exactly equations variables equal number nonterminals grammar 
solution systems discussed 
computing prefix suffix probabilities substantial problem left point computation constants equation 
derived rule probabilities generation probabilities prefix suffix chapter 
grams stochastic context free grammars computation prefix probabilities scfgs generally useful applications solved lri algorithm jelinek lafferty 
chapter seen computation carried efficiently sparsely parameterized scfgs probabilistic version earley parser 
computing suffix probabilities obviously symmetrical task example create mirrored scfg reversing order right hand side symbols productions run prefix probability computation mirror grammar 
note case bigrams particularly simple form prefix suffix probabilities required left corner right corner probabilities obtained single matrix inversion jelinek lafferty corresponding left corner matrix probabilistic earley parser corresponding right corner matrix 
interesting compare relative ease solve substring expectation problem seemingly similar problem finding substring probabilities probability generates instances problem studied 
shown lead non linear system equations 
crucial difference expectations additive respect cases corresponding probabilities cases occur string 
containing string boundaries grams complete gram grammar includes strings delimited special marker denoting string 
section introduced symbol purpose 
generate expectations grams adjoining string boundaries original scfg grammar augmented new top level production old start symbol start symbol augmented grammar 
algorithm simply applied augmented grammar give desired gram probabilities including marker 
efficiency complexity issues summarizing previous section compute gram probability solving linear systems equations form gram gram prefix 
computation shared grams prefix essentially system needs solved gram interested 
news required linear number grams correspondingly limited needs probabilities subset possible grams 
example compute probabilities demand cache results 
chapter 
grams stochastic context free grammars form examine systems equations time 
written matrix notation identity matrix represents vector unknowns get coefficient matrix right hand side vector indexed nonterminals 

expression arises bringing variables side equation order collect coefficients 
see dependencies particular bigram right hand side vector coefficient matrix depends grammar 
standard method lu decomposition see press 
enables solve bigram time standard full system number nonterminals variables 
lu decomposition cubic incurred 
full computation dominated quadratic effort solving system gram 
furthermore quadratic cost worst case incurred grammar contained possible rule empirically computation linear number nonterminals grammars sparse nonterminal bounded number nonterminals independent total grammar size 
consistency scfgs blindly applying gram algorithm scfg arbitrary probabilities lead surprising results 
consider simple grammar see expected frequency unigram abbreviation equation chapter 
grams stochastic context free grammars leads infinity probabilities solution negative 
striking manifestation failure grammar consistent sense booth thompson see section 
inconsistent grammar stochastic derivation process non zero probability terminating 
expected length generated strings infinite case 
booth thompson derive criterion checking consistency scfg find matrix expected number occurrences nonterminal step expansion nonterminal sure powers converge consistent 
grammar grammar matrix confirm earlier observation noting converges iff 
notice identical matrix occurs linear equations gram computation 
actual coefficient matrix inverse exists written geometric sum series converges precisely converges 
shown existence gram problem equivalent consistency grammar question 
furthermore solution vector consist non negative numbers sum product non negative values equations 
matrix inverse turn special role scfg sense universal problem solver series global quantities associated probabilistic grammars 
brief overview appendix chapter 
experiments algorithm described implemented generate bigrams speech recognizer part spoken language system jurafsky 
speech decoder language model components system experiment assess benefit bigram probabilities obtained scfgs versus estimating directly available training corpus 
system domain inquiries restaurants city berkeley 
table gives statistics training test corpora language models involved experiment 
experiments context free grammar hand written domain 
computing bigram probabilities scfg nonterminals involves solving linear systems unigram alternative version criterion check magnitude largest eigenvalues spectral radius 
value grammar inconsistent consistent 
chapter 
grams stochastic context free grammars training corpus test corpus 
sentences 
words bigram vocabulary bigram coverage scfg productions scfg vocabulary scfg coverage table corpora language model statistics 
coverage measured percentage sentences parsed non zero probability language model 
expectations linear systems bigram expectations 
process takes hours sparcstation non optimized lisp implementation 
experiments results described overlap reported jurafsky 

experiment recognizer bigrams estimated directly training corpus smoothing resulting word error rate 
experiment different set bigram probabilities computed context free grammar probabilities previously estimated training corpus standard em techniques 
resulted word error rate 
may surprisingly low coverage underlying cfgs notice conversion bigrams bound result constraining language model effectively increasing coverage 
comparison purposes ran experiment bigrams computed indirectly monte carlo sampling scfg samples 
result slightly worse confirming precise computation inherent advantage omit words constructions scfg assigns low probability 
experiment bigrams generated scfg augmented raw training data proportion 
attempted optimize mixture proportion deleted interpolation jelinek mercer 
bigram estimates obtained word error rate dropped represents statistically significant improvement experiments 
table summarizes figures adds points comparison pure scfg language model mixture model interpolates bigram scfg 
notice case different experiment language model standard bigram albeit obtained mixing counts obtained data scfg 
system referred inefficiency actual number nonterminals rank coefficient matrix grammar converted simple normal form introduced chapter 
proportion comes original system method described bigrams estimated scfg random sampling 
generating sentence samples give converging estimates bigrams 
bigrams raw training sentences simply added randomly generated ones 
verified bigrams estimated scfg identical ones computed directly method described 
chapter 
grams stochastic context free grammars word error bigram estimated raw data bigram computed scfg monte carlo sampling bigram scfg plus data scfg mixture bigram scfg table speech recognition accuracy various language models 
hand standard weighted mixture distinct submodels described section 
experiments support argument earlier sophisticated language models far perfect improve gram estimates obtained directly sample data 
see bulk improvement come scfg smoothing bigram statistics constraints imposed scfg possibly combined mixture language models 
summary described algorithm compute closed form distribution grams probabilistic language stochastic context free grammar 
algorithm computing substring expectations expressed systems linear equations derived grammar 
listed steps complete gram scfg computation 
concreteness give version specific bigrams 

compute prefix left corner suffix right corner probabilities nonterminal word pair 

compute coefficient matrix right hand sides systems linear equations equa tions 

lu decompose coefficient matrix 

compute unigram expectations word grammar solving lu system unigram right hand sides computed step 
compute bigram expectations word pair solving lu system bigram right hand sides computed step 
compute bigram probability unigram expectation dividing bigram expectation chapter 
grams stochastic context free grammars algorithm practical context medium scale speech understanding system gave improved estimates bigram language model hand written scfg small amounts available training data 
deriving gram probabilities sophisticated language models appears generally useful technique improve direct estimation grams allows available higher level linguistic knowledge effectively integrated speech decoding tasks place strong constraints usable language models 
appendix related problems seen gram expectations scfgs obtained solving linear systems matrix identity matrix moment expectation matrix nonterminal occurrences single nonterminal expansion 
turns number apparently unrelated problems arising connection scfgs probabilistic grammars solutions matrix 
briefly surveyed detailed proofs 
expected string length compute expected number terminals string system solved right hand side vector containing average number terminals generated single production nonterminal 
example productions lhs entry indexed right hand side vector solution vector contains expected lengths sublanguages generated 
nonterminals 
expected sentence string length entry solution vector 
problems solution easily generalized obtain expected number terminals particular type occuring string booth thompson 
derivation entropy derivation entropy average number bits required specify derivation scfg 
computed right hand side vector contains average negative log probabilities productions lhs nonterminal 
example chapter 
grams stochastic context free grammars right hand side entry derivation entropy grammar solution vector entry indexed start symbol 
log expected number nonterminal occurrences log consider problem starting nonterminal nonterminals type matrix formed expectations assuming exist 
easy generated 
verify satisfies familiar recurrence get way look outcome expected number nonterminals type represented column vector column identity matrix 
problem type described fact nonterminal expectations general problems type sense 
quantities considered conversely linear system solution defined right hand side vector coefficient matrix represented weighted sums nonterminal expectations weighting provided vector example expected length strings generated nonterminal seen principles weighted sum average number generated expected number terminals produced single derivation step 
written matrix form nb get system section solving derivation entropy gram expectations turn linear weighted sums nonterminal expectations needless say quantities defined underlying scfg consistent 
grammar types significance extends types probabilistic grammars markov branching processes states 
example hmms computations apply nonterminal replaced state appropriate 
moment matrix hmms just transition matrix expresses expected number visits state 

chapter directions final chapter suggest number potential continuations described dissertation 
probabilistic language modeling certainly proven theoretically practically useful approach including model merging paradigm structural learning studied 
rise concrete applications case hmms gives new perspectives linguistic issues tried show mainly discussion scfg pag modeling frameworks 
surprisingly date raises questions provides definitive answers 
apart model specific problems discussed preceding chapters number fundamental issues identified 
invariably lead worthwhile avenues 
formal characterization learning dynamics extremely useful able predict sample complexity target grammar merging learning algorithm number samples required reliably learn target grammar 
step include prediction samples needed target grammar equivalent globally optimal posterior probability 
difficult problem predict limited search procedure specified type able find global optimum 
general problem sample complexity interesting draw distinction sample structure sample distribution 
words sensitive learner sample representative regarding possible strings language exhibits different frequency statistics 
question pointed robert wilensky 
chapter 
directions noisy samples related problem concerns potential presence noise sample samples randomly altered independent process random replacement symbols 
plain model merging algorithms simply try model language resulting composition undistorted language noise process 
probabilities distortions noise small expect recover original language model pruning low probability parts induced model structures 
pruning techniques applied successfully conjunction hmm merging algorithm applied multiple pronunciation models section cases training corpus known contain outlier mislabeled samples reported wooters 
formal tion conditions method successful obviously useful 
informative search heuristics biases far clearly shows expected increased model expressiveness requires varied repertoire search operators turn necessitates increasingly non local search strategies 
carefully chosen heuristics macro operators counter increase search complexity cases 
raises question principled sources search bias 
main theme studies considerable amount linguistic structure model merging solely non informative priors dumb search strategies 
linguistic theories strong priori bases help cases uninformed approaches turn insufficient 
prerequisite approach predictions linguistic theory cast effective search heuristics 
induction model specialization pure model merging operators leave weak generative capacity models unchanged produce strictly inclusive model 
mentioned alternative inverse approach chapter model splitting specialization 
local nature search process expect improvements adding operators effectively undo previous merging steps 
learning algorithm goes general specific incorporates different global bias face local search tend err side general specific model 
having types biases available allows choosing mixing needs particular applications 
example important resulting model high coverage new data prefer general models 
approaches literature state far entirely likelihood criteria 
bayesian evaluation methods thesis clearly separable merging search components approach apply systems 
practical step chapter 
directions unified induction framework study specializing algorithms combined posterior probability maximization advantage 
new applications remarked natural language ideal application domain exclusively mainly syntax oriented probabilistic characterizations due wide range extra syntactic constraints subject 
formal language models starting widely areas computational biology graphical modeling picture grammars document structure analysis 
probabilistic learning approaches especially structural learning await study areas 
natural language remains important topic 
difficulties cited earlier indicate single learning algorithm paradigm hope practical way inducing suitable models large corpora 
natural strategy selective combined partial solutions indicated chapter 
major problem regard apart obvious right partial solutions worth combining find unifying principles allow different approaches talk 
probability theory derived information theoretic concepts description length play central role 
new types probabilistic models ultimately probabilistic approaches language need identify alternatives traditional formalism date 
seen formalism defines underlying notion derivation structure set conditional independence assumptions defining probabilities derivations 
choosing design trade capturing relevant probabilistic contingencies domain computational expense associated algorithms parsing estimation properties learnability model merging complexity robustness added criteria 
model merging natural theoretical framework context applies widely model classes serve basis comparison questions learnability 
bibliography aho alfred ravi sethi jeffrey ullman 

compilers principles techniques tools 
reading mass addison wesley 
jeffrey ullman 

theory parsing translation compiling 
volume parsing 
englewood cliffs prentice hall 
angluin smith 

inductive inference theory methods 
acm computing surveys 
bahl frederick jelinek robert mercer 

maximum likelihood approach continuous speech recognition 
ieee transactions pattern analysis machine intelligence 
baker james 
trainable grammars speech recognition 
speech communication papers th meeting acoustical society america ed 
jared wolf dennis klatt mit cambridge mass baldi pierre yves chauvin tim mcclure 

hidden markov models molecular biology new algorithms applications 
advances neural information processing systems ed 
stephen jos hanson jack cowan lee giles 
san mateo ca morgan kaufmann 
baum leonard ted petrie george soules norman weiss 

maximization technique oc statistical analysis probabilistic functions markov chains 
annals mathematical statistics 
bell timothy john cleary ian witten 

text compression 
englewood cliffs prentice hall 
booth taylor richard thompson 

applying probability measures languages 
ieee transactions computers 
bourlard herv nelson morgan 

connectionist speech recognition 
hybrid approach 
boston mass kluwer academic publishers 
bibliography brill eric 

automatic grammar induction parsing free text transformation approach 
proceedings arpa workshop human language technology briscoe ted john carroll 

generalized probabilistic lr parsing natural language corpora unification grammars 
computational linguistics 
brown peter vincent della pietra peter desouza lai robert mercer 

class gram models natural language 
computational linguistics 
buntine 
theory refinement bayesian networks 
seventh conference uncertainty artificial intelligence anaheim ca 
buntine wray 

learning classification trees 
artificial intelligence frontiers statistics ai statistics iii ed 
hand 
chapman hall 
cheeseman peter james kelly matthew self john stutz taylor don freeman 

autoclass bayesian classification system 
proceedings th international conference machine learning university michigan ann arbor mich chen 
identification contextual factors pronunciation networks 
proceedings ieee conference acoustics speech signal processing volume albuquerque nm 
church kenneth william gale 

comparison enhanced turing deleted estimation methods estimating probabilities english bigrams 
computer speech language 
cleeremans axel 
mechanisms implicit learning 
parallel distributed processing model sequence acquisition 
pittsburgh pa department psychology carnegie mellon university dissertation 
cook craig rosenfeld alan aronson 

grammatical inference hill climbing 
information sciences 
cooper gregory edward herskovits 

bayesian method induction probabilistic networks data 
machine learning 
anna renato de mori roberto giorgio satta 

computation island 
ieee transactions pattern analysis machine intelligence 
cover thomas joy thomas 

elements information theory 
new york john wiley sons cox 
probability frequency reasonable expectation 
american journal physics 
bibliography dagan ido fernando pereira lillian lee 

similarity estimation word rence probabilities 
proceedings th annual meeting association computational linguistics new mexico state university las cruces nm 
de saussure ferdinand 

cours de linguistique 
paris 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
earley jay 

efficient context free parsing algorithm 
communications acm 
evans 
grammatical inference techniques pattern analysis 
software engineering ed 
tou 
new york academic press 
fass 
learning context free languages structured sentences 
acm sigact news 
feldman lakoff bailey narayanan regier stolcke 

years 
ai review 
special issue integration natural language vision processing appear 
feldman jerome george lakoff andreas stolcke susan weber 

miniature language acquisition touchstone cognitive science 
proceedings th annual conference cognitive science society mit cambridge mass fillmore charles 
mechanisms construction grammar 
proceedings fourteenth annual meeting berkeley linguistics society ed 
shelley annie helen berkeley ca 
fujisaki jelinek cocke black 

probabilistic parsing method sentence disambiguation 
current issues parsing technology ed 
masaru tomita chapter 
boston kluwer academic publishers 
garofolo 
getting started darpa timit cd rom acoustic phonetic continuous speech database 
national institute standards technology nist maryland 
gauvain jean luc chin hin lee 

bayesian learning gaussian mixture densities hidden markov models 
proceedings darpa speech natural language processing workshop 
pacific grove ca defence advanced research projects agency information science technology office 
gazdar gerald klein sag 

generalized phrase structure grammar 
cambridge mass harvard university press 
geman stuart elie bienenstock ren doursat 

neural networks bias variance dilemma 
neural computation 
bibliography donald geman 

stochastic relaxation gibbs distributions bayesian restoration images 
ieee transactions pattern analysis machine intelligence 
graham susan michael harrison walter ruzzo 

improved context free recognizer 
acm transactions programming languages systems 
gull 
bayesian inductive inference maximum entropy 
maximum entropy bayesian methods science engineering volume foundations ed 
erickson smith 
dordrecht kluwer 
haussler david anders krogh mian sj lander 

protein modeling hidden markov models analysis globins 
technical report ucsc crl computer information sciences university california santa cruz ca 
revised sept 
hinton geoffrey terrence sejnowski 

learning relearning boltzmann machines 
parallel distributed processing explorations microstructure cognition ed 
david rumelhart james mcclelland volume foundations 
cambridge mass bradford books mit press 
louis 

prolegomena theory language 
baltimore press 
translated francis danish original 
hopcroft john jeffrey ullman 

automata theory languages computation 
reading mass addison wesley 
horning james jay 

study grammatical inference 
technical report cs computer science department stanford university stanford ca 
jelinek frederick 

markov source modeling text generation 
impact processing techniques communications ed 

dordrecht nijhoff 
john lafferty 

computation probabilityof initial substring generation stochastic context free grammars 
computational linguistics 
robert mercer 

basic methods probabilistic context free grammars 
speech recognition understanding 
advances trends applications ed 
pietro renato de mori volume nato advanced sciences institutes series 
berlin springer verlag 
proceedings nato advanced study institute italy july 
robert mercer 

interpolated estimation markov source parameters sparse data 
proceedings workshop pattern recognition practice amsterdam 
jones mark jason eisner 

probabilistic parser applications 
aaai workshop statistically nlp techniques san jose ca 
bibliography 

probabilistic parser applied software testing documents 
proceedings th national conference artificial intelligence san jose ca 
aaai press 
jurafsky daniel chuck wooters gary jonathan segal andreas stolcke eric nelson morgan 

berkeley restaurant project 
proceedings international conference spoken language processing yokohama 
chuck wooters gary jonathan segal andreas stolcke nelson morgan 

integrating experimental models syntax phonology accent dialect speech recognizer 
aaai workshop integration natural language speech processing ed 
paul seattle wa 
kaplan ronald joan bresnan 

lexical functional grammar formal system grammatical representation 
mental representation grammatical relations ed 
joan bresnan 
cambridge mass mit press 
katz slava 
estimation probabilities sparse data language model component speech recognizer 
ieee transactions acoustics speech signal processing 
kupiec julian 

hidden markov estimation unrestricted stochastic context free grammars 
proceedings ieee conference acoustics speech signal processing volume san francisco 


robust part speech tagging hidden markov model 
computer speech language 
langley pat 
simplicity representation change grammar induction 
unpublished mss 
lari young 

estimation stochastic context free grammars inside outside algorithm 
computer speech language 


applications stochastic context free grammars inside outside algorithm 
computer speech language 
lee fu 

stochastic linguistics picture recognition 
technical report tr ee school engineering purdue university 
magerman david mitchell marcus 

pearl probabilistic chart parser 
proceedings th conference european chapter association computational linguistics berlin germany 
carl weir 

efficiency robustness accuracy chart parsing 
proceedings th annual meeting association computational linguistics university delaware newark delaware 
bibliography mitchell tom 
need biases learning generalizations 
technical report cbm tr computer science department rutgers university new brunswick 
generalization search 
artificial intelligence 
morgan james richard meier newport 

structural packaging input language learning contributions prosodic morphological marking phrases acquisition language 
cognitive psychology 
nakagawa sei ichi 

spoken sentence recognition time synchronous parsing algorithm context free grammar 
proceedings ieee conference acoustics speech signal processing volume dallas texas 
neal radford 
probabilistic inference markov chain monte carlo methods 
technical report crg tr department computer science university toronto 
ney hermann 

stage dynamic programming algorithm connected word recognition 
ieee transactions acoustics speech signal processing 


stochastic grammars pattern recognition 
speech recognition understanding 
advances trends applications ed 
pietro renato de mori volume nato advanced sciences institutes series 
berlin springer verlag 
proceedings nato advanced study institute italy july 
omohundro stephen 
best model merging dynamic learning recognition 
technical report tr international computer science institute berkeley ca 


modification earley algorithm speech recognition 
advances speech understanding dialog systems ed 
niemann lang volume nato advanced sciences institutes series 
berlin springer verlag 
proceedings nato advanced study institute bad germany july 
pearl judea 

probabilistic reasoning intelligent systems networks plausible inference 
san mateo ca morgan kaufman 
pereira fernando yves schabes 

inside outside reestimation partially bracketed corpora 
proceedings th annual meeting association computational linguistics university delaware newark delaware 
sara jerome feldman 

learning automata ordered examples 
machine learning 
press william brian flannery saul teukolsky william vetterling 

numerical recipes art scientific computing 
cambridge cambridge university press 
bibliography quinlan ross ronald rivest 

inferring decision trees minimum description length principle 
information computation 
rabiner juang 

hidden markov models 
ieee assp magazine 

implicit learning artifical grammars 
journal verbal learning verbal behavior 
redner richard homer walker 

mixture densities maximum likelihood em algorithm 
siam review 
regier terry 
acquisition lexical semantics spatial terms connectionist model ceptual categorization 
berkeley ca computer science division university california dissertation 
resnik philip 

probabilistic tree adjoining grammar framework statistical natural language processing 
proceedings th nantes france 
richards gibson 

english pictures 
new york washington square press 
riley michael 
statistical model generating pronunciation networks 
proceedings ieee conference acoustics speech signal processing volume toronto 
rissanen 

universal prior integers estimation minimum description length 
annals statistics 
ron dana yoram singer naftali tishby 

power amnesia 
advances neural information processing systems ed 
jack cowan gerald tesauro joshua alspector 
san mateo ca morgan kaufmann 
sakakibara 

learning context free grammars structural data polynomial time 
theoretical computer science 
michael brown richard hughey mian sj lander rebecca un david haussler 

application stochastic context free grammars folding aligning modeling homologous rna sequences 
technical report ucsc crl computer information sciences university california santa cruz ca 
schabes yves 
inside outside algorithm estimating parameters hidden stochastic context free grammar earley algorithm 
unpublished mss 
second workshop mathematics language may 


stochastic lexicalized tree adjoining grammars 
proceedings th international conference computational linguistics nantes france 
bibliography schwartz richard yen lu chow 

best algorithm efficient exact procedure finding sentence hypotheses 
proceedings ieee conference acoustics speech signal processing volume albuquerque nm 
shieber stuart 
unification approaches grammar 
number csli lecture note series 
stanford ca center study language information 
stolcke andreas 

efficient probabilistic context free parsing algorithm computes prefix probabilities 
technical report tr international computer science institute berkeley ca 
appear computational linguistics 


boogie manual bayesian object oriented grammar induction estimation 
internal memo international computer science institute 
stephen omohundro 

hidden markov model induction bayesian model merging 
advances neural information processing systems ed 
stephen jos hanson jack cowan lee giles 
san mateo ca morgan kaufmann 
stephen omohundro 

best model merging hidden markov model induction 
technical report tr international computer science institute berkeley ca 
stephen omohundro 

inducing probabilistic grammars bayesian model merging 
grammatical inference applications 
second international colloquium grammatical inference spain 
springer verlag 
appear 
jonathan segal 

precise gram probabilities stochastic context free grammars 
proceedings th annual meeting association computational linguistics new mexico state university las cruces nm 
thomason michael erik granum 

dynamic programming inference markov networks finite set sample strings 
ieee transactions pattern analysis machine intelligence 
tomita masaru 

dynamic construction finite automata examples hill climbing 
proceedings th annual conference cognitive science society ann arbor mich 

efficient parsing natural language 
boston kluwer academic publishers 
underwood rebecca christine 

stochastic context free grammars modeling small nuclear acids 
technical report crl computer information sciences university california santa cruz ca 
viterbi 
error bounds convolutional codes asymptotically optimum decoding algorithm 
ieee transactions information theory 
bibliography wallace freeman 

estimation inference compact coding 
journal royal statistical society series 
wolff 
grammar discovery data compression 
proceedings aisb gi conference artificial intelligence hamburg 
wooters charles 
lexical modeling speaker independent speech understanding system 
berkeley ca university california dissertation 
wooters chuck andreas stolcke 

multiple pronunciation lexical modeling speaker independent speech understanding system 
proceedings international conference spoken language processing yokohama 
wright 
lr parsing probabilistic grammars input uncertainty speech recognition 
computer speech language 
zue victor james glass david hong leung michael phillips joseph polifroni stephanie seneff 

integration speech recognition natural language processing mit voyager system 
proceedings ieee conference acoustics speech signal processing volume toronto 
