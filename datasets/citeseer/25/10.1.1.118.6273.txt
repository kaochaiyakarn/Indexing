informed prefetching caching hugo patterson garth gibson daniel jim zelenka may cmu cs school computer science carnegie mellon university pittsburgh pa underutilization disk parallelism file cache buffers traditional file systems induces stall time degrades performance modern microprocessor systems 
aggressive mechanisms tailor file system resource management needs intensive applications 
particular show application disclosed access patterns hints expose exploit parallelism allocate dynamically file buffers competing demands prefetching hinted blocks caching hinted blocks reuse caching data accesses 
approach estimates impact alternative buffer allocations application execution time applies cost benefit analysis allocate buffers greatest impact 
implemented informed prefetching caching dec osf operating system measured performance mhz alpha equipped disks running range applications including text search scientific visualization relational database queries speech recognition computational chemistry 
informed prefetching reduces execution time applications 
informed caching reduces execution time fifth application 
department electrical computer engineering carnegie mellon university www cs cmu edu web groups pdl research partially supported national science foundation number ecd advanced research projects agency contract number dabt ibm graduate fellowship 
views contained document authors interpreted representing official policies expressed implied nsf arpa ibm government 
keywords caching prefetching file systems hints resource management management disk accesses main memory file buffers basic functions file system 
goal disk management start accesses early avoid latency maximize disk throughput 
file buffer management goal exploit locality access stream minimize number disk accesses 
show hints intensive applications prefetch aggressively eliminate stall time maximizing buffer availability caching 
show allocate cache buffers dynamically competing hinting non hinting applications greatest performance benefit 
principle reasons revisit file system resource management increasing availability storage parallelism frequent underutilization increasing occurrence applications dependent file access performance ability intensive applications offer hints beneficial performance 
storage parallelism increasingly available growth disk array products striping device drivers 
hardware software arrays promise throughput needed balance faster cpus distributing data single file system disk arms salem 
embarrassingly parallel workloads benefit immediately 
large accesses benefit parallel transfer multiple concurrent accesses benefit independent disk actuators 
unfortunately workloads parallel consist serial streams comparatively small non sequential accesses 
workloads access disk time idling disks array 
disk seek rotational latencies dominate service time 
disk arrays improve performance workloads multiprocessor improves performance single threaded program 
prefetching strategies adaptive storage parallelism needed parallelize workloads 
applications processing larger objects executing cpu bound components faster 
file cache ratios decrease proportion processor performance amdahl law tells performance increasingly depend performance patterson 
unfortunately simply growing cache increase cache hit ratios expect 
example sprite group caching study led predict higher hit ratios larger caches 
larger caches installed hit ratios changed files grown just fast caches ousterhout baker 
problem especially acute growing class intensive applications 
examples include text search scientific visualization relational database queries speech recognition computational chemistry 
amount data processed large relative file cache sizes locality poor limited accesses frequently non sequential stall time significant fraction total execution time 
applications access patterns largely predictable 
predictability explicit prefetching asynchronous doing resource management 
depth application prefetch depends throughput application varies applications place demands system 
second asynchronously fetched data may flush useful data file cache 
asynchronously fetched file blocks indistinguishable block virtual memory requiring programmer explicitly aware virtual image size avoid losing far paging gained concurrent recommend predictability applications inform file system demands 
specifically propose applications disclose accesses hints file system 
show information exploit storage parallelism balance caching prefetching distribute cache buffers competing applications 
allocate buffers step process 
independently estimate impact execution time alternative buffer allocations 
find prefetch horizon benefit prefetching application catches cost flushing hinted data overhead prefetching back cache shrinking lru cache increases cache ratio average stall time accesses 
step normalize estimates global standard time saved lost unit time buffer allocated 
standardized estimates compare alternatives reallocate buffers maximize net reduction execution time 
example compare benefit prefetching block cost flushing hinted block buffer shrinking lru cache buffer prefetch benefit greater cost valuable buffer 
rest fully explains justifies approach 
section reviews prior describes disclosure hints 
section describes implementation dec osf file system informed prefetching disclosure informed caching describes applications test suite reports test applications benefit substantially 
section introduces cost benefit model informed caching 
section describes implementation informed prefetching caching dec osf file system 
section reports fifth application benefits substantially informed caching 
section discusses directions 
prior hints established broadly applicable technique improving system performance 
lampson reports operating systems alto pilot networking arpanet ethernet language implementation smalltalk lampson 
broadly examples consult possibly date cache hint short circuit expensive computation blocking event 
context file systems historical information file caching prefetching 
ubiquitous cache replacement algorithm relies history accesses choose buffer replacement 
prefetching successful approach sequential readahead mckusick 
osf aggressive example prefetching blocks ahead detects long sequential runs 
notably kotz looked detecting complex access patterns prefetching non sequentially file kotz 
level files database objects number researchers looked inferring accesses past accesses korner kotz tait palmer griffioen 
danger speculative prefetching historical access patterns risks hurting helping performance smith 
result danger speculative prefetching usually conservative waiting theories confirmed number demand accesses 
alternate class hints express system component advance knowledge impact 
familiar occurs form policy advice application virtual memory file cache modules 
hints application recommends resource management policy statically dynamically determined improve performance application trivedi sun cao 
large integrated applications detailed knowledge may available 
database community long taken advantage buffer management 
buffer manager access plan query help determine number buffers allocate sacco chou cornell ng chen 
ng faloutsos sellis marginal gains considered question benefit query derive additional buffer 
stimulated development approach cache management 
stimulated chen roussopoulos supplement knowledge access plan history past access patterns plan contain sufficient detail 
relatively little done combination caching prefetching 
example cao considers question context complete knowledge accesses 
advocate different form hints advance knowledge call disclosure patterson 
application discloses resource requirements hints describe requests terms existing request interface 
example disclosing hint indicate particular file going read sequentially times succession corresponding advising hint specify named file prefetched cached caching policy name mru advice exploits programmer knowledge application system implementations recommend resources managed 
contrast disclosure simply programmer revealing knowledge application behavior 
disclosure principle advantages advice 
expresses information independent system implementation remains correct application execution environment system implementation hardware platform changes 
disclosure mechanism portable optimizations 
second disclosure provides evidence policy decision policy decision robust 
specifically system easily honor particular piece advice little free memory cache file example information disclosure choose partial measure 
third disclosure expressed terms interface application uses issue accesses terms file names file descriptors byte ranges inodes cache buffers file blocks conforms software engineering principles modularity 
implementations disclosing hints issued control system call 
hints list series byte ranges accessed order indicate file accessed sequentially 
order hints indicates order subsequent accesses 
minimal interface meets needs applications test suite 
richer languages expressing exploiting disclosure include collective calls kotz operations structured files grimshaw unordered sets ebling 
disclosing hints informed prefetching caching system delivers primary benefits 
list accesses informed prefetching parallelize request stream available disk resources 
prefetching access disk disk scheduling batching effectively increase disk throughput 
list accesses informed caching outperform lru caching independent prefetching 
experimental results show benefits 
informed prefetching section describe evaluate simple informed prefetching mechanism tip implemented osf file system applied suite intensive benchmarks 
singlethreaded operate data fetched file system needed 
bound common usage 
derive substantial benefit prefetching 
shall see fifth needs informed caching obtain significant benefits 
testbed implementation testbed dec workstation containing mhz processor mb memory fast scsi adapters hosting hp gb disks 
machine runs dec osf version monolithic kernel 
osf file system contains unified buffer cache ubc module trades memory file cache virtual memory 
eliminate buffer cache size factor experiments fixed cache mb kb buffers 
system drives bound disk array striping pseudo device stripe unit kb 
device driver maps forwards accesses appropriate disk device driver 
demand accesses forwarded immediately prefetch reads forwarded fewer outstanding requests drive 
forwarding prefetch requests ensures subsystem resilience transient cpu saturation limiting priority inversion prefetch demand requests 
scan sorts queued prefetch requests 
tip informed prefetching system integrated unified buffer cache 
uses simple mechanism manage resources allows maximum file cache buffers cache hold hinted unread data 
number buffers limit tip prefetches available hint 
hinted blocks cache promoted tail osf lru list 
application accesses hinted block time tip reduces count unread buffers resumes prefetching 
unread buffers may age cache triggering prefetching 
tip running mid bsd ffs mach 
soon ported ux server mach system decstation 
equipped disks user level system able best reduce elapsed time seek intensive data visualization patterson 
summer ported tip alpha testbed exploit greater cpu disk performance 
default osf file system implements optimizations give excellent sequential read performance 
coalesces contiguous blocks cluster requests disk single access 
second aggressive readahead mechanism detects sequential accesses file 
longer run sequential accesses ahead prefetches maximum clusters blocks 
large sequential accesses cat gb file dev null osf achieves mb disks 
primarily interested reducing total execution time intensive application suite elapsed time basic metric 
systems attempt reduce application cpu demands tend interested application stall time 
stall time left recover mechanisms show benefit 
goal exploit unused disk parallelism convert applications bound cpu bound 
application run competition arrays disks 
comparison show results running applications issuing hints 
reported measurements average independent tests 
test run system cold cache 
sequence runs file system formatted block size fragment size inter block rotational delay maximum blocks file cylinder group bytes inodes parameters default run data copied file system 
agrep agrep variant grep written wu manber university arizona wu :10.1.1.48.8488
full text pattern matching program allows errors 
invoked simplest form opens files specified command line time argument order reads sequentially 
elapsed time sec agrep disks wood wood tip incl incl tip kern kern tip elapsed time sec sorting agrep man disks man man tip fifo man tip scan arguments agrep completely determine files agrep access issue hints soon application invoked 
agrep simply loops argument list informs file system names order files sequentially read 
searching data collections software header files mail messages hints agrep frequently specify hundreds files small benefit history readahead 
text search benchmark comprises different executions agrep searching simple string files directory newsgroup messages topic consuming disk blocks wood unformatted osf manual pages consuming disk blocks man usr include header files consuming disk blocks incl osf kernel source files consuming disk blocks kern 
figures report elapsed time searches 
note tip agrep largely unable exploit disk array parallelism little parallelism workload 
files searched serially small osf readahead achieve parallel transfer 
agrep disclosure accesses exposes concurrency 
informed prefetching leverages hints exploit disk parallelism reduces elapsed time agrep searches 
testbed arrays disks sufficient achieve benefit 
gap cpu performance grows expect informed prefetching take advantage larger arrays greater benefits 
elapsed time sec disks plane tip plane tip plane tip 
elapsed time search visualization informed prefetching 
figures report elapsed time searches files different directories 
tip agrep unable exploit parallelism disk array 
shows benefit increasing throughput disk accesses sorting prefetch requests 
effect especially pronounced disk disk bandwidth premium 
shows elapsed time rendering orthogonal planes mb dataset 
plane stored sequentially disk osf readahead performs 
contrast seek required read block plane 
sequentiality osf readahead ineffective informed hints tip able prefetch parallel mask latency seeks 
application demonstrates second benefits disclosing hints increased disk throughput better disk scheduling 
shows effect scheduling dramatically disk 
scan curve corresponds scan sorting prefetches plus sorting prefetches disk described section obtain results 
fifo curve shows elapsed time search reorder prefetches forwarding disk 
shows sorting accesses overlapping computation reduce elapsed time 
sorting prefetches reduces elapsed time 
benefit sorting decreases larger arrays 
extent fixed number prefetches spread disks individual disk queues deep opportunity scheduling optimization 
primary reason disks added array raw disk bandwidth available 
performance bottleneck shifts disks cpu optimizing disk performance important 
interactive scientific visualization tool developed national center supercomputer applications university illinois ncsa 
features lets scientists view arbitrary planar slices dimensional data false color mapping 
datasets may originate broad range applications simulations pollution modelling magnetic resonance imaging tend large 
assumed disks slow performance possible data main memory 
applications including require entire dataset reside memory 
memory expensive amount available constrains scientists higher resolution larger datasets 
informed prefetching invalidates slow disk assumption practical core computing interactive applications 
demonstrate added core capability 
render slice core dataset iteratively determines data point maps pixel reads datum memory applies false coloring writes pixel output pixel array 
contrast render slice core dataset splits loop 
manage internal cache generate hints maps pixels data point coordinates stores mappings array 
having determined data blocks needed render current slice flushes unneeded blocks cache gives hints tip reads needed blocks disk 
second half split loop reads cached pixel mappings reads corresponding data cached blocks applies false coloring 
general generating hints may require little extra precompute file accesses 
agrep quick loop argument list 
splits loop precompute blocks needed 
cost extra negligible case agrep 
net cost may minimized caching result precomputation 
cases shall see postgres restructuring code precompute needed blocks improves performance hints 
test dataset consists bit floating point values requiring mb disk storage 
dataset organized kb blocks data points 
blocks stored disk order 
test renders random slice orthogonal planes 
reports elapsed time rendering slices informed prefetching 
tip osf readahead effective plane stored sequentially 
soon requested slice deviates plane sequentiality lost performance 
plane rows stored sequentially osf able take advantage disks array 
plane block requires seek performance requiring fifteen seconds render slice 
unacceptable interactive application 
informed prefetching slice orientations rendered elapsed time seconds 
tip prefetches parallel mask cost seek block plane 
sum elapsed time slices orientations reduced factor 
informed prefetching performance acceptable working huge core datasets 
agrep disk batching scheduling little effect components slice requested ascending order offset file order fetch disk 
postgres postgres version stonebraker stonebraker extensible object oriented relational database system uc berkeley 
test postgres executes join relations 
outer relation contains unindexed tuples mb inner relation tuples mb indexed mb 
average tuples outer relation finds match inner relation output tuples written sequentially 
perform join postgres reads outer relation sequentially 
outer tuple postgres checks inner relation index matching inner tuple reads tuple inner relation 
perspective storage inner relation index accessed randomly 
inner relation reads incur full latency disk access random defeats readahead poor locality defeats caching 
disclose inner relation accesses employ loop splitting technique similar 
precomputation phase postgres reads outer relation disclosing sequential access looks outer relation tuple address index stores addresses array 
postgres discloses precomputed block address tip 
second pass postgres outer relation skips index lookup directly reads outer relation tuple address stored array 
shows elapsed time required join conditions standard postgres postgres precomputation loop giving hints postgres precomputation loop giving hints 
simply splitting loop reduces elapsed time 
loop split buffer cache better job caching index inner relation data blocks polluting cache 
postgres reads outer relation twice fewer total disk os precomputation run 
case precomputing disk accesses saves time 
delivering hints tip reduces elapsed time additional total reduction standard postgres 
sphinx sphinx lee high quality speaker independent continuous speech recognition system 
experiments sphinx recognizing second recording commonly sphinx regression testing 
sphinx represents acoustics hidden markov models uses viterbi beam search prune unpromising word combinations models 
achieve higher accuracy sphinx uses language model effect second level pruning 
language model table conditional probability word pairs word triples 
ms acoustical frame second level pruning words ended frame 
potential words probability recognized conditioned probability occurring triple recognized words occurring pair recognized word entry elapsed time sec postgres disks postgres postgres postgres tip language model current triple 
improve accuracy sphinx similar passes search data structure time restricting language model results previous pass 
sphinx came core system 
commonly dictionary containing words language model megabytes size 
addition internal caches search data structures virtual memory paging occurs machine mb memory 
modified sphinx fetch disk language model word pairs word triples needed 
enables sphinx run mb test machine fast mb machine 
additionally modified sphinx disclose word pairs word triples needed evaluate potential words offered frame 
language model sparsely populated frame byte ranges consulted sphinx internal cache 
high variance number pairs triples consulted fetched storage parallelism employed 
shows elapsed time sphinx recognizing second regression test hints 
previous applications sphinx derives small benefit disk array 
informed prefetching elapsed time sphinx test reduced 
davidson algorithm elapsed time sec sphinx disks sphinx sphinx tip multi configuration fock suite computational chemistry programs atomic structures calculations obtained vanderbilt university 
davidson algorithm element suite computes successive refinement extreme elapsed time sec davidson algorithm disks dvd tip dvd 
elapsed time postgres sphinx davidson informed prefetching 
shows elapsed time join standard postgres relational database postgres restructured precompute offsets inner relation restructured postgres gives hints 
restructuring improves accesses locality cache performance runs faster standard postgres 
delivering hints improves performance 
shows benefits informed prefetching sphinx speech recognition program 
shows performance davidson algorithm applied computational chemistry problem 
algorithm repeatedly reads large file sequentially 
osf aggressive readahead algorithm performs slightly better informed prefetching access pattern incurs little overhead 
normalized elapsed time disks disks eigenvalue eigenvector pairs large sparse real symmetric matrix stored disk 
test uses lowest energy states sodium atom basis size large matrix mb 
core davidson algorithm improves estimate extreme eigenpairs computing extreme eigenpairs smaller derived matrix 
iteration core computes new derived matrix matrix vector multiplication involving large disk matrix 
algorithm repeatedly accesses large file sequentially 
annotating code give hints straight forward 
start iteration davidson algorithm discloses file sequential read anticipated iteration 
reports elapsed time entire computation informed prefetching 
hints disclose sequential access large file osf aggressive readahead matches performance informed prefetching fact performs slightly better incurs overhead 
hints davidson test benefits significantly extra bandwidth second disk cpu bound 
osf informed prefetching uses mb cache buffers 
mb matrix fit cache lru replacement algorithm flushes blocks reused 
see sections informed caching able hint repeated access modify cache behavior significantly reduce elapsed time davidson algorithm 
summary informed prefetching experiments davidson sphinx postgres agrep shows test applications elapsed time give hints fraction elapsed time 
agrep multiple parts show sum times individual parts 
see normalized stall time 
normalized elapsed stall times test applications 
shows elapsed time hints fraction elapsed time hints applications 
informed prefetching improve osf readahead large sequential accesses davidson show significant benefits applications 
goal informed prefetching reduce latency 
shows stall time hints fraction stall time hints 
exception davidson informed prefetching uses parallelism disk arrays eliminate stall time 
intensive applications cpu bound informed prefetching disk arrays combine improve performance 
davidson benefit informed prefetching sphinx elapsed time reduced postgres join elapsed time reduced elapsed time sum slices reduced agrep elapsed time sum searches reduced 
informed prefetching reduces latency achieve results 
illustrates showing time application spends stalled informed prefetching fraction stall time 
exception davidson served osf readahead informed prefetching eliminates stall time 
informed prefetching leverages parallelism disk arrays eliminate stall time possible simply overlapping computation activity single disk 
high performance workstation alpha testbed needs modest array disks eliminate stall time intensive applications 
processor performance improves expect informed prefetching take advantage greater disk parallelism benefits informed prefetching increase 
section demonstrated informed prefetching deliver benefits ascribe disclosure application knowledge accesses 
tip expose parallelism exploit parallel prefetching disk arrays improved disk scheduling 
performance davidson algorithm shows tip unable exploit disclosure deliver third benefit informed caching 
turn attention informed caching 
informed caching cost benefit analysis approach goal informed cache manager allocate cache buffers minimize application elapsed time 
basic approach estimate impact execution time alternative buffer allocations choose best allocation 
shown manager choose broad uses buffer caching data traditional lru queue prefetching data hints predictor accesses caching data predictor indicates reused 
practice extremely difficult estimate performance allocations global level huge number possible allocations complex interactions alternative uses cache buffers 
avoid complexities developed framework global allocation depends local estimates impact execution time basis comparing local estimates 
framework allocation decisions buffer buffer basis reallocation opportunities arise 
informed cache manager repeatedly faces decisions block replaced buffer needed prefetching service demand request 
cache buffer prefetch data 
manager uses step process decisions 
independently estimates impact execution time lru cache buffer prefetching block flushing hinted block 
converts estimates common basis comparison amount time saved lost buffer access 
allocates buffers obtain biggest bang buffer 
describing framework informed caching start describing model system performance derive estimates impact cache buffer different ways 
system model applications demand accesses hinted accesses lru cache prefetched blocks execution time application number requests average application cpu time requests average time takes service request 
time service request depends requested data cache 
hit time takes read block cache 
block cache needs fetched disk may delivered application 
addition latency fetch disk requests suffer computational overhead driver allocating buffer queuing request drive servicing interrupt completes 
time service request misses cache sum times prefetching benefit reducing execution time masking disk latency shown section stealing buffer lru cache request misses cache pay delay hit holding hinted block cache saves costs involved prefetching block back cache 
holding hinted buffer lru cache buffer increase number hits 
cache prefetch hinted sequences cached blocks 
uses cache buffers 
cache buffers may hold data demand accesses lru queue 
may hold data hints indicate soon 
prefetch data hints 
job cache manager allocate buffers different uses maximize cache hits initiating prefetches early minimize time spent waiting data 
ti cpu disk hit rate cost deallocating lru buffer time portion requests hit cache cache hit ratio 
cache hit ratio function number buffers cache workload 
demand accesses shows cost buffer lru list prefetching decrease cache hit ratio 
true lru replacement stack algorithm 
decrease cache hit ratio results increase average access time derivative respect informed cache manager directly continually estimates described section computes expected impact losing lru buffer 
drawback approach point estimate 
general may smooth function suggested 
function may 
large jump hit rate entire working set application fits buffer cache 
cases better average marginal cost region includes jump hit rate 
benefit prefetching slope marginal hit rate cache buffers 
impact stealing buffer lru cache 
buffer taken lru cache hit rate drops proportion marginal hit rate 
increases average number cache misses average time required service request 
increase cost buffer prefetching 
ti cache working set buffers 
avoiding local minima lru cost estimate 
point estimate cost losing buffer possible lose sight largescale effects occur working set application fits cache 
avoid broader estimate cost looking past points inflection long term average marginal cost 
benefit prefetching block avoidance disk component cost contrast lru buffers usefulness averaged accesses benefit prefetching time reduction cost access block 
note repeated accesses block desirable prefetch block 
prefetched access block available cache 
note prefetching avoid cpu overhead disk access driver disk upper bound benefit prefetching block 
blocks benefit starting prefetch disk informed cache manager opportunity reconsider prefetch decision soon hit rate large macro marginal cost small local marginal cost ti 
ti disk disk hit file access event 
prefetch delayed complete needed consider benefit starting prefetch 
key observation application data consumption rate bounded 
minimum hit limits data consumption 
general consumption limited cpu application computation requests 
steady state data streaming application processes data fast driver reduces data consumption rate 
estimate time application consumes data requests hit driver benefit prefetching block needed 
avoid consistently failing completed prefetches bursts data consumption define benefit prefetching assuming burst occur 
particular assume cpu driver go zero 
case shown benefit prefetching block requests 
important consequence relation assuming adequate bandwidth benefit prefetching requests 
call prefetch horizon 
prefetching ahead prefetch horizon wastes buffers cache data 
comparing lru cost prefetching benefit prefetch horizon 
prefetch horizon 
data consumption limited minimum hit time takes read data cache 
assuming adequate bandwidth disk queues benefit prefetching ahead prefetch horizon disk hit general cpu driver limit data consumption rate 
estimates cost buffer lru cache benefit prefetching access requests unfortunately estimates directly comparable 
cost lru average cost access benefit prefetching accrues lump requests 
solve problem defining basis comparing estimates 
shared resource allocating cache buffers 
smallest denomination resource occupation buffer access period 
buffers spent reduce application execution time 
basis comparison common currency cache manager change application execution time access buffer allocated 
cpu bx driver bx conversion independent estimates common basis straightforward 
lru cache effectively number hits access buffer 
cost losing lru cache buffer computed terms time access buffer conversion necessary 
convert bx average benefit prefetching requests buffer occupied 
conceptually benefit greater cost tying buffer accesses cause additional lru cache 
mechanism common basis comparison relieves informed cache manager need understand combined effect 
cost benefit estimates independently compared terms common basis 
factor take account difference rates demand accesses rd hinted accesses rh compensate difference normalize estimates rates 
say buffer reallocated lru cache prefetching bx 
dh cost flushing hinted block benefit prefetching prefetch horizon true caching 
section compute cost flushing cache block hint indicates reused 
flush cached block hint accesses prefetch back cache add execution time cpu cost accessing disk driver hand flushing block frees buffer 
assume block prefetched back cache prefetch horizon flushing block frees buffer accesses 
terms common basis cost flushing block driver 
estimate faulty flush candidate prefetch horizon may impossible prefetch block back needed 
case cost flushing access cost driver plus non overlapped disk access time paid block flushed furthermore prefetch horizon prefetch block imminent 
safe assume flushing block free buffer access 
summarizing change execution time expected result flushing block prefetching back terms common basis averaged number accesses buffer freed 
expression terms common basis allows comparison value lru cache valuable block 
allows comparison benefit prefetch buffer needed purpose 
case note cost flushing hinted block greater benefit prefetching block equation 
hysteresis estimates reduces likelihood thrashing 
block flushed prefetched back soon vice versa 
putting global min max estimates value cost action informed cache manager choose expected cost stealing lru cache valuable block expected benefit prefetching hinted uncached block expected cost flushing hinted cache block 
resolve informed cache manager questions 
block replaced buffer needed prefetching service demand request 
globally valuable block cache 

cache buffer prefetch data 
prefetch expected benefit greater expected cost flushing stealing valuable block 
identify globally valuable block cache informed cache manager maintains separate estimator lru cache independent stream hints 
estimator determines costs flushing stealing blocks information 
possible single block tracked estimators 
example hint refer block resides lru queue 
case estimators independently estimates cost flushing block 
lru cache value block depends order lru queue 
block position queue value estimated hit 
hint estimators rank blocks cost flushing determined equation 
cost access disk blocks tracked estimator valuable block simply furthest 
values determined estimator normalized rate accesses estimator shown equation 
estimators determined values single block global value block maximum normalized values independent estimators 
example block may far lru queue hint says accessed soon assign block maximum valuations 
globally valuable block block maximum valuation minimal blocks 
replacement policy employs global min max valuation blocks 
overhead estimation prohibitive 
practice described section value small number blocks needs determined find globally valuable block 
note informed cache manager choose restricted set blocks 
lru buffer may prefetch block 
cached hinted block may flushed service demand sense buffers belong individual estimators 
note buffer allocation done buffer time 
high level decision estimator blocks 
informed caching example mru aid understanding informed caching discovers caching policy show exhibits mru behavior repeated access sequences 
illustrates example 
start iteration sequence repeats accesses cache manager prefetches prefetch horizon 
block consumed candidate replacement prefetching service demand misses 
hit rate function indicates blocks lru queue don get hits blocks cached blocks prefetch horizon mru replacement consumed consumed valuable hinted block just consumed 
prefetching continues replacing blocks lru list leaving hinted blocks cache consumption 
process continues blocks devoted caching repeated sequence number lru buffers shrinks 
common hit rate functions fewer buffers lru cache valuable 
eventually value lru buffer exceeds benefit retaining consumed hinted block reuse accesses 
prefetch mru block flushed cached blocks outstanding hints furthest 
point wave prefetching consumption flushing moves remaining blocks interaction 
prefetch horizon limits prefetching buffers wave 
disk array delivers blocks faster application consumes risk cache manager cached blocks prefetch 
mru behavior cache manager assured 
cache manger strikes balance number buffers prefetching caching hinted blocks lru caching 
informed cache manager discovers mru caching specifically coded implement policy 
expect similar performance arbitrary access sequences blocks may reused 
appropriate number blocks cache hinted data flushing occur block access farthest flushed 
generalizing non uniform device performance pattern repeats accesses commonly suggested blocks prefetched order accessed cao 
correct accessed devices take time complete fetch 
general case data may local disk data may far side wide area network 
remote blocks network server disk substituted disk determining benefit prefetching prefetch horizon 
cause benefit prefetching remote blocks exceed prefetching earlier local blocks 
informed caching framework described section adapt differences correct trade 
similar effect order flushing cached blocks outstanding hints 

mru behavior informed cache manager repeated access sequences 
number blocks allocated caching repeated access pattern grows caching benefit sufficient hold additional buffer accesses reused 
point valuable buffer just consumed access furthest 
block recycled prefetch block prefetch horizon 
wave prefetching consumption recycling moves accesses joins blocks cached iteration data 
implementation informed caching prefetching cache structure tip cache manager replaces unified buffer cache osf 
organized concept estimator determines value data blocks servicing stream related accesses 
start lru estimator handles blocks stream demand accesses 
process issues hints estimator stream hinted access 
accesses hinting processes belong global stream demand accesses 
blocks replaced global min max valuation blocks described section 
blush appears find globally valuable block estimator needs consider value block cache 
extremely expensive 
practice sufficient estimator able identify valuable block estimate value 
lru estimator perspective block valuable 
hint estimator assuming access time blocks valuable block accessed farthest 
estimator converts value estimate common basis time saved lost access declares converted estimate 
global arbitrator normalizes estimates rate requests going estimator ranks estimators normalized estimate 
arbitrator needs find globally valuable block asks valued estimator pick valued block 
forth estimator treats block longer cache declares arbitrator value valuable remaining blocks 
estimators determine valuable subset blocks cache 
block subset estimator said tracking value block 
estimator may maintain interest blocks just picked guarantees blocks tracks valuable globally declared value valuable block 
cases estimator interested particular block 
estimator picks candidate block replacement story 
interested estimators queried see want track block save replacement 
estimator agrees track block estimation block valuable lowest globally declared value 
estimator saves candidate replacement arbitrator repeats process asking valued estimator pick new candidate block replacement 
estimators pick replacement blocks tracking 
point view estimator block cache 
blocks tracked estimator remain cache forever 
estimator considers block valuable track replaced 
block replaced immediately example contains dirty data special orphan estimator tracks block till ready replaced 
estimating marginal lru hit rate hint estimators directly apply equations section estimate values blocks tracking 
lru estimator 
missing piece hit rate function determined equation 
lru block replacement stack algorithm ordering blocks lru queue independent size cache 
observing queue buffers cache hits occur possible history estimate cache hit rate function number buffers cache cache size tip informed cache manager number buffers lru queue may vary dynamically 
just important know cache perform buffers 
solution borrowed maria ebling ebling ghost buffers buffer headers data allocated record access hit buffers cache 
lru estimator tracks blocks 
picks head list replacement buffer turns ghost remains lru list 
requests block find ghost record fact cache larger request hit 
length lru queue including ghosts limited total number buffers cache 
maintaining hit ratios separately size cache expensive 
hit ratio particular size probably useful local average lower variance 
reasons tip piecewise estimate function shown 
lru queue segmented contiguous disjoint regions hits segment recorded 
marginal hit rate cache sizes segment number hits segment divided size segment normalized total number accesses 
prefetcher hit rate tracked buffers ghost buffers prefetching estimator passes prefetcher ordered list blocks prefetch current progress hints hinting application 
block marked position hint list prefetcher tell far block 
prefetcher computes benefit prefetching desired non resident page normalizes sorts estimators arbitrator 
prefetcher compares cost losing currently valuable block cache benefit prefetching 
benefit exceeds cost prefetcher acquires buffer initiates prefetch 
estimates benefit prefetching block resorts estimators 
new hints arrive hinted data consumed application estimators may ask prefetcher update estimate benefit prefetching blocks estimator 
cache buffers lru list valuable tracked buffer 
piecewise estimation marginal hit rate lru list 
list broken segments buffer tagged indicate segment 
tag updated buffer passes segment 
cache hit segment segment hint count incremented 
segment hit count divided number buffers segment normalized total number accesses lru cache piecewise approximation marginal hit rate buffers segment 
clustering prefetches osf derives significant performance benefits clustering transfer blocks disk access 
buffer allocated prefetch block part cluster 
decision prefetch block cost driver performing paid 
blocks piggyback avoid costs 
blocks clustered prefetching incur full overhead performing possibly cost unmasked disk latency 
exactly costs considered deciding flush hinted block 
decision include block cluster decision flush hinted block 
prefetcher decided prefetch block tries build cluster 
checks contiguous hinted blocks currently cache 
queries appropriate estimator see wants track block 
caching block valuable caching globally valuable block 
buffer reallocated block included cluster 
informed caching experiments implementation informed caching osf runs testbed hardware evaluations section 
report effect davidson application benefits informed prefetching simple lru cache manager 
look application range array cache sizes running competition range cache sizes competing buffers bound non disclosing application 
discovering mru recall section davidson core file mb size read sequentially multiple times 
osf aggressive readahead performs informed prefetching simple access pattern 
static lru replacement policy osf tip poor mb cache 
blocks flushed reused 
informed caching replacement policy dynamic adaptive 
shows elapsed time davidson benchmark data striped disks size informed cache mb 
enabled disclosure informed caching reduces elapsed time benchmark disk 
disk bandwidth limited improved caching avoids disk latency 
disks prefetching masks disk latency informed caching reduces execution time avoiding cpu overhead extra disk accesses 
shows davidson elapsed time hints function cache size 
hints extra buffers entire mb dataset fits cache 
contrast informed cache manager min max global valuation blocks yields smooth exploitation additional cache buffers expect mru replacement policy 
notion prefetch horizon limits buffers prefetching disk bandwidth flush cache prefetched blocks 
informed cache manager effectively balances allocation cache buffers prefetching caching 
balancing contention cache buffers informed caching cost benefit model balances allocation buffers competing applications 
show effect ran highly bound repetitive non hinting application cat mb file mb file dev null concurrently hinting run davidson 
non hinting application poor cache buffers cache big hold mb file 
elapsed time sec davidson algorithm disks mb cache dvd dvd tip 
benefit informed caching repeated accesses 
shows informed caching reduces elapsed time davidson repeated accesses disk improved caching avoids disk latency 
disks prefetching masks disk latency informed caching reduces execution time avoiding overhead going disk 
shows informed caching discovers mru policy uses additional buffers increase cache hits reduce execution time 
contrast lru caching derives benefit additional buffers cache entire dataset 
file fit cache application runs data high rate negligible amount computation block 
elapsed time difficult interpret applications different elapsed times selected non hinting application complete report cache ratio davidson application experienced point 
shows results cache sizes mb mb 
caches smaller mb informed cache manager finds locality non hinting application lru cache 
marginal hit rate effectively zero 
davidson application estimator find value caching blocks 
buffers migrate lru queue davidson estimator mru style caching 
cache increases size mb mb additional buffers reduce davidson ratio 
cache size reaches mb non hinting application lru cache 
buffers migrate davidson estimator davidson ratio jumps nearly 
davidson execution time drops non hinting application completes quickly gets way davidson buffers complete execution 
non hinting application need buffers fixed cache grows mb additional buffers go davidson ratio drops 
results show informed cache manager effectively balances cache buffers hinting applications 
elapsed time sec davidson algorithm buffer cache size mb disk cache rate cache rate davidson buffer cache size mb disk 
competition buffers hinting non hinting applications 
background non hinting task poor lru cache buffers mb data fits cache 
caches smaller mb informed cache manager shrinks lru cache uses buffers mru style caching hinting davidson application ratio drops 
mb background task best buffers gets buffers davidson ratio jumps nearly 
cache grows mb davidson gets additional buffers ratio declines 
traditional shallow readahead lru file caching longer provide satisfactory resource management growing number bound applications 
disk parallelism cache buffers face serial workloads large working sets 
advocate disclosure application knowledge accesses enable informed prefetching informed caching 
resource managers expose workload parallelism exploit storage parallelism apply disk scheduling increase disk throughput adapt caching policies dynamic needs running applications 
key achieving goals strike balance desire prefetch desire cache 
framework informed caching cost benefit model value buffer 
show independent local estimates value caching block lru queue prefetching block caching block hinted reuse 
define basis comparing estimates time gained lost buffer access interval 
develop global min max algorithm arbitrate estimates maximize global usefulness buffer 
results taken experiments suite intensive applications executing dec array disks 
applications include text search data visualization database join speech recognition computational chemistry 
exception computational chemistry applications exploits parallelism disk array 
informed prefetching disks reduces elapsed time remaining applications 
computational chemistry repeatedly reads large file sequentially osf aggressive readahead informed prefetching 
informed caching adaptive policy values application blocks lower older blocks discovers mru policy improves performance 
experimental results show resource management framework effectively balances resources prefetching caching applications disclose 
informed caching informed prefetching provide powerful resource management scheme adapts available storage bandwidth adapts application buffers optimizes disk throughput scheduling batching 
results reported taken running system remain interesting related questions 
area hint generation richer hint languages significantly improve ability programmers disclose accesses 
easier programmer automatic generation high quality hints begun problem 
number issues area resource management 
currently hints accurate appropriate strategies dealing imprecise useful hints needed 
cost benefit model easily adapt non uniform device bandwidths replacing average disk access time appropriate value 
implemented feature 
exciting lies exploiting extensibility resource management framework 
value estimates independently local information compared common currency possible add new types estimators 
example virtual memory estimator track vm pages integrating vm buffer cache management 
acknowledgment wish number people contributed including charlotte fischer atomic structure calculation group department computer science vanderbilt university help davidson algorithm ravi sphinx group cmu su initial port tip osf mach david golub debugging coding contributions chris wrote striping driver alex ported version alpha satyanarayanan early contribution ideas pdl support tolerance rush complete 
baker baker hartman kupfer shirriff ousterhout measurements distributed file system proc 
th symp 
operating system principles pacific grove ca october pp 

cao cao felten li implementation performance application controlled file caching proc 
usenix symp 
operating systems design implementation monterey ca november pp 
cao cao felten karlin li study integrated prefetching caching strategies appear sigmetrics 
chen chen cm 
roussopoulos adaptive database buffer allocation query feedback proc 
th vldb conference dublin ireland pp 

chou chou dewitt evaluation buffer management strategies relational database systems proc 
th int 
conf 
large data bases stockholm pp 

cornell cornell yu integration buffer management query optimization relational database environment proc 
th int 
conf 
large data bases amsterdam aug pp 

krishnan vitter practical prefetching data compression proc 
acm conf 
management data sigmod washington dc may pp 

ebling ebling steere overcoming network bottleneck mobile computing proc 
workshop mobile computing systems applications december 
multics input output system proc 
rd symp 
operating system principles pp 
griffioen griffioen appleton reducing file system latency predictive approach proc 
summer usenix conference boston ma 
grimshaw grimshaw jr object oriented extensible file systems computer science report 
tr university virginia july 
huang huang hsiao hon mei hwang kai fu lee rosenfeld sphinx ii speech recognition system overview computer speech language uk vol april pp 

korner korner intelligent caching remote file service proc 
tenth int 
conf 
distributed computing systems pp 
kotz kotz ellis practical prefetching techniques parallel file systems proc 
international conf 
parallel distributed information systems miami beach florida dec pp 

kotz kotz disk directed mimd multiprocessors proc 
usenix symp 
operating systems design implementation monterey ca november pp 

lampson lampson hints computer system design proc 
th symp 
operating system principles woods pp 

lee lee hon reddy overview sphinx speech recognition system ieee transactions acoustics speech signal processing usa vol jan pp 
mckusick mckusick joy leffler fabry fast file system unix acm trans 
computer systems august pp 

ncsa national center supercomputing applications 
window system 
ng ng faloutsos sellis flexible buffer allocation marginal gains proc 
acm conf 
management data sigmod pp 

ousterhout ousterhout da costa harrison kunze kupfer thompson trace driven analysis unix bsd file system proc 
th symp 
operating system principles orcas island wa december pp 

palmer palmer zdonik fido cache learns fetch brown university technical report cs 
patterson patterson gibson katz case redundant arrays inexpensive disks raid proc 
acm conf 
management data sigmod chicago il june pp 

patterson patterson gibson satyanarayanan status report research transparent informed prefetching acm operating systems review april pp 

patterson patterson gibson exposing concurrency informed prefetching proc 
third int 
conf 
parallel distributed information systems austin tx sept pp 

ravishankar ravishankar parallel implementation fast beam search speaker independent continuous speech recognition computer science automation indian institute science bangalore india 
sacco sacco mechanism managing buffer pool relational database system hot set model proc 
eighth int 
conf 
large data bases september pp 

salem salem garcia molina disk striping proc 
nd ieee int 
conf 
data engineering 
smith smith disk cache ratio analysis design considerations acm trans 
computer systems august pp 

fischer davidson program finding selected extreme eigenpairs large sparse real symmetric matrix computer physics communications vol 
pp 

stonebraker stonebraker rowe design postgres proceedings acm sigmod international conference management data washington dc usa may 
stonebraker stonebraker rowe implementation postgres ieee transactions knowledge data engineering vol march pp 
sun sun microsystems sun os manual part number revision may 
tait tait duchamp detection exploitation file working sets proc 
th int 
conf 
distributed computing systems arlington tx may pp 

trivedi trivedi analysis computing pp 

wu wu manber agrep fast approximate pattern matching tool proc :10.1.1.48.8488
winter usenix conference san francisco ca jan pp 


