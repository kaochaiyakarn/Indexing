ieee transactions evolutionary computation vol 
april evolutionary algorithm guided mutation maximum clique problem zhang member ieee sun edward tsang member ieee estimation distribution algorithms sample new solutions offspring probability model characterizes distribution promising solutions search space generation 
location information solutions far actual positions solutions search space directly generating offspring existing estimation distribution algorithms 
introduces new operator called guided mutation 
guided mutation generates offspring combination global statistical information location information solutions far 
evolutionary algorithm guided mutation ea maximum clique problem proposed 
guided mutation ea adopts strategy searching different search areas different search phases 
marchiori heuristic applied new solution produce maximal clique ea experimental results show ea outperforms heuristic genetic algorithm marchiori best evolutionary algorithm reported far mimic algorithm dimacs benchmark graphs 
index terms estimation distribution algorithms evolutionary algorithm guided mutation heuristics hybrid genetic algorithm maximum clique problem mcp 
evolutionary algorithms genetic algorithms gas scatter search estimation distribution algorithms edas population solutions combine generate new solutions offspring may repaired improved heuristic operators local search 
crossover mutation main operators generating offspring conventional gas 
operators directly act selected solutions parents 
crossover operator applies pair parents swaps parts parents produce offspring 
mutation operator randomly alters part parent solution produce offspring 
scatter search uses linear combinations selected solutions heuristic improvement rounding process generating new solutions 
information locations solutions far actual positions solutions search space directly gas scatter search 
edas generate offspring quite different way 
maintain probability model characterizes distribution promising solutions iteration generation 
probability model updated manuscript received january revised october 
supported part engineering physical sciences research council epsrc gr 
authors department computer science university essex park colchester sq 
mail essex 
ac uk essex ac uk edward essex ac uk 
digital object identifier ieee global statistical information extracted current population 
offspring generated sampling model 
global information search space collected search far produce offspring edas 
information locations solutions far directly guide search 
pe proposed hybrid algorithm offspring generated crossover mutation eda operators 
preliminary experimental results promising 
propose new offspring generating operator called guided mutation evolutionary algorithms 
guided mutation regarded combination conventional mutation operator eda offspring generating scheme 
guided probability model guided mutation alters parent solution produce new solution 
resultant solution hopefully fall close promising area characterized probability model 
directly takes user specified percentage elements parent better solution far 
way similarity offspring parent controlled extent 
clique graph set pairwise adjacent nodes 
maximal clique clique proper subset clique 
maximum clique clique maximum cardinality called maximum clique number 
maximum clique maximal vice versa 
graph maximum clique problem mcp find maximum clique 
mcp best known problems combinatorial optimization 
applications coding theory fault diagnosis multiprocessor systems constraint satisfaction computer vision mobile networks underlying problem formulated instance mcp 
mcp np complete 
polynomial time algorithm approximating maximum clique factor number nodes graph 
facts indicate mcp difficult solve 
excellent overview algorithms complexity applications mcp 
due inherent difficulty importance mcp considerable efforts devoted developing heuristics 
aim heuristics find maximal clique large possible 
sound theory heuristics comparison performances different heuristics mainly extensive experimentation 
set benchmark graphs different applications collected purpose graphs available dimacs rutgers edu challenges 
zhang evolutionary algorithm guided mutation mcp attempts solve mcp evolutionary algorithms 
scatter search algorithm mcp proposed 
cater park shows simple ga effective mcp implies gas need customized incorporate techniques order improve performances problem 
fact strengths weaknesses simple ga studied theoretically 
fitness function graded penalty term penalizing infeasible solutions modified genetic operator local search operator problem specific representation gas mcp 
heuristic genetic algorithm proposed marchiori naive greedy heuristic procedure applied new solution subset nodes graph generated uniform crossover mutation 
greedy heuristic enlarges subset nodes adding nodes randomly selected reduces clique enlarges maximal clique 
algorithm marchiori outperforms existing gas scatter search dimacs benchmark graphs terms quality solutions speed 
proposes evolutionary algorithm guided mutation ea mcp 
main features ea follows 
guided mutation new solutions generated applying guided mutation operator best solution current population resultant solutions far best solution far fall promising area 
heuristic repair new solution generated guided mutation operator may clique heuristic repair operator marchiori applied new solution generate clique 
partitioning search space search space divided search areas algorithm focuses different search areas different phases 
rest organized follows 
section ii introduces guided mutation 
ea mcp section iii 
experimental study comparisons section iv 
conclude section ii 
guided mutation operator key issues design evolutionary algorithms generate offspring 
proximate optimality principle underlying assumption heuristics assumes solutions similar structure 
assumption reasonable real world problems percentage common edges locally optimal solutions traveling salesman problem obtained lin kernighan method average 
assumption ideal offspring generator able produce solution close best solutions far 
suppose current population evolutionary algorithm local search consists best locally optimal solutions far new solution generated conventional mutation close similar parent may far away better solutions mutation utilize global information extracted current population 
edas extract global statistical information previous search represent probability model characterizes distribution promising solutions search space 
new solutions generated sampling model 
location information locally optimal solutions far directly edas mechanism directly control similarity new solutions solution 
idea proposed operator call guided mutation combine global statistical information location information solutions far overcome shortcoming gas edas 
different probability models introduced edas modeling distribution promising solutions 
univariate marginal distribution umd model simplest univariate marginal distribution algorithm population incremental learning compact ga 
search space umd model uses probability vector characterize distribution promising solutions search space probability value th position promising solution 
guided mutation operator uses probability vector guide mutate way guided mutation operator input output flip coin head probability head turns probability set set guided mutation operator directly copied parent randomly sampled probability vector larger elements sampled probability vector words similar mutation rate conventional mutation controls similarity offspring parent parent chosen best solutions far 
correlated mutation real vectors probability generating offspring steepest ascent direction larger directions 
conventional mutation binary strings probability vector generated parent vector entirely determined hamming distance guided mutation operator regarded discrete counterpart correlated mutation 
probability vector guided mutation learned updated generation modeling distribution promising solutions 
elements offspring sampled probability vector expected fall close promising area 
sampling provides diversity search 
ieee transactions evolutionary computation vol 
april iii 
algorithm mcp section describes detail main components ea mcp 
ea potential solution mcp encoded binary string 
generation ea maintains population binary strings probability vector search procedure divided phases search focus different search areas different phases 
new solutions generated guided mutation operator sampling randomly particular search area 
representation fitness graph node set edge set 
set nodes encoded string node search space fitness defined cardinality represents clique 
new solution generated guided mutation repaired need define fitness values infeasible solutions 
partitioning search space search space mcp divided subspaces follows obviously wehave string represents set nodes cardinality goal find clique large possible 
maximal clique cardinality necessary explore subspaces 
considerable amount effort unwise search proximate optimality principle maximum clique number far bigger proposed algorithm explores different subspaces search space different search phases 
lower bound low maximum clique number computed 
search phase area predefined integer number 
maximal clique larger cardinality current search phase search area phase way search focused promising repair heuristic induce set nodes string set nodes may clique 
produce clique marchiori heuristic repair 
repair operator input set nodes output clique step extraction step set step go step 
randomly pick node remove step flip coin head probability head turns remove remove nodes connected step go step 
step extension step set step return randomly pick node remove step node connected nodes add step go step 
repair heuristic operator clique step 
generally speaking small 
small step nodes may removed 
step simply extends maximal clique 
initialization update probability vector probability vector guided mutation section ii needs initialized updated ea suppose current population strings initialized binary search phase search needs restarted 
percentage binary strings value th element regarded center generation ea binary strings selected current population form parent set updating probability vector contain strings probability vector updated way pbil algorithm learning rate 
bigger contribution strings updated probability vector 
learning brings probability vector center parent set 
algorithm ea works follows 
step set parameters population size main algorithm number partitioning search space update probability vector guided mutation operator 
repair step set randomly pick apply repair operator obtain maximal clique set 
zhang evolutionary algorithm guided mutation mcp step randomly pick strings apply repair operator resultant strings form initialize probability vector 
step select best strings form parent set update probability vector 
step apply guided mutation operator fittest string times apply repair operator resultant strings get cliques 
add cliques form stopping condition met return largest clique far 
step set largest clique set go step 
step strings identical go step go step 
step lower bound low maximum clique number obtained 
population probability vector initialized step search step selects fittest strings parents update step strings generated applying guided mutation operator fittest string current parent set 
mutated strings repaired 
resultant strings join parents form population generation 
larger clique step search move new area search space 
members current population identical step search restarted diversify population 
reasons selecting largest clique fittest clique mutated step twofold 
desirable new cliques similar largest clique far proximate optimality principle 
second cliques far contribute new cliques probability vector iv 
computational experiments section study ea experimentally compare best ga mcp dimacs benchmark problems 
compare performances ea mimic test problems 
ea implemented mentioned earlier parameter repair operator small 
suggestion set generally speaking lower bound obtained step far maximum clique number proximate optimality principle 
set small number 
set experiments 
set population size marchiori 
algorithm stops calls repair operator 
experiments performed amd mhz 
effects apart parameters ea learning rate update probability vector guided mutation operator balancing fig 

surface average sizes largest cliques runs 
tradeoff global information location information solutions far 
assess effects performance ea ea tested dimacs benchmark problem random graph nodes density fig 
shows resulting surface average size largest cliques independent runs different parameter settings table lists corresponding data 
conclude results shown fig 

location information utilized guided mutation operator 
seen algorithm performs better phenomenon clearly indicates location information contribute positively performance algorithm 
global statistical information collected past contribution new probability vector decreases contribution old global statistical information increase 
fig 
shows algorithm worse suggests global statistical information collected past beneficial 
contributions main components main difference ea marchiori ea uses guided mutation operator uniform crossover operator generating new offspring 
ea strategy searching different areas different search phases 
natural question guided mutation partitioning search space positive contribution performance algorithm 
answer question compare performance ea algorithms 
marchiori uses uniform crossover operator generating offspring 
partitioning search space algorithm 
evolutionary algorithm uniform crossover partitioning search space ea ux ieee transactions evolutionary computation vol 
april table effect average size largest cliques different parameter setting fig 

comparisons ea ux ea 
ea uses uniform crossover guided mutation 
population size algorithms set 
set ea experimental results section iv 
independent runs algorithm performed 
average size largest cliques generation runs recorded algorithm 
fig 
plots evolution average size largest cliques algorithms 
seen ea ux performs better marchiori algorithm 
note main difference algorithms ea ux adopts strategy search space 
experiment supports claim partitioning search space improve performance algorithm 
fig 
shows ea outperforms ea ux indicates guided mutation operator performs better uniform crossover operator test problem 
results surprising uniform crossover operator uses location information selected parents guided mutation operator combines global statistical information location information parents 
ea generates offspring way umda uses uniform crossover 
observe figs 
performance ea worse 
shown behaviors ga uniform crossover umda similar case large populations observation suggests differences umda ga uniform crossover significant case small population 
uniform crossover hamming distance offspring parents offspring takes elements parent 
contract offspring generated umda far away members current population 
recorded number times step executed ea 
average step executed times times due larger clique step 
comparisons best evolutionary algorithm mcp reported far 
ea compared dimacs graphs 
graphs random graph size density mann steiner triple graph nodes edges graph size graph size hamming hamming graph keller keller graph nodes edges hat graphs size algorithms run independently times graph 
population size algorithms 
algorithms terminated calls repair operator 
set ea table ii shows experimental results including best size largest clique runs avg average size cliques runs std standard deviation sizes cliques runs dimacs size largest clique algorithms second dimacs challenges time run time seconds algorithm test result tailed test significance level alternative hypothesis mean size cliques obtained ea larger obtained 
absolute value statistic 
suggests ea better terms solution quality 
running time ea longer mainly due computational overheads guided mutation operator partitioning search space 
extra time clearly compensated quality solutions produced 
test results suggest ea outperforms graphs terms mean size example consider population containing probability vector umda learned population 
vector may sampled probability vector offspring hamming distance members current population 
impossible uniform crossover produce offspring 
zhang evolutionary algorithm guided mutation mcp table ii comparison results ea best size largest clique avg average size cliques std standard deviation sizes cliques dimacs size largest clique algorithms time run time seconds algorithm 
test test results cliques 
graph instances ea larger cliques 
table ii shows ea obtained best known results instances 
comparisons mimic probability model guided mutation operator ea variables treated independently 
part parent solution altered model multivariate dependencies processed extent random way 
advanced edas mimic fda boa identify utilize dependence relationships variables way 
edas seemingly powerful solving complicated optimization problem 
guided mutation advanced edas directly control similarity offspring best solutions far 
limited number dependence relationships considered edas due computational complexity 
may hinder pure edas solving hard problem 
case evolutionary algorithms local search hard problem distribution locally optimal solutions complicated approximation provided advanced edas may poor 
compared performances ea mimic algorithm ea uses mimic way generate new solutions 
mimic may simplest advanced edas considering variable dependencies 
computational complexity advanced edas prohibitively high solving large scale problems 
chose mimic comparison 
parameter setting ea section iv 
population size mimic set small populations may degrade performance advanced edas 
fair comparison mimic ea terminated calls repair operator 
algorithms run times graph 
experimental results table iii 
tailed test results significance level ieee transactions evolutionary computation vol 
april table iii comparison results ea mimic ea best size largest clique avg average size cliques std standard deviation sizes cliques dimacs size largest clique algorithms time run time seconds algorithm 
test test results table iii alternative hypothesis mean size cliques obtained ea larger obtained mimic 
instances ea better mimic terms quality solutions 
running time mimic longer ea mimic needs additional computational overheads searching optimal markovian chain generation 
conventional gas crossover mutation generating offspring edas sample offspring probability model 
crossover mutation location information parent solutions edas global information search space collected search process 
introduced guided mutation operator generating offspring evolutionary algorithms 
guided probability model characterizes distribution promising solutions search space guided mutation operator alters parent solution generate offspring 
combining global information location information parent guided mutation operator attempts generate promising solutions 
proposed ea hybrid evolutionary algorithm guided mutation mcp 
guided mutation operator ea adopts strategy searching different areas different search phases 
comparison ea best hybrid evolutionary algorithm mcp reported far 
experimental results show ea outperforms 
showed guided mutation operator partitioning search space contribute positively performance algorithm 
experimental results show ea performs better mimic mcp suggests location information ignored sufficient limited zhang evolutionary algorithm guided mutation mcp number dependence relationships solving hard optimization search problem 
pointed ea best metaheuristic mcp 
best heuristic problem may reactive local search battiti times faster ea find best known results dimacs graphs 
reactive local search complicated sophisticated ea main purpose study simple techniques guided mutation partitioning search space show improve performance evolutionary algorithm 
intend refine ea apply solve hard optimization search problems 
acknowledgment authors grateful dr marchiori sending code 
yao anonymous reviewers associate editor mills constructive comments suggestions 
glover heuristics integer programming surrogate constraints dec sci vol 
pp 

baluja population incremental learning method integrating genetic search function optimization competitive learning school comput 
sci carnegie mellon univ pittsburgh pa tech 
rep cmu cs 
equation response selection prediction evol 
comput vol 
pp 

mahnig rodriguez schemata distributions graphical models evolutionary optimization heuristics vol 
pp 

baluja davies fast probabilistic modeling combinatorial optimization proc 
th national th conf 
artif 
intell 
appl 
artif 
intell madison wi pp 

pelikan goldberg cant paz boa bayesian optimization algorithm proc 
genetic evol 
comput 
conf 
gecco banzhaf daida eiben garzon honavar smith eds pp 

zhang bayesian framework evolutionary computation proc 
congr 
evol 
comput pp 

thierens expanding discrete continuous edas idea proc 
parallel prob 
solving ppsn vi schoenauer deb rudolph yao lutton merelo 
schwefel eds paris france pp 

larra aga lozano estimation distribution algorithms new tool evolutionary computation 
norwell ma kluwer 
hastad clique hard approximate proc 
th annu 
symp 

comput 
sci 
burlington vt oct pp 

bomze pelillo maximum clique problem handbook combinatorial optimization 
du eds 
norwell ma kluwer vol 

rego scatter search algorithm maximum clique problem instituto politecnico de lisboa portugal tech 
rep 
carter park genetic algorithms finding large cliques experimental study comput 
sci 
dept boston univ boston ma tech 
rep bu cs 
park carter effectiveness genetic search combinatorial optimization comput 
sci 
dept boston univ boston ma tech 
rep bu cs 
yao individual population analysis hitting time population evolutionary algorithms ieee trans 
evol 
comput vol 
pp 
oct 
zhang stability fixed points limit models univariate marginal distribution algorithm factorized distribution algorithm ieee trans 
evol 
comput vol 

feb pp 

yao study drift analysis estimating computation time evolutionary algorithms natural comput vol 
pp 

analytic framework analyzing computation time evolutionary algorithms artif 
intell vol 
pp 

ck evolutionary heuristic maximum independent set problem proc 
st ieee conf 
evol 
comput pp 

genetic algorithm heuristic solving weighted maximum independent set equivalent problems oper 
res 
soc vol 
pp 

murthy parthasarathy sastry clique finding genetic approach proc 
st ieee conf 
evol 
comput pp 

bui hybrid genetic algorithm maximum clique problem proc 
th int 
conf 
genetic algorithms eshelman ed pittsburgh pa july pp 

object oriented implementation heuristic search methods graph coloring maximum clique satisfiability siam alg discr 
meth vol 
pp 

foster soule genetic algorithms find maximum cliques dept comput 
sci univ idaho moscow id tech 
rep lal 
marchiori simple heuristic genetic algorithm maximum clique problem proc 
acm symp 
appl 
comput pp 

genetic iterated multistart local search maximum clique problem applications evolutionary computing 
berlin germany springer verlag lncs pp 

glover laguna tabu search 
norwell ma kluwer 
lin kernighan effective heuristic algorithm traveling salesman problem oper 
res vol 
pp 

harik lobo goldberg compact genetic algorithm ieee trans 
evol 
comput vol 
pp 
nov 
eiben smith evolutionary computing 
berlin germany springer verlag 
de bonet isbell viola mimic finding optima estimating probability densities advances neural information processing systems mozer jordan eds ma mit press pp 

pelikan goldberg cant paz bayesian optimization algorithm population sizing time convergence illinois genetic algorithm lab univ illinois urbana champaign urbana il rep jan 
pe larra aga rosales rez ga eda hybrid evolutionary algorithm genetic estimation distribution algorithms lecture notes artificial intelligence ottawa canada may pp 

proc 
th int 
conf 
ind eng 
appl 
artif 
intell 
expert syst 
thierens scalability problem simple genetic algorithms evol 
comput vol 
pp 

battiti reactive local search maximum clique problem algorithmica vol 
pp 

zhang received sc 
degree mathematics university china sc 
degree applied mathematics ph information engineering university xi china respectively 
lecturer department computer science university essex colchester 
worked national laboratory parallel processing computing institute technology hong kong china hong kong polytechnic university hong kong german national research center information technology fraunhofer gesellschaft sankt augustin germany university manchester institute science technology manchester researcher 
main research areas evolutionary computation optimization neural networks data analysis applications 
ieee transactions evolutionary computation vol 
april sun received sc 
degree computational mathematics xi university xi china 
currently working ph degree computer science university essex colchester worked department computer science engineering chinese university hong kong research assistant 
worked department computer science university essex senior research officer 
main research areas evolutionary computation optimization metaheuristics telecommunication networks 
edward tsang received degree business administration chinese university hong kong sc 
ph degrees computer science university essex colchester respectively 
broad interest applied artificial intelligence particular computational finance scheduling heuristic search constraint satisfaction optimization 
currently professor computer science university essex colchester leads computational finance group constraint satisfaction optimization group 
deputy director centre computational finance economic agents interdisciplinary centre 
dr tsang chairs technical committee computational finance ieee computational intelligence society 
details essex ac uk csp edward 
