journal machine learning research submitted published latent dirichlet allocation david blei blei cs berkeley edu computer science division university california berkeley ca usa andrew ng ang cs stanford edu computer science department stanford university stanford ca usa michael jordan jordan cs berkeley edu computer science division department statistics university california berkeley ca usa editor john lafferty describe latent dirichlet allocation lda generative probabilistic model collections discrete data text corpora 
lda level hierarchical bayesian model item collection modeled finite mixture underlying set topics 
topic turn modeled infinite mixture underlying set topic probabilities 
context text modeling topic probabilities provide explicit representation document 
efficient approximate inference techniques variational methods em algorithm empirical bayes parameter estimation 
report results document modeling text classification collaborative filtering comparing mixture unigrams model probabilistic lsi model 

consider problem modeling text corpora collections discrete data 
goal find short descriptions members collection enable efficient processing large collections preserving essential statistical relationships useful basic tasks classification novelty detection summarization similarity relevance judgments 
significant progress problem researchers field information retrieval ir baeza yates ribeiro neto 
basic methodology proposed ir researchers text corpora methodology successfully deployed modern internet search engines reduces document corpus vector real numbers represents ratios counts 
popular tf idf scheme salton mcgill basic vocabulary words terms chosen document corpus count formed number occurrences word 
suitable normalization term frequency count compared inverse document frequency count measures number occurrences david blei andrew ng michael jordan 
blei ng jordan word entire corpus generally log scale suitably normalized 
result term document matrix columns contain tf idf values documents corpus 
tf idf scheme reduces documents arbitrary length fixed length lists numbers 
tf idf reduction appealing features notably basic identification sets words discriminative documents collection approach provides relatively small amount reduction description length reveals little way inter statistical structure 
address shortcomings ir researchers proposed dimensionality reduction techniques notably latent semantic indexing lsi deerwester 
lsi uses singular value decomposition matrix identify linear subspace space tf idf features captures variance collection 
approach achieve significant compression large collections 
furthermore deerwester argue derived features lsi linear combinations original tf idf features capture aspects basic linguistic notions synonymy polysemy 
substantiate claims regarding lsi study relative strengths weaknesses useful develop generative probabilistic model text corpora study ability lsi recover aspects generative model data papadimitriou 
generative model text clear adopt lsi methodology attempt proceed directly fitting model data maximum likelihood bayesian methods 
significant step forward regard hofmann probabilistic lsi plsi model known aspect model alternative lsi 
plsi approach describe detail section models word document sample mixture model mixture components multinomial random variables viewed representations topics word generated single topic different words document may generated different topics 
document represented list mixing proportions mixture components reduced probability distribution fixed set topics 
distribution reduced description associated document 
hofmann useful step probabilistic modeling text incomplete provides probabilistic model level documents 
plsi document represented list numbers mixing proportions topics generative probabilistic model numbers 
leads problems number parameters model grows linearly size corpus leads serious problems overfitting clear assign probability document outside training set 
see proceed plsi consider fundamental probabilistic assumptions underlying class dimensionality reduction methods includes lsi plsi 
methods bag words assumption order words document neglected 
language probability theory assumption exchangeability words document aldous 
stated formally methods assume documents exchangeable specific ordering documents corpus neglected 
classic representation theorem due de finetti establishes collection exchangeable random variables representation mixture distribution general infinite mixture 
wish consider exchangeable representations documents words need consider mixture models capture exchangeability words documents 
latent dirichlet allocation line thinking leads latent dirichlet allocation lda model current 
important emphasize assumption exchangeability equivalent assumption random variables independent identically distributed 
exchangeability essentially interpreted meaning conditionally independent identically distributed conditioning respect underlying latent parameter probability distribution 
conditionally joint distribution random variables simple factored marginally latent parameter joint distribution quite complex 
assumption exchangeability clearly major simplifying assumption domain text modeling principal justification leads methods computationally efficient exchangeability assumptions necessarily lead methods restricted simple frequency counts linear operations 
aim demonstrate current de finetti theorem seriously capture significant intra document statistical structure mixing distribution 
worth noting large number generalizations basic notion exchangeability including various forms partial exchangeability representation theorems available cases diaconis 
discuss current focuses simple bag words models lead mixture distributions single words unigrams methods applicable richer models involve mixtures larger structural units grams paragraphs 
organized follows 
section introduce basic notation terminology 
lda model section compared related latent variable models section 
discuss inference parameter estimation lda section 
illustrative example fitting lda data provided section 
empirical results text modeling text classification collaborative filtering section 
section presents 

notation terminology language text collections referring entities words documents corpora useful helps guide intuition particularly introduce latent variables aim capture notions topics 
important note lda model necessarily tied text applications problems involving collections data including data domains collaborative filtering content image retrieval bioinformatics 
section experimental results collaborative filtering domain 
formally define terms word basic unit discrete data defined item vocabulary indexed 

represent words unit basis vectors single component equal components equal zero 
superscripts denote components vth word vocabulary represented vector document sequence words denoted 
wn wn nth word sequence 
corpus collection documents denoted 
wm 
blei ng jordan wish find probabilistic model corpus assigns high probability members corpus assigns high probability similar documents 

latent dirichlet allocation latent dirichlet allocation lda generative probabilistic model corpus 
basic idea documents represented random mixtures latent topics topic characterized distribution words 
lda assumes generative process document corpus 
choose poisson 

choose dir 

words wn choose topic zn multinomial 
choose word wn wn zn multinomial probability conditioned topic zn 
simplifying assumptions basic model remove subsequent sections 
dimensionality dirichlet distribution dimensionality topic variable assumed known fixed 
second word probabilities parameterized matrix treat fixed quantity estimated 
poisson assumption critical follows realistic document length distributions needed 
furthermore note independent data generating variables 
ancillary variable generally ignore randomness subsequent development 
dimensional dirichlet random variable take values simplex vector lies simplex probability density simplex parameter vector components gamma function 
dirichlet convenient distribution simplex exponential family finite dimensional sufficient statistics conjugate multinomial distribution 
section properties facilitate development inference parameter estimation algorithms lda 
parameters joint distribution topic mixture set topics set words zn wn zn 
refer latent multinomial variables lda model topics exploit text oriented intuitions epistemological claims regarding latent variables utility representing probability distributions sets words 
latent dirichlet allocation graphical model representation lda 
boxes plates representing replicates 
outer plate represents documents inner plate represents repeated choice topics words document 
zn simply unique zi 
integrating summing obtain marginal distribution document zn wn zn 
product marginal probabilities single documents obtain probability corpus nd zn lda model represented probabilistic graphical model 
clear levels lda representation 
parameters parameters assumed sampled process generating corpus 
variables document level variables sampled document 
variables word level variables sampled word document 
important distinguish lda simple dirichlet multinomial clustering model 
classical clustering model involve level model dirichlet sampled corpus multinomial clustering variable selected document corpus set words selected document conditional cluster variable 
clustering models model restricts document associated single topic 
lda hand involves levels notably topic node sampled repeatedly document 
model documents associated multiple topics 
structures similar shown studied bayesian statistical modeling referred hierarchical models gelman precisely conditionally independent hierarchical models kass 
models referred parametric empirical bayes models term refers particular model structure methods estimating parameters model morris 
discuss section adopt empirical bayes approach estimating parameters simple implementations lda consider fuller bayesian approaches 
lda exchangeability blei ng jordan finite set random variables 
zn said exchangeable joint distribution invariant permutation 
permutation integers 
zn 

infinite sequence random variables infinitely exchangeable finite subsequence exchangeable 
de finetti representation theorem states joint distribution infinitely exchangeable sequence random variables random parameter drawn distribution random variables question independent identically distributed conditioned parameter 
lda assume words generated topics fixed conditional distributions topics infinitely exchangeable document 
de finetti theorem probability sequence words topics form zn wn zn random parameter multinomial topics 
obtain lda distribution documents eq 
marginalizing topic variables endowing dirichlet distribution 
continuous mixture unigrams lda model shown somewhat elaborate level models studied classical hierarchical bayesian literature 
marginalizing hidden topic variable understand lda level model 
particular form word distribution 
note random quantity depends 
define generative process document 
choose dir 

words wn choose word wn wn 
process defines marginal distribution document continuous mixture distribution wn wn mixture components mixture weights 
illustrates interpretation lda 
depicts distribution induced particular instance lda model 
note distribution simplex attained kv parameters exhibits interesting multimodal structure 
latent dirichlet allocation example density unigram distributions lda words topics 
triangle embedded plane simplex representing possible multinomial distributions words 
vertices triangle corresponds deterministic distribution assigns probability words midpoint edge gives probability words centroid triangle uniform distribution words 
points marked locations multinomial distributions topics surface shown top simplex example density simplex multinomial distributions words lda 

relationship latent variable models section compare lda simpler latent variable models text unigram model mixture unigrams plsi model 
furthermore unified geometric interpretation models highlights key differences similarities 
unigram model unigram model words document drawn independently single multinomial distribution illustrated graphical model 
wn 
blei ng jordan unigram mixture unigrams plsi aspect model graphical model representation different models discrete data 
mixture unigrams augment unigram model discrete random topic variable obtain mixture unigrams model nigam 
mixture model document generated choosing topic generating words independently conditional multinomial 
probability document wn 
estimated corpus word distributions viewed representations topics assumption document exhibits exactly topic 
empirical results section illustrate assumption limiting effectively model large collection documents 
contrast lda model allows documents exhibit multiple topics different degrees 
achieved cost just additional parameter parameters associated mixture unigrams versus parameters associated lda 
probabilistic latent semantic indexing probabilistic latent semantic indexing plsi widely document model hofmann :10.1.1.1.4458
plsi model illustrated posits document label word wn latent dirichlet allocation conditionally independent unobserved topic wn wn 
plsi model attempts relax simplifying assumption mixture unigrams model document generated topic 
sense capture possibility document may contain multiple topics serves mixture weights topics particular document important note dummy index list documents training set 
multinomial random variable possible values training documents model learns topic mixtures documents trained 
reason plsi defined generative model documents natural way assign probability previously unseen document 
difficulty plsi stems distribution indexed training documents number parameters estimated grows linearly number training documents 
parameters topic plsi model multinomial distributions size mixtures hidden topics 
gives kv km parameters linear growth linear growth parameters suggests model prone overfitting empirically overfitting serious problem see section 
practice tempering heuristic smooth parameters model acceptable predictive performance 
shown overfitting occur tempering popescul 
lda overcomes problems treating topic mixture weights parameter hidden random variable large set individual parameters explicitly linked training set 
described section lda defined generative model generalizes easily new documents 
furthermore kv parameters topic lda model grow size training corpus 
see section lda suffer overfitting issues plsi 
geometric interpretation way illustrating differences lda latent topic models considering geometry latent space seeing document represented geometry model 
models described unigram mixture unigrams plsi lda operate space distributions words 
distribution viewed point simplex call word simplex 
unigram model finds single point word simplex posits words corpus come corresponding distribution 
latent variable models consider points word simplex form sub simplex points call topic simplex 
note point topic simplex point word simplex 
different latent variable models topic simplex different ways generate document 
mixture unigrams model posits document points word simplex corners topic simplex chosen randomly words document drawn distribution corresponding point 
topic blei ng jordan topic topic simplex topic word simplex topic simplex topics embedded word simplex words 
corners word simplex correspond distributions word respectively probability 
points topic simplex correspond different distributions words 
mixture unigrams places document corners topic simplex 
plsi model induces empirical distribution topic simplex denoted lda places smooth distribution topic simplex denoted contour lines 
plsi model posits word training document comes randomly chosen topic 
topics drawn document specific distribution topics point topic simplex 
distribution document set training documents defines empirical distribution topic simplex 
lda posits word observed unseen documents generated randomly chosen topic drawn distribution randomly chosen parameter 
parameter sampled document smooth distribution topic simplex 
differences highlighted 
inference parameter estimation described motivation lda illustrated conceptual advantages latent topic models 
section turn attention procedures inference parameter estimation lda 
latent dirichlet allocation left graphical model representation lda 
right graphical model representation variational distribution approximate posterior lda 
inference key inferential problem need solve order lda computing posterior distribution hidden variables document unfortunately distribution intractable compute general 
normalize distribution marginalize hidden variables write eq 
terms model parameters function intractable due coupling summation latent topics 
shows function expectation particular extension dirichlet distribution represented special hypergeometric functions 
bayesian context censored discrete data represent posterior setting random parameter 
posterior distribution intractable exact inference wide variety approximate inference algorithms considered lda including laplace approximation variational approximation markov chain monte carlo jordan 
section describe simple convexity variational algorithm inference lda discuss alternatives section 
variational inference basic idea convexity variational inference jensen inequality obtain adjustable lower bound log likelihood jordan 
essentially considers family lower bounds indexed set variational parameters 
variational parameters chosen optimization procedure attempts find tightest possible lower bound 
simple way obtain tractable family lower bounds consider simple modifications original graphical model edges nodes removed 
consider particular lda model shown left 
problematic coupling blei ng jordan arises due edges dropping edges nodes endowing resulting simplified graphical model free variational parameters obtain family distributions latent variables 
family characterized variational distribution zn dirichlet parameter multinomial parameters 
free variational parameters 
having specified simplified family probability distributions step set optimization problem determines values variational parameters 
show appendix desideratum finding tight lower bound log likelihood translates directly optimization problem 
optimizing values variational parameters minimizing kullback leibler kl divergence variational distribution true posterior 
minimization achieved iterative fixed point method 
particular show appendix computing derivatives kl divergence setting equal zero obtain pair update equations ni exp eq log ni 
show appendix expectation multinomial update computed follows eq log derivative log function computable taylor approximations abramowitz stegun 
eqs 
appealing intuitive interpretation 
dirichlet update posterior dirichlet expected observations taken variational distribution zn 
multinomial update akin bayes theorem zn wn wn zn zn zn approximated exponential expected value logarithm variational distribution 
important note variational distribution conditional distribution varying function occurs optimization problem eq 
conducted fixed yields optimizing parameters function write resulting variational distribution dependence explicit 
variational distribution viewed approximation posterior distribution 
language text optimizing parameters document specific 
particular view dirichlet parameters providing representation document topic simplex 
initialize ni initialize repeat latent dirichlet allocation ni exp ti normalize sum 
convergence variational inference algorithm lda 
summarize variational inference procedure appropriate starting points pseudocode clear iteration variational inference lda requires operations 
empirically find number iterations required single document order number words document 
yields total number operations roughly order parameter estimation section empirical bayes method parameter estimation lda model see section fuller bayesian approach 
particular corpus documents wm wish find parameters maximize marginal log likelihood data log wd 
described quantity computed tractably 
variational inference provides tractable lower bound log likelihood bound maximize respect 
find approximate empirical bayes estimates lda model alternating variational em procedure maximizes lower bound respect variational parameters fixed values variational parameters maximizes lower bound respect model parameters 
provide detailed derivation variational em algorithm lda appendix 
derivation yields iterative algorithm 
step document find optimizing values variational parameters 
done described previous section 

step maximize resulting lower bound log likelihood respect model parameters 
corresponds finding maximum likelihood estimates expected sufficient statistics document approximate posterior computed step 
blei ng jordan graphical model representation smoothed lda model 
steps repeated lower bound log likelihood converges 
appendix show step update conditional multinomial parameter written analytically nd dn 
show step update dirichlet parameter implemented efficient newton raphson method hessian inverted linear time 
smoothing large vocabulary size characteristic document corpora creates serious problems sparsity 
new document contain words appear documents training corpus 
maximum likelihood estimates multinomial parameters assign zero probability words zero probability new documents 
standard approach coping problem smooth multinomial parameters assigning positive probability vocabulary items observed training set jelinek 
laplace smoothing commonly essentially yields mean posterior distribution uniform dirichlet prior multinomial parameters 
unfortunately mixture model setting simple laplace smoothing longer justified maximum posteriori method implemented practice cf 
nigam 
fact placing dirichlet prior multinomial parameter obtain intractable posterior mixture model setting reason obtains intractable posterior basic lda model 
proposed solution problem simply apply variational inference methods extended model includes dirichlet smoothing multinomial parameter 
lda setting obtain extended graphical model shown 
treat random matrix row mixture component assume row independently drawn exchangeable dirichlet distribution 
extend inference procedures treat random variables endowed posterior distribution 
exchangeable dirichlet simply dirichlet distribution single scalar parameter 
density dirichlet eq 
component 
latent dirichlet allocation conditioned data 
move empirical bayes procedure section consider fuller bayesian approach lda 
consider variational approach bayesian inference places separable distribution random variables attias dir qd zd qd variational distribution defined lda eq 

easily verified resulting variational inference procedure yields eqs 
update equations variational parameters respectively additional update new variational parameter nd dn iterating equations convergence yields approximate posterior distribution left hyperparameter exchangeable dirichlet hyperparameter 
approach setting hyperparameters approximate empirical bayes variational em find maximum likelihood estimates parameters marginal likelihood 
procedures described appendix 
example section provide illustrative example lda model real data 
data documents subset trec ap corpus harman 
removing standard list words em algorithm described section find dirichlet conditional multinomial parameters topic lda model 
top words resulting multinomial distributions illustrated top 
hoped distributions capture underlying topics corpus named topics 
emphasized section advantages lda related latent variable models provides defined inference procedures previously unseen documents 
illustrate lda works performing inference held document examining resulting variational posterior parameters 
bottom document trec ap corpus parameter estimation 
algorithm section computed variational posterior dirichlet parameters article variational posterior multinomial parameters word article 
recall ith posterior dirichlet parameter approximately ith prior dirichlet parameter plus expected number words generated ith topic see eq 

prior dirichlet parameters subtracted posterior dirichlet parameters indicate expected number words allocated topic particular document 
example article bottom close topics significantly larger mean 
looking corresponding distributions words identifies topics mixed form document top 
blei ng jordan insight comes examining parameters 
distributions approximate zn tend peak possible topic values 
article text words color coded values ith color qn 
illustration identify different topics mixed document text 
demonstrating power lda posterior analysis highlights limitations 
particular bag words assumption allows words generated topic william randolph hearst foundation allocated different topics 
overcoming limitation require form extension basic lda model particular relax bag words assumption assuming partial exchangeability word sequences 

applications empirical results section discuss empirical evaluation lda problem domains document modeling document classification collaborative filtering 
mixture models expected complete log likelihood data local maxima points mixture components equal 
avoid local maxima important initialize em algorithm appropriately 
experiments initialize em seeding conditional multinomial distribution documents reducing effective total length words smoothing vocabulary 
essentially approximation scheme described heckerman meila 
document modeling trained number latent variable models including lda text corpora compare generalization performance models 
documents corpora treated unlabeled goal density estimation wish achieve high likelihood held test set 
particular computed perplexity held test set evaluate models 
perplexity convention language modeling monotonically decreasing likelihood test data equivalent inverse geometric mean word likelihood 
lower perplexity score indicates better generalization performance 
formally test set documents perplexity perplexity exp log wd md nd experiments corpus scientific abstracts elegans community avery containing abstracts unique terms subset trec ap corpus containing newswire articles unique terms 
cases held data test purposes trained models remaining 
preprocessing data 
note simply perplexity merit comparing models 
models compare unigram bag words models discussed interest information retrieval context 
attempting language modeling enterprise require examine trigram higher order models 
note passing extensions lda considered involve dirichlet multinomial trigrams unigrams 
leave exploration extensions language modeling 
latent dirichlet allocation william randolph hearst foundation give lincoln center metropolitan opera new york school 
board felt real opportunity mark performing arts act bit important traditional areas support health medical research education social services hearst foundation president randolph hearst said monday announcing 
lincoln center share new building house young artists provide new public facilities 
metropolitan opera new york receive 
school music performing arts taught get 
hearst foundation leading supporter lincoln center consolidated corporate fund usual annual donation 
example article ap corpus 
color codes different factor word generated 
perplexity perplexity blei ng jordan number topics smoothed unigram smoothed 
unigrams lda fold plsi number topics smoothed unigram smoothed 
unigrams lda fold plsi perplexity results top ap bottom corpora lda unigram model mixture unigrams plsi 
latent dirichlet allocation num 
topics perplexity mult 

perplexity plsi table overfitting mixture unigrams plsi models ap corpus 
similar behavior observed corpus reported 
removed standard list words corpus 
ap data removed words occurred 
compared lda unigram mixture unigrams plsi models described section 
trained hidden variable models em exactly stopping criteria average change expected log likelihood 
plsi model mixture unigrams suffer serious overfitting issues different reasons 
phenomenon illustrated table 
mixture unigrams model overfitting result peaked posteriors training set phenomenon familiar supervised setting model known naive bayes model rennie :10.1.1.1.6194
leads nearly deterministic clustering training documents step determine word probabilities mixture component step 
previously unseen document may best fit resulting mixture components probably contain word occur training documents assigned component 
words small probability causes perplexity new document explode 
increases documents training corpus partitioned finer collections induce words small probabilities 
mixture unigrams alleviate overfitting variational bayesian smoothing scheme section 
ensures words probability mixture component 
plsi case hard clustering problem alleviated fact document allowed exhibit different proportion topics 
plsi refers training documents different overfitting problem arises due dimensionality parameter 
reasonable approach assigning probability previously unseen document marginalizing wn 
essentially integrating empirical distribution topic simplex see 
method inference theoretically sound causes model overfit 
topic distribution components close zero topics appear document 
certain words small probability estimates blei ng jordan mixture component 
determining probability new document marginalization training documents exhibit similar proportion topics contribute likelihood 
training document topic proportions word small probability constituent topics cause perplexity explode 
gets larger chance training document exhibit topics cover words new document decreases perplexity grows 
note plsi overfit quickly respect mixture unigrams 
overfitting problem essentially stems restriction document exhibit topic proportions seen training documents 
constraint free choose proportions topics new document 
alternative approach folding heuristic suggested hofmann ignores parameters dnew 
note gives plsi model unfair advantage allowing parameters test data 
lda suffers problems 
plsi document exhibit different proportion underlying topics 
lda easily assign probability new document heuristics needed new document endowed different set topic proportions associated documents training corpus 
presents perplexity model corpora different values plsi model mixture unigrams suitably corrected overfitting 
latent variable models perform better simple unigram model 
lda consistently performs better models 
document classification text classification problem wish classify document mutually exclusive classes 
classification problem may wish consider generative approaches discriminative approaches 
particular lda module class obtain generative model classification 
interest lda discriminative framework focus section 
challenging aspect document classification problem choice features 
treating individual words features yields rich large feature set joachims 
way reduce feature set lda model dimensionality reduction 
particular lda reduces document fixed set real valued features posterior dirichlet parameters associated document 
interest see discriminatory information lose reducing document description parameters 
conducted binary classification experiments reuters dataset 
dataset contains documents words 
experiments estimated parameters lda model documents true class label 
trained support vector machine svm low dimensional representations provided lda compared svm svm trained word features 
svmlight software package joachims compared svm trained word features trained features induced topic lda model 
note reduce feature space percent case 
accuracy latent dirichlet allocation proportion data training lda features word features accuracy lda features word features proportion data training classification results binary classification problems reuters dataset different proportions training data 
graph earn vs earn 
graph grain vs grain 
predictive perplexity lda fold plsi smoothed 
unigrams number topics results collaborative filtering eachmovie data 
shows results 
see little reduction classification performance lda features cases performance improved lda features 
results need suggest topic representation provided lda may useful fast filtering algorithm feature selection text classification 
collaborative filtering blei ng jordan final experiment uses eachmovie collaborative filtering data 
data set collection users indicates preferred movie choices 
user movies chosen analogous document words document respectively 
collaborative filtering task follows 
train model fully observed set users 
unobserved user shown movies preferred user asked predict held movie different algorithms evaluated likelihood assign held movie 
precisely define predictive perplexity test users predictive perplexity exp log wd nd wd nd restricted eachmovie dataset users positively rated movies positive rating stars 
divided set users training users testing users 
mixture unigrams model probability movie set observed movies obtained posterior distribution topics 
plsi model probability held movie equation computed folding previously seen movies 
lda model probability held movie integrating posterior dirichlet variational inference method described section 
note quantity efficient compute 
interchange sum integral sign compute linear combination dirichlet expectations 
vocabulary movies find predictive perplexities illustrated 
mixture unigrams model plsi corrected overfitting best predictive perplexities obtained lda model 

discussion described latent dirichlet allocation flexible generative probabilistic model collections discrete data 
lda simple exchangeability assumption words topics document realized straightforward application de finetti representation theorem 
view lda dimensionality reduction technique spirit lsi proper underlying generative probabilistic semantics sense type data models 
exact inference intractable lda large suite approximate inference algorithms inference parameter estimation lda framework 
simple convexity variational approach inference showing yields fast latent dirichlet allocation algorithm resulting reasonable comparative performance terms test set likelihood 
approaches considered include laplace approximation higher order variational techniques monte carlo methods 
particular kappen general methodology converting low order variational lower bounds higher order variational bounds 
possible achieve higher accuracy dispensing requirement maintaining bound minka lafferty shown improved inferential accuracy obtained lda model higher order variational technique known expectation propagation 
griffiths steyvers markov chain monte carlo algorithm lda 
lda simple model view competitor methods lsi plsi setting dimensionality reduction document collections discrete corpora intended illustrative way probabilistic models scaled provide useful inferential machinery domains involving multiple levels structure 
principal advantages generative models lda include modularity extensibility 
probabilistic module lda readily embedded complex model property possessed lsi 
pairs lda modules model relationships images corresponding descriptive captions blei jordan 
numerous possible extensions lda 
example lda readily extended continuous data non multinomial data 
case mixture models including finite mixture models hidden markov models emission probability wn zn contributes likelihood value inference procedures lda likelihoods readily substituted place 
particular straightforward develop continuous variant lda gaussian observables place multinomials 
simple extension lda comes allowing mixtures dirichlet distributions place single dirichlet lda 
allows richer structure latent topic space particular allows form document clustering different clustering achieved shared topics 
variety extensions lda considered distributions topic variables elaborated 
example arrange topics time series essentially relaxing full exchangeability assumption partial exchangeability 
consider partially exchangeable models condition exogenous variables example topic distribution conditioned features paragraph sentence providing powerful text model information obtained parser 
supported national science foundation nsf iis multidisciplinary research program department defense muri 
andrew ng david blei additionally supported fellowships microsoft 
abramowitz stegun editors 
handbook mathematical functions 
dover new york 
blei ng jordan aldous 
exchangeability related topics 
cole de probabilit de saint flour xiii pages 
springer berlin 
attias 
variational bayesian framework graphical models 
advances neural information processing systems 
avery 
genetic center bibliography 

url elegans edu wli baeza yates ribeiro neto 
modern information retrieval 
acm press new york 
blei jordan 
modeling annotated data 
technical report ucb csd berkeley computer science division 
de finetti 
theory probability 
vol 

john wiley sons chichester 
reprint translation 
deerwester dumais landauer furnas harshman 
indexing latent semantic analysis 
journal american society information science 
diaconis 
progress de finetti notions exchangeability 
bayesian statistics valencia pages 
oxford univ press new york 

multiple hypergeometric functions probabilistic interpretations statistical uses 
journal american statistical association 
jiang kadane 
bayesian methods censored categorical data 
journal american statistical association 
gelman carlin stern rubin 
bayesian data analysis 
chapman hall london 
griffiths steyvers 
probabilistic approach semantic representation 
proceedings th annual conference cognitive science society 
harman 
overview text retrieval conference trec 
proceedings text retrieval conference trec pages 
heckerman meila 
experimental comparison clustering initialization methods 
machine learning 
hofmann 
probabilistic latent semantic indexing 
proceedings second annual international sigir conference 
jelinek 
statistical methods speech recognition 
mit press cambridge ma 
joachims 
making large scale svm learning practical 
advances kernel methods support vector learning 
press 
jordan editor 
learning graphical models 
mit press cambridge ma 
latent dirichlet allocation jordan ghahramani jaakkola saul 
variational methods graphical models 
machine learning 
kass 
approximate bayesian inference conditionally independent hierarchical models parametric empirical bayes models 
journal american statistical association 
kappen 
general lower bounds computer generated higher order expansions 
uncertainty artificial intelligence proceedings eighteenth conference 
minka 
estimating dirichlet distribution 
technical report 
minka lafferty 
expectation propagation generative aspect model 
uncertainty artificial intelligence uai 
morris 
parametric empirical bayes inference theory applications 
journal american statistical association 
discussion 
nigam lafferty mccallum 
maximum entropy text classification 
ijcai workshop machine learning information filtering pages 
nigam mccallum thrun mitchell 
text classification labeled unlabeled documents em 
machine learning 
papadimitriou tamaki raghavan vempala 
latent semantic indexing probabilistic analysis 
pages 
popescul ungar pennock lawrence 
probabilistic models unified collaborative content recommendation sparse data environments 
uncertainty artificial intelligence proceedings seventeenth conference 
rennie 
improving multi class text classification naive bayes 
technical report 

maximum likelihood estimation dirichlet distributions 
journal computation simulation 
salton mcgill editors 
modern information retrieval 
mcgraw hill 
appendix inference parameter estimation appendix derive variational inference procedure eqs 
parameter maximization procedure conditional multinomial eq 
dirichlet 
deriving useful property dirichlet distribution 
computing log blei ng jordan need compute expected value log single probability component dirichlet arises repeatedly deriving inference parameter estimation procedures lda 
value easily computed natural parameterization exponential family representation dirichlet distribution 
recall distribution exponential family written form exp natural parameter sufficient statistic log normalization factor 
write dirichlet form exponentiating log eq 
exp log log log form immediately see natural parameter dirichlet sufficient statistic log furthermore general fact derivative log normalization factor respect natural parameter equal expectation sufficient statistic obtain log digamma function derivative log gamma function 
newton raphson methods hessian special structure section describe linear algorithm usually cubic newton raphson optimization method 
method maximum likelihood estimation dirichlet distribution minka 
newton raphson optimization technique finds stationary point function iterating new old old old hessian matrix gradient respectively point 
general algorithm scales due matrix inversion 
hessian matrix form diag diag defined diagonal matrix elements vector diagonal apply matrix inversion lemma obtain diag diag diag multiplying gradient obtain ith component gi hi latent dirichlet allocation kj observe expression depends values hi gi yields newton raphson algorithm linear time complexity 
variational inference section derive variational inference algorithm described section 
recall involves variational distribution zn surrogate posterior distribution variational parameters set optimization procedure describe 
jordan 
bounding log likelihood document jensen inequality 
omitting parameters simplicity log log log log logq eq log eq logq 
see jensen inequality provides lower bound log likelihood arbitrary variational distribution 
easily verified difference left hand side right hand side eq 
kl divergence variational posterior probability true posterior probability 
letting denote right hand side eq 
restored dependence variational parameters notation log 
shows maximizing lower bound respect equivalent minimizing kl divergence variational posterior probability true posterior probability optimization problem earlier eq 

expand lower bound factorizations eq log eq log eq log eq logq eq logq 
blei ng jordan expand eq 
terms model parameters variational parameters 
lines expands terms bound log log ni log log ni log ni eq 

log sections show maximize lower bound respect variational parameters 
variational multinomial maximize eq 
respect ni probability nth word generated latent topic observe constrained maximization ni 
form lagrangian isolating terms contain ni adding appropriate lagrange multipliers 
iv appropriate 
recall wn vector size exactly component equal select unique ni ni ni log iv ni log ni ni dropped arguments simplicity subscript ni denotes retained terms function ni 
derivatives respect ni obtain ni log iv log ni 
setting derivative zero yields maximizing value variational parameter ni cf 
eq 
ni iv exp 
variational dirichlet latent dirichlet allocation maximize eq 
respect ith component posterior dirichlet parameter 
terms containing simplifies log log ni ni log log 
take derivative respect ni setting equation zero yields maximum ni 
eq 
depends variational multinomial full variational inference requires alternating eqs 
bound converges 
parameter estimation final section consider problem obtaining empirical bayes estimates model parameters 
solve problem variational lower bound surrogate intractable marginal log likelihood variational parameters fixed values variational inference 
obtain approximate empirical bayes estimates maximizing lower bound respect model parameters 
far considered log likelihood single document 
assumption exchangeability documents log likelihood corpus 
wm sum log likelihoods individual documents variational lower bound sum individual variational bounds 
remainder section abuse notation total variational bound indexing document specific terms individual bounds summing documents 
recall section approach finding empirical bayes estimates variational em procedure 
variational step discussed appendix maximize bound respect variational parameters 
step describe section maximize bound respect model parameters 
procedure viewed coordinate ascent conditional multinomials blei ng jordan maximize respect isolate terms add lagrange multipliers nd dn log take derivative respect set zero find dirichlet terms contain log nd dn log derivative respect gives 
di di derivative depends iterative method find maximal 
particular hessian form eq 
invoke linear time newton raphson algorithm described appendix 
note algorithm find empirical bayes point estimate scalar parameter exchangeable dirichlet smoothed lda model section 

