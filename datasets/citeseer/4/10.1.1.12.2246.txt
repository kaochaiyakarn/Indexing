extracting hidden context michael harries claude sammut department arti cial intelligence school computer science engineering university new south wales sydney australia 
mail cse unsw edu au kim horn rmb australia limited level underwood house pitt street sydney australia 
mail kim rmb com au unsw cse tr november university new south wales school computer science engineering university new south wales sydney australia submitted machine learning journal special issue context sensitive learning 
concept drift due hidden changes context complicates learning domains including nancial prediction medical diagnosis network performance 
existing machine learning approaches problem incremental learning line paradigm 
batch line learners tend ine ective domains hidden changes context assume training set homogeneous 
line meta learning approach identi cation hidden context 
new approach existing batch learner process contextual clustering identify stable hidden contexts associated context speci locally stable concepts 
approach broadly applicable extraction context re ected time spacial attributes 
algorithms approach evaluated 
successful application approach complex control task 
prediction real world domains complicated potentially unstable underlying phenomena 
financial market behaviour instance change dramatically changes contract prices interest rates ation rates budget announcements political world events 
concept de nitions may learned context invalid new context 
concept drift due changes context directly re ected attributes 
changes context re ected attribute said hidden 
hidden changes context cause problems predictive approach assumes concept stability 
machine learning approaches broadly categorised batch incremental 
batch systems learn line examining large collection instances en masse form single concept 
change concept de nition new observations processed 
common approach learning domains hidden changes context incremental learning approach importance older items progressively decayed 
popular implementation originally window instances concept updates derived 
examples approach include 
swift adaption changes context achieved dynamically varying window size response changes accuracy concept complexity 
domains context expected change earlier contexts hold time 
contexts repeat domains nancial prediction dynamic control represented data mining tasks 
domains prediction accuracy improved storing knowledge past contexts re 
flora addresses domains contexts recur storing retrieving concepts appear stable learner traverses series input data 
situations constraint learn incrementally 
example organisations maintain large data bases historical data prime targets data mining 
data bases may hold instances belong number contexts context explicitly recorded 
data bases incorporate time essential attribute example nancial records stock market price data 
interest mining datasets nature suggests need systems learn global concepts sensitive hidden contexts 
systems flora show line recognition stable concepts useful line prediction 
batch line learning domains hidden changes context extensively explored 
alternative line learning domains hidden changes context examine data en masse attempt directly identify concepts associated stable hidden contexts 
potential bene ts listed 
context speci known local concepts part multiple model line predictive system 
local concepts veri ed experts improve domain understanding 
identi ed hidden contexts reasoned allowing global concept augmented expectations hidden context duration order stability 
identi ed hidden contexts provide target characteristics selecting additional attributes outside world part iterative data mining process 
article presents splice meta learning system implements context sensitive batch learning approach 
splice designed identify intervals stable hidden context induce re ne local concepts associated hidden contexts 
proceed describing existing machine learners detecting changes context 
rst implementation splice evaluated 
shortcomings method discussed new method splice designed improve shortcomings evaluated 
application splice complex task 
identifying context change domains hidden changes context time differentiate hidden contexts 
existing machine learning approaches domains explicitly represent time assume current context captured focusing examples 
implication hidden context re ected contiguous intervals time 
example attempt build system predict changes stock market produce decision tree 
year year attribute true market rising attribute false market falling year attribute true market rising attribute false market falling tree contains embedded knowledge intervals time attribute predictive onward attribute predictive 
time case year monotonically increasing attribute classi cation decision tree attribute domain expected recurring hidden context information prior interval time valuable 
decision tree example information changes context 
de ne context context attribute values tend stable contiguous intervals attribute known environmental attribute 
ability decision trees capture context associated fact decision tree algorithms form context sensitive feature selection 
number machine learning algorithms regarded including decision tree algorithms rule induction algorithms ilp systems 
systems produce concepts containing local information context 
contiguous intervals time re ect hidden attribute context call time environmental attribute 
environmental attribute restricted time ordinal attribute instances hidden context liable contiguous 
restriction principle dimension 
alternatives time environmental attributes dimensions space space time combinations 
environmental attribute utilise machine learning algorithm gain information hidden changes context 
accuracy change points dependent hidden context duration number di erent contexts complexity local concept noise 
identi ed context change points expected contain errors types noise serial correlation errors form additional incorrect change points 
errors due repetition tests time di erent parts concept 
take form group values clustered hidden changes context 
errors omission changes context missed altogether 
initial set identi ed context changes re ned contextual clustering 
contextual clustering combines similar intervals dataset similarity intervals degree partial model accurate intervals 
splice splice splice designed recognise stable context extract local concepts domains hidden changes context 
splice ordinal attribute environmental attribute order preserve clarity discussion substituted time broader term environmental attribute 
splice meta level algorithm incorporates existing batch learner 
study underlying learner decision tree learner modi cations 
underlying learner splice principle replaced machine learner able provide splits time 
expect underlying learner deal noise splice need mechanism deal noise directly 
splice algorithm detailed 
consists stages 
partition dataset 
perform contextual clustering 
learn local concepts examine turn 
partition dataset 
splice rst uses underlying learner build initial concept data set 
splice uses initial concept decision tree 
learning concept description domain including time identify splits time important concept description 
splits interpreted possible change context 
split time extracted initial concept de ne intervals dataset associated fragments initial concept termed partial concepts 
partial concept consists rules embedded leaves original decision tree act examples interval environmental attribute 
perform contextual clustering 
stage attempt cluster intervals identi ed 
splice determines accuracy partial concept examples interval error rate combination partial concept interval recorded local accuracy matrix 
partial concept considered cover interval data set error rate percentage classifying interval initial concept reused shifting data series associated interval relevant time values partial concept 
splice input required algorithm data set environmental attribute 
threshold accuracy parameter possible range 
stage partition dataset batch learner classify initial data set produce initial concept 
extract tests time identi ed initial concept 
tests time partition dataset intervals 
tests time partition initial concept partial concepts 
partial concepts fragments initial concept associated particular interval time 
stage perform contextual clustering evaluate accuracy partial concept interval data 
rate partial concept coverage data set 
coverage total number examples intervals classi ed partial concept better threshold accuracy create ordered set partial concepts 
empty select best partial concept create new cluster covered intervals 
intervals cluster remove associated partial concept stage learn local concepts apply batch learner contextual cluster order learn new local concept 
context delineated time boundaries cluster 
splice output consists local concepts produced 
splice algorithm splice performance accuracy threshold parameter default setting 
partial concept rated terms data set coverage 
number instances intervals data set covers 
ordered set partial concepts created 
clustering procedure operates follows 
partial concept highest coverage selected set intervals covers form new cluster 
partial concepts associated intervals removed set step repeated best candidate concept set empty 
learn local concepts 
underlying learner learn local concept contextual cluster previous stage 
important stage environmental attribute time included attribute set 
splice able exploit recurring contexts improved local concept quality building larger combined data sets 
splice performance section rst provides arti cial domain splice evaluated 
continues walk splice algorithm problem drawn sample domain describes investigations splice performance prediction task splice accuracy local concept identi cation 
stagger data set data sets experiments evaluating stagger subsequently 
approach underlying philosophy di erent allows comparison results 
domain chosen arti cial program generate data 
program allows control recurrence contexts factors noise duration 
domain attributes time size colour shape 
time treated continuous attribute 
size possible values small medium large 
colour possible values red green blue 
shape possible values circular triangular square 
experiments noise implies class randomly selected probability ofn 
method generating noise chosen consistent 
splice performance table local accuracy matrix partial concept error individual intervals 
interval range partial concepts program randomly generates series examples attribute space 
example unique time stamp boolean classi cation target concepts 
target concepts 
size small colour red 
colour green shape circular 
size medium size large arti cial contexts created xing target concepts stagger concepts preset intervals data series 
splice walk splice applied simple dataset recurring concepts 
training set consists stagger concepts instances instances instances repeating instances instances instances 
noise applied data set 
partition dataset 
initial concept decision tree generated succinct easy interpret 
contain substantial amount information past contexts changes context 
splits identi ed time initial concept de ne partial concepts intervals data set 
splice performance size small time time colour blue colour red time time time time colour green time time shape square shape circular triangular time time time time size medium large time time time time time colour green colour red blue time time shape square triangular shape circular time time time colour green colour red blue shape square triangular shape circular decision tree time splice performance current best partial concept data items covered context context context context context local concept learnt colour green blue colour red size small size medium large equals target concept size small colour red current best partial concept data items covered context context context context local concept learnt size small size medium large equals target concept size medium size large current best partial concept data items covered context context context context local concept learnt colour green colour red blue shape square triangular shape circular size small size medium large close target concept correct current best partial concept data items covered context context context context local concept learnt colour green colour red blue shape square triangular shape circular equals target concept colour green shape circular annotated extract splice log splice performance contextual clustering 
partial concept evaluated intervals data set 
table local accuracy matrix lam shows error rate achieved partial concept instances drawn interval 
instance partial concept error rate data set drawn interval error rate data set drawn interval 
intervals shown lam avery brief duration shown range column result spurious splits time initial concept 
lam input clustering operation local concepts 
contextual clusters produce local concepts shown 
rst local concepts generated splice able successfully identify combine non contiguous intervals context 
results run target concepts successfully identi ed additional incorrect local concept identi ed 
splice correctly identify target concepts minimise number incorrectly identi ed concepts 
task splice correctly identi ed concept times trials concept times trials concept times trials 
splice identi ed average incorrect spurious concepts trial 
identi cation local concepts provide utility 
pressing question application local concepts prediction task 
prediction splice vs splice local concepts ectively line prediction domain hidden changes context 
experiment compares accuracy splice trained data set containing changes hidden context 
training resulting concepts prediction similar data set 
provides baseline performance task trained attribute time 
comparison altogether fair designed domains hidden changes context 
training set consisted concepts instances instances instances 
test set consisted concepts instances instances instances repeated instances instances instances 
experiments reported splice run threshold accuracy parameter set default 
noted underlying learner run default pruning parameters sub setting 
splice performance correct splice test series splice comparison trained noise apply local concepts identi ed splice prediction purposes necessary devise method selecting relevant local concepts 
trivial problem purposes experiment arbitrarily chose simple method 
classi cation accuracy local concept examples recorded 
accurate concept predicting class example 
ties accuracy solved randomly selecting local concepts 
rst case classi ed randomly selected local concept 
results figures show average classi cation success rates levels noise splice randomly generated training test sets 
noise generated training set 
shows splice successfully identi ed local concepts training set correct local concept successfully selected prediction purposes better cases 
extreme dips accuracy contexts change ect local concept selection method 
performs relatively concept accuracy approximately 
concepts correctly classi es cases 
noise increases performance splice gradually declines 
shows noise worst result achieved splice classi cation accuracy concept 
hand classifying approximately accuracy achieved gure 
predictive stability range noise testament stability situations 
splice performance correct splice test series splice comparison trained noise task similar line learning task tackled flora stagger 
combination splice simple strategy selection current local concept ective simple context sensitive prediction task 
selection mechanism assumes local concepts correct splice immediately moves maximum accuracy new local concept 
similar domain flora family particular flora learner designed exploit recurring context appear reach level accuracy splice line learning method flora requires time fully re ect changes context 
comparison problematic number reasons 
splice advantage rst seeing training set containing instances context classify correct assumption possible contexts seen 
iterative learners advantage continuous feedback unconstrained updating concepts 
splice feedback constrained current local concepts adaptation feedback 
splice learnt local concept second chance 
complex real world domains bene cial combination splice adaptive line learner 
experiment looked prediction single concept duration 
splice perform di erent levels noise concept duration 
splice performance correctly identified noise noise noise noise duration target concepts splice identi cation concept ects noise duration splice splice able correctly induce stagger concepts range noise context duration conditions 
splice exploits longer concept durations set negative ects additional noise 
splice trained randomly generated training set containing examples stagger concepts 
set local concepts learnt splice assessed correctness target concept 
results show proportion correct local concept identi cations achieved average number incorrect local concepts identi ed 
training sets generated range concept duration noise 
concept duration corresponds number instances active 
duration ranges instances instances 
noise ranges 
training set consists concept instances concept instances concept instances duration combination noise level concept duration repeated times 
results figures show accuracy splice correctly identifying target concept varying levels concept duration noise 
domain splice behaved graceful degradation performance noise levels increase 
concept duration reduces negative ect noise 
shows number incorrect concepts learnt splice di erent levels noise training concept duration 
splice behaved showing graceful degradation performance increased noise bounded numbers incorrect concepts learnt 
results show domain splice behaved changing levels noise duration target concept 
splice able take advantage additional concept duration order minimise splice performance correctly identified correctly identified incorrect concepts generated noise noise noise noise duration target concepts splice identi cation concept noise noise noise noise duration target concepts splice identi cation concept noise noise noise noise duration target concepts splice incorrect concepts identi ed splice ect noise 
noise levels number spurious concepts identi ed fell reasonable bounds 
results splice promising quality recognition splice largely dependent accuracy partitions partitioning 
words splice restricted domains partitioning ective 
alternate algorithm splice designed domains high levels noise context repetition 
splice splice dependent partitioning ability ofthe underlying algorithm correctly detect changes context 
di culty inducing correct classi er related number regions peaks described 
measure domain average information entropy concept relevant attributes 
drawing de nitions domain di culty anticipate algorithm context changes domain characteristics increase context repetition 
irrelevant attributes 
noise 
splice designed minimise impact poor initial partitioning 
splice algorithm detailed 
splice begins partitioning data set algorithm random split domain knowledge 
denote version splice random partitioning splice 
splice restriction underlying machine learning algorithm 
partitions form initial contextual clusters ccs 
cc create initial interim concepts 
stage contextual clustering clusters individual items basis interim concept accuracy xed size window surrounding original data item 
replaces splice contextual clustering clusters intervals created partitioning stage 
new ccs created clustering initial dataset items similarity represented interim concepts 
new ccs training sets creation new interim concepts 
contextual clustering stage iterated contextual context re nement 
nal set interim concepts form output local concepts 
boundaries nal seed sets form context boundaries 
examine contextual clustering detail 
splice splice algorithm input environmental attribute ordered data set 
stage partition dataset partition dataset environmental attribute splice 
pre set number random splits 
prior domain knowledge 
identi ed partitions form initial contextual clusters ccs turn create initial interim concepts 
stage contextual clustering combination interim concept item original data set allocated score total accuracy concept items xed size environmental attribute surrounding item 
cluster original data set items share maximum scores interim concept 
clusters form new set ccs 
create new set interim concepts new ccs 
stage repeated interim concepts change xed number iterations completed 
stage create local concepts nal set interim concepts form output local concepts 
splice algorithm splice performance contextual clustering stage clusters similarity context represented current interim concept 
similarity context shared interim concept classi cation accuracy window surrounding data item 
de ne simple measure wij represent similarity interim concept example wij interim concept misclassi es example interim concept correctly classi es example window size new ccs built allocating example cluster examples maximum weight wij associated interim concept new set interim concepts created applying new ccs 
contextual clustering halted agiven number iterations 
splice performance section shows experiments splice 
rst direct comparison splice recognition stagger concepts domain range noise duration xed number hidden changes context 
second compares concept recognition abilities splice splice splice range noise context change levels xed duration 
concept recognition splice vs splice splice shown superior splice series stagger concept recognition task changes context 
basic stagger concept recognition problem section altered repeating training set times giving total changes context 
versions splice trained datasets range noise context duration 
sets local concepts learnt assessed target concept 
results show average number correct local concept identi cations achieved average number incorrect local concepts identi ed 
splice performance concepts correctly identified splice noise noise noise noise duration target concepts concepts correctly identified splice noise noise noise noise duration target concepts number concepts correctly recognised splice splice training sets generated range context duration noise 
context duration corresponds number instances concept active 
training set consists random examples classi ed pattern contexts repetitions structure concept instances concept instances concept instances 
duration ranges instances instances noise ranges 
results average repetitions combination noise duration 
results shows average number stagger concepts correctly recognised splice splice context duration noise level 
versions splice converge recognition concepts higher context durations 
splice converges recognition local concepts quickly levels noise 
shows number incorrect local concepts induced splice splice 
charts show number incorrect concepts induced context duration noise level 
levels noise splice induces incorrect concepts splice 
number incorrect concepts similar noise high context duration 
task versions splice respond range training set noise context duration 
versions exploit increased context durations minimise ect noise 
splice consistently induces correct incorrect local concepts splice 
results suggests splice clustering mechanism better able overcome ects frequent context changes high levels noise 
experiment investigate result xing context duration experiments reported splice run contextual clustering iterations 
noted underlying learner run default parameters sub setting 
splice performance incorrect concepts generated splice noise noise noise noise duration target concepts incorrect concepts generated splice noise noise noise noise duration target concepts incorrect concepts splice splice testing di erent levels context change 
splice splice random seeding experiment shows high levels context repetition lead reduction recognition accuracy splice 
splice shown improve high levels context repetition 
splice random partitioning shown ective domains 
previous experiment looked ects di erent context durations local concept recognition splice splice 
demonstrated splice superior accuracy xed number context changes provided little insight ect di erent levels context change 
experiment investigates ects di erent levels context repetition noise 
rst compare concept recognition accuracy splice splice 
splice results subsequently compared accuracy achieved splice splice random partitioning 
algorithm evaluated ability correctly induce stagger local concepts training set containing hidden changes context 
training set consisted randomly generated stagger instances classi ed pattern contexts repetitions structure concept instances concept instances concept instances varies 
ects noise evaluated range noise 
results shown average iterations combination noise 
splice random partitions 
results shows number stagger concepts correctly induced splice splice combination context repetition noise 
splice performance concepts recognised noise noise noise noise noise splice repetitions concepts recognised noise noise noise noise noise splice repetitions splice splice concept recognition primary feature splice chart initial rise subsequent decline concept recognition number repetitions increases 
exception noise splice decline accuracy 
splice chart shows increasing concept recognition maximum concepts increases repetition 
splice splice achieve similar levels concept recognition single iteration contexts repetition 
splice recognises concepts levels repetition greater 
exception noise versions recognise concepts repetition levels 
splice competitive context repetition 
fact greater numbers context change negative ect accuracy achieved 
due failure partitioning method domains changes context 
splice interesting substantially splice ective domains relatively context changes 
anticipate stronger partitioning method splice resilient frequent changes context 
splice algorithm hand improves concept recognition context repetition increases 
splice ected poor initial partitioning re builds context boundaries iteration contextual clustering 
poor initial partition minimal ect splice able take advantage increases context examples 
shows number correct stagger concepts induced splice random partitions 
chart shows rise recognition accuracy repetitions increase maximum concepts recognised noise levels 
number concepts recognised similar splice 
similarity splice splice shows domain partitioning provides bene random partitioning splice 
complex domains bias provided initial partitioning ect splice accuracy 
results attained learning fly concepts recognised noise noise noise noise noise splice repetitions splice concept recognition splice provide indication power splice clustering method 
learning fly test splice methodology wished apply substantially complex domain arti cial data described 
available data collected ight simulation experiments behavioural cloning 
previous domain necessary explicitly divide domain series individual learning tasks stages 
splice able induce ective pilot substantial proportion original ight plan explicitly provided stages 
sections brie describe problem domain application splice 
domain learning fly experiments intended demonstrate possible build controllers complex dynamic systems recording actions skilled operator response current state system 
ight simulator chosen dynamic system complex system requires high degree skill operate successfully understood 
experimental setup collect data human subjects ying predetermined ight plan 
data input induction program 
ight plan provided human subjects 
take altitude feet 

level distance feet starting point 
turn right compass heading approximately degrees 
learning fly 
north south distance feet turn left head back runway 
turn considered complete azimuth degrees degrees 

line runway 

descend runway keeping line 

land runway 
log includes attributes showing position motion control attributes 
position motion attributes ground limit wing stall twist elevation azimuth roll speed elevation speed azimuth speed airspeed distance altitude distance fuel 
control attributes rollers elevator thrust aps 
implementation unrealistic 
decision trees induced logged data tested compiling trees autopilot code simulator ying simulator 
original experiments subjects ew ight plan times 
data set records produced 
originally thought combined data submitted learning program 
proved complex task learning systems available 
problems largely due mixing data di erent contexts 
rst critical type context pilot 
di erent pilots di erent ying styles responses situation di er 
separated pilot 
furthermore actions pilot di er stage ight 
pilot adopts di erent strategies depending turning aircraft climbing landing succeed induction program able distinguish di erent cases 
classi cation learning programs available manual separation data ight stages required 
pilots intermediate ight goals division stages onerous 
divisions immediately obvious 
initial division example lining descending separated di erent stages 
separation decision trees generated runway 
line stage introduced successful behavioural clone produced 
stages behavioural cloning human intervention included quite lot trial experimentation 
described suggests ight stages treated di erent contexts splice approach automate separate ight appropriate contexts learning 
learning fly flying splice domain introduces additional di culty 
control actions available pilot previous behavioural cloning experiments built decisions trees actions stages resulting decision trees switched depending current stage 
splice applied learning tasks viz building controller elevators rollers thrust aps guarantee exactly context divisions 
causes problems actions coordinated 
example turn aircraft rollers elevators 
contexts actions coincide new roller action say may commenced corresponding elevator action may start time causing lack coordination failure execute correct manoeuvre 
problem avoided rollers elevators single attribute corresponding stick position 
rollers take discrete values elevators take discrete values combined attribute possible values 
represented 
switching local concepts ight trivial immediate feedback classi cation accuracy available 
previous experiments reported voting mechanism select current context local concept 
domain chose decision tree context selection 
examples nal local concept labelled relevant local concept 
decision tree induced attributes provided learning individual local concepts subsequently select current context instant ight 
original splice wij formula classication accuracy perform class frequencies wildly di erent 
represented classes dominating contextual clustering process leading clusters similar classi cation represented classes dissimilar classi cation poorly represented classes 
problematic successful depend correct classi cation rare classes 
problem reduced altering weighting formula wij give equal importance accuracy classes window ignoring relative representations di erent classes 
wij formula wij cx cm cm learning fly number classes cm class number example interim concept misclassi es example interim concept correctly classi es example window size splice augmented recognise domain discontinuities ight altering wij predictions ight ight example incorporated wij 
results able successfully rst stages ight training data extracted stages 
noted changes domain combining rollers elevator unable rst turn explicit division domain stages 
shows successful splice ight stages 
best ight 
sample complete ight 
settings splice instances 
post pruning turned 
iterations clustering stage 
initial partitioning set equal divisions rst ight 
addition stages ight causes catastrophic interference rst stages stages 
splice unable completely distinguish parts ight 
splice synthesising controllers stages rst time automated procedure successful identifying contexts complex domain 
decision tree select current context reasonably ective inelegant 
learning problem combines complex understood domain poorly represented task 
combination test bed extending context selection methods reasoning context 
related height feet north south feet related flight comparison splice sample flight east west feet splice closely related flora family line learners designed adapt hidden changes context drawing changes concept window instances 
rapid adaption changes context assured altering window size response changes prediction accuracy concept complexity 
version flora adapted domains recurring hidden context storing stable concepts possible re context change suspected 
order guard concept drift genuinely new concepts mistaken selection prior stable concept stable concept updated match examples current window 
splice extends concept storing stable concepts adjunct line learning primary focus line batch learning approach 
line learning methods deal concept drift decay importance older instances 
stagger instance probably rst machine learning system dealing concept drift 
system moves forward data series search frontier conjunctive disjunctive features altered changes statistics measuring logical su ciency ls logical necessity ln 
goal converge succinct concept description 
system allows concept drift backtracking search frontier statistics current charac related fall beneath threshold 
failed characterisation removed search frontier replaced alternate characterisations 
discarding characterisations longer ective provides decay inthe importance older information 
line learners explicit window heuristic similar flora 
batch learners line learning domains concept drift repeatedly learning window instances 
window update mechanism need rst rst organisation discards older examples new item appears similar region attribute space 
line learners domains hidden changes context assume context tend contiguous time 
splice broadens assumption allowing context contiguous environmental attribute usually time space 
splice need minor changes deal environmental attribute dimensions 
little literature dealing hidden changes context batch approach 
probably batch learning methods assume available information directly represented attributes provided hidden changes context treated noise 
suspect domains unseen context important cation accuracy reported tends focus successful representations including explicit representation previously hidden context 
process rarely explicit 
explicitly refer augmentation domain representation previously hidden context task learning achieved domain representation augmented splitting learning task sub tasks 
substantial dealing known changes context 
approach known context divide domain series di erent learning tasks induce di erent classi ers task switch classi ers current context 
method applied learning domain target recognition 
application local concepts prediction classication article similar model switching approach 
transfer knowledge learnt context new previously unseen context similar line adaption hidden change context 
knowledge embedded decision tree transfered new context applying tiered structure 
xed decision tree rst tier 
second tier providing soft matching weights leaf decision tree trained second context 
similar tiered structure originally proposed dealing exible contexts 
knowledge existing network signi cantly increase speed learning new context weights existing network initialise new neural network 
methods transfer knowledge context adapt splice local concepts line manner analogous flora 
context far de ned attribute values tend stable contiguous intervals environmental attribute 
context interpreted ect contextual attribute interpretation context sensitive attribute 
classi er accuracy improved context sensitive attributes instance learning multivariate regression methods contextual normalisation contextual expansion contextual weighting 
instance learning splice underlying learner hidden contexts recognised splice utilised techniques line prediction 
somewhat di erent line method designed detect exploit contextual attributes metal 
case contextual attributes considered predictive relevance attributes 
metal works detected contextual attributes trigger changes set features classi er 
approach context de nition quite di erent splice philosophy similar 
widmer concludes stating 
identi cation contextual features rst step naming able reason contexts long term goal splice approach precisely 
summarise splice begins build bridge line methods dealing hidden changes context batch methods dealing known change context 
splice applies line assumption contexts liable contiguous environmental attribute broader problem detecting extracting hidden context associated concepts 
article new line paradigm recognising dealing hidden changes context 
hidden changes context occur domain prediction task poorly understood context di cult isolate attribute 
domains hidden context data mining problems nancial market prediction behavioural cloning 
previous hidden changes context line learning approach 
new approach splice uses line batch meta learning extract hidden context induce associated local concepts 
incorporates existing machine learning systems article 
implementations splice 
evaluation splice approach included line prediction task series hidden context recognition tasks complex control task 
splice uses context sensitive feature selection algorithm divide data series changes context 
process called contextual clustering applied intervals group intervals appear context 
process uses semantics concepts induced interval measure similarity context 
resulting clusters create context speci concepts specify context boundaries 
splice di ers splice primarily method contextual clustering 
splice clusters basis individual members data series 
context boundaries restricted boundaries partitioning stage context boundaries re ned 
splice robust quality initial partitioning 
splice successfully detected dealt hidden context complex control task 
learning fly behavioral cloning domain learning autopilot series sample xed ight plan 
previous domain required user specify stages ight 
splice able successfully substantial fragment initial ight plan stages contexts speci ed 
rst time automated procedure successful identifying context complex domain 
anumber improvements splice algorithms 
partitioning method shown problematic splice high levels noise hidden changes context 
existing machine learning system provide partitioning elegant better solution may implement specialised method designed deal additional complexity attributes 
approach augment decision tree algorithm allow splits selected attributes 
splice splice provide direct comparison relative advantage dividing domain set contexts 
comparison method minimum description length mdl heuristic 
mdl principle best theory concept minimise amount information need sent sender receiver receiver correctly classify items shared dataset 
case information sent contain local concepts context switching method list exceptions 
allow comparison contextsensitive global concept local concepts context switching context insensitive global concept 
contextual clustering method mdl heuristic guide search possible context divisions 
approaches selecting current context line voting method domains immediate feedback decision tree domain immediate feedback 
sophisticated approaches model hidden context 
model knowledge expected context duration order stability 
incorporate existing attributes domain feedback 
decision tree context switching learning task primitive implementation model existing attributes select context 
exciting possibility contexts identi ed splice guide search external world attribute similar characteristics 
attributes incorporated current attribute set allowing bootstrapping domain representation 
knowledge discovery databases kdd approach includes notion analysts reiterate data selection learning data mining tasks 
method provide way automated agent select potentially useful attributes outside world extend existing domain knowledge 
michael harries partially supported australian postgraduate award industrial 
aha kibler albert 
instance learning algorithms 
machine learning 
clark niblett 
cn induction algorithm 
machine learning 
scott clearwater tze pin cheng haym hirsh 
incremental batch learning 
proceedings sixth international workshop machine learning pages 
morgan kaufmann publishers 
pedro domingos 
context sensitive feature selection lazy learners 
arti cial intelligence review 
special issue lazy learning edited david aha 
usama fayyad irani 
multi interval discretization continuous valued attributes classi cation learning 
th international joint conference arti cial intelligence pages california 
morgan kaufmann 
usama fayyad gregory shapiro padhraic smyth 
data mining knowledge discovery overview 
usama fayyad gregory shapiro padhraic smyth uthurusamy editors advances knowledge discovery data mining 
mit press 
harries horn 
detecting concept drift nancial time series prediction symbolic machine learning 
xin yao editor eighth australian joint conference onarti cial intelligence pages singapore 
world scienti publishing 
harries horn 
learning stable concepts domains hidden changes context 
kubat widmer editors learning context sensitive domains workshop notes 
th international conference machine learning bari italy 
katz collins robust classi ers robust features 
neural computation 
jansson 
control procedure cobweb presence concept drift 
pavel brazdil editor european conference machine learning pages berlin 
springer verlag 
kubat 
floating approximation time varying knowledge bases 
pattern recognition letters 
kubat 
machine learning approach load balancing computer networks 
cybernetics systems journal 
kubat widmer 
adapting drift continuous domains 
proceedings th european conference machine learning pages berlin 
springer 
miroslav kubat 
second tier decision trees 
machine learning proceedings th international conference pages california 
morgan kaufmann 
michalski 
learning exible concepts ideas method tiered representation 
michalski editors machine learning arti cial intelligence approach vol 
iii 
morgan kaufmann 
pratt 
discriminability transfer neural networks 
hanson giles cowan editors advances neural information processing systems pages 
morgan kaufmann 
pratt candace kamm 
direct transfer learned information neural networks 
proceedings th national conference arti cial intelligence aaai 
quinlan 
learning logical de nitions relations 
machine learning 
quinlan 
programs machine learning 
morgan kaufmann publishers san mateo california 
larry rendell 
improving design induction methods analyzing algorithm functionality data concept complexity 
th international joint conference arti cial intelligence pages california 
morgan kaufmann 
rissanen 
universal prior integers estimation minimum description length 
annals statistics 
density adaptive learning forgetting 
machine learning proceedings tenth international conference pages san mateo california 
morgan kaufmann publishers 
claude sammut scott hurst dana donald michie 
learning machine learning proceedings ninth international conference pages san mateo california 
morgan kaufmann publishers 
schlimmer granger jr incremental processing tracking concept drift 
proceedings aaai pages los altos california 
morgan kaufmann publishers je ory schlimmer richard granger jr incremental learning noisy data 
machine learning 
turney 
exploiting context learning classify 
pavel brazdil editor european conference machine learning pages berlin 
springer verlag 
turney 
robust classi cation context sensitive features 
industrial engineering applications arti cial intel expert systems 
peter turney michael halasz 
contextual normalization applied aircraft gas turbine engine diagnosis 
journal applied intelligence 
widmer kubat 
ective learning dynamic environments explicit concept tracking 
pavel brazdil editor european conference machine learning pages berlin 
springer verlag 
widmer kubat 
learning presence concept drift hidden contexts 
machine learning 
gerhard widmer 
recognition exploitation contextual clues incremental meta learning 
saitta editor machine learning proceedings thirteenth international workshop pages san francisco 
morgan kaufmann 
