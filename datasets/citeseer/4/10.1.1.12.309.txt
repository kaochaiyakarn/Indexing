journal machine learning research submitted published cluster ensembles knowledge reuse framework combining multiple partitions alexander strehl alexander strehl com joydeep ghosh ghosh ece utexas edu department electrical computer engineering university texas austin austin tx usa editor claire cardie introduces problem combining multiple partitionings set objects single consolidated clustering accessing features algorithms determined partitionings 
identify application scenarios resultant knowledge reuse framework call cluster ensembles 
cluster ensemble problem formalized combinatorial optimization problem terms shared mutual information 
addition direct maximization approach propose ective cient techniques obtaining high quality combiners consensus functions 
combiner induces similarity measure partitionings objects 
second combiner hypergraph partitioning 
third collapses groups clusters meta clusters compete object determine combined clustering 
due low computational costs techniques quite feasible supra consensus function evaluates approaches objective function picks best solution situation 
evaluate effectiveness cluster ensembles qualitatively di erent application scenarios original clusters formed non identical sets features ii original clustering algorithms worked non identical sets objects iii common data set main purpose combining multiple clusterings improve quality robustness solution 
promising results obtained situations synthetic real data sets 
keywords cluster analysis clustering partitioning unsupervised learning multi learner systems ensemble mutual information consensus functions knowledge reuse 
notion integrating multiple data sources learned models disciplines example combining estimators econometrics granger evidences rule systems barnett multi sensor data fusion dasarathy 
simple ective type multi learner system ensemble component learner typically regressor classifier tries solve task 
early studies combining multiple rankings works borda condorcet pre date french revolution ghosh area noticeably came life past alexander strehl joydeep ghosh 
strehl ghosh decade series dedicated workshops kittler roli 
main goal ensembles improve accuracy robustness classification regression task spectacular improvements obtained wide variety data sets sharkey 
classification regression settings approaches proposed combining multiple clusterings 
notable exceptions include strict consensus clustering designing evolutionary trees typically leading solution lower resolution individual solutions combining results clusterings data set solution resides common known feature space example combining multiple sets cluster centers obtained means di erent initializations bradley fayyad 
introduce problem combining multiple partitionings set objects accessing original features 
call cluster ensemble problem motivate new constrained formulation shortly 
note combiner examine cluster label original features framework knowledge reuse bollacker ghosh 
cluster ensemble design problem di cult designing classifier ensembles cluster labels symbolic solve correspondence problem 
addition number shape clusters provided individual solutions may vary clustering method particular view data available method 
desired number clusters known advance 
fact right number clusters data set depends scale data inspected equally valid substantially di erent answers obtained data ghosh 
call particular clustering algorithm specific view data clusterer 
clusterer outputs clustering labeling comprising group labels objects 
may provide additional information descriptions cluster means shall information 
primary motivations developing cluster ensembles defined exploit reuse existing knowledge implicit legacy clusterings enable clustering distributed data sets cases raw data shared pooled restrictions due ownership privacy storage consider application domains greater detail 
knowledge reuse 
applications variety clusterings objects consideration may exist desires integrate clusterings single solution information influence new clustering di erent set features objects 
encounter application scenario clustering visitors website market basket analysis order facilitate direct marketing campaign strehl ghosh 
variety legacy customer segmentations demographics credit rating geographical region purchasing patterns 
see section related details 
cluster ensembles retail stores obviously reluctant throw domain knowledge wanted reuse pre existing knowledge create single consolidated clustering 
note legacy clusterings largely provided human experts companies proprietary methods information legacy segmentations going back original features algorithms obtain clusterings 
experience instrumental formulation cluster ensemble problem 
notable aspect engagement sets customers purchasing retail outlets website respectively significant overlap identical 
cluster ensemble problem allow missing labels individual clusterings 
applications legacy clusterings available constrained information useful 
example may wish combine reconcile clustering categorization web pages text analysis available yahoo 
dmoz manually crafted taxonomies internet service providers request patterns frequencies indicated user personal bookmarks preferences 
second example clustering mortgage loan applications information application forms supplemented segmentations applicants indicated external sources scores provided fair isaac 
distributed computing 
desire perform distributed data mining increasingly felt government industry 
related information acquired stored geographically distributed locations due organizational operational constraints kargupta chan needs process data situ far possible 
contrast machine learning algorithms invariably assume data available single centralized location 
argue transferring data single location performing series merges joins get single albeit large flat file favorite algorithms randomizing subsampling file 
practice approach may feasible computational bandwidth storage costs 
certain cases may possible due variety real life constraints including security privacy proprietary nature data accompanying ownership issues need fault tolerant distribution data services real time processing requirements constraints imposed law prodromidis 
interestingly severity constraints evident late government agencies attempt integrate databases analytical techniques 
cluster ensemble employed privacy preserving scenarios possible centrally collect records cluster analysis distributed computing entities share smaller amounts higher level information cluster labels 
ensemble feature distributed clustering situations processor clusterer access limited number features attributes object observes particular aspect view data 
aspects completely disjoint features partial overlaps 
gene function prediction separate gene clusterings obtained diverse sources gene sequence strehl ghosh comparisons combinations dna microarray data independent experiments mining biological literature medline 
orthogonal scenario object distributed clustering processor clusterer access subset objects cluster observed objects 
example corporations tend split customers cient management 
analysis clustering performed locally cluster ensemble provides way obtaining holistic analysis complete integration local data warehouses 
consider cluster ensembles reasons classification ensembles improve quality robustness results 
classification regression problems analytically shown gains ensemble methods involving strong learners directly related amount diversity individual component models krogh vedelsby tumer ghosh 
desires individual model powerful time models di erent inductive biases generalize distinct ways dietterich 
surprising ensembles popular integrating relatively unstable models decision trees multi layered perceptrons 
diversity beneficial clustering context created numerous ways including di erent features represent objects 
example images represented pixels histograms location parameters perceptual primitives scene coordinates varying number location initial cluster centers iterative algorithms means varying order data presentation line methods birch portfolio di erent clustering algorithms density means soft variants fuzzy means graph partitioning statistical mechanics known comparative performance di erent clustering methods vary significantly data sets 
example popular means algorithm performs miserably situations data accurately characterized mixture gaussians identical covariance matrices karypis 
fact di cult data sets comparative studies multiple clustering algorithms typically show variability results studies comparing results strong learners classification richard lippmann 
potential greater gains ensemble purpose improving clustering quality 
note contrast knowledge reuse distributed clustering scenarios situation combination mechanism access original features 
restriction consensus mechanism cluster labels case solely simplify problem limit scope solution just combiners multiple classifiers solely classifier outputs example voting averaging methods richer design space available 
cluster ensembles cluster ensemble 
consensus function combines clusterings variety sources resorting original object features algorithms 
final related motivation cluster ensemble build robust clustering portfolio perform wide range data sets little hand tuning 
example ensemble includes approaches means som kohonen dbscan ester typically low dimensional metric spaces algorithms tailored high dimensional sparse spaces spherical means dhillon modha jaccard graph partitioning strehl ghosh may perform dimensional dimensional spaces having switch models 
characteristic attractive general practitioner 
notation 

denote set objects samples points 
partitioning objects clusters represented set sets objects 
label vector clusterer function delivers label vector tuple objects 
shows basic setup cluster ensemble set labelings combined single labeling consensus labeling consensus function 
vector matrix transposition indicated superscript 
superscript brackets denotes index exponent 
organization 
section formally define design cluster ensemble optimization problem propose appropriate objective function 
section propose compare ective cient combining schemes tackle combinatorial complexity problem 
section describe applications cluster ensembles scenarios described show results real artificial data 
strehl ghosh 
cluster ensemble problem section illustrate problem combining multiple clusterings propose suitable objective function determining single consensus clustering explore feasibility directly optimizing objective function greedy approaches 
illustrative example illustrate combining clusterings simple example 
label vectors specify clusterings set objects see table inspection label vectors reveals clusterings logically identical 
clustering introduces dispute objects clustering quite inconsistent ones groupings contains missing data 
look combined clustering clusters 
intuitively combined clustering share information possible labelings 
inspection suggests reasonable integrated clustering equivalent clusterings 
fact performing exhaustive search unique clusterings elements groups shown clustering shares maximum information label vectors terms formally introduced subsection 
simple example illustrates challenges 
seen label vector unique 
fact unique clustering 
equivalent representations integer label vectors 
representation satisfies constraints ii 
max 
constraint enforces object label cluster 
second constraint assures cluster label successive object label occurred label greater highest label far 
allowing representations fulfill constraints integer vector representation forced unique 
transforming labels canonical form solves combining problem clusterings 
discrepancy labelings deal complex correspondence problem 
general number clusters cluster interpretation may vary tremendously models 
objective function cluster ensembles groupings th grouping having clusters consensus function defined function nr mapping set clusterings integrated clustering 

set groupings 
denoted 
priori information relative importance individual groupings reasonable goal consensus answer seek clustering shares information original clusterings 
cluster ensembles mutual information symmetric measure quantify statistical information shared distributions cover thomas provides sound indication shared information pair clusterings 
random variables described cluster labeling groups respectively 
denote mutual information denote entropy show metric 
upper bound easier interpretation comparisons normalized version ranges desirable 
normalizations possible observation min 
include normalizing arithmetic geometric mean 
prefer geometric mean analogy normalized inner product hilbert space 
normalized mutual information nmi nmi 
see nmi desired 
equation needs estimated sampled quantities provided clusterings 
number objects cluster number objects cluster denote number objects cluster group equation normalized mutual information estimate nmi nmi log nn log log 
pairwise measure mutual information define measure set labelings single labeling average normalized mutual information nmi 
propose optimal combined clustering opt maximal average mutual information individual labelings number consensus clusters desired words objective function opt defined opt arg max nmi 
earlier strehl ghosh slightly di erent normalization balanced clusters desired nmi log log arithmetic mean assuming maximum entropy caused perfect balancing 
strehl ghosh goes possible partitions strehl ghosh 
note formulation treats individual clustering equally 
easily generalize definition weighted average may preferable certain individual solutions important 
may situations labels known objects missing data label vectors 
cases consensus clustering objective equation generalized computing weighted average mutual information known labels weights proportional comprehensiveness labelings measured fraction known labels 
set object indices known labels th labeling 
generalized objective function opt arg max nmi 
direct greedy optimization approaches objective functions equations represent di cult combinatorial optimization problems 
exhaustive search possible clusterings labels maximum formidable objects partitions 
possible clusterings approximately jain dubes 
example ways form groups objects 
variety known greedy search techniques including simulated annealing genetic algorithms tried find reasonable solution 
investigated approaches detail expect consensus ensemble problem applied large data sets computationally expensive approaches unattractive 
get feel quality time tradeo involved devised studied greedy optimization scheme operates single label changes representative single labeling indicated highest labelings initial labeling greedy algorithm 
object current label changed possible labels objective reevaluated 
increases object label changed best new value algorithm proceeds object 
objects checked possible improvements sweep completed 
label changed sweep initiate new sweep 
algorithm terminates full sweep change labels indicating local optimum reached 
algorithm readily modified probabilistically accept decreases boltzmann machine 
local optimization procedures strong dependency initialization 
running greedy search starting random labeling computationally intractable tends result poor local optima 
initialization close optimum computation extremely slow due exponential time complexity 
experiments typically averaged hour run ghz pc implementation 
results greedy approach shown section 
cluster ensembles section proposes algorithms far cient greedy approach deliver similar quality results 
algorithms developed intuitive heuristics vantage point direct maximization 

cient consensus functions section introduce cient heuristics solve cluster ensemble problem 
algorithms approach problem transforming set clusterings hypergraph representation 
cluster similarity partitioning algorithm cspa 
clustering signifies relationship objects cluster establish measure pairwise similarity 
induced similarity measure objects yielding combined clustering 
hypergraph partitioning algorithm 
algorithm approximate maximum mutual information objective constrained minimum cut objective 
essentially cluster ensemble problem posed partitioning problem suitably defined hypergraph hyperedges represent clusters 
meta clustering algorithm mcla 
objective integration viewed cluster correspondence problem 
essentially groups clusters meta clusters identified consolidated 
subsections describe common hypergraph representation cspa mcla 
section discusses di erences algorithms evaluates performance controlled experiment 
representing sets clusterings hypergraph step proposed consensus functions transform cluster label vectors suitable hypergraph representation 
subsection describe set clusterings mapped hypergraph 
hypergraph consists vertices hyperedges 
edge regular graph connects exactly vertices 
hyperedge generalization edge connect set vertices 
label vector construct binary membership indicator matrix column cluster represented hyperedge illustrated table 
entries row binary membership indicator matrix add row corresponds object known label 
rows objects unknown label zero 
concatenated block matrix 
defines adjacency matrix hypergraph vertices hyperedges 
column vector specifies hyperedge indicates vertex corresponding row part hyperedge indicates 
mapped cluster hyperedge set clusterings hypergraph 

generalizing algorithms soft clustering simply contains posterior probabilities cluster membership 
strehl ghosh table illustrative cluster ensemble problem original label vectors left equivalent hypergraph representation hyperedges right 
cluster transformed hyperedge 
cluster similarity partitioning algorithm cspa coarse resolution viewpoint objects similarity cluster similarity binary similarity matrix readily created clustering 
entry wise average matrices representing sets groupings yields similarity matrix finer resolution 
entries denote fraction clusterings objects cluster computed sparse matrix multiplication hh illustrates generation cluster similarity matrix example table 
similarity matrix objects reasonable similarity clustering algorithm 
chose partition induced similarity graph vertex object edge weight similarity metis karypis kumar robust scalable properties :10.1.1.106.4101
cspa simplest obvious heuristic computational storage complexity quadratic opposed approaches near linear hypergraph partitioning algorithm second algorithm direct approach cluster ensembles re partitions data clusters indications strong bonds 
cluster ensemble problem formulated partitioning hypergraph cutting minimal number hyperedges 
call approach hypergraph partitioning algorithm 
hyperedges considered weight 
vertices equally weighted 
note includes way relationship information cspa considers pairwise relationships 
look hyperedge separator partitions hypergraph unconnected components approximately size 
note obtaining comparable sized partitions standard constraint graph partitioning clustering approaches 
approach extended soft clusterings objects posterior probabilities cluster membership cluster ensembles combined student version matlab illustration cluster similarity partitioning algorithm cspa cluster ensemble example problem table 
clustering contributes similarity matrix 
matrix entries shown darkness proportional similarity 
average objects yield consensus 
avoids trivial partitions karypis strehl ghosh 
hand means natural data clusters highly imbalanced graph partitioning approach appropriate 
results maintain vertex imbalance imposing constraint max 
hypergraph partitioning studied area kernighan lin alpert kahng algorithm details omitted brevity 
hypergraph partitioning package hmetis karypis 
hmetis gives high quality partitions scalable 
please note hypergraph partitioning general provision partially cut hyperedges 
means sensitivity hyperedge left group cut 
problematic applications 
consider example table 
simplicity assume hyperedges 
partitionings cut hyperedges 
intuitively superior hyperedge remains versus second 
standard hypergraph partitioning equivalent quality cut number hyperedges 
meta clustering algorithm mcla subsection introduce third algorithm solve cluster ensemble problem 
meta clustering algorithm mcla clustering clusters 
yields object wise confidence estimates cluster membership 
represent cluster hyperedge 
idea mcla group collapse related hyperedges assign object collapsed hyperedge participates strongly 
hyperedges considered related purpose collapsing determined graph clustering hyperedges 
refer cluster hyperedges meta cluster collapsing reduces number hyperedges detailed steps construct meta graph 
view indicator vectors hyperedges vertices regular undirected graph meta graph 
edge strehl ghosh illustration hypergraph partitioning algorithm cluster ensemble example problem table 
hyperedge represented closed curve enclosing vertices connects 
combined clustering minimal hyperedge cut balanced possible clusters objects 
weights proportional similarity vertices 
suitable similarity measure binary jaccard measure ratio intersection union sets objects corresponding hyperedges 
formally edge weight vertices defined binary jaccard measure corresponding indicator vectors ha clusters non overlapping hard edges vertices clustering meta graph partite shown 
cluster hyperedges 
find matching labels partitioning meta graph balanced meta clusters 
graph partitioning package metis step 
results clustering vectors 
meta cluster approximately vertices 
vertex meta graph represents distinct cluster label meta cluster represents group corresponding labels 
collapse meta clusters 
meta clusters collapse hyperedges single meta hyperedge 
meta hyperedge association vector contains entry object describing level association corresponding meta cluster 
level computed averaging indicator vectors particular meta cluster 
entry indicates weakest strongest association respectively 
compete objects 
step object assigned associated specifically object assigned meta cluster highest entry association vector 
ties broken randomly 
confidence assignment reflected winner share association ratio winner association sum associations 
note meta cluster guaranteed win object 
labels final combined clustering 

weighted average initial clusterings associated confidences soft clustering 
cluster ensembles illustrates meta clustering example table 
shows original partite meta graph 
meta clusters indicated symbols 
consider meta cluster markers 
collapsing hyperedges yields meta hyperedge association vector subsequently meta cluster win competition vertices objects represent cluster resulting integrated clustering 
proposed meta clustering algorithm robustly outputs optimal clusterings equivalent clusterings uncertainty objects reflected confidences objects respectively 
illustration meta clustering algorithm mcla cluster ensemble example problem table 
partite meta graph shown 
edge darkness increases edge weight 
vertex positions slightly perturbed expose occluded edges 
meta clusters indicated symbols 
discussion comparison take look worst case time complexity proposed algorithms 
assuming quasi linear hyper graph metis cspa kr mcla nk 
fastest closely followed mcla tends small 
cspa slower impractical large greedy strehl ghosh approach described previous section slowest intractable large performed controlled experiment allows compare properties proposed consensus functions 
partition objects groups random obtain original clustering 
duplicate clustering times 
labelings fraction labels replaced random labels uniform distribution feed noisy labelings proposed consensus functions 
resulting combined labeling evaluated ways 
measure normalized objective function ensemble output individual labels 
second measure normalized mutual information consensus labeling original undistorted labeling nmi 
better comparison added random label generator baseline method 
performance measures hypothetical consensus function returns original labels included illustrate maximum performance low noise settings 
shows results 
noise increases labelings share information maximum obtainable decreases techniques top 
performs worst experiment believe due current inability cater partially cut edges 
low noise mcla cspa recover original labelings 
mcla retains cspa presence medium high noise 
interestingly high noise settings cspa exceeds mcla performance 
note high noise settings original labels lower average normalized mutual information 
set labels completely random consensus algorithms recover little common information original labeling fully unrelated 
realistically noise exceed mcla perform best simple controlled experiment 
illustrates algorithms recover true labeling presence noise robust clustering 
noise increases labelings share information true labeling ensemble nmi decreases 
ranking algorithms measure mcla best followed cspa worst 
fact mcla recovers original labeling noise scenario 
noise algorithms essentially ranking regardless nmi 
indicates proposed objective function suitable choice real applications nmi available 
direct greedy optimization approach performs similar cspa terms nmi scores lower mcla cases 
terms greedy approach returns higher score cspa mcla unrealistically high noise levels 
importantly greedy approach tractable datapoints dimensions clusters due high computational complexity 
experiment indicates mcla best suited terms time complexity quality 
applications experiments described sections observe combining method result higher 
labels obtained random permutation 
groups balanced 

low noise settings original labels global maximum share mutual information distorted labelings 
cluster ensembles noise fraction induced set labelings average mutual information ensemble output set noisy labelings random labels greedy cspa mcla original labels noise fraction induced set labelings mutual information ensemble output original labels random labels greedy cspa mcla original labels comparison consensus functions terms objective function top terms normalized mutual information original labels nmi bottom various noise levels 
fitted sigmoid squared error shown algorithms show trend 
strehl ghosh particular setups 
fact mcla tends best low noise diversity settings cspa tend better high noise diversity settings 
mcla assumes meaningful cluster correspondences true little noise diversity 
useful methods 
objective function added advantage allows add stage selects best consensus function supervisory information simply selecting highest 
experiments report results supra consensus function obtained running algorithms cspa mcla selecting greatest 
significant di erences notable trends observed algorithms level detail described 
note supra consensus function completely unsupervised avoids problem selecting best combiner data set 

consensus clustering applications experiments consensus functions enable variety new approaches problems 
introducing data sets evaluation methodology discuss section distributed clustering performed entity access limited subset features 
section shows operate limited subsets objects combine enabling distributed clustering 
section illustrate robustness clustering increased combining set clusterings 
data sets illustrate cluster ensemble applications real artificial data sets 
table summarizes basic properties data sets left parameter choices right 
data set artificially generated contains points dimensional gaussian clusters means diagonal covariance matrices diagonal elements 
second data set contains points multivariate gaussian distributions points space 
clusters variance di erent means 
means drawn uniform distribution unit hypercube 
artificial data sets available download strehl com 
third data set pen recognition handwritten digits 
publicly available uci machine learning repository contributed alpaydin 
contains spatial features training test cases objects 
classes roughly equal size balanced clusters data corresponding digits 
fourth data set text clustering 
original yahoo 
news categories data business entertainment sub category art cable culture film industry media multimedia music online people review stage television variety health politics sports technology 
data publicly available ftp ftp cs umn edu dept users boley series boley 
strehl 

raw word document matrix consists cluster ensembles name features features categories balance similarity clusters real euclidean real euclidean real euclidean yahoo ordinal cosine table overview data sets cluster ensemble experiments 
balance defined ratio average category size largest category size 
non normalized occurrence frequencies stemmed words porter su stripping algorithm frakes 
pruning words occur times average insignificant example generic example new results 
call data set yahoo 
respectively 
clustering yahoo clusters noted 
chose times number categories natural number clusters indicated preliminary runs visualization 
euclidean similarity 
yahoo cosine similarity 
evaluation criteria evaluating quality clustering non trivial ill posed task 
fact definitions objective functions clusterings exist jain dubes 
internal criteria formulate quality function data similarities 
example mean squared error criterion means measures compactness popular evaluation criteria 
measures isolation mincut criterion uses sum edge weights clusters graph partitioning 
internal criteria clustering optimization problem clusterer evaluate performance tune results accordingly 
external criteria hand impose quality additional external information clusterer category labels 
approach appropriate groupings ultimately evaluated externally humans 
example objects categorized external source class labels available information theoretic measures quantify match categorization clustering 
previously average purity entropy measures assess clustering quality worst best boley 
average purity intuitive understand favors small clusters singletons score highest 
monolithic clustering single cluster objects receives score high fraction objects biggest category 
data sets number close maximum quality intuitive quality low 
entropy measure better purity favors smaller clusters 
normalized mutual information provides measure impartial respect compared purity entropy 
reaches maximum value 
greater number clusters categories allows modeling multi modal categories 
strehl ghosh sets labels perfect correspondence 
shall categorization labels evaluate cluster quality computing nmi defined equation 
feature distributed clustering fdc feature distributed clustering fdc show cluster ensembles boost quality results combining set clusterings obtained partial views data 
mentioned distributed databases integrated single centralized location proprietary data aspects privacy concerns performance issues result scenarios 
situations realistic clusterer database transmit cluster labels attributes record central location combined supra consensus function 
experiments simulate scenario running having access restricted small subset features subspace 
clusterer partial view data 
note experiments views created common full feature space public domain data sets real life scenario di erent views determined priori application specific way 
clusterer access objects 
find groups views subspaces clustering technique 
combining stage individual cluster labels integrated supra consensus function 
discuss experimental results data lend illustration 
create random views selection pair features data euclidean graph partitioning view obtain individual clusterings 
individual clusterings combined supra consensus function proposed previous section 
clusters linearly separable full space 
clustering space yields original generative labels referred clustering 
clustering illustrated coloring data points space spanned second principal components pcs 
shows final fdc result combining subspace clusterings 
clustering computed randomly selected feature pairs 
subspaces shown 
rows corresponds random selection feature dimensions 
chosen feature pairs row shows clustering left feature pair shown axis clustering labels data projected global principal components right 
consistent appearance clusters rows dot colors shapes matched meta clusters 
points clusters meta cluster share color shape plots 
subspace clusters segregated due overlaps 
supra consensus function combine partial knowledge clusterings clustering 
fdc results clearly superior individual results right compared clustering 
best individual result mislabeled points consensus points mislabeled 
conducted fdc experiments data sets 
table summarizes results comparison benchmarks 
choice number random cluster ensembles pc pc pc clustering fdc consensus clustering data 
data points projected principal components data 
clustering obtained graph partitioning euclidean similarity original space identical generating distribution assignment 
consensus clustering derived combination clusterings obtained random feature pairs see 
consensus clustering clearly superior compared individual results right compared clustering 
subspaces dimensionality currently driven user 
example yahoo case clusterings performed dimensions occurrence frequencies random words 
average quality results best quality 
supra consensus function combine labelings yields quality mutual information average individual clustering 
scenarios consensus clustering better best individual input clustering better average quality individual clusterings 
processing features possible limited views exist cluster ensemble boost results significantly compared individual clusterings 
combiner feature information consensus clustering tends poorer clustering features 
discussed knowledge reuse application scenarios original features unavailable comparison feature clustering done 
supra consensus function chooses mcla cspa results di erence statistically significant 
noted mcla faster method choice needed 
delivers poor improves complex data yahoo 
mcla cspa performed significantly better data sets 
object distributed clustering odc dual application described previous section object distributed clustering odc 
scenario individual limited selection object population access features objects provided 
strehl ghosh pc pc pc pc pc illustration feature distributed clustering fdc data 
row corresponds random selection feature dimensions 
chosen feature pairs row shows clustering colored obtained subspace spanned selected feature pair left visualizes clusters plane global principal components right 
subspace clusters segregated due strong overlaps 
supra consensus function combine partial knowledge clusterings far superior clustering 
cluster ensembles input parameters quality data sub features consensus max subspace average subspace min subspace space models nmi nmi maxq avg dims nmi nmi nmi yahoo table fdc results 
consensus clustering better best individual subspace clustering 
somewhat di cult fdc labelings partial 
access original features combiner needs overlap labelings establish meaningful consensus object distribution naturally result operational constraints application scenarios 
example individual stores retail may records visitors store people visit store result desired overlap 
hand data centralized may artificially distribute sense running clustering algorithms di erent overlapping samples record wise partitions data combine results provide computational speedup individual super linear time complexity 
subsection discuss consensus functions overlapping sub samples 
propose wrapper clustering algorithm simulates scenario distributed objects combiner access original features 
odc introduce object partitioning corresponding clustering merging step 
actual clustering referred inner loop clustering 
pre clustering partitioning step entire set objects decomposed overlapping partitions 
partitions overlapping 
set partitions provides full coverage odc framework parameterized number partitions repetition factor 
repetition factor defines total number points processed partitions combined approximately vn 
assume data ordered contiguous indexed subsample equivalent random subsample 
simulation decided give partition number objects maximize speedup 
partition nv objects 
number objects partition propose simple coordinated sampling strategy partition objects deterministically picked union partitions provides full coverage objects 
remaining objects particular partition picked randomly 
features available merge partitions locations feature space reach consensus 
strehl ghosh replacement objects partition 
ways coordinated sampling 
limit discussion strategy brevity 
partition processed independent identical chosen appropriately application domain 
simplicity number clusters sub partitions 
post clustering merging step done supra consensus function 

partition looks fraction data missing labels 
su cient overlap supra consensus function ties individual clusters delivers consensus clustering 
performed experiment demonstrate odc framework perform clustering partially overlapping samples access original features 
graph partitioning clusterer processor 
shows results data sets 
plot shows relative mutual information fraction mutual information retained compared clustering objects features function number partitions 
fix sum number objects partitions double number objects repetition factor 
plot ranges odc result marked 
plots fitted sigmoid function summarize behavior odc scenario 
clearly tradeo number partitions versus quality 
approaches vn clusterer receives single point reasonable grouping 
example yahoo case processing partitions retains full quality 
complex data sets combining partial partitionings yields quality 
fact clustered partitions quality 
observed easier data sets smaller absolute loss quality partitions regarding proposed techniques consensus algorithms achieved similar scores significant di erences yahoo 
instabilities data set delivering inferior consensus clusterings compared mcla 
general believe loss quality main causes 
reduction considered pairwise relations problem simplified speedup increases 
point relationship information lost reconstruct original clusters 
second factor related balancing constraints graph partitioner inner loop sampling strategies maintain balance enforcing clustering hurts quality 
relaxed inner loop clusterer improve results 
distributed clustering cluster ensemble particularly useful inner loop clustering algorithm superlinear complexity fast consensus function mcla 
case additional speedups obtained distribution objects 
assume inner loop clusterer complexity example similarity approaches cient agglomerative clustering cluster ensembles odc results 
clustering quality measured relative mutual information function number partitions various data sets yahoo 
sum number samples partitions fixed 
plot contains experimental results graph partitioning inner loop 
fitted sigmoid 
processing time reduced factor yahoo data preserving quality 
strehl ghosh uses mcla supra consensus function 
define speedup computation time full clustering divided time odc approach 
overhead mcla consensus functions grows linearly negligible compared clustering 
asymptotic sequential speedup approximately odc seq partition clustered communication separate processor 
integration time dimensional label vector entire similarity matrix transmitted combiner 
odc save computation time enables trivial fold parallelization 
consequently processor computer utilized asymptotic speedup odc par obtained 
example computing time approximately partition half original size consequently processed quarter time 
partitions odc takes time original processing 
experiments partitions yields correspondingly approximate sequential parallel speedups 
example yahoo sped fold processors full length quality 
fact clustered original time quality 
robust centralized clustering rcc consensus function introduce redundancy foster robustness choosing fine tuning single clusterer ensemble employed results combined 
particularly useful clustering performed closed loop human interaction 
goal robust centralized clustering rcc perform wide variety data distributions fixed ensemble 
rcc clusterer access features objects 
clusterer take di erent approach 
fact approaches diverse best results 
di erent distance similarity measures euclidean cosine techniques graph agglomerative means strehl 
ensemble clusterings integrated consensus function access original features 
show rcc yield robust results low dimensional metric spaces high dimensional sparse spaces modifications experiment set 
diverse clustering algorithms implemented self organizing map hypergraph partitioning means distance euclidean cosine correlation extended jaccard graph partitioning similarity euclidean cosine correlation extended jaccard 
implementation details individual algorithms strehl 

rcc performed times sample sizes dga dga pend data sets 
case yahoo data sample sizes fewer samples su cient meaningfully partition clusters 
di erent sample sizes provide insight cluster quality improves data available 
quality improvement depends clusterer data 
example complex data sets require data quality reaches 
cspa reduce speedups obtained distribution 
cluster ensembles maximum 
computed random clustering experiment establish baseline performance 
random clustering consists labels drawn uniform distribution quality terms di erence mutual information compared random clustering algorithm approaches consensus shown 
shows learning curves average quality algorithms versus rcc 
top row results data shown 
external viewpoint consensus function clusterings euclidean cosine extended jaccard means graph partitioning self organizing feature map poor clusterings hypergraph partitioning correlation means correlation graph partitioning 
sample size rcc results better individual algorithm quality evaluations 
noticeable deterioration caused poor clusterings 
average rcc mutual information quality nmi higher average quality individual algorithms excluding random 
case yahoo data bottom row consensus function received poor clusterings euclidean means graph partitioning selforganizing feature map clusterings hypergraph partitioning cosine correlation extended jaccard means excellent clusterings cosine correlation extended jaccard graph partitioning 
rcc results average excellent despite presence clusterings 
fact level rcc average quality better average qualities algorithms excluding random 
shows scenario cluster ensembles robust 
similar results obtained 
cases individual approaches comparably hypergraph partitioning 
supra consensus function learns ignore hypergraph partitioning results yields consensus clustering quality 
shows rcc consistently better scenarios picking random average single technique 
looking consensus techniques need apparent clear winner 
mcla generally highest followed cspa performed poorly 
yahoo cspa highest approximately equally mcla performed poorly 
believe due fact diversity yahoo clusterings 
cspa better suited cluster correspondence assumed 
experimental results clearly show cluster ensembles increase robustness risk intolerant settings 
generally hard evaluate clusters high dimensional problems cluster ensemble throw models problem integrate consensus function yield stable results 
user category labels pick single best model 
ensemble automatically focuses appropriate data 
experiments diversity poorly performing 
diverse comparably performing quality significantly outperforms best individual clusterer seen subsection 
strehl ghosh som hgp rcc km nmi km km km gp gp gp gp som hgp rcc km nmi km km km gp gp gp gp som hgp rcc km nmi km km km gp gp gp gp som hgp rcc km nmi km km km gp gp gp gp detailed rcc results 
learning curves top row second row third row yahoo bottom row data 
learning curve shows di erence mutual information quality nmi compared random sample sizes 
bars data point indicate standard deviations experiments 
column corresponds particular clustering algorithm 
rightmost column gives rcc quality combining results algorithms 
rcc yields robust results scenarios 

related mentioned extensive body combining multiple classifiers regression models sharkey ghosh relatively little date combining multiple clusterings machine learning literature 
traditional pattern recognition substantial body largely theoretical consensus classification mid earlier neumann norton barthelemy 
studies term classification general sense encompassing partitions dendrograms trees 
today operations typically referred clusterings 
consensus classification profile set classifications cluster ensembles summary rcc results 
average learning curves rcc learning curves yahoo data 
learning curve shows di erence mutual information quality nmi compared random sample sizes 
bars datapoint indicate standard deviations experiments 
upper curve gives rcc quality combining results algorithms 
lower curve average performance algorithms 
rcc yields robust results scenario 
strehl ghosh sought integrated single consensus classification 
largely interested obtaining strict consensus identifies supremum infimum pairs clusterings relation sub cluster define partial ordering 
note approach strict consensus results level scale resolution original clusterings 
fact presence strong noise results trivial supremum single cluster objects infimum returned set singletons 
techniques computationally expensive meant smaller data sets 
prominent application strict consensus computational biology community obtain phylogenetic trees kim kannan 
set dna sequences generate evolutionary trees criteria maximum parsimony obtains trees score function 
cases biologists look strict consensus tree supremum lower resolution compatible individual trees 
note systems di erent cluster ensembles hierarchical clusterings typically unrooted trees ii domain specific distance metrics example distance evaluation criteria parsimony specificity density iii require strict consensus 
interesting application consensus clustering help supervised learning generate single decision tree fold cross validated results 
objects clustered positions paths decision trees 
objects majority class cluster selected form new training set generates consensus decision tree 
goal obtain single simplified decision tree compromising classification accuracy 
techniques multiple clusterings created evaluated intermediate steps process attaining single higher quality clustering 
example fisher examined methods iteratively improving initial set hierarchical clustering solutions fisher 
fayyad 
way obtaining multiple approximate means solutions main memory making single pass database combining means get final set cluster centers 
works summary representation cluster terms base features available integration mechanism opposed knowledge reuse framework cluster labels available 
evidence accumulation framework proposed multiple means higher value final anticipated answer run common data set fred jain 
results form occurrence similarity matrix records fraction solutions pair points fell cluster 
ect multiple fine level clusterings essentially come robust similarity indicator having flavor classical shared nearest neighbors measure jarvis patrick 
single link clustering similarity matrix 
occurrence matrix analogous cspa course clusterings generated fred jain legacy finer level resolution 
cluster ensembles exploit multiple existing groupings data 
analogous approaches exist supervised learning scenarios class labels known categories life long learning thrun learning learn thrun cluster ensembles pratt knowledge reuse bollacker ghosh 
researchers attempted directly reuse internal state information classifiers belief related classification tasks may benefit common internal features 
approach idea weights hidden layers multi layer perceptron mlp classifier architecture represent knowledge shared multiple tasks trained simultaneously caruana 
pratt uses trained weights mlp network initialize weights mlp trained related task 
related silver mercer developed system consisting task networks experience network 
experience network tries learn converged weights related task networks order initialize weights target task networks 
weight initialization techniques resulted improved learning speed 
weight reuse mlp type classifiers state reuse methods developed 
approach developed thrun sullivan nearest neighbor classifier dimensions input space scaled bring examples class closer pushing examples di erent classes apart 
scaling vector derived classification task related task 
previously proposed knowledge reuse framework labels produced old classifiers improve generalization performance new classifier di erent related task bollacker ghosh 
improvement facilitated supra classifier accesses outputs old new classifiers need training data create old classifiers 
substantial gains achieved training set size new problem small compensated extraction information existing related solutions 
application cluster ensembles combine multiple clusterings obtained partial sets features 
problem approached case collective data mining kargupta 
johnson kargupta feasible approach combining distributed agglomerative clusterings introduced 
local site generates dendrogram 
dendrograms collected pairwise similarities objects created 
combined clustering derived similarities 
kargupta 
distributed method principal components analysis introduced clustering 
usefulness having multiple views data better clustering recognized 
multi aspect clustering modha similarity matrices computed separately integrated weighting scheme 
mehrotra proposed multi viewpoint clustering clusterings semi automatically structure rules knowledge base 
usefulness having multiple clusterings objects capture relatively independent aspects information objects convey target set variables motivations multivariate extension information bottleneck principle friedman 
interest emerged semi supervised methods form bridge classification clustering augmenting limited training set labeled data larger amount unlabelled data 
powerful idea training blum mitchell success hinges presence multiple redundantly su cient views data 
example muslea 
introduced multi view algorithm including active strehl ghosh sampling training 
nigam ghani investigated ectiveness training semi supervised settings 
proposed algorithms hypergraph representations extensively studied garey johnson 
hypergraphs previously single high dimensional clustering han strehl combining multiple groupings 
mutual information cover thomas useful measure variety contexts 
example information bottleneck method slonim tishby uses mutual information reduce dimensionality preserving information possible class labels 
mutual information evaluate clusterings comparing cluster labels class labels strehl integrate multiple clusterings 
conceptual level consensus functions operate similarities pairs objects indicated cluster labels 
attractiveness considering objects embedded derived dis similarity space opposed original feature space shown classification applications strehl ghosh 

concluding remarks introduced cluster ensemble problem provided ective cient algorithms solve 
defined mutual information objective function enables automatically select best solution algorithms build supra consensus function 
conducted experiments show cluster ensembles introduce robustness speedup superlinear clustering algorithms dramatically improve sets subspace clusterings quite di erent domains 
document clustering yahoo 
web pages showed combining example clusterings obtained random words double quality compared best single result 
algorithms data sets available download strehl com 
cluster ensemble general framework enables wide range applications 
purpose basic problem formulation explore application scenarios 
issues aspects theoretical practical worthy investigation 
example formulation allows di erent provide varying numbers clusters experiments clusterer 
reality clusterings done distributed fashion di erent organizations di erent data views goals quite vary site site 
consensus clustering fairly robust variations 
preliminary results issue reported ghosh 

indicate cluster ensembles helpful determining reasonable value consensus solution value peaks desirable range 
typically gives better results best individual solution ensemble members cluster data highly varying resolutions 
provides evidence suitability criterion indicator nmi true labels plot vs nmi large number experiments shows correlation coe cient 
cluster ensembles worthwhile includes thorough theoretical analysis average normalized mutual information objective including applied soft clusterings 
plan explore possible greedy optimization schemes detail 
greedy scheme introduced section practical 
post processing step refine solutions large 
example supra consensus labeling initialization best single input clustering 
preliminary experiments indicate post processing ects labels yields slightly improved results 
direction better understand biases proposed consensus functions 
extend application scenarios 
real applications variety hybrids investigated fdc odc rcc scenarios encountered 
cluster ensembles enable federated data mining systems top distributed heterogeneous databases 
acknowledgments acknowledge support nsf ecs faculty partnership award ibm tivoli ibm intel 
claire cardie careful prompt editing anonymous referees helpful comments 
alpert kahng 
directions netlist partitioning survey 
integration vlsi journal 
barnett 
computational methods mathematical theory evidence 
proc 
ijcai pages 
barthelemy 
ordered sets problems comparison consensus classifications 
journal classification 
blum mitchell 
combining labeled unlabeled data training 
proceedings th annual conference computational learning theory colt pages 
boley gini gross han hastings karypis kumar mobasher moore 
partitioning clustering web document categorization 
decision support systems 
kurt bollacker joydeep ghosh 
supra classifier architecture scalable knowledge reuse 
proc 
int conf 
machine learning icml pages july 
kurt bollacker joydeep ghosh 
ective supra classifiers knowledge base construction 
pattern recognition letters november 
bradley fayyad 
refining initial points means clustering 
proc 
int conf 
machine learning icml pages july 
strehl ghosh rich caruana 
learning related tasks time backpropagation 
advances neural information processing systems pages 
ghosh 
scale clustering radial basis function network 
ieee transactions neural networks sept 
thomas cover joy thomas 
elements information theory 
wiley 
dasarathy 
decision fusion 
ieee cs press los alamitos ca 
dhillon modha 
concept decompositions large sparse text data clustering 
machine learning january 
dietterich 
ensemble methods machine learning 
kittler roli editors multiple classifier systems pages 
lncs vol 
springer 
ester kriegel sander xu 
density algorithm discovering clusters large spatial databases noise 
proceedings nd international conference kdd pages 
fayyad reina bradley 
initialization iterative refinement clustering algorithms 
proc 
th intl 
conf 
machine learning icml pages 
doug fisher 
iterative optimization simplification hierarchical clusterings 
journal artificial intelligence research 
frakes 
stemming algorithms 
frakes baeza yates editors information retrieval data structures algorithms pages 
prentice hall new jersey 
fred jain 
data clustering evidence accumulation 
proc 
icpr page appear 
friedman slonim tishby 
multivariate information bottleneck 
proc 
seventeenth conf 
uncertainty artificial intelligence uai 
aaai press 
michael garey david johnson 
computers intractability guide theory np completeness 
freeman san francisco ca 
ghosh 
systems back 
keynote talk rd int workshop multiple classifier systems june 
downloadable www lans ece utexas edu publications html 
ghosh 
systems back invited 
roli kittler editors multiple classifier systems pages 
lncs vol 
springer 
ghosh strehl 
consensus framework integrating distributed clusterings limited knowledge sharing 
proc 
nsf workshop generation data mining baltimore pages nov 
cluster ensembles granger 
combining forecasts years 
journal forecasting 
han karypis kumar mobasher 
clustering highdimensional space hypergraph models 
technical report university minnesota department computer science 
jain dubes 
algorithms clustering data 
prentice hall new jersey 
jarvis patrick 
clustering similarity measure shared nearest neighbors 
ieee transactions computers 
johnson kargupta 
collective hierarchical clustering distributed heterogeneous data 
zaki ho editors large scale parallel kdd systems volume lecture notes computer science pages 
springer verlag 
kannan 
computing local consensus trees 
association computing machinery society industrial applied mathematics proceedings acm siam symposium discrete algorithms pages 
kargupta chan editors 
advances distributed parallel knowledge discovery 
aaai mit press cambridge ma 
kargupta huang krishnamoorthy johnson 
distributed clustering collective principal component analysis 
knowledge information systems journal special issue distributed parallel knowledge discovery 
kargupta park hershberger johnson 
collective data mining new perspective distributed data mining 
kargupta philip chan editors advances distributed parallel knowledge discovery 
mit aaai press 
karypis 
han kumar 
chameleon hierarchical clustering dynamic modeling 
ieee computer august 
karypis kumar 
fast high quality multilevel scheme partitioning irregular graphs 
siam journal scientific computing 
george karypis aggarwal vipin kumar shekhar 
multilevel hypergraph partitioning applications vlsi domain 
proceedings design automation conference 
nada lavrac 
consensus decision trees consensus hierarchical clustering data relabelling reduction 
proceedings ecml volume lnai pages 
springer 
kernighan lin 
cient heuristic procedure partitioning graphs 
bell systems technical journal 
kim 
tutorial phylogenetic tree estimation 
intelligent systems molecular biology heidelberg 
strehl ghosh kittler roli editors 
multiple classifier systems 
lncs vol 
springer 
teuvo kohonen 
self organizing maps 
springer berlin heidelberg 
second extended edition 
krogh vedelsby 
neural network ensembles cross validation active learning 
touretzky tesauro leen editors advances neural information processing systems pages 
mehrotra 
multi viewpoint clustering analysis mvp ca technology mission rule set development case retrieval 
technical report afrl vs tr air force research laboratory 
modha scott 
clustering hypertext applications web searching 
proceedings acm hypertext conference san antonio tx may june 
ion muslea steve minton craig knoblock 
selective sampling semi supervised learning robust multi view learning 
ijcai workshop text learning supervision 
neumann norton 
clustering isolation consensus problem partitions 
journal classification 
neumann norton 
lattice consensus methods 
journal classification 
nigam ghani 
analyzing applicability ectiveness training 
proceedings cikm th acm international conference information knowledge management pages 
acm 
duin 
generalized kernel approach classification 
journal machine learning research special issue kernel methods 
pratt 
experiments transfer knowledge neural networks 
hanson rivest editors computational learning theory natural learning systems constraints prospects chapter pages 
mit press 
prodromidis chan stolfo 
meta learning distributed data mining systems issues approaches 
kargupta chan editors advances distributed parallel knowledge discovery 
aaai mit press cambridge ma 
richard lippmann 
neural network classifiers estimate bayesian posteriori probabilities 
neural computation 
sharkey 
combining artificial neural networks 
connection science 
cluster ensembles sharkey 
combining artificial neural nets 
springer verlag 
silver mercer 
parallel transfer task knowledge dynamic learning rates measure relatedness 
connection science special issue transfer inductive systems 
slonim tishby 
agglomerative information bottleneck 
proc 
nips pages 
mit press 
strehl ghosh 
cluster ensembles knowledge reuse framework combining partitionings 
proceedings aaai edmonton canada pages 
aaai july 
alexander strehl joydeep ghosh 
scalable approach balanced high dimensional clustering market baskets 
proc 
bangalore volume lncs pages 
springer december 
alexander strehl joydeep ghosh 
relationship clustering visualization high dimensional data mining 
informs journal computing 
press 
alexander strehl joydeep ghosh raymond mooney 
impact similarity measures web page clustering 
proc 
aaai workshop ai web search aaai austin pages 
aaai july 
thrun 
learning th thing easier learning 
mozer touretzky hasselmo editors advances neural information processing systems pages 
mit press cambridge ma 
thrun pratt 
learning learn 
kluwer academic norwell ma 
sebastian thrun joseph sullivan 
discovering structure multiple learning tasks tc 
th international conference machine learning pages 
tumer ghosh 
linear order statistics combiners pattern classification 
sharkey editor combining artificial neural nets pages 
springer verlag 

