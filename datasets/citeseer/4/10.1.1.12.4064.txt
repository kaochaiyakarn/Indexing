implementation modern web search engine cluster yuntis fully functional prototype complete web search engine features comparable available commercial grade search engines 
particular yuntis supports page quality scoring global web linkage graph extensively exploits text associated links computes pages keywords lists similar pages quality provides flexible query language 
reports experiences year development process yuntis presenting design issues software architecture implementation details performance measurements 
internet scale web search engines represent crucial web information access tools pose software system design implementation challenges involve processing unprecedented volumes data 
equip search engines sophisticated features compounds architectural scale complexity requires integration non trivial algorithms efficiently huge amounts real world data 
yuntis prototype implementation scalable cluster web search engine provides modern search engine functionalities global linkage page scoring relevance weighting phrase extraction indexing generation keywords lists similar pages web pages 
entire yuntis prototype consists lines code represents man year effort 
discuss design implementation issues involved prototyping process yuntis 
intend shed light internal workings feature reach modern web search engine serve blueprint development yuntis 
sections provide background motivation development yuntis respectively 
section describes architecture 
implementation main processing activities yuntis covered section 
section quantifies performance yuntis 
conclude discussing section 
maxim tzi cker chiueh department computer science stony brook university maxim cs sunysb edu chiueh cs sunysb edu background basic service offered web search engines returning set web page urls response user query composed words providing fast hopefully accurate navigation service unstructured set interlinked pages 
achieve search engine acquire examine set urls wishes provide searching capabilities 
usually done fetching pages individual web servers starting seed set encountered links obeying policy limits orders set examined pages 
fetched pages preprocessed allow efficient answering queries 
usually involves inverted word indexing encountered word engine maintains set urls word occurs relevant possibly positional information regarding individual occurrences words 
indexes kept format allows fast intersection merging querying time example sorted order contained urls 
contents examined pages kept relevant page fragments pages users quickly 
frequently indexes constructed instance answer queries backlinks page 
modern search engines google example associate page index text related contained links pointing page :10.1.1.109.4049
appropriate selection weighing text fragments engine leverage page descriptions embedded incoming links 
developing ideas page kleinberg search engines include non trivial methods estimating relevance quality web page query linkage graph web 
methods significantly improve quality search results evidenced search engine improvements pioneered google 
consider methods computing page quality importance scores iterative computation known web linkage graph 
widespread useful search engine functions query spelling correction utilizing collected word frequencies 
understanding certain words form phrases serve descriptive items individual words 
determining descriptive keywords pages page clustering classification advertisement targeting 
automatically clustering search results different subgroups appropriately naming 
building high quality lists pages similar allowing users find alternatives analogs known site 
researchers described design implementation experiences building different operations large scale web search engines example architecture mercator web crawler reported heydon najork 
brin page document design details early google search engine prototype :10.1.1.109.4049
design possibilities tradeoffs repository web pages covered hirai 
bharat describe experiences building fast web page linkage connectivity server 
different architectures distributed inverted indexing schemes discussed melnik ribeiro neto 
contrast primarily focuses design implementation details considerations comprehensive extensible search engine prototype implements analogs derivatives individual functions discussed mentioned papers features 
motivation initially wanted experiment new model voting model computing various quality scores web pages linkage structure web pages context implementing web searching functions 
planned implement various extensions model utilize additional metadata rating categorizing web pages example metadata parsed mined crawled pages metadata external sources directory structure dumps open directory project 
needs underlying system crawling web pages indexing contents doing manipulations derived data presenting query results form appropriate easy evaluation 
system sufficiently scalable support experiments real datasets considerable size page scoring algorithms linkage general produce better results working data 
reasonably scalable complete web search engine implementation openly available easily modify extend experiment needed consider available subcomponents design build prototype 
existing web search engine implementations trade secrets developed systems meant handle small datasets workstation non open research prototypes designed experiment specific search engine technique 
design yuntis main design goals yuntis follows scalability data preparation tens millions pages processed days 
utilization clusters workstations improving scalability 
faster development simple architecture 
extensibility trying new information retrieval algorithms features 
query performance flexibility adequate quickly evaluating quality search results investigating possible ways improvement 
chose implementation language functionality yuntis facilitates development compromising efficiency 
attain decent manageability relatively large code base adopted practice introducing needed abstraction layers enable aggressive code reuse 
templates inline functions multiple inheritance virtual functions provide ways generating efficient code getting close low level bit manipulation needed 
classes inheritance define interfaces provide changeable implementations 
template classes employed reuse complex tasks concepts 
additional abstraction layers introduce run time overheads reuse benefits important building prototype 
high level yuntis architecture maximize utilization cluster pc workstations connected lan yuntis prototype composed interacting processes running cluster nodes see 
instance prototype operational cluster node runs database worker process responsible storing processing retrieving data assigned disks node 
needed node run parser process respectively retrieve parse web pages stored corresponding web lvs db querier seed parser db worker db worker db worker page page page doc 
parser doc 
parser doc 
parser web server web server web server db manager yuntis cluster processes architecture 
node 
database manager process running times particular node 
process serves central control point keeps track yuntis processes cluster helps connect directly 
web servers answering user queries run cluster nodes joined linux virtual server load balancer single service 
auxiliary processes 
database querier helps low level manual examination inspection data managed database worker processes 
database initiate rebuilding data tables feeding system essential data set existing data files 
seed data parsing data dumping process introduce initial data systems extract interesting data 
typical operation scenario yuntis involves starting database manager workers importing initial url seed set directory metadata crawling seed urls parsers complete preprocessing crawled dataset start web server process es answering user search queries 
discuss stages detail section 
library building blocks major design decisions early development affect aspects system 
decisions choosing architecture data storage manipulation querying approach node node cluster data communication 
decided approach interaction web servers providing web pages web clients querying system 
activities capture main processing search engine cluster 
addition pro cess model chosen integrate activities distributed system interacting processes 
choices employ reuse code existing library application implement needed functionality afresh assessing suitability existing code bases comparing expected costs choices 
choices comprehensive performance architecture compatibility suitability testing 
informal evaluation deemed costly testing justified low expectation payoff reveal substantially efficient design choice 
example existing text web page indexing libraries ht dig glimpse designed part distributed large scale web search engine cost redesigning reusing comparable writing code 
process model needed architecture process space simultaneously efficiently support high volumes communication cluster nodes large amounts disk network communication servers clients significant mixing exchange data communicated ways 
wanted support multiple activities kind individually need wait completion network interprocess disk achieve chose event driven programming model uses primary thread control handles incoming events data polling loop 
model processes prototype 
model avoids multi threading overheads task switching stack allocation synchronization locking complexities 
requires introduce call callback interfaces potentially blocking operations abstraction levels file socket operations exchanging data remote database table 
non model requires ensure processing large data items split smaller chunks process react events processing 
event polling loop generalized support interfaces operating system efficient similar kqueue 
added support fully asynchronous disk operations pool worker threads communicating pipe main thread 
reason choosing essentially event driven architecture web server performance studies showing web servers architecture heavy loads significantly outperform web servers apache locate process thread request 
apache code base different process architecture targeted support highly configurable web servers 
web servers thttpd designed just fast lightweight web servers providing modular extensible architecture 
architecture communication servers clients handled extensible hierarchy classes react network socket disk events 
intra cluster communication needed high efficiency communication specific application architecture generality flexibility interoperability applications architectures 
existing network communication frameworks corba soap dcom communication cluster workstations 
employ network message passing libraries mpi pvm appear designed scientific computing oriented support tasks frequently multiprocessors mind actively local disks cluster workstations communicate actively network hosts 
inadequate communication calls mpi pvm require lot threads needs intensive communication 
scalable primitives simultaneously wait messages arriving different points readiness disk network instance connections 
consecutively developed cluster communication primitives 
information service call callback interface set possibly remote procedures consume produce small data items long data streams 
data exchanged untyped byte sequences procedures identified integers 
easy way wrap standard typed interface 
implemented support clients implementations set communicate common tcp socket 
disk data storage full featured database systems mainly expected data processing load required employ distributed system running cluster workstations light weight data management primitives 
needed data storage system minimal processing storage overheads oriented optimizing throughput data manipulation operations latency atomicity individual updates 
high commercial databases appeared satisfy requirements completely time may 
indirect support choice fact largescale web search engines data management libraries page indexing data 
hand current design quite modular easily add database table implementations interface database management library berkeley db database management system provided configured achieve adequate performance 
set database manipulation primitives developed handle large scale disk data efficiently 
lowest abstraction level virtual files large continuous byte arrays data containers database tables 
implementations virtual file interface multiple physical files memory mapped file memory regions 
unified interface allows database access code run physical files memory regions 
database table call callback interface abstraction level defines uniform interface different kinds database tables share common set operations add delete read update part record identified key 
database table implementation composed disjoint interface information services instance allows database table distributed multiple cluster nodes keeping data table physical placement completely transparent code clients 
support safe concurrent accesses database table provide optional exclusive shared locking database record database table levels 
highest abstraction level classes templates define typed objects stored database tables exchanged information services concisely write procedures exchange information database tables es call callback interfaces 
abstraction level enables hide implementation details database tables clean typed interface cost small additional run time overheads 
example frequently read write data table record interested just fields 
external libraries tools heavily relied existing basic compatible libraries tools ones discussed earlier 
standard template library stl proved useful modify enhance memory management functionality adding real memory deallocation eliminate hash table implementation inefficiency erasing elements large sparse table 
gnu library convenient logging assertion checking debugging especially gnu debugger gdb due bugs crashes working core dumps generated processes 
consequently rely logging attaching gdb running process consumes fair amount processing resources 
selective execution logging extensive run time assertion checking greatly helped debugging parallel distributed system 
externalization template library approach provides clean efficient extensible way convert typed object byte sequence compact transmission processes cluster workstations long term storage disk 
parallel compilation gnu utility simple scripts makefiles right granularity individual object files allowed reduce build times substantially utilizing cluster nodes compilation 
example full yuntis build min compilation min linking workstation takes min workstations 
data organization store information kinds objects web hosts urls web sites sets urls probably authored entity encountered words phrases directory categories 
persistently stored data objects presently organized different logical data tables 
data table split partitions evenly distributed cluster nodes 
data tables split partitions data respectively related kinds objects 
numbers chosen ensure manageable size partition data tables targeted size manipulated dataset 
data tables partitions structures indexed array fixed sized records array fixed sized records sorted field record heap addressed set records queues fixed variable sized records 
structures cover needs new data table structures introduced needed 
records structures queues randomly accessible small fixed sized keys 
systemwide keys data tables contain portion choose partition rest key partition locate specific record small set matching records case sorted array structure 
kinds web world objects data tables map object names internal identifiers index fixed sized information records turn contain pointers tables variable sized information related object 
organization easy allows reasonably compact efficient data representation 
partition store data record chosen hash values derived name object record related 
example hash value url maps ith partition items url name url information record lists back forward links url stored ith partition corresponding data table 
result data organization database key textual name object readily determines database partition cluster node object belongs 
data accesses database client choose communicate directly right worker consulting central lookup service 
data manipulation basic form manipulation data stored data tables individual data records parts read written local remote request accessing client activity waits completion request 
kinds inefficiencies eliminate network latency delay remote accesses local data access delays overheads 
occur data needed complete data access brought memory disk cpu cache memory 
involve substantial processing overheads working data file operations accessing memory regions 
avoid inefficiencies rely batched delayed execution data manipulation operations see chiueh full details 
large volume data reading update possible organized sequential reading data table partition files concurrently cluster nodes 
cases need perform sequence data accesses remote core data execute sequence immediately 
batch needed initiation information queue associated group related data table partitions sequence data accesses needs 
batching done remote node cases need immediate confirmation batching completed order continue 
network communication delays masked 
large number initiation records batched queue justify costs processing proceed execute batch loading mapping memory needed data partitions working data memory 
data tables guarantee partitions fit available memory sequentially read disk 
data tables utilization file mapping cache os significantly improved 
approach limited hardware resources guarantee large spectrum dataset sizes cases data manipulation happens data local memory cpu cache low overhead memory access primitives 
model processing utilizes primitives support database tables composed disjoint partitions buffered queues physical files fast operation batching classes start arbiter execution operation batches individual batched operations transparent memory loading mapping selected database table partitions time execution operations batch 
execution batched operation consists manipulating database data memory scheduling operations batching input data appropriate queue possibly cluster nodes 
wait completion inter node queueing batch boundaries 
inter node communication delays block execution individual operations 
high level data processing tasks organized controlling algorithm database manager process initiates execution appropriate operation batches initial generation operations 
proceed cluster nodes parallel 
flow control execution operation batches operation generation database table scanning need flow control hand increase cpu utilization operations allowed execute parallel case block hand batch execution execution single operation paused resumed inter cluster communication buffers needlessly large processed 
adopted solution initiate certain large number operations parallel pause resume execution appropriate checks callbacks depending number pending inter cluster requests node 
allowing order pending inter cluster requests appears fine yuntis workloads 
exact number operations potentially started parallel tuned depending nature processing done class operations ranges 
load execute batch unload load execute batch unload load execute batch unload operation batches execution pipeline 
cpu pipeline data processing organized execution operation batches optimize scheduling pipeline see 
batch goes consecutive stages reading mapping database partitions disk execution operations writing modified database data disk 
middle stage cpu intensive intensive 
shared exclusive locks associated sequence operations achieve pipeline style exclusive overlapped execution cpu intensive sections 
requires double number data partitions data manipulated adjacent batches fits available memory node 
implementation yuntis sections describe implementation details associated issues major processing activities yuntis order execution 
table provides coarse breakdown code sizes major yuntis subsystems 
starting components database manager process started cluster node begins listening designated tcp port 
database worker processes started nodes start listening designated tcp port potential clients advertise presence manager connecting 
soon manager knows workers sends information host port numbers workers worker 
point worker establishes direct tcp connections workers reports complete readiness manager 
processes connected system similar fashion 
process connects manager workers ready information host port numbers workers 
process connects communicates worker directly 
control connections maintained manager processes 
particular clean disconnection shutdown system 
crawling indexing web pages initial step get set web pages organize data form ready usage 
subsystem code lines code bytes logical modules basic libraries web libraries info 
services data storage search engine total table yuntis subsystem code size breakdown 
acquiring initial data data parsing process read file list seed urls files contain xml dump directory structure open directory project publicly available online 
files parsed read appropriate actions initiated database workers inject data system 
way data acquisition parse data files essential data tables available run prototype rebuild data system 
essential tables log domain name resolution results encountered host names log url fetching errors data tables containing compressed raw web pages robots txt files 
rebuilding tables done parser cluster workstation reads injects workers portion data table stored cluster node 
rebuilding example avoid large set web pages modified structure data tables system 
fetching documents web component crawling actual fetching web pages web servers 
done processes cluster workstation 
fetch queue data table constructed incrementally include newly encountered urls 
reads portion queue located node attempts keep retrieving documents parallel obeying robots exclusion conventions 
involves retrieving update consulting contents robots txt files appropriate hosts 
mask response delays individual web servers wish fetch documents parallel simultaneous tcp connections cluster muscle university traffic 
document retrieved successfully compressed stored document database partition local cluster node appropriate record added document parsing queue 
url space responsibility fetching storing split cluster nodes split performed way urls host assigned cluster node 
result particular polite web servers processes need perform negotiation communicate solely local worker processes 
potential downside approach urls get distributed cluster nodes unevenly 
practice saw deviation average number urls node 
parsing documents document parsing factored separate activity fetching documents way obtaining 
parsing performed parser processes cluster nodes 
parsers dequeue information records parse queue retrieve decompress documents parse inject results parsing appropriate database workers 
parsers start portion parse queue documents local cluster node switch documents nodes local queue gets empty 
activities parser happen streaming mode parser communicate workers results parsing long document reading document 
attempt initiate parsing documents parallel node parsers wait document data remote responses workers 
planned optimization eliminate cost page decompression parsing fetched pages 
optimization small dictionary frequent words parser substantial portion word indexing map word strings internal identifiers directly parsers 
having full dictionary feasible instance collected words pages crawl 
word link indexing currently parse html text documents 
full text documents indexed information prominence distances words derived html sentence structure 
links indexed text contained link anchor surrounds link anchor small distance belong anchors text structurally preceding html headers 
links web page weighted estimate prominence page 
result parsing batch data needed perform actual indexing appropriate queues appropriate worker nodes 
parsing done parsing parsing documents indexing batches executed general data manipulation approach 
result gets constructed unsorted inverted indexes words forward backward linkage information information records newly encountered hosts urls sites words 
quality focused crawling breadth crawling performed described components link indexing process includes efficient way collecting newly encountered urls fetching 
problem breadth crawling fall unintended crawler traps fetch lot urls people going interested 
example crawling sunysb edu domain university crawler encounter huge documentation code revision archive mirrors pages quickly reachable breadth search 
employ approach crawling focused page quality scores 
implementation fetching significant new portion urls existing fetch queue instance third number urls fetched earlier execute operations needed index links parsed documents compute page quality scores voting model urls currently known web linkage graph see section 
fetch queue rebuilt organized priority queue filling urls best scores 
document fetching parsing continues new fetch queue starting best urls score estimates 
avoid wasting resources unimportant pages 
post crawling data preprocessing fetched parsed desired set pages indexing operations initiated parsing performed proceed data processing 
involves building lists backlinks lists urls sites located host urls belonging site 
site collection web pages assumed controlled single entity person organization 
presently url assigned exactly site treat sites hosts home pages traditional username syntax home pages located popular web hosting services 
merge hosts domain suffix site control single commercial entity 
order perform phrase extraction indexing described direct page link word indexes constructed 
associated urls contain word identifiers document words words associated links document 
global data processing collecting desired set web pages perform data processing steps collected data specific kind 
computing page quality scores version developed voting model compute global query independent quality scores known web pages 
model subsumes google pagerank approach provides way assess importance web page reliability information terms different information retrieval tasks 
example page scores weigh word occurrence counts unimportant pages skew word frequency distribution 
model uses notion web site determining initial power influence final scores properly discount ability intra site links increase site scores 
result site receive high score just virtue large heavily interlinked case public formulation pagerank 
score computation proceeds stages 
main stage composed iterations consists propagating score increments links collecting destination pages 
large volume data processing score computation steps organized efficient batched execution method 
approach iterations monotonically incur smaller amount increments propagate number increments number web pages concerned reduces 
exploit rely caching touching needed data iterations 
collecting statistics statistics collection various data tables happens separate step parallel cluster nodes sequential scanning relevant data table partitions statistics different data partitions joined 
interesting statistics estimation number objects certain parameter greater lower value 
achieved closely approximating dependencies distribution values object parameters histogram approximations distributions 
types parameters interested quality scores web pages follow power law distribution meaning objects small values parameter close objects high values parameter 
knowledge fitting distributions approximations moving internal boundaries collected histograms 
result passes data arrive close approximation significantly improve passes 
extracting indexing phrases experiment phrases instance keywords documents perform phrase extraction index extracted phrases 
phrases simply sequences words occur frequently 
phrase extraction done stages corresponding possible phrase lengths presently limit length 
stage starts considering forward word phrase indexes documents constitute top respect page quality scores section 
reduce processing costs manipulated low score documents 
sequences words word phrase length counted weighted document quality scores phrase candidates 
candidates high scores chosen phrases 
new phrases indexed checking word sequences documents really instance chosen phrases 
eventually complete forward inverted phrase indexes built 
phrase extraction algorithm purely statistical rely linguistic knowledge 
extracts common noun phrases computer science new york incomplete phrases involve prepositions pronouns 
additional linguistic rules easily added 
filling page scores indexes order able answer user queries quicker beneficial put page quality scores urls inverted indexes sort lists urls scores 
consecutively retrieve portion intersection union url lists words highest url scores constitutes result query examine lists 
page quality scores filled indexes simply consulting score values appropriate urls url information records done delayed batched execution framework information records urls fit memory cluster 
save space map byte floating point score values byte integers mapping derived approximating distribution score values see section 
addition fill similar approximated scores indexes words phrases associated links pointing urls 
keep full url identifiers linking urls indexes 
allows quickly assess weight words describe url incoming links 
sorting various indexes approximated page score values done separate data table modification step 
building linkage information separate stage construct data tables backward url url linkage forward web site url sites linkage backward url sites site linkage forward url url linkage data 
time incorporate page quality scores link lists querying 
extracting keywords compute characteristic keywords encountered pages assessing document similarity 
aggregated keyword lists web sites odp categories 
keyword lists served user part information record url site category 
keywords words phrases related url constructed follows inverted indexes words frequent rare determined hand tuned bounds examined candidate keywords attached urls receive highest scores query composed particular word 
word url pairs common score boundary pass candidate keyword tuned reduce processing yielding candidates choose 
document link text indexes considered similarly query answering extracted document keywords heavily influenced link text descriptions 
able get sensible keywords fetched documents 
urls best candidate keywords chosen rules keep keywords url depending url quality score try discard keywords scores smaller fixed factor logscale best keyword url 
discard keywords phrase containing candidate keyword url score magnitude log scale 
resulting url keywords aggregated candidate keyword sets sites categories similarly pruned 
building directory data open directory project freely provides classification directory structure similar yahoo size quality 
selected part odp directory structure imported incorporated yuntis 
description texts titles listed url fully indexed special kind link texts 
directory structure fully indexed user easily navigate search directory parts see categories url subcategory men tioned 
reason feature appears unique yuntis despite numerous web sites odp data 
interesting problem resolve determine portion subcategory relations directory graph treated subset inclusion relations 
ideally want treat subcategory relations way impossible subcategory graph odp acyclic general 
ended efficient iterative heuristic graph marking algorithm improved behaves better cases tried corresponding methods odp google 
note directory indexing manipulation algorithms distributed cluster utilize delayed batched execution framework 
finding similar pages locating web pages similar topic service purpose set pages sites useful web navigation tool 
algorithms techniques finding similar pages tasks clustering classifying web pages 
yuntis lists similar pages pages sites respect different criteria pages linked high score pages closely source page pages high scored keywords common source page pages link pages source page 
computation organized efficient volume processing relevant pieces similarity evidence 
example textual similarity go pages keywords find pages word keyword choose highest similarity evidences word send relevant destination pages 
destination page similarity evidences different keywords source page combined portion similar pages kept 
result processing consumes linear time number known web pages exact amount processing similar pages kept regulated values determine evidence consider qualify storage 
data compaction checkpointing data table partitions organized heaps variable sized records usually unused gaps extensively manipulated empty space reserved fast expected growth records 
space freed record shrinking record 
reclaim space disk introduced data table compaction stage removes gaps heap table partitions adjusts indexes records partition contained associated information partition 
cheap accomplish information partition easily fits memory 
data preparation yuntis organized stages roughly correspond previous subsections 
alleviate consequences hardware software failures introduced data checkpointing stages take considerable time option restart processing checkpoint 
checkpointing done synchronizing data memory files disks duplicating files making new hard links 
want modify file hard link duplicate data preserve integrity earlier checkpoints 
answering user queries user queries answered web server processes handle requests interact database workers get needed data format results html pages 
query answering standard queries organized sequential reading intersecting merging beginnings relevant sorted inverted indexes structure query 
additional information candidate resulting urls queried parallel urls clustered web sites url names document fragments relevant query displayed user 
flexible data examination yuntis supports boolean connectives near andthen types basic queries freely combined connectives 
cases exact information intra document positions maintained indexes utilized connectives 
interesting consequence phrase extraction indexing considerably speed example factor common queries implicitly include indexed phrases 
cases merging indexes words form phrase done phrase indexing 
performance evaluation describe hardware configuration cluster discuss measured performance sizes handled datasets 
hardware configuration presently yuntis installed node cluster linux pc workstations running red hat linux kernel 
system amd athlon xp cpu mb ddr ram connected mhz bus gb rpm maxtor eide disks model average seek time msec 
large partitions documents stored urls seen hyper links seen inter site links web sites recognized host names seen canonical hosts seen words seen phrases extracted avg 
words document avg 
links document avg 
document size kb avg 
urls site avg 
word length char total final data size mb avg 
data document compressed documents mb inverted doc 
text indexes mb inverted link text indexes mb word data mb keyword data mb page similarity data mb linkage indexes mb url data mb host site mb category data forward word indexes total table data size statistics 
mb disks joined lvm ext file system data storage 
nodes connected local network full duplex mbps port switch network cards 
ample gbps back plane capacity switch ensured bottleneck configuration 
full duplex mbps connectivity cluster node performance bottleneck far seen sustained traffic mbps mbps node potentially available mbps 
additional optimizations increase cpu power leading higher communication volume generated node higher capacity cluster connectivity instance channel bonding mbps cards node 
central management workstation nfs server connected switch noticeably participate workloads yuntis simply providing central place logs configuration files times executables 
cluster connected outside world mbps campus network mbps oc university link 
dataset sizes table provides various size statistics 
example see bulk stored data falls text indexes similarity lists compressed documents 
data performance figures particular crawl pages started fetching urls listed non international portion odp 
numbers simply provide order magnitude estimates typical figures get similar datasets similar hardware 
bzip library compressing individual web pages 
achieves compression factor 
bzip affective applied individual pages compressing document data table partitions save space 
seriously considered additional compression data tables compact data representation bit fields 
data preparation performance demonstrated batched data driven approach data manipulation leads performance improvements factor better cpu memory file cache utilization 
performance sensitive amounts available memory 
table provides various performance utilization figures different stages data preprocessing 
second column gives amount file data duplication needed maintain checkpoint previous stage 
data shows phrase extraction similarity precomputation filling scores indexes expensive tasks basic tasks parsing indexing 
various stages exhibit different intensity disk network nature perform differently terms cpu utilization 
planning instrument prototype determine stage exact contributions possible factors non cpu utilization improve discovered limitations 
peak fetching speed reaches doc sec mb sec document data 
peak parsing speed reaches doc sec 
sustained speed parsing complete indexing encountered words links doc sec 
observed crawling speed crawling pages fetch queue rebuilding getting pages doc sec 
speed parsing complete incurred indexing doc sec 
speed subsequent preprocessing doc sec 
total data preparation processing stage time min cpu utilization disk mb netw 
mb total disk data gb user sys 
idle cloned kept doc 
parsing indexing post parsing indexing pre score statistics page quality scores linkage indexing phrase extr 
indexing word index merging scores word index scores link text index word statistics choosing keywords building directory data word index sorting finding similar pages scores indexes sorting indexes data table compaction table average cluster node data processing performance resource utilization 
speed excluding fetching documents docs sec 
provided performance scales perfectly respect number cluster nodes data size take cluster machines process weeks pages presently covered google looks quite reasonable requirement 
query answering performance yuntis built experimenting different search engine data preprocessing stages optimize query answering speeds basic acceptable level 
single word queries usually take sec relevant data memory sec needed data memory cluster nodes 
longer times dominated need consult thousands url information records scattered disks 
currently caching intermediate query results automatic local file data caching memory os 
performance multiword queries heavily depends specific words straightforward sequential intersecting word indexes significantly outperformed optimized zig zag merging binary search cases large indexes yield smaller intersection 
quality search results illustrate usability yuntis provide samples table report yuntis served searches day average february encourage reader try yuntis 
cs sunysb edu 
enhancements directions number general yuntis improvements extensions performance resource utilization optimizations especially query answering better tuning various parameters implementation novel searching services instance classifying pages odp directory structure 
table shows phrase extraction indexing important areas performance optimization 
approach move phrase indexing initial document parsing derive phrase dictionary separate stage smaller subset representative documents 
significant project build support automatic rebalancing fault tolerance cluster nodes automatically share equally seamlessly added go execution affecting performance 
approach consider sets related partitions different batches operations atomic units data moved 
results query university keywords www apple com pages linked www com pages textually www cs sunysb edu www indiana edu apple computer www toyota com www sunysb edu www umich edu apple macintosh www vw com www cs uiuc edu www stanford edu apple computers www com www cs umass edu www edu macintosh computer www com www cs berkeley edu www uiuc edu macintosh computers www suzuki com www cs colorado edu www cam ac uk apple www porsche com www cs man ac uk www bham ac uk quick time www com www cs stanford edu www cmu edu apple www com www cs virginia edu www msu edu computer www com www cs unc edu www cornell edu macintosh www com www suny edu important scalability balancing issue posed abundance power law distributed properties web 
result data tables records large example individual occurrence lists frequent words grow gb document datasets records table kb 
handling reading updating appending sorting large records efficiently requires special care attempting load map memory working sequentially small portions 
addition records reflect poorly ability divide equally cluster nodes random partitioning set records 
known solution split records subparts example word occurrence list divided partitioning url space 
planning investigate compatibility approach processing tasks need consider records best splitting records table extremely large ones 
described software architecture major employed abstractions techniques implementation main processing tasks yuntis lines feature rich operational search engine prototype 
discussed current configuration performance characteristics handled datasets outlined existing problems roads improvements 
implementation yuntis allowed experiment evaluate identify enhancements voting model assessing quality relevance web pages 
true search engine functions phrase indexing keyword extraction similarity lists precomputation directory data usage integration system table top results typical yuntis queries 
working realistic datasets millions web pages 
important contributors success approach data partitioning operation batching provided high cluster performance task specific optimizations leading convenience implementation faster prototyping 
second modular layered typed architecture data management cluster processing allowed build debug extend optimize prototype rapidly 
third event driven call callback processing model useful allowing relatively simple efficient coherent design components comprehensive search engine cluster 
acknowledgments supported part nsf iri mip eia ani aci 
greatly benefited feedback shepherd erez zadok usenix anonymous reviewers 
availability yuntis prototype accessed online yuntis cs sunysb edu 
source code available download www cs 
sunysb edu maxim yuntis 
adamic 
zipf power laws pareto ranking tutorial 
technical report xerox palo alto research center 
apache web server www apache org 
berkeley database www com 
krishna bharat andrei broder monika henzinger kumar suresh venkatasubramanian 
connectivity server fast access linkage information web 
proceedings th international world wide web conference april 
sergey brin lawrence page 
anatomy large scale hypertextual web search engine 
proceedings th international world wide web conference april 
bzip data compressor www 
com bzip 
junghoo cho hector garcia molina lawrence page 
efficient crawling url ordering 
proceedings seventh world wide web conference 
common object request broker architecture www corba org 
distributed component object model www 
microsoft com com tech dcom asp 
gnu project debugger sources redhat 
com gdb 
google www google com 
allan heydon marc najork 
mercator scalable extensible web crawler 
world wide web december 
jun hirai sriram raghavan hector garcia molina andreas paepcke 
webbase repository web pages 
proceedings th international world wide web conference amsterdam netherlands may 
ht dig search engine www org 
text search engine www 
org html 
dan 
problem www 
com html 
jon kleinberg 
authoritative sources hyperlinked environment 
proceedings ninth annual acm siam symposium discrete algorithms pages san francisco california january 
jonathan lemon 
kqueue generic scalable event notification facility 
proceedings freenix track usenix pages berkeley california june 
maxim 
rank computation methods web documents 
technical report tr department computer science suny stony brook stony brook new york november 
maxim 
voting model ranking web pages 
peter graham maheswaran editors proceedings international conference internet computing pages las vegas nevada june 
maxim tzi cker chiueh 
data preparation large scale web search engines 
vldb proceedings th international conference large data bases august hong kong china 
logical volume manager www 
com products lvm htm 
linux virtual server www 
org 
sergey melnik sriram raghavan beverly yang hector garcia molina 
building distributed full text index web 
proceedings th international world wide web conference hong kong may 
message passing interface www unix mcs 
anl gov mpi 
gnu library www gnu org software 
open directory project www dmoz org 
lawrence page sergey brin rajeev motwani terry winograd 
pagerank citation ranking bringing order web 
technical report stanford university california 
jef 
web server comparisons www acme com software thttpd benchmarks html 
parallel virtual machine www csm ornl 
gov pvm 
berthier ribeiro neto moura ziviani 
efficient distributed algorithms build inverted files 
proceedings nd annual international acm sigir conference information retrieval pages berkeley california august 
web robots exclusion www org wc exclusion html 
simple object access protocol www 
org tr soap 
standard template library www sgi com tech stl 
simple web indexing system humans org 
thttpd web server www acme com software thttpd 
webglimpse search engine software webglimpse net 
externalization template library 
sourceforge net 
yahoo 
www yahoo com 
