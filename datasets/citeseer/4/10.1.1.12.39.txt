draft version appear nd intl 
conf 
autonomous agents multiagent systems aamas coordination multiagent reinforcement learning bayesian approach georgios department computer science university toronto toronto canada cs toronto edu emphasis multiagent reinforcement learning marl research placed ensuring marl algorithms eventually converge desirable equilibria 
standard reinforcement learning convergence generally requires sufficient exploration strategy space 
exploration comes price form penalties opportunities 
multiagent settings problem exacerbated need agents coordinate policies equilibria 
propose bayesian model optimal exploration marl problems allows exploration costs weighed expected benefits notion value information 
standard rl models model requires reasoning actions influence behavior agents 
develop tractable approximations optimal bayesian exploration report experiments illustrating benefits approach identical interest games 

application reinforcement learning rl multiagent systems received considerable attention 
multiagent settings effect benefit agent actions directly influenced agents 
adds complexity learning problem requiring agent learn effects actions coordinate align action choices agents 
fortunately rl methods lead coordinated equilibrium behavior 
empirical theoretical investigations shown standard single agent rl methods circumstances lead equilibria methods designed explicitly account behavior agents :10.1.1.159.617:10.1.1.138.2589:10.1.1.135.717
multiagent reinforcement learning marl algorithms face difficulties encountered single agent settings existence multiple equilibria 
games unique equilibrium strategies values target learned agents defined 
multiple equilibria marl methods face problem agents coordinate choice equilibrium just ac permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
copyright acm xxxxx xx xx xx 
craig boutilier department computer science university toronto toronto canada cs toronto edu tions 
empirically influence multiple equilibria marl algorithms subtle certain game properties convergence undesirable equilibria :10.1.1.159.617
obvious reasons rl methods converge desirable optimal equilibria 
number heuristic exploration strategies proposed fact increase probability guarantee optimal equilibria reached identical interest games 
unfortunately methods encourage force convergence optimal equilibria great cost 
coordination strategy profile requires exploration parts policy space 
case benefits eventual coordination optimal equilibrium ought weighed cost terms reward sacrificed learning play equilibrium 
simply classic rl exploration exploitation tradeoff multiagent guise 
standard rl choice exploiting knows effects actions rewards executing action current knowledge appears best exploring gain information actions rewards potential change action appears best 
multiagent setting tradeoff exists respect action reward information 
aspect comes bear influence action choice action choices agents 
words exploit current knowledge strategies explore try find information strategies 
develop model accounts generalized exploration exploitation tradeoff marl 
adopt bayesian model approach marl single agent model described 
value information play key role determining agent exploration policy 
specifically value action consists components estimated value current model estimates expected decision theoretic value information provides informally ability information change decisions 
augment parts value calculation marl context 
estimated value action current model estimates requires predicting action influence action choices agents 
value information associated action includes information provides agents strategies just environment model 
changes require agent possess model strategies agents adopt bayesian view 
putting derive optimal exploration methods bayesian multiagent systems 
reviewing relevant background describing related existence multiple equilibria negative impact known theoretical results marl :10.1.1.138.2589
draft version appear nd intl 
conf 
autonomous agents multiagent systems aamas marl develop general bayesian model describe certain computational approximations optimal exploration 
describe number experiments illustrating approach experiments focusing identical interest games exclusive target research heuristic exploration methods 

background basic background rl stochastic games 
bayesian reinforcement learning assume agent learning control stochastic environment modeled markov decision process mdp pr finite state action sets reward function dynamics pr 
dynamics pr refers family transition distributions pr probability state reached action taken denotes probability reward obtained state reached 
agent charged constructing optimal markovian policy maximizes expected sum discounted rewards infinite horizon 
policy value computed standard algorithms policy value iteration 
rl setting agent direct access model components pr 
learn policy interactions environment 
number rl techniques learn optimal policy focus model rl methods learner maintains estimated mdp pr set experiences obtained far 
stage suitable intervals mdp solved exactly approximately 
bayesian methods model rl allow agents incorporate priors explore optimally 
assume prior density possible dynamics reward distributions whichis updated data point letting denote current state action history observer posterior determine appropriate action choice stage 
formulation renders update tractable assuming convenient prior 
specifically assumptions density factored product independent local densities transition distribution pr density dirichlet 
model require dirichlet parameter vector entries possible successor state expectation pr ns update dirichlet straightforward prior data vector number observed transitions si posterior parameter vector posterior transition models factored posteriors local families form pr subset history involving transitions updates dirichlet parameters 
treat distribution support finite set possible values general density functions 
write family distributions notational clarity 
assume reward densities modeled similarly dirichlet prior reward probabilities dearden allow gaussian reward distributions incurs serious complications 
bayesian approach advantages approaches model rl 
allows natural incorporation prior knowledge 
second approximations optimal bayesian exploration take advantage model 
elaborate optimal exploration marl context 
stochastic games coordination normal form game tuple ai ui collection agents ai set actions available agent agent payoff function 
letting ai denote set joint actions ui denotes real valued utility obtained agents execute refer ai mixed strategy 
profile collection strategies agent 
write refer agent component denote reduced strategy profile dictating strategies weuse denote full profile obtained augmenting reduced strategy profile 
best response strategy ui ui ai 
br set best responses 
nash equilibrium profile br agents nash equilibria generally viewed standard solution concept games form 
widely recognized equilibrium concept certain descriptive prescriptive deficiencies 
important problem fact games may multiple equilibria leading problem equilibrium selection 
example consider simple player identical interest game called penalty game shown standard matrix form agent moves moves :10.1.1.159.617
payoffs players identical penalty 
pure equilibria corresponding agents matching moves 
optimal equilibria symmetry game induces coordination problem agents 
means breaking symmetry risk incurring penalty choose different optimal equilibria agents fact focus suboptimal equilibrium learning models popular means tackling equilibrium selection problem 
assuming repeated play stage game methods require agent prediction play current stage history interactions play current stage game predictions 
simple model fictitious play stage agent uses empirical distribution observed actions agents past iterations reflective mixed strategy play current stage agent plays best response estimated mixed strategies 
method known converge various senses equilibria certain classes games 
interesting learning model bayesian approach kalai lehrer 
model agent maintains distribution strategies played agents 
strategy space confined strategies stage game allows beliefs strategies agent adopt repeated game 
standard bayesian updating methods maintain beliefs time best responses played agent expectation strategy profiles 
strategy repeated game mapping observed history play stochastic action choice 
admits possibility modeling agents learning processes 
draft version appear nd intl 
conf 
autonomous agents multiagent systems aamas repeated games special case stochastic games viewed multiagent extension mdps 
formally stochastic game ai pr ri consists components 
agents action sets ai typical game components pr mdp pr refers joint actions ai 
ri reward function agent defined states pairs 
aim agent mdp act maximize expected sum discounted rewards 
presence agents requires treating problem game theoretic fashion 
particular classes games zero sum stochastic games algorithms value iteration compute markovian equilibrium strategies 
existence multiple stage game equilibria problem plagues construction optimal strategies stochastic games 
consider simple identical interest example stochastic game shown 
game optimal strategy profiles maximize agents reward 
agent chooses opt state choosing action takes agents high probability state agents choose choose joint strategy gives optimal equilibrium 
intuitively existence equilibria gives rise coordination problem state 
means coordination assuming agents choose part equilibrium randomly chance obtaining expected immediate reward 
basis tempted propose methods agents decide opt agent takes action obtain safe payoff 
allow means coordination example simple learning rules fictitious play randomization sequential nature problem means short term risk compensated eventual stream high payoffs coordinate 
boutilier argues solution games assuming generally history dependent mechanism resolving stage game coordination problems requires explicit reasoning odds benefits coordination expected cost comparing alternatives 
repeated games viewed stochastic games single state 
concern just equilibrium stage game performance sequence interactions techniques solving stochastic games applied solving repeated games 
furthermore points regarding risks associated specific learning rules coordination applied equal force repeated games 
multiagent rl section describe existing approaches marl point efforts augment standard rl schemes encourage rl agents multiagent systems converge optimal equilibria 
intuitively marl viewed direct indirect application rl techniques stochastic games underlying model transitions rewards unknown 
cases assumed learner unaware chooses ignore existence agents 
formally suppose underlying stochastic game ai pr ri consider case agent knows structure games knows set agents actions available agent set states knows dynamics pr reward functions ri 
agents learn act world experience 
point time agents known state agent executes actions state resulting joint action induces transition state probability pr reward ri agent ri 
assume agent observe actions chosen agents resulting state rewards ri 
game theoretic approaches marl proposed littman devised extension learning zero sum markov games called minimax :10.1.1.135.717
state agents estimated values joint actions compute estimated equilibrium value state 
generalization update rule equilibrium value replaces usual max operator minimax shown converge equilibrium value stochastic game 
hu wellman apply similar ideas equilibrium computation estimated values estimate state values general sum games somewhat weaker convergence guarantees :10.1.1.138.2589
algorithms devised agents observe behavior counterparts 
identical interest games drawn attention ai providing suitable models task distribution decomposition teams agents 
claus boutilier proposed marl methods repeated games context :10.1.1.159.617
simple joint action learner jal protocol learned myopic stage values joint actions standard learning updates 
novelty approach lies exploration strategy fictitious play protocol estimates strategies agents exploration biased expected value actions 
specifically estimated value action expected value expectation taken fictitious play beliefs agents strategies 
semi greedy exploration method converge equilibrium underlying stage game 
drawback jal method fact equilibrium converges depends specific path play stochastic 
certain equilibria exhibit serious resistance example odds converging optimal equilibrium penalty game described quite small decrease dramatically magnitude penalty 
claus boutilier propose heuristic methods bias exploration optimal equilibria instance action selection biased actions form part optimal equilibrium 
penalty game instance despite fact agent may predicted play strategy look unpromising repeated play justified optimistically assuming play part optimal equilibrium 
motivated fact repeated play eventually draw equilibrium 
issue learning optimal equilibria identical interest games addressed greater detail 
lauer riedmiller describe learning method identical interest stochastic games explicitly embodies optimistic assumption value estimates 
kudenko propose method called repeated games uses optimistic assumption bias exploration context individual learners explicit access actions performed agents :10.1.1.159.617
wang sandholm similarly optimistic assumption repeated games guarantee convergence optimal equilibrium :10.1.1.19.2669
critique methods 

bayesian view marl activity described marl identical interest games focused exclusively devising methods ensure eventual convergence optimal equilibria 
cooperative games pursuit defined circumstances may justified 
methods account fact forcing agents undertake actions potentially draft version appear nd intl 
conf 
autonomous agents multiagent systems aamas tic effects order reach optimal equilibrium dramatic impact accumulated reward 
penalty game devised show highly penalized states bias supposedly rational agents away certain equilibria optimistic exploration methods ignore blindly pursue equilibria costs 
certain performance metrics average reward infinite horizon justify techniques 
discounted reward criterion methods designed tradeoff long term benefit short term cost addressed 
tradeoff discussed context known model stochastic games 
section attempt address tradeoff rl context 
formulate bayesian approach model marl 
maintaining probabilistic beliefs space models space opponent strategies learning agents explicitly account effects actions knowledge underlying model knowledge agent strategies expected immediate reward expected behavior agents 
components classical parts single agent bayesian rl model 
components key multiagent extension allowing agent explicitly reason potential costs benefits coordination 
theoretical underpinnings assume stochastic game agent knows game structure reward transition models 
learning agent able observe actions taken agents resulting game state rewards received agents 
agent experience point time simply state joint action taken rn vector rewards received agent resulting state 
bayesian marl agent prior distribution space possible models space possible strategies employed agents 
beliefs updated agent acts observes results actions action choices agents 
strategies agents may history dependent allow bayesian agent ba assign positive support strategies 
order accurate predictions actions take ba monitor appropriate observable history 
general history summary thereof required function strategies ba assigns positive support 
assume ba keeps track sufficient history predictions 
belief state ba form pm ps pm density space possible models games played ps joint density possible strategies played agents current state system summary relevant aspects current history game sufficient predict action choice agent strategy consistent ps 
experience ba update belief state standard bayesian methods 
updated belief state refined measures bias optimality cast techniques favorable light 
example ba believe opponent strategy lies space finite state controller depends joint actions played ba need keep track actions 
uses fictitious play beliefs viewed dirichlet priors strategies history need maintained 
updated densities obtained bayes rule specifically pr pm pr ps 
andh suitable update observed history 
model combines aspects bayesian reinforcement learning bayesian strategy modeling 
belief state maintenance tractable admit computationally viable methods action selection assume specific form beliefs 
prior models factored independent local models re wards transitions 
assume independent priors re ward distributions state andp system dynamics state joint action pair 
assume local densities dirichlet conjugate multinomial distributions wish learn 
means density represented small number hyperparameters expected transition probabilities computed readily density updated easily 
example ba prior beliefs transition probabilities joint action state represented vector parameter successor state expected transition probability specific state updating beliefs described section 
note independence local densities assured update 
second assume beliefs opponent strategies factored represented convenient form 
example natural assume strategies agents independent 
simple fictitious play type models model ba beliefs opponent strategy correspond dirichlet priors agent mixed strategy space allow ready update computation expectations 
beliefs require history stored belief state 
similarly distributions specific classes finite state controllers 
pursue development models simple opponent models experiments 
development tractable classes realistic opponent models remains interesting problem 
provide different perspective bayesian exploration described 
value performing action ai belief state viewed involving main components expected value respect current belief state impact current belief state 
component typical rl second captures expected value information action 
action gives rise response environment changes agent beliefs changes belief influence subsequent action choice expected reward wish possibly indirectly quantify value information determining impact subsequent decisions 
need computed directly combined object level expected value bellman equations belief state mdp ai pr pr ai pr ai max ai ai equations describe solution pomdp represents exploration exploitation problem conversion belief state mdp 
principle solved method solving high dimensional continuous mdps course practice number computational shortcuts approximations required detail 
complete specification draft version appear nd intl 
conf 
autonomous agents multiagent systems aamas discounted average accumulated reward runs naive vpi sampling analytic step lookahead method wolf combined boltzmann exploration optimistic boltzmann exploration kk number iterations 
priors straightforward definition terms pr pr pr discounted average accumulated reward runs pr ps pr pm pr pm note evaluation eqs 
trivial decomposed dirichlet priors mentioned 
formulation determines optimal policy function ba belief state 
policy incorporates tradeoffs exploration exploitation respect underlying dynamics reward model respect behavior agents 
bayesian rl bayesian learning games explicit exploration actions required 
course important realize model may converge optimal policy true underlying stochastic game 
priors fail reflect true model unfortunate samples early easily mislead agent 
precisely behavior allows agent learn behave drastic penalty 
computational approximations solving belief state mdp generally computationally infeasible 
specific marl problems generality solution defining value possible belief state needed anyway 
belief states reachable agent specific initial belief state 
directed search solution technique solve mdp agent current belief state consider form myopic immediate successor belief states considered optimal values estimated voi lookahead 
formally myopic action selection defined follows 
belief state function ai ai qm ai pr pr ai pr ai vm vm max ai pm ps ai action performed maximum myopic value 
eq 
differs eq 
myopic value function vm naive vpi sampling analytic step lookahead method wolf combined boltzmann exploration optimistic boltzmann exploration kk number iterations 
priors penalty game results discounted average accumulated reward runs naive vpi sampling analytic step lookahead method wolf combined boltzmann exploration optimistic boltzmann exploration kk naive vpi sampling uninformative priors penalty analytic step la uninformative priors penalty number iterations inform 
priors defined expected value optimal action current state assuming fixed distribution models strategies 
intuitively myopic approximation performs step lookahead belief space evaluates successor states determining expected value ba fixed distribution models fixed distribution successor states 
computation involves evaluation finite number successor belief states states number joint actions number rewards size state space restricts number reachable states plausible strategies 
important note accurate results realized doing multistage lookahead requisite increase computational cost 
conversely optimal action approximated sampling successor belief states induced distributions defined eqs 
branching factor problematic 
final bottleneck involves evaluation myopic value function vm successor belief states 
note terms ai values standard mdps evaluated standard methods direct evaluation integral models generally impossible 
sampling techniques 
specifically number models sampled corresponding mdps solved expected estimated averaging sampled results 
various techniques making process efficient including importance sampling allowing results mdp multiple times reweighting repair solution mdp solving related mdp 
furthermore certain classes problems repeated games evaluation performed directly 
instance suppose repeated game learned ba strategy model consists fictitious play beliefs 
immediate expected reward action ai taken ba specific successor belief state expectation respect estimated reward distribution fictitious play beliefs 
maximizing action highest immediate reward best action subsequent stages repeated game fixed immediate reward myopic value assumption beliefs fixed long term value 
approaches motivated approximating direct myopic solution exploration pomdp different approach approximation proposed 
algorithm note vm provides crude measure value belief state fixed uncertainty 
measures include expected value 
draft version appear nd intl 
conf 
autonomous agents multiagent systems aamas chain world domain 
experimental results conducted number experiments repeated games stochastic games evaluate bayesian approach 
focus player identical interest games largely compare existing methods encouraging convergence optimal equilibria 
bayesian methods examined step lookahead discounted average accumulated reward runs bayesian exploration wolf number iterations chain world results discounted average accumulated reward runs bayesian exploration wolf number iterations step lookahead belief space estimates myopic value obtaining perfect information 
suppose agent current belief state expected value action 
leta bethe action highest expected value state second highest 
define gain associated learning true value fact follows gain intuitively gain reflects effect decision quality learning true value specific action state cases learned causes change decision case estimated optimal action learned worse predicted second action learned better predicted 
third case change decision induced information impact decision quality 
adapted setting computational approximation approach called naive sampling involves steps finite set models sampled density pm sampled mdp solved density ps strategies giving optimal values ai ai mdp average value ai mdps ai compute gain ai ai average mdps gain ai method bol naive sampling method estimating vpi described section 
cases bayesian agents simple fictitious play model represent uncertainty agent strategy 
iteration ba believes opponent play action empirical probability observed past games beliefs independent state 
bol repeated games allow immediate computation expected values infinite horizon successor belief states expected reward joint action readily combined ba fictitious play beliefs compute expected value action infinite horizon model uncertainty reward 
repeated multi state games 
cases models sampled estimate vpi agent actions 
strategy priors bas dirichlet parameters prior counts opponent action 
model priors similarly uninformative state action pair prior distribution reward transition distributions experiment noted 
compare method different algorithms stochastic version penalty game described 
game altered joint actions provide stochastic reward mean value shown game matrix 
ai define value ai ai ai execute action highest value 
approach benefit importance sampling repair minor modifications 
key advantage approach computationally effective step lookahead requires sampling solving mdps multiple belief states 
price paid approximation inherent perfect information assumption execution joint action come close providing perfect information 
compare bayesian approach algorithms 
kk refers algorithm kudenko biases exploration encourage convergence optimal equilibria exactly types games 
related heuristic algorithms bias exploration optimistically optimistic boltzmann ob combined ob cb evaluated :10.1.1.159.617
kk algorithms observe predictions agent play 
test general algorithm wolf phc works arbitrary general sum stochastic games special heuristics equilibrium selection 
case game played learning agents type 
parameters algorithms empirically tuned give performance 
set experiments tested agents stochastic penalty game set discount factors 
bol agents uninformative priors set reward values 
results appear figures showing total discounted reward accumulated learning agents averaged trials 
discounted accumulated reward pro joint action gives rise distinct rewards 
specifically possible reward joint action equal expected probability agent dirichlet priors 
draft version appear nd intl 
conf 
autonomous agents multiagent systems aamas vides suitable way measure cost paid attempt coordinate benefits coordination lack thereof 
results show bayesian methods perform significantly better regard methods designed force convergence optimal equilibrium 
ob cb kk converge optimal equilibrium virtually runs clearly pay high price 
comparison ratio convergence optimal equilibrium nonoptimal equilibrium nonequilibrium bol 
surprisingly wolf phc better ob cb kk tests 
fact method outperforms agent case due largely fact converges nonequilibrium times suboptimal equilibrium times 
wolf phc converge instance optimal equilibrium 
repeated comparison difficult problem mean penalty increasing variance reward action 
test benefits bayesian perspective included version bol informative priors giving strong information value rewards 
results shown averaged runs 
wolf phc cb converge optimal equilibrium time increased reward stochasticity impossible kk ob converge 
methods perform poorly discounted reward 
bayesian methods perform better 
surprisingly agents bol informative priors better uninformed counterparts high penalty despite high discount factor converge suboptimal equilibrium time times respectively 
applied bayesian approach identical interest multi state stochastic games 
version chain world modified multiagent coordination illustrated 
optimal joint policy agents action state actions payoff state reached 
coordinating leads immediate smaller payoff state resets process 
unmatched actions result zero reward self transitions omitted diagram clarity 
transitions noisy chance agent action effect opposite action 
original chain world difficult standard rl algorithms penalty game experiments run games interesting initial segments graphs shown 
rewards stochastic means shown 
discounted average accumulated reward runs bayesian exploration wolf number iterations opt results low noise discounted average accumulated reward runs bayesian exploration wolf number iterations especially difficult requirement coordination progress 
compared wolf phc domain different discount factors plotting total discounted reward received averaged runs 
graphs show initial segments results project smoothly iterations 
see compares favorably wolf phc terms online performance 
bayesian agents converged optimal policy runs intuitively reflecting increased risk aversion due increased discounting 
wolf phc rarely managed reach state runs early converge optimal policy 
bayesian approach case manages encourage intelligent exploration action space way trades risks predicted rewards see increased exploration higher discount factor expected 
second multi state game opt game shown 
transitions stochastic action selected agent having effect opposite action probability 
versions problem tested low noise probability action effect reversed medium noise level probability roughly 
low noise optimal policy domain deterministic agent opts play coordinated choice medium noise opt policy opt policy safe play moving adopted roughly equal value 
compared wolf phc different discount rates low noise results shown high noise results 
terms discounted reward bayesian agents compare favorably wolf phc 
low noise problem converged optimal policy runs 
wolf agents converged optimal policy times 
medium noise chose opt policy runs manage learn coordinate opt cases 
interestingly wolf phc converged opt policy recall policies optimal medium noise 

concluding remarks described bayesian approach modeling marl problems allows agents explicitly reason uncertainty regarding underlying domain strategies counterparts 
provided formulation optimal exploration draft version appear nd intl 
conf 
autonomous agents multiagent systems aamas discounted average accumulated reward runs discounted average accumulated reward runs bayesian exploration wolf number iterations bayesian exploration wolf number iterations opt results medium noise model developed computational approximations bayesian exploration marl 
experimental results demonstrate quite effectively bayesian exploration enables agents tradeoffs described 
results show enhance online performance measured reward accumulated learning marl agents coordination problems compared heuristic exploration techniques explicitly try induce convergence optimal equilibria 
implies bas run risk converging suboptimal policy risk taken willingly due consideration learning process agent current beliefs domain 
see bas find optimal strategies case 
key ba willingness exploit knows confident knowledge simply needs confident willing sacrifice certain alternatives 
framework general experiments confined identical interest games fictitious play style beliefs admittedly simple opponent models 
results encouraging need extended ways 
empirically application framework general problems critical importance verify utility 
computational approximations estimating vpi solving belief state mdp needed 
development computationally tractable means representing reasoning distributions strategy models required 

boutilier 
sequential optimality coordination multiagent systems 
proceedings sixteenth international joint conference artificial intelligence pages stockholm 
bowling 
convergence problems general sum multiagent reinforcement learning 
proceedings seventeenth international conference machine learning pages stanford ca 
bowling veloso 
rational convergent learning stochastic games 
proceedings seventeenth international joint conference artificial intelligence pages seattle 
claus boutilier :10.1.1.159.617
dynamics reinforcement learning cooperative multiagent systems 
proceedings fifteenth national conference artificial intelligence pages madison wi 
dearden friedman andre 
model bayesian exploration 
proceedings fifteenth conference uncertainty artificial intelligence pages stockholm 

competitive markov decision processes 
springer verlag new york 
drew fudenberg david levine 
theory learning games 
mit press cambridge ma 
selten 
general theory equilibrium selection games 
mit press cambridge 
hu wellman 
multiagent reinforcement learning theoretical framework algorithm 
proceedings fifteenth international conference machine learning pages madison wi 
samet 
learning play games extensive form valuation 
economics peer reviews economics publications 
kalai lehrer 
rational learning leads nash equilibrium 
econometrica 
kudenko 
reinforcement learning coordination cooperative multi agent systems 
proceedings eighteenth national conference artificial intelligence pages edmonton 
lauer riedmiller 
algorithm distributed reinforcement learning cooperative multi agent systems 
proceedings seventeenth international conference machine learning pages stanford ca 
littman :10.1.1.135.717
markov games framework multi agent reinforcement learning 
proceedings eleventh international conference machine learning pages new brunswick nj 
littman ri 
generalized reinforcement learning model convergence applications 
proceedings thirteenth international conference machine learning pages bari italy 
robinson 
iterative method solving game 
annals mathematics 
shapley 
stochastic games 
proceedings national academy sciences 
tan 
multi agent reinforcement learning independent vs cooperative agents 
proceedings tenth international conference machine learning pages amherst ma 
wang sandholm :10.1.1.19.2669
reinforcement learning play optimal nash equilibrium team markov games 
advances neural information processing systems nips vancouver 
appear 
