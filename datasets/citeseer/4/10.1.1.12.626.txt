empirical comparison boosting algorithms real data sets artificial class noise ross mcdonald david hand imperial college london shell research 
boosting algorithms means building strong ensemble classifier aggregating sequence weak hypotheses 
consider best known boosting algorithms adaboost logitboost brownboost 
algorithms adaptive maintaining set example class weights focus attention base learner examples hardest classify 
conduct empirical study compare performance algorithms measured terms test error rate real data sets 
tests consist series cross samples 
validation set aside third data chosen random test set fit boosting algorithm remaining thirds binary stumps base learner 
stage record final training test error rates report average errors confidence interval 
add artificial class noise data sets randomly reassigning class labels repeat experiment 
find brownboost proves overfit circumstance algorithm incorporates extra parameter allows compensate noisy examples 
boosting algorithms origins analysis theory surrounding pac probably approximately correct learning model introduced valiant 
pac framework data set said strongly pac learnable exists classifier achieve arbitrarily low error rate instances set 
weak learnable set requires algorithm marginally outperform random guessing terms error rate 
kearns valiant proposed definitions learnability equivalent proven shown possible transform weak learner arbitrarily strong 
schapire published hypothesis boosting algorithm 
robust boost majority bbm algorithm introduced freund time 
essential property boosting algorithm possible derive upper bound final training error rate 
precursor algorithms defined upper bound terms fl amount weak learner guaranteed outperform random guessing class case weak learner guaranteed yield error rate gamma fl 
practice usually unreasonable assume base learning algorithm outperform fixed error rate definition weak learner requires outperform random guessing arbitrarily small amount 
freund schapire published adaptive algorithm known adaboost prior assumptions base learner focus subsequent research 
adaboost short adaptive boosting algorithm characterised adaptive way generates combines weak hypotheses 
monotonically decreasing upper bound training error derived performance individual component hypotheses 
base learner consistently outperform random guessing iterate long eventually achieve arbitrarily small error rate 
possible derive approximate upper bound error rate fitted aggregate hypothesis new data 
subsequently observed adaboost effect approximating stagewise additive logistic regression model optimising exponential criterion 
leads new variants adaboost fit additive models directly 
variant logitboost uses newton steps optimise loss criterion 
general terms observed boosting algorithms tend overfit number iterations run 
particularly susceptible class noise take mean proportion class labels redefined random note authors alternative definition 
examples hardest classify noisy data weak hypotheses induced iterations examples dominate tend undue influence final combined hypothesis leading poor generalisation performance 
empirical comparison methods constructing ensembles decision trees dietterich concluded performance adaboost destroyed classification noise 
brownboost introduced freund boost majority algorithm may help address problem 
derived considering happens bbm algorithm example reweighting assumed happen continuous time 
leads adaptive algorithm resembles adaboost incorporates extra parameter time parameter roughly corresponds proportion noise training data 
algorithm knows advance long run attempt fit examples learnable remaining time 
noisy examples estimate time parameter brownboost capable avoiding overfitting 
shown adaboost special case brownboost limit time parameter allowed tend infinity 
conduct series empirical tests real data sets implementations adaboost logitboost brownboost algorithms 
report results terms test error rate 
randomly reassign class labels datasets rerun tests 
sections briefly describe boosting algorithms turn 
setting adaboost brownboost algorithms adopt notation consistent freund schapire 
multi class logitboost algorithm quoted friedman 
section describe empirical study detail report findings 
section summarise 
adaboost adaboost adaptive boosting algorithm received deal attention introduced freund schapire 
multi class version algorithm uses hamming decoding error correcting output codes ecoc method allwein 
see full description 
algorithm equivalent adaboost 
mh algorithm described analogue multi class extension brownboost algorithm 
adaboost works fitting base learner training data vector matrix weights 
updated increasing relative weight assigned examples misclassified current round 
forces learner focus examples finds harder classify 
iterations output hypotheses combined series probabilistic estimates training accuracy 
adaboost algorithm may characterised way hypothesis weights ff selected example weight update step 
iteration fl gain weak learner random guessing hypothesis weight chosen ff ln fl gamma fl weight update step multiplies weights exponential function confidence prediction times true label value scaled gammaff multi class adaptation algorithm maintain separate weight example class label 
calling base learner take account possibility fit binary class model model returns separate independent predictions class labels 
case assume coding matrix theta matrix diagonal entries 
assume hypotheses generated base learners output confidence rated predictions real values range gamma 
adaboost algorithm multi class version inputs ecoc matrix theta coding matrix training set set labelled examples xn yn xn yn fy yk yn associated matrix set binary labels gamma 
weights theta vector initial weights say xn yn weaklearn weak learning algorithm 

binary base learner call weaklearn times time passing weight distribution defined normalizing xn yn fixed training data set alongside binary labels defined column matrix multi class base learner call weaklearn passing training data full set weights 
receive weaklearn set hypotheses advantage random guessing pm xn xn xn fl 

select ff ln fl gammafl 
weight update xn yn xn exp gammaff xn xn output final hypotheses sign ff 
multi class adaboost algorithm logitboost logitboost algorithm observation adaboost essence fitting additive logistic regression model training data 
additive model approximation function form fm constants determined fm basis functions 
assume mapping seek fit strong aggregate hypothesis weak hypotheses shown class adaboost algorithm fitting model minimising criterion gammayf true class label gamma 
logitboost minimises criterion newton steps fit additive logistic regression model directly optimise binomial log likelihood gammalog gamma yf multi class version algorithm quoted 
logitboost algorithm multi class version 
start weights 

repeat repeat compute working responses weights jth class gammap gammap gamma ii 
fit function weighted squares regression ij weights ij 
set gamma gamma fmk 
set enforcing condition fk 
output classifier argmax 
multi class logitboost algorithm quoted 
brownboost brownboost continuous time adaptive version boost majority algorithm 
quote multi class extension algorithm 
derivation algorithm scope briefly summarise key points 
total time parameter sets total amount time algorithm set run 
iteration quantity subtracted algorithm terminates remaining time reaches 
example yn class algorithm maintains margin 
initially set iteration updated yn yn ff label related example class ecoc matrix 
hypothesis weights ff derived solving differential equation weight updates function margin 
relate parameter final training error ffl strong hypothesis ffl gamma erf erf error function 
select guarantee desired final error 
brownboost algorithm multi class version inputs ecoc matrix theta coding matrix training set set labelled examples xn yn xn yn fy yk yn associated matrix set binary labels gamma 
weaklearn weak learning algorithm 
positive real valued parameter 
small constant avoid degenerate cases 
data structures prediction value example associate set real valued margins 
margin example xn yn label iteration denoted xn yn 
initial value margins zero xn yn 
initialize remaining time 
associate example label positive weight xn yn gamma xn 
binary base learner call weaklearn times time passing weight distribution defined normalizing xn yn fixed training data set alongside binary labels defined column matrix multi class base learner call weaklearn passing training data full set weights 
receive weaklearn set hypotheses advantage random guessing pm xn xn xn fl 
fl ff real valued variables obey differential equation dt dff fl pm exp gamma xn ffh xn gammat xn exp gamma xn ffh xn gammat xn yn xn constants context 
boundary conditions ff solve set equations find ff ff fl 
margin update xn yn xn yn ff xn 
update remaining time gamma output final hypotheses erf ff 
multi class brownboost algorithm 
experiments conducted series tests data sets summarised table 
data sets exception credit taken uci machine learning repository 
credit data set credit scoring data set type commonly commercial banking 
details data omitted table commercial reasons 
wisconsin known diagnostic data set breast cancer compiled dr william wolberg university wisconsin hospitals 
wine data chemical analysis italian wines balance data records results psychological experiment class data sets included test multi class versions algorithms 
data class set entries attributes classes distribution wisconsin credit wine balance table summary table data sets experiments 
order ensure algorithmic convergence time available credit data set curtailed examples 
indicator variables substituted categorical variables occurred see section details 
constructed noisy version data set assigning randomly chosen incorrect class label training examples 
implemented adaboost brownboost matlab purpose written binary stump algorithm base learner 
logitboost implemented plus internal tree function generate binary stumps 
experiment consisted trials logitboost 
trial third data examples selected random set aside test set 
remaining thirds examples train algorithm 
recorded final error rates output hypothesis training test data 
trained adaboost logitboost original data give benchmark comparison recall adaboost equivalent brownboost final training error set 
trained algorithms noisy data 
trials adaboost allowed run training loss matched expected training loss rate data set maximum iterations 
approach coding matrix theta matrix diagonal entries entries 
logitboost allowed run training loss matched expected loss rate maximum iterations 
avoided numerical instabilities algorithm prescriptions 
brownboost calculated appropriate values equation 
training test loss error rates trial recorded confidence interval tables 
artificial class noise adaboost logitboost data set training error test error training error test error wisconsin sigma sigma sigma sigma credit sigma sigma sigma sigma wine sigma sigma sigma sigma balance sigma sigma sigma sigma table error rates adaboost logitboost unmodified data sets confidence intervals artificial class noise 
artificial class noise adaboost logitboost brownboost data set training error test error training error test error training error test error wisconsin sigma sigma sigma sigma sigma sigma credit sigma sigma sigma sigma sigma sigma wine sigma sigma sigma sigma sigma sigma balance sigma sigma sigma sigma sigma sigma table error rates adaboost logitboost brownboost data sets artificial class noise confidence intervals 
discussion results interesting logitboost proves better able fit unmodified versions data sets appears yield better generalisation error adaboost 
exception wine data set algorithms able learn fully trials presumably consists relatively examples 
case confidence interval logitboost wide draw firm 
broadly speaking results bear claims adaboost especially susceptible class noise providing strong evidence brownboost particularly robust situations 
surprised logitboost compares favourably brownboost suggests property algorithm possibly fact implicitly avoids making large weight updates especially robust overfitting 
immediately evident test error rates tables class noise real data seriously impairs generalisation performance adaboost 
appear tally observations dietterich 
implementing brownboost able calculate value directly prior knowledge 
course real situation know level class noise advance 
remains seen difficult prove estimate practice 
ram supported shell research research number engineering physical sciences research council 

uci machine learning repository 
www ics uci edu mlearn mlrepository html 

allwein schapire singer 
reducing multiclass binary unifying approach margin classifiers 
journal machine learning research 

bauer kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning 

breiman friedman olshen stone 
classification regression trees 
wadsworth 

dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
ai magazine 

freund 
boosting weak learning algorithm majority 
information computation 

freund 
adaptive version boost majority algorithm 
machine learning 

freund schapire 
decision theoretic generalization line learning application boosting 
second european conference computational learning theory 

freund schapire 
short boosting 
journal japanese society artificial intelligence 

friedman hastie tibshirani 
additive logistic regression statistical view boosting 
annals statistics 

hand 
construction assessment classification rules 
john wiley sons chichester 

jiang 
results weakly accurate base learners boosting regression classification 
proceedings international workshop multiple classifier systems pages 

kearns valiant 
learning boolean formulae finite automata hard factoring 
technical report tr harvard university aiken computation laboratory 

mangasarian wolberg 
cancer diagnosis linear programming 
siam news 

mcdonald hand 
multi class extension brownboost algorithm 
submission 

quinlan 
effect noise concept learning 
michalski carbonell mitchell editors machine learning artificial intelligence approach volume san mateo ca 
morgan kauffmann 

quinlan 
programs machine learning 
morgan kaufmann 

quinlan 
bagging boosting 
aaai iaai 

schapire 
strength weak learnability 
machine learning 

schapire freund bartlett lee 
boosting margin new explanation effectiveness voting methods 
annals statistics 

schapire singer 
improved boosting algorithms confidence rated predictions 
machine learning 

valiant 
theory learnable 
artificial intelligence language processing 
