guidance relational reinforcement learning kurt driessens sa zeroski department computer science department intelligent systems leuven jo stefan institute belgium slovenia kurt driessens cs kuleuven ac saso dzeroski ijs si reinforcement learning learning particular encounter major problems dealing large state spaces 
learning function tabular form may infeasible excessive amount memory needed store table converges state visited multiple times 
second rewards state space may sparse random exploration discovered extremely slowly 
rst problem solved learning generalization encountered examples neural net decision tree 
relational reinforcement learning rrl approach learning feasible structural domains incorporating relational learner learning 
solve second problem reasonable policies provide guidance suggested 
investigate best ways provide guidance di erent domains 
learning watkins form reinforcement learning optimal policy learned implicitly form function takes state action pair input outputs quality action state 
optimal action state action largest value 
main limitations standard qlearning related number di erent state action pairs may exist 
principle represented table entry state action pair 
states actions characterized parameters number pairs grows combinatorially number parameters easily large making infeasible represent function tabular form learn accurately convergence function happens state action pair visited times 
problem typically solved integrating learning algorithm inductive learner learns function generalizes state action pairs 
reasonable estimates value state action pair having visited 
examples include neural networks bertsekas tsitsiklis nearest neighbour methods smart kaelbling regression trees chapman kaelbling 
relational learner employed zeroski zeroski name relational reinforcement learning rrl 
rrl uses rst order representations states actions learns rst order regression tree kramer blockeel maps structural descriptions real numbers 
rst order representations gives rrl broader application domain classical learning approaches 
examples relatively complex applications described detail include learning solve simple planning tasks blocks world learning play certain computer games tetris 
dealing large state spaces usually case structured domains problem encountered learning rewards may distributed sparsely space 
previous driessens zeroski suggested supply guidance learning system start learning process provide knowledge explore rest state space 
try explore ways supplying guidance rrl 
remainder structured follows 
section discuss di erent ways guidance incorporated qlearning approach 
section number structural domains discuss speci diculties encountered solving tasks domain 
section experimental results domains comparing di erent ways providing guidance rrl conclude section 
related refer previous driessens zeroski 
approach rrl tg system rrl tg algorithm described driessens 
rrl system learning system uses rst order representation encountered states actions resulting function 
rrl starts running normal episode just standard reinforcement learning algorithm uses episode generate set examples tree induction 
episode rst order regression tree builder tg supplied encountered state action qvalue triplets incrementally builds rst order regression tree represents 
episode values predicted generated tree calculate values set examples 
providing guidance random policies hard time reaching sparsely spread rewards large world possible reach rewards reasonable number steps reasonable policies 
optimal policies certainly reasonable non optimal policies easy easier implement generate optimal ones 
obvious candidate non optimal reasonable controller human expert 
integrate guidance reasonable policies supply relational reinforcement learning system policy generate number behaviour traces rewards actions trace receive 
case human controller log normal operation system corresponding rewards 
generated traces translated state action qvalue triplets rrl algorithm 
generalisation algorithm responsible generation rrl state action qvalue triplets generated way build partial 
earlier driessens zeroski supplied guidance traces learning experiment 
generally turned help rrl reach higher levels performance noticed occurrence overgeneralization regression algorithm tg 
propose counter overgeneralization ect caused supply optimal state action combinations interleaving guided traces exploration traces 
compare di erent rates guidance supplied 
guided traces actions chosen pre de ned reasonable policy observed rrl 
exploration traces actions chosen autonomously rrl current estimate boltzmann statistics kaelbling :10.1.1.134.2462
generalization engine tg di erence types traces 
simply transform encountered state action value triplet training example learn 
formal explanation approach driessens zeroski 
test di erent suggestions example applications blocks world computer game tetris 
optimal policy provide guidance blocks world dicult impossible construct optimal policy tetris 
compare guidance di erent fairly simple policies performs considerably better 
case blocks world try go step kind guidance provide 
test performance strategy generated rrl regular time intervals tell rrl cases failed 
time rrl asks guidance allow ask help cases clear 
clear 


move 
floor 
floor 
example state action blocks world 
solve 
named approach active similarity technique active learning strategies classi cation tasks cohn 
domains blocks world blocks world blocks rst testing environment 
blocks oor stacked 
represent states actions blocks world shown prolog notation 
available actions move block block oor 
limit number blocks oor 
addition supply rrl algorithm number blocks number stacks background predicates equal height di erence ordinary subtraction numerical values 
blocks blocks world large illustrate ect guidance learning remaining simple explore di erent guidance strategies 
blocks close possible states blocks world 
study goal putting speci ed block top speci ed block 
please note rrl permits variables description goal function allows learn stack blocks having restart learning process changing names blocks 
blocks world blocks states satisfy speci goal 
reward case goal state reached optimal number steps episode ends reward tetris game 

limits number steps allowed episode 
tetris game tetris known puzzle video game played dimensional grid 
di erently shaped blocks fall top game eld ll grid 
object game keep blocks top game eld 
move dropping blocks right left rotate fall 
horizontal row completely lled line disappears player scores points 
blocks pile top game eld game ends 
tests looked strategic part game shape dropping block decide optimal orientation location block game eld 
low level actions turn move left move right reach subgoal trivial easily learned rrl 
dropping block instantaneous 
impossible setup slide block place sideways 
represent full state tetris game type dropping block included 
allow rrl predicates topblock fits tetris invented owned tetris blue planet software 

system gets reward action point action deleted line 
comments domains large state spaces hard solve ordinary learning 
problems tackled rrl learning shown bene added guidance driessens zeroski 
tetris policy supply reward steps quite frequent 
single bad action long lasting ects hiding obvious rewards learning algorithm 
fact tetris stochastic game shape falling block unknown hard application learning 
tetris game optimal hard may impossible construct 
experiments expect spreading guidance uences learning performance 
terms learning speed expect guidance help learner discover high yielding policies earlier learning experiment 
early discovery important states actions early availability state action pairs generalization engine expect possible learner reach higher level performance higher average reward testing current strategy available time 
test ects compare rrl di erent guidance frequencies setup run rrl natural form form supply guidance learning episode driessens zeroski 
give possibility train certain number episodes regular time intervals extract learned policy rrl test number randomly generated test problems 
rrl guidance replace number episodes traces hand coded policy 
test result manner original rrl 
note matter frequency supply rrl guidance average reward number episodes blocks world original rrl guided traces start guided traces learning curves blocks world 
total amount guided traces cases 
blocks world clearly shown spreading guidance helps terms learning speed level performance 
providing equal amount guidance learning gives relatively comparable increase level performance terms learning speed 
explained interaction tg algorithm fact optimal traces 
supply rrl optimal traces overgeneralization occurs 
generalization engine encounters non optimal action learns distinguish optimal non optimal actions 
create tree separates states different distances goal state exploration expand tree account optimal non optimal actions states 
trees usually larger 
rrl able generalize non optimal actions states close goal optimal actions states little goal leaf tree 
spreading guidance supplying optimal traces interleaved random learning process traces avoids overgeneralization 
course problem consequence solution strongly related model building average reward number episodes blocks world guided traces guided traces guided traces comparing di erent guidance frequencies 
ation algorithms tree builders rulebased systems 
generalization techniques instance approaches su er problem 
shows uence di erent frequencies provide guidance 
note cases equal amount guidance 
intuitively best spread available guidance thin possible performed experiments show negative results doing 
spreading guidance small amount available guided trace episodes prevent guidance having ect 
speci solution sparsity guidance rrl tg algorithm rrl ask guidance episodes bound permanent decision 
case tree learner tg new split leaf 
guidance case speci check reasonable policy contradict proposed split 
alternatively decide store guided traces re traces forgotten split tg tree 
looking general solution try provide larger batch guidance rrl time try explore state space 
related human teaching strategy providing student perfect strategy start learning ective average reward number episodes blocks world guided traces guided comparing spread guidance delayed batch guidance 
ing student strategy time explore systems behavior 
shows inferior results approach compared spread guidance probably due large size batch 
note learning takes place faster guidance provided 
tetris game guidance strategy tetris experiment simple included rules 
take action creates new holes increase current height wall playing eld 

action type take action increase current height wall playing eld 

action type taken take action create new hole 

action type exists take random action 
strategy scores average lines game 
due lack sucient time run experiments shown graphs performance curves single learning experiments show erratic behavior 
better results nal version 
average reward number episodes tetris original rrl guided traces start guided traces start guided traces learning curves tetris 
average reward number episodes tetris guided traces guided traces guided traces comparing di erent guidance frequencies 
shows improvement rrl guidance 
problem overgeneralization apparent tetris experiments experiment starting traces perform better original rrl included experiment gave rrl guided traces start experiment show rrl bene early guidance 
improvement received spreading guidance apparent blocks world 
note average number lines deleted rrl rises game strategy guidance reaches lines game 
shows comparison guidance frequencies 
noticed blocks world experiments providing lot guided traces average reward number episodes tetris simple strategy better strategy comparing di erent guiding policies 
experiment slow progress rrl system initial stages experiment little di erence performances experiment 
test uence performance policy guidance designed simple strategy tetris 
addition rules tested number deleted lines block cause got performance guidance strategy lines game average 
results di erent strategies shown 
graph shows signi cant increase performance better policy guide rrl 
shows exploration insensitivity table qlearning kaelbling carry learning generalization :10.1.1.134.2462:10.1.1.134.2462
notice guidance strategy improved lines game improvement resulting strategy learned rrl smaller 
active guidance stated section blocks world episode started randomly generated starting position rrl know cases failed reach goal state give rrl opportunity ask guided traces starting states 
allow rrl explore parts state space knowledge supply tg algorithm examples correctly predicted 
average reward number episodes blocks world guided traces active learning curves blocks world 
shows results active guidance 
little di erence performance approaches learning experiments active guidance succeeds pushing rrl better performance learning experiment 
percentage cases rrl reach goal state reduced 
completely consistent expect 
systems see new examples able increase accuracy function 
algorithms large part state space covered function speci examples provided active guidance allow function extended cover outer reaches state space 
idea active guidance attractive intuitively practice easy extend approach applications stochastic actions xed starting state tetris game block dropped chosen randomly starting state empty playing eld 
tetris game imagine remembering entire sequence blocks asking guidance strategy game sequence large di erence state provided di erent actions anticipate ect approach small 
step active guidance stochastic environments keep track actions states large negative ect 
example tetris game notice large increase height wall playing eld 
remembered states ask guidance 
approach requires large amount administration inside learning system needs priori indication bad results actions 
explored ways including guidance reinforcement learning 
relational reinforcement learning rrl experiments assume results carry forms qlearning regression approximate function 
shown spreading guidance entire learning large uence speed learning level performance obtained 
results obtained approach su er guidance spread 
feel spreading guidance possible yield best results provided guidance available lose uence noise elimination 
uence performance strategy guidance small 
assuming reasonable policy policy able discover rewards statespace reasonable rate policy learned rrl depends little performance policy guidance 
rrl system able improve performance guidance policy 
active guidance help improve performance rrl case deterministic applications 
speci examples provided active guidance helps function cover larger area state space allowing learning algorithm zoom unknown areas state space 
hard expand approach stochastic applications maintaining simplicity elegance original idea 
possible directions include tighter integration guidance generalization engine rrl tg hand investigation ects guidance types learn ing algorithms instance qlearning hand 
bertsekas tsitsiklis 

neuro dynamic programming 
athena scienti blockeel de raedt ramon 

top induction clustering trees 
pages 
chapman kaelbling 

input generalization delayed reinforcement learning algorithm performance 
pages 
cohn atlas ladner 

improving generalization active learning 

driessens zeroski 

integrating experimentation guidance relational reinforcement learning 
sammut ho mann editors proceedings nineteenth international conference machine learning pages 
morgan kaufmann publishers driessens ramon blockeel 

speeding relational reinforcement learning incremental rst order decision tree learner 
proceedings th european conference machine learning pages 
springer verlag 
zeroski de raedt blockeel 

relational reinforcement learning 
shavlik editor proceedings th international conference machine learning icml pages 
morgan kaufmann 
kaelbling littman moore 

reinforcement learning survey 
journal arti cial intelligence research 
stefan kramer 

structural regression trees 
proceedings thirteenth national conference arti cial intelligence pages cambridge menlo park 
aaai press mit press 
smart kaelbling 

practical reinforcement learning continuous spaces 
proceedings th international conference machine learning pages 
morgan kaufmann 
christopher watkins 

learning delayed rewards 
ph thesis king college cambridge 
