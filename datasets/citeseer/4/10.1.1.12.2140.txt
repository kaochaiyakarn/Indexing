integrated approach hierarchy abstraction pomdps pineau sebastian thrun august cmu ri tr school computer science carnegie mellon university pittsburgh pa research sponsored nsf itr robotics career program darpa mars program contract darpa program contract darpa mica program contract gratefully acknowledged 
views contained document authors interpreted necessarily representing ocial policies endorsements expressed implied united states government sponsoring institutions 
presents algorithm planning structured partially observable markov decision processes pomdps 
new algorithm named polca policy contingent abstraction uses action decomposition partition complex pomdp problems hierarchy smaller subproblems 
low level subtasks solved rst partial policies model actions context higher level subtasks 
levels hierarchy subtasks need consider reduced action state observation space 
reduced action set provided designer reduced state observations sets discovered automatically subtask subtask basis 
typically results lower level subtasks having high resolution state observations features high level subtasks tend low resolution state observation features 
presents detailed overview polca context pomdp hierarchical planning execution algorithm 
includes theoretical results demonstrating special case fully observable mdps algorithm converges recursively optimal solution 
experimental results included demonstrate usefulness approach range problems show favorable performance compared competing function approximation pomdp algorithms 
presents real world implementation deployment robotic system uses polca context high level robot behavior control task 
keywords markov decision process pomdp reinforcement learning hierarchical planning abstraction robot control dialogue systems 
problem planning uncertainty received signi cant attention scienti community past years 
real world problem domains robotics human computer interfaces characterized signi cant degrees uncertainty 
long recognized considering uncertainty planning decision making imperative design robust computer systems 
long established approach planning uncertainty markov decision processes mdps 
mdps allow stochastic action ects described probabilistic state transition functions 
result mdp planning just sequence actions case classical strips planning fikes nilsson 
policy action selection prescribes choice action possible state encountered execution 
large domains computing policies computationally challenging 
example techniques value iteration highly popular approach mdp policy computation require time quadratic number states 
overcome problem various researchers developed techniques exploit intrinsic properties domain 
dominant paradigms large scale mdp problem solving function approximation structural decomposition abstraction 
function approximation techniques generalize states actions calculating policies reducing number states actions considered 
seminal example approach tesauro td gammon program integration multilayer perceptrons sutton td family algorithms led policy achieved world class backgammon playing strength 
examples include domains continuous state spaces exact methods inapplicable certain robot path planning problems moore atkeson 
equally important class ecient algorithms structural decomposition domain 
idea planning problem decomposed collection smaller problems solved separately 
divide conquer strategy lead considerable computational savings 
seminal example structural decomposition approach dietterich max algorithm 
max decomposes planning problem hierarchy smaller subproblems 
subproblem de ned reduced state action space possesses subgoal generally sub reward function requires termination condition 
restricting subproblems smaller state action spaces solved eciently 
results signi cant speed ups solving complex problems 
note literature structural decomposition mdps ers range alternative algorithms improved planning structural decomposition singh dayan hinton kaelbling dean lin boutilier meuleau singh cohn sutton parr russell wang mahadevan andre russell 
approaches limited apply mdps 
cope stochastic action ects stochastic imperfect sensors 
mdps rely assumption state world sucient statistic thereof sensed reliably noise point time 
clearly case real world problems 
robotics example sensor limitations pervasive seemingly simple problem recovering state sensor measurements key subject research entire sub elds borenstein everett feng cox wilfong 
similarly research natural language dialog systems typically seeks devise techniques recovering state information conversing person asoh torrance 
problems state world partially measurable motivate research described 
long recognized general framework addressing planning sensor limitations partially observable markov decision processes pomdps sondik kaelbling littman cassandra monahan 
pomdps just mdps important di erence policies de ned information states world states ones directly observable 
space information states space beliefs system regarding world state 
information states easily calculated measurements noisy imperfect sensors 
pomdps information states typically represented probability distributions world states 
unfortunately space probability distribution states larger state space reason planning pomdps computationally harder mdps 
fact best known bound exact solution doubly exponential planning horizon singly exponential cases starting state known 
enormous complexity arguably important obstacle applying pomdps successfully practice 
computational complexity pomdp planning need fast approximations pomdps prevalent mdps 
unfortunately design ecient pomdp algorithms received relatively little attention scienti community compared mdps 
function approximation approaches proposed papers white lovejoy littman cassandra kaelbling parr russell brafman thrun roy thrun hauskrecht bayer dietterich bonet ge ner consistently demonstrating signi cantly faster planning problems hard exact planners :10.1.1.51.5703:10.1.1.126.4744
lack ective methods employ structural decompositions domain 
describes algorithm hierarchical pomdp planning algorithm called polca 
similar dietterich max algorithm new algorithm decomposes planning problem hierarchy subproblems 
subproblem treated separate pomdp solved separately subproblems 
lowest level problem solving polca solves collection pomdp problems characterized reduced action space 
reduced action space leads signi cant computational savings number actions major impact computational costs planning 
high level subproblems action set contains actions subsume lower level subtasks policy 
addition reduced action sets subproblem uses subset state observation spaces 
reduction follows directly reduced action set eliminating action natural ignore state observation features ected speci action 
furthermore state observation abstraction high level subtasks delayed lower level subtasks solved name algorithm polca policy contingent abstraction represents fact abstraction high level subtasks contingent policy lower level subtasks 
show aspect algorithm key obtaining signi cant state abstraction 
solving complex problems hierarchical mdp approaches calculate combine partial value functions full value function 
contrast hierarchical pomdp approach focuses complex policy combining 
moving away value function able circumvent subtask termination conditions required virtually hierarchical mdp algorithms 
particularly important nature pomdps state perception incomplete rendering determination state termination condition impossible 
result algorithm applied range structured pomdp problems 
solving exactly able successfully address partially observable planning problems requiring non trivial information gathering sequences 
problems scope existing function approximation pomdp solutions 
addition stating basic polca algorithm ers systematic experimental results di erent domains 
domains possess natural structure facilitates hierarchical decompositions 
particular task instance information contingent pomdp successful policy requires multiple information gathering actions putting reach pomdp approaches 
addition describe high level robot control task unique service robot deployed real world application assistant elderly person controlled explicit pomdp style representations uncertainty 
algorithm rst hierarchical pomdp approach hernandez mahadevan theocharous mahadevan wiering schmidhuber 
earlier algorithms typically signi cant assumptions regarding ability fully observe completion subproblems require 
note approach assumptions 
important reliance human designer provide structural decomposition 
decade research simpler mdp paradigm shown nding decompositions automatically extremely dicult pickett barto ryan mcgovern barto thrun schwartz 
argued russell norvig providing structural decompositions provides opportunity bring bear background knowledge human designer naturally possess speed planning 
second assumption concerns fact certain tasks arti cial sub reward functions provided 
property shared rich body mdps exceptions exist thought opportunity bring bear background knowledge human designer 
assumptions extensive comparisons state art algorithms polca outperforms information contingent tasks exhibiting comparable performance goal seeking tasks 

partially observable markov decision processes probabilistic model exposition brief partially observable markov decision processes pomdps setting forth notation 
similar kaelbling 

formally pomdp characterized distinct quantities denoted states 
state world denoted set states denoted fs state time denoted discrete time index 
state directly observable pomdps agent compute belief state space observations 
infer belief regarding world state agent take sensor measurements 
set measurements observations denoted fo observation time denoted observation usually incomplete projection world state contaminated sensor noise 
actions 
act world agent set actions denoted fa actions ect state world 
choosing right action function history core problem pomdps 
pomdps instances markov processes world state separates pearl past 
notational convenience commonly assumed actions observations alternated time 
assumption restrict general expressiveness approach 
fully de ne pomdp specify probabilistic laws describe state transitions observations 
law distributions initial state probability distribution probability domain state time 
distribution de ned states state transition probability distribution probability transitioning state agent state selects action 
conditional probability distribution 
notation suggests time invariant stochastic matrix change time 
time variant state transition probabilities state include time related variable 
observation probability distribution probability agent perceive observation executing action state conditional probability de ned triplets 
de ne objective action selection function optimized planning process 
pomdps mdps alike common assume agent reward function reward function 
goal agent maximize reward time 
mathematically commonly de ned nite sum form reward time mathematical expectation discount factor ensures sum equation nite 
easy see reward function general goal functions agent seeks arrive goal states 
items states actions observations reward probability distributions de ne probabilistic world model underlies pomdp 
belief computation key characteristic sets pomdps aside probabilistic models mdps fact state directly observable 
agent perceive observations fo convey incomplete information world state 
rst glance appear agent maintain complete trace observations actions executed select optimal action 
wellknown fact history summarized belief distribution boyen koller posterior probability distribution belief distribution sucient statistic history suces condition selection actions growing sequence past observations actions 
furthermore belief time calculated recursively belief time step earlier action observation 
normalizing constant 
equation equivalent decades old bayes lter commonly applied context hidden markov models rabiner 
continuous generalization forms basis kalman lters kalman 
interesting consider nature belief distributions 
nite state spaces assumed belief continuous quantity de ned simplex describing space distributions state space large state spaces calculating belief update equation computationally challenging 
research led ecient techniques belief state computation exploits structure domain expressed bayes networks boyen koller poupart boutilier 
note far complex aspect pomdp planning generation policy action selection described 
example robotics calculating beliefs state spaces states easily done real time burgard 
contrast calculating optimal action selection policies exactly appears infeasible environments states kaelbling directly size state space complexity optimal policies 
reason exclusively addresses computational complexity involved policy generation planning assuming state spaces hand small belief calculated exactly 
computing policy central objective pomdp compute policy selecting actions 
policy form belief distribution action chosen policy 
particular interest notion optimal policy policy maximizes expected discounted cumulative reward argmax clearly computing optimal policy challenging 
de ned high dimensional continuous space space belief distributions 
straightforward approach nding optimal policies remains value iteration approach iterations dynamic programming applied compute increasingly accurate values belief state value function maps belief states values max th value function constructed th virtue recursive equation max conditional probability belief function denotes indicator function argument true function belief updating function de ned equation 
remarkable result sondik shows nite horizon problem terminates nite time value function piecewise linear convex continuous function belief composed nitely linear pieces 
iterations value expected sum possibly discounted pay agent receives time steps belief state step look ahead policy reminiscent update rule equation argmax maximizes expectation discounted sum time steps 
optimal planning horizon 
bounded time pomdp problems solved exactly appropriate choice horizon 
environment agent able bound planning horizon advance policy approximation optimal quality improve planning horizon pomdps nite state action observation spaces solved ectively selecting appropriate unfortunately best known exact algorithms computing optimal value function appear worst case require time doubly exponential planning horizon kaelbling singly exponential cases initial state known 
speci cally single step value iteration may require space time order jaj number actions number observations represents set linear components necessary represent value function horizon 
practice appears grow singly exponentially clever mechanisms pruning unnecessary linear functions computing value function enormous computational complexity serious impediment applying pomdps practical problems 
remarked research led function approximation approaches white lovejoy littman parr russell brafman thrun roy thrun hauskrecht bayer dietterich bonet ge ner lack ective methods employ structural decompositions lower computational burden imposed pomdps :10.1.1.51.5703:10.1.1.126.4744

hierarchical task decompositions key idea polca reduce complexity planning hierarchically decomposing pomdp problems 
basic idea polca structural decomposition analog hierarchical mdp literature task naturally maps hierarchy subtasks planner take advantage structure solving individual subtasks separately jointly 
computational savings methodology arise fact solving subtasks usually computationally ecient solving single task times large 
mdps problem solving usually requires time quadratic size state space gives indication savings attain optimal decomposition 
pomdps complexity calculating policies larger may doubly exponential planning horizon worst case singly exponential practical problems 
potential savings may attain structural decomposing pomdp problem larger demonstrated empirically experimental results section 
common mdp strategy de ning subtasks partitioning state space applicable decomposing pomdps special attention paid fact state fully observable 
reason action reduction greater impact state reduction planning complexity pomdps equation polca relies structural decomposition task action space 
decomposition expressed task hierarchy illustrated 
state reduction follows action reduction form subtask speci abstraction 
general form task hierarchy structurally decomposes action space pomdp 
hierarchy subsumes problem divided subtasks fh respective action sets fa fa fa actions fa original action set actions actions introduced represent structural constraints 
de ning task hierarchies formally rst discuss intuition 
conjecture task hierarchies re ect natural decomposition complex task simpler subtasks 
formally task hierarchy represented tree 
top level root tree represents task de ned pomdp 
bottom level individual leaf corresponds individual action henceforth refer primitive action 
primitive actions represent lowest level policy choice 
internal nodes tree represent subtasks 
subtasks de ned limited sets subtasks primitive actions speci ed children tree 
internal node hierarchy double interpretation 
relative children speci es task involves limited set subtasks primitive actions 
relative tasks higher hierarchy speci es action action invoking subtask 
bar denote action 
subtasks polca second important characteristic apart de ned limited number tasks actions 
value subtask depends subset state features similarly subset observations relevant 
examples situations experimental results section 
accommodate situations task hierarchy speci es node subset states observations relevant respective subtask 
hierarchical approach relies important assumptions related domain knowledge 
structured mdp approaches assume hierarchical subtask decomposition provided designer 
constitutes prior knowledge brought bear domain facilitate planning 
addition assume pomdp model original non hierarchical problem 
sections provide formal de nition task hierarchy illustrate alters planning plan execution pomdps 
argued notion task hierarchies raises fundamental questions exploit task hierarchies pomdp planning plan execution 
rst question non trivial nodes hierarchy represent tasks relative children actions relative parents 
raises important issues example tie value functions subtasks actions 
second question non trivial decision plan execution time subtask responsible selecting nal primitive action executed agent 
polca developed speci cally goal solving pomdps address speci case mdp problem solving 
section start introducing simpler mdp formulation polca shares similarities earlier algorithms maxq dietterich andre russell 
section complete general pomdp formulation 

hierarchical mdps markov decision process mdp special case pomdp current state world assumed fully observable time step 
mdp de ned tuple fs rg meaning pomdps state set action set de nes transition probabilities de nes costs rewards 
full observability assumption mdps unique observation emitted state 
necessary consider observation probabilities planning belief tracking trivial 
structural assumptions proposed hierarchical mdp algorithm requires set structural assumptions similar earlier hierarchical mdp approaches 
formally assume task hierarchy pomdps de ned tree 
tree nodes internal labeled leaf nodes 
leaf node contains primitive action individual actions may multiple leaves task hierarchy 
primitive action associated leaf tree chosen eliminated mdp rst place 
internal node corresponds de ned mdp subtask containing fa set actions allowed subtask hierarchy action immediate child set include primitive actions fa actions corresponding lower level subtasks 
set actions completely speci ed subtask 
fz set states relevant subtask function de nes state abstraction mapping states clusters states planning occur clusters states 
assume abstraction preserves sucient resolution solve function identity function case subtask comprise states state abstraction automatically part algorithm 
fs set terminal states subtask fs subtask terminated reaching state point control returned higher level subtask initiated pseudo reward function specifying relative desirability terminal state 
de nition practice set zero goal achieving terminal states large negative value non goal terminal states 
provided designer assume clustered set automatically subtask original set polca requires parameterized model mdp domain fs rg 
contrast hierarchical reinforcement learning algorithms rely exploration acquire model 
require perform automatic state abstraction 
planning algorithm subtask hierarchical planning algorithm optimizes corresponding local policy de nition subtask action set say policy de ned action subset local policy 
table describes hierarchical mdp planning algorithm 
computes set local policies subtask simple steps explained details 

notation gh terminal states goal non goal common th attempt avoid confusion notation transition probabilities 
plan step re formulate structured state space 
subtask bottom ordering step set parameters 


step minimize states step solve subtask table main planning function 
function called parameterized mdp model rst argument hierarchy second argument 
step re formulate structured state space steps apply subtask separately highly world state di erent clustering assignments nal policy di erent subtasks 
consequently step reformulates state space adding state variable re ect hierarchy state 
idea rst suggested ham framework parr russell 
new state space 
equal cross product original state space fs hierarchy state fh hm nal structured state space 
fh 

hm 
hm 
step set parameters purpose second step appropriately translates transition reward parameters speci ed fs tg structured problem representation 
involves translating parameters original state space structured state space 
typically straight forward parameter conditioned primitive actions 
involves inferring parameters newly introduced actions non trivial operation 
subtask action set fa fa set primitive actions invoke corresponding lower level subtasks fh equations translate parameters original mdp structured state space 
case primitive actions full line transition arrows 



case actions dotted line transition arrows 

hp 
hp 
note equations depend hp nal policy subtask important feature algorithm 
reason traverse hierarchy bottom way subtask parameterized subtasks hierarchy solved 
hierarchical ordering important implications state abstraction example signi cantly increasing clustering possibilities subtasks multiple terminal states 
shown experimental section 
step minimize states goal step learn minimization function mapping individual states clusters states 
state abstraction called state clustering model minimization reduce size planning problem accelerating solving 
automatic state abstraction done subtask subtask basis mdp model minimization algorithm dean givan 
infer function mapping states fh 

expanding set clusters fc initialize state clustering 

ii check stability cluster cluster deemed stable 



iii cluster unstable split fc step ii satis ed corresponding re assignment 
typically done evaluating cluster splits greedily choosing split improves stability 
summary part set overly general clusters proposed parts ii iii applied repeatedly gradually splitting clusters salient di erences model parameters intra cluster di erences 
algorithm exhibits desirable properties state proof see dean givan dean givan leach details 
states cluster value 

clarify notation case consider ap action subtask ap subsumes subtask hp hp policy hp state 
planning clusters converges optimal solution 

algorithm relaxed allow approximate stable state abstraction 

assuming mdp factored state space steps implemented avoid fully enumerating state space 
point particular interest avoiding full state space enumeration paramount large problems state abstraction planning solving 
part original model minimization algorithm dean givan possible compute abstraction abstracting just abstracts 
hand crafting abstraction functions hierarchical mdps dietterich andre russell 
advantage abstracting allow di erent state abstractions di erent actions potentially resulting exponential reduction number clusters 
policy visits state action pair new mdp states state action pairs transitions xed policy 
run clustering algorithm new mdp 
step solve subtask purpose step learn value function policy subtask state clustering step described step ensures states cluster share value convenient fold value function updates directly clustering algorithm 
state minimization augmented value updates iii equations iv update value cluster max argmax case algorithm terminates converges 
non hierarchical mdps certain conditions equivalent boutilier dearden goldszmidt decision tree representation mdp value functions 
desired optimized approximate value function problem retrieved simply looking value function top subtask 

illustrates entire process simple state subtask problem 
procedure starts model state problem action hierarchy containing subtasks respective action sets fa fa structured state space augments state problem adding subtask identi er variable fh subtask lower level addressed rst 
starts undergoing parameterizing parameters conditioned actions fa transposed original model subtask 
model minimization procedure reduces states clusters symmetry transition probabilities 
value iteration yields nal policy subtask 
subtask address 
parameterization step transposes parameters original model describe uses policy model action clustering subtask reduces subtask states 
value iteration yields nal policy illustrated bottom right corner 
generating policies subtasks achieved full planning solution problem 
mdp hierarchy set parameters cluster states solve policy set parameters cluster states solve policy structured state space simple state problem subtask hierarchy 
non zero transition probabilities illustrated mdp top left corner 
subtask hierarchy illustrated top right corner 
note nal policy bottom left model context higher level subtask right column 
reward function included keep example simple 
execution algorithm necessary specify execution algorithm uses collection local policies extract global policy 
hierarchical execution algorithm maps current state primitive action executed agent 
pre computing global policy explicitly propose recursive online algorithm generate policy action time step 
execution corresponds trace subtask tree 
algorithm described table starts consulting local policy root task yields policy action execute primitive action return action child subtask spanned execute child table recursive execution function 
function initially called root subtask rst argument current state second argument 
primitive 
case action policy corresponding lower level subtask consulted hierarchy primitive action selected 
primitive action selected execution trace terminated action applied agent 
important emphasize full top trace hierarchy repeated time step 
departure hierarchical mdp planning algorithms operate subtask multiple time steps terminal state reached common approach impractical pomdps guarantee detection terminal states 
uses polling approach consistent kaelbling dietterich polca extends easily pomdps 
theoretical implications better discuss theoretical performance algorithm introduce useful de nitions adapted dietterich de nition hierarchical optimality class policies consistent hierarchy policy said hierarchically optimal achieves reward policies de nition recursive optimality subtask action set class policies available assuming xed policies subtasks hierarchy policy said recursively optimal achieves reward policies set subtask policies recursively optimal policies recursively optimal respect children 
de nitions commonly literature describe compare hierarchical mdp planning algorithms 
example known maxq achieves recursive optimality dietterich ham parr russell options sutton andre russell achieve hierarchical optimality 
main di erence cases attributed context 
recursively optimal solution obtained subtask policies optimized regards context called 
contrast hierarchical optimality achieved keeping track possible contexts calling subtasks key subtasks multiple goal states 
result trade solution quality representation hierarchical optimality stronger optimality result recursively optimal algorithm allows state abstraction 
theorem recursive optimality polca 
fs rg mdp fh hm subtask graph de ned terminal states functions 
planning algorithm table computes recursively optimal policy consistent proof start proving theorem holds case planning algorithm applied state abstraction step table 
structural induction requires rst show policy lowest level subtask recursively optimal assuming policy higher level subtask recursively optimal 
rst consider low level subtask containing primitive actions fa actions 
applying steps yields local policy convergence value iteration apply step policy optimal respect action set furthermore strictly optimal recursively optimal 
consider higher level subtask containing action set fa actions associated corresponding subtasks fh assume subtasks respective policies hp hq shown recursively optimal 
applying steps yields policy proof contradiction show recursively optimal 
assume exists policy di ers consequently max max substituting equation equation simplifying max max max max general case hold say similarly conclude recursively optimal 
extension proof case state abstraction depends proof included dean givan shows model minimization algorithm preserves policy quality 
terminate section pointing polca achieves recursive optimality stronger hierarchical optimality speci cally xes low level subtask policies prior solving higher level subtasks 
show experimental section restricting polca weaker form optimality achieve greater state abstraction possible 

hierarchical pomdps attempt generalizing hierarchical mdp algorithms address partially observable mdps overcome obstacles 
planning execution respect beliefs states 
second unreasonable assume terminal states subgoals detected 
section constitutes cornerstone presents full polca algorithm pomdp generalization version introduced section 
algorithm remains unchanged mdp formulation important modi cations necessary accommodate partial observability include observation space observation probabilities algorithm 
structural assumptions structural assumptions necessary hierarchical pomdp planning part similar hierarchical mdp assumptions 
add set observations relevant subtask 
formally stated task hierarchy internal node represents separate pomdp subtask de ned fa set actions allowed subtask fz clustered set states relevant subtask fo reduced set observations relevant subtask conditioned action observation abstraction excludes observations observed action 
abstraction preserves sucient resolution solve observation abstraction subsets automatically 
fs set terminal states subtask fs de nition goal states required help specify pseudo reward function goal states need observed planning execution furthermore pomdp domains perfectly acceptable de ne 
pseudo reward function specifying relative desirability terminal state 
typically subtasks uniform reward constant pseudo reward set zero goal satisfying terminal states negative constant non goal terminal states 
subtasks non uniform reward pseudo reward set real reward 

say speci ed general case fully observed 
distinction useful cases terminal states speci ed process determining pseudo reward function 
hierarchical mdps require model domain case pomdp model fs rg 
planning algorithm pomdp formulation polca planning algorithm remains largely unchanged mdp version table 
notable di erences 
parameter setting step extended include observation probabilities 
state abstraction algorithm complicated slightly need consider observation probabilities clustering states 
introduce automatic observation abstraction 
actual subtask solving uses appropriate pomdp techniques 
table presents hierarchical pomdp planning algorithm 
plan polca step re formulate structured state space 
subtask bottom ordering step set parameters 


step minimize states step minimize observations step solve subtask table main polca planning function 
function called pomdp model fs rg rst argument hierarchical constraints fh hm second argument 
step re formulate structured state space rst step identical mdp pomdp formulations section simply stated new state space 
equal cross product original state space fs hierarchy state fh hm step set parameters step translates pomdp parameters speci ed fs rg structured state space 
speci cation reward transition parameters identical mdp case section add speci cation observation parameters 
pomdp fs rg set parameters subtask equations case primitive actions 




case actions 

hp 
hp 
hp 
explained section action available subtask subsumes subtask hp policy state pomdp formulation polca special case mdp version parameter assignment case constitutes approximation 
step minimize states state clustering procedure pomdps extends mdp model minimization dean givan consider observation probabilities checking stability clusters 
mdps section automatic state abstraction algorithm starts selecting set initial clusters reward parameters 
cluster partition gradually re ned di erences transition observation parameters 
infer function mapping states fh 

expanding set clusters fc initialize state clustering see equation 
ii check stability cluster cluster deemed stable 





iii cluster unstable split see equation step minimize observations step automatically determines observation abstraction de nes subset relevant observations action subtask observation abstraction accelerate solving ect pomdp solving tremendous complexity step planning exponential number observations equation 
automatic observation abstraction done subtask subtask basis subtask action action basis 

subtask action observations observed need considered 
ect exclude observations zero probability state clusters 
step solve subtask step focuses learning pomdp value function policy subtask case pomdps mdps solving delayed compact state observation sets 
state observation abstraction functions rst re de ne pomdp parameters terms clusters 



planning clusters states observations realized exact pomdp solver 
typically apply incremental pruning algorithm described cassandra littman zhang code obtained cassandra 
large real world domains augmented mdp algorithm described roy thrun 
pomdp policy execution task hierarchies signi cant change hierarchical pomdp execution compared mdp case fact pomdps require belief updating time step prior consulting policy 
subtask uses di erent state clustering follows local policy expressed local belief 
de nition subtask say belief de ned clusters local belief 
update local belief subtask separately latest pair update global belief equation 
policy lookup traverses tree local belief subtask extracted global belief resulting simple projection subtask state clustering function 
table describes complete hierarchical pomdp execution algorithm 
completes exposition general polca algorithm 
theoretical implications mdps guarantee nding recursively optimal solution unable theoretical claims regarding quality hierarchical pomdp solution 
fact easily demonstrate nal solution generally sub optimal simply considering equations 
parameterization actions constitutes approximation simple reason subtask policy considered execute polca primitive action return action child subtask spanned execute child table polca execution function 
function initially called root subtask rst argument current global belief second argument 
corners belief state belief restricted single state ignoring policy action may called high uncertainty beliefs 
approximation necessary subtask modeled pomdp de nition parameters assumed linear belief 
despite approximation empirical results section demonstrate usefulness approach wide range pomdp problems 
embedded hierarchical pomdp planning algorithm important contributions area model minimization 
pomdp extension mdp state minimization algorithm dean givan 
second proposed separate algorithm perform observation abstraction 
include theorems stating algorithmic contributions sound respect pomdp solving independent hierarchical context 
proofs theorems included appendix theorem optimality state abstraction pomdps 
fs rg pomdp 
state minimization algorithm section preserves sucient information learn optimal policy theorem optimality observation abstraction pomdps 
fs rg pomdp 
observation minimization algorithm section preserves sucient information learn optimal policy 
experimental evaluation hierarchical mdp approach hierarchical pomdp algorithms compare polca exists number established hierarchical mdp approaches 
reason experimental evaluation comparing mdp version polca known hierarchical mdp algorithms dietterich maxq dietterich andre russell 
assume action subtask decomposition leverage state abstraction accelerate mdp problem solving 
state abstraction derived hand maxq clustering automatic polca 
important di erences type abstraction exploited maxq compared polca 
rst recall maxq decomposed representation value function 



expected reward action expected reward completing subtask having taken expected reward nishing entire task having completed part decomposition rst introduced maxq 
full part decomposition de ned andre russell 
advantage decomposing value function way resides ability perform state abstraction separately component problems yields greater abstraction compared clustering monolithic function 
speci cally typically allows signi cant funnel abstraction 
funnel abstraction achieved subtask terminal states example moving robot doorway decoupling costs terminal states 
polca state abstraction non decomposed di erent advantage comes state abstraction 
speci cally abstraction higher level subtasks takes place lower level subtasks solved abstraction high level subtasks needs consistent policy lowerlevel subtask possible policies current subtask 
contrast state abstraction maxq consistent policy lower level subtask 
addition state abstraction automatic polca take advantage symmetry domain re ections rotations see balch reduce required parameter set 
purpose rst experiment investigate ects di erences 
experiment speci cally looks amount state abstraction allowed method 
worth emphasizing quantity state abstraction interest direct indicator policy computation times 
hierarchical mdp results taxi domain taxi domain known hierarchical mdp problem dietterich 
task control taxi agent goal picking passenger initial location dropping desired destination 
initial state selected randomly fully observable transitions deterministic 
represents maxq control hierarchy domain 
structured state space domain called taxi described features fx passenger destination hg fh root get ut nav nav nav nav addition consider second domain taxi passenger start location grid 
obtain concise representation qr stored primitive actions case actions recursively calculated online necessary see dietterich details 

practice automatically nding zc non trivial 
main diculty estimating 

qr 
action 
get root nav put north south east west putdown pickup taxi domain represented features fx passenger 
represent grid world passenger fy destination fy gg 
taxi agent select actions fn pickup 
actions uniform reward 
reward pickup action agent passenger location 
reward putdown action agent destination passenger taxi 
compared fy gg taxi 
task harder maxq need represent completion costs nav 
structural assumption require respectively values states primitive actions values states primitive actions represent solution taxi taxi tasks 
figures compare state abstraction results task terms number parameters necessary learn solution 
cases approaches maxq polca yield considerable savings compared full set parameters required optimal mdp solution 
tasks requires parameters maxq polca large part abstracts away full state features destination irrelevant get closer look di erences maxq polca see taxi task number parameters required comparable values maxq polca 
requires additional parameters represents external completion costs polca gets abstraction low level subtasks nav automatically exploits symmetry domain approaches fail 
case taxi task results illustrate advantage polca problems subtasks multiple termination conditions 
domain maxq require parameters capture completion costs subtasks nav get subtask terminate large number states passenger cells 
polca hand uses symmetry domain constrained subtask ordering achieve signi cantly state abstraction 
point abstraction results learning maxq taxi published results dietterich andre russell taxi results hand crafted careful reading algorithm 
algorithms learn hierarchically optimal policy tasks 

experimental evaluation hierarchical pomdp approach section move pomdp domains experimental results comparing polca algorithm established pomdp solving algorithms exact approximate contrasting pomdp problems 
rst domain solved exactly little structure speak 
second domain partially observable variant taxi domain section 
third domain pomdp requires policy multi step information gathering actions unsolvable approximate pomdp algorithms 
exists large variety pomdp solution algorithms exact approximate 
target problems compare performance polca exact pomdp solver approximate algorithms 
algorithms described hauskrecht ers thorough comparative analysis approximate value pomdp approaches 
approaches consider include 
pomdp solution obtained incremental pruning algorithm cassandra cassandra 
exact pomdp solution exponential time complexity 

mdp lookahead solution obtained solving problem fully observable mdp value iteration algorithm disregarding state uncertainty 
control policy obtained combining mdp value function single step exact pomdp solution 
result policy assumes full observability step current 
solution polynomial size state action sets 

qmdp solution obtained solving problem fully observable keeping track values values 
pomdp policy extracted linearly extrapolating values cover entire belief space action maximum expected value 
result policy assumes uncertainty immediate time step full observability 
solution polynomial size state action sets 

fib fast informed bound solution obtained solving problem qmdp solution di erence values weighted observation probabilities 
pomdp policy extracted linearizing modi ed values belief state action maximum expected value 
algorithm polynomial size state action observation sets 

qmdp lookahead solution obtained rst optimizing qmdp value function qmdp solution set seed single step exact pomdp solving 
maxq polca solution type nav get put root number parameters required nd solution taxi task 
hierarchical semi markov learning dietterich maxq dietterich andre russell polca algorithm clustering values see section 
maxq polca solution type nav get put root number parameters required nd solution taxi task 
solution exponential time complexity due step full pomdp solution 

fib lookahead solution obtained rst optimizing fib value function fib solution set seed single step exact pomdp solving 
solution exponential time complexity due step full pomdp solution 

umdp planner assumes completely unobservable version pomdp problem applies full pomdp solution worst case problem characterization 
advantage size pomdp solution number vectors grows multiplicative factor jaj iteration 
consequently complexity exponential horizon length exact pomdp solving 
nal policy extracted set vectors pomdp 

polca planner uses algorithm introduced speci cally table 
subtask solved exact pomdp algorithm complexity exponential size largest subtask typically substantially exact planning due reduced action state observation sets 
set equations directly adapted hauskrecht illustrates value iteration update formula various approaches 
value function expression mathematically bounded expression listed list 
mdp max mdp qmdp max qmdp ib max ib pomdp max pomdp umdp max max simulation domain part painting problem rst task considered part painting problem described kushmerick hanks weld 
selected suciently small solved exactly 
contains little structure valuable sanity test structured algorithm polca 
task consists processing part may may awed 
part awed rejected alternately part awed painted shipped 
pomdp state described boolean assignment state features awed painted 
assignments included state set includes possible states fun awed un awed painted awed painted awed 
addition domain contains possible actions paint ship observations 
shipping un awed painted part yields reward shipping yields reward 
similarly rejecting awed piece yields reward rejecting yields reward 
inspecting part yields noisy observation 
painting part generally expected ect painted paint painted painted paint painted case part generally hides paint paint shows action hierarchy task 
possible hierarchies intuitively hierarchy minimum knowledge problem 
inspect reject paint ship action hierarchy part painting task 
table contains results experiment set di erent pomdp planners 
small domain evaluating performance policy multiple simulation trials look directly policy yielded di erent planning method 
observe di erent algorithms learned policies indicated policy column 
illustrates policies 
policy clearly poor rejecting part achieves goal time 
hand optimal policy near optimal policy achieve goal time failing action inspect returns incorrect observation 
fact nearly identical discount factor reward paint action zero 
optimal policy yields higher reward virtue faster reset rate 
ect approximation introduced modelling action seen policy problem solution cpu time policy jsj jaj secs pomdp mdp lookahead qmdp fib qmdp lookahead fib lookahead umdp polca table performance results part painting task inspect reject paint paint ship reject inspect reject paint ship policies part painting task 
nodes show actions arrows indicate observations appropriate dotted lines indicate task reset occurs part rejected shipped 
planning time approximate algorithms extremely short mdp lookahead qmdp fib qmdp lookahead fib lookahead umdp compared exact solution 
planning polca resulted signi cant time savings exact solution vs 
conclude small problems little structure speak polca able nd reasonable policies 
observe qmdp approximate algorithm able nd optimal policy fraction time required polca 
attribute optimal performance qmdp fact problem contains single information gathering action 
qmdp algorithm able select information gathering action represents lowest risk option high uncertainty beliefs 
simulation domain cheese taxi problem section addresses robot navigation task cross dietterich taxi problem added state uncertainty mccallum cheese maze 
domain interesting combines cheese maze state uncertainty taxi task hierarchical structure 
assume taxi agent operating world con guration cheese maze agent pickup passenger located state proceed deliver passenger desired destination 
state space represented discrete states formed cross product state variables taxi locations fs 
destinations fs 
agent access actions south east west query pickup perceive distinct observations fo destinations destinations 
passenger destination 
destination 
state space cheese taxi task 
rst observations received motion action applied partially disambiguating taxi current location 
de ned mccallum observation localization signature indicating wall placement directions immediately adjacent location 
convention states fs look identical respectively fs fs nally states unique identi er 
observations destinations provided noise response query action fully disambiguating taxi destination state variable passenger onboard 
null observation received pickup putdown actions 
state transition model deterministic example motion actions expected transition ects north actions 
addition agent incurs reward action nal reward delivering passenger correct location 
reward incurred applying pickup putdown actions incorrectly 
sources uncertainty problem 
mccallum original cheese maze task initial location taxi randomly distributed maze cells fs 
disambiguated sequence motion actions 
secondly passenger destination randomly selected passenger picked observed query action 
thirdly taxi passenger onboard cells small possibility passenger change mind suddenly select destination 
new destination observed query action 
transition reward parameters consistent original taxi task observations parameters exception query action borrowed directly original cheese maze 
adopt taxi task usual hierarchical action decomposition shown 
problem previously considered part painting problem requires pseudo reward function subtasks uniform reward nav uniform reward function nav 
arti cially reward achievement partial goals nav subtask pseudo reward function nav nav similarly 
identical pseudo reward function dietterich 
results cheese taxi domain pomdp solving algorithms 
illustrates sum rewards accomplish full task averaged trials illustrates computation time necessary reach solution 
gures include results di erent hierarchical pomdp solutions polca polca 
polca full algorithm described section 
polca uses algorithm state observation abstraction leads longer solution time 
decomposition 
umdp solution stands apart extremely poor performance policy goal dropping passenger occasionally reached starting positions forcefully terminated trial lasting time steps 
furthermore policy uses query action attempts putdown passenger wrong destination half trials 
policies agent starts sequence motion actions serves progress passenger location disambiguating location 
reaching full certainty agent selects pickup action 
agent takes sequence motion actions interspersed query action leads correct passenger destination 
main di erence top policies polca polca qmdp lookahead fib lookahead best ones pomdp mdp qmdp fib policy rst sequence motion action going random start location passenger station 
better policies exhibit optimized action sequences minimum number moves required simultaneously disambiguate position pomdp qmdp fib umdp polca polca solution type reward pro le solving cheese taxi task 
pomdp qmdp fib umdp polca polca solution type computation time computation time solving cheese taxi task 
umdp pomdp algorithms terminated hours computation converged solution 
root put subtasks polca solutions polca polca terminated convergence 
cases intermediate solution completed iteration evaluate algorithm generate results 
reach 
approximate algorithms mdp lookahead qmdp fib unable correctly trade progress goal versus localization information consequently require extra steps reach 
exact pomdp algorithm theoretically able nd shortest action sequence require longer computation 
terminated hrs computation having completed iterations exact pomdp solving 
domain computation time polca abstraction signi cantly longer approximate algorithms umdp bene ts terms policy performance approximations qmdp lookahead fib lookahead marginal 
conclude polca algorithm able perform par approximate algorithms domain longer computation time required necessarily appropriate choice domain 
part painting problem qmdp lookahead fib lookahead perform quite large part problem requires single information gathering action 
consider domain level approximation insucient policy achieved multi step information gathering actions 
simulation domain questions game new pomdp domain interactive game called questions know animal vegetable mineral 
player game rst player selects speci object mind second player guess object second player allowed ask series nog questions person answer truthfully animal 
green 
turtle 
second player wins round correctly identi es object questions name game 
modeling game pomdp goal learn pomdp policy correctly guesses object selected user 
represent possible object state 
action space involves types actions guesses questions 
guess object state space turtle 
list questions sucient disambiguate state objects green 
fruit 
mineral 
observation space contains items corresponding possible verbal responses non pomdp player having picked object 
pomdp domain easily scaled adding objects new object automatically adds state action information eliciting questions added necessary 
example prototypical information contingent pomdp characterized large action space relative state space includes variety information gathering actions 
respect model parameterization natural rules game state transitions self transitions 
add small probability randomly transitioning current state object allowing rst player change mind target object 
traditionally part game adding stochasticity state transitions interesting pomdp 
assume question state stays probability randomly changes states cumulative probability 
reward consistently question actions guess actions yield reward guess correct reward 
task reset time policy selects guess action 
observation probabilities question action re ects state example turtle green turtle green noise turtle green implemented object version domain 
pomdp representation contains states object actions guesses questions observations noise 
considered alternate hierarchical decompositions domain 
illustrates rst decomposition referred 
case domain decomposed subtasks action redundancy subtasks 
preliminary experiments decomposition quickly showed computation necessary apply hierarchical planning spent solving subtask vegetable proposed second decomposition referred illustrated 
decomposition partitions action space vegetable subtask produce new lower level subtasks real vegetable fruit applied hierarchical planning algorithm twice decomposition generated policies full set algorithms 
domain performance policy evaluated simulation independent trials 
trials failing guess time steps terminated 
shows sum rewards run averaged trials plotted function number value iteration updates completed case hierarchical planners full number iterations completed subtask 
results clearly illustrate failures qmdp fib algorithms faced information contingent pomdp domain 
looking closely policies generated qmdp fib algorithms noted unable di erentiate various question actions randomly select questions belief suciently certain guess 
certainty threshold slightly lower fib algorithm explaining slightly performance 
qmdp algorithm hand able take correct guess trial spent time steps asking random questions useful ect 
expected performance exact pomdp solver terms accumulated reward exceeds approximate methods 
hierarchical approach converge approximately iterations converge slightly sub optimal policies 
furthermore note additional structural assumptions cause greater loss performance compared 
presents results plotting reward performance function computation time 
graph clearly shows computational savings note log time axis obtained hierarchical structural assumptions 
comparing see trade resulting di erent 
convention game plant related objects identi ed vegetables 
mineral animal vegetable action hierarchy questions domain 
mineral animal fruit vegetable real vegetable action hierarchy questions domain 
iterations pomdp polca polca fib qmdp simulation results questions domain 
qmdp fib results plotted constants representing optimized performance 
mdp lookahead qmdp lookahead fib lookahead illustrated gure generated results equivalent qmdp performance 
time secs pomdp polca polca fib qmdp simulation results function computation time questions domain 
pomdp computations including hierarchical subtasks assumed pruning criterion 
structural assumptions 
conclude polca hierarchical decomposition preserves suf cient richness representation successfully address information contingent pomdps 
furthermore design hierarchy ectively control trade performance computation 
possible approaches solve problem investigated include odd pomdp bayer dietterich decision trees quinlan 
stochasticity state transitions decision trees poor choice speci formulation questions domain 
particularly interested domain shares commonalities pomdp dialogue modeling roy pineau thrun 
modeling robot interaction manager pomdp section inclusion actions crucial policy human robot interactions typically ambiguities errors noise 
game essence provides stylized naturally scalable version interaction task allowing perform comparative analyses simulation moving real world implementation 
extremely valuable tool diculty staging real human robot dialogue experiments 
reasons believe domain information contingent pomdps maze navigation task goal driven pomdps particular robot navigation domain 

real world application domain section follow simulation experiments real world implementation polca approach context robot behavioral manager 
high level robot control popular topic ai decades research led reputable collection architectures arkin brooks gat 
existing architectures rarely take uncertainty account planning 
propose robot control architecture pomdp incorporates high level uncertainty obtained navigation sensors laser range nder interaction sensors speech recognition touchscreen performs high level control 
pomdp able arbitrate information gathering performance related actions negotiate goals di erent specialized modules 
unfortunately pomdps size necessary robot control order magnitude larger exact pomdp algorithms tackle 
domain ers signi cant structure form multiple alternate goals decomposable goals making candidate polca 
domain pomdp policy control high level behavior robot context interactive task 
conducted part larger project dedicated development prototype nursing assistant robot montemerlo pollack pineau thrun 
goal project develop personalized robotic technology play active role providing improved care services non institutionalized elderly people 
target user elderly individual mild cognitive physical impairment 
robot pearl shown right primary test bed pomdp behavior management system 
wheeled robot board speaker system microphone speech input output 
uses sphinx ii speech recognition system ravishankar festival speech synthesis system black 
board pcs connected internet wireless ethernet 
pearl robotic nursing assistant interacting elderly people nursing facility 
domain polca controls high level decision making large set robot activities verbal non verbal 
typical activities include sending robot preselected locations accompanying person locations engaging person conversation ering general information speci cognitive reminders 
task involves integration multiple robot sensors pomdp belief state 
current sensors include laser readings speech recognition touch screen input 
exhibit signi cant uncertainty attributed large part poor speech recognition noisy navigation sensors erroneous human input 
table shows sample dialogue robot test subject pomdp policy 
uncertainty management pomdps robot chooses ask clari cation question occasions 
number questions depends clarity person speech detected sphinx speech recognition system 
mdp policy exhibit similar adaptability poor recognition performance 
pomdp modeling formally test performance polca algorithm domain developed experimental scenario tested ectiveness pomdp interaction manager onboard robot context guidance task 
scenario required pearl interact elderly residents primary goal reminding scheduled appointment secondary goal providing interesting information 
course scenario pearl navigate observation true state action reward pearl hello request begun say hello pearl start ask repeat pearl time want time say time pearl abc want tv ask station pearl abc want abc say abc pearl nbc want nbc con rm channel nbc pearl want nbc say nbc pearl go pretty send robot ask robot pearl hello send robot bedroom con rm robot place pearl bedroom send robot bedroom go bedroom pearl go hello send robot ask robot pearl kitchen hello send robot kitchen go kitchen table sample dialogue test subject 
actions bold font clari cation actions generated pomdp high uncertainty speech signal 
resident room establish contact possibly accompany person center eventually return recharging station 
task typically required robot answer simple information requests test subject example providing time weather forecast 
pearl high level behavior including speech motion commands completely governed pomdp interaction manager 
scenario robot interface domain modeled states described collection multi valued state features 
states directly observable robot interface manager robot able perceive distinct observations 
state observation features listed table 
observations perceived di erent modalities cases listed observations constitute summary complex sensor information 
example case laser range nder raw laser data processed correlated map determine robot reached known landmark laser 
similarly case user emitted speech signal keyword lter applied output speech recognizer give weather forecast tomorrow 
weather 
general speech recognition touchscreen input redundant sensors generating information assumed greater degree reliability coming touchscreen 
reminder observations received high level intelligent scheduling module 
software component developed mccarthy pollack context pearl project reasons temporally user activity goal issuing appropriately timed cognitive reminders warn person upcoming scheduled events need take medication doctor appointment social activities 
response observations robot select distinct actions falling broad categories communicate con ver state features feature values home room room checklist observation features feature values speech time weather go unknown touchscreen time weather go laser reminder checklist table component description human robot interaction scenario con con con con move discrete action enumerated invoked de ned sequence operations part robot requires robot rst look forecast wireless ethernet emit tomorrow weather sunny high 
actions communicate category involved combination redundant speech synthesis touchscreen display selected information question test subject modalities simultaneously 
sensory limitations common target population redundant audio visual communication important input output robot 
actions move category translated sequence motor commands motion planner uses dynamic programming plan path robot current position destination roy thrun 
pomdp model parameters selected designer 
reward structure hand crafted re ects relative costs applying actions terms robot resources robot motions actions typically spoken veri cation questions re ecting appropriateness action respect state 
example positive rewards correctly satisfying goal large negative rewards applying action unnecessarily small negative reward veri cation questions state condition scenario implemented tested single policy generated polca approach shows action hierarchy domain 
dif culties involved carrying experiments elderly subjects prohibitively dicult stage perform full scale comparative evaluation alternate pomdp solutions 
rest contact move inform assist remind action hierarchy robot experiment 
experimental results performed days formal experiments robot autonomously led full involving di erent elderly people 
shows example guidance experiment involving elderly person uses walking aid 
sequence images illustrates major stages successful delivery contacting person explaining reason visit walking facility providing information successful delivery case weather 
policy generated polca successfully controlled robot human intervention guidance experiments 
subjects completed scenario diculty pleased experience 
experiment speech recognition performance particularly poor due signi cant amount 
video footage see www cs cmu edu thrun movies pearl assist mpg pearl approaching elderly reminding appointment guidance corridor entering dept 
asking weather forecast pearl leaves example successful guidance experiment 
pearl picks patient outside room reminds appointment walks person department responds request weather report 
interaction communication took place speech touch sensitive display 
ambient noise redundancy ered touch screen allowed users communicate dialogue manager diculty 
addition early experiments robot lacked ability adapt speed person guidance center causing run away slow moving test subjects 
corrected addition second laser back robot allowing adapt speed appropriately 
experiment constitutes encouraging evidence appropriate approximations pomdp control feasible useful real world robotic applications 

concluding discussion novel approach solve complex pomdp problems 
polca appears especially suited information contingent problems ability select alternate information gathering actions paramount successful planning existing algorithms fail 
approach successfully robot system control high level robot behavior context interactive service task 
furthermore special case mdp planning approach appears compare favorably existing mdp approaches problems automatically discovered state abstraction possible previous algorithms 
conclude discussing manners extended relations hierarchical planning 
hierarchical planning algorithm requires having nontrivial local reward functions subtask cases requires pseudo reward function 
possible loosely divide hierarchical pomdps types problems single goal multi goal domains 
division types interesting ers insight problems may require pseudo reward functions 
rst type problems characterized having single large goal achieved sequentially completing relevant subgoals 
subgoal generally associated subtask 
taxi task section example problem 
case nal goal completion rewarded subtasks need provided hand crafted pseudo reward functions 
possible reward shaping ng harada russell er insight automatically de ning appropriate pseudo reward functions 
contrast second type problems includes di erent alternative goals satis ed uni ed framework 
information contingent pomdp questions domain section instance problem 
type problems hierarchy typically partition goals subtasks local reward assumption met diculty need pseudo rewards 
second automated state clustering algorithm described section tends useful mdps applied requiring full enumeration state space 
necessary complexity clustering algorithm equivalent planning algorithm impractical large problems hierarchical planning needed 
general possible obtain stable clustering solution fully enumerating state space 
case pomdps exponential complexity computing solution equation means clustering algorithm polynomial size domain means prohibitive compared planning costs 
feasible compute stable clustering states 
coarser approximate clustering may preferable reduces size problem planning time 
third polca uses monolithic function decomposed function equation unable achieve full funnel abstraction 
relevant mdp version polca costs ectively decoupled subtask terminal states 
believe versions polca decomposed function greater savings 
fourth results brie fact subtasks solved completion 
particular results assume subtasks root put terminated iterations respectively 
opens question lower level subtasks solved fully convergence higher level subtasks solved 
large domains may better interleave planning subtasks di erent levels ensure time solution 
require re parameterization actions equations time lower level policy changed 
practice overhead small compared actual pomdp solving applied subtask probably advantageous start computation high level tasks early policies low level tasks extremely poor change frequently 
polca combines action decomposition automated state observation abstraction er highly structured approach pomdp planning 
general prevalence abstraction direct result imposing hierarchy 
predict better understanding interaction action hierarchies state observation abstraction may lead better ways exploiting structure problem solving inspire new methods automatically discovering action hierarchies 
acknowledgments authors acknowledge invaluable contributions researchers david andre tom dietterich geo gordon michael littman nicholas roy 
andre russell 

programmable reinforcement learning agents 
lean dietterich tresp 
eds advances neural information processing systems nips vol 
pp 

mit press 
andre russell 

state abstraction programmable reinforcement learning agents 
proceedings eighteenth national conference arti cial intelligence aaai pp 

arkin 

behavior robotics 
mit press 
asoh matsui 

socially embedded learning oce conversant robot 
proceedings th international joint conference arti cial intelligence ijcai pp 

bayer dietterich 

pomdp approximation algorithm anticipates need observe 
springer verlag ed proceedings paci rim conference arti cial intelligence pricai lecture notes computer science pp 
new york 
black 

festival speech synthesis system 
edition 
bonet ge ner 

planning heuristic search 
arti cial intelligence 
borenstein everett feng 

navigating mobile robots systems techniques 
peters boutilier brafman geib 

prioritized goal decomposition markov decision processes synthesis classical decision theoretic planning 
proceedings th international joint conference arti cial intelligence ijcai pp 

boutilier dearden goldszmidt 

stochastic dynamic programming factored representations 
arti cial intelligence 
boyen koller 

tractable inference complex stochastic processes 
proceedings fourteenth conference uncertainty arti cial intelligence uai pp 

brafman 

heuristic variable grid solution method pomdps 
proceedings fourteenth national conference arti cial intelligence aaai pp 

brooks 

robust layered control system mobile robot 
tech 
rep tr ai memo mit 
burgard cremers fox lakemeyer schulz steiner thrun 

experiences interactive museum tour guide robot 
arti cial intelligence 


questions neural net internet 
www net index html 
cassandra 

tony pomdp solve page 
www cs brown edu research ai pomdp code index html 
cassandra littman zhang 

incremental pruning simple fast exact method partially observable markov decision processes 
proceedings thirteenth conference uncertainty arti cial intelligence uai pp 

cox wilfong 
eds 

autonomous robot vehicles 
springer verlag 
dayan hinton 

feudal reinforcement learning 
advances neural information processing systems nips vol 
pp 
san francisco ca 
morgan kaufmann 
dean givan 

model minimization markov decision processes 
proceedings fourteenth national conference arti cial intelligence aaai pp 

dean givan leach 

model reduction techniques computing approximately optimal solutions markov decision processes 
proceedings thirteenth conference uncertainty arti cial intelligence uai pp 

dean lin 

decomposition techniques planning stochastic domains 
proceedings th international joint conference arti cial intelligence ijcai pp 

dietterich 

hierarchical reinforcement learning maxq value function decomposition 
journal arti cial intelligence research 
dietterich 

overview maxq hierarchical reinforcement learning 
walsh 
eds proceedings symposium abstraction reformulation approximation sara lecture notes arti cial intelligence pp 
new york 
springer verlag 
fikes nilsson 

strips new approach application theorem proving problem solving 
arti cial intelligence 
gat 

esl language supporting robust plan execution embedded autonomous agents 
aaai fall symposium issues plan execution 
hauskrecht 

value function approximations partially observable markov decision processes 
journal arti cial intelligence research 


learning hierarchical decomposition factored mdps 
machine learning proceedings international conference icml 
hernandez mahadevan 

hierarchical memory reinforcement learning 
advances neural information processing systems nips vol 
pp 



stochastic processes filtering theory 
academic new york 
kaelbling 

hierarchical reinforcement learning preliminary results 
machine learning proceedings international conference icml pp 

kaelbling littman cassandra 

planning acting partially observable stochastic domains 
arti cial intelligence 
kalman 

new approach linear ltering prediction problems 
transactions asme journal basic engineering 
kushmerick hanks weld 

algorithm probabilistic planning 
arti cial intelligence 
littman cassandra kaelbling 

learning policies partially environments scaling 
proceedings twelfth international conference machine learning pp 

lovejoy 

survey algorithmic methods partially observable markov decision processes 
annals operations research 
mccallum 

overcoming incomplete perception utile distinction memory 
machine learning proceedings international conference icml pp 

mccarthy pollack 

plan personalized cognitive 
proceedings th international conference ai planning scheduling aips 
mcgovern barto 

automatic discovery subgoals reinforcement learning diverse density 
machine learning proceedings international conference icml pp 

meuleau hauskrecht kim peshkin kaelbling dean boutilier 

solving large weakly coupled markov decision processes 
proceedings fifteenth national conference arti cial intelligence aaai pp 

monahan 

survey partially observable markov decision processes theory models algorithms 
management science 
montemerlo pineau roy thrun verma 

mobile robotic guide elderly 
proceedings eighteenth national conference arti cial intelligence aaai pp 

moore atkeson 

parti game algorithm variable resolution reinforcement learning multidimensional state spaces 
machine learning 
ng harada russell 

policy invariance reward transformations theory application reward shaping 
machine learning proceedings international conference icml pp 

parr russell 

approximating optimal policies partially observable stochastic domains 
proceedings th international joint conference arti cial intelligence ijcai pp 
montreal quebec 
morgan kau mann 
parr russell 

reinforcement learning hierarchies machines 
advances neural information processing systems nips vol 
pp 

pearl 

probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
pickett barto 

algorithm creating useful reinforcement learning 
machine learning proceedings international conference icml 
pineau thrun 

high robot behavior control pomdps 
workshop cognitive robotics national conference arti cial intelligence aaai pp 

pollack engberg matthews thrun brown ramakrishnan dunbar jacob mccarthy montemerlo pineau roy 

pearl mobile robotic assistant elderly 
workshop automation caregiver role intelligent technology elder care national conference arti cial intelligence aaai pp 

poupart boutilier 

value directed belief state approximation pomdps 
proceedings sixteenth conference uncertainty arti cial intelligence uai pp 

quinlan 

induction decision trees 
machine learning 
rabiner 

tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
ravishankar 

ecient algorithms speech recognition 
ph thesis school computer science carnegie mellon university 
roy pineau thrun 

spoken dialog management robots 
proceedings th annual meeting association computational linguistics acl 
roy thrun 

coastal navigation mobile robots 
advances neural information processing systems nips vol 
pp 

roy thrun 

robot navigation policy search 
proceedings ieee rsj international conference intelligent robots systems iros 
russell norvig 

arti cial intelligence modern approach 
prentice hall 
ryan 

models automatically generate reinforcement learning hierarchies 
machine learning proceedings international conference icml 
singh 

transfer learning composing solutions elemental sequential tasks 
machine learning 
singh cohn 

dynamically merge markov decision processes 
advances neural information processing systems nips vol 
pp 

sondik 

optimal control partially observable markov processes 
ph thesis stanford university 
sutton 

learning predict methods temporal di erences 
machine learning 
sutton precup singh 

mdps semi mdps framework temporal abstraction reinforcement learning 
arti cial intelligence 
tesauro 

temporal di erence learning td gammon 
communications acm 
theocharous mahadevan 

learning hierarchical partially observable markov decision process models robot navigation 
proceedings ieee international conference robotics automation icra pp 

thrun 

monte carlo pomdps 
advances neural information processing systems nips vol 
pp 

thrun schwartz 

finding structure reinforcement learning 
advances neural information processing systems nips vol 
pp 

torrance 

natural communication robots 
master thesis department electrical computer science mit 
wang mahadevan 

hierarchical optimization policy coupled decision processes 
machine learning proceedings international conference icml pp 

white 

survey solution techniques partially observed markov decision process 
annals operations research 
wiering schmidhuber 

hq learning 
adaptive behavior 
balch 

symmetry markov decision processes implications single agent multiagent learning 
machine learning proceedings international conference icml pp 

appendix convergence pomdp state observation abstraction appendix contains proofs theorems section 
theorem optimality state abstraction pomdps 
fs rg pomdp 
state minimization algorithm section preserves sucient information learn optimal policy proof consider states matching cluster assignment obtained pomdp state clustering algorithm section 
proof induction show beliefs fb fb di er probability states identical value 
consider value time max fs max fs assuming equation substitute equation max fs substitute equation max fs leading assume values time equal show values time equal max max equation substitute equation max pomdp stability criterion equation conjunction equation belief update equation equation infer conditioned observation max leading 
theorem optimality observation abstraction pomdps 
fs rg pomdp 
observation minimization algorithm section preserves sucient information learn optimal policy proof consider observation satis es equation excluded set consider second set og 
consider pomdp subtasks identical way rst uses second uses show solving generates solution 
solution obtained solution obtained proof induction rst consider max max conclude assume consider exact pomdp value update equation section max max substituting equation re arranging get max 
interest clarity assume state abstraction sh extending proof case state abstraction trivial 
expanding term max eliminate term observation abstraction condition equation max conclude loss performance results eliminating observation satis es equation 

