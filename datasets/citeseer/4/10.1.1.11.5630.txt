line evaluation framework recommender systems hayes massa cunningham trinity college dublin ireland cunningham cs tcd itc irst sommarive loc 
pant povo trento italy massa irst itc 
techniques currently evaluate recommender systems 
techniques involve line analysis evaluation methods machine learning information retrieval 
argue line analysis useful user satisfaction recommendation strategy measured line context 
propose new evaluation framework involves paired test recommender systems simultaneously compete give best recommendations user time 
user interface interaction model system 
framework enables specify api different recommendation strategies may take part competition 
api defines issues access data interaction model means gathering positive feedback user 
way possible obtain relative measure user satisfaction systems 
acting recommendations people normal part life 
eat restaurant advice friend see movie having read review newspaper choice 
case decision act recommendation essentially premises trust recommender second assume recommender sufficient knowledge tastes tastes people third assume recommender knowledge alternatives available 
recommendations take shortcut things having try things dislike having acquire knowledge informed decision 
unsurprisingly systems automate facility popular internet 
irrespective techniques success automated recommender reliant trust user having sufficient knowledge user requirements having knowledge range items available 
increasingly demand objective evaluation criteria types systems 
stems difficulty evaluating recommender better judging criteria making evaluation 
aspects recommender system analyse instance ease certainly important factor success 
hci factors presentation interaction model employed 
common evaluation approaches performed line techniques machine learning information retrieval cross validation measures recall precision 
evaluative framework recommender systems idea system utility comparative measure recommendation strategy performs 
draw attention fact evaluation measure real people willing act advice system 
necessary evaluate recommendation strategies part live fully realised applications 
sections introduce related evaluation methods machine learning information retrieval community propose techniques may provide useful insights line evaluation methodology truly gauge user satisfaction recommendation strategy 
section introduces new evaluation methodology compare utility different recommendation strategies running fully realised application 
line analysis methodology plays recommendation strategy line setting measures relative degree success strategy user utilises recommendations system 
framework doesn measure absolute user satisfaction relative user satisfaction system 
examine maintain user trust evaluation period offer recommendation engine equal opportunity success 
section discuss advantages open issues related deployment proposed methodology 
existing approaches konstan riedl suggest existing approaches evaluating recommender systems divided categories line evaluation performance recommender mechanism evaluated existing datasets 
line evaluation performance evaluated users running recommender system 
konstan riedl argue line evaluation problematic need field fully engineered system build community users 
consequently favour line evaluation better easier 
line evaluation uses existing datasets publicly available movie dataset data gathered operation recommender prototype 
line evaluation recommendation seen information retrieval selection subset assets relevant user 
perspective metrics evaluation known measures precision recall 
precision proportion retrieved assets relevant recall proportion relevant assets retrieved 
basu hirsh evaluating recommender system defined movies rated user relevant set 
precision percentage upper quartile movies set returned 
alternative viewing recommendation information retrieval problem view classification regression problem 
situation users rated assets recommendation problem may cast prediction ratings regression problem 
alternatively may viewed classification problem classification asset liked disliked 
measure accuracy may error classification absolute root mean square rms error regression measure correlation predicted ratings actual ratings pearson correlation coefficient 
allows type evaluation common machine learning data partitioned training test data training data produce predictions test data 
estimates may improved fold cross validation leave evaluation rating predicted data rating 
interesting line approach taken swearingen sinha recommendations recommender engines compared recommendations friends :10.1.1.23.9764
section outline drawbacks line analysis 
suggest datasets evaluation ignore context recommendations 
users may give assets quite different ratings depending interests information needs time 
elaborating swearingen sinha propose methodology compare relative strengths recommendation strategies automated 
evaluating contexts recommender systems generally operate augmentative systems larger application systems 
intuitively purpose help user exploit resources available larger application domain 
employing recommender system goal translate continued user satisfaction continued system resources 
continuing resources advice recommender requires analysis conducting evaluation recommender system 
recommendation strategy maintain user satisfaction 
generally recommendation engine list resources user feedback user profile 
simplest way ordered list recommendations score calculated algorithm 
score reflect similarity resource user profile query score predicted collaborative filtering algorithm 
recommendation engine may choose strategies relevant list resources user 
smyth show recommendation set contains similar items may highly redundant small diverse set offer user choice 
swearingen sinha research suggest including trust generating items recommendation set perceived useful user :10.1.1.23.9764
area music recommendation hayes introduce notion context sensitivity boosts recommendations pertinent users current listening habits 
cases concern maintaining trust user providing items useful user context 
define context discourse informs users current behaviour system current requirements motivation previous experience preferences knowledge available 
problems existing approaches section described evaluation recommender system machine learning perspective treated recommendation classification regression task 
number problems line approach 
classification regression established machine learning techniques applied datasets available uci machine learning repository 
typically measure generalisation error indicates classification regression model performing 
evidence suggest people sensitive minor improvements generalisation error far algorithmic performance concerned 
fact standard social research methodologies include metrics test retest correlation account variance occurs people test different occasions 
hill observed reliability correlation ratings taken weeks part bellcore project 
situation complicated fact users may give explicit feedback recommendable items 
noise introduced system infer implicit score user usage system 
factor exacerbated latency time recommendation ultimately consumed 
recommendation task defined classification regression task 
mistake assume domain specific datasets eachmovie movielens projects test recommendation system 
datasets contain date time field allows data ordered 
current evaluation practices information 
classification regression purposes data point treated set un ordered feature value pairs 
unordered dataset algorithm judged better generalisation error 
may provide insight success failure recommendation strategy gives knowledge lower generalisation error translated continued system 
indication resources result recommendation 
drawbacks line evaluation quite apparent consider date time information 
consider evaluation scenario simulate usage patterns user dataset 
temporal constraints dataset predictions simulated user discrete intervals time 
time slice recommender engine may data system instant 
prediction evaluate user goes items predict 
user doesn item prediction algorithm shouldn necessarily considered false positive test data collected advice algorithm 
little information indicate prediction successful data set biased recommendation strategy 
example enforces view line analysis captures context 
context affected recommender algorithm 
suggest successfully evaluate algorithm data algorithm simulated test temporal constraints observed 
dataset tied methods domain collected inappropriate datasets test data evaluating recommendation strategy 
order accurately measure effectiveness recommender system line situation need capture large amount state information user interaction system 
section propose new evaluation framework involves line paired test recommender systems able define relative measure user satisfaction having capture implicit notion context 
methodology section outline methodology evaluating competing recommendation strategies 
evaluation environment consists real line application community users defined recommendation task specific user interface 
application serviced competing recommendation engines 
order able gauge relative measure user satisfaction strategies necessary deploy application log user interactions respect recommendation engines 
comparing usage recommendations possible say strategy performed better 
order isolate recommendation strategies keep aspects influence user satisfaction interface interaction model 
system user unaware source recommendations 
proposed methodology seen competition different approaches solving problem case winning user satisfaction winner defined user recommendations 
define framework rules competition 
framework deal issues define access resources making recommendations define winner 
architecture framework similar standard recommender system architecture 
big difference recommender engine wrapper component recommendation engines receives recommendation requests call relevant methods respective engines 
result sets policy presentation assembler component 
interface wrapper recommender engine need modify part deployed system 
presentation assembler merge policy application receive recommendation user choice recommendation engine private resources ask recommendation recommendation component recommendation engine private resources available resources feedback detector log file eval indexes fig 

impact recommender system architecture 
order manage specific competition alternative solutions elements clearly specified available resources api defining resources competing algorithms access order provide recommendations recommender methods api defining methods competing recommender implement presentation policy defines recommendations provided algorithms user evaluative feedback defines user actions considered evidence preference algorithm comparing metric defines analyse evaluative feedback order determine winner 
briefly describe detail elements described 
available resources making recommender competition important thing algorithms run conditions access basic resources 
different strategies may rely different data 
example collaborative filtering algorithms rely user rating data content approaches rely resources semantic description 
possibilities specifying available resources define api standard language declare advance source knowledge imdb www imdb com movies 
language specify api free trade different needs minimising impact deployed architecture specific language developed example java minimising constraints solution provider configuring recommendation engine stand component protocols corba soap 
recommender methods element specifies api defining methods boot recommend recommender implement interface order wrapper invoke 
boot usually parameters serves construct internal model algorithm 
inputs outputs recommend method really depend specific kind recommender system example book seller site input userid output unsorted set books 
cd burner application cocoa input partial compilation songs output set past compilations users 
personalised radio system smartradio input userid listening history past listened songs output ordered list recommended songs 
presentation policy defines way output recommend method competing strategies user 
open issue related user know combined output different recommender engines 
consider possible presentation policies merged set synthesis single result set 
achieved simply interleaving items result set 
item result set considered priority access position alternated recommender engine 
duplicated items presentation assembler 
contrasting set presentation clearly separated result sets 
case result sets kept clearly separated user knows come different algorithms 
cascading set alternate presentation result set 
request recommendations result set recommender 
request result set recommender 
course depending domain specific application particular policy may appropriate 
example merged set solution recommendation strategy emphasises diversity presenting heterogeneous selection items user 
merging results destroy effect advantage strategy hope gain 
contrasting set demands effort user needs aware receiving result sets different strategies 
cocoa itc suited domains output highly structured dependencies items 
example holiday package recommender contains accommodation itinerary information web radio recommender mixing different proposed radio programmes violate integrity 
cascading set policy presents issues user know receiving recommendations different recommender algorithms alternatively 
user may perceive differences alternate rounds recommendations aware system logic may mistrust stability system 
evaluation feedback important issue 
framework depends able derive user actions preference algorithm 
simplest situation occurs user clicks preferred recommendations possible infer recommender strategy better 
discussed section user able express opinion new items know 
partial solution provide preview items recommendations swearingen sinha appealing users recommendation systems 
ultimately user able evaluate item experienced listened song read article watched movie 
means inferring feedback 
example measuring time reading web page listening song 
adding item shopping cart certainly considered positive feedback 
recognised inferring preferences way noisier receiving explicit information user 
case extract user actions judgement relative user satisfaction respect strategies 
comparison metric defines combine user evaluative feedback order say winner 

simplest way just count number rounds won competing systems 
certain algorithms collaborative filtering may start perform sufficient data collected 
need analyse performance curve system cumulative score 
discussion clear need deploy real application order deeper evaluation proposed framework 
introduce critical factors empirical analysis clearer 
main advantage introduced pairwise comparision evaluation suffer due changes user community due changing conditions number items recommend number new items 
time aren required explicitly detect factors potentially affect notion context 
especially true temporal dimension plays key role recommendation process 
date time information recorded datasets eachmovie earlier example demonstrated straightforward information line evaluation 
question data storage lack evaluation methodology 
drawbacks framework need fully realised system users 
employ real users volunteers depends state deployment application 
running system registered users 
deploy system choice volunteer users 
approaches problems 
real system careful users significantly lowering level service enjoy 
badly performing algorithm may impact trust users system 
approach employ form reinforcement learning armed bandit technique privilege recommendations better performing algorithm 
approach assumes bandit algorithms constant performance time 
case algorithms collaborative filtering perform sufficient data collected 
bootstrap problem obviously pertinent initialising beta system volunteer users 
little preference data system recommendation algorithms perform badly result 
acknowledge may difference mentality type user 
real user may tolerant recommender error 
ultimate test real user stays service provided 
volunteer users conscious roles testers system trial period 
aspect framework neglected concerns recommendation process 
tacit assumption underlying proposal recommendation step accomplished shot operation 
process receiving recommendations may seperate requirements elicitation process 
evaluating competing strategies need ensure similar types interaction models 
shot recommenders need compared shot recommenders conversational recommenders compared conversational recommenders sharing api 
case systems shorter dialogue judged better standard line evaluation 
entirely possible strategy longer dialogue achieves greater user satisfaction line evaluation 
success may due ordering questions intuitively giving brief explanations step 
introduce possibility extending framework provide api integration third party recommender systems 
easily envisage deployment evaluation competing algorithms developed house integration testing third party software raises difficult technological issues 
directions argued generalisation error representative measure user satisfaction recommender system 
propose line methodology pairwise comparison complement line analysis 
defined evaluation framework account advantages possible drawbacks 
drawn attention relationship existing approaches order highlight motivations underlying 
step test proposed framework show example method works practice 
short term goal collect empirical evidence enables direct comparison line line evaluation 
goal concerned definition management call competition prototype recommendation systems smartradio cocoa 
objective assess sustainable proposed framework technological level 

massa 
compositional recommender systems case reasoning approach 
acm sigir workshop recommender systems new orleans 

massa 
collaborative radio community 
proceedings adaptive hypermedia malaga spain 
springer verlag 

baeza yates ribeiro neto 
modern information retrieval 
addison wesley 

basu hirsh 
learning user models recommendation 
um workshop machine learning user modelling 

breese heckerman kadie 
empirical analysis predictive algorithms collaborative filtering 
proceedings fourteenth conference uncertainty artificial intelligence madison wi july 
morgan kaufmann 

hayes cunningham 
smart radio building music radio fly 
expert systems cambridge uk 

hayes cunningham 
programme driven radio 
proceedings th european conference artificial intelligence lyons france april 
ios press 

hill stead rosenstein furnas 
recommending evaluating choices virtual community 
chi 

konstan riedl 
research resources recommender systems 
chi workshop interacting recommender systems 

smyth 
similarity vs diversity 
proceedings th international conference case reasoning vancouver canada 
springer verlag 

swearingen sinha 
algorithms hci perspective recommender systems 
acm sigir workshop recommender systems new orleans 

stephan 
framework evaluation adaptive cbr systems 
proceedings th german workshop case reasoning pages baden baden germany 
