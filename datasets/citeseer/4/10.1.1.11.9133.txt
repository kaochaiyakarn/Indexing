journal arti cial intelligence research 

submitted published policy recognition hidden markov model hung bui cs edu au venkatesh cs edu au geo west geoff cs edu au department computer science university technology po box perth wa australia method recognising agent behaviour dynamic noisy uncertain domains multiple levels abstraction 
term problem line plan recognition uncertainty view generally probabilistic inference stochastic process representing execution agent plan 
contributions twofold 
terms probabilistic inference introduce hidden markov model ahmm novel type stochastic processes provide dynamic bayesian network dbn structure analyse properties network 
describe application rao blackwellised particle filter ahmm allows construct ecient hybrid inference method model 
terms plan recognition propose novel plan recognition framework ahmm plan execution model 
rao blackwellised hybrid inference ahmm take advantage independence properties inherent model plan execution leading algorithm online probabilistic plan recognition scales number levels plan hierarchy 
illustrates stochastic models plan execution complex exhibit special structures exploited lead ecient plan recognition algorithms 
demonstrate usefulness ahmm framework behaviour recognition system complex spatial environment distributed video surveillance data 

plan recognition problem plan recognition problem inferring actor plan watching actor actions ects 
actor behaviour follows hierarchical plan structure 
plan recognition observer needs infer actor plans sub plans di erent levels abstraction plan hierarchy 
problem complicated sources uncertainty inherent actor planning process stochastic aspect plan re nement plan non deterministically re ned di erent sub plans stochastic outcomes actions action non deterministically result di erent outcomes 
furthermore observer deal third source uncertainty arising noise inaccuracy observation actor plan 
addition observer able perform plan recognition task online observations actor plan streaming 
refer general problem line plan recognition uncertainty 
ai access foundation morgan kaufmann publishers 
rights reserved 
bui venkatesh west seminal plan recognition kautz allen considers plan hierarchy deal uncertainty aspects problem 
result approach postulate set possible plans actor unable determine plan probable 
important role uncertainty reasoning plan recognition recognised charniak goldman bauer van beek bayesian probability argued appropriate model charniak goldman van beek 
dynamic line aspect plan recognition considered pynadath wellman goldman geib miller huber durfee wellman albrecht zukerman nicholson 
shares view online plan recognition largely problem probabilistic inference stochastic process models execution actor plan 
view ers general coherent framework modelling di erent sources uncertainty stochastic process need deal quite complex especially consider large plan hierarchy 
main issue computational complexity dealing type stochastic processes complexity scalable complex plan hierarchies 
aim signi cance demonstrate type plan recognition problems described scales reasonably respect number levels abstraction plan hierarchy 
contrast common sense analysis levels plan hierarchy introduce variables stochastic process turn results exponential complexity number levels hierarchy 
order achieve rst assume general stochastic model plan execution model sources uncertainty involved 
model planning hierarchy abstraction uncertainty developed probabilistic planning community sutton precup singh parr russell varaiya hauskrecht meuleau kaelbling dean boutilier dean lin 
advantage adopt basic model known markov policies amp model plan execution 
amp extension policy markov decision processes mdp enables policy invoke re ned policies policy hierarchy 
amp similar contingent plan prescribes sub plan invoked applicable state world achieve intended goal represent uncertainty plan re nement outcomes actions 
amp described simply terms state space markov policy selects set amp amp model plan execution helps focus structure policy hierarchy 
execution amp leads special stochastic process called markov model amm 
noisy observation environment state ects action modelled making state hidden similar hidden state hidden markov models rabiner 
result interesting novel stochastic process term hidden markov model 
intuitively 
known options policies markov decision processes supervisor policies 
policy recognition hidden markov model ahmm models amp causes adoption policies actions di erent levels abstraction turn generate sequence states observations 
plan recognition task observer ahmm corresponding actor plan hierarchy asked infer current policy executed actor levels hierarchy account sequence observations currently available 
amounts reversing direction causality ahmm determine set policies explain sequence observations hand 
shall refer problem policy recognition 
viewing ahmm type dynamic bayesian network dean kanazawa nicholson brady known complexity kind inferencing dbn depends size representation called belief state conditional joint distribution variables dbn time observation sequence boyen koller 
ask question policy hierarchy ect size belief state representation corresponding ahmm 
generally policy hierarchy levels belief state variables size joint distribution exp 
ahmm speci network structure exhibits certain conditional independence properties variables exploited eciency 
rst identify useful independence properties ahmm show compact representation special belief state case state sequence correctly observed full observability assumption starting time policy known 
consequently policy recognition case performed eciently updating ahmm compact belief state 
partial result restricted useful leads important observation general belief state represented compactly approximated eciently collection compact special belief states 
inference problem ahmm particularly amenable technique called rao blackwellisation casella robert allows construct hybrid inference methods combine exact inference approximate sampling inference greater eciency 
application rao blackwellisation ahmm structure reduces sampling space need approximate space xed dimension depend ensuring hybrid inference algorithm scales contributions twofold 
terms stochastic processes dynamic bayesian networks introduce ahmm novel type stochastic processes provide dbn structure analyse properties network 
application rao blackwellised particle filter ahmm results ecient hybrid inference method stochastic model 
terms plan recognition propose novel plan recognition framework probabilistic inference ahmm plan execution model 
complexity inference problem addressed applying range developed techniques probabilistic reasoning plan recognition problem 
illustrates stochastic models plan execution complex exhibit certain special structures exploited construct ecient plan recognition algorithms 
bui venkatesh west structure main body organised follows 
section introduces background material dynamic bayesian networks probabilistic inference 
section formally de nes markov policy policy hierarchy 
section presents ahmm dbn representation conditional independence properties 
algorithms policy recognition discussed section rst special tractable case general case 
section presents experimental results ahmm framework including real time system recognising people behaviour complex spatial environment distributed video surveillance data 
section provides comparative review related probabilistic plan recognition 
conclude discuss directions research section 
background probabilistic inference aim section readers concepts probabilistic inference 
subsections discuss bayesian networks bn dynamic bayesian networks dbn general 
subsection discuss sequential importance sampling sis algorithm general approximate sampling inference method dynamic models 
subsections introduce rao blackwellisation technique improving sampling methods utilising certain special structures dynamic model 
rao blackwellisation key computational technique performing policy recognition 
bayesian networks bayesian network bn pearl jensen castillo gutierrez known probabilistic network belief network established framework dealing uncertainty 
provides graphical compact representation joint probability distribution set domain variables form directed acyclic graph dag nodes correspond domain variables 
node links parent nodes pa parameterised conditional probability node parents pr pa 
network structure parameters encode factorisation joint probability distribution jpd pr pr pa 
bayesian network conditional independence statements form independent variables sets variables asserted separated network structure separation graph separation concept dags pearl 
network structure bn captures certain conditional independence properties domain variables exploited ecient inference 
main inference task bayesian network calculate conditional probability set variables values set variables evidence 
types computation techniques doing 
exact inference algorithms lauritzen spiegelhalter jensen lauritzen olesen ambrosio compute exact value conditional probability required analytical transformation exploits conditional independence relationships variables network 
policy recognition hidden markov model approximative inference algorithms pearl york henrion fung chang shachter peot compute approximation required probability usually obtained forward sampling henrion fung chang shachter peot variance bayesian importance sampling geweke gibbs monte carlo markov chain sampling pearl york 
algorithms advantages simple implementation applied types network trade accuracy estimates computation resources 
known exact inference bn np hard respect network size cooper approximate inference scales network size np hard respect hard bound accuracy estimates dagum luby 
light theoretical results approximate inference useful large networks exact computation intractable certain degree error probability estimate tolerated application 
dynamic bayesian networks model temporal dynamics environment dynamic bayesian network dbn dean kanazawa nicholson brady dagum horvitz special bayesian network architecture representing evolution domain variables time 
dbn consists sequence time slices time slice contains set variables representing state environment current time 
time slice bayesian network network structure replicated time slice 
temporal dynamics environment encoded network links time slice 
addition time slice contain observation nodes model possibly noisy observation current state environment 
dbn sequence observations want draw predictions state variables predicting unobserved variables past smoothing 
problem solved inference algorithm bayesian networks described 
want revise prediction observations arrive time reapplying inference algorithm time observation sequence changes costly especially sequence grows 
avoid need keep joint distribution variables current time slice observation sequence date 
probability distribution termed belief state known ltering distribution plays important role inferencing dbn 
existing inference schemes dbn involve maintaining updating belief state ltering 
new observation received current belief state rolled time slice ahead evolution model conditioned new observation obtain updated belief state 
obvious problem approach size belief state need maintain 
noted interaction variables dbn localised variables belief state highly connected boyen koller 
marginalisation past time slices usually destroys conditional independence current time slice 
size belief state large exact inference methods kj intractable necessary maintain approximation actual belief state form approximate bui venkatesh west distribution represented compactly boyen koller form set weighted samples sequential monte carlo sampling methods doucet godsill andrieu kanazawa koller russell liu chen 
simple case dbn time slice single state variable observation node known hidden markov model hmm rabiner 
filtering simple structure solved dynamic programming discrete hmm rabiner kalman ltering linear gaussian model kalman 
extensions hmm multiple hidden interacting chains coupled hidden markov models chmm factorial hidden markov models fhmm proposed brand ghahramani jordan jordan ghahramani saul :10.1.1.16.2929:10.1.1.16.2929
models size belief state exponential number hidden chains 
inference parameter estimation problems intractable number hidden chains large 
reason approximate techniques required 
chmm brand employs deterministic approximation approximates full dynamic programming keeping xed number heads highest probabilities 
heads chosen deterministically randomly sampling methods 
fhmm ghahramani jordan jordan uses variational approximation jordan ghahramani jaakkola saul approximates full fhmm structure ed tractable structure :10.1.1.16.2929:10.1.1.16.2929
idea similar structured approximation method boyen koller 
ahmm viewed type coupled factorial hmm ahmm consists number interacting chains 
type interaction ahmm di erent types interaction considered brand jordan ghahramani jordan :10.1.1.16.2929:10.1.1.16.2929
main focus ahmm dynamics temporal abstraction chains correlation time interval 
addition node ahmm speci meaning policy state policy termination status links clear causal interpretation policy selection persistence model 
contrast coupled factorial hmm nodes links usually clear semantic causal interpretation 
advantage prior knowledge temporal decomposition process incorporated ahmm naturally 
sequential importance sampling sis sequential importance sampling sis doucet liu chen known particle filter pf general monte carlo approximation scheme dynamic stochastic models 
principle sis method called bayesian importance sampling bis estimator static case geweke 
suppose want estimate quantity dx mean random variable density note taken identity function event simply pr 
arbitrary density function termed importance distribution 
usually importance distribution chosen easy obtain 
weight properly de ned support subset support policy recognition hidden markov model random samples 
expectation estimation rewritten dx dx expression bis estimator obtained bis fx samples taken normalised weight 
note normalised weight computed weight function weight function need computed normalising constant factor 
dynamic case want estimate sequences random variables represents observation available time markov sequence observation hmm 
dbn corresponds set state variables corresponds set observations time slice sis method applies general case non markov depends introduce importance distribution obtain estimator sis ensure obtain sample online sample new value sequence current observation arrives restricted form restriction weight function weight updated online weight updating factor time sampling distribution time means factorised parts choosing di erent obtain di erent forms di erent important distributions example hmm chosen jx jx likelihood weighting lw method chosen jx jx likelihood weighting evidence reversal lw er kanazawa 
general forward chosen corresponding weight 
optimal bui venkatesh west sense discussed doucet chosen associating 
general sis approximation scheme follows 
time maintain sample sequences corresponding weight values fw current observation arrives sequence lengthened new value sampled distribution 
weight value updated 
new samples new weights obtained expectation functional estimated 
procedure enhanced re sampling step markov chain sampling step see doucet 
doucet de freitas murphy russell 
describe important improvements sis 
rao blackwellisation rao blackwellisation general technique improving accuracy sampling methods analytically variables sampling remainder casella robert 
simplest form consider problem estimating expectation joint product variables direct monte carlo sampling obtain estimator 
alternatively rao blackwellised estimator derived sampling variable variable integrated analytically rb jr 
convenience referred rao variable 
rao blackwellised estimator rb generally accurate number samples direct consequence rao blackwell theorem gives relationship unconditional conditional variance varx var xjy var xjy applying problem estimating var var jr var jr var var jr 
suggests direct sampling error rb sampling sample marginalise smaller error sampling number samples degenerated case 
bayesian importance sampling variance convergence result geweke easily prove number samples tend nity rb bis generally better bis number samples 

note improvements orthogonal rao blackwellisation procedure discussed subsequently 
implementation policy recognition algorithm sections include re sampling step crucial keeping error sis time control 
policy recognition hidden markov model sis rao blackwellisation rb sis sis form bis rao blackwellisation improve performance liu chen doucet 
consider problem estimating expectation variable joint product variables 
shall restrict case markov observation represented dbn 
addition consider depends current variable expectation ltering distribution 
example event event depends fx jt tg estimate aj letting ajx ajx aj 
applying rao blackwellisation setting 
sis estimate obtain estimator sis bene doing increase accuracy estimator need sample variables side sample need compute exact inference method 
furthermore sis procedure estimate require additional complexity sequence generally longer depends comparison normal sis estimator sis eq 
number samples accurate computationally demanding compute 
see clearly involved implementing rb sis method look rao blackwellised belief state belief state dynamic process rao variables observed posterior 
entities needed rb sis procedure computed distributions 
functional rewritten terms addition performing sis estimate eq 
weight sampling distribution computed computing rb belief state posterior essential step rb sis method 
maintain rb belief state sample rb variables crucial done eciently exact inference method 
composed variables case dbn choice rao variables rao blackwellised belief state maintained tractable way 
rao blackwellisation especially useful set variables dbn split parts conditioning rst part structure second part tractable amenable exact inference 
bui venkatesh west sample sample jo update weight compute posterior rb bel state jr compute new rb belief state compute compute estimator rb sis general dbn general rb sis algorithm fig 

illustrating purpose assume optimal corresponding jo 
time point need maintain samples sample addition sample weight need store representation rb belief state corresponding sample sequence 
number applications rb sis method known rao blackwellised particle filter rbpf discussed literature 
general framework rb sis inference dbns doucet 
murphy murphy russell 
authors mainly focused case sequence rao variables markov example rb variables root nodes time slice 
assumption simpli es sampling step rb procedure obtaining sample rb variable time straightforward 
previous bui venkatesh west introduced hybrid inference method ahmm special case statespace decomposition policy hierarchy essentially rb sis method 
note applied sequence rao variables satisfy markov property 
case care taken design ecient sampling step especially sampling distribution rb variable tractable form 
non markov rb variables appears special models bayesian missing data model liu chen partially observed gaussian state space model andrieu doucet rb belief state maintained kalman lter 
rao blackwellised belief state tractable context variables framework context speci independence boutilier friedman goldszmidt koller conveniently rao variables murphy 
context variable acts mixing gate di erent bayesian network structures conditioning variables simplify structure remaining vari policy recognition hidden markov model ables 
property context variables boutilier 
suggested cut set variables cut set conditioning inference method pearl 
cut set variables play similar role rao variables help simplify structure remaining network 
rao blackwellised sampling summing possible values cut set variables intractable number representative sampled values 
idea combining exact approximate inference rb sampling similar hybrid inference scheme described dawid kj lauritzen unclear rb sampling described model communicating belief universe 
dawid hybrid inference mainly inference networks mixture continuous discrete variables opposed rb goal improve sampling performance 

markov policies section formally introduce amp concept originating literature probabilistic planning mdps sutton parr russell varaiya hauskrecht dean lin 
main motivation probabilistic planning scale mdp planning problems large state space 
noted hierarchical organisation policies help reduce complexity mdp planning similar role played plan hierarchy classical planning sacerdoti 
comparison classical plan hierarchy policy hierarchy model di erent sources uncertainty planning process stochastic actions uncertain action outcomes stochastic environment dynamics 
planning concerned nding optimal policy reward function focuses policy recognition inverse problem infer agent policies watching ects agent actions 
problems share common element model stochastic plan hierarchy 
policy recognition possible derive information reward function observing agent behaviour choose omitting model reward function optimality notion 
leaves model open tracking arbitrary agent behaviours regardless optimal 
general model actions policies mdp world modelled set possible states termed state space 
state agent set actions available action employed cause world evolve state transition probability 
agent plan actions modelled policy prescribes agent choose action state 
policy modelled selection function state probability agent choose action easy see xed policy resulting state sequence markov chain transition probabilities pr 
policy viewed markov chain state space 
bui venkatesh west local policies original mdp behaviours modelled levels primitive action level plan level policy 
consider policies select re ned policies number abstraction levels 
idea form intermediate level policies policies de ned local region state space having certain terminating condition invoked executed just primitive actions varaiya sutton 
de nition local policy 
local policy tuple hs set applicable states 
set destination states 

stopping probabilities selection function 
current state probability action selected policy state set models local region policy applicable 
called set applicable states policy start state shall assume discrete shall concerned technical details generalising ahmm formulation continuous state space case 
stopping condition policy modelled set possible destination states set positive stopping probabilities probability policy terminate current state possible allow policy state outside enforce condition terminal destination state 
want consider policies deterministic stopping condition 
case destination terminal destination 
deterministically terminating policy ignore redundant parameter need specify set destinations starting state local policy de ned generates markov sequence states transition model 
time destination state reached process stops probability 
process starts terminates states destination states play role possible exits local region state space 
want clear policy currently referred shall subscripted notations denote elements policy 
fig 
illustrates local policy visualised 
fig 
shows set applicable states set destinations chain starting terminating bayesian network fig 
provides detailed view chain start nish 
bayesian network fig 
view chain interested starting stopping states 
policies local policy de ned selects set primitive actions 
similarly generally de ne higher level policies select set policies 
policy recognition hidden markov model visualisation policy de nition policy 
set policies 
policy policies tuple hs set applicable states 
set destination states 

set stopping probabilities 
selection function probability selects policy state note recursiveness de nition allows policy select set policies 
base level primitive actions viewed policies 
primitive actions time step sutton 
idea policies suitable stopping condition viewed just primitive actions rst explicit sutton introduces model representing stopping probabilities 
subsequent sutton introduces policy concept name options 
execution policy follows 
starting state selects policy distribution 
selected policy executed terminated state destination state policy stops probability 
continues new policy selected executed termination fig 

remarks representation policy needed 
denote subset policies applicable policy de ned sure state selects policies applicable selection function 
helps keep speci cation selection function manageable size set policies chosen large 
addition speci cation selection function stopping probabilities factored representations boutilier dearden goldszmidt case state space composite set relatively independent variables 
ensures compact speci cation bui venkatesh west chain generated policy probabilities conditioned state variable state space high dimension 
policy hierarchy policies building blocks construct hierarchy policies follows de nition policy hierarchy 
policy hierarchy sequence number levels hierarchy set primitive actions set policies policies top level policy executed invokes sequence level policies invokes sequence level policies 
level policy invoke sequence primitive actions leads sequence states 
execution generates state sequence terminates destination states sequence simply markov chain suitable stopping conditions 
generally non markovian despite fact policies markov select lower level policies solely current state sutton 
knowing current state provide information current intermediate level policies ect selection state intuitively means agent behaviour achieve goal usually non markovian choice actions depends current state current intermediate intentions agent 
term dynamical process executing top level policy markov model amm 
states partially observable observation modelled usual observation model pr 
resulting process termed hidden markov model ahmm states hidden hidden markov model rabiner 
idea having higher level policy controlling lower level ones mdp traced back varaiya investigated layer structure similar level policy hierarchy deterministic stopping condition 
varaiya showed sub process obtained sub sampling state policy recognition hidden markov model environment partition sequence time level policy terminates markov policies level simply play role extended action 
framework policy hierarchy consider lifted model policies level observations time points policy level ends considered 
level policies considered primitive actions lifted model treated normal model 
state space region decomposition cases state space dimensions exhibit natural hierarchical structure 
example spatial domain set ground positions divided small local spaces rooms corridors set local spaces grouped form larger space higher level buildings 
intuitive method constructing policy hierarchy case called region decomposition state space dean lin hauskrecht 
state space successively partitioned sequence partitions pk pk corresponding levels abstraction pk fsg coarsest partition nest 
region periphery er de ned set states connected state er set peripheral states level er er 
fig 
shows example state space representing building partitioned regions corresponding rooms 
peripheral states region shown fig fig shows peripheral states 
construct policy hierarchy rst de ne region set policies applicable having er destination states 
example room fig de ne set policies model agent di erent behaviours inside room getting particular door 
policies initiated inside room terminate agent steps room necessarily target door policy fail achieve intended bui venkatesh west target 
note er policies de ned manner deterministic stopping conditions 
set policies de ned higher level region de ne set policies model agent behaviours inside region applicable state space destination set er constraint policies policies previously de ned level achieve goals 
example policy navigate room doors get building gate 
set policies de ned level continuing doing higher levels obtain policy hierarchy 
policy hierarchy constructed state space region decomposition termed srd policy hierarchy 
srd policy hierarchy property set applicable states policies abstraction level forms partition state space 
state sequence resulting execution top level policy infer exact starting terminating times intermediate level policies 
example level starting stopping times policies level time indices state sequence crosses region boundary region partition section show property helps simplify complexity policy recognition problem 
policy hierarchy example example consider task monitor predict movement agent building shown fig 

room represented grid adjacent rooms connected door center common edge 
entrances building labeled north west south east 
addition door center building acts entrance building north wing south wing 
state cell agent move possible directions blocked wall 
policy hierarchy model agent behaviour environment constructed region decomposition levels abstraction 
firstly region hierarchy constructed 
partition environment consists rooms level wings north south level entire building level 
behaviours agent level room represented set level policies 
example room level policies model agent behaviours exiting room di erent doors 
essentially markov chains room terminate outside room 
way represent policies specify movement action agent take current position current heading 
higher level agent behaviours wing speci ed 
example level policies wing model agent behaviours exiting wing wing exits 
policies built top set level policies de ned 
specify level policies agent take leave wing intended exit 
top level agent behaviours entire building speci ed 
example top level policies model agent behaviours leaving building building exits sample policies parameters fig 

policy recognition hidden markov model go level policy right level current state destination level policy 
destination right go right door level policy go back door level policy level policy current state destination prior top level policy back door right door environment parameters ahmm go level policy example policy hierarchy amm plan execution model amm formal plan execution model plan recognition process 
subsection discuss expressiveness amm formal plan speci cation language suitability amm encode plans context plan recognition 
note discussion focuses representational aspect amm 
discussion computational aspects amm ahmm comparison works probabilistic plan recognition section 
amm particularly suited representing goal directed behaviours different levels abstraction 
policy amm viewed plan trying achieve particular goal 
classical plan policy speci es course actions applicable states similar contingent plan 
policy means goal achieved attempt achieve goal current policy failed 
interpretation persistence policy ts persistence model intentions cohen levesque intention ends guarantee intended goal achieved 
conceptually types destination states corresponds intended goal states corresponds unintended failure states resulting stochastic nature execution plan 
due generality amm need distinguish types successful termination states unsuccessful ones treated possible destination states albeit di erent reaching probabilities 

expect agent reach intended destination state random failure state 
bui venkatesh west amm model plan execution allows blur di erence planning re planning 
time moves recognition classical plan recognition agent intention 
existing framework probabilistic plan recognition explicitly represent current state relationship states adoption termination current plans ignored goldman 
impossible tell current plan failed new plan attempt recover failure current plan succeeded new plan part new higher level goal 
expressive language describing probabilistic plan hierarchical machines ham proposed parr russell parr 
ham policy replaced stochastic nite automaton call machines lower level 
policies written machines type 
machine choose machines correspond policies lower level go back start state called machines terminated 
ham framework allows machines arbitrary nite number machine states transition probabilities readily represent complex plans concatenation policies alternative policy paths possible represent machine ham policy amm cost augmenting state space include machine states machines current call stack 
size amm new state space exponential respect number nested levels ham call stack 
shows theory expressiveness ham policy hierarchy performing policy recognition ham equivalent policy hierarchy probably unwise state space exponentially large conversion 
better idea represent internal state machine variable dbn perform inference dbn structure directly 
amm closely related model probabilistic plan recognition called probabilistic state dependent grammar psdg independently proposed pynadath pynadath wellman 
psdg described probabilistic context free grammar pcfg jelinek la erty mercer augmented state space state transition probability table terminal symbol pcfg 
addition probability production rule state dependent 
result terminal symbol acts primitive actions non terminal symbol chooses expansion depending current state 
interestingly psdg directly related ham language described similar way production rule grammars related nite automata 
psdg convert equivalent ham constructing machine non terminating symbol modelling production rules non terminating symbol automaton 
policy hierarchy equivalent special class psdg production rules form allowed 
rule models adoption lower level policy higher level policy models termination policy psdg model considered pynadath pynadath wellman allows general rules form recursion symbol 
exceptions goldman pynadath wellman discussed detail section 
constraint recursion calling stack keep stack nite 
policy recognition hidden markov model located expansion 
psdg policy expanded sequence policies lower level executed control returned higher level policy 
implicit assumption policy sequence terminates state policy sequence applicable 
assumption language ahmm de ne compound policy policy simply orderly executes sequence policies lower level independent current state 
psdg equivalent ahmm compound policies form allowed 
amm closely follows models probabilistic planning model recognise behaviours autonomous agent decision making process equivalent mdp 
useful formal language specifying contingent plans execution monitored policy recognition algorithm 
language rich specify range useful human behaviours especially domains natural hierarchical decomposition state space 
section presents application ahmm framework problem recognising people behaviours complex spatial environment 
policy ahmm represents evolution possible trajectories people movement person performs certain task environment heading door computer certain location policies di erent levels represent evolution trajectories di erent levels abstraction 
due existing hierarchy domain policies constructed region decomposition state space 
environment populated multiple cameras divided di erent zones provide current location tracking target albeit noisy 
noisy observations readily handled observation model ahmm 
policy recognition algorithm applied infer person current policy di erent levels hierarchy 
main restriction current ahmm model consider toplevel policy time unable model inter leaving concurrent plans 
subtle restriction assumption high level policy selects lower level policies depending current state 
state space interpreted states external environment assumption implies actor full observation current state re nes intentions actor observation current state entire observation history 
note restrictions ahmm apply case psdg model 

dynamic bayesian network representation section describe dynamic bayesian network dbn representation ahmm 
network serves purposes tool derive probabilistic independence property stochastic model computational framework policy recognition algorithms section 
network construction time represent current state represent current policy level represent status boolean variable indicating bui venkatesh west sub network policy termination policy terminates current time 
variables current time slice full dbn 
convenience notation refers set current policies presenting full network rst describe sub structures model policies terminated selected 
full dbn easily constructed sub structures 
policy termination de nition policies level policy terminates lower level policy terminates terminates probability 
bayesian network representation terminating status parent nodes fig 

parent variable plays special role 
meaning lower level policy terminates current time pr gives conditional probability parent variables fig 

terminate deterministically determined independent parent variables notion context speci independence csi boutilier safely remove links parents context false fig 

bottom level primitive action terminates immediately modelling execution single top level policy assume top level policy terminate remains unchanged note time exists variable termed highest level termination time knowing value equivalent knowing terminating status current policies 
policy selection current policy general dependent higher level policy previous state previous policy level status bayesian network variables parents fig 

depen policy recognition hidden markov model prev prev prev prev prev prev prev prev prev sub network policy selection dency broken cases depending value parent node previous policy terminated current policy previous variable independent context links current policy removed nodes merged fig 

previous policy terminated current policy selected higher level policy probability pr 
context independent corresponding link bayesian network removed fig 

full dbn full dynamic bayesian network constructed policy status state variables putting sub networks policy termination selection fig 

top level remove status nodes merge single node base level remove status nodes links model observation hidden states observation layer attached state layer shown fig 

suppose context variable known 
modify full dbn corresponding link removal node merging rules 
result intuitive tree shaped network fig 
policy nodes corresponding policy entire duration grouped 
grouping done knowing value equivalent knowing exact duration policy hierarchy 
expect performing probabilistic inference structure simple full dbn fig 

particular state sequence known remainder network fig 
singly connected directed graph undirected cycles allowing inference performed complexity linear size network pearl 
policy recognition algorithms follow exploit extensively particular tractable case ahmm 
bui venkatesh west action level status policy policy status state observation dbn representation hidden markov model 
simpli ed network duration policy known action nodes omitted clarity policy recognition hidden markov model conditional independence current time slice discussion identi es tractable case ahmm requires knowledge entire history state policy status variables 
subsection focus conditional independence property nodes current time slice nodes belief state inference algorithm ahmm independence properties variables exploited provide compact representation belief state reduce inference complexity 
due way policies invoked amm intuitive higher level policies uence happens lower level current level 
precisely level policy know starting state course execution fully determined determined means uence happening higher levels 
furthermore know long policy executed equivalently starting time current state execution determined 
higher level policies uence current state execution starting state starting time 
words know starting time starting state current higher level policies completely independent current lower level policies current state 
theorem formally states precise form 
note condition obtained strictest conditional variables unknown examples higher level policies uence lower level ones 
theorem 
random variables representing starting time starting state respectively current level policy tg denote set current policies level fs denote set current policies level current state 
proof 
sketch intuitive proof theorem bayesian network manipulation rules context speci independence discussed 
alternative proof csi bui 
rst note theorem obvious looking full dbn fig 

shall proceed modifying network structure context know time policies level terminate remove links policies new policies time 
hand time current time policies level terminate group policies level time node representing current policy level network manipulation steps result network structure shown fig 

modi ed network structure obtained observe bui venkatesh west 
time state network structure conditioned separated new structure 
independent 
policy recognition section address problem policy recognition framework ahmm 
assume policy hierarchy modelled ahmm top level policy details execution unknown 
problem determine top level policy current policies lower levels current sequence observations 
concrete terms interested conditional probability pr especially marginals pr levels computing probabilities gives information current policies levels abstraction current action top level policy account observations date 
typical monitoring situations probabilities need computed online new observation available 
required update belief state ltering distribution ahmm time point problem generally intractable belief state ecient representation ords closed form update procedure 
case belief state joint distribution discrete variables pr 
structure imposed belief state complexity updating exponential cope complexity generally resort form approximation trade accuracy computational resources 
hand analysis policy recognition hidden markov model ahmm network previous section suggests problem inference ahmm tractable special case history state terminating status variables known 
motivated property ahmm main aim section derive hybrid inference scheme combines approximation tractable exact inference eciency 
rst treat special case policy recognition belief state ahmm tractable structure 
hybrid inference scheme general case rao blackwellised sequential importance sampling rb sis method 
policy recognition tractable case address policy recognition problem assumptions state sequence observed certainty exact time policy starts ends known 
precisely observation time includes state history policy termination history 
belief state need compute case pr posterior absorbing observation time pr 
rst assumption means observer knows true current state referred full observability 
states fully observable ignore observation layer fo ahmm deal amm 
second assumption means observer fully aware current policy ends new policy begins 
policy hierarchy constructed region decomposition state space subsection termination status inferred directly state sequence 
srd policy hierarchies full observability condition needed second assumption subsumed rst left 
srd policy hierarchies assumptions usually restrictive policy recognition algorithm useful 
algorithm special case form exact step hybrid algorithm subsection general case 
representation belief state rst look conditional joint distribution pr 
termination history derive precisely starting time current level policy maxf ft tj tg maxf ft tj kg hand knowing starting time state history gives starting state starting time starting state derived theorem obtain level words conditional joint distribution represented bayesian network simple chain structure 
denote chain network pr term belief chain role plays representation belief state fig 

chain drawn links bui venkatesh west root representation belief state point away level node say chain root level root chain moved level simply reversing links lying standard link reversal operation bayesian networks shachter 
node belief chain manageable size 
principle domain set policies level domain set possible states 
large basically want model larger state space set policies cover state space large 
sizes domains grow exponential particular state number policies applicable state remain relatively constant independent policy know starting state implies set level policies applicable local domain avoid exponential dependency similarly domain taken set neighbouring states reachable performing primitive action 
state term maximum number relevant objects applicable policies actions neighbouring states single level degree connectivity domain modelled 
size conditional probability table link belief chain size belief chain kn 
construct belief state current terminating status solely determined current policies current state belief state factorised pr pr pr pr note variable equivalent set variables fe full belief state realised adding links current policies current state terminating status variables fig 

size belief state policy recognition hidden markov model belief state updating kn 
state composite orthogonal variables factored representation size belief state representation depend exponentially dimensionality state space 
discuss factored representations subsection 
updating belief state belief state represented simple belief network fig 
expect general exact inference method updating belief state kj eciently 
general method works undirected network representation belief state distribution inconvenient want sample distribution 
describe algorithm updates belief state closed form directed network fig 

assuming complete speci cation belief state parameters bayesian network representation need compute parameters new network done steps standard roll belief state dbn absorbing new evidence projecting belief state time step 
rst step corresponds instantiation variables bayesian network obtain conditional joint distribution checking conditional independence relationships fig 
easy see simple chain network structure 
conceptually problem update parameters chain absorb evidence form new chain done number link reversal steps follows 
instantiate rst move root chain variable parents instantiated deleted network fig 

instantiate equivalent value assignment starting iteratively reverse links fig 

algebraic forms rst link reversal operation corresponds bui venkatesh west computing probabilities pr pr pr pr pr pr second link reversal corresponds pr pr pr ectively th link reversal step positions root chain absorbs evidence repeating link reversal operations obtain new chain root level 
note need incorporate instantiations direct consequences instantiation parameters chain 
upward links remain marginal level downward links obtained results link reversal operations pr pr pr pr pr pr second step continue compute policies levels higher terminate retain upper sub chain lower part new policy created policy state new sub chain formed variables parameters pr 
note domain newly created node 
new chain combination sub chains chain root level see fig 

chain new belief state obtained simply adding terminating status variables fe completes procedure updating belief state allowing compute belief state time step 
belief state joint distribution current variables due simple structure marginal distribution single variable computed easily 
example interested current level policy marginal probability pr simply marginal level node chain readily obtained chain parameters 
complexity belief state updating procedure time proportional needs modify bottom levels belief state 
hand probability current policy level terminates assumed exponentially small average updating complexity time step exp constant bounded depend number levels policy hierarchy 
terms number policies states updating complexity linear size policy node belief chain linear degree connectivity domain 
policy recognition hidden markov model belief state updating policy recognition general case return general case policy recognition assumptions previous subsection 
inference tasks ahmm dicult 
starting times starting states current policies known certainty theorem 
set current policies longer forms chain structure conditional independence properties current time slice longer hold 
hope represent belief state simple structure previously 
exact method updating belief state operate structure size exponential bound intractable large 
cope complexity approximation scheme sequential importance sampling sis doucet liu chen kanazawa employed 
previous bui venkatesh west applied sis method known likelihood weighting evidence reversal lw er kanazawa ahmm network structure 
sis method needs sample product space layers ahmm accurate inecient large key get ineciency utilise special structure ahmm particularly special tractable case keep set variables need sampled minimum 
improvement sis method achieve subsection name rao blackwellised sis rb sis method 
rao blackwellisation speci cally allows marginalisation variables analytically samples remaining variables 
result reduces averaged error measured variance estimator casella robert 
bui venkatesh west order apply rb sis ahmm main problem identify variables rao variables sampled remaining variables marginalised analytically 
key choosing variables shown variables observed rao blackwellised belief state tractable 
subsection demonstrated state history terminating status history observed belief state simple network structure updated constant average complexity 
conveniently rao variable note variables context variables help simplify network structure ahmm state variables help remaining network singly connected exact inference operate eciently see subsection 
rb sis ahmm discuss speci application rb sis problem belief state updating policy recognition ahmm 
main objective rb sis estimate conditional probability policy currently executed level current sequence observations pr 
mapping rb sis general framework subsection ahmm structure set current variables set current policies terminating status nodes current state 
probability estimation pr viewed expectation letting pr pr pr pr rb sis estimate expectation shall split sets variables set rb variables set remaining variables set current policies 
functional depends rb variables obtained integrating remaining variables eq 
form pr pr pr marginal belief chain time 
rb belief state belief state ahmm rb variables known pr pr identical special belief state discussed subsection minor modi cation attach observation variable function rb belief state computed eciently exact inference techniques described 
rb sis implemented eciently minimal overhead exact inference 
main rb sis algorithm ahmm fig 

note need sample rb variables sample addition weights policy recognition hidden markov model sample sample update weight compute posterior rb bel state js compute belief chain compute new belief state compute compute estimator pr rb sis policy recognition highest level termination er sampling rao variables ahmm maintain parametric representation rao blackwellised belief state value function sample weights samples values function combined yield approximation details obtain new samples time step worth noting 
optimal sampling distribution sample bui venkatesh west rb variables need perform evidence reversal step 
done positioning root belief chain reverse link gives network structure er exactly see fig 
evidence absorbed marginal distribution weight obtained product evidence reversal step 
order sample er need compute marginal distribution variables forward sampling sample variable er starting root node proceeding upward 
de nition highest level policy termination sampling rst level assign value 
unnecessary samples policy nodes way discarded 
new samples updating rb belief state identical belief state updating procedure described 
function obtained computing corresponding marginal new belief chain time step complexity maintaining sample sampling new rb variables updating rb belief state average bounded constant 
complexity maintaining sample average 
prediction needed sample compute manipulating chain complexity 
complexity time step prediction needs nk 
comparison sis method lw er rb sis order computational complexity sis complexity nk 
sis method needs sample layers ahmm rb sis method needs sample sequences variables avoids having sample policy sequences rao blackwellisation dimension sample space smaller importantly grow result accuracy approximation rb sis method depend height hierarchy contrast due problems sampling high dimensional space accuracy sis methods tends degrade especially large 
performing evidence reversal factored state space cases state space cartesian product state variables representing relatively independent properties state 
state space large specifying action usual transition probability matrix problematic 
advantageous case represent state information factored form representing state variable separate node lumping single node shown factored representations specify transition probability action compact form action ect small number state variables speci cation ects actions regularities boutilier 

term evidence reversal refer general procedure link observation node reversed prior sampling kanazawa allowing sample optimal sampling distribution policy recognition hidden markov model representation belief chain rb belief state take direct advantage factored representation actions 
chain parameter link precisely transition probability action previous state note known due rao blackwellisation 
conditional distribution extracted compact factored representation general form bayesian network variables fs convenience denote bayesian network 
network usually sparse exact inference operate eciently 
example special case fs independent factored completely product marginals factored representations part rb belief state care taken performing evidence reversal reverse link state variable observation node 
procedure evidence reversal discussed previously see fig 
rst position root node need compute represent distribution pr 
factored state space case joint distribution state variables fs conditioning current action factored representation state variables fs utilised resulting complexity exponential key get diculty keep speci cation distribution current state conditioned current action vice versa 
computing er jo rst position root chain reverse evidence algebraic form factorisation joint distribution current action state current observation pr jo pr pr jo fig 
illustrates evidence reversal procedure 
model depicted arbitrary bayesian network 
observation model speci ed attaching observation nodes fo state variables 
network representing distribution pr denoted obs 
rst look rst term rhs 
er represent distribution pr 
note er obtained conditioning obs observation achieved applying exact inference method clustering algorithm lauritzen spiegelhalter network obs 
second term rhs note pr pr obs integration readily obtained product performing clustering algorithm obs 
pr known compute pr pr pr pr shows belief state evidence reversal er simple structure exploits independence relationships state variables fs current action sampling rb variables structure proceed bui venkatesh west 
belief state evidence reversal belief state evidence reversal 

er obs evidence reversal factored state space follows pr rst sample er sample obtained sample proceed sample remaining nodes network er obtain sample usual 
note weight pr computed eciently pr pr pr evidence reversal procedure value need perform exact inference structure obs 
complexity procedure heavily depends complexity network structure noted due nature factored representation usually sparse structure exact inference performed eciently 
example special case completely factored product independent state variables independently observed complexity linear 
experimental results section experimental results policy recognition algorithm 
subsection demonstrate ectiveness rao blackwellised sampling method policy recognition comparing performance rao blackwellised procedure likelihood weighting sampling synthetic tracking task 
subsection application ahmm framework problem tracking human behaviours complex spatial environment distributed video surveillance data 
policy recognition hidden markov model rm rm rm rm rm rm rm rm north wing south wing environment sample trajectory time destination probabilities west south east north probabilities top level destinations time ectiveness rao blackwellisation demonstrate ectiveness rao blackwellised inference method ahmm consider synthetic tracking task required monitor predict movement agent building environment previously discussed subsection 
structure ahmm shown fig 

parameters policies chosen manually simulate movement agent building 
simulate observation noise assume observation agent true position neighbouring cells probabilities prede ned observation model 
bui venkatesh west std 
deviation sample size sis sqrt rb sis sqrt sample size average error cpu time second sample size sis rb sis sample size cpu time std 
deviation cpu time second sis rb sis cpu time average error performance pro les sis vs rb sis policy recognition hidden markov model sample size sis rb sis eciency coecients sis rb sis implement rb sis method re sampling policy hierarchy speci cation simulated observation sequence input algorithm 
typical run algorithm return probability main building exit wing exit room door agent currently heading 
example track shown fig 

observations track arrive time prediction probability distribution main building exit track heading shown fig 

illustrate advantage rb sis implement sis method lw er re sampling kanazawa compare performance algorithms 
run algorithms di erent sample population sizes obtain performance pro les 
sample size standard deviation runs estimated probabilities top level policies measure expected error probability estimates 
record average time taken update iteration 
fig 
plots standard deviation algorithms di erent sample sizes 
behaviour error follows closely theoretical curve sis rb sis 
expected number samples rb sis algorithm delivers better accuracy 
fig 
plots average cpu time taken iteration versus sample size 
expected linear rb sis twice longer due overhead updating rb belief state processing sample 
fig 
plots actual cpu time taken versus expected error algorithms 
shows cpu time spent rb sis method signi cantly reduces error probability estimates 
note algorithm quantity approximately constant dependency cancels 
constant eciency coecient measure performance sampling algorithm independent number samples 
example algorithm twice smaller coecient deliver accuracy half cpu time half variance cpu time 
fig 
plots eciency coecients sis rb sis sis bui venkatesh west rb sis 
indicates performance gain order magnitude folds rb sis 
application tracking human behaviours policy recognition algorithm implemented real time surveillance system tracks behaviour people complex indoor environment surveillance video data 
environment consists corridor vision lab see fig 

people enter exit scene left right entrance corridor 
system static cameras overlapping eld views cover ground plane scene 
entire environment divided grid cells current cell position tracked object acts current state ahmm 
cameras calibrated return current position tracked object ground returned coordinates unreliable cameras deal noisy video frames occlusion objects scene 
information low level tracking done multiple cameras readers referred nguyen venkatesh west bui 
assume observation state area surrounding observation model matrix specifying observation likelihood cell neighbourhood current state 
policy hierarchy behaviours environment constructed follows 
construct region hierarchy levels 
bottom level identify regions special interest corridor areas surrounding linux server nt server printer remaining free space vision lab fig 

higher level regions vision lab grouped 
top level consists entire environment 
policy hierarchy representing people behaviors levels corresponding levels region hierarchy see fig 

bottom level interested behaviours take place regions interest 
example near linux server person linux machine simply passing region leading di erent policies 
similar policies de ned nt server region printer region small 
corridor inside vision lab region construct di erent policies corresponding di erent destinations person heading 
region special policy representing walk behaviour 
middle level policies de ned corridor oce space representing person plan exiting space left right entrance door vision lab 
de ne policy vision lab represent typical behaviour lab user go linux server followed go printer 
top level region environment de ne policies representing person leaving scene left right entrance 
fig 
show concurrent trajectories di erent people environment 
sample video frames captured di erent cameras system shown fig 

ahmm model de ned sequence observations returned cameras rst determine performance pro les rb sis sis real 
consider di erent groups lab users group give rise di erent policy level 
policy recognition hidden markov model camera camera camera table nt server region printer book shelf book shelf camera camera table table left entrance right entrance region region region region vision lab corridor office office region region time slice time slice time slice linux server time slice camera environment trajectory person bui venkatesh west camera camera camera table table linux server nt server printer book shelf book shelf camera camera table table left entrance right entrance printer region corridor office office time slice time slice vision lab linux region nt region region camera trajectory person middle level bottom level top level policies policies policies nt region policies policies policy policies environment corridor offices vision lab policies office policies policies office corridor linux region printer region empty space region policy hierarchy policy recognition hidden markov model person enters scene person enters scene 
cpu time second rb sis sis error vs cpu time cpu time second sis rb sis eciency coecients performance rb sis sis real tracking data bui venkatesh west time left right probabilities person leaving scene entrances top level policies environment 
algorithms behave similar way previous experiment simulated data 
fig 
shows error curve cpu time algorithms 
eciency ecient rb sis case rb sis sis sis 
shows rb sis performs times better sis domain 
surveillance system low level tracking module returns observations rate approximately second 
observation passed rb sis algorithm produces probability estimate current policy di erent levels hierarchy 
moment surveillance system run real time amd machines 
examples output returned system trajectories fig 
fig 

fig 
shows probabilities person exiting environment left right entrance denoted left right respectively 
left increases person heading left entrance see trajectory fig 

left approximately constant time slice person inside vision lab 
middle level policy de ned vision lab movement inside lab independent nal exit entrance 
time slice left decreases person leaving lab turning right entering oce 
increases approaches leaving oce turning left going left entrance 
contrast right falls quickly zero time 
look results querying bottom level policies 
fig 
shows distribution possible destinations person time slice time slice region see trajectory fig 
probabilities obtained show system able correctly detect walk behaviour 
nal result fig 
shows inferred behaviours person linux server region 
initially probabilities linux server passing 
person stays position extended period time system able identify correct behaviour person linux server 
policy recognition hidden markov model time linux printer nt behaviours person inside vision lab time linux pass behaviour person inside linux server region bui venkatesh west 
related probabilistic plan recognition case probabilistic inference plan recognition argued convincingly charniak goldman 
plan recognition bayesian network charniak goldman static network 
approach run problems process line stream evidence plan 
approaches pynadath wellman goldman huber albrecht dynamic stochastic models plan recognition suitable doing line plan recognition uncertainty 
closely related model ahmm probabilistic grammar psdg pynadath pynadath wellman 
comparison representational aspect models discussed subsection 
terms algorithms plan recognition pynadath wellman er exact method deal case states fully observable 
states partially observable brute force approach suggested amounts summing possible states 
note fully observable case belief state need deal large policy starting times unknown 
exact method pynadath wellman complexity maintaining belief state exponential number levels psdg expansion hierarchy height policy hierarchy 
hand rb sis policy recognition algorithm handle partially observable states rao blackwellisation procedure ensures sampling algorithm scales number levels policy hierarchy 
furthermore noted subsection consider compound policies psdg converted ahmm 
framework compound policy represented just normal policy slight modi cation variable take values value indicates compound policy terminated 
policy recognition algorithm modi ed model 
similar ahmm psdg goldman 
detailed model plan execution process 
rich language probabilistic horn abduction able model sophisticated plan structures interleaved concurrent plans partially ordered plans 
serves mainly representational framework provides analysis complexity plan recognition setting 
probabilistic plan recognition date employed coarser models plan execution 
ignored important uence state world agent planning decision goldman 
best knowledge date addressed problem partial noisy observation state 
psdg look observation outcomes actions assume action observed directly accurately 
note kind simplifying assumptions needed previous computational complexity performing probabilistic plan recognition remains manageable 
contrary illustrates plan recognition dynamic stochastic model 
course srd policy hierarchy considered full observability 
policy recognition hidden markov model complex exhibit special types conditional independence exploited lead ecient plan recognition algorithms 

summary approach line plan recognition uncertainty ahmm model execution stochastic plan hierarchy noisy observation 
ahmm novel type stochastic processes capable representing rich class plans associating uncertainty planning plan observation process 
rst analyse ahmm structure conditional independence properties 
leads proposed hybrid rao blackwellised sequential importance sampling rb sis algorithm performing belief state updating ltering ahmm exploits structure ahmm greater eciency scalability 
show complexity rb sis applied ahmm depends linearly number levels policy hierarchy sampling error depend terms plan recognition results show stochastic process representing execution plan hierarchy complex exhibit certain conditional independence properties inherent dynamics planning acting process 
independence properties exploited help reduce complexity performing inference plan execution stochastic model leading feasible scalable algorithms line plan recognition noisy uncertain domains 
scalability algorithm policy recognition provides possibility consider complex plan hierarchies detailed models plan execution process 
key achieve eciency shown combination developed techniques probabilistic inference compact representations bayesian networks context sensitive independence factored representations hybrid dbn inference take advantage compact representations rao blackwellisation 
research directions possible 
investigate ahmm consider problem learning parameters ahmm database observation sequences learn plan execution model observing multiple episodes agent executing plan 
structure ahmm suggests try learn model policy separately 
observe execution policy separately learning problem reduced hmm parameter re estimation level policies simple frequency counting higher level policies 
observation sequence long episode clear cut temporal boundary policies problem type parameter estimation dbn hidden variables techniques dealing hidden variables em dempster laird rubin applied 
extensions ahmm model expressive suitable representing complex agents plans 
example expressive plan execution model ham model parr considered state independent sequences policies represented 
current model enriched consider set top level policies interleaved execution 
expect new models exhibit context speci independence properties similar bui venkatesh west ahmm rao blackwellised sampling methods policy recognition models derived 
anonymous reviewers insightful comments helped improve presentation contents 
albrecht zukerman nicholson 

bayesian models keyhole plan recognition adventure game 
user modelling user adapted interaction 
andrieu doucet 

particle ltering partially observed gaussian state space models 
tech 
rep cued infeng tr 
signal processing group university cambridge cambridge uk 
bauer 

integrating probabilistic reasoning plan recognition 
proceedings eleventh european conference arti cial intelligence 
boutilier dearden goldszmidt 

stochastic dynamic programming factored representations 
arti cial intelligence 
appear 
boutilier friedman goldszmidt koller 

context speci independence bayesian networks 
proceedings annual conference uncertainty arti cial intelligence 
boyen koller 

tractable inference complex stochastic processes 
proceedings fourteenth annual conference uncertainty arti cial intelligence 
brand 

coupled hidden markov models modeling interacting processes 
tech 
rep mit media lab 
bui venkatesh west 

layered dynamic bayesian networks spatio temporal modelling 
intelligent data analysis 
bui venkatesh west 

recognition markov policies 
proceedings national conference arti cial intelligence aaai pp 

casella robert 

rao blackwellisation sampling schemes 
biometrika 
castillo gutierrez 

expert systems probabilistic network models 
springer 
charniak goldman 

bayesian model plan recognition 
arti cial intelligence 
cohen levesque 

intention choice commitment 
arti cial intelligence 
policy recognition hidden markov model cooper 

computational complexity probabilistic inference baysian belief networks 
arti cial intelligence 
dagum luby 

approximating probabilistic inference bayesian belief networks np hard 
arti cial intelligence 
dagum horvitz 

dynamic network models forecasting 
proceedings eighth annual conference uncertainty arti cial intelligence pp 

ambrosio 

incremental probabilistic inference 
proceedings ninth annual conference uncertainty arti cial intelligence pp 

dawid kj lauritzen 

hybrid propagation junction trees 
zadeh 
ed advances intelligent computing lecture notes computer science pp 

dean kanazawa 

model reasoning persistence causation 
computational intelligence 
dean lin 

decomposition techniques planning stochastic domains 
proceedings fourteenth international joint conference arti cial intelligence ijcai 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
doucet de freitas murphy russell 

rao blackwellised particle ltering dynamic bayesian networks 
proceedings sixteenth annual conference uncertainty arti cial intelligence 
doucet godsill andrieu 

sequential monte carlo sampling methods bayesian ltering 
statistics computing 
varaiya 

multilayer control large markov chains 
ieee transactions automatic control 
fung chang 

weighting integrating evidence stochastic simulation bayesian networks 
proceedings fifth conference uncertainty arti cial intelligence 
geweke 

bayesian inference econometric models monte carlo integration 
econometrica 
ghahramani jordan 

factorial hidden markov models 
machine learning 
goldman geib miller 

new model plan recognition 
proceedings fifteenth annual conference uncertainty arti cial intelligence 
hauskrecht meuleau kaelbling dean boutilier 

hierarchical solution markov decision processes macro actions 
proceedings fourteenth annual conference uncertainty arti cial intelligence 
henrion 

propagating uncertainty bayesian networks probabilistic logic sampling 
lemmer kanal 
eds uncertainty arti cial intelligence amsterdam 
north holland 
bui venkatesh west huber durfee wellman 

automated mapping plans plan recognition 
proceedings tenth annual conference uncertainty arti cial intelligence 
jelinek la erty mercer 

basic methods probabilistic context free grammar 
mori 
eds advances speech recognition understanding pp 

springer verlag 
jensen 

bayesian networks 
springer 
jensen lauritzen olesen 

bayesian updating recursive graphical models local computations 
computational statistics quarterly 
jordan ghahramani jaakkola saul 

variational methods graphical models 
machine learning 
appear 
jordan ghahramani saul 

hidden markov decision trees 
mozer jordan petsche 
eds advances neural information processing systems cambridge ma 
mit press 
kalman 

new approach linear ltering prediction problems 
journal basic engineering 
kanazawa koller russell 

stochastic simulation algorithms dynamic probabilistic networks 
proceedings eleventh annual conference uncertainty arti cial intelligence pp 

kautz allen 

generalized plan recognition 
proceedings fifth national conference arti cial intelligence pp 



computational scheme reasoning dynamic probabilistic networks 
proceedings eighth annual conference uncertainty arti cial intelligence pp 

kj 

computational system dynamic time sliced bayesian networks 
international journal forecasting 
lauritzen spiegelhalter 

local computations probabilities graphical structures application expert systems 
journal royal statistical society 
liu chen 

sequential monte carlo methods dynamic systems 
journal american statistical association 
murphy russell 

rao blackwellised particle ltering dynamic bayesian networks 
doucet de freitas gordon 
eds sequential monte carlo methods practice 
springer verlag 
murphy 

bayesian map learning dynamic environments 
advances neural information processing systems pp 

mit press 
nguyen venkatesh west bui 

coordination multiple cameras track multiple people 
proceedings asian conference computer vision pp 

policy recognition hidden markov model nicholson brady 

data association problem monitoring robot vehicles dynamic belief networks 
proceedings tenth european conference arti cial intelligence pp 

parr 

hierarchical control learning markov decision processes 
ph thesis university california berkeley 
parr russell 

reinforcement learning hierarchies machines 
advances neural information processing sytems nips 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann san mateo ca 
pearl 

evidential reasoning stochastic simulation causal models 
arti cial intelligence 
pynadath 

probabilistic grammars plan recognition 
ph thesis computer science engineering university michigan 
pynadath wellman 

accounting context plan recognition application trac monitoring 
proceedings eleventh annual conference uncertainty arti cial intelligence 
pynadath wellman 

probabilistic state dependent grammars plan recognition 
proceedings sixteenth annual conference uncertainty arti cial intelligence 
rabiner 

tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
sacerdoti 

planning hierarchy abstraction spaces 
arti cial intelligence 
shachter 

evaluating uence diagrams 
operations research 
shachter peot 

simulation approaches general probabilistic inference belief networks 
proceedings fifth conference uncertainty arti cial intelligence 
sutton 

td models modelling world mixture time scales 
proceedings internation conference machine learning icml 
sutton precup singh 

mdp semi mdps framework temporal abstraction reinforcement learning 
arti cial intelligence 
van beek 

investigation probabilistic interpretations heuristics plan recognition 
proceedings fifth international conference user modeling pp 

york 

gibbs sampler expert systems 
arti cial intelligence 

