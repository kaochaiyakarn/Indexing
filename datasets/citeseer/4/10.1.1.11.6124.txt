dortmund fachbereich informatik lehrstuhl viii intelligenz text categorization support vector machines learning relevant features ls report thorsten joachims dortmund 
november revised 
april dortmund fachbereich informatik university dortmund computer science department des viii ki fachbereich informatik der universitt dortmund research reports unit 
viii ai computer science department university dortmund issn dortmund fachbereich informatik lehrstuhl viii dortmund issn requests university dortmund fachbereich informatik lehrstuhl viii dortmund mail reports ls informatik uni de ftp ftp ai uni de pub www www ai informatik uni dortmund de ls reports html text categorization support vector machines learning relevant features ls report thorsten joachims dortmund 
november revised 
april universitt dortmund fachbereich informatik explores support vector machines svms learning text classi examples 
analyzes particular properties learning text data identifies svms appropriate task 
empirical results support theoretical findings 
svms achieve substantial improvements currently best performing methods behave robustly variety different learning tasks 
furthermore fully automatic eliminating need manual parameter tuning 
rapid growth online information text categorization key techniques handling organizing text data 
text categorization classify news stories hayes weinstein masand find interesting information www lang shoham guide users search hypertext joachims 
building text classifiers hand difficult time consuming desirable learn classifiers examples 
explore identify benefits support vector machines svms text categorization 
svms new learning method introduced vapnik vapnik cortes vapnik boser 
founded terms computational learning theory open theoretical understanding analysis 
reviewing standard feature vector representation text section identify particular properties text representation section 
argue support vector machines suited learning setting 
empirical results section support claim 
compared state art methods svms show substantial performance gains 
contrast conventional text classification methods svms prove robust eliminating need expensive parameter tuning 
text classification goal text categorization classification documents fixed number predefined categories 
document multiple exactly category 
machine learning objective learn classifiers examples category assignments automatically 
supervised learning problem 
facilitate effective efficient learning category treated separate binary classification problem 
problem answers question document assigned particular category 
representing text representation problem strong impact generalization accuracy learning system 
documents typically strings characters transformed representation suitable learning algorithm classification task 
ir research suggests word stems representation units ordering document minor importance tasks 
word stem derived occurrence form word removing case information porter 
example computes computing computer mapped stem comput 
terms word word stem synonymously 
leads attribute value representation text 
distinct word wi corre sponds feature tf wi number times word wi occurs document value 
shows example feature vector particular document 
text classification baseball specs graphics hockey car clinton unix space compute representing text feature vector 
avoid unnecessarily large feature vectors words considered features oc cur training data times words 
basic representation known scaling dimensions feature vector inverse document frequency idf wi salton leads improved performance 
idf wi calculated document frequency df wi number documents word wi occurs 
total number training documents 
intuitively inverse document frequency word low occurs documents highest word occurs 
different document lengths document feature vector normalized unit length 
feature selection text categorization usually confronted feature spaces containing dimensions exceeding number available training examples 
noted need feature selection conventional learning meth ods possible improve generalization accuracy avoid overfitting yang pedersen 
popular approach feature selection select subset available fea tures methods df thresholding yang pedersen test term strength criterion yang wilbur 
commonly effective yang pedersen method selecting features information gain criterion 
setup yang pedersen 
words ranked information gain 
select subset features words highest mutual information chosen 
words ignored 
svms text categorization 
bayes random features ranked information learning best features 
svms text categorization 
find methods promising learning text classifiers find properties text 
high dimensional input space learning text classifiers deal features 
svms overfitting protection necessarily depend number features potential handle large feature spaces 
irrelevant features way avoid high dimensional input spaces assume features irrelevant 
feature selection tries determine 
unfortunately text categorization irrelevant features 
shows results experiment reuters cq category see section 
features ranked binary mutual information 
naive bayes classifier see trained features ranked 
results show features ranked lowest contain considerable information somewhat relevant 
classifier worst features performance better random 
features completely redundant leads conjecture classifier combine features learn dense concept feature selection hurt performance due loss information 
document vectors sparse document di corresponding document vector contains entries zero 
kivinen kivinen give theoretical empirical evidence mistake bound model additive algorithms similar inductive bias svms suited problems dense concepts sparse instances 
text categorization problems linearly separable ohsumed categories support vector machines linearly separable reuters see section tasks 
inseparability reuters categories due dubious documents containing just words blab blab blab body obvious misclassifications human indexers 
idea svms find linear polynomial rbf separators 
arguments give evidence svms perform text categorization 
support vector machines support vector machines structural risk minimization principle vapnik computational learning theory 
idea structural risk minimization find hypothesis guarantee lowest true error 
true error probability error unseen randomly selected test example 
upper bound connects true error hypothesis error training set complexity vapnik 
error ln ln bound holds probability 
denotes number training examples vc dimension vcdim vapnik property hypothesis space indicates expressiveness 
equation reflects known trade complexity hypothesis space training error 
simple hypothesis space small vcdim probably contain approximating functions lead high training true error 
hand rich hypothesis space high vcdim lead small training error second term right hand side large 
situation commonly called overfitting 
conclude crucial pick hypothesis space right complexity 
structural risk minimization done defining structure hypothesis spaces hi respective vc dimension di increases 
ch chc goal find index minimum 
build structure increasing vcdim 
learn linear threshold functions type sign building structure number features feature selection strategy support vector machines uses refined structure acknowledges fact features text categorization relevant 
remember linear threshold functions features vcdim 
qq support vector machines find hyperplane separates positive negative training examples maximum margin 
examples closest hyperplane called support vectors marked circles 
lemma 
vapnik consider hyperplanes sign 
hypotheses 
example vectors contained ball radius required examples 
set hyperplane vcdim bounded please note vcdim hyperplanes necessarily depend number features 
vcdim depends euclidean length weight vector 
means generalize high dimensional spaces hypothesis small weight vector 
basic form support vector machines find hyperplane separates training data shortest weight vector 
hyperplane separates positive negative training examples maximum margin 
illustrates 
finding hyperplane translated optimization problem minimize vi yi 
yi equals document di class 
constraints require training examples classified correctly 
lemma draw vcdim structure element separating hyperplane comes 
bound similar shawe taylor gives bound true error hyperplane classification task 
optimization problem difficult handle numerically lagrange multipliers translate problem equivalent quadratic optimization problem vapnik 
minimize ai support vector machines iyi vi kind optimization problem efficient algorithms exist guaranteed find global optimum 
result optimization process set coefficients minimum 
coefficients construct hyperplane fulfilling 
equation shows resulting weight vector hyperplane constructed linear combination training examples 
examples contribute coefficient greater zero 
vectors called support vectors 
support vectors marked circles 
training examples minimum distance hyperplane 
calculate arbitrary support vectors class 
non linear hypothesis spaces learn nonlinear hypotheses svms convolution functions 
de pending type convolution function svms learn polynomial classifiers radial basis function rbf classifiers layer sigmoid neural nets 
cpo le ci la ki oid tanh convolution functions satisfy mercer theorem see vapnik 
means compute inner product vectors mapped new feature space non linear mapping convolution function simply substitute occurrence inner product equations desired convolution function 
support vector machine finds hyperplane non linear feature space separates training data widest margin 
finding best parameter values convolution functions parameters introduced 
polynomial convolution degree rbfs variance pick appropriate values parameters automatically 
procedure vapnik experiments refined version algorithm osuna 
efficiently handle problems support vectors converges fast minimal memory requirements 
non separable problems inspired bound 
train support vector machine different values 
estimate vcdim hypotheses pick lowest vcdim 
compute length weight vector formula document vectors normalized unit length easy show radius ball containing training examples tightly bound polynomial af please note procedure selecting appropriate parameter values fully automatic look test data requires expensive cross validation 
non separable problems far assumed training data separable error 
possible chosen hypothesis space 
cortes vapnik cortes vapnik suggest slack variables 
simpler approach taken 
optimization values coefficients monitored 
training examples high contribute lot inseparability data 
value ai exceeds certain threshold ai corresponding training example removed training set 
svm trained remaining data 
conventional learning methods compares support vector machines standard methods shown results text categorization problems previous studies 
method represents different machine learning approach density estimation naive bayes classifier rocchio algorithm popular learning method information retrieval instance nearest neighbor classifier decision tree rule learner 
naive bayes classifier idea naive bayes classifier probabilistic model text 
estimation parameters model possible strong assumptions incorporated 
word unigram models text words assumed occur independently words document 
goal estimate pr ld probability document class 
perfect knowledge pr ld optimum performance achieved assigned class iff pr ld bayes rule 
unigram model text leads estimate pr ld see joachims pt 
pr wd tf wd pr pr 
hi pr wi pr hi pr wi experiments probabilities estimated fraction documents respective category 
pr wil pr wil called laplace estimator joachims 
rocchio algorithm type classifier relevance feedback algorithm originally proposed rocchio rocchio vector space retrieval model salton 
extensively text classification 
normalized document vectors positive examples negative examples summed 
linear component decision rule computed fi rocchio requires negative elements vector set 
fi parameter adjusts relative impact positive negative training examples 
performance resulting classifier strongly depends choice fl 
classify new document cosine computed 
appropriate threshold cosine leads binary classification rule 
nearest neighbors nearest neighbor nn classifiers show performance text categorization tasks yang 
follows setup yang 
cosine similarity metric 
knn denotes indexes documents highest cosine document classify yi cos cos di details mitchell 
decision tree classifier quinlan decision tree algorithm experiments 
popular decision tree algorithm shown results variety problem 
default parameter settings rule post pruning turned 
outputs confidence value classifying new examples 
value compute precision recall tables see section 
previous results decision tree rule learning algorithms reported lewis ringuette moulinier 
experiments experiments compare performance svms polynomial rbf convolution operators conventional learning methods 
tes collections test collections empirical evaluation done test collection 
reuters dataset www research att com lewis reuters html compiled david lewis originally collected carnegie group reuters newswire 
modapte split leading corpus training documents test documents 
potential topic categories training test example 
stemming word removal training corpus contains distinct terms occur documents 
reuters collection know direct correspondence words categories 
category wheat example occurrence word wheat document predictor 
second test collection taken ohsumed corpus ftp edu pub ohsumed compiled william hersh 
connection words categories direct 
documents abstracts training second testing 
classification task considered assign documents multiple categories mesh diseases categories 
document belongs category indexed indexing term category 
stemming word removal training corpus contains distinct terms occur documents 
performance measures despite theoretical problems certain arbitrariness precision recall breakeven point measure performance stay extend compatible previously published results 
precision recall breakeven point know statistics recall precision widely information retrieval 
apply binary classification problems 
precision probability document predicted class truly belongs class 
recall probability document belonging class classified class 
high recall high precision exists trade 
methods examined category assignments thresholding confidence value 
adjusting threshold achieve different levels recall precision 
prr method raghavan interpolation 
precision recall defined binary classification tasks results multiple binary tasks need averaged get single performance value multiple class problems 
done microaveraging yang 
setting results procedure 
classification threshold lowered simultaneously binary tasks 
value microaveraged precision recall computed merged contingency table 
arrive merged table contingency tables binary tasks added componentwise 
precision recall breakeven point defined value precision recall equal 
note may multiple breakeven points 
case multiple breakeven points lowest selected 
case breakeven cosine similarities comparable classes method proportional assignment wiener rocchio algorithm come improved confidence values 
experiments svm poly svm rbf bayes rocchio nn earn acq money fx grain crude trade interest ship wheat corn 

combined combined precision breakeven point frequent reuters categories microaveraged performance reuters categories 
nn rocchio achieve highest performance features nn fi rocchio 
naive bayes performs best features 
point defined zero 
results figures show results reuters ohsumed corpus 
sure results conventional methods biased inappropriate choice parameters extensive experimentation done 
methods run selecting best best best best best features see section 
number features values fi rocchio algorithm nn classifier tried 
results parameters best performance test set reported 
reuters data nn classifier performs best conventional methods see 
replicates findings yang 
slightly worse perform decision tree method rocchio algorithm 
naive bayes classifier shows worst results 
compared conventional methods svms perform better independent choice parameters 
complex hypotheses spaces polynomials degree overfitting occurs despite features 
demonstrates ability svms handle large feature spaces feature selection 
numbers printed bold mark parameter setting lowest vcdim estimate described section 
results show strategy suited pick parameter setting automatically 
computing microaveraged precision breakeven point hypotheses lowest vcdim class leads performance polynomials radial basis functions 
substantial improvement best performing conventional method best parameter setting 
rbf results reuters corpus revised 
experiments earlier version report articles marked parsed way body ignored 
svm poly svm rbf bayes rocchio nn pathology cardiovascular system 
combined combined precision breakeven point frequent ohsumed categories microaveraged performance ohsumed categories 
nn rocchio bayes achieve highest performance features nn fl rocchio 
performs best features 
support vector machine better nn categories ties significant improvement binomial sign test 
results ohsumed collection similar 
nn best conventional method 
fails task heavy overfitting observed features 
svms perform substantially better methods 
rbf support vector machine outperforms nn categories significant improvement 
reuters ohsumed collection rbf convolution performs slightly better polynomial convolution 
comparing training time svms roughly comparable expensive naive bayes rocchio nn 
current research improve efficiency svm type quadratic programming problems 
svms faster nn classification time especially reduced set burges method 
introduces support vector machines text categorization 
provides theoretical empirical evidence svms suited text categorization 
theoretical analysis concludes svms acknowledge particular properties text high dimensional feature spaces features relevant dense concept vector sparse instance vectors 
experimental results show svms consistently achieve performance categorization tasks outperforming existing methods substantially significantly 
ability generalize high dimensional feature spaces svms eliminate need feature selection making application text categorization considerably easier 
advantage svms conventional methods robustness 
svms show performance experiments avoiding catastrophic failure observed conventional methods tasks 
furthermore svms require parameter tuning find parameter settings automatically 
svms promising easy method learning text classifiers examples 
advisor prof morik ralf klinkenberg marc craven comments 
ken lang providing code 
balabanovic shoham balabanovic shoham 

learning infor mation retrieval agents experiments automated web browsing 
working notes aaai spring symposium series information gathering distributed heterogeneous environments 
aaai press 
boser boser guyon vapnik 

training algorithm optimal margin classifiers 
conference computational learning theory colt pages 
burges sch kopf burges sch kopf 

improving accu racy speed support vector machines 
neural information processing systems volume 
cortes vapnik cortes vapnik 

support vector networks 
machine learning 
hayes weinstein hayes weinstein 

construe tis system content indexing database news stories 
annual conference innovative applications ai 
joachims joachims 

probabilistic analysis rocchio algorithm tfidf text categorization 
international conference machine learning icml 
joachims joachims freitag mitchell 

webwatcher tour guide world wide web 
international joint conference artificial intelligence 
kivinen kivinen warmuth auer 

algorithm vs winnow linear vs logarithmic mistake bounds input variables relevant 
conference computational learning theory 
lang lang 

newsweeder learning filter netnews 
international conference machine learning icml 
lewis ringuette lewis ringuette 

comparison learning algorithms text classification 
third annual symposium document analysis information retrieval pages 
masand masand waltz 

classifying news stories memory reasoning 
international acm sigir conference research development information retrieval pages 
mitchell mitchell 

machine learning 
mcgraw hill 
moulinier moulinier ganascia 

text cate symbolic approach 
annual symposium document analysis information retrieval sdair 
osuna osuna freund girosi 

improved training algorithm support vector machines 
ieee workshop neural networks signal processing 
porter porter 

algorithm suffix stripping 
program automated library information systems 
quinlan quinlan 

cd programs machine learning 
morgan kaufmann 
raghavan raghavan jung 

critical investigation recall precision measures retrieval system performance 
acm transactions information systems 
rocchio rocchio 

relevance feedback information retrieval 
salton editor smart retrieval system experiments automatic document processing pages 
prentice hm salton salton 

developments automatic text retrieval 
science 
salton buckley salton buckley 

term weighting approaches automatic text retrieval 
information processing management 
hull pedersen 

comparison classifiers document representations routing problem 
international acm sigir conference research development information retrieval 
shawe taylor shawe taylor bartlett williamson anthony 

structural risk minimization data dependent hierarchies 
technical report nc tr neurocolt 
vapnik vapnik 

estimation dependencies empirical data 
springer series statistics 
springer verlag 
vapnik vapnik 

nature statistical learning theory 
springer new york 
wiener wiener pedersen weigend 

neural net approach topic spotting 
annual symposium document analysis information retrieval sdair 
yang yang 

evaluation statistical approaches text tion 
technical report cmu cs carnegie mellon university 
yang pedersen yang pedersen 

comparative study fea ture selection text categorization 
international conference machine learning icml 
yang wilbur yang wilbur 

corpus statistics re move redundant words text categorization 
journal american society information science 
