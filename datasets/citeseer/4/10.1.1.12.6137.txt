technical report tr university british columbia bayesian latent semantic analysis multimedia databases de freitas barnard october dept computer science university british columbia main mall vancouver canada uc berkeley computer science division soda hall berkeley ca usa bayesian mixture model probabilistic latent semantic analysis documents images text 
bayesian perspective allows perform automatic regularisation obtain sparser coherent clustering models 
enables encode priori knowledge word image preferences 
learnt model browsing digital databases information retrieval image text queries image annotation adding words image text illustration adding images text 
probabilistic latent semantic analysis plsa models analysis text hofmann text images barnard forsyth led exciting developments computer vision image understanding multimedia database browsing document retrieval :10.1.1.130.299
extend proposed models mixture model adopting bayesian paradigm allowing categorical continuous variables 
restrict simple mixture model tutorial purposes 
bayesian treatment aspect hierarchical models de freitas barnard 
paradigm allows encode preferences application speci words images 
allows perform regularisation principle ockham razor 
models describe data reasonably simpler model automatically chosen 
model speci cation nx denote collection documents heterogeneous database document assumed di erent attributes na 
attributes may categorical continuous 
examples categorical attributes include standard document meta data movie ratings media type word presence document contain speci word 
features derived images multimedia signals typically continuous valued attributes 
noted database non relational relations treated 
general goal analysis group documents homogeneous classes probabilistic way 
accomplish document assumed drawn nite mixture model iid nc nc na encompasses model parameters denotes mixing weights denotes parameters mixture component densities denotes number components 
mixture model de ned standard probability simplex nc 
generating document involves steps 
select notation boldface roman letters refer collections items italic roman letters refer individual items 
example single word written group nw words written nw simplicity denote random variable realisation 
mixture component probability sample document mixture component 
modelling inference perspective convenient introduce latent allocation variables indicate particular document belongs speci group indicator variables fz correspond sample distribution independently express mixture model di erent form useful subsequent derivations cj nc na belongs group 
note equation allows recover expression equation 
learning problem consideration known supervised learning allocation variables known unsupervised learning 
modelling assumptions likelihood function entire data set xj nx nc na nc nc cnx nx na subsections describe various forms likelihood take depending type attributes 
continuous attributes algorithms developed section apply type mixture component density belonging exponential family 
concentrate gaussian multinomial densities 
gaussian attributes arise treat various image features colour texture samples gaussian distributions barnard forsyth belongie carson greenspan malik :10.1.1.130.299
instance ga dimensional vector drawn normal distribution nga density exp 
parameter vector contains means covariances 
example mixture model gaussian attributes represent follows iid nc na nga mixture gaussian products expressed mixture multivariate gaussians size ga block diagonal covariance 
general structure covariance depends partition gaussian attributes 
dimension vectors increases number covariance parameters estimate 
suggested hunt jorgensen start diagonal covariance assume independence estimate parameters assign observations clusters study cluster correlation matrices 
variables highly correlated may grouped subvector estimation procedure repeated 
categorical attributes assume th attribute take discrete values precisely probability th value occurring de ned jj 
model distribution variable multinomial model nv categorical case parameter vector nv 
note need parameters normalisation condition 
mixture categorical variables expressed iid nc na nv categorical variables assume independent nv nv allow take account correlations priori nv nv course introduces extra parameters poses trade discussed context gaussian attributes section 
categorical variables abound text multimedia applications 
information ltering categorical variables model document features ratings people movie discussed document 
indicator variables particular object appear document 
text mining tasks typically arising language modelling machine translation information retrieval categorical variables model frequently word appears document beeferman berger la erty hofmann puzicha nigam mccallum thrun mitchell 
similarly extend analysis model documents hypertext links world wide web cohn hofmann :10.1.1.33.6843
text scenario assume data available occurrence table word counts denotes number times word appears document computational simplicity model documents adopting standard bag words model 
ignore word order contextual information 
standard naive bayes assumption cluster word assumed independent remaining words 
cluster variable models correlation words 
assumptions text document distributed iid nc na case denotes number words vocabulary dictionary denotes total number words document denotes probability word cluster 
notational simplicity ignored normalisation factor multinomial density assume document length class independent 
extend text model incorporating links table word counts cohn hofmann :10.1.1.33.6843
information great relevance design search engines brin page kleinberg 
cohn hofmann text links weighted heuristic constants :10.1.1.33.6843
bayesian framework weighting performed prior automatically computed data 
mixed attributes natural combine categorical continuous attributes multimedia applications 
may cluster documents words images combining multinomial model words gaussian model image features barnard forsyth iid nc aw aw nag ag ag exp ag ag ag ag ag case aw ag denotes total number attributes :10.1.1.130.299
note gaussian attributes di erent dimension 
note particular attribute correspond mixture distribution 
example conditionally gaussian attributes known location scale models chang hunt jorgensen tate model cluster associations discrete variable continuous variables 
instance ga dimensional vector rst ga entries drawn normal distribution conditionally categorical variable entry 
mathematical terms nga 
categorical variable assumed distributed 
prior speci cation bayesian extension maximum likelihood approach justi ed points 
ill conditioning maximum likelihood framework likelihood unbounded 
example dealing mixtures gaussians prevents mixture component density assigned single observation 
happens variance goes zero likelihood goes nity causing serious ill conditioning problems 
circumvent common problem people prune components hand add extra tuning parameters ridge regression marquardt 
bayesian perspective prior reduces problem 

priori knowledge prior specify domain speci knowledge rules derived expert subjective preferences favouring simpler models 

regularisation data set nite noisy needs take care tting data 
show prior distribution favour simpler smooth models avoid tting noise extrapolate reasonable 

multiple overlapping copies clusters ml estimation splits underlying category components identical parameters component weights add correct 
bayesian estimation avoids problem specifying priors favour sparse models 

starting point sophisticated modelling bayesian perspective lays groundwork sophisticated models enable example achieve robustness respect speci cation prior distributions parameter tuning perform model selection extend point estimators average estimators consider di erent loss functions principled way see example andrieu de freitas doucet bernardo smith stephens 
follow hierarchical bayesian strategy unknown parameters allocation variables regarded drawn appropriate prior distributions 
acknowledge uncertainty exact form prior specifying terms unknown parameters hyperparameters 
hyperparameters turn assumed drawn appropriate 
idea hierarchical approach increasing levels inference higher level priors increasingly di 
avoid having specify parameters obtain results independent parameter tuning 
directed probabilistic graphical model lsa model 
shown hierarchical bayesian model levels inference level xj nx level ii zj nx level iii case mixtures gaussian discrete components parameters hyperparameters 
discuss choice prior hyperprior models subsections 
priors mixing variables categorical parameters allocation variables assumed drawn multinomial distribution nc admits density nc place conjugate dirichlet prior mixing coecients nc having density 
nc nc 
denotes gamma function similarly place dirichlet prior distribution assume priors independent nc na 
nv nv example text mining application place single dirichlet prior word probabilities follows nc na na limit computational storage cost reasonable set hyperparameters value nv priors gaussian parameters considering multivariate normal distributions adopt normal inverse wishart prior celeux raftery robert diebolt robert mclachlan peel nga 
nga 
nga 
denotes wishart distribution density jr 
ra nga exp tr 
ra nga nga nga 
ra nga expressions 
symmetric positive de nite ga ga matrix regularisation parameters ga univariate case wishart distribution reduces gamma distribution ga ra lastly normal components handled simply imposing condition hyperparameters regularisation empirical bayes model selection marginal posterior probability parameters zjx marginal likelihood xj important distributions arising bayesian statistics 
terms integrals notational simplicity introduced parameter vector jx jx xj xj marginal posterior needed computing parameters probability particular word cluster document corpus 
marginal likelihood plays fundamental role model selection 
particular comparing model hypotheses need compute ratio marginal likelihoods known bayes factor xj jh xj jh intuitively ratio provides measure data increased decreased odds model respect 
case di erent model hypothesis correspond di erent sets hyperparameters 
notice marginals related bayes rule xj xj jx xj likelihood hyperparameter inference level normalising distribution evidence parameter inference level 
rigorous bayesian analysis involve specifying priors hyperparameters 
require develop computationally demanding estimation algorithms variational methods markov chain monte carlo simulation 
opt pragmatic solutions 
choose hyperparameters priori preferences data estimate point estimate hyperparameters 
case aim nd maximises xj 
trying nd model hypothesis 
approach estimating priors data empirical bayes method known maximum likelihood type ii carlin louis 
assumption jx fairly sharply peaked mode consequently approximations jx valid 
describe em algorithm carrying maximisation marginal likelihood section 
bayes factor model selection approach inherent ockham factor regulariser 
hypotheses explain data reasonably simpler hypotheses preferred prior probability favour simple models gull je berger mackay 
illustrate consider situation decide mp df df df df df parameter model parameter model comparing models uniform priors 
analysis shows bayesian approach favour simpler model long explains data 
parameter parameter model 
choose uniform priors jh jh shown 
assume probable point mp posteriors approximated mp jx mp jx follows bayes rule marginal likelihood model xjh xj mp mp jh 
consequently bayes factor xjh xjh xj mp xj mp jh 
jh 
xj mp xj mp 
see bayes factor ratio likelihoods times factor favours simpler model 
start uniform distributions parameters bayesian approach favour simpler hypothesis long explains data 
mixture setting initially guess components required bayesian method penalises extra components 
take argument considering possibility having nite number components priori 
mixture model element follows nc may interpreted probability distribution pr titterington smith makov 

denotes probability measure puts mass support point model expressed terms lebesgue stieltjes integral dg nite number components appropriately weighed prior 
notice sampling parameters simply sample prior distributions dirichlet process 
posterior distribution posterior distribution jx obtained multiplying prior likelihood distributions normalising product 
distribution solution inference problem bayesian statistics 
derive estimates interest mean mode high probability intervals 
data posterior factorised follows jx nx jx nx bayes rule rst term right hand side expanded terms likelihood prior jz nc na nc na nc na nc na result posterior distribution expressed general form jx nc na nc 
nx nc na nc na subsections derive expressions posterior speci cases categorical gaussian attributes 
posterior categorical attributes considering categorical attribute substitute likelihood equation prior equation equation obtain jx nc kc nc na nv nx denotes total number documents assigned class unnormalised posterior draws nc nc nc cluster attribute draws nv nv nv example text mining likelihood de ned terms equation prior equation jx nc kc nc na may draw na na na posterior gaussian attributes multiplying priors equation likelihood equation completing squares obtains conjugated posterior values nga nga 
new expressions gaussian mean wishart variance parameter nx nx computation parameters mixture model computed analytically knows mixture indicator variables 
result resort numerical methods 
section em algorithms baum petrie soules weiss dempster laird rubin compute ml map point estimates mixture model 
convergence rate em linear slow mixture components close simple easy program guaranteed converge monotonically local maximum likelihood function xj lindsay mclachlan peel redner walker 
particular design em algorithms straightforward family densities 
possesses sucient statistic xed dimension parameters 
case xj factored terms xj jt independent kernel density jt depends sucient statistics 
distributions interest exponential families property 
lastly maximum likelihood type ii algorithm estimate hyperparameters 
maximum likelihood estimation em algorithm initialisation em algorithm ml estimation iterates steps 

step compute expected value complete log likelihood function respect distribution allocation variables ml zjx old log xj old refers value parameters previous time step 

step perform maximisation new arg max ml ml function expanded follows see appendix ml nx nc cjx old log na step compute cjx old easily accomplished follows cjx old old nc old na nc na step model single categorical variable th attribute involves computing nv nc nv corresponding step requires maximise ml subject constraints nc nv 
appendix shows lagrange multipliers accomplish goal 
resulting expressions nx nx nx text mining example ml ml nx nc cjx old log na constraint na em derivation text documents yields na nc na nx nx nx gaussian attributes step straightforward application equation 
hard show maximising ml function constraint probabilities add obtains update equations step mclachlan peel chapter nx nx nx case components update estimate common covariance nc equation 
remaining estimates stay case 
maximum posteriori estimation em algorithm em formulation map estimation straightforward 
simply augment objective function step ml adding log prior densities 
map objective function map zjx old log zjx old log xj log ml log done appendix 
resulting expressions model single discrete attribute nx nx nx particular case text nx nx expressions derived considering posterior modes mode jz mode jz mode jz replacing cluster indicator variable posterior expectation 
ml updates follow setting dirichlet hyper parameters uniform prior 
similarly gaussian case compute map estimates maximising log posterior obtain ga 

nx adopting uninformative priors ga 
map estimates simplify estimates equation 
estimating hyperparameters appendix derive step em algorithm maximise jx case discrete distributions 
works maximising expected marginal log likelihood zjx log xj zjx log xj results set equations needs iterated order increase lower bound marginal likelihood new nx nc nc new nx nv nx nv log digamma function 
possible derive faster newton raphson schemes shown appendix take care algorithm numerically unstable 
lastly possible derive estimators compute hyperparameters gaussian components discussed chen gelman carlin stern rubin 
applications classi cation information retrieval important facility image text databases world wide web retrieval user queries 
wish support queries text image features categorical variables combinations depicted 
queries search engine tiger www typical queries search results multimedia databases may want retrieve documents images text cartoons 
soft sense combinations items taken consideration documents item considered 
queries easily speci ed images 
mathematically problem probabilistic classi cation new observation query populations reduces derivation cjx nx jz cjx nx nc nx jz jx nx computing probability query belongs cluster note rst term numerator results marginalisation jz jz jz browsing data visualisation data mining mixture models suitable studying coherence groups 
allow visualise data identify hidden patterns 
feature great bene browsing image databases 
typically setting image databases content easy internalize navigate dicult normally involves human input 
goals automate task 
key issue browsing clusters sense user 
user nds clusters coherent internalize kind structure represent 
furthermore small portion cluster represent accurately suggest kinds pictures exploring cluster 
data compression clustering methods allow obtain lower dimensional representation items database 
may reduce storage requirements increase eciency retrieval systems 
general easier search structured lower dimensional space clusters search items database 
annotation illustration recognition framework build application takes text selected document suggests images go text barnard forsyth :10.1.1.130.299
application essentially process linking pictures words 
clear symmetry just easily go way link words pictures 
auto annotate process interesting number reasons 
image produce reasonable words image feature existing text search infrastructure broaden searches con nes system 
example consider image search user sketch 
sketch contains orange ball upper right corner annotation model return words sun sunset 
words turn search images match sketch text search engine 
association text images interesting computer vision perspective form minimally supervised learning semantic labels image features barnard forsyth :10.1.1.130.299
shown goal recognition label image segments reasonably 
extreme accuracy task clearly thinking argue doing signi cantly better chance useful tiger water grass river river grass tiger water paws annotation left recognition right goals 
care bootstrap machine recognition 
doing signi cantly better chance general task indicates system learnt correspondences image components words 
means system learnt minimal supervision recognition 
turn interesting face key vision problem approach general recognition 
systems built relatively ective recognizing speci things usually speci circumstances 
doing tasks generally required lot high quality training data 
lot data available nature 
shortage image data text especially includes video 
information required contained data sets looking recognition problem way bear fruit 
experiments section synthetic examples illustrate behaviour various algorithms 
proceed assess performance algorithms corel annotated image database 
synthetic experiments text example possible create arti cial documents multinomial model generate 
way know exact cluster word probabilities assess performance various learning techniques 
rst experiment randomly generated documents di erent words allowing repetition clusters 
clusters selected probabilities 
tried estimate parameters generating model ml map empirical bayes eb approaches 
algorithms number clusters assumed number em iterations set 
map case set 
eb case choice parameters done automatically 
shows ml map eb cluster labels cluster probabilities simple text example 
map eb approaches recover true cluster probabilities 
cluster probabilities computed method 
clearly map eb approaches recover generating probabilities greedy maximum likelihood approach fails accomplish 
common lsa lsi literature prune negligible clusters heuristic procedures 
ad hoc methods play substantial role results 
great advantage bayesian approach problem large extent 
varies threshold values measures clusters active bayesian methods perform better shown 
gure shows pruning ml context sensitive value chosen threshold 
assess performance algorithms simple retrieval task 
docu effective number clusters threshold ml map eb ective number clusters 
map eb approaches fairly insensitive cluster pruning 
true ml technique 
ments generated word probabilities true cluster probabilities 
em maximisation queried models generated algorithms document training set test set single word 
queries results shown table 
queries originate cluster eb method consistent 
ability assigning documents training test set generated cluster estimated cluster measure performance 
repeated experiment times compute number times models failed assign test training set documents cluster 
gain insight variance test repeated times 
results obtained shown table 
con rm bayesian approaches reliable 
problems ml approach creates multiple repetitions mixture component mixture weights repetitions add closely true mixture weight 
bayesian schemes hand place irrelevant mixture components regions low probability 
illustrated means example query ml map eb table table shows probabilities cluster membership document training set top document test set middle single word 
documents belong cluster 
eb map methods outperform ml approach 
ml map eb mean variance mean variance table number times algorithms fail assign documents training test set originated cluster generating model cluster learnt models 
table shows results choices mixing proportions generative model 
involving gaussian distributions text section 
text arbitrary gaussian features demo example cluster documents attributes text words vocabulary univariate gaussian bivariate gaussian 
documents generated clusters probability 
word probabilities previous experiment set generating gaussians dimensional case 
dimensional case set gaussian clusters 
hypothesized number clusters chosen number em iterations xed 
chose discrete prior parameters selected gaussian hyperparameters follows 
copied values ensure estimates relatively insensitive reasonable changes prior 
eb method experiment applied estimate hyperparameters shows contour plot bivariate gaussian clusters means clusters computed algorithms 
note bayesian approaches place mixture components region low probability 
hand ml gaussian attribute dim dim data data ml means map means eb means contour plots bivariate attribute second synthetic example 
gure shows means cluster components ml map eb approaches 
approach places mixture components location 
works mixture components weights add right value 
wasteful unsatisfactory 
large image text database performed experiments corel image database 
database contains images annotated approximately keywords 
images cds provided corel samples particular theme 
database suitable testing algorithms 
clustered documents corresponding image keywords di erent sets attributes text image features combined text images 
image features derived image histograms 
speci cally image histogram projected lower dimensional space histograms 
space pca space image histograms larger set images see blobworld belongie details note example entire image treated blob histogram features pca components entire image 
possible hierarchical models treat image mixture blobs barnard forsyth :10.1.1.130.299
map eb approaches lead parsimonious representations testing scenarios example clustering contents rst cds database documents assuming number clusters algorithms yielded cluster probabilities shown 
clearly ml map eb cluster labels cluster probabilities corel example 
bayesian schemes nd number themes accordance human corel choice 
addition clusters coherent 
advantages combining image text attributes model people relate images semantic visual content 
example want pictures tigers light green search word tiger light picture 
hopefully return images tigers dark places 
shows example clustering images cds containing tigers text image features results separate coherent clusters 
dicult results format available cs berkeley edu papers clustering bayesian index html 
result clustering documents images keywords 
example clusters obvious text image semantics 
text level groups relate word tiger image level tigers light green tigers dark places 
results query trees river sky mountain results text query conjunction words appear database 
model generalises reasonable extent 
performed retrieval experiments learnt models 
shows query consisting conjunction words appear database able retrieve images clear models generalising reasonably 
query results results image query 
results query trees river sky mountain results image query added text 
model returns images sunsets time trees 
figures show querying system works adding words image bias results useful ways 
shown improve probabilistic semantic modelling adopting bayesian approach 
particular able obtain parsimonious coherent models 
currently exploring modelling algorithmic application validation extensions 
david cohn arnaud doucet david forsyth jan puzicha stuart russell 
notation speci meanings number attributes 
number clusters 
number documents 
number discrete values 
number times word appears document number words document mixture coecients 
mixture indicator variables 
parameters multinomial components 
mean gaussian components 
covariance gaussian components 
symbols stacked vector vector th component missing entry matrix th row th column 
dimensional matrix size identity matrix dimension matrix ones dimension matrix zeros dimension euclidean dimensional space 
dz distribution density conditional distribution dz dy joint distribution dz distributed dz 
operators functions transpose matrix inverse matrix tr trace matrix jaj determinant matrix diag matrix entries diagonal 
indicator function set 
dz dirac delta function impulse function 
expectation random variable var variance random variable exp 
exponential function 
gamma function 
digamma function 
log 
logarithmic function base ln 
min max extrema respect integer value 
inf sup extrema respect real value 
arg min argument minimises operand 
arg max argument maximises operand 
standard probability distributions bernoulli br zj beta zj multinomial dirichlet zj gaussian zj exp wishart jr 
exp tr 
gamma ga zj exp inverse gamma ig zj exp poisson pn zj 
exp uniform ua dz expanding ml function ml function expanded follows ml zjx old log nx cj zjx old log nx nc na zjx old nx nc log na nc nc cnx nx nc log na nx jx old nx nc log na nc nc cnx nx jx old expression greatly simpli ed noticing nc nc cnx nx jx old nc nc nc nc cnx nx jx old cjx old nx nc jx old cjx old cjx old consequently ml function simpli es ml nx nc cjx old log na step categorical attributes ml estimates step categorical variables need maximise ml subject constraints nc nv 
compute introduce lagrange multipliers maximise lagrangian ml nc di erentiating respect equating zero 
want compute nx nc cjx old log na nc nx cjx old summing sides get estimate cjx old nx mixture components discrete need compute nx nc cjx old log na nv nv nx cjx old summing sides get nx cjx old estimate nx nx map estimates unconstrained objective function map ml log compute proceed previous section di erentiating augmented lagrangian respect equating zero nx nc cjx old log na nc log nc nc nc nx cjx old summing sides get estimate nx similarly constrained maximisation requires compute nx nc cjx old log na nv nv log nc na nv nv nv nx cjx old yielding nx nx step categorical hyperparameters compute hyperparameters maximise expected marginal log likelihood follows zjx old log xj zjx old log xj instance compute maximising zjx old log nx nc na nc nc nc zjx old log nc nc nc kc zjx old log nc nc nc nc zjx old log nc nc nc log nc nc nc nx log nc log nc nc log nx log derivative respect yields nc nc nx log digamma function 
may solve xed point equations iterative updates minka new nx nc nc possible employ newton raphson schemes narayanan 
requires compute hessian matrix follows nc nc nc nc diag nx 
function second derivative 
newton raphson algorithm proceeds follows new old eciency expanded terms matrix inversion lemma 
similar estimate obtained rst de ning objective function log nx nc na nv nc na nv nv nv log nc na nv nv nv nx nc na log nv nv nx nv nx derivative respect yields nv nv nx nx corresponding hessian matrix nc nc nv nv nx diag nx nx xed point updates follow previous case 
andrieu de freitas doucet 

robust full bayesian learning radial basis networks appear neural computation 
barnard forsyth 

learning semantics words pictures appear iccv 
baum petrie soules weiss 

maximization technique occurring statistical analysis probabilistic functions markov chains annals mathematical statistics 
beeferman berger la erty 

statistical models text segmentation machine learning special issue natural language learning 
belongie carson greenspan malik 

color texture image segmentation em application content image retrieval international conference computer vision pp 

celeux raftery robert 

inference model cluster analysis statistics computing 
bernardo smith 

bayesian theory wiley series applied probability statistics 
brin page 

anatomy large scale hypertextual web search engine th international world wide web conference 
carlin louis 

bayes empirical bayes methods data analysis second edn chapman hall 
chang 

classi cation dichotomous continuous variables journal american statistical association 
chen 

bayesian inference normal dispersion matrix application stochastic multiple regression analysis journal royal statistical society 
series 
cohn hofmann 

missing link probabilistic model document content hypertext connectivity leen dietterich tresp eds advances neural information processing systems vol 

de freitas barnard 

bayesian latent semantic analysis review computer science division uc berkeley 
dempster laird rubin 

maximum likelihood incomplete data em algorithm journal royal statistical society series 
diebolt robert 

estimation nite mixture distributions bayesian sampling journal royal statistical society 
gelman carlin stern rubin 

bayesian data analysis chapman hall 


thinking foundations probability applications minnesota press minneapolis 
gull 

bayesian inductive inference maximum entropy erickson smith eds maximum entropy bayesian methods science engineering kluwer academic publishers pp 

hofmann 

probabilistic latent semantic analysis uncertainty arti cial intelligence 
hofmann puzicha 

unsupervised learning dyadic data technical report tr international computer science institute 
hunt jorgensen 

mixture model clustering brief program australian new zealand journal statistics 
je berger 

ockham razor bayesian analysis american scientist 
kleinberg 

authoritative sources hyperlinked environment annual acm siam symposium discrete algorithms vol 



distance populations mixed continuous categorical variables biometrika 
lindsay 

mixture models theory geometry applications nsf cbms regional conference series probability statistics alexandria virginia institute mathematical statistics american statistical association 
mackay 

bayesian interpolation neural computation 
marquardt 

ridge regression practice american statistician 
mclachlan peel 

finite mixture models wiley series probability statistics new york 
minka 

estimating dirichlet distribution unpublished 
department computer science mit 
narayanan 

maximum likelihood estimation parameters dirichlet distribution applied statistics 
nigam mccallum thrun mitchell 

text classi cation labeled unlabeled documents em appear machine learning 
tate 

multivariate correlation models mixed discrete continuous variables annals mathematical statistics 
redner walker 

mixture densities maximum likelihood em algorithm siam review 


maximum likelihood estimation dirichlet distributions journal statistical computation simulation 
stephens 

bayesian methods mixtures normal distributions phd thesis department statistics oxford university england 
titterington smith makov 

statistical analysis finite mixture distributions john wiley sons san diego 

