appear adaptive behavior incremental evolution complex general behavior gomez department computer sciences university texas austin austin tx cs utexas edu risto miikkulainen department computer sciences university texas austin austin tx risto cs utexas edu june researchers demonstrated complex action sequences learned neuro evolution evolving neural networks genetic algorithms 
complex general behavior evading predators avoiding obstacles tied specific environments turns difficult evolve 
system discovers mechanical strategies moving back forth help agent cope effective appear believable generalize new environments 
problem general strategy difficult evolution system discover directly 
proposes approach complex general behavior learned incrementally starting simpler behavior gradually making task challenging general 
task transitions implemented successive stages delta coding evolving modifications allows converged populations adapt new task 
method tested stochastic dynamic task prey capture compared direct evolution 
incremental approach evolves effective general behavior scale harder tasks 
neuro evolution ne training artificial neural networks genetic algorithms proven attractive alternative standard learning methods reinforcement environments nolfi colombetti dorigo yamauchi beer nolfi parisi 
ne strings values chromosomes representing neural network parameters connection weights thresholds connectivity recombined principle natural selection order find optimal network problem 
ne systems consist mobile agent static environment agent learn policy minimizes travel distance certain desirable states cliff law miikkulainen nolfi parisi 
goal states defined spatially unique coordinates easy assess individual doing comparing known optimal path actual path 
tasks require increasingly complex behavior type metric difficult define 
wall obstacle avoidance predator prey scenarios examples tasks describe general behavior tied specific spatial locations 
critical problem trying evolve complex behaviors strategies emerge mechanical 
strategies yield reasonable performance terms maximizing fitness measure exhibit kind intelligent responsiveness environment required ultimately accomplish objective 
competence task side effect mechanical strategy agent learn general task learns relatively primitive behavior 
example predator evolved capture prey may exhibit simple strategy moving back forth circle environment 
behavior easy evolve guarantees better fitness moving little population easily region solution space manifests strategy 
pathological region solution non mechanical general behavior required accomplish task different mechanical behavior 
complex behaviors difficult evolve primarily usually reactive response current state behaviors require memory ability predict states information previous states 
example prey capture prey goes outside sensory range agent remember saw prey 
terms neuro evolution means neural network architectures evolved recurrent harder deal 
presents method evolving complex general behavior incrementally 
evaluating population task course evolution evaluated relatively easy task increasingly harder tasks effort build goal behavior 
techniques allow enforced sub populations possible evolve neural networks memory delta coding possible adapt new task 
results incremental evolution article show possible achieve complex general behavior evolutionary neural networks 
prey capture prey capture task demonstrate incremental evolution special case class problems known pursuit evasion 
pursuit evasion contests consist environment containing minimum entities predator prey 
predator moves environment trying capture prey prey attempts avoid capture predator 
scenarios kind particularly interesting arena study adaptive behavior ubiquitous natural ecosystems offer relatively simple objective requires complex sensory motor coordination respect environment moving entity miller cliff 
accomplish prey capture task agent moving simulated environment able entity prey fixed number time steps 
agent detect prey limited distance 
prey moves sensor range agent remember detected 
prey capture simple description implementation demands level behavioral sophistication susceptible emergence mechanical strategies 
environment environment consists square spatially temporally discrete grid world 
agent prey occupy single grid space move directions fn wg time step 
agent considered captured prey occupy grid cell 
prey moves probabilistically tendency move away agent grows stronger agent gets closer see appendix definition enemy algorithm due lin 
prey moves speed set 
value probability prey action time step 
prey speed time 
note environment continuous speed task quite easy prey moving rate 
discrete world prey capture prey moving speed really moving speed agent part time 
environment provides non trivial task relatively high complexity 
agent learn move grid cell occupied prey mapping sensory input appropriate actions 
prey moves outside sensory range agent longer receives direct sensory stimulus prey rely short term memory decide action bring prey closer ultimately sensor range 
formally environment described sensory state machine ssm wilson fe agent sensory stimulus action time respectively mapping describes environmental dynamics agent perspective 
sensory stimulus sense vector action finite set sensory stimuli fe may result 
cardinality fe pair environment non deterministic 
sense vector denote unique global state agent sense entire environment total accuracy form memory required help agent predict prey located 
words uncertainty reduced keeping track previous sense action vectors fe gamma gamma gamma gamma environment deterministic contains finite number states exists cardinality fe possible pairs agent reliably predict fe prey capture environment particularly difficult prey actions partly random exist non determinism eliminated 
agent learn predict exactly reduce uncertainty memory 
agent representation agent controlled fully connected recurrent neural network sigmoidal units 
time step unit receives input input layer units 
allows agent maintain temporal information necessary performing task 
agent moves environment detect presence prey specified sensor range 
input unit assigned sectors sensory array 
prey area covered sensory array unit corresponding sector set 
additional unit set prey closer half sector 
units afford coarse encoding relative enemy position 
radial nature sensory apparatus gives greater sensitivity prey movement close range crucial 
units detect walls directions 
wall comes sensor range corresponding unit activated degree proportional wall distance agent 
output unit possible actions 
time step agent selects action corresponding unit highest activation 
representation provides agent sensory input imprecise limited range 
order perform prey capture agent learn associate sensory input appropriate pursuit actions exploit short term memory recurrent connections remember saw prey moves sensor range 
incremental evolution attacking complex general task prey capture head outset evolution process conventional evolutionary methods suffer inefficient performance 
task demanding wall detectors recurrent network environment input layer prey capture environment agent network 
grid world occupied agent prey 
sectors circles agent represent sensory array 
sectors divided levels proximity 
sector represented node input neural network controlling agent shown 
input unit activated prey enters corresponding sector 
agent brings prey inner circle array unit activated 
wall detector units activated proportional agent distance wall direction provided wall sensor range 
situation shown input unit activated enemy far nw area 
wall detector unit activated small amount east wall just sensor range 
input fed fully recurrent neural network network activation previous time step 
case agent move north north output unit highest activation 
exert significant selective pressure population early stages evolution 
individuals perform poorly ga gets trapped region solution space 
population evolved easier version complex task may possible discover region solution space accessible 
case may possible evolve population easier version accessible 
way ultimate task achieved evolving incrementally sequence tasks starting task evolved directly 
order formulate scheme incremental evolution necessary distinction evaluation task evaluate agent fitness reproduction goal task agent ultimately evolved perform nolfi parisi task connotes environment behavioral objective 
goal task seen culmination series progressively demanding evaluation tasks ft ng total number evaluation tasks goal task 
set tasks ordered easier number tasks required determined experimentally 
evaluation task derived transforming goal task specification appropriate behavior readily evolvable 
task transformation preserve underlying structure environment goal task objective 
heuristics applied derive effective evaluation tasks increase density relevant experiences trial network evaluated greater information shorter amount time evaluation task easier acquisition fundamental goal task skills feasible 
combination heuristics evolution prey capture 
evaluation task environments designed temporally concentrate experience facilitate skill acquisition 
set evaluation tasks derived evolution goal task behavior proceed evolving evaluation task succession 
evolved satisfactory level performance achieved 
completed instantiated evolution resumes 
process continues goal task completed 
number researchers applied task decomposition shaping learning complex tasks tractable colombetti dorigo lin perkins hayes singh 
typically approaches complex task broken simpler components subtasks learned separate systems gas rule bases combined achieve goal task 
contrast incremental evolution proposed wieland saravanan fogel single system learns succession tasks 
adaptation process similar continual lifelong learning elman ring thrun motivated learning real life 
instance goal task driving formula race car grand prix level imagine lattice tasks arranged order increasing difficulty leading goal task 
tasks arranged lattice may epistemic paths accomplishing goal task 
particular case tasks include learning drive car safely learning drive fast competing novice drivers competing stock cars competing 
endeavor drive formula car having gained competence forms racing importantly learning drive car place 
experiments follow general learning principle exploited evolution having agent compete environment challenging response growing expertise 
neuro evolution method enforced sub populations delta coding 
neuro evolution method similar symbiotic adaptive neuro evolution sane moriarty moriarty miikkulainen 
sane shown powerful reinforcement learning method tasks sparse reinforcement 
order apply method evolving complex general behavior extensions necessary enforced sub populations build recurrent networks delta coding assist transferring new tasks 
sane sane differs ne systems evolves population neurons complete networks 
neurons combined form hidden layers feed forward networks evaluated problem 
evolution sane proceeds sequence steps 
initialization 
number hidden units networks formed specified population neuron chromosomes created 
chromosome encodes input output connection weights neuron random string binary numbers 

evaluation 
set neurons selected randomly population form hidden layer feedforward network 
network submitted trial evaluated task awarded fitness score 
score added cumulative fitness neuron participated network 
process repeated neuron participated average trials 
task environment symbiotic adaptive neuro evolution sane 
population consists hidden neurons input output connections 
networks formed randomly choosing neurons hidden layer 
networks evaluated task fitness distributed neurons participated network 
neurons evaluated way recombination performed neuron population 

recombination 
average fitness neuron calculated dividing cumulative fitness number trials participated 
neurons ranked average fitness 
neuron top quartile recombined higher ranking neuron point crossover mutation low levels create offspring replace lowest ranking half population 

goto 
sane neurons compete basis average networks participate perform 
high average fitness means neuron contributes forming successful networks consequently suggests ability cooperate neurons 
time neurons evolve result networks 
sane approach proven faster efficient reinforcement learning methods moriarty miikkulainen moriarty 
reason evolving partial solutions neurons full solutions networks automatically maintains diversity population 
type neuron genotype begins take population networks formed contain copies genotype 
difficult tasks usually require different hidden neurons networks perform 
incur low fitness dominant genotype selected bringing diversity back population 
matter fact advanced stages sane evolution converging population single individual standard ga approaches neuron population clusters species groups individuals perform specialized functions target behavior moriarty 
enforced sub populations esp enforced sub populations sane population consists individual neurons full networks subset neurons put form complete network 
esp allocates separate population units network neuron recombined members sub population 
esp speeds sane evolution reasons subpopulations gradually form sane circumscribed design esp species organize single large population progressive specialization hindered recombination specializations usually fulfill relatively orthogonal roles network 
second networks formed esp task environment enforced sub populations method esp 
population neurons segregated sub populations shown clusters circles 
network formed randomly selecting neuron subpopulation 
consist representative evolving specialization neuron evaluated performs role context players 
sane networks contain multiple members specializations omit members evaluations consistent 
main contribution esp allows evolution recurrent networks 
neuron behavior recurrent network critically dependent neurons connected 
sane forms networks randomly selecting neurons single population neuron rely combined similar neurons trials 
neuron behaves way trial may behave differently sane obtain accurate information fitness recurrent neurons 
sup population architecture esp evaluation neuron consistent 
neuron recurrent connection weight associated neurons subpopulation sub populations specialize neurons evolve expect increasing certainty kinds neurons connected 
recurrent connections neurons adapted reliably 
evolution progresses sub population decline diversity 
problem especially incremental evolution converged population easily adapt new task 
accomplish task transfer despite convergence esp combined iterative search technique known delta coding 
delta coding idea delta coding whitley search optimal modifications current best solution 
conventional single population ga population candidate solutions converged delta coding invoked saving best solution initializing population new individuals called delta chromosomes 
delta chromosomes length number genes best solution consist values delta values represent differences best solution 
new population evolved selecting delta chromosomes adding delta values best solution evaluating result 
delta chromosomes improve solution selected reproduction 
delta coding explores hyper space neighborhood best previous solution 
delta coding applied multiple times successive delta populations representing differences previous best solution 
experiments delta coding implemented esp sub population architecture 
neuron sub populations reached minimal diversity best solution best network specification saved 
new sub populations initialized delta chromosomes neuron best solution dedicated sub population delta chromosomes evolved improve specifically 
esp selects delta chromosome sub population adds delta values connection weights neurons best solution 
sub populations converge best delta value cauchy distribution ff 
delta values represent small modifications best solution large values possible 
delta chromosomes added best solution form new best solution iteration delta phase 
delta coding developed whitley enhance fine local tuning capability genetic algorithms numerical optimization 
potential adaptive behavior lies facilitation task transfer 
delta coding provides mechanism transitioning evolution progressively demanding task delta gamma 
delta gamma 
delta gamma 
delta delta delta delta gamma 
delta delta transition involves saving best network best generation evaluation task initializing sub populations delta chromosomes evolved adapt best task evolved best best modification best formed adding best delta chromosomes best 
solution best adapted 
task transitions weights best solution may need change radically 
especially true new task introduces novel input patterns 
initial delta values need generated probability density function concentrates values local search space occasionally permitting values larger magnitude 
distribution desirable properties cauchy distribution ff ff distribution delta values fall interval interval sigma ff 
prey capture performance effectiveness incremental evolution esp delta coding tested prey capture task compared direct evolution 
determine difficult tasks solved prey speed short term memory requirements task varied 
simulation setup simulations parameter settings sup population size sensor range chromosome mutation probability initial random weight range units ff cauchy distribution networks fully connected neuron chromosomes encoded strings floating point numbers 
arithmetic crossover generate new neurons 
chromosome mutated probability replacing randomly chosen weight value random value range 
techniques parameters effective experimentally small deviations produce roughly equivalent results 
generation evolution networks constructed evaluated way agent placed center theta grid world prey placed random position just agent sensor range 
agent prey alternate action time step prey captured maximum number time steps reached 
agent captures prey prey moved new initial position just sensor range agent allowed moves capture prey 
process continues agent fails capture prey moves value interpreted maximum time agent survive feeding 
number times agent captured prey fitness score 
agent receive high fitness able catch prey initial states deal favorably prey non deterministic behavior 
difficulty task adjusted ways force network follow prey tighter predict behavior accurately prey speed probability making move increased 
second enforce believable behavior short term memory prey allowed greater number moves agent allowed move 
moves prey moves speed 
prey head start guarantees trial contain situations require memory maximizing density task relevant experiences trial 
evaluation tasks referred notation number initial moves prey take prey speed 
agents evolved directly incrementally accomplish prey initial moves agent allowed move continues move speed agent 
agent considered accomplished task capture prey times single trial 
direct evolution direct evolution simulations evaluation task remains constant evolution 
words networks subjected goal task 
lower plot shows results direct evolution 
clearly seen direct evolution powerful solve networks generation perform poorly provide adequate differentiation reproduction environment simply difficult single individual perform significantly average 
networks improve slightly generations invariably trapped region weight space sub populations converged basic task skills acquired 
best individuals move environment times mechanical fashion 
order individual perform know chase fast moving prey remember location 
likelihood encountering individual proficiency random population extremely low direct evolution failed single time simulations 
generations incremental direct performance direct incremental evolution prey capture task 
maximum fitness generation plotted approaches 
direct evolution bottom plot slight progress stalls generations 
plot average simulations 
incremental evolution proceeds task transitions seen abrupt plot eventually solves goal task 
incremental plot average simulations 
included different number generations evaluation task time stretched transitions lined 
incremental evolution incremental evolution population evolved task capturing stationary sensory range 
initial task accomplished best performing network saved delta coding invoked evolve number initial steps increased prey speed steps 
words incremental evolution schedule delta gamma 
delta gamma 
delta gamma 
delta gamma 
delta gamma 
delta gamma 
delta gamma 
sequence tasks forces agent develop short term memory learn deal fast moving prey 
sequences possible natural 
able pursue agent able know phase evolutionary regime discussed separately subsections 
summarizes results 
capturing immobile prey initial stage serves bootstrap entire incremental evolution process presenting task evolved initial random population 
initial evaluation task sufficient variation performance networks direct genetic search 
memory needed accomplish agent need concern state sensory array pursuit involved agent needs able predict results actions 
easier environment networks able survive significantly longer 
importantly survive longer performing primitive form fundamental skills required prey capture 
agent may capturing prey east west north south 
course evolution genetic recombination skills eventually produces adapted individual capture prey directions 
increasing initial prey moves delta delta delta number initial prey moves incremented ability agent remember position prey increasingly important 
evaluation task prey move sensor range 
probabilistic policy prey remain sensor range moves 
increased probability prey moving sensor range distance agent increases 
situations demand memory introduced gradually agent capture prey time ability remember prey position 
tasks rapidly transitioned agent possess general memory right away 
completed best network capture prey regardless direction disappeared far moves 
increasing prey speed delta delta delta delta evolving agent reliably remember prey get prey mobile 
prey position encoded agent recurrent network prey moves sensory inputs match internal representation situation perform 
prey moves third time possible agent capture prey moves time takes agent capture 
agents follow prey just move advantage selected 
task transitions prey gradually faster evolution favor networks pay attention current sensory input determining prey location 
eventually networks emerge pursue capture prey moving time step solving goal task 
incremental evolution changes task small networks formed previous population occasionally perform 
possible evolution discriminate bad genotypes progress goal task 
experimental analysis general prey capture behavior evolved solutions look 
kind networks resulted kind behaviors exhibit 
result incremental evolution outperforms direct evolution significant neuro evolution method involved esp delta coding frame moves frame moves frame moves frame moves frame moves frame moves example prey capture behavior 
prey gets head start moves moves outside sensory array 
moves agent catches sight relying memory saw prey 
eventually agent pins prey wall captures 
similar scenarios occur virtually initial states individual moves vary due stochastic nature prey 
powerful method right 
issues examined subsections 
prey capture behavior shows sequence snap shots illustrate typical prey capture scenario goal task 
frame agent denoted letter initial position prey placed random position just sensor range 
point agent see prey 
frame taken prey moves 
prey outside agent sensory array agent moved 
frame agent moves 
move selected prey se sector 
moves rely recollection agent saw prey 
agent approaches prey may see moves prey begins flee 
move frame agent re acquired prey sensory array bear 
prey move time step agent capture trapping wall 
behavior seen frame agent pursues wall moves limited captured frame 
similar prey capture behavior evolved simulations 
behavior easy describe involves sophisticated components remembering location prey time steps driving lesioned neuron network performance table prey capture performance lesioned network 
successful networks systematically lesioned removing input weights neurons turn 
lesioned network tested prey capture task performance compared original network 
example neuron network lesioned able capture prey times complete network single trial 
results averages trials 
similar results obtained networks tested 
prey wall capturing wall 
important successful agents perform strategy different initial states prey behaves stochastically 
sense agents display believable complex general behavior 
network analysis successful prey capture networks look different specializations contribute interact prey capture 
way analyze contributions individual neurons perform lesion study remove neurons network observe effects network behavior 
prey capture networks fully recurrent units serve output units 
units completely removed network 
removing example north output unit obvious uninteresting effect preventing agent moving north 
unit lesioned disabling input connections connection sensory array allowing receive recurrent signals neurons 
functional role lesioned neuron may inferred observing behavior damaged network prey capture 
main result lesion study networks quite robust table 
neuron neuron lesioned behavior completely break agent tendency pursue prey preserved large extent able perform significant prey capture 
neurons lesioned simultaneously corresponding double degradation performance varying neurons 
difficult attribute particular behavior particular neuron 
coding behavior distributed network 
results line feedforward sane networks controlling mobile robot moriarty elementary behaviors advancing turning stopping front obstacles distributed multiple units 
apparently behaviors distributed 
recurrent weights successful network close zero means neuron modulates behavior neurons 
result functions distributed network system robust degradations lesions noise inaccurate weights values 
pole balancing prey capture performance results section showed incremental evolution powerful direct evolution 
esp simply weak method case result little significance 
result important shown strong neuro evolution method chosen range tasks 
esp implemented standard reinforcement learning task pole balancing shown outperforms best known neuro evolution methods 
basic pole balancing system consists pole wheeled cart finite stretch track 
objective apply force cart regular time intervals pole balanced indefinitely method pole balance attempts failures mean best worst sd genitor sane esp table performance esp balancing single pole compared neuro evolution methods 
results sane genitor obtained moriarty miikkulainen showed faster standard temporal difference methods reinforcement learning adaptive heuristic critic anderson barto learning watkins dayan 
results averages simulations 
cart stays track boundaries 
state system defined variables angle pole vertical angular velocity pole position cart track velocity cart common configuration force restricted constant magnitude bang bang control pole length meter see appendix equations parameters task variations 
table shows comparison esp neuro evolution methods basic sane system moriarty miikkulainen genitor system whitley 
genitor search method adaptive mutation evolves full neural networks 
simulations pole balance attempt consisted placing cart center track pole vertical position allowing network control system exceeded degrees cart moved track 
esp built feed forward networks generation units resulting approximately number weights network sane 
average esp solutions half pole balance attempts sane attempts genitor 
consistent methods 
basic pole balancing setup extended ways problem difficult 
simple modification allow force continuous specified range 
second controller may provided recurrent connections compute derivatives order balance pole 
conventional ne method networks recurrent units wieland able evolve controller problem average generations 
esp solved task generations average simulations tested half networks generation 
challenging problem place second pole 
task saravanan fogel evolutionary computation techniques evolve feed forward network hidden units balance poles average generations 
esp able perform task average generations averaged simulations 
results show esp powerful neuro evolution method 
able solve easy difficult pole balancing tasks faster ne techniques 
seen section solve prey capture task directly 
complex general behavior achieved current evolution methods incremental evolution 
discussion prey capture experiments illustrate potential incremental genetic search 
method applicable problem naturally decomposed sequence increasingly complex tasks 
decomposition performed user provided system evolution commences 
user determine sequence tasks reliably afford successful transitions population size 
may easy may clear way simplify task decorrelating completely goal task may easy tell relative difficulties tasks 
artificial life robot control task sequences usually easy come goal task subsumes natural layers behavior brooks 
example avoiding moving obstacles wall evolved incrementally 
robot evolved move avoiding stationary obstacles obstacles moving slow speeds 
speed obstacles increased robot evolved follow wall 
long consecutive tasks distant system able perform transition 
situations case delta coding invoked task performance ceases improve significantly 
words delta coding crucial design task sequence just right 
evolved controller serve starting point build increasingly sophisticated behavior continued application esp delta coding 
fact esp delta coding approach modify neural network regardless trained 
way possible start evolution basic behaviors built initial best network 
ideally system discover sequence evaluation tasks automatically user providing dimensions prey speed number initial moves 
system start base level task environmental parameters set trivial values speed initial moves 
task achieved values incremented small amount individual performed best initial task longer cope environment 
parameter settings serve evaluation task population 
possible vary environment parameters continuously generation generation response average performance 
prey capture experiments constitute starting point research methods evolve complex general behavior 
extensions algorithm possible automating task transitions extending esp delta coding algorithm networks vary architecture 
tasks real world require complex general behavior including game playing motor control 
major direction apply incremental evolution tasks 
shows task difficult evolve directly neuro evolution applied incrementally achieve desired complex behavior 
approach enforced sub populations allows evolution recurrent networks necessary tasks require memory 
delta coding technique allows evolution transition tasks population lost diversity previous task 
approach applicable real world domains game playing robot control artificial life natural hierarchy behaviors simple complex exists 
acknowledgments special oliver gomez help preparing illustrations 
research supported part national science foundation iri 
appendix prey movement algorithm prey actions chosen stochastically 
step gamma time user defined prey move time choose actions north south east north distribution prob exp delta angle delta dist angle angle direction action direction prey agent dist distance prey agent angle gamma dist gamma dist dist gamma dist dist 
appendix pole balancing parameters equations motion poles balanced single cart gamma sgn gamma cos sin pi effective force th pole cart sin cos pi sin effective mass th pole gamma cos parameters basic single pole problem 
sym 
description value position cart track angle pole vertical deg 
force applied cart gravitational acceleration half length pole mass cart kg mass pole kg parameters double pole problem 
sym 
description value position cart track angle pole vertical deg 
force applied cart gravitational acceleration half length th pole mass cart kg mass th pole kg kg coefficient friction cart track coefficient friction th pole hinge anderson 

learning control inverted pendulum neural networks 
ieee control systems magazine 
barto sutton anderson 

neuronlike adaptive elements solve difficult learning control problems 
ieee transactions systems man cybernetics smc 
brooks 

robust layered control system mobile robot 
ieee journal robotics automation 
cliff harvey husbands 

incremental evolution neural network architectures adaptive behavior 
technical report csrp school cognitive computing sciences university sussex brighton uk 
colombetti dorigo 

learning control autonomous robot distributed genetic algorithms 
meyer roitblat wilson editors animals animats proceedings nd international conference simulation adaptive behavior 
mit press 
colombetti dorigo 

robot shaping developing situated agents learning 
technical report tr international computer science institute berkeley ca 
elman 

incremental learning importance starting small 
proceedings th annual conference cognitive science society 
hillsdale nj erlbaum 
law miikkulainen 

grounding robotic control genetic neural networks 
technical report ai department computer sciences university texas austin 
lin 

self improving reactive agents reinforcement learning planning teaching 
machine learning 
lin 

hierarchical learning robot skills reinforcement 
proceedings international joint conference neural networks 
ieee 
miller cliff 

evolution pursuit evasion biological gametheoretic foundations 
technical report csrp school cognitive computing sciences university sussex brighton uk 
moriarty 

symbiotic evolution neural networks sequential decision tasks 
phd thesis department computer sciences university texas austin austin tx 
technical report ut ai 
moriarty miikkulainen 

efficient reinforcement learning symbiotic evolution 
machine learning 
moriarty miikkulainen 

evolving obstacle avoidance behavior robot arm 
maes mataric meyer pollack editors animals animats fourth international conference simulation adaptive behavior sab 
nolfi elman parisi 

learning evolution neural networks 
technical report center research language university california san diego 
nolfi parisi 

teaching inputs correspond desired responses ecological neural networks 
neural processing letters 
nolfi parisi 

learning adapt changing environments evolving neural networks 
technical report institute psychology national research council rome italy 


reinforcement learning control actions noisy nonmarkovian domains 
technical report unsw cse tr school computer science engineering university new south wales sydney australia 
perkins hayes 

robot shaping principles methods architectures 
technical report department artifical intelligence university edinburgh 
ring 

continual learning reinforcement environments 
phd thesis department computer sciences university texas austin austin texas 
saravanan fogel 

evolving neural control systems 
ieee expert 
singh 

transfer learning composing solutions elemental sequential tasks 
machine learning 
thrun 

explanation neural network learning lifelong learning approach 
kluwer academic publishers 
watkins dayan 

learning 
machine learning 
whitley 

genitor different genetic algorithm 
proceedings rocky mountain conference artificial intelligence 
computer science department colorado state university 
whitley mathias 

delta coding iterative search strategy genetic 
proceedings fourth international conference genetic algorithms 
los altos ca morgan kaufmann 
wieland 

evolving controls unstable systems 
touretzky elman sejnowski editors proceedings connectionist models summer school 
san mateo ca morgan kaufmann 
wieland 

evolving neural network controllers unstable systems 
proceedings international joint conference neural networks seattle wa vol 
ii 
piscataway nj ieee 
wilson 

animat path ai 
meyer wilson editors proceedings international conference simulation adaptive behavior 
mit press bradford books 
yamauchi beer 

integrating reactive sequential learning behavior dynamical neural networks 
cliff husbands meyer wilson editors animals animats proceedings rd international conference simulation adaptive behavior 
mit press bradford books 
