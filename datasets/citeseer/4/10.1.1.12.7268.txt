cpar classification predictive association rules studies data mining proposed new classification approach called associative classification reports achieves higher classification accuracy traditional classification approaches 
approach suffers major deficiencies generates large number association rules leads high processing overhead confidence rule evaluation measure may lead overfitting 
comparison associative classification traditional rule classifiers foil ripper substantially faster accuracy cases may high 
propose new classification approach cpar classification predictive association rules combines advantages associative classification traditional rule classification 
generating large number candidate rules associative classification cpar adopts greedy algorithm generate rules directly training data 
cpar generates tests rules traditional rule classifiers avoid missing important rules 
avoid overfitting cpar uses expected accuracy evaluate rule uses best rules prediction 
years new approach called associative classification proposed integrate association rule mining classification 
uses association rule mining algorithm apriori generate complete set association rules :10.1.1.3.2424
selects small set high quality rules uses rule set prediction 
experiments show approach achieves higher accuracy traditional classification approaches 
associative classification suffers efficiency due supported part national science foundation nsf iis research fund univ illinois ibm faculty award 
yin jiawei han university illinois urbana champaign cs uiuc edu facts generates large number rules association rule mining takes efforts select high quality rules 
propose novel approach called cpar classification predictive association rules 
cpar inherits basic idea foil rule generation integrates features associative classification predictive rule analysis 
comparison associative classification cpar advantages cpar generates smaller set high quality predictive rules directly dataset avoid generating redundant rules cpar generates rule considering set rules predicting class label example cpar uses best rules example satisfies 
cpar employs features improve accuracy efficiency cpar uses dynamic programming avoid repeated calculation rule generation generating rules selecting best literal close best literals selected important rules missed 
cpar generates smaller set rules higher quality lower redundancy comparison associative classification 
result cpar time efficient rule generation prediction achieves high accuracy associative classification 
outline follows 
section reviews general ideas rule classification 
section describes rule generation process cpar 
section discusses predict class labels unseen examples rules generated 
experimental results performance study section conclude study section 
rule classification set tuples 
tuple follows scheme 
ak 
ak attributes 
continuous attribute discretized categorical attribute 
definition 
literal literal attributevalue pair form ai ai attribute value 
tuple satisfies literal ai ti ti value th attribute definition 
rule rule takes form pl consists conjunction literals 
pl associated class label tuple satisfies rule body satisfies literal rule 
satisfies body predicts class rule contains zero literal body satisfied tuple 
important association rule classifiers cba 
cba generates association rules certain support confidence thresholds candidate rules 
selects small set rules form classifier 
predicting class label example best rule highest confidence body satisfied example chosen prediction 
generates evaluates rules similar way cba uses efficient structure :10.1.1.3.2424
major difference uses multiple rules prediction weighted experiments show outperforms cba accuracy 
datasets contain large number rows columns rule generation rule selection cba time consuming 
novel approach proposed overcome problem generating candidate rules small set predictive rules directly generated dataset rule prediction coverage analysis 
rule generation basic idea cpar foil introduced section 
rule generation algorithm cpar developed step step sections 
foil brief 
foil order inductive learner proposed ross quinlan greedy algorithm learns rules distinguish positive examples negative ones 
foil repeatedly searches current best rule removes positive examples covered rule positive examples data set covered 
algorithm foil 
multi class problems foil applied class class examples positive examples classes negative ones 
rules classes merged form result rule set 
selecting literals foil gain measure information gained adding literal algorithm 
foil input training set 
sets positive negative examples respectively 
output set rules predicting class labels examples 
procedure oil rule set rule empty rule length max rule length find literal brings gain append remove examples satisfying remove examples satisfying remove examples satisfying body return foil algorithm current rule 
suppose positive examples negative examples satisfying current rule body 
literal added positive negative examples satisfying new rule body 
foil gain defined gain log log gain number bits saved representing positive examples adding lemma 
running time foil 
foil takes time examples having attributes attribute having values average foil generates rules 
proof 
km possible literals 
searching literal rule examples left 
takes time identify literal 
suppose literal covers remaining examples 
searching th literal examples left 
searching rule takes time generating rules takes time 
predictive rule mining 
section propose predictive rule mining prm algorithm modifies foil achieve higher accuracy efficiency 
reason foil achieve high accuracy generates small number rules 
prm example correctly covered rule removing weight decreased multiplying factor 
weighted version foil produces rules positive example usually covered 
time consuming part foil evaluating literal searching highest gain 
fact similar calculate gain need know information stored data structure called 
definition 
stores information corresponding rule 
numbers positive negative examples satisfying body 

possible literal numbers positive negative examples satisfying body rule rule constructed appending exists corresponding current rule gain literal calculated best km time independent size dataset 
empty rule easily computed training set 
scan attribute tuple increase number positive negative examples satisfying predicate 
takes nk time compute initial 
adds removes example dataset takes time update 
algorithm predictive rule mining achieves higher efficiency foil large datasets 
lemma 
predictive rule mining algorithm takes nk time 
proof 
process building rule removes example 
takes time update removing example 
takes nk time build rule 
building rule changes weights examples takes nk time update 
predictive rule mining prm takes nk time 
rule generation cpar 
associative classification association rule mining generate candidate rules includes conjunctions literals meet support threshold 
subset rules selected candidates 
subset built combining best rules example 
considered doing exhaustive search rule generation 
prm higher efficiency lower accuracy associative classification 
prm rule generated remaining dataset 
suppose example remaining dataset covered rule algorithm 
predictive rule mining prm input output algorithm 
procedure rule mining set weight example rule set compute rule true find best literal gain min gain break append example satisfying body remove change removal example satisfying body weight weight change weight decreased return predictive rule mining algorithm just generated 
sure best rule generated greedy algorithm generated remaining dataset dataset 
prm generate certain number rules example depending weight decay factor 
rules necessarily best rules reasons illustrated 
selecting literals rule building process prm selects best literal ignores 
literals similar gain 
usually rules similar accuracy remaining dataset 
best rule may best rule dataset 
prm selects may lead missing important rules 
novel approach called cpar proposed 
stands middle exhaustive greedy algorithms combines advantages 
cpar builds rules adding literals similar prm 
ignoring literals best cpar keeps close best literals rule building process 
doing cpar select literal time build rules simultaneously 
detailed description rule generation algorithm cpar 
suppose certain step process building rule finding best literal literal similar gain differ 
continuing building rule appending appended current rule create new rule pushed queue 
time new rule built queue checked 
empty rule extracted taken current rule 
forms depth search rule generation 
example 
shows example cpar generates rules 
literal selected literals similar gain higher literals 
literal selected rule generated direction 
rule taken current rule 
literals similar gain selected rule generated directions 
way rules generated 


rules generated cpar 
lemma 
cpar rule generation takes nk time 
proof 
examining example satisfies rule body examine satisfies literals 
takes constant expected time determine example satisfies rule body 
cpar rule generation algorithm rule extracted queue need find examples satisfying rule body remaining examples calculate takes nk time 
lemma know takes nk build rule takes nk time build rule set 
prediction rules rule evaluation 
making prediction rule needs evaluated determine prediction power 
rule pl define expected accuracy probability example satisfying body belongs class rob satisfies body 
laplace expected error estimate estimate accuracy rules defined follows 
expected accuracy rule nc number classes total number examples satisfying rule body nc examples belong predicted class rule 
classification 
rule set containing rules class best rules class prediction procedure select rules bodies satisfied example rules selected step select best rules class compare average expected accuracy best rules class choose class highest expected accuracy predicted class 
multiple rules prediction accuracy rules precisely estimated expect single rule perfectly predict class label example satisfying body 
best rules rules different number rules different classes want lower ranked rules prediction rules prediction 
experimental results conducted extensive performance study evaluate accuracy efficiency cpar compare ripper cba 
datasets uci machine learning repository 
experiments performed ghz pentium pc gb main memory 
approaches implemented authors 
parameters cpar set 
rule generation algorithm set min gain 
best rules prediction 
table shows accuracy approaches datasets uci ml repository 
fold cross validation dataset 
table compares running training time ripper claimed efficient cba cpar datasets 
notice table uses arithmetic geometric average 
running times different datasets dataset ripper cba cpar anneal austral auto breast cleve crx diabetes german glass heart hepatic horse hypo iris labor led lymph pima sick sonar tic tac vehicle waveform wine zoo average table accuracy ripper cba cpar differ lot arithmetic average dominated time consuming datasets 
geometric average equal weight put dataset 
consider geometric average reasonable measure 
table shows average number rules ripper cpar 
ripper cpar arithmetic average geometric average table running time sec ripper cpar new classification approach called cpar developed integrate classification association rule mining 
performance study cpar achieves high accuracy efficiency credited ripper cpar arithmetic average geometric average table number rules ripper cpar distinguished features uses greedy approach rule generation efficient generating candidate rules uses dynamic programming approach avoid repeated calculation rule generation selects multiple literals builds multiple rules simultaneously uses expected accuracy evaluate rules uses best rules prediction 
cpar represents new approach efficient high quality classification 
interesting enhance efficiency scalability approach compare established classification schemes 
strength derived predictive rules motivates perform depth study alternative approaches effective association rule mining 
agrawal srikant 
fast algorithms mining association rules 
vldb pp 
santiago chile sept 
clark boswell 
rule induction cn improvements 
proc 
european working session learning ewsl pp 
porto portugal mar 
cohen 
fast effective rule induction 
icml pp 
tahoe city ca july 
gehrke ramakrishnan ganti 
rainforest framework fast decision tree construction large datasets 
vldb pp 
new york ny aug 
han pei yin :10.1.1.3.2424
mining frequent patterns candidate generation 
sigmod pp 
dallas tx may 
li han pei 
accurate efficient classification multiple class association rules 
icdm pp 
san jose ca nov 
liu hsu ma 
integrating classification association rule mining 
kdd pp 
new york ny aug 
quinlan 
programs machine learning 
morgan kaufmann 
quinlan cameron jones 
foil midterm report 
proc 
european conf 
machine learning pp 
vienna austria 
