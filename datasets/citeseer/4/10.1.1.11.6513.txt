novel approach extract subset independent sources multidimensional observations priori information incorporated learning algorithm available 
constrained independent component analysis cica extended new constraints newton learning algorithm proposed give optimal solution constrained optimization problem 
convergence effect parameters learning algorithm analyzed 
simulations mixtures deterministic random signals synthetic fmri data demonstrate efficacy accuracy proposed algorithm 
keywords ica constrained independent component analysis cica ica 
source separation problems signal detection noise cancellation expect estimate desired single source interesting subset sources mixtures thereof example problems speech extraction financial analysis 
conventionally second order methods minimum mean square error mmse technique detect extract recognize desired signals applications limited due nd order statistics 
kurtosis negentropy contrast function unit ica algorithms proposed separate single source set mixtures independent sources :10.1.1.131.165
extraction particular source algorithms determined contrast function arbitrary due local minima 
researchers introduced asymmetric information sparse decomposition signals fourth order cumulants algorithms solve problems drawbacks system reliability complexity 
denote time varying observed signal source signal con independent components ics linear ica assumes signal linear mixture ics matrix size represents linear memoryless mixing channels 
assumes com ica wei lu school computer engineering nanyang technological university singapore email ntu edu sg xi learning algorithm fig 

illustration neural network extracting multiple desired independent sources input mixtures signals denotes weight matrix output signals 
plete ica hold assumption simplicity relaxed losing generality 
time index omitted simplicity equations 
presents general approach extract desired independent sources observations minimal knowledge original sources algorithm incorporates priori information sources form rough template referred signal technique constrained independent component analysis cica adopted systematically introduce measure closeness output ica contrast function :10.1.1.19.9949:10.1.1.19.9949
extraction expressed constrained optimization problem solved neural network algorithm newton learning 
algorithm extracting desired source introduced extended situation subset sources desired 
fig 
shows neural network multiple neurons output matrix containing weight vectors need learned 
single source extraction output vector denotes 
yj 
previous methods 
second order approach second order approaches minimum mean square error mmse maximum correlation mc criteria extract signal close 
mmse approaches mean square lms method estimate desired output 
minimum mse optimum weight covariance matrix signal temporal expectation 
corresponding output gives signal close secondorder statistics 
show second order methods insufficient recover independent source input order extract ics say optimum weight vector vector canonical base vector constant vector elements zero th element substituting eq 
eq 
vector sources presumed components mutually independent statistical sense covariance matrix diagonal matrix diagonal elements variance corresponding component 
inverse diagonal matrix diagonal el vector ements written note signal identical orig inal source 
order canonical base vector zero equal zero 
general impossible conditions true assume component biggest correlation sources may non zero correlation 
long ic non zero correlations extraction statistically independent sources achieved second order statistical techniques signal available 

unit ica higher order statistics adopted estimate independent sources ica 
estimating ica model consisting matrix ics classical ica unit ica simply finds weight vector product equals ics :10.1.1.131.165
negentropy defined natural informationtheoretic contrast function unit ica gaussian random variable variance output signal denotes differential entropy 
maximizing negentropy produces independent component 
hyv rinen introduced flexible reliable approximation negentropy positive constant function gaussian variable having zero mean unit variance 
practical functions suggested general purpose function better suited super gaussian sub gaussian signals respectively :10.1.1.131.165
negentropy unit contrast function theoretically obtain ic having maximum negentropy sources 
unit ica method produce desired independent source 
learning algorithm guaranteed converge global maximum times local convergent point depends initial weight vector learning rates factors :10.1.1.131.165:10.1.1.131.165

ica blind signal separation problems may want reliably obtain particular desired component set desired sources automatically discard rest uninteresting signals noises 
instances trace desired signals available example stimulation scheme fmri experiments 
section variation classical ica proposed set desired independent components ics extracted incorporating prior information signals refer technique ica 
signals carry information desired sources identical corresponding desired signals 
section technique unit ica extend ica multi 

unit ica goal obtain learning algorithm satisfies conditions simultaneously estimated output ics mixed input signal extracted ic closest signal distance criteria 
suppose contrast function unit ica negentropy function may local global optimum solutions give output identical independent source closeness estimated output measured norm closest minimum value 
assuming ics closest write inequality relationship optimum vector corresponds desired output 
exists threshold closeness equal threshold parameter vectors treating formula feasible constraint unit ica contrast function model problem framework constrained independent component analysis cica equality constraint included ensure contrast function weight vector bounded :10.1.1.19.9949:10.1.1.19.9949:10.1.1.19.9949
ic satisfies conditions defined problem problem solved global convergent algorithm 
introducing slack variable transform inequality constraint equality explicitly manipulating optimum augmented lagrangian function problem eq 
lagrange multipliers constraints respectively scalar penalty param eter denotes euclidean norm penalty term ensure optimization problem held condition local convexity assumption 
find maximum adapted newton learning method iteration index positive learning rate added avoid uncertainty convergence derivative respect derivatives respect simplify approximated product scalar value input covariance positive negative sign coincident inversion hessian matrix scalar covariance matrix second derivatives 
approximate newton learning optimum multipliers iteratively updating gradient ascent method expectation equations estimated samples input learning algorithm network able achieve local maximum optimum point defined kuhn tucker kt triple satisfies order conditions suppose network local maximum perturbed small vector truncated taylor series expansion second term equal local maximum 
system converge third term tive small words hessian matrix negative definite known secondorder condition 
simplicity consider approxi mated eq 
examination valid original hessian 
negative input covariance matrix non singular 
condition true cases large number sample points signals available 
singular near singular inputs small number samples transformed negative definite scalar non singular matrix applying whitening process pca 
consider functions eqs 
scalar value negative general purpose functions negative super gaussian signal sub gaussian signal algorithm converged locally uses function general cases function super gaussian sub gaussian signal 
value critical convergence algorithm 
suitable desired ic defined constrained optimization problem algorithm converged globally produce particular ic output 
upper bound range algorithm convergent point 
small algorithm may converge constraint causes algorithm unstable 
practice algorithm better uses small initially avoid going local optimum gradually increases converge global maximum 

ica multi set corresponding signals available problem easily extended extract desired independent sources simultaneously 
output corresponds unique independent source different individual constrained optimizations defined eq 
able produce globally optimal solution output component distinguishable 
problem ica multi written number desired independent sources extracted containing corresponding augmented lagrangian function sets lagrange multipliers inequality equality constraints respectively parameters form penalty terms 
ica multi combines individual unit ica learns weight vectors simultaneously 
newton learning algorithm extensively derived learn weight matrix vector equals obtained hessian matrix represents diagonal matrix diagonal elements zeros diagonal vector inside gradient respect corresponding learning lagrange multipliers gradient ascent method derivatives constraints define neuron produce particular ic different practice improper values cause different neurons converge independent source 
exact adjustment threshold impossible little knowledge sources may desired postprocess weight vectors decorrelating learning iteration prevent different neurons estimating independent source follows inverse square root obtained eigenvalue decomposition simple calculation :10.1.1.131.165
decorrelation process eq 
helps global convergence achieved network unit reaches global optimum 
selection closeness measure depends form signals available 
common measure closeness estimated output mean square error mse measure requires zero means unit variances 
alternatively correlation closeness measure output normalized value correlation bounded 
proper choice closeness function helps easily choose threshold algorithm robust globally convergent 

experiments demonstrate technique experiments synthetic data compare accuracy effectiveness algorithm second order method unit ica 
accuracy extracted ics measured desired output snr db pi table 
signal noise ratio snr output performance indices pi network individually extracting desired sources algorithm 
signal noise ratio snr db denotes variances signal mse denotes mean square error original extracted signals 
performance network measured performance index pi th element vector 

mixtures random deterministic signals zero mean unit variance independent sources gaussian noise signal periodic deterministic signals random signals sub gaussian super gaussian randomly mixed obtain mixtures 
random signal simulated applying operation roughly gave signs data samples desired source 
deterministic signal simulated series impulses having period desired source 
algorithm unit ica ran mse closeness measure extract deterministic random signal individually corresponding 
expected network converged produce output signals identical desired sources cases 
table shows results extracting desired sources technique high snrs low pis indicate performance algorithm 
output waveforms displayed fig 

results second order method experiment settings compared 
low snrs average value db poor pis average indicated failure method separate desired sources 
previously proposed unit ica algorithm run mixtures produced signal identical irrespective super gaussian source maximum negentropy sources 
algorithm extract gaussian noise signed closest trained network produced output identical gaussian signal snr db pi 
classical ica algorithms unable deal gaussian signals signal statistical properties higher nd order 
trace source algorithm extract gaussian signals closeness signal measured nd order statistics 
samples time series samples time series samples time series sample time series samples time series sample time series sample time series sample time series fig 

signals sources mixture inputs outputs experiments extract desired signals individually independent sources gaussian noise periodic deterministic signals random signals mixture inputs output extracting desired signal respectively output extracting desired signal respectively outputs extracted respectively 

synthetic fmri time series data multiple input stimuli fmri experiments 
fmri time responses activated brain voxels confounded physiological signals cardiac respiratory blood flow electronic noise scanners 
simulate situation fmri time responses activated voxels input stimulus functions convolved gamma response function sinusoid functions frequencies hz hz hz represent periodic activations blood flow cardiac respiratory interferences respectively random gaussian noise generated 
mixed mimic time series generated fmri experiment generate inputs algorithm 
function scheme outputs respectively 
experiment correlation measure closeness sources 
algorithm converged iterations pi 
outputs gave signals close original fmri time responses snr db db respectively 
extracted waveforms shown fig 

normal unit ica algorithm able separate fmri response signals experiment produced output signals identical sinusoid samples time series samples time series samples time series samples time series samples time series samples time series fig 

waveforms sources mixture inputs output simulation extracting multiple fmri time domain response signals 
fmri activation responses sinusoid sources gaussian noise mixtures input stimulation extract activation signal extracted activation identical input stimulation extract activation signal extracted activation identical sources times histograms higher 

novel algorithm extract desired independent components step process signals carry priori information desired sources 
problem formulated cica framework newton learning algorithm derived robustness analyzed 
algorithm able extract independent periodic sources having distributions including gaussians close signals real time closeness measures properly chosen parameters properly adjusted 
experiments demonstrated advantages superiority algorithm compared earlier methods 
second order methods nd order statistics failed extract independent sources close 
technique accurately extracted independent sources high snr low pi negentropy higher order statistical property contrast function 
source extracted previous ica determined negentropy 
algorithm simultaneously extract desired sources available information introduced signals 
extensive application algorithm fmri data analysis 

back application independent component analysis extracting structure stock returns neural systems vol 
pp 

goldstein reed optimal generalized theory signal representation proceedings international conference acoustics speech signal processing vol 
pp 

hyv rinen oja fast fixed point algorithm independent component analysis neural computation vol :10.1.1.131.165
pp 

extraction single source multichannel data sparse decomposition technical report 
luo hu ling liu principal independent component analysis ieee trans 
neural networks vol 
pp 

lu constrained independent component analysis advances neural information processing systems nips mit press pp 

comon independent component analysis new concept signal processing vol 
pp 

girolami extraction independent signal sources exploratory projection pursuit network lateral inhibition vision image signal processing iee proceedings vol 
pp 
october 
hyv rinen new approximations differential entropy independent component analysis projection pursuit advances neural information processing systems nips pp 

von modeling response analysis functional mri time series human brain mapping vol 
pp 

karhunen oja wang class neural networks independent component analysis ieee transactions neural networks vol 
pp 

lu extracting task related components functional mri submitted ica 
