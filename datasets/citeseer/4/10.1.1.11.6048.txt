ratcliff van zandt mckoon page connectionist diffusion models reaction time roger ratcliff northwestern university van zandt johns hopkins university gail mckoon northwestern university short title modeling reaction time address correspondence roger ratcliff psychology department northwestern university evanston il 
connectionist frameworks grain mcclelland bsb anderson diffusion model ratcliff evaluated data signal detection task 
subjects asked choose possible responses stimulus provided feedback choice correct 
dependent variables included response probabilities reaction times correct error responses reaction time distributions independent variables stimulus value stimulus probability lag abrupt switch stimulus probability 
diffusion model accounted aspects asymptotic data including error reaction times previously problem 
connectionist models accounted aspects data adequately failed greater lesser degree important ways model similar diffusion model 
connectionist learning mechanisms unable account initial learning abrupt changes stimulus probability 
results provide advance development diffusion model show long tradition reaction time research theory fertile domain development testing connectionist assumptions decisions generated time 
research aimed investigating information processed time long influential history psychology 
general textbook discussed simple choice reaction time behaviors shapes reaction time distributions individual differences reaction time effects reaction time experimental variables stimulus intensity 
topics raised article 
cognitive revolution gave rise modern cognitive psychology reaction time entered spotlight major dependent variable 
considerable effort devoted development theories explain information processed time yield mean response times distributions response times accuracy levels 
current theoretical issues include example serial versus parallel processes continuous versus discrete processes efforts continue comprehensive theories time course processing 
summary state reaction time theory luce 
main difficulty modeling dependent variables reaction time probability correct versus error responses modeled integrated framework 
connectionist models relatively new class models surge development testing taken place years 
models offer promise explanations cognitive tasks learned 
models learning result individual trials stimuli trial feedback model response correct 
processes response stimulus chosen usually assumed parallel interactive nonlinear continuous 
processing characteristics theoretical choices examined reaction time modeling 
potentially fruitful connectionist models meet reaction time models joint effort theory development competitive model testing evaluation 
carrying reaction time research forward meet relatively new domain connectionist modeling purpose investigations described article 
specifically asked connectionist models accommodate wide ranging kinds data critical reaction time domain time account learning 
second purpose test develop standard model ratcliff diffusion model 
standard information processing models connectionist models different insights offer fullest advantage insights gained kinds models pushed far possibly 
goal best accomplished arena investigation allows simultaneous testing kinds models 
research described article arena chose simple signal detection paradigm 
connectionist models assume decision required trial experimental task comes processes integrate accumulate information time 
early connectionist models achilles heel failure match assumption specific mechanisms predict full range empirical measures time course processing including probabilities response versus response times interactions error versus correct response times shapes response time distributions 
models successful combining structural assumptions information represented algorithms learn task producing outputs qualitatively matched mean accuracy responses mean reaction time originally designed provide simultaneous account complete range data 
article focus connectionist frameworks designed specifically deal full range measures 
anderson brain state box model bsb autoassociative matrix model 
mcclelland grain framework provides list principles exploration processing time 
principles constructed multilayer models examination 
bsb model grain models iterative algorithm takes variable number steps reach decision stimulus input produce output take amount time 
features potentially allow models learn cognitive task predict multiple aspects performance reflected shapes distributions response times speed accuracy interactions 
important advance 
attempt deal complete range reaction time data context framework originally developed examine learning adds new degree complexity connectionist models 
connectionist models psychology attempted explain behavior dependent variable usually variable closely related learning probability correct response 
number traditional non connectionist information processing models shown provide account time course processing see luce townsend ashby 
models provide accounts learning strength deal multiple dependent variables measure decision processes 
diffusion model seen extension earlier random walk models link heath stone developed ratcliff choice tasks 
chosen comparison connectionist models reasons 
member general class random walk models provide better accounts experimental results counter models absolute criteria various serial parallel models see luce 
second diffusion model successfully applied wide range experimental paradigms accurately accounting mean reaction times error rates shapes reaction time distributions effects deadline response signal manipulations 
exemplars class random walk models link heath stone fit aspects data explicit mathematical expressions describing characteristics data diffusion model 
example ratcliff van zandt mckoon page diffusion model explicit formulas distributions reaction times distributions processes point time meyer ratcliff 
third diffusion model designed explain fast single stage decision processes respect similar connectionist models evaluated 
assumption underlying diffusion model information accumulated continuously time 
boundaries accumulation process possible response choices 
information accumulated starting point boundaries sufficient information accumulated boundaries crossed decision 
noise process leads variations rate information accumulated time stimulus different occasions lead decision require amount time decision 
aspects model allow account shapes distributions response times speed accuracy interactions 
chose simple signal detection task provide comprehensive data base compare contrast test diffusion model models connectionist frameworks 
simultaneous goals test connectionist assumptions time course decision processes individual trials test assumptions empirical context assumptions learning tested second extend diffusion model new experimental task evaluating ability deal correct error reaction times reaction time distributions response probabilities third compare connectionist models diffusion model domain domain empirical situation strong predictions data 
describing experimental task chose data 
diffusion model connectionist models derived grain framework bsb model applied data 
signal detection paradigm important choice evaluation comparison connectionist standard models reaction time experimental paradigm provide testing ground 
choose experimental task research main criteria adopted 
task involve relatively simple stimuli assumptions stimuli represented interfere examinations reaction time accuracy 
second task allow examination full range measures reaction time models routinely tested reaction times correct incorrect responses accuracy reaction time shapes distributions reaction times hazard functions 
third response probability accuracy span range near chance near ceiling full range correct error reaction times examined function response probability 
fourth task representative wide range experimental tasks produce data typical variations interactions standard speed accuracy measures 
fifth task learning component statistical properties stimuli expected engage learning mechanisms connectionist models sixth learning component allow examination sequential effects learning trials predicted connectionist models 
task chose met criteria just listed 
signal detection paradigm adapted watson see precursors lee smith vickers vickers vickers probability learning paradigms atkinson bower estes 
turns paradigm analog general recognition randomization technique ashby 
application trial array asterisks computer screen observer asked decide number asterisks display high low 
number asterisks chosen distributions numbers high distribution low distribution distribution fixed mean standard deviation numbers 
feedback trial tell observer response correctly indicated distribution stimulus chosen 
feedback observer information distributions 
distributions overlapped substantially trials feedback observer highly accurate 
display asterisks example come high distribution trial low distribution 
signal detection paradigm advantages possible choices 
variable underlying performance number asterisks varied small steps high probability response high probability response 
example display asterisks strongly supports low choice display asterisks strongly supports high choice display asterisks middle strongly supporting choice 
lexical decision example comparable variation possible stimulus word 
paradigm offers great deal experimental control allowing measurement speed accuracy interactions needed stringent tests models 
important advantage signal detection task generality results obtained 
provides data typical large class signal detection procedures 
example watson digit numbers tones differing frequency lee digit numbers gray scale stimuli line lengths vickers binary dot patterns line lengths lamps flashing varying rates randomly oriented line segments 
ratcliff patches black white pixels varying brightness dimension patches red green pixels varying red green patches black white pixels required different judgments brightness 
signal detection task representative cognitive paradigms lexical decision matching tasks recognition memory semantic verification assumed basis decisions tasks unidimensional continuum 
example lexical decision model claim word decision depends amount activation familiarity lexical memory evoked presentation stimulus response value high word response low nonword response 
similarly different matching task model claim decisions number perceived differences stimulus elements large number different response small number response krueger 
data asterisk signal detection task provides speed accuracy interactions shapes response time distributions cognitive paradigms 
diffusion model connectionist models fail account pattern reaction time accuracy data signal detection task probably fail account similar patterns cognitive tasks 
signal detection task allows investigation particularly difficult problem standard reaction time models 
currently reaction time model able account different relations observed empirically correct error response times 
generally accuracy stressed important speed task difficult difficult perceptual discriminations difficult recognition memory tasks ratcliff murdock reaction times errors slower reaction times correct responses 
contrast speed stressed accuracy task easy choice reaction time error reaction times faster correct reaction times see luce discussion available data 
signal detection task task intermediate difficulty individual subjects adopt different criteria 
experiment subject showed errors faster correct responses showed errors slower correct responses subjects showed mixture patterns see smith vickers 
signal detection paradigm offers data suitable examining model successfully explained varying relationships correct error reaction times 
overview preview steps research 
began examining response time accuracy measures learning measures 
order get stable results response time accuracy measures signal detection paradigm modeling collected large amount empirical data individual subjects experiment 
second diffusion model fit data 
model provided account data including accurate quantitative fits served demonstration tractability paradigm modeling provided level explanation data compare connectionist models 
third models grain framework bsb model tested data 
ratcliff van zandt mckoon page fourth fifth steps added learning measures 
fourth step experiment examine adaptation 
middle test session probability stimuli drawn versus distributions abruptly changed 
questions fast subjects adapt change connectionist models adapt equally quickly diffusion model accurately plausibly account subjects different behaviors different probability conditions 
fifth step examine initial learning 
experiment examined trials learning determine quickly subjects learn task 
subjects appeared learn quickly experiments total subjects experiments meant observations earliest trials 
experiment subjects demonstrate generality fast learning 
experiments different measures large set tests connectionist models different overlapping set tests diffusion model 
completely successful outcome connectionist models provided account learning adaptation measures response time accuracy measures 
diffusion model learning mechanism best possible outcome complete accurate account response time accuracy measures account coherent different conditions subjects adapted performance experiments 
detailed description experiments data model applications give paragraphs brief look models fared 
diffusion model 
unanticipated experimental finding subjects showed radically different patterns data expected broad levels mean reaction time accuracy complex interactions reaction time accuracy measures 
diffusion model provided explanation different patterns behavior came generating predictions accurately matched subjects individual data 
diffusion model explanation showed single underlying variable govern subjects performance model showed large differences performance arise subjects basing responses underlying variable 
accounting response time accuracy data diffusion model able explain different patterns correct versus error reaction times came different subjects problem solved previous reaction time models 
important find model important way diffusion model assuming value parameter model variable trials fixed ratcliff 
words assumption aspects decision process example starting point mean drift rate held constant trials 
finding model gain significant power handle data variable fixed parameter values offers avenue modeling previously exploited cognitive theory see van zandt ratcliff 
bsb model 
bsb model reasonably successful dealing mean reaction time accuracy correctly predict error reaction times 
model provided moderately account adaptation stimulus probability condition get model show initial learning fast subjects 
grain models 
model assumed learning began experimental trials continued experiment 
model stimulus feedback sequences equivalent experiments learn perform task feedback 
model assumed learning taken place experimental trials began pre experiment training consisted training model possible stimulus reproduce output probability stimulus drawn high distribution 
model correctly account error reaction times sequential effects trial speed subjects adapted changes stimulus probability speed initial learning 
second model generally successful response time accuracy measures sequential effects 
accommodate subjects speeds initial learning changes stimulus probability 
sum diffusion model succeeded extremely domain providing coherent account correct error reaction times reaction time distributions accuracy providing explanation drives decision process domain include learning 
connectionist models give satisfactory account response time accuracy measures learning measures investigations lay failed provide foundation theory development evaluation 
experiment experiment subjects tested multiple sessions signal detection task order collect stable data analyses reaction time distributions error reaction times individual differences subjects 
method subjects 
subjects northwestern university undergraduates males female paid participation sessions 
normal corrected normal vision 
stimuli apparatus 
asterisks displayed grid upper left corner vga monitor visual angle degrees horizontally degrees vertically 
appeared light characters dark background high brightness contrast clearly visible 
vga monitors driven ibm style microcomputers controlled stimulus presentation time recorded responses response times 
number asterisks presentation trial selected randomly sampling discrete approximately normal distributions means standard deviation watson 
discriminability distributions approximately 
distributions crossed number number referred crossover point distributions 
display positions asterisks trial selected randomly possible positions character grid 
procedure 
subjects instructed number asterisks trial selected random groups numbers low group high group low group fewer asterisks average high group 
subjects task decide number asterisks came low group case press key computer keyboard high group case press key 
response incorrect subject informed immediately response 
subjects understood completely accurate numbers middle range come distribution task give best judgment 
provide motivation subjects payoff scheme awarded points correct response penalized point incorrect response 
block trials subject earn points 
subjects encouraged responses quickly told goal maximize total number points earned course experiment 
points add payment rate experiment 
trial began presentation asterisks 
remained screen subject responded point screen erased 
response correct ms waiting period asterisks trial 
response error message error point appeared screen ms followed trial ms 
block trials completed minutes 
blocks subject encouraged take brief rest desired 
design 
subject performed sessions subject performed sessions approximately weeks 
session comprised blocks trials 
block half stimuli sampled low distribution half sampled high distribution 
total observations session subject 
session analysis subject resulting total observations subject 
block trials session discarded analyses 
results data analyses trials response times ms greater ms discarded constituted data 
ratcliff van zandt mckoon page subjects showed large individual differences performance 
subject produced quite long reaction times ms range produced short reaction times ms range intermediate 
modeling perspective range behaviors positive aspect data requires models flexibility 
models constrained fit average data adequately individual data extreme subjects 
presentation data divided parts 
shown frequency subjects high low responses followed relative frequency high low stimuli 
subjects showed sequential effects response trial affected response previous trial 
second responses generally slowed number asterisks display nearer crossover point distributions 
subjects relationship correct error reaction times varied 
third distributions reaction times showed typical skewed shape hazard functions rose reached asymptote fell slightly typical tasks see luce 
response probability sequential effects shows probability low response subject function number asterisks previous response 
probabilities fall smoothly zero cross th percentile point close number low high distributions crossed 
subjects performed systematic biases 
subjects differed sequential effects 
subjects response little high prior response high curves labeled versus 
contrast subject showed reverse effect prior response high greater probability current response low curves versus 
subjects showed sequential effect depended feedback previous response subject showed sequential effects 
individual differences cf 
challenge models mechanism produces sequential effects flexible behave opposite ways different subjects 
fact sequential effects dependent prior response prior feedback consistent earlier studies psychophysical tasks thomas williams choice reaction time cohen see luce ch 
studies particularly absolute identification ward find feedback affected response probability 
earliest investigations signal detection paradigms originally appeared explanation learning take prior feedback account kac thomas showed learning modeled assuming criterion shifts stimulus value learning depend directly prior feedback 
thomas account deal paradigms feedback subject 
experimental results consistent early signal detection results choice reaction time results 
subjects knew feedback inconsistent stimuli correct response high low 
large number sessions tested subject probably explains feedback response affect performance 
insert response probability mean reaction time sequential effects reaction time small order ms reaction times averaged previous feedback previous response 
shows mean reaction time function displayed number asterisks high low responses 
generally responses slowed crossover point 
insert purposes exposition defined error responses crossover point low responses numbers greater labeled errors high responses numbers 
error label responses convenient way describing 
response type exactly error best response correct alternative 
note definition correspond feedback subjects error point feedback determined distribution number drawn position relative crossover point 
error terminology compactness description article 
subjects showed different patterns error versus correct response times 
subjects errors extreme stimulus numbers numbers faster correct responses numbers extreme errors slower correct responses 
subject errors faster correct responses subject errors slower correct responses 
difference subjects challenge modeling outlined model able account variation time explaining commonalities subjects 
addition model able account switch slow errors fast errors response probability changes subjects 
compact way combine reaction time data response probability data plot jointly latency probability function pike 
reaction time functions high low responses reasonably symmetric crossover point collapsed 
example reaction times low responses asterisks averaged reaction times symmetrically equivalent high responses asterisks probability low response asterisks averaged probability high response asterisks 
average reaction time plotted average response probability shown 
different patterns error versus correct response times show degree latency probability functions symmetric 
errors generally correspond responses probability 
correct response probability corresponds error response probability 
example probability correct response corresponding error probability 
correct responses corresponding errors response times latency probability function symmetric inverted shaped function maximum 
function subject asymmetric errors slower corresponding correct responses see 
subjects functions asymmetric errors slower correct responses extreme errors faster correct responses 
subject function symmetrical errors little faster correct responses 
providing summary data shape latency response probability function allows discrimination various traditional sequential sampling models reaction time pike vickers vickers 
example simple random walk model predicts symmetrical inverted shaped function counter models absolute criterion predict increasing function fast correct responses slow errors vickers 
shows data subjects conform predicted functions elementary versions models pike pattern subject handled models smith vickers vickers pattern subject link heath 
patterns data typical reported vickers vickers 
paradigms 
insert reaction time distributions shows representative reaction time distributions stimuli response low time asterisks 
reaction time research distributions skewed right 
subject showed wider central region possible shorter tail distributions subjects 
subject showed large reductions reaction time function session data subject sessions analyzed separately 
shows partition unimodal distributions narrower skewed 
insert figures shows reaction time hazard functions derived reaction time distribution histograms 
hazard function gives likelihood point time process terminate instant time terminated time 
mathematically represented density function time ratcliff van zandt mckoon page cumulative distribution function large approaches approaches zero estimate unstable luce severe oscillations estimate common 
hazard function method testing detailed hypotheses distribution family set data arises see luce 
hazard functions diagnostic distribution underlies observed hazard function variability trials see luce van zandt ratcliff useful assessing model predictions 
hazard functions calculated histograms equal width bins 
hazard functions reasonably stable ms subjects ms subject 
hazard functions stable ms top panel ms bottom panel 
hazard functions shown peaked rising rapidly maximum falling gradually extreme tail unstable 
subject shows hazard functions sessions data show similar trends subjects 
summary data experiment targets modeling sequential effects previous response current response effect feedback previous response second shapes functions different patterns error versus correct response times shown shapes third shapes reaction time distributions correct error responses fourth shapes hazard functions 
aspects data typically cognitive paradigms 
success failure models significant just signal detection paradigm application models cognitive paradigms 
intriguing variations subjects shown differences mean response times accuracy rates differences speed errors versus correct responses 
task requires subjects learn respond appropriately stimuli drawn simple probability distributions data challenge current models qualitative behavior predicted models match flexibility shown patterns data subjects 
example models predict fast errors predict slow errors predict crossovers models correctly predict shape reaction time distributions 
sections show diffusion model takes challenge data move consideration connectionist models 
diffusion model diffusion model originally developed explain processes information retrieved memory time ratcliff 
successfully predicted data binary choice recognition memory tasks sternberg continuous memory list study test paradigms 
important modifications applied perceptual matching letter strings ratcliff varied consistent mapping procedures sternberg paradigm strayer kramer new paradigms speed accuracy decomposition procedure meyer irwin osman ratcliff 
cases accounted speed accuracy relations mean response times shapes reaction time distributions 
models class received strong support data single cell recordings monkeys 
major failing diffusion model inability explain relationship correct error response times 
experiment subject showed faster error correct responses subject showed slower error correct responses subjects showed slower errors intermediate stimuli faster errors extreme stimuli 
ratcliff stated diffusion model predict slower errors correct responses 
show model fact accurately predict varying patterns error versus correct response times 
model appeared fail past applications lack computer power prevented adequate search possible parameter spaces 
diffusion model information stimulus continuously available accumulated time decision boundaries 
mean rate accumulation positive process generally moves positive boundary mean rate negative generally moves negative boundary 
sources variance diffusion model 
rate accumulation drift rate noisy varies mean moves boundary 
second mean rate accumulation information different items experimental condition different instances item 
ratcliff built trial variance model account differences variability memory strength individual stimuli single experimental condition 
amount trial variance set constant fitting experimental data 
noted parameter space automatically searched different values source variance allowed model fit different patterns error reaction times 
details qualitative behavior model see appendix 
diffusion model random walk counter models generally understood extension signal detection theory time domain pike 
sampling determine single value strength stimulus diffusion model accumulates information continuously repeated samples 
average repeated samples mean drift rate standard deviation gives variability drift rate 
fitting diffusion model data experiment parameters diffusion model illustrated described follows starting point boundary parameters distances positive boundary starting point negative boundary respectively 
represent amount evidence needed produce positive response negative response 
apparent high low biases subjects response patterns reason expect experimental design set eliminated free parameter fits data experiment 
parameter mean value drift rate stimuli experimental condition 
different possible number asterisks stimulus display different experimental condition different values parameter variability parameters standard deviation drift individual process standard deviation mean drift rate different trials stimulus 
fourth parameter model encoding response time parameter er represents non decision components reaction time 
parameter representing variability drift trial set value fits diffusion model data free parameter 
scaling parameter altered parameters model multiplied divided ratio old new values produce exactly finishing time distributions response probabilities altered 
value chosen close value earlier applications model comparisons parameter values applications ratcliff 
insert fit model experimental conditions 
fit model representative conditions adjusting parameters er values drift rate condition produce best fit model data conditions 
er held constant varied produce predictions conditions data collapsed groups 
conditions fitting chosen represent widely spaced parts latency response probability function 
example subject values response probability chosen 
values corresponds sets reaction time data number probability high response equaled chosen value number probability low response equaled chosen value 
fitting program adjusted values plus parameters er minimize sum squares standard function minimization routine 
data different subjects fit individually values plus parameters free vary subjects 
ratcliff van zandt mckoon page parameter estimates shown table 
insert table sums squares function minimization constructed follows 
empirical reaction time distributions fit ex gaussian distribution convolution normal exponential distributions 
ex gaussian shown provide summary empirical reaction time distributions ratcliff ratcliff murdock parameters describe shape distribution 
theoretical distributions generated diffusion model theoretical distributions fit ex gaussian distribution 
parameters ex gaussians served meeting point empirical data theoretical predictions model roughly represents position leading edge distribution represents extent tail distribution 
sum squares function sum squared differences theoretically derived empirically derived values ex gaussian summary parameters plus sum squared differences theoretical empirical values response probability weighted standard errors 
fitting routine minimized sums squares function diffusion model parameters see appendix full presentation 
ex gaussian third parameter needed diffusion model produces rise reaction time distribution close rise observed experimental data 
fits model shown figures direct fits model data 
fit reaction time distributions directly quantiles distributions 
obtained fits different procedures 
pointed values subject merely representative experimental conditions served purpose summarizing range data allowing parameter values er fixed subject 
sweep conditions varied low value high value 
turned conditions accommodated ranging 
varying range parameters fixed values diffusion model predict conditions standard measures probability high response speed high low responses shapes distributions response times hazard functions distributions 
sections show model success predictions 
latency response probability functions error reaction times test diffusion model examine accurately fit latency response probability functions displayed 
functions shown fits diffusion model 
model fits data single parameter varying 
model explains large differences average reaction time subjects result differences parameter differences boundary positions see table 
error bars shown represent plus minus standard errors reaction time 
fits data points lying standard errors outside theoretical functions 
different shapes latency response probability functions different subjects reflect different patterns error versus correct response times 
model shows flexibility accounting different shapes different values parameters er example model able fit near symmetry function subject extreme asymmetry function subjects 
main determinant symmetry asymmetry size standard deviation mean drift values trial trial see table values 
see standard deviation mean drift trials determines shapes latency probability functions consider diffusion process starting point halfway boundaries variability drift trials error reaction times correct reaction times 
variability drift rate trials introduced responses weighted average reaction times different drift rates weighted probability response 
table illustrates weighting 
columns show correct error reaction times response probability 
illustrate variability drift average row adjacent rows average reaction time accuracy drift variability average rows constant drift fixed 
results averaging shown columns 
example averaging constant drift values average error response time ms computed error response times values ms ms ms weighted probability ms corresponding average correct response time weighted probabilities respectively ms graphing probabilities responses probability correct column minus probability correct errors averaged correct error response times columns produces asymmetrically shaped latency probability function subjects 
insert table diffusion model fails accurately capture shape probability function subject model shows error responses faster correct responses contrast data error responses systematically faster correct responses 
explanation fast errors random walk models fast errors result variability starting point walk see ratcliff 
starting point near boundary probability reaching boundary error fast reaction time higher starting point away boundary slow errors lower probability 
averaging faster errors weighted higher probability slower errors weighted lower probability produces faster errors average case starting point constant 
test explanation tenable subject compared model prediction single starting point see table predictions normal distribution starting points standard deviation 
result shown 
standard deviation value error reaction times speeded exactly match data 
subjects larger variability range errors extreme stimuli faster produces fast errors subjects slightly better accord data little effect response times intermediate errors 
insert earlier research ratcliff stated diffusion model correctly predict error reaction times 
recognition memory paradigms error reaction times generally slower correct reaction times slow diffusion model predict 
shows signal detection paradigm reaction times reasonably fit range response probability values including values including values responses errors small deviations fast error reaction times noted subject 
reason diffusion model predicts error reaction times correctly minimization program able adjust drift variability parameter 
practical reasons fitting hand lack computational power ratcliff fixed parameter adjusting parameters model 
old recognition memory data new minimization program showed fits error reaction times obtained recognition memory paradigms ratcliff data reported 
important note variability drift variability starting point diffusion process allow model predict complicated pattern error correct reaction times 
van zandt ratcliff showed seemingly decisive tests models variability parameters models trials introduced 
results go step showing patterns data explained diffusion model fixed mean drift rate fixed starting point explained variability parameters 
reaction time distributions hazard functions second test diffusion model examine accurately predict shapes reaction time distributions 
figures demonstrate 
figures show empirical distributions model predictions conditions subject condition response ratcliff van zandt mckoon page probability high near condition probability lower range model predictions high low responses probability level response boundaries symmetrical fits 
hazard functions figures showed functions rising asymptote rising asymptote falling slightly 
functions shown conditions distribution shapes hazard function shapes deviate illustrated patterns conditions 
accuracy diffusion model predictions especially noteworthy fitting diffusion model parameters summarize distribution shape 
additional free parameters fitting distributions er values mean drift rate insert figures goodness fit values distributions shown figures 
computed observed expected numbers counts histograms figures 
distributions theoretical empirical distributions significantly different 
comparable goodness fit models reaction time distributions ratcliff murdock 
possible reasons perfect fits including variations behaviors subjects sessions slow day fast day long term practice effects variability non decision component processing er 
factors produce abrupt predicted rise leading edge distribution misses occur 
fits systematic misprediction 
stimulus probability drift rate recognition memory drift rate diffusion model corresponds strength familiarity item memory ratcliff 
item familiar meaningful subject studied longer 
dimension drift rate correspond signal detection paradigm 
subjects ought choice information experiment 
piece information number trials feedback probability number asterisks drawn high distribution versus low distribution 
possibility subjects base choices probability 
long tradition probability learning literature see atkinson bower estes especially estes detailed review know subjects situations 
data experiment directly suggests subjects experiment probability matching 
instance response probabilities shown directly correspond probability number asterisks high low response probability functions abrupt compared stimulus probability function 
diffusion model mean drift rate derived stimulus probability 
words mean drift rate number asterisks reflect probability number drawn possible distributions high distribution low distribution 
discovered possibility intuition plotted drift rates subjects function experimental condition number asterisks compared drift rates probabilities possible number asterisks drawn high versus low distribution typical experimental sequence 
subjects functions shown 
probability values typical experimental sequence generated algorithm generate stimuli experiment length sequence total number trials subject receive sessions combined 
possible number asterisks probability asterisks drawn high distribution plotted 
order plot probabilities scale subjects drift rates probabilities transformed lie subtracting multiplying result probability middle range corresponded drift rate middle range 
dramatic results individual subjects similar drift rates conditions despite large differences performance measures 
second subjects drift rates correspond closely stimulus probability 
insert unexpected results reasons 
subjects performed differently radical ways terms reaction times speed accuracy trade offs parameters diffusion model drift rate 
examination individual reaction time data gives clue significant underlying similarities invariances subjects 
second plot drift rates plot response probabilities drift rates clearly map response probability drift rate functions gradual range numbers asterisks response probability functions steeper climb middle range asymptote quickly 
diffusion model subjects basing decisions underlying variable 
shows stimulus probability mapped diffusion process produces observed response probability reaction time data 
similar drift rate stimulus probability correspondences ratcliff range experimental procedures including brightness discriminations red green color discriminations different brightness judgments 
relationship drift rate stimulus probability number parameters diffusion model reduced 
drift rates values longer needed parameters model 
replaced transformed probability values 
parameters subject original fit diffusion model replaced parameters needed transform probability drift rate 
means complex pattern results including response probabilities shapes reaction time distributions hazard functions correct error responses modeled accurately diffusion model free parameters subject parameters scale probability drift variability starting point er results appeared implicate stimulus probability probability matching function driving performance rule function driving drift distance criterion 
subjects setting criterion dimension numerosity responding high number asterisks stimulus greater criterion responding low number asterisks stimulus criterion 
variability encoding stimuli possibly variability criterion setting lead gradual transition mean drift rate high low 
domain categorization research instance models distance criterion models compared maddox ashby nosofsky concluded great deal mimicking classes models 
early instance models essentially predicted probability matching function guiding responses individual subjects produced deterministic responding sharper functions predicted 
exemplar models random walk decision process shown gradual stimulus probability functions mapped sharper response probability functions summation random walk known property sequential sampling models 
situation categorization literature mimicking classes models applies research function driving diffusion process stimulus probability distance criterion 
resolution situation require experiments produce differential predictions kinds models 
reviewers asked expected priori stimulus probability function driving drift rate diffusion model random walk process probabilistic 
reasons relationship necessarily expected 
random walk discrete analog diffusion process feller ratcliff probability step response boundary directly derived stimulus probability 
limit random walk produce diffusion process step probability approach stimulus probability 
second discrete random walk probability step boundary versus stimulus probability experimental paradigms 
example choice reaction time feedback perfectly accurate stimulus probability 
stimulus probability probability step ratcliff van zandt mckoon page correct boundary probability process deterministic correct account data 
third random walk models rate approach boundaries derived stimulus probability 
example link heath random walk model assumed distance moved boundary step function distance stimulus noisy criterion 
fourth performed pilot experiment digit numbers stimuli arrays asterisks 
feedback stimulus selection scheme experiment 
results showed subjects set fixed criterion number stimuli higher lower produced approximately mean reaction times response probabilities near ceiling 
fits diffusion model data drift rates function stimulus probability 
sum reasonable hypothesis entertain drift rate transformation stimulus probability follow directly theory diffusion model random walk models assumptions data sets contradict hypothesis see ratcliff 
sequential effects 
aspect data left discuss sequential effects response 
turns diffusion model account effects way accounts subjects switch feedback scheme accommodate changes probabilities stimuli high versus low feedback 
experiment examines detail consequences switching postpone discussion sequential effects presentation results experiment 
summary success diffusion model application gives account data parametrically time offers insight stimuli signal detection paradigm controlled responses aspects performance common subjects aspects different 
way see directly data subjects took stimulus probability account performance way see exactly relationship stimulus probability response probability response time 
model shows stimulus probability distance criterion transformed drift rate turn acting mechanics diffusion model produce range characteristics data 
time model shows different individual subjects show quite different response time accuracy profiles governed variable stimulus probability 
fact diffusion model fits data raises issue 
mean reaction time response probability taken account difficult falsify model 
shapes reaction time distributions taken account falsification easy model predicts reaction time distributions skewed right skewed left bimodal places tight limits minimum reaction time differ error correct responses particular experimental condition 
model strong prediction experimental situations boundary positions fixed example experiment manipulation variables trial impossible subjects anticipate condition tested impossible shift boundaries conditions see ratcliff experiment 
situation model predicts drift rate varies experimental conditions increases reaction time come spread tail reaction time distribution relatively little change fastest responses ex gaussian terms correspond roughly increasing times 
way look issue consider failure diffusion model fit data interpreted 
model predicts large spread tail distribution shifts leading edge small 
shifts leading edge large relative spread tail signal stage processing inserted condition experiment relative 
inserted stage show shift onset growth accuracy response signal data see discussion hacker examples shifts leading edge reaction time distributions 
diffusion model initial inspection hard falsify tight constraints especially predictions shapes reaction time distributions 
success application diffusion model signal detection paradigm significant step forward traditional modeling 
time description offered model information accumulated time leads accurate unified account error correct response times including shapes reaction time distributions hazard functions 
past successful models simultaneously accurately predict aspects data 
alternative standard reaction time models diffusion model class sequential sampling models includes random walk models counter models runs models see luce 
models random walk models discrete versions diffusion model family diffusion model show behavior diffusion model 
standard random walk model predicts mean number steps cross boundary correct error responses starting point equidistant boundaries 
order account choice reaction time data added starting point variability random walk produce error reaction times faster correct reaction times 
link link heath allowed step size process sampled non normal distribution depending distribution allows error responses faster slower correct responses 
models produce cross correct error reaction times obtained subjects experiments 
models adapted variability parameters trials produce success diffusion model 
fixed criterion counter models recruitment model assume information features stimulus accumulated fixed criterion choice tasks criteria 
models differ random walk model absolute relative criteria 
fixed criterion counter models predictions odds experimental data predict reaction time distributions normal criterion number counts increased 
predict maximum reaction time number counts process criterial number counts counter plus experimental data suggest fixed upper limit reaction time 
fixed criterion counter models usually predict negatively skewed error reaction time distributions contrary experimental data 
accumulator models smith vickers vickers generalizations fixed criterion counter models assume evidence continuous discrete counts time steps discrete 
smith vickers extended early accumulator model assuming time steps exponential distribution 
modification produces correct error reaction time distributions positively skewed 
criteria increased reaction time distributions normal contrary experimental data 
smith vickers assume response criteria variable starting point random walk model diffusion model variability increased criterion position distribution criterion positions skewed predicted reaction time distributions skewed model capable fitting experimental data reported 
unclear parameter invariance diffusion model obtained accumulator model 
popular alternative sequential sampling models class strength latency models distance criterion models 
models assume reaction time function distance stimulus value decision criterion 
popular instantiation class reaction time exponential function distance criterion murdock ashby maddox see vickers 
model predictions accuracy reaction time consistent data model prediction error reaction time distributions negatively skewed inconsistent experimental data 
model predicts errors slower correct responses peak strength distribution produces correct responses away criterion tail distribution produces errors 
shows schematic mapping strength normal distribution reaction time exponential strength time transfer function 
resulting reaction time distribution negatively skewed 
murdock shown adding criterion variability cf 
variability starting point ratcliff van zandt mckoon page drift diffusion model help produce distributions experimental data clearly help case error reaction times produce correct predictions cases 
insert connectionist models appealing features connectionist models provide explicit mechanisms learning instances stimuli feedback models learn mappings stimuli responses mappings lead predictions probabilities different responses different stimuli 
connectionist models process response stimulus chosen involves iterative recycling activation natural models generate predictions response time measures 
predictions response time generated earlier models wrong predictions models largely untested 
discussion treatment response time measures connectionist domain review earlier models problems proceed test models mcclelland grain framework mcclelland movellan mcclelland anderson bsb model response time response probability data experiment 
experiments examine models fuller context time dependent learning measures 
cascade model mcclelland 
cascade model models examine possibility information flowing continuously discrete stages processing 
key assumption stage processing previous stage terminated 
model ability account mean response time data cascade notion posed challenge additive factors logic sternberg separate effects different variables different stages processing 
cascade model eventually shown shortcomings seriously predicted proportion trials processing terminate ashby 
added assumptions processing terminated model adequately predict reaction time means variances ashby 
model set stage dynamic models activation flow series stages 
interactive activation model mcclelland rumelhart 
interactive activation model designed explain performance simple letter word identification tasks 
immediate successor cascade model designed predict response times decisions probabilities making different decisions 
model information words represented network levels nodes letter feature line segment nodes letter nodes word nodes 
features stimulus word input feature level activation flows nodes different levels amount activation asymptotes paradigms stimulus masked reaches maximum 
asymptotic maximum amount activation word letter level decide word response 
main problem model current context deterministic specific stimulus word model produces activation value 
model produce errors placing variability decision rule probability particular response relative amounts activation possible responses luce choice rule luce 
changing model deterministic output activation value word trial require assumptions introduce variance stopping rule processing map activation values decision criteria 
proposals cohen dunbar mcclelland jacobs add assumptions existing models attempt overcome problems proposals limited specific domains 
seidenberg mcclelland model 
interactive activation model model designed explain decisions words model represents information nodes distributed fashion 
level nodes represents orthographic information phonological information third hidden layer nodes 
orthographic information word stimulus input orthographic layer activation flows nodes layers single iteration output computed 
naming response output phonological layer compared correct representation word best matching word chosen 
lexical word nonword decisions output pattern activation orthographic layer matched representation input system stimulus 
match indicates word decision bad match nonword decision 
assumption decisions relate response times response time depends quality match poorer match leads longer decision times 
assumption note model obvious support predictions full range timedependent variables 
matched filter model anderson 
earliest neural network models designed deal psychological data matched filter model 
developed explain performance fixed set sternberg paradigm sternberg 
relevant version paradigm small set items designated receive positive response set items designated receive negative response 
model assumes items represented vectors features learning formation positive filter summing vectors positive items formation negative filter summing vectors negative items 
vector test item compared filters dot products vector filters 
output comparison process accumulated time reaches positive negative criterion 
model accurately predict mean response time function size positive set predict sequential effects designed account accuracy shapes reaction time distributions 
anderson bsb model seen update matched filter model attempt account fuller range measures 
summary 
early connectionist models designed address questions structure process various cognitive tasks 
models deal reaction time dependent variable primary concern behavior mean response time experimental conditions models concerned able account detailed characteristics distributions response times relationships reaction time accuracy 
models impressive accomplished newer models designed take step full range phenomena associated reaction time accuracy 
models grain framework grain framework mcclelland designed guide construction models decisions interactive processes evolve time variability built processing 
goal account response time response probability decisions required task 
original grain proposal specify learning mechanism learning key component connectionist models especially component competing information processing models examine response time probability learning leads asymptotic performance 
mentioned reason choosing signal detection task task connectionist models expected learn task allowed measurement usual response time accuracy dependent variables 
earlier connectionist models processing grain models assumed take place continuous manner information activation flows gradually interactively connectionist network 
key feature grain models different earlier models variability processing 
time activation input node amount input includes random number 
addition random noise guarantees time reach decision choice response vary stimuli repeated instances single stimulus 
grain framework articles 
mcclelland provided set general principles define grain approach movellan mcclelland see applied grain architecture problem learning produce variable outputs corresponded random variables probability distributions 
grain provides general modeling framework single specific model faced multiple options developing grain models test data experiment 
sections follow explain choices 
ratcliff van zandt mckoon page grain models activation flows network series cycles 
layer model activation flows input output hidden layers 
cycle stimulus activation stimulus input node input layer transmitted nodes hidden layer 
net amount activation input node sum activation values nodes connected weighted connection strengths noise added sum transformed nonlinearly produce activation values 
hidden layer activation transmitted output layer 
activation stimulus maintained input layer subsequent cycles activation input layer output layer transmitted hidden layer activation hidden layer transmitted back output layer 
cycle constitutes iteration time step 
cycles continue criterion level activation reached output layer 
response time determined number cycles reach criterion 
learning experiment versus learning prior experiment 
perform signal detection task connectionist network trained numerosity judgment trained discriminate large number asterisks small number asterisks 
purpose fitting data experiment goal fit response time accuracy data fit data diffusion model 
models constrained speed subjects learn perform task 
experiments added constraint predictions reaction time accuracy tested context experimental data learning 
tried training methods grain models training take place course trials simulated experiment take place prior experiment 
tested alternatives 
movellan mcclelland examined ability network learn produce output values approximate various probability distributions normal distributions binary distributions exclusive 
number learning trials typically layer model number trials session experiment grain model tested followed movellan mcclelland learn trials simulated experiment 
similar architecture similar error correcting learning rule rate learning movellan mcclelland 
alternative training method suggested mcclelland mcnaughton reilly see ratcliff argued learning structure domain numerosity occur extended time period system exposed possible stimuli multiple times random order 
argument performance signal detection task function long term knowledge numerosity 
second grain model network trained produce possible number asterisks probability number drawn high distribution 
training occurred prior tests model simulated trials experiment 
number layers 
choice layer model layer model grain framework movellan mcclelland usher mcclelland 
layer model sufficient data signal detection task main investigations chose implement layer models 
layer model model tasks word identification lexical decision word naming mcclelland rumelhart plaut seidenberg mcclelland wanted generalize tasks :10.1.1.23.4807
wanted possible model apply tasks asterisk stimuli example different matching task displays subject required decide high low number asterisks respond display high number low number respond different 
ratcliff examined task kind stimuli varying brightness 
layer model able perform task logically equivalent exclusive task layer models perform see movellan mcclelland discussion plaut :10.1.1.23.4807
learning rule 
perform signal detection task network trained respond high high stimulus low low stimulus 
grain framework specify learning rules followed movellan mcclelland version contrastive hebbian learning algorithm similar mean field learning algorithm peterson hartman see application mccloskey 
iterative error correcting rule behaves manner similar backpropagation rule 
net input averaging versus activation averaging 
net input node calculated running average weighted sum prior averaged net input current net input mcclelland running average amount activation node mcclelland rumelhart 
methods smooth large fluctuations amount activation 
focus report results 
grain model learning experiment choices architecture learning just described determined grain models evaluated signal detection data 
model appeared project plausible fit data allowing generalization cognitive tasks layer model learning place experiment prior 
assumed subjects come experiment having learned represent numerosity learn experimenter defined high low response probabilities appropriate task 
discuss model 
architecture form training various ways stimulus information numbers asterisks represented 
scheme chose represent stimulus composed asterisks vector length allow numbers element set elements set elements set 
elements set allow generalization similar numbers number elements called window size parameter model 
layers model input layer nodes represent stimulus input vector output layer node indicate response movellan mcclelland hidden layer nodes hidden layer node connected output node input node 
initial weights connections nodes set random numbers uniform distribution size range parameter model 
activation value single output node determined response high node activation value near low near 
note response criteria function similar boundaries high low responses diffusion model 
illustrates grain model full description model equations appendix 
insert simulations number trials subjects received experiment 
check accuracy simulations authors article implemented model independently 
stimuli input model trials sequentially stimulus feedback just subjects experiment 
parameter adjustment achieve best possible fits model data accomplished hand insufficient computer power embed model minimization routine see discussion section section 
contrastive hebbian mean field algorithm phases free phase activation stimulus held constant input nodes clamped phase activation held constant input nodes desired output held constant output node details see appendix 
simulate decision process stimulus vector corresponding stimulus input network input level activation allowed flow input output layers hidden layer hidden layer output layer 
initial version model input node time step running average net input 
free phase cycles continued value output node reached positive near high negative near low criterion 
number cycles represent response time 
feedback system entering output node activation allowed flow output node clamped feedback value ratcliff van zandt mckoon page input nodes original stimulus values 
contrastive hebbian algorithm modify weights connecting pair nodes amount modification depending difference product activation values nodes connected weight free phase clamped phase multiplied learning rate parameter see appendix 
free parameters model plus parameters scale number iterations time 
window size number positions stimulus value set 
absolute value criterion response output node positive value high response negative value low response 
learning rate parameter contrastive hebbian algorithm parameter absolute value limits uniform distribution random numbers initial weights chosen 
parameters determined flow activation obtain running average net input node proportion node prior average input added node current input 
standard deviation value noise mean added net input 
parameters controlled iterative process see appendix 
simulations run check different sets parameter values fits model data sections come successful set parameters shown table 
fits model sets response criteria values show criteria manipulations account behaviors different subjects 
insert table model fit asymptotic data trials discarded 
took trials simulated data relatively stable example high responses slower trials asymptotic value low responses trials simulation 
extreme values asterisks occur took trials simulation accurately classify 
slow learning problem documented discussed experiments 
response probability sequential effects shows model predicted probabilities low responses experimental conditions possible numbers asterisks sequential conditions 
top panel shows functions criteria responding set value activation output node bottom panel shows functions criteria 
general shape functions data 
predicted sequential effects larger data importantly dependent prior feedback data show sequential effects dependent prior response 
way change behavior model predict sequential effects prior feedback feedback controls weight changes learning network trials 
insert response probability mean reaction time shows model predictions response time function experimental condition 
predictions follow data correct responses response time slows number asterisks nears point high low distributions cross 
data error responses speed errors extreme model predicts error responses slow errors extreme 
misprediction indicated fact response time functions types responses monotonically increasing high response probability low response probability 
insert figures conversion latency response probability functions shown 
generally model predicts monotonically increasing functions correct error responses data show inverted shaped functions 
parameter manipulations affected pattern results 
reaction time distributions shows predicted reaction time distributions levels response probability 
predicted distributions skewed right matches data 
extent skew measured distance distribution tail mode shows reasonable approximation experimental data 
insert figures shows predicted hazard functions distributions shown 
generally show rapid rise peak followed slight fall data followed rise tail data 
distributions response probability rise tail extreme tail distribution numerical instability estimate hazard function 
distributions response probability rise tail occurs right mean reaction time condition 
case rise hazard function occurs region extreme tail distribution attributed instability see glaser discussion hazard functions 
reaction time distributions look plausible hazard functions low probability conditions differ significantly data portion distribution mean 
summary model captures features data fails important respects 
incorrectly predicts sequential effects prior feedback prior responses incorrect prediction comes learning rule sequence simulated experimental trials 
model mistakenly predicts error responses slower correct responses fails capture data inverted shaped latency probability functions 
fails predict correct shape hazard functions 
parameter values model correctly account sequential effects latency probability functions concern parameter values led success 
fits model best find 
altering parameter values led problems comparisons described relative results learning rate weight modification contrastive hebbian algorithm 
learning rate reduced fewer extreme errors large sequential effects data 
learning rate increased extreme stimuli fast reaction times variability distribution reaction times just single value responses 
examine happens learning turned initial training learning rate trials reduced trials kept value 
trials learning rate reduced size sequential effects reduced response probability extreme stimuli variability response time number steps criterion errors stimuli response time range steps stimuli produced errors 
errors slow relative correct responses steps slower extreme errors 
window size stimulus representation 
small processes terminate minimum number steps variability reaction time 
response probability chance range asterisks 
window size increased results window size 
initial weights 
initial weights increased range range behavior model change significantly 
running average parameter 
changing size parameter alters minimum reaction time significantly affect qualitative behavior model respect reaction times response probabilities change sequential effects 
response criterion 
response criterion decreased processes finish iterations 
response criterion raised reaction time slowed little qualitative behavior ratcliff van zandt mckoon page model altered 
annealing schedule 
iterative decision process transformation net input activation adjusted fixed net input larger effect activation iteration called simulated annealing 
formula mapping net input activation see appendix net input divided constant 
contrastive hebbian algorithm value constant reduced successive iteration size net input larger effect activation 
practice done multiplying iteration peterson hartman 
eliminating annealing large difference reaction times 
responses extreme conditions iterations non extreme conditions processes terminate iterations 
parameter reduced 

see table responses iterations extreme conditions extreme conditions processes terminate iterations 
produces distributions tails far long relative mean 
possibilities investigate grain model learning takes place prior experimental trials 
discuss various alternatives model learning experiment 
appear minor alterations large impact predictions model 
running average computed activation net input maximum value activation changed result large value noise proportion new input added running average activation value 
running average net input large value noise change activation amount prior value 
means changes activation gradual running average activation net input 
predictions model running average activation similar predictions running average net input 
accuracy functions similar sequential effects followed prior feedback prior response 
latency probability functions increased correct error responses conditions extreme errors fast reflecting responses thousands simulation 
latency probability functions similar inverted shaped functions exhibited data experiment 
reaction time distributions extremely long tails small proportion processes terminate iteration limit computer program tests iterations 
means distributions tails distributions mode experimental data mean occurred little tail mode 
corresponding hazard functions highly peaked fell asymptotes quarter peak contrast data asymptotes little peak 
parameters version model shown second column table 
considered ways representing number displayed asterisks 
scheme number displayed asterisks elements element input vector randomly chosen assigned value elements assigned value 
second scheme thermometer representation number asterisks displayed represented elements starting 
example number elements assigned nodes 
problem schemes response probability failed reach ceiling floor number asterisks extreme shaped response probability functions lower slope data 
random assignment representation source problem fact randomly assigning inputs input nodes turning depending random assignment produces consistent mapping number asterisks stimulus input representation learning 
thermometer representation low units inconsistent training trained low response low stimuli trained high response high stimuli 
schemes manipulated learning rate proportion old net input averaged new net input amount variability added activation annealing scaling parameter nonlinear transformation net input activation manipulations altered results 
possibility model predictions improved account variability subjects encoded stimuli 
test window representation assumed stimulus number number encoded normally distributed mean standard deviation 
effect model predictions 
effect adding variability computationally equivalent increasing standard deviation high low distributions factor 
may ways produce sequential effects consistent data 
example assumed response alternatives primed prior response 
mechanism model predicts large sequential effects prior feedback larger effects observed data added effects produced response priming 
examined layer version model 
mainly interested behavior layer models layers required perform interesting behavioral tasks mcclelland rumelhart seidenberg mcclelland 
completeness note results layer model 
layer model input node connected output node directly 
simulations produced results similar results layer model net input averaging activation averaging 
example net input averaging version functions monotonic inverted shaped data 
sequential effects response probability showed large effects prior feedback just layer model 
reaction time distributions long tails distributions response probability hazard functions rose immediately mean reaction time data functions roughly constant mean reaction time 
sum find alternative parameter values alternative assumptions structure processing improve predictions grain model learning experiment 
cases manipulation fix deficiency fix problem introduced improved predictions 
appeared possible evaluated manipulations jointly produced results better 
model predicts sequential effects dependent prior feedback 
grain model learning prior experiment major problem grain model just discussed learning rule led sequential effects dependent prior feedback 
attempt avoid problem examined grain model weights connections input hidden output nodes fixed training prior simulation experiment 
question model train distinguish high low numerosity 
diffusion model underlying variable controlled performance probability stimulus came high versus low distribution 
decided train grain model probability 
model trained input number asterisks give output linear transformation probability number asterisks came high distribution 
algorithm model variability activation net input 
model architecture model learning experiment input layer nodes hidden layer nodes output layer node shown 
pre experiment training phase stimuli network random order 
stimulus number asterisks input nodes network trained produce output activation stimulus probability provided output node probability stimulus came high distribution scaled lie 
scaling necessary training network produce activation values extreme stimuli produced ceiling floor effects reaction time example stimuli asterisks processes terminated ratcliff van zandt mckoon page exactly number steps 
learning rate contrastive hebbian algorithm running average parameter annealing scaling parameter zero initial weights varied iterations output activation target activation noise added system 
parameters sufficient training trials run weights produced transformed probability values output node target values 
training behavior model evaluated experimental trials network just human subjects 
variability noise added net input transformation activation 
parameters model connection weights fixed prior training adjusted hand model produced results qualitatively matched data 
response times errors faster response times correct responses experimental data subject subjects 
means model failed produce asymmetric inverted shaped functions typical data 
attempt produce experimental pattern obtained subjects slow error reaction times relative correct responses added new source variability model equivalent drift variability derived experience diffusion model 
specifically assumed stimulus encoded way trials number asterisks stimulus model number number plus number drawn random normal distribution mean standard deviation 
example stimulus asterisks input model asterisks asterisks number 
variance mimics variation instances stimulus part diffusion model parameter 
addition variable encoding model job accounting reaction times correct error responses probabilities responses minor exceptions shapes reaction time distributions hazard functions 
earlier model model account sequential effects 
parameters model obtain qualitative fits shown third column table 
response probability reaction time reaction time distributions 
response probability function number asterisks mimics human data 
mean reaction times slowed number asterisks crossover point human data extreme errors faster correct responses errors conditions closer crossover point slower correct responses 
latency probability functions model quite similar subjects 
insert figures reaction time distributions skewed right just human subject data 
fastest responses number iterations conditions slow mean reaction time increases experimental data ms effect subject 
hazard functions appear increase level high probability conditions variability tail distribution estimates unstable 
hazard functions response probability bottom panels rise rapidly immediately mean reaction time match experimental data 
sequential effects 
model produce sequential effects single assumption added produce different patterns sequential effects subjects experiment 
problem different subjects produced different patterns sequential effects showed effect prior response produced opposite response greater probability produced response greater probability 
assumption changing criteria residual activation trial specific subject 
discussion sequential effects details reaction time distributions model provided reasonably qualitative explanation data experiment 
possible knowledge gained diffusion model fits stimulus probability appropriate training function 
diffusion model provided training function able get grain model guesswork select appropriate training function 
course stimulus probability intuitively obvious candidate training function pointers gained earlier literature probability matching literature estes 
important general point intuitions training functions extend possible situations 
payoffs instructions produce bias response see ratcliff hacker stimulus probability correct training function 
finding training function difficult structure problem linear criteria nonlinear boundaries categorization space maddox ashby difficult subjects discover true statistical structure problem responded function invention 
intuition reliably provide training function connectionist model question possible automatic learning algorithm search proper function 
raises complex difficult issues 
idea model produce qualitative fits data required 
model produce qualitative fits unable produce fast errors contrary experimental data automatic fitting algorithm produce systematic distortions 
example grain model investigated learning experiment systematically mispredicted data error response times slower correct response times 
automatic fitting algorithm distort fits correct response times trying fit fast error responses 
problem determining model produce qualitative fits greater problems arising complexities multiple measures performance large numbers parameters 
situations dependent variable training function closely related data fit 
example data fit probabilities subjects choose versus response probabilities closely related activation values output nodes output activation values produced feedback response correct individual trial 
training function rely related feedback response correct current state modeling data fit include reaction time response probabilities learning algorithm access reaction time data 
consequence training algorithm having access reaction time data fitting model data necessary train test adjust parameters model fits accuracy reaction time train test adjust 
multiple recycling training testing parameter adjustment difficult difficulty greater large number parameters required learning phase parameters training function plus learning rate initial weights learning criterion parameter logistic net input activation transformation test phase logistic net input activation bias unit running average parameter proportion old activation net input added new value annealing parameter amount noise response criteria variability mapping stimulus input value equivalent drift variability diffusion model mapping number iterations reaction time non decisional component reaction time 
example experiment stimulus values 
assigning different parameter stimulus value result unmanageable fitting program fitting parameters take years 
reducing number stimulus values spanning digit range way producing faster learning program 
reduction number parameters practical issues speed 
diffusion model fast workstation single set predictions generated min 
optimal set parameter values produce fits data obtained hours iterations 
contrast connectionist models examined takes hour produce set predictions 
automatic fitting program take weeks months produce set optimized fits set data number parameters reduced stimulus values plus parameters model 
complicating factor connectionist models distributed representations 
means trained stimuli ratcliff van zandt mckoon page learning algorithms learning stimuli individually result catastrophic interference mccloskey cohen ratcliff algorithms forgetting bsb model early training forgotten training 
case diffusion model stimulus separate drift rate relationships stimuli assessed model fitted obtain separate drift rates 
setting aside problems developing algorithms automatically find training function connectionist model problem plausibility basic notion networks 
implicit search appropriate function idea people perform tasks asterisk signal detection task preexisting networks networks parts networks usher mcclelland 
plausibility idea subjects know lot relevant experimental task come experiment learn task rapidly experiment 
problem task network appropriate properties chosen assembled multitude pre existing networks 
example long term knowledge numerosity different tasks tasks statistical structure performed different kinds dimensions stimuli 
assemble network particular task different issues considered dimension involved numerosity tone frequency word familiarity scale involved task numerosity function relating stimuli response choices signal noise represented 
time connectionist modeling networks begun address issues 
connectionist model reasonable job accounting experimental data leaves important larger questions unresolved 
model approach embodied grain modeling framework give account assemble networks specific tasks learning function networks chosen 
anderson bsb model anderson applied brain state box model bsb anderson silverstein ritz jones explain reaction times letter string matching paradigms 
bsb model assumes stimulus item represented vector elements called state vector memory prior experience represented matrix elements 
memory single item matrix composed products pair elements vector product vector transpose 
memory items single matrix sum matrices items 
test item input system vector multiplied memory matrix vector produced output 
test item previously learned system output vector matches input vector variability depends items learned 
signal detection paradigm vectors stimuli divided parts elements representing stimulus elements representing response learned 
high stimuli represented setting response elements low stimuli represented setting response elements 
test item input system response elements state vector set zero vector multiplied memory matrix produce output response elements 
new state vector multiplied matrix produce better representation response part vector process iterated representation response reached criterion 
number iterations taken reaction time response 
anderson iterative vector matrix multiplication process augmented factors produce stability iterations 
new state vector iteration product multiplication plus proportion original input vector input part vector change iterations proportion previous state vector 
updating rule gx df initial input constant little constants 
vector time decision process weighted sum vector time original input product vector time memory matrix 
second element output vector stimulus response absolute value exceeded limit replaced limit value 
corresponds bounding box brain state box 
decision process terminated elements response portion vector reached response criterion number iterations exceeded maximum 
simulating signal detection task response produced feedback provided 
correct response entered response portion vector high low stimulus stimulus portion vector 
memory matrix updated hff constant representing memory decay constant 
matrix represents sum products element vector element vector 
decay element contain number times element paired element minus number times element paired element cf probability matching anderson 
interactive process bsb deterministic stimulus memory matrix output produced 
anderson argues positive feature model variability performance comes variability sequence stimulus feedback pairs model trials 
implement model signal detection task input vectors elements stimulus elements response 
response element criteria termination iterative process values 
stimulus representation grain models 
number elements window size stimulus number set elements distant set 
value window size parameter 
stimuli greater fewer elements non zero example stimuli elements including stimulus number elements higher stimulus value nonzero 
vector normalized size sum nonzero elements equaled 
model tested sequences stimuli just subjects experiment 
initially memory matrix set zero trials model begins responses trials produce response 
trials performance begins asymptote decay memory matrix decay parameter 
discussion learning postponed experiments 
parameters fits decay constant matrix updates limit value vector value response criteria set reaction time er mapping cycles reaction time set respectively 
adjusted parameters hand produce behavior close possible qualitative trends observed data especially response probability functions reaction time distributions 
response probability sequential effects shows model predictions probabilities low responses experimental conditions 
functions similar data grain model bsb predicts response affected prior response prior feedback 
bsb model weights new inputs feedback prior trial prior memory updating rule sequential effects response system produced time feedback 
model modified predict different patterns sequential effects observed different subjects 
subject showing bias away prior response model weight opposite prior response damage learning 
insert response probability mean reaction time latency response probability function shown diverges data model inability produce inverted shaped function function monotonically increasing response probability decreases 
data predicted reaction time functions show slow errors errors faster correct responses 
data error responses extreme stimuli high numbers ratcliff van zandt mckoon page asterisks low ones produced model 
result model deterministic processing variability coming random sequence stimuli fact extreme stimuli consistently assigned response 
insert figures reaction time distributions shapes reaction time distributions produced model shown similar experimental data skewing right 
data hazard functions peaked fall slowly peak asymptote peak 
insert figures learning initial learning took trials performance reached asymptote reaction time 
response probability quite variable trials performance asymptote 
adding variability way bsb model fails fitting inverted shaped functions error reaction times slower correct reaction times 
decided try adding variability processing order produce small proportion fast errors better mimic data 
add variability features input representation allowed randomly reverse cycle processing 
example stimulus asterisks reversals give element element 
problem modification feature reversals extreme elements began dominate learned infrequent encounters stimuli huge increase probability error stimuli errors 
way tried get model produce fast infrequent errors extreme stimuli reverse response assignment small probability 
stimulus generated low distribution time associated high feedback 
produce errors extreme tails slow ones 
reasonable way get bsb model produce extreme errors fast reaction times 
model sums products elements introducing errors randomly reduces accuracy extreme stimuli leading errors stimuli 
summary bsb model qualitatively mispredicted error reaction times problems sequential effects subjects 
unable discover way modify model deal failures 
model simple straightforward model may additional assumptions allow deal better experimental data 
unable find point 
difference model grain models bsb model constrained grain models 
bsb model fit reaction time data theoretical development model needed 
experiment experiment designed test diffusion model connectionist models full range accuracy response time measures interactions 
diffusion model domain performed 
connectionist model learning prior experiment performed part able diffusion model guide applied data 
experiment designed focus learning domain connectionist models 
ways response time accuracy interactions complex central models looked experiment 
data fits models context turn learning issues experiments 
experiment ask quickly subjects adjust probabilities responding high versus low new feedback conditions 
experiment ask quickly experiment subjects arrive asymptotic performance 
models tested experiment data experiments offer different constraints 
models successfully account latency probability functions experiment bsb grain model learning experiment learning data chance show models 
diffusion model learning mechanisms way explain subjects learn respond correctly asterisk task switch feedback scheme 
diffusion model able explain asymptotic data probability distributions underlying performance different blocks trials different experiment 
diffusion model provide coherent account shifts drift rate function shifts probability distributions changes parameters conditions 
turns model successful product find model explain asymptotic behavior assuming kinds parameter changes required explain sequential effects 
connectionist model learning prior experiment guided diffusion model application data experiment turns true experiments 
just diffusion model mechanism dealing dynamics switching probability conditions dynamics sequential effects explain asymptotic behavior appropriate added assumptions 
preceding discussion raises question construed connectionist viewpoint 
overview aim experiment examine speed subjects switch middle experimental session feedback scheme 
experiment stimuli distributions high low chosen equally true conditions experiment refer equal bias condition 
conditions experiment stimuli chosen distributions unequally stimuli chosen low distribution probability high distribution probability low bias condition reverse low distribution probability high distribution probability high bias condition 
conditions switched middle session low high bias vice versa 
drawing distributions unequally consequences 
stimuli block trials condition stimuli 
stimulus drawn distribution draws biased feedback consistent distribution 
example condition stimulus crossover point distributions change crossover point experiment equally high low feedback 
high bias condition stimulus chosen high distribution low distribution high low feedback low bias condition chosen low high distribution 
method subjects 
subjects northwestern university undergraduates male female paid participation 
normal corrected normal vision 
stimuli 
number asterisks trial drawn low distribution mean high distribution mean 
standard deviation distributions giving value 
distributions crossed number 
distributions changed slightly experiment give observations condition central region asterisks 
procedure 
procedure experiment monetary payoff scheme motivate subjects performance 
subjects encouraged responses quickly told goal maximize total number points earned course experiment 
points awarded correct response point subtracted incorrect response 
subjects paid base rate session told pay supplemented total number points earned ratcliff van zandt mckoon page points 
block trials subject earn points additional cents pay 
design 
subject performed sessions approximately weeks 
session comprised blocks trials 
sessions rest breaks inserted blocks experiment 
practice sessions stimuli drawn low high distributions equal probability experiment 
remaining sessions blocks organized follows blocks probability choice distributions practice sessions 
sets blocks implement switches bias condition 
set began block blocks high bias low bias opposite bias blocks 
sets possible number blocks preceding switch 
sets switch high low bias reverse 
order sets assignment direction switch set random constraint equal number high low low high switches possible numbers blocks preceding switches 
results data analyses trials response times ms greater ms discarded data 
high low bias blocks trials collapsed experimental results symmetrical 
done subtracting number asterisks high bias trial reversing subject response 
allowed high responses high bias condition low responses low bias condition combined 
meant data terms preferred responses high responses high bias condition low responses low bias condition non preferred responses low responses high bias condition high responses low bias condition 
example stimulus high bias condition preferred response high stimulus low bias condition preferred response low 
data stimulus high bias condition response high combined data stimulus low bias condition response low produce average response probability response time 
similarly data stimulus high bias condition response low combined data stimulus low bias condition response high produce average response probability response time 
main new result experiment subjects adapted rapidly switch bias condition 
shows probability subjects gave preferred response switch high low bias conditions 
curves divide stimuli groups asterisks asterisks asterisks 
middle range subjects time give response trials biased high high bias condition low low bias condition 
asymptotic probability preferred response 
shows reached asymptotic probability trials switch 
switch probability preferred response trials switch probability preferred response moved thirds way asymptote 
insert stimuli preferred response low bias condition high bias condition moved asymptotic probability prior switch reach probability switch trials 
non preferred stimuli moved asymptote switch asymptote switch trials 
sum adaptation rapid trials required move way asymptote 
adaptation connectionist models grain model learning prior experiment mechanism learning experiment course give account speed subjects switch bias condition 
examine bsb model grain model learning experiment adapt switches rapidly subjects trained models stable performance parameters experiment 
means models problems accounting latency probability functions experiment issue ability account adaptation 
models trained mimic low bias condition chose stimulus received mainly high feedback condition model low feedback trials 
trained model stimulus low feedback model produced mainly low responses stimuli mainly high responses stimuli crossover point 
mimic switching high bias condition stimulus trials trial high feedback allowing model chance learn respond mainly high stimuli 
trial stimulus checked stimuli times see average response model gave 
bsb model learning trial stimulus crossover point moved trials moved respectively note sequence asymptote sequence trials randomly chosen 
trials bsb model moved thirds way crossover point symmetrical approximating course adaptation subjects shown 
bsb model predicts performance essentially computes running average input plus feedback weighting places current input relative prior input determines quickly adapts changes probability 
parameters model account asymptotic behavior account rapid changes performance come probability high versus low feedback stimuli changed 
grain model parameter values produce fits experiment produced rapid adaptation 
second trial crossover point moved thirds way adapting faster human subjects 
changes large mimic adaptation data cf catastrophic interference mccloskey cohen ratcliff 
grain model possibly better adding kind bias processing example input node set represent bias cohen 
node weighted bias conditions weight response 
scheme require external intervention tell system bias changed 
fitting diffusion model asymptotic data grain model learning prior experiment diffusion model mechanism learning experiment account speed subjects adapt bias switches 
tests diffusion model account asymptotic response latency probability functions reaction time distributions experiment experiment provide probability matching explanation drift rate individual subjects experiment 
data second third blocks trials switch data analyses asymptotic performance 
diffusion model fit data way experiment choosing stimulus conditions fix model parameters generating predictions parameters full range conditions 
half number observations subject experiment variability data 
latency probability functions data model predictions shown 
top figures show latency probability functions response bottom figures show latency probability functions preferred response 
cases responses far left correspond errors defined experiment 
subjects sizable difference reaction time preferred non preferred responses ms reaction time functions relatively flat function response probability 
model accounts trends reasonably capturing error versus correct reaction times differences small 
model job reaction time distributions preferred responses subject 
parameters model shown table 
ratcliff van zandt mckoon page insert figures model fits data reasonably ask model say subjects adjust asymptotic behavior deal unequal stimulus probabilities 
way adjusted shown parameters table move starting point closer boundary preferred response boundary non preferred response cf 
ratcliff 
second way adjusted shift drift rates conform new probabilities stimuli chosen high versus low distributions 
experiment drift rates diffusion model represented probability matching 
drift rate possible number asterisks matched probability number drawn possible distributions 
prediction follows result drift rates change experiment match probabilities reflected high versus low bias conditions 
data confirm prediction 
drift rates plotted function number asterisks data low bias condition combined data high bias condition flipping number asterisks scale midpoint combined 
subjects drift rates lie virtually top probability function high bias condition probability transformed experiment 
theoretical probability functions high bias conditions derived density functions controlled assignment feedback responses functions shown bottom panel 
example case cross point distributions high bias condition crossover point 
shift reflected probability curves top panel midpoints probability zero point left hand axis correspond asterisks respectively 
stimulus value corresponds drift rate zero thought criterion setting drift rates high low responses 
stimuli criterion stimulus value average positive mean drift value produce high response average stimuli extreme average negative drift rate produce low response 
zero point adjustable criterion setting response boundary positions applications explain effects varying instructions payoffs probability 
criteria setting diffusion model center debate explain performance different matching task proctor proctor rao ratcliff ratcliff hacker 
debate difference different response times reflected stage processing proctor proctor rao result different criteria settings 
ratcliff showed diffusion model explain data recourse separate stage processing adjusting settings zero point drift response boundary positions data sets 
results experiment shows shift drift criterion zero point bottom panel 
sequential effects diffusion model 
experiment subjects showed different patterns sequential effects showed greater probability responding direction prior response showed opposite effect showed sequential effects 
postponed discussion sequential effects point turned explained mechanisms account effects changes probability stimuli drawn high versus low distributions 
sequential effects shown produced changes starting point 
changes starting point subjects respectively give correct differences response probabilities respectively 
changes give ms changes response time far large comparison data 
example reaction time middle points response probability function differences reaction times ms ms ms ms faster repetition response opposed switching opposite response subjects respectively 
shifts starting point drift rate needed 
relatively small change drift rate subject example produces changes response probability magnitude shown see ratcliff produces little change reaction time 
small shift drift rate combined small change starting point model accurately fits sequential effects accuracy response time 
sequential effects explained mechanisms account subjects responses change probability stimuli drawn high versus low distribution 
general diffusion model produces sequential effects response probability accompanied small changes reaction time shifts drift rate 
model produces large sequential effects reaction time smaller effects response probability shifts boundary position 
data shifts boundary position drift rate needed greater lesser degrees different subjects 
sequential effects grain model learning prior experiment 
model learning mechanism sequential effects modeled new assumptions 
examined possibilities 
resetting activation output nodes zero trial portion activation output nodes carried trial case subject experiment sign activation reversed sequential effects reversed relative subjects 
implemented idea setting amount activation carried forward final value trial producing appropriate changes response probability large changes response time 
example assuming scaling factor ms response time activation cycle ms reaction time effect change probability 
contrast effect subject ms response time response probability 
second possibility response criteria vary function prior response way boundary positions varied diffusion model 
change response criteria reaction time difference right ms accuracy difference small 
third possibility assume response produced bias interpret stimulus way previous stimulus subjects opposite way subject experiment 
done input level node take input output prior trial 
effect shifting drift rate function diffusion model see experiment 
combination assumptions response criteria drift rate bias fits data mimicking assumptions fitting diffusion model 
summary 
experiment showed adaptation large changes probability stimuli chosen high versus low distributions relatively rapid occurs trials 
matches adaptation bsb model slower predicted grain model learning experiment 
diffusion model grain model learning prior experiment learning mechanisms account speed subjects adaptation explain asymptotic performance shifted relative bias conditions experiment give account aspects decision process changed pair 
adaptation sequential effects common theoretical explanation models changes performance result switches probability stimuli coming high low distributions modeled shifts drift rate function boundary positions sequential effects modeled way 
experiment experiment designed find quickly subjects learn discrimination task experiments 
provides important constraint connectionist models 
tailor initial learning mimic data able call preexisting networks scale time allowed experimental trials 
subjects tested trials high low stimuli feedback equally 
goal grain model learning experiment bsb model mimic quickly subjects learned 
goal grain model prior training clear 
task solved connectionist process select assemble previously existing network perform task selection number trials required subjects reach optimum performance 
ratcliff van zandt mckoon page subjects 
subjects northwestern university undergraduates participated course credit 
subject performed session trials lasting minutes 
stimuli 
stimuli probabilities presentation exactly experiment 
procedure 
procedure experiment payoff scheme 
initial instructions provided subjects follows experiment shown number asterisks left side screen 
number asterisks range 
kinds asterisk patterns 
low patterns contain average fewer asterisks high patterns 
task determine number asterisks shown screen high low 
patterns contain close asterisks white space ambiguous may called high low computer 
respond high pattern computer thinks low get brief error message vice versa 
subjects told key computer keyboard low response key high response 
moderately extreme stimuli labeled high low examples 
results responses reaction times greater sec ms eliminated data results averaged subjects 
top panel shows response probability function number asterisks separate functions different blocks test trials 
response probability function trials shape block trials 
function similar subjects experiment especially subject 
latency probability functions bottom panel roughly similar shapes blocks trials 
differences responses speed little block block 
block show fast errors extreme stimuli 
sequential effects experiment depend feedback initial learning response probability functions blocks trials experiment show sequential effects feedback subjects learning task 
fact turned true 
effect prior feedback accuracy size effect prior response 
reaction time trends variable conditions 
feedback subjects initial learning aid calibrating task 
responses extreme stimuli block trials indicate quickly subjects learned 
left rightmost points function block trials represent observations response subjects correct responses 
insert discussion interpretation offered data experiment diffusion model subjects discrimination decisions probability matching stimulus probability transformed model drift rate 
data show subjects exhibit probability matching performance earliest trials experiment 
appears subjects quickly calibrate probability matching minimal instructions plus example stimuli 
addition results experiment showed subjects quickly switch probability condition 
connectionist models results show little training experiment 
model structure built allows task performed reasonably accurately outset little learning derived task stimuli feedback 
model accomplish select appropriate pre existing network layers nodes layers weights connections nodes basis instructions couple example stimuli feedback 
output network scaled correct range respond high numbers asterisks give fast correct responses extreme numbers asterisks 
anderson discusses learning issue presentation bsb model different judgments 
model learning trials testing needed bsb simulations article 
bsb model match speed learning human subjects 
likewise grain model learning learn task quickly subjects learn 
data experiments designed constrain possible explanations learning abrupt changes parameters task connectionist frameworks 
indicate prior knowledge needs selecting appropriate networks perform task calibration 
models initially untrained network trouble explaining adaptation shown experiments 
general discussion aims research article evaluate connectionist models diffusion model account reaction time phenomena carry evaluation context connectionist models applied learning adaptation phenomena 
success enterprise demonstrated number significant new findings enumerated paragraphs 
new findings pertinent individual models 
connectionist models example put contact time measures traditionally reaction time research 
diffusion model new insights gained model application new paradigm paradigm chosen part relevance tests learning behavior connectionist models 
new findings general 
anticipated insights gained diffusion model choose training function connectionist models 
appreciated severe constraints imposed connectionist models joint consideration reaction time accuracy learning measures 
significant outcome platform provided research 
explain reaction time phenomena standard set diffusion model new models explain explicitly fit correct error reaction times reaction time distributions accuracy 
research diffusion model needs place general framework explain learning adaptation phenomena mechanisms decision criteria set 
connectionist models goals explaining learning response time developed goals mind simultaneously 
comprehensive difficult research program laid connectionist traditional reaction time models 
diffusion model priori domain diffusion model limited connectionist models encompass learning 
domain success diffusion model explaining empirical data pleasant surprise 
parameters subject model accurately fit correct error reaction times probabilities shapes distributions hazard functions 
addition parameters model account sequential effects 
previously model able explain relative speeds correct error responses diffusion model ability significant advance 
diffusion model offered insights behavior subjects 
importantly model showed behavior related stimulus probability 
intuition suggest subjects match responses probabilities stimuli coming high versus low distributions data directly showed 
model extraction drift rates response time response probability data discovered underlying variable stimulus probability 
subjects shifts trial trial probability condition shifts probability condition explained changes behavior changes boundary position changes zero point drift rate drift criterion follow stimulus probability 
fitting diffusion model led explanations bases subjects decision making 
model provided understanding individual subjects rely underlying information basis decisions produce quite different speed accuracy profiles see ratcliff experiment 
experiment responses subjects slow responses subject fast responses fourth subject intermediate speed 
model ratcliff van zandt mckoon page differences came differences far response boundaries set starting point 
subjects differed speed correct versus error responses subject showed slightly faster errors correct responses subject showed error responses slower correct responses subjects showed errors extreme stimuli faster correct responses errors extreme stimuli slower correct responses 
model different patterns due different amounts variability stimuli encoded trial different amounts variability starting point decision process trial trial 
subject slightly faster error correct response times little variability encoding trials subjects 
subjects kept distances boundaries starting point constant sessions subject reduced 
success diffusion model provides impetus adding variability parameter values models 
example model predicts inverted shaped latency probability function errors extreme stimuli faster errors non extreme stimuli predict asymmetric function variability added appropriate parameters see pike 
just diffusion model error reaction times mixture slower processes higher probability faster processes lower probability average mixture slower single process drift rate mean mixture 
sum diffusion model provided complete accurate account measures time course processing involved decisions number asterisks stimulus high low 
optimistic model provide equally account rapid cognitive binary choice decisions 
major omission model mechanism decision criteria set learned 
standard reaction time models lack mechanism 
points need conceptualize models larger theoretical framework includes mechanisms learning adaptation see nosofsky smith 
connectionist models connectionist models examined explain response time accuracy phenomena learning adaptation 
bsb model produce right error response times learn task quickly subjects 
problems fixed adding variability model find way speeding learning address second problem mean changing model altogether incorporate prior knowledge 
model learning experiment problems accurately produce sequential effects relations correct error response times adaptation stimulus probability condition 
find variations parameter values architectural assumptions remedy problems 
grain model learning prior experiment fit response time response probability data hazard function fits missed significantly adapt stimulus probability condition additional assumptions 
regard similar diffusion model 
fact grain model learning prior experiment diffusion model regarded approximations came diffusion model guide training grain model 
explorations connectionist models completely successful 
believe important findings 
connectionist models considered aiming high hurdle 
intended describe decisions reached time doing account empirical measures decision processes including learning adaptation 
findings suggest difficult model move account mean response time mean probability response versus fuller account 
second important finding arising attempts connectionist models explain response time phenomena better understanding problem originally appreciated interaction kinds measures learning response time 
connectionist models learn feedback trial model response updates weights network basis feedback response correct 
decision processes constrained learning feedback 
holding model responsible response probabilities response times second dependent variable attendant response time measures shapes response time distributions error response times imposes constraints learning algorithm access 
means feedback may sufficient allow model fit response probabilities sufficient produce fits reaction time measures 
adding complexity problem connectionist models assume distributed representation means model trained respond stimulus set 
putting connectionist model learning built processing experiment deriving fits model data involves training model random sequence stimuli trials proceed producing response time accuracy predictions responses model stimuli second comparing predictions empirical data adjust parameters model repeating cycle predictions data match 
model training prior experiment sequence trials weights connections nodes set training model produce functional mapping stimuli output training phase complete testing carried simulated trials produce predictions response measures compared empirical data parameters model adjusted training testing cycle repeated best fit obtained 
current workstations process finding best possible fit connectionist model data take weeks data asterisk task 
reason fit connectionist models data hand automatic search algorithms 
issue initially appreciated difficulty complexity problems imposed requiring models learn adapt changes stimulus conditions 
subjects learned asterisk task quickly models models start completely blank slate learning feedback trials 
pre existing knowledge built 
possibility combine select pre existing networks choose calibrate networks issue tackled connectionist research 
possibility incorporate knowledge structure model model learn extra knowledge required particular combination stimuli task 
approach received serious examination domain 
course explored tiny portion space connectionist models 
grain provides set principles model construction constructed models variants 
learning rules representations assumptions cycling activation tried number possible models large 
grain kinds neural connectionist models addressing reaction time phenomena 
article barely scratched surface application neural connectionist models reaction time phenomena 
comparing diffusion model connectionist models view diffusion model general purpose decision mechanism variety situations tasks decisions models variety kinds information 
example random walk diffusion models simple choice reaction time link link heath smith stone letter matching ratcliff discrimination different tasks ratcliff recognition memory ratcliff categorization nosofsky word identification implicit memory ratcliff mckoon decision making townsend 
diffusion process provides general mechanism application experimental data produces value drift rate experimental condition 
drift rates provide measure latent variable driving decision process long diffusion model accurately fits response time accuracy aspects data 
way competing theories latent variable tested 
example suppose theory experiment subjects basing decisions single dimension stimuli distance criterion 
diffusion model extract drift rates accuracy reaction time data drift rates provide functional form mapping distance ratcliff van zandt mckoon page criterion drift rate 
theory wrong subjects basing decisions example stimulus probability drift rates extracted correspond stimulus probability distance criterion 
falsify theory course situations stimulus probability distance criterion exactly mimic 
diffusion model allows precise shape drift rate function extracted empirical data 
decision process stationary constant time course processing ratcliff smith possible examine drift rate different points time course deadline response signal methods drift rate developing models dynamical changes information driving decision process 
important practical point contrast connectionist models diffusion model allows working backwards response time accuracy data individual condition characterization values stimulus dimension driving decision process 
connectionist models examined 
connectionist models part model corresponds individual experimental condition 
reason stimuli conditions learned means information represented collectively weights network individually 
motivating principle connectionist models interactivity various levels processing features letters words 
different levels usually designed broken apart processing stages decision stage separated computation activation arises input stimulus 
consequence points way backwards individual response time accuracy data derive equivalent drift rate represent latent variable driving decision process 
model fit data forward direction dimension represent stimuli chosen way represent stimulus dimension second model trained task third stimuli processed model give predictions evaluate empirical data 
asterisk task example suppose know subjects decisions distance criterion stimulus probability 
representation stimuli input flexible accommodate possibilities 
form restrictive model fit 
hard see failure point correct functional form training function 
diffusion model choice decisions accounts wide range experimental data including response probabilities correct error response times shapes response time distributions 
model sets standard competing theoretical schemes tested 
diffusion model designed general purpose decision mechanism applied wide variety tasks require single rapid binary decision 
mechanism pointed different sources evidence function task requirements 
decision criteria set topics research 
hope evaluation connectionist models serve challenge spur development models fit full range experimental data 
anticipate development difficult efforts shown complexity involved efforts move simple account mean reaction time mapped single output quantity model full account response time accuracy data simultaneously explaining learning 
evaluating connectionist models easy task models simulations parallel processes involving nonlinear transformations 
problem compounded large amount computer time hours days required simulations produce optimal fits 
despite difficulties possible lay plausible range assumptions representation process doing reveal problems constellations assumptions models tried 
obvious criticism tried possible models 
hope encouraged explore possibilities data starting point aim constructing successful connectionist models 
anderson 

theory recognition items short memorized lists 
psychological review 
anderson 

having neurons thoughts 
eds relating theory data essays human memory honor bennet murdock pp 
hillsdale nj erlbaum 
anderson silverstein ritz jones 

distinctive features categorical perception probability learning applications neural model 
psychological review 
ashby 

deriving exact predictions cascade model 
psychological review 
ashby 

decision rules perception categorization multidimensional stimuli 
journal experimental psychology learning memory cognition 
ashby maddox 

response time theory separability integrality speeded classification 
journal mathematical psychology 
atkinson bower 

mathematical learning theory 
john wiley new york 
pike 

alternative stochastic models choice 
british journal mathematical statistical psychology 


sequential redundancy speed serial choice responding task 
quarterly journal psychology 


estimating response time hazard functions exposition extension 
journal mathematical psychology 


constrained spline estimator hazard function 
psychometrika 
luce 

evidence auditory simple reaction times change level detectors 
perception psychophysics 
townsend 

decision field theory approach decision making uncertain environment 
psychological review 
cohen dunbar mcclelland 

control automatic processes parallel distributed processing account stroop effect 
psychological review 


discriminating semantic learned episodic associations speed accuracy study 
cognitive psychology 
watson 

effects decision criterion latencies binary decisions 
perception psychophysics 
estes 

models men 
american psychologist 
estes 

probability learning 
melton ed categories human learning 
new york academic press 
estes 

response processes cognitive models 
lorch brien eds 
sources coherence text comprehension 
hillsdale nj erlbaum 
cohen 

choice reactions ordered memory scanning process 
eds attention performance pp 

san diego ca academic press 
feller 

probability theory applications 
new york john wiley sons 
ratcliff van zandt mckoon page glaser 

related failure rate characterizations 
journal american statistical association 
ratcliff 

time course item associative information implications global memory models 
journal experimental psychology learning memory cognition 
hacker 

speed accuracy recency judgments events short term memory 
journal experimental psychology learning memory cognition 


neural control voluntary movement initiation 
science 
mewhort 

analysis response time distributions example stroop task 
psychological bulletin 


analysis response time distributions study cognitive processes 
journal experimental psychology learning memory cognition 


inferred components reaction times function duration 
journal experimental psychology 
jacobs 

testing variant interactive activation model different word recognition experiments 
journal experimental psychology human perception performance 
kac 

note learning signal detection 
ire transactions information theory 

krueger 

theory perceptual matching 
psychological review 
kruschke 
exemplar connectionist model category learning 
psychological review 


recruitment theory simple behavior 
psychometrika 


information theory choice reaction time 
new york wiley 
lee 

categorizing externally distributed stimulus samples continua 
journal experimental psychology 
link 
relative judgement theory choice response time 
journal mathematical psychology 
link heath 

sequential theory psychological discrimination 
psychometrika 
luce 

individual choice behavior 
new york wiley 
luce 

response times 
new york oxford university press 
mcclelland 

time relations mental processes examination systems processes cascade 
psychological review 
mcclelland 

stochastic interactive processes effect context perception 
cognitive psychology 
mcclelland 

theory information processing graded random interactive networks 
meyer eds attention performance xiv synergies experimental psychology artificial intelligence cognitive neuroscience 
cambridge ma mit press 
mcclelland rumelhart 

interactive activation model context effects letter perception part 
account basic findings 
psychological review 
mcclelland mcnaughton reilly 

complementary learning systems hippocampus neocortex insights successes failures connectionist models learning memory 
psychological review 
mccloskey cohen 

catastrophic interference connectionist networks sequential learning problem 
bower ed psychology learning motivation advances research theory pp 

new york academic press 
mccloskey 

preliminary results distributed model arithmetic fact retrieval 
campbell ed nature origin mathematical skills pp 

elsevier 


serial retrieval processes recovery order information 
journal experimental psychology general 
maddox ashby 

comparing decision bound exemplar models categorization 
perception psychophysics 
mccloskey cohen 

catastrophic interference connectionist networks sequential learning problem 
bower ed psychology learning motivation advances research theory pp 

new york academic press 
meyer irwin osman 

dynamics cognition mental processes inferred speed accuracy decomposition technique 
psychological review 
movellan mcclelland 

learning continuous probability distributions contrastive hebbian algorithm 
tech 
rep 
pdp cns department psychology carnegie mellon university 
movellan mcclelland 
learning continuous probability distributions symmetric diffusion networks 
cognitive science 
murdock 

analysis strength latency relationship 
memory cognition 


response latencies discriminations recency 
journal experimental psychology learning memory cognition 
nosofsky 

exemplar random walk model speeded classification 
psychological review 
peterson hartman 

explorations mean field theory learning algorithm 
neural networks 
pike 

response latency models signal detection 
psychological review 
plaut mcclelland seidenberg patterson 

understanding normal impaired word reading computational principles quasi regular domains 
psychological review 
proctor 

response bias criteria settings fast phenomenon reply ratcliff 
psychological review 
proctor rao 

evidence different disparity attributable response bias 
perception psychophysics 
ratcliff 

theory memory retrieval 
psychological review 
ratcliff 

group reaction time distributions analysis distribution statistics 
psychological bulletin 
ratcliff 

note modelling accumulation information rate accumulation changes time 
journal mathematical psychology 
ratcliff 

theory order relations perceptual matching 
psychological review 
ratcliff 

theoretical interpretations speed accuracy positive negative responses 
psychological review 
ratcliff 

speed accuracy positive negative responses 
psychological review 
ratcliff van zandt mckoon page ratcliff 

continuous versus discrete information processing modeling accumulation partial information 
psychological review 
ratcliff 

connectionist models recognition memory constraints imposed learning forgetting functions 
psychological review 
ratcliff hacker 

speed accuracy different responses perceptual matching 
perception psychophysics 
ratcliff hacker 

misguided reaction time differences reply proctor rao 
perception psychophysics 
ratcliff mckoon 

speed accuracy processing false statements semantic information 
journal experimental psychology learning memory cognition 
ratcliff mckoon 

similarity information versus relational information differences time course retrieval 
cognitive psychology 
ratcliff mckoon 

counter model implicit priming perceptual word identification 
psychological review 
ratcliff murdock jr 

retrieval processes recognition memory 
psychological review 
ratcliff 

modeling response times choice decisions 
press 
psychonomic science 
seidenberg mcclelland 

distributed developmental model word recognition naming 
psychological review 
smith 

principled models visual simple reaction time 
psychological review 
smith vickers 

accumulator model choice discrimination 
journal mathematical psychology 
sternberg 

high speed scanning human memory 
science 
sternberg 

discovery processing stages extensions method 
koster ed attention performance ii 
amsterdam north holland 
stone 

models choice reaction time 
psychometrika 
strayer kramer 

strategies automaticity basic findings conceptual framework 
journal experimental psychology learning memory cognition 
strayer kramer 

strategies automaticity ii 
dynamic aspects strategy adjustment 
journal experimental psychology learning memory cognition 


elusive tradeoff speed versus accuracy visual discrimination tasks 
perception psychophysics 
thomas 

class additive learning models error correcting probability matching 
journal mathematical psychology 
thomas 

criterion adjustment probability matching 
perception psychophysics 
williams 

theory criterion setting application sequential dependencies 
psychological review 
townsend ashby 
stochastic modeling elementary psychological processes cambridge cambridge university press 
usher mcclelland 

time course perceptual choice model principles neural computation 
technical report pdp cns 
department psychology carnegie mellon university 
van zandt ratcliff 

statistical mimicking reaction time distributions mixtures parameter variability 
psychonomic bulletin review 
vickers 

decision processes visual perception 
new york academic press 
vickers 

discriminating frequency occurrence alternative events 
acta psychologica 
ward 

sequential effects memory category judgments 
journal experimental psychology 


experimental psychology 
new york henry holt 
footnotes note setting successful run fitting process usually requires runs way process result obtained program quite sensitive starting values parameters 
initial parameter values close final values estimates start diverge numerical overflow underflow occurs program terminates 
author note research supported national institute mental health hd mh mh national institute communication disorders dc national science foundation sbr 
wish james mcclelland useful suggestions discussions research 
correspondence concerning article addressed roger ratcliff psychology department northwestern university evanston il 
captions 
probability low response subjects experiment 

mean reaction time subjects experiment 

latency response probability functions data figures subjects experiment 

typical reaction time distributions moderately high accuracy levels responses stimuli asterisk range subjects experiment 

reaction time distributions hazard functions subject sessions upper graphs sessions lower graphs experiment 

hazard functions reaction time distributions shown 

diffusion model parameters model 

diffusion model fits latency response probability functions experiment 
error bars represent standard deviations 

latency probability function predictions diffusion model starting point variability upper panel starting point variability bottom panel standard deviation subject experiment 

diffusion model fits reaction time distributions subjects experiment 
asterisks model predictions 
ratcliff van zandt mckoon page 
diffusion model fits reaction time distributions subjects experiment 
asterisks model predictions 

diffusion model fits reaction time hazard functions subjects experiment 
asterisks model predictions 

diffusion model fits reaction time hazard functions subjects experiment 
asterisks model predictions 

diffusion model drift rates subjects numbers 
asterisks represent probability number asterisks came high stimulus distribution transformed range probability range subtracting multiplying 
probability function asterisks variability derived running simulations trials subject sessions 

example strength latency model mapping strength exponential latency function reaction time distribution 

illustration architecture grain model input representation 

accuracy functions predicted grain model learning experiment data experiment 

reaction time functions predicted grain model learning experiment data experiment 

latency response probability functions predicted model learning experiment data experiment 

reaction time distributions predicted grain model learning experiment data experiment 

reaction time hazard functions predicted grain model learning experiment data experiment 

accuracy functions predicted grain model learning prior experiment data experiment 

reaction time functions predicted grain model learning prior experiment data experiment 

latency response probability functions predicted model learning prior experiment data experiment 

reaction time distributions predicted grain model learning prior experiment data experiment 

reaction time hazard functions predicted grain model learning prior experiment data experiment 

probability responding low predicted bsb model data experiment 

reaction time functions predicted bsb model data experiment 

latency response probability functions predicted bsb model data experiment 

reaction time distributions predicted bsb model data experiment 

reaction time hazard functions predicted bsb model data experiment 

response probability function number trials switch bias condition 
high bias conditions preferred stimuli curve asterisks intermediate stimuli curve asterisks non preferred stimuli curve asterisks 
low bias condition preferred stimuli curve asterisks intermediate stimuli curve asterisks non preferred stimuli curve asterisks 
asymptotic values curves switch curve curve curve minus asymptotic probabilities 

latency probability functions subjects experiment high low probability responses predictions diffusion model 

sample reaction time distributions subjects experiment 
predictions diffusion model shown asterisks 

top panel drift rate estimated data subjects experiment 
data fits average high responses high stimuli stimulus selected high distribution high distribution probable low responses low stimuli stimulus selected low distribution low distribution probable 
shown probability high response bias bias conditions 
transformed range range subtracting probability multiplying 
drift rates follow probability stimulus selected probable distribution 
bottom panel density functions stimuli experiment tall curves represent condition small curve represents low bias condition stimuli selected high distribution time 
probability curves top panel derived bottom curves 

top panel probability responding high different blocks trials experiment 
bottom panel latency probability functions experiment different blocks trials experiment 
