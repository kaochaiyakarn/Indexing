task dependent evolution modularity neural networks michael christian igel marc toussaint institut neuroinformatik ruhr bochum jj bochum telephone fax igel mt neuroinformatik 
ruhr uni bochum 
de connection science vol 
exist ideas assumptions development meaning larity biological technical neural systems 
empirically study evolution connectionist models context modular problems 
purpose define quantitative measures degree modularity monitor evolutionary processes different constraints 
turns modularity problem reflected architecture adapted systems learning counterbalance imperfection architecture 
demand fast learning systems increases selective pressure modularity 
performance biological technical neural systems crucially depends ar 
case feed forward neural network nn architecture may defined underlying graph constituting number neurons way neurons connected 
particularly property architectures modularity raised lot interest researchers dealing biological technical aspects neural computation 
appears obvious emphasise modularity neural systems vertebrate brain highly modular anatomical functional sense 
important stress different concepts modules 
uses word module usually referring fact brains structured cells columns layers regions divide labour information processing various ways revised version gecco late breaking igel toussaint 
elman 
definition modularity viewpoint behavioural cognitive science fodor fodor module specialised encapsulated mental organ evolved handle specific information types particular relevance species 
general modularity viewed fundamental design principle complex systems 
example strategy divide complicated problem smaller subproblems may easier solve problem led modular nns manually de signed architectures consisting expert networks cf jordan jacobs sharkey 
simon gave elaborate arguments favour modularity 
claimed development complex systems requires stable subsystems combined hierarchical manner 
similarly engineer argue easier design complex system smaller building blocks tested systems reliably solve self contained problem 
arguments modularity viewed outcome system design necessity development highly complex systems 
case modularity architecture result product creation process natural evolution case biological systems 
technical framework role natural evolution played evolutionary algorithms task finding suitable network problem solved means evolutionary computation please refer survey yao 
arguments similar ones evolutionary algorithms optimisation nns designed way modular architectures favoured 
achieved indirect encoding schemes adjacency graph describing nn directly stored genome individual non trivial grammar genotype phenotype mapping utilised construct nn genotype kitano gruau friedrich sendhoff sendhoff kreutz :10.1.1.51.7930
believe fundamental unanswered questions 
want know circumstances modular architectures neural systems evolve 
basic question raises example kind tasks modular architectures favourable 
expressive general measures modularity 
article test hypothesis modular problems solved better modular nns non modular ones 
claim appears natural presumed literature 
utilise evolutionary algorithms search nns tackling artificial modular problems 
algorithms explicit bias modular architectures modular networks beneficial evolve 
increase modularity observed hypothesis reconsidered 
measures quantify modularity proposed employed trace evolution modular architectures 
different tasks lamarckian darwinian inheritance considered order test task dependency hypothesis 
particular want verify demand fast gradient learning increases need modularity 
investigation related di els 
studied non lamarckian evolution architectural modularity nns single hidden layer context problem cave kosslyn 
modular architectures develop improve learning performance networks evolve hidden neurons connected output neurons 
extend results introducing finer grained measures different aspects modularity arbitrarily connected feed forward networks considering structure weights investigating influence different constraints scheme inheritance weights goal optimisation 
section define concept modularity nns introduce measures degree 
section simulations succeeding section discuss results 
article ends outlook 
michael christian igel marc toussaint zz sample architecture illustrate measure degree modularity 
modularity neural networks concentrate feed forward nns 
architecture nn defined directed graph vertexes correspond neurons edges indicate flow information 
performance nn depends architecture weights associated edges 
usually gradient optimisation applied adjustment weights search suitable architecture task sets hard discrete optimisation problem tackled successfully means evolutionary algorithms yao 
evolution connectionist models carried improve performance nns technical purposes investigate fundamental issues interaction evolution learning nolfi parisi 
people intuitive idea modular architectures networks 
usually refer structural modularity defining modules highly connected areas sparsely connected surrounding functional modularity see toussaint different approaches 
giving general mathematical definition providing measure degree modularity capturing concepts modularity mentioned section formidable task 
propose general definition modularity focus specific character problems architectures call problem modular solved number non interacting subsystems 
assume nn deal completely separable problem output variables 
ym 
separable means set input variables 
xn divided disjoint subsets ej underlying input output mapping 
ym 
xn bc expressed fj depends variables cj 
definition extension lain shin 
separable problem bc disadvantageous divide network separated ones system bc called highly modular 
context modularity network related strong separate parts interfere 
wc give possible definitions measure 
neurons bc distinguished source information presuming partitioning inputs arc known neurons solely get input directly indirectly subset ej 
arc denoted pure neurons 
example neurons modularity neural networks pure 
neurons mixed certain degree receive input subsets input neurons 
degree modularity defined average degree hidden output neurons calculated procedure tuple di 
di assigned neuron indicating degree dependency neuron different input subsets 
input neurons di zero equal index subset input neuron belongs set di xi xi ej values di hidden output neurons defined recursively di sum runs neurons th neuron gets input wi weight connection neuron neuron neglecting weights assuming equal yields example tuples depicted 
quantify degree neuron means variance di higher higher neuron shown maximum value reached pure neurons 
modularity measure network average variance hidden output neurons weight mapped interval means factor 
weights neglected equation architecture denote measure defined equation 
cases completely separated network corresponds value homogeneous 
case example 
measures focus basic information processing units ow identification separation modules modules defined subgraphs input subproblem definition related concept strongly connected regions low degree surrounding 
definition modules determined bottom fashion procedure appears suitable quantify extend network regarded hierarchical assembly modules 
usually strict partitioning input space scenario division known 
measures alternatively calculated top fashion starting outputs pure neurons partitioning usually known summing output connections th unit calculation 
resulting top version regarded finer grained version measure di 

example monitor top version fully connected nn completely separable problem described 
batch learning described 
shows sub problem learnt nn compare dash dotted line rs 
thin dotted line 
reflected modularity measure initial phase learning problem dominates problem bold solid line modularity strongly increases bold dotted line 
adapting problem restructuring process observed focus task reduced decreases subsequently restructuring increases 
results michael christian marc toussaint weight 
modularity asymmetry sse sse learning cycles development weight modularity learning problem fully connected architecture single hidden layer neurons 
shown medians independent learning trials 
sum squares error training shown separated part 
bolt dotted line depicts top version weight solid line characterises mean asymmetry neurons larger value stronger neurons connected problem 
accordance finding sub problem learnt additionally demonstrate fully connected network development modularity learning measured 
focus evolution modularity 
experimental framework section simulations evolution nns 
experiments distinguish different kinds application optimised nns denoted different tasks 
ann denoted accurate model designed solving single problem instance representing input output mapping induced sample data set accurately possible 
search accurate model frequent application nn optimisation 
task fast learner architecture learn problem short time 
fast learning task sensible dealing number different related tasks problem learnt changed generation remains separable sense section 
results fast learner comparable accurate modelling task performed fast learner optimisation trials changing problem generation problem learnt repeatedly fast possible 
superscripts indicate problem periodically changed 
section see choice task strong impact development modularity optimisation outcomes 
kinds discussed ek 
ik shown learner preferred ik necessary ko cope class problems ko able ko ko number buk problems 
may main differences nns evolved ek 
experimental framework data set choose completely separable problem sense section 
different classifi cation problems boolean inputs 
evolutionary trial class labels corresponding inputs problem chosen randomly half input patterns belong class 
data set contains possible combinations inputs different patterns true false encoded respectively 
neural networks nns slx input linear output neurons 
absolute value measure modularity strongly influenced number hidden neurons connections 
particular turned modular nns evolve case slight selective pressure small nns 
avoid side effects keep size nns constant 
experiments performed hidden neurons sigmoidal activation function connections 
resulting networks large solve problem sparse allow evolution separated modules 
experiments slightly modified architecture sizes yield qualitatively similar results 
maximum number layers architecture restriction hidden neuron connected directly indirectly input output neuron 
training performed means igel improved version rprop algorithm riedmiller braun powerful gradient batch learning algorithm 
aim training minimisation mean squared error mse mean squared difference nn outputs target values yj 
compute classification result network mapping positive outputs true negative ones false 
classification error class rate incorrect classifications 
evolutionary algorithm prior generation individuals initialised random 
connections randomly spread architecture weights initialised values interval 
operator employed generate offspring generation random move single connections connection deleted inserted random position randomly 
means number connections remains constant see 
parents generation selected offspring means selection best offspring form parent population schwefel 
depending task optimisation accurate model fast learner algorithms differ slightly 
case generation individual trained max cycles 
lamarckian trials learning starts weights learning previous generation 
darwinian style evolution architecture encoded individual genome weights prior learning interval 
fitness function te model class mse includes errors learning 
case evolution fast learner generation weights individual randomly interval darwinian style learning takes place long misclassified patterns left maximum number michael christian marc toussaint mean squared error accurate lamarckian accurate darwinian fast darwinian fast darwinian fast learner learning time generations fat darwinian fast darwinian generations relevant terms fitness function 
max cycles 
case fitness fast 
class mse stp denotes cycle learning stopped 
choice effects ha influence primary aim classification levelled ou learning smaller rose 
discussion experimental results graphs represent median results independent trials 
depicts evo lution relevant fitness terms mse stp 
classification error hardly interest vanishes generation models 
succeeding approximately generations algorithms focus reduction learning time mean squared error respectively evolutionary progress observable 
displays development bottom architecture modularity arch 
weight modularity weight experimental scenarios 
find random fluctuations modularity increases fitness decreases 
general increase modularity supports intuition modular networks advantageous modular problems 
observations discussed give detailed insight relation modularity measures experimental scenarios clearly exhibits accurate learners accurate darwinian accurate lamarckian efficiently decrease mse fast learners fast darwinian minimise classification learning time stp reaching small mse 
shows fast learners reach significantly higher architecture modularity arch 
accurate learners rank sum test accurate learners reach significantly higher weight modularity jv weight fast learners discussion experimental results architecture modularity weight modularity fast darwinian fast darwinian accurate darwinian accurate lamarckian generations darwinian fast darwinian accurate darwinian accurate lamarckian generations average modularity parent population 

comparing fast learners see reiterated change problem yields higher architecture modularity arch 
modularity remains unchanged 
understand observations reconsider relation jv arch 
jv weight 
plausible high architecture modularity jv induces high jv weight architecture determines terms summation 
particular jv implies jv weight 
inverse need true network non modular architecture may reach high jv weight having weights cross connections set zero 
sense suitable adjustment weights counterbalance non modular architecture favour weight modularity jv weight fine tuning weights lead small rose explains find algorithms minimise rose accurate darwinian accurate lamarckian high weight modularity close 
contrast fast darwinian learners high weight modularity rose orders magnitude higher reach high architecture modularity see 
task learning fast aim precise adjustment weights aims optimal architectural learn classification 
learning stopped reaching goal fine adjustment weights take place 
fast learners rely appropriate architecture modularity accurate models counterbalance architectural imperfection accurate weight adjustment longer learning period 
summarising evolutionary pressure architecture modularity increased task fast learning 
turn reduction rose modular prob lems increases weight modularity architecture particularly modular induces lower pressure architecture modularity 
effect depends weakly scheme inheritance 
regarding observation architecture modularity jv maximal networks learn number different related sense section section problems 
problem learnt repeatedly certain cross connections necessarily interfere learning depending particular characteristics problem learning algorithm 
problem changed problems undesired connections interfere learning 
important adapt architecture common aspects problems example efficient learning algorithm adjust weight undesirable connection small absolute value 
especially fact batch learning method leads speculation problem modularity detected quickly learning algorithm gradient calculation averages possible cases 
correlations subproblems cancelled 
michael christian igel marc toussaint corresponds increase jv arch 
need fast robust learning demands modular architectures 
addition performed experiments changed conditions considered sigmoidal output neurons linear ones applied mse cost function target values ii mse cost function target values iii cross entropy cost function target values 
case results qualitatively ones 
second third case results reproduced 
reason may unbounded learning dynamics saturated output neurons second third case absolute weight values orders magnitude greater scenarios 
effect explain results mse cost function target values lead different degrees modularity cross entropy cost function 
outlook aims better understanding development functional importance modularity neural networks nns 
introduced measures degree modularity nns allow characterise architecture weight configuration 
measures proved understand development modularity learning evolution nns 
designed experiments order distinguish different optimality criteria learning classification maximally fast learning regression maximally accurate time different inheritance schemes lamarckian inheritance trained weights darwinian weight inheritance 
results show modularity increases nn effi ciency increase task dependent fast learning criterion weight tance significantly enforces development architecture modularity accurate regression criterion weight inheritance enforces development modular weight configuration 
generally gave support hypotheses modular nns beneficial solving modular problems need modularity stronger fast robust learning part task definition 
broader perspective efforts aim evolution large systems modular information processing 
additional measures modularity developed address specific information theoretic aspects modular information processing 
measures guide development new types nns allow modularity priori today common nns 
findings confirm appropriateness modular architectures classes problems evolutionary algorithms specifically designed find modular architectures toussaint 
new mutation operators types encoding recursive grammar encodings kitano gruau friedrich sendhoff sendhoff kreutz analysed developed :10.1.1.51.7930
yon stolen beneficial discussions anonymous reviewers helpful comments suggestions 
acknowledge financial support bmbf loki lb 

simulating evolution modular neural systems 
moore stenning eds proceedings third annual conference cognitive science society mahwah pp 

lawrence erlbaum associates 
di parisi 
evolving modular architectures neural networks 
french eds proceedings sixth neural computation psychology workshop evolution learning development pp 

springer verlag 
elman bates johnson smith parisi 
rethinking connectionist perspective development 
mit press 
fodor 

modularity mind essay faculty psychology 
mit press 
friedrich 
evolutionary method find building blocks architectures artificial neural networks 
sixth international conference information processing management uncertainty knowledge systems pp 

gruau 

automatic definition modular neural networks 
adaptive behavior 

optimization problem classes neural networks learn learn 
yao fogel eds ieee symposium combinations evolutionary computation neural networks pp 

ieee press 
igel toussaint 
task dependent evolution modularity neural networks quantitative case study 
goodman ed late breaking papers genetic evolutionary computation conference gecco pp 

igel 
empirical evaluation improved rprop learning algorithm 
neurocomputing 
press 
jordan jacobs 
modular hierarchical learning systems 
arbib ed handbook brain theory neural networks pp 

mit press 
kitano 

designing neural networks genetic algorithms graph generation system 
complex systems 
lain 
shin 
formation dynamics modules dual tasking multilayer feed forward neural network 
physical review 
nolfi parisi 
learning evolution 
autonomous robots 
riedmiller braun 
direct adaptive method faster backpropagation learning rprop algorithm 
proceedings ieee international conference neural networks pp 

ieee press 
cave kosslyn 
processed separate cortical visual systems 
computational investigation 
journal cognitive neuroscience 


evolution optimum seeking 
sl th computer technology series 
john wiley sons 


evolution structures optimization artificial neural structures information processing 
aachen germany shaker verlag 

variable encoding modular neural networks time series prediction 
congress evolutionary computation cec volume pp 

ieee press 
sharkey 

combining artificial neural networks 
connection science 
michael christian igel marc toussaint sharkey 

modularity combining artificial neural nets 
connection science 
simon 

architecture complexity 
proceedings american philosophical society 

monster ghost connection machine mod neural systems theoretical evolutionary research 
proceedings acm ieee supercomputing conference 
acm press ieee press 
toussaint 

neural model multi expert architectures 
proceedings international joint conference neural networks ijcnn pp 

ieee press 
toussaint 

model selection disability neural networks decompose tasks 
proceedings international joint conference neural networks ijcnn pp 

ieee press 
yao 

evolving artificial neural networks 
proceedings ieee 

