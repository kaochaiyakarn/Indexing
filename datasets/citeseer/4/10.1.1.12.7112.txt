point value iteration anytime algorithm pomdps pineau geoff gordon sebastian thrun carnegie mellon university robotics institute forbes avenue pittsburgh pa cs cmu edu introduces point value iteration pbvi algorithm pomdp planning 
pbvi approximates exact value iteration solution selecting small set representative belief points planning 
stochastic trajectories choose belief points maintaining value hyperplane point able successfully solve large problems including robotic tag domain pomdp version popular game 
value iteration algorithm planning partially observable markov decision processes pomdps introduced sondik numerous authors refined cassandra kaelbling zhang zhang solve harder problems 
situation currently stands pomdp value iteration algorithms widely believed able scale real world sized problems 
distinct interdependent reasons limited scalability pomdp value iteration algorithms 
widely known reason called curse dimensionality kaelbling problem physical states pomdp planners reason belief states dimensional continuous space 
naive approaches discretizing belief space scale exponentially number states 
known reason poor scaling behavior call curse history pomdp value iteration ways breadth search space belief states 
starting initial belief state grows set histories corresponding reachable belief simulating pomdp forward 
number distinct histories considered grows exponentially planning horizon 
various clever pruning strategies littman cassandra proposed whittle set histories considered pruning steps usually expensive difference constant factors order growth 
history dimensionality related higher dimension belief space room distinct histories 
act independently planning complexity grow exponentially horizon problems states problems large number physical states may small number relevant histories 
domains curse history affects pomdp value iteration far strongly curse dimensionality kaelbling zhou hansen number distinct histories algorithm maintains far better predictor running time number states 
main claim avoid curse history real world pomdps curse dimensionality problem 
building insight point value iteration pbvi new approximate pomdp planning algorithm 
pbvi selects small set representative belief points iteratively applies value updates points 
point update significantly efficient exact update quadratic vs exponential updates value value gradient generalizes better unexplored beliefs grid approximations update value lovejoy brafman hauskrecht zhou hansen bonet 
pbvi guaranteed bounded error 
exploiting insight policy search methods mdp exploration ng jordan thrun pbvi uses explorative stochastic trajectories select belief points reducing number belief points necessary find solution 
presents empirical results demonstrating successful performance algorithm large states robot domain called tag inspired game 
order magnitude larger problems commonly test scalable pomdp algorithms 
addition include results known pomdps pbvi able match control quality fewer belief points performance earlier algorithms 
overview pomdps pomdp framework generalized model planning uncertainty kaelbling sondik assume pomdp represented fs finite set discrete states set discrete actions set discrete observations providing incomplete noisy state information 
pomdp model parameterized initial belief distribution distribution describing probability transitioning state state action oja distribution describing probability observing state action reward signal received executing action state discount factor 
key assumption pomdps state partially observable rely concept belief state denoted represent probability distribution states 
belief sufficient statistic history jb updated time step incorporate latest action observation pair normalizing constant 
goal pomdp planning find sequence actions fa maximizing expected sum rewards 
state necessarily fully observable goal maximize expected reward belief 
value function formulated maxa optimized exactly value function property piece wise linear convex belief sondik see fig 
left side 
consecutive iterations solution set consists set vectors vn vector consists jsj dimensional hyper plane jsj states defines value function bounded region belief vn max vn 
addition vector associated action defining best immediate policy assuming subsequent optimal behavior steps defined respectively sets 
th horizon value function built previous solution vn backup operator notation hv denote exact value backup max max ojs js number algorithms proposed implement backup directly manipulating vectors combination set projection pruning operations sondik cassandra zhang zhang describe straight forward version exact pomdp value iteration 
implement exact update hv generate intermediate sets step create cross sum observations includes step take union sets step practice vectors final set may completely dominated vector combination vectors 
pruned away affecting solution 
generally involves solving multiple successive linear programs requiring significant time usually worthwhile avoid explosion solution size 
better understand complexity exact update jv number vectors previous solution set 
step creates projections step generates joj cross sums 
worse case new solution jv size joj time jsj joj 
exponential growth occurs iteration importance pruning away unnecessary vectors clear 
highlights impetus approximate solutions 
point value iteration understood fact pomdp problems arbitrary action observation sequences infinite length reach points belief simplex 
unnecessary plan equally possible belief exact algorithms preferable concentrate planning probable beliefs 
point value iteration pbvi algorithm solves pomdp finite set belief points fb initializes separate vector selected point repeatedly updates value backups value vector 
shown maintaining full vector belief point pbvi preserves piece wise linear convex properties value function defines value function entire belief simplex 
contrast grid approaches lovejoy brafman hauskrecht zhou hansen bonet update value belief grid point 
pomdp value function representation pbvi left grid right 
complete pbvi algorithm designed anytime algorithm interleaving steps value iteration steps belief set expansion 
starts initial set belief points applies series backup operations 
grows set belief points finds new solution expanded set 
interleaving value backup iterations expansions belief set pbvi offers range solutions gradually trading computation time solution quality 
describe efficiently perform point value backups select belief points 
point value backup plan optimally finite set belief points modify backup operator eqn vector belief point maintained 
apply point update hv start creating projections exactly eqn step cross sum operation eqn simplified fact operating finite set points 
construct step arg max 
find best action belief point step arg max 
performing point updates backup creates projections exact vi 
final solution limited containing jbj components time 
full point value update takes polynomial time crucial size solution set constant iterations 
result pruning vectors solving linear programs crucial exact pomdp algorithms unnecessary 
pruning step refrain adding vector included arises nearby belief points support vector fig 

belief point set expansion explained pbvi focuses planning relevant beliefs 
pbvi initializes belief set containing initial belief set expanded way 
point stochastically simulate single step forward trajectory action getting new beliefs fb measure euclidean distance closest point keep new belief farthest away point expansion phase set doubles size pre defined maximum maximum selected time constraints performance requirements 
expansion phase followed steps value function pbvi offers anytime solution expansion 
step stochastic simulation action sample state belief distribution sample observation distribution compute ba bayesian update eqn 
addition initial belief point set pbvi requires number desired expansions horizon length algorithm terminates expansions completed 
convergence error bounds belief set horizon pbvi produces estimate error true value function bounded 
bound depends densely samples belief simplex 
denser sampling converges necessarily converge error bound holds 
cutting pbvi iteration sufficiently large horizon know difference optimal large 
error bounded kv kv term bounded theorem second bounded kv 
remainder section states proves error bound 
define density set belief points maximum distance legal belief precisely max 
min kb prove lemma error introduced pbvi pruning step prune rmax rmin proof 
point pbvi worst pruning error closest norm sampled belief vector maximal maximal erroneously pruning pbvi error 

hand maximal 

prune 





add zero 



opt 

collect terms kb bk holder def rmax min see text inequality holds vector represents reward achievable starting state sequence actions observations 
theorem belief set horizon error pbvi algorithm kv bounded rmax rmin specified accuracy threshold expansions proceed belief point set guarantees specific performance bound 
error bound described section 
assumes finite horizon problem 
applied pbvi infinite horizon problems selecting horizon rmax rmin 
beliefs reachable don need sample 
densely replace 
set reachable beliefs 

error bounds convergence results hold 
proof jjv jj def jj hv hv jj def jj hv hv jj hv jj triangle ineq 
prune hv jj def prune prune jjv jj contraction prune def rmax rmin lemma rmax min series sum experimental results domain tag popular game goal search tag moving opponent modeled pomdp planning problem consists selecting actions robot seek tag opponent 
current formulation opponent moves stochastically fixed policy described model 
shows live robot moves capture opponent 
spatial configuration domain planning illustrated 
domain order magnitude larger states pomdp problems considered far literature cassandra proposed new challenge fast scalable pomdp algorithms 
single iteration optimal value iteration problem size produce vectors pruning 
robots playing tag tag configuration tag domain states actions observations state space described cross product features robot fs opponent fs tagged agents start independently selected random position game finishes opponent fs tagged robot select actions 
reward imposed move action tag action results reward robot opponent location 
game robot position fully observable effect move action predictable deterministic effect robot position opponent completely unobservable agents cell 
time step opponent omniscient knowledge moves away robot stays place opponent robot opponent robot opponent robot shows performance pbvi tag domain 
results averaged runs algorithm times different randomly chosen start position run 
shows gradual improvement performance samples added shown data point represents new expansion belief set value backups 
addition pbvi apply qmdp approximation baseline comparison hauskrecht qmdp approximation calculated solving pomdp fully observable linearizing values obtain value belief max 
approximation quick compute remarkably effective domains 
tag domain lacks representational power compute policy 
additional experiments comparison known problems analyze performance pbvi applied known problems pomdp literature 
selected maze hallway hallway commonly test scalable pomdp algorithms littman brafman poon presents results domain 
replicating earlier experiments results maze averaged runs reset goal terminate steps results hallway hallway averaged runs terminate goal max steps 
cases pbvi able find policy 
table compares pbvi performance previously published results comparing goal completion rates sum rewards policy computation time number required belief points 
domains pbvi achieves competitive performance fewer samples 
validation belief set expansion investigate validity approach generating new belief states section compared approach techniques appear promising 
cases assume initial belief part model sole point initial set init consider expansion methods 
random ra 
stochastic simulation random action 
stochastic simulation greedy action 
stochastic simulation explorative action ra method consists sampling belief point uniform distribution entire belief simplex 
standard pbvi expansion heuristic section 
similarly uses single step forward simulation try actions randomly selects automatically accepts posterior belief 
time secs pbvi qmdp time secs pbvi qmdp time secs pbvi qmdp time secs pbvi qmdp pbvi performance problems tag left maze center left hallway center right hallway right method goal reward time jbj maze tiger grid qmdp grid brafman poon pbvi hallway qmdp qmdp littman poon pbvi hallway qmdp qmdp littman grid brafman poon pbvi tag qmdp pbvi applicable available table results pomdp domains 
marked computed results computed different platforms time comparisons may approximate best 
results assume standard lookahead controller 
uses value function solution pick best action belief performs simulation get new belief revisit hallway hallway problems section tag domain compare performance heuristics 
problem apply pbvi belief point selection heuristics include qmdp approximation baseline comparison 
shows computation time versus reward performance domain 
cases random expansion heuristic ra outperformed trajectory heuristics 
hallway hallway remains unclear heuristics best 
larger tag domain explorative action selection appears best 
results indicate particularly effective covering large dimensional beliefs 
smaller domains choice heuristic matter 
related significant done years improve tractability pomdp solutions 
number increasingly efficient exact value iteration algorithms proposed cassandra kaelbling zhang zhang successful finding optimal solutions generally limited small problems dozen states plan optimally beliefs 
pbvi avoids exponential growth plan size restricting value updates finite set reachable beliefs 
second approximate value iteration algorithms consist grid methods lovejoy solve larger problems states updating value discrete grid points 
require full coverage belief space problematic dimensionality increases 
non regular grids partially alleviated issue brafman hauskrecht variable resolution grids zhou hansen appears promising terms scalability 
lack data resulting policies prevent comparisons stage 
grid approaches pbvi include extremes belief simplex set points encountered stochastic simulation 
leads significant reduction number points necessary 
third different approach policy search methods optimize pomdp solutions baxter bartlett kearns ng jordan successfully solving multi dimensional continuous state problems 
view strength lies fact restrict optimization reachable beliefs pbvi exploits explorative belief selection 
unfortunately policy search techniques hampered slow changing gradients poor local minimum typically require selection restricted policy class 
pbvi shares similarities independently developed algorithm poon point updates gradual addition belief points key differences 
pbvi value update differs point update poon zhang zhang requires building projections 
pbvi building projections faster cases multiple belief points support vector jv jbj 
second accepts updates improve value max hv 
places belief points extremes simplex grows set belief points mentioned leads points pbvi slower planning times shown table 
time secs ra qmdp time secs ra qmdp time secs qmdp belief expansion results problems hallway left hallway center tag right presents pbvi scalable anytime algorithm approximate pomdp solving 
algorithm applied robot version successfully developed policy capturing moving opponent 
pbvi compared favorably pomdp solvers known domains 
success pbvi attributed factors directly target curse history 
trajectory approach select belief points focuses planning reachable belief 
second uses fixed set belief points perform fast value backups 
experiments pbvi successfully beats back curse history solve pomdps order magnitude larger previous algorithms tag domain states compared hallway problem pomdp scaling studies 
having overcome curse history identify hurdle pomdp research turns old fashioned mdp problem having distinct physical states 
arises cost updating point value function quadratic number states 
problem necessarily easy overcome believe existing literature poupart boutilier roy gordon contains approaches combined pbvi allow scale pomdp value iteration problems order magnitude larger 
considerable step making pomdps usable real world problems 
baxter bartlett baxter bartlett 
reinforcement learning pomdps direct gradient ascent 
icml 
bonet bonet 
optimal grid algorithm partially markov decision processes 
icml 
brafman brafman 
heuristic variable grid solution method pomdps 
aaai 
cassandra cassandra littman zhang 
incremental pruning simple fast exact method partially observable markov decision processes 
uai 
cassandra cassandra 
tony pomdp page 
www cs brown edu research ai pomdp code index html 
hauskrecht hauskrecht 
value function approximations partially observable markov decision processes 
journal artificial intelligence research 
kaelbling kaelbling littman cassandra 
planning acting partially observable stochastic domains 
artificial intelligence 
kearns kearns mansour ng 
approximate planning large pomdps reusable trajectories 
nips 
littman littman cassandra kaelbling 
learning policies partially environments scaling 
icml 
lovejoy lovejoy 
computationally feasible bounds partially observed markov decision processes 
operations research 
ng jordan ng jordan 
pegasus policy search method large mdps pomdps 
uai 
poon 
poon 
fast heuristic algorithm decisiontheoretic planning 
master thesis hong kong university science technology 
poupart boutilier poupart boutilier 
compression pomdps 
nips 
gordon thrun 
locating moving entities dynamic indoor environments teams mobile robots 
aamas 
roy gordon roy gordon 
exponential family pca belief compression pomdps 
nips 
sondik sondik 
optimal control partially observable markov processes 
phd thesis stanford university 
thrun thrun 
handbook intelligent control neural fuzzy adaptive approaches chapter role exploration learning control 
van nostrand reinhold 
zhang zhang zhang zhang 
speeding convergence value iteration partially observable markov decision processes 
journal artificial intelligence research 
zhou hansen zhou hansen 
improved grid approximation algorithm pomdps 
ijcai 
