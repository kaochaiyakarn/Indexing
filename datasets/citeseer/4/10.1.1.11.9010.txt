partially supervised classification text documents bing liu comp nus edu sg wee sun lee comp nus edu sg school computing national university singapore singapore mit alliance singapore philip yu ibm com ibm watson research center new york yorktown heights ny usa li comp nus edu sg school computing national university singapore singapore investigate problem set documents particular topic class large set mixed documents contains documents class types documents identify documents class 
key feature problem labeled non document traditional machine learning techniques inapplicable need labeled documents classes 
call problem partially supervised classification 
show problem posed constrained optimization problem appropriate conditions solutions constrained optimization problem give solutions partially supervised classification problem 
novel technique solve problem demonstrate effectiveness technique extensive experimentation 

text categorization classification automated assigning text documents pre defined classes 
common approach building text classifier manually label set documents pre defined categories classes learning algorithm produce classifier 
classifier assign classes documents words contain 
approach building text classifier commonly called supervised learning training documents labeled manually pre defined classes 
main bottleneck building classifier large prohibitive number labeled training documents needed build accurate classifiers 
shown nigam unlabeled data helpful classifier building 
approach basically uses small labeled set documents class large set unlabeled documents build classifiers 
show labeled unlabeled documents better small labeled set 
technique alleviates labor intensive effort 
study different problem common practice 
aims identify particular class documents set mixed documents contain class documents kinds documents 
call class documents interested positive documents 
call rest documents negative documents 
problem seen classification problem involving classes positive negative 
labeled negative documents mixed positive documents mixed set 
traditional techniques require labeled positive labeled negative documents build classifier 
research aim build classifier positive set mixed set 
save labor intensive effort manual labeling negative documents 
finding targeted positive documents mixed set important problem 
growing volume online text documents available world wide web internet news feeds digital libraries wants find documents related interest 
example may want build repository machine learning research papers 
start initial set papers icml proceedings 
find machine learning papers related online journals conference series aaai ijcai conferences 
similarly bookmarks positive documents may want documents interest internet sources labeling negative documents 
applications positive documents usually available worked particular task time accumulated related documents 
positive document available initially finding documents web source relatively simple yahoo web directory 
set find class documents sources manual labeling negative documents source 
key feature problems labeled negative document traditional classification methods inapplicable need labeled documents class 
call problem partially supervised classification positive documents considered labeled class positive labeled negative documents 
show theoretically partially supervised classification problem posed constrained optimization problem 
particular show noiseless case fixed target function known function class maps features document keywords label document algorithm selects function correctly classifies positive documents minimizes number mixed documents classified positive expected error high probability mixed documents positive documents size generalize result case may fixed target function maps features label document labels may noisy function class may powerful contain target function 
case aim find function performs close best possible subject desired expected recall 
corresponding optimization problem minimize number mixed documents classified positive subject fraction positive documents corrected classified 
theory indicates information available solve partially supervised classification problem constrained optimization problem required appears difficult solve practice 
propose novel heuristic technique solving partially supervised classification problem text domain 
algorithm built naive bayesian classifier mc nigam conjunction em expectation maximization algorithm dempster 
algorithm main novelties building initial classifier naive bayes em algorithm select documents negative documents mixed set spies 
documents positive documents reinitialize em algorithm order obtain local maximum likelihood function 
em algorithm generates sequence solutions increase likelihood function 
classification error sequence solutions may necessarily improving fact performance quite deteriorates likelihood increases 
motivated theoretical considerations give heuristic estimate probability error estimate select classifier sequence classifiers produced em algorithm 
carried extensive experiments public domain document datasets 
results show proposed technique effective computationally efficient 

related text classification studied extensively past information retrieval machine learning data mining 
number techniques proposed rocchio algorithm rocchio naive bayesian method lewis ringuette nearest neighbour yang support vector machines joachims 
existing techniques require labeled data classes building classifier 
designed solving partially supervised classification 
note naive bayesian method base technique strong foundation em efficient 
addition focus demonstrate potential partially supervised classification similar approach applied complex classifiers 
theoretical study probably approximately correct pac learning positive unlabeled examples done denis 
study concentrates computational complexity learning shows function classes learnable statistical queries model kearns learnable positive unlabeled examples 
presents algorithm learning modified decision tree algorithm statistical query model 
learning positive example studied theoretically muggleton bayesian framework distribution functions examples assumed known 
result obtained muggleton similar theoretical result noiseless case 
extend practical noisy case 
practical point view propose novel technique solve problem text domain 
technique produces remarkably results 
line related learning small labeled set nigam shahshahani 
works small set labeled data class large unlabeled set classifier building 
shown unlabeled data helps classification 
works clearly different labeled document negative class 
proposed method different traditional information retrieval salton 
information retrieval query document large document collection system retrieves ranks documents collection similarities query document 
web search uses similar approach 
perform document classification produces ranking documents similarity measures 

theoretical foundations partially supervised classification section gives theoretical foundations partially supervised classification 
assume examples generated independently fixed distribution 
text domain set possible documents set negative positive classes term examples denotes documents labels 
denote probability event chosen randomly 
finite sample denote probability respect choosing example uniformly random set sets examples positive sample size drawn independently conditional distribution conditioned unlabeled sample size drawn independently marginal distribution learning algorithm select function class functions classifier 
obtain insights learning possible partially supervised case rewrite probability error 
substituting equation obtain note constant 
able hold small learning approximately minimizing 
holding small minimizing approximately minimizing holding recall set positive examples set unlabeled examples large 
measure complexity function classes vc dimension vapnik chervonenkis function class 
vc dimension standard measure complexity computational learning theory see 
anthony bartlett 
class function finite set restriction set possible valued functions domain obtained class 
vc dimension size largest set points words larger vc dimension domain possible functions obviously finite set func tions 
vc dimension class thresholded linear functions see 
anthony bartlett 
class functions naive bayes subset linear functions 
vc dimension class number words classifier 
bounds obtained meant illustrate rate sample size grow 
constants optimized way noiseless case noiseless case function drawn 
function correctly classify documents set positive documents say achieves total recall error minimization learning algorithm produce function 
want show appropriate conditions selecting function minimizes unlabeled examples subject classifying positive examples correctly give function small expected error 
retrieval applications concerned precision recall error 
define expected recall er expected precision ep 
theorem gives conditions function minimizes performs 
theorem permissible class functions vc dimension target function 
proofs omitted due lack space full version measurability condition need concern practice 
see haussler 
drawn distribution positive examples unlabeled examples drawn independently subset achieves total recall probability 
noisy case noiseless case noisy case need equal independently drawn 
models case labels flipped probability case target function may belong deal noisy cases assume user wants functions function class classification user know contains target function labels noisy 
user required specify target expected recall 
learning algorithm tries output function high probability expected recall close expected precision close best possible functions expected recall close 
achieve algorithm draws set positive examples set unlabeled examples finds function minimizes unlabeled examples subject constraint fraction errors positive examples 
theorem permissible class functions vc dimension 
drawn distribution positive examples 
unlabeled examples drawn independently 
subset achieves recall subset achieves expected recall 
probability er ep 

proposed technique previous section showed theoretically positive mixed document sets build accurate classifiers high sufficient documents available 
suggested theoretical method suffers serious practical drawbacks constrained optimization problem may easy solve function class interested practical problem appear easy choose desired recall level give classifier function class 
section proposes practical heuristic technique naive bayes classifier em algorithm perform task 
naive bayesian text classification naive bayesian method popular techniques text classification 
shown perform extremely practice researchers mccallum nigam lewis ringuette 
set training documents document considered ordered list words 
denote word position document word vocabulary 
vocabulary set words consider classification 
set pre defined classes case 
order perform classification need compute posterior probability class document 
bayesian probability multinomial model laplacian smoothing count number times word occurs document depends class label document 
probabilities words independent class obtain naive bayes classifier class highest assigned class document 
em algorithm expectation maximization em algorithm dempster popular class iterative algorithms maximum likelihood estimation problems incomplete data 
fill missing values data existing values computing expected value missing value 
em algorithm consists steps expectation step maximization step 
expectation step basically fills missing data 
parameters estimated maximization step missing data filled reconstructed 
leads iteration algorithm 
naive bayes classifier steps em identical build classifier equations expectation step equation maximization step 
note probability class document takes value 
ability em missing data exactly want 
regard positive documents class value want know class value document mixed set 
em help assign probabilistic class label document mixed set 
number iterations probabilities converge 
initialization important order find maximum likelihood function 
class documents initialize parameters class 
easily assign probability negative class documents start 
propose technique deal problem finding negative documents initializing em 
proposed technique consists main steps reinitialization building selecting final 
step reinitialization applying em algorithm em algorithm applied context follows initially assign positive document class label document mixed set class label 
initial labelling naive bayesian classifier nb built 
classifier classify documents mixed document set 
particular nb compute posterior probability document mixed set equation assigned new probabilistic class label 
class probability positive document remains process 
revised new classifier nb built new values mixed documents positive documents 
iteration starts 
iterative process goes em converges 
algorithm called em initial em 
note process assigning probabilistic class label document compute sufficient build new nb information computed initial process positive set remains 
em 
build initial naive bayesian classifier nb document sets 
loop classifier parameters change 
document 
compute current nb 
update probabilistically assigned class new nb built process 
em algorithm naive bayesian classifier 
final probabilistic class label document classify mixed set identify positive documents 
experiment results show technique able improve classification compared technique simply applies naive bayesian technique original documents assuming mixed documents negative class 
technique performs classification easy datasets positive negative documents easily separated 
difficult datasets significant improvements achieved 
reason em hard datasets initialization strongly biased positive documents 
propose novel technique deal problem 
introducing spy documents mixed set solve problem discussed need initialization balance positive negative documents 
know documents negative identify negative documents mixed set 
discussed section em algorithm helps separate positive negative documents 
em position identify negative documents mixed set 
issue obtain reliable information identification 
sending spy documents positive set mixed set 
idea proven crucial 
approach randomly selects documents positive set experiment 
documents spies denoted 
added mixed set 
spies behave identically unknown positive documents allows reliably infer behaviour unknown positive documents 
em algorithm utilized mixed set spy documents 
em algorithm completes probabilistic labels spies decide documents negative 
threshold employed decision discussed 
documents lower probabilities negative documents denoted 
documents spies included higher probabilities unlabeled documents denoted 
detailed algorithm identifying negative documents unlabeled set 
algorithm step 




assign document class 
assign document class 
run em 
classify document 
determine probability threshold 
document 
probability 


identifying negative documents 
discuss determine threshold 
set spies probabilistic label assigned 
intuitively minimum probability threshold value means want retrieve spy documents 
noiseless domain minimum probability acceptable 
real life document collections outliers noise 
minimum probability unreliable 
reason posterior probability outlier document smaller actual negative documents 
know noise level data 
estimate trying noise levels selecting best 
sort documents 
selected noise level decide select documents probability 
experimented noise levels turns difference choose reason clear discussing step 
system summary objective step proposed algorithm achieve results 
left hand side shows initial situation 
mixed set positive negative documents 
know documents positive negative 
spies positive set added mixed set 
right hand side shows result technique achieves help spies 
observe positive documents mixed set put unlabeled set negative documents put negative set 
purity higher mixed set 
step building final classifier document sets step builds final classifier 
em algorithm employed document sets 
step carried follows 
put spy documents back positive set 
assign document positive set fixed class label change iteration em 

assign document negative set initial class changes iteration em 

document unlabeled set assigned label initially 
iteration em assigned probabilistic label 
subsequent iterations set participate em newly assigned probabilistic classes 

run em algorithm document sets converges 
em stops final classifier produced 
call step em procedure em spy em 
turn discuss percentage selecting negative documents matter long reasonable range step em algorithm probabilities document sets allowed change 
positive documents em positive negative mix spies positive positive negative spies unlabeled 
step re initialization slowly correct situation moving positive side 
experiments worked 
experimented classification results similar 
selecting classifier em algorithm works local maximum classifier approaching separates positive negative documents 
may true 
example situations positive negative documents consists clusters clusters may separable 
situation may better naive bayesian classifier iterating em 
general iteration em algorithm gives classifier may potentially better classifier classifier produced convergence 
run em iterations set classifier choose 
try estimate classification error approximation equation 
note equation unlabeled sample knew able calculate probability error measure 
furthermore estimate corresponding measurement positive set get estimate probability error require estimate 
choose estimate change probability error order decide iteration em choose final classifier 
equation change probability error iteration pr pr pr pr pr 
approximation approximation obtain pr pr pr pr pr select iteration final classifier time positive 
approximation error difference 
estimate require reasonably approximation 
reinitialization em algorithm usually able provide starting point near local minimum estimate far wrong 
find selection criteria effective 

empirical evaluation evaluation measures task identify retrieve positive documents mixed set appropriate information retrieval measures purpose 
popular measures score breakeven point 
score defined precision recall 
score measures performance system particular class see shaw theoretical bases practical advantages 
breakeven point value recall precision equal lewis ringuette 
breakeven point measure suitable task evaluates sorting order class probabilities documents 
give indication classification performance 
value hand reflects average effect precision recall 
small value small 
large value large 
suitable purpose want identify positive documents undesirable small precision small recall 
experiment results report accuracy results 
experiment datasets experiments large document corpora created datasets 
newsgroups lang 
contains different usenet discussion groups categorized main categories computer recreation science talk 
remove usenet headers discarding subject line experiments 
task identify positive documents set mixed documents choose categories positive classes various individual categories combinations categories form negative class documents 
collection webkb nigam 
webkb consists web pages number university computer science departments 
pages divided categories student faculty course project staff dept 
non categories student faculty cours project staff dept experiment purposes html tags removed 
objective recover positive documents put mixed set 
note need separate test sets normal classification 
seen test set 
experiment divide full positive set subsets positive document set algorithm full positive set set remaining documents 
documents put negative set form mixed set 
put documents wish create realistic situations skewed datasets 
believe realistic situations mixed set large number positive documents small 
vary create different settings experiments 
positive negative pos size size pos nb nb em em em em graphic stud course proj course course rec compu hockey baseball os win wind project student faculty student electr space med electr rec sci compu sci politics rec sci talk os win atheism rel misc rel misc pol misc pol guns pol misc comp windows sci rec talk pc os pc graphic average experiment results techniques tested experiments naive bayesian classifier nb bayesian technique directly applied produce classifier classify documents 
em applies em algorithm converges spies 
final classifier classify identify positive documents 
em spies re initialize em build final classifier 
error estimate select classifier sequence classifiers produced em reinitialization 
positive documents spies experiments 
threshold choosing negative documents experiment results different settings shown tables 
table shows full results datasets descriptions datasets full version 
row negative document set consists documents single category 
row row negative set contains documents multiple categories 
table column gives dataset number 
columns give names positive negative document sets respectively 
column gives total number positive documents column gives total number documents 
column gives number positive documents 
columns give value accuracy nb mixed set dataset 
note value measures retrieval results positive documents accuracy measures set 
columns show table 
results datasets value accuracy em 
run em iterations denoted em em improve 
columns give corresponding results em 
experiments necessary run em spies convergence extracting negative documents 
iterations em sufficient 
running iteration em significantly improve final results 
iteration em reinitialization iterations em reinitialization save computation method efficient scans data times 
final row table gives average results column 
results average values random runs 
due space limitations table summarize average results datasets settings 
tables observe em outperforms methods dramatically values 
accuracy better em slightly better nb 
datasets highly skewed positive sets small accuracy reflect classification performance 
note nb performs quite fraction positive documents small row table 
surprising assume documents negative correct naive bayes method able tolerate small amount noise training data 
note practical situations items positive class form significant fraction items environment 
example positive class class movies may enjoy watching expect movies form significant fraction movies 
natural question ask steps settings pos size size pos nb nb em em em em table 
summary results 
em em em table 
score accuracy results different datasets indicating necessity reinitialization selection classifier 
column descriptor corresponds method constructing datasets described section 
izing selecting model converged model necessary performance 
comparison obtained best results em reinitialization iterations value decide best result 
result achievable practice way decide best result actual negative data 
list table em outperforms omniscient version em showing reinitialization essential improved performance 
show reinitialization account improvement calculating performance em selecting model em th iteration em reinitialization 
shown table selecting model significantly outperforms simply final iteration reinitialized version em 
note classifier selection method applied em estimates inaccurate initially assign documents class negative 
iteration proposed technique step proposed technique linear number documents find em iterations sufficient step complexity step second step complexity em iterations needed 
technique linear number documents scans document sets required build final classifier 

studied problem classification partial information class labeled positive documents set mixed documents 
show theoretically information positive unlabelled data build accurate classifiers 
proposed novel technique solve problem text domain 
algorithm utilizes em algorithm naive bayesian classification method 
reinitialize em algorithm runs positive documents negative documents mixed set 
estimate classification error order select classifier classifiers produced iterations em algorithm 
extensive experiments show proposed technique produces extremely accurate classifiers positive class known 

bing liu li wee sun lee acknowledge support star nus academic research fund 
anthony bartlett 

neural network learning theoretical foundations 
cambridge university press 


measurement theoretical investigation mz metric 
information retrieval research pp 

dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
denis 

pac learning positive statistical queries 
proc 
th international conference algorithmic learning theory alt pp 

springer verlag 
haussler 

decision theoretic generalizations pac model neural net learning applications 
inform 
comput 
joachims 

text categorization support vector machines learning relevant features 
technical report ls report 
kearns 

efficient noise tolerant learning statistical queries 
journal acm 
lang 

newsweeder learning filter netnews 
international conference machine learning pp 

denis gilleron grappa 

learning positive unlabeled examples 
alt 
lewis ringuette 

comparison learning algorithms text categorization 
third annual symposium document analysis information retrieval pp 

mccallum nigam 

comparison event models naive bayes text classification 
muggleton 

learning positive data 
machine learning appear 
nigam mccallum thrun mitchell 

learning classify text labeled unlabeled documents 
aaai pp 

madison aaai press menlo park 
rocchio 

relevance feedback information retrieval 
salton ed 
smart retrieval system experiments automatic document processing 
englewood cliffs nj 
salton 

developments automatic relevance feedback information retrieval 
science 
shahshahani 

effect unlabeled samples reducing small sample size problem mitigating hughes phenomenon 
ieee trans 
geoscience remote sensing 
shaw 

foundation evaluation 
american society information science 
vapnik chervonenkis 

uniform convergence relative frequencies events probabilities 
theory applications 
yang 

evaluation statistical approaches text categorization 
journal information retrieval 
