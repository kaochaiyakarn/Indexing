journal machine learning research xx xx submitted published generalized kernel approach dissimilarity classification el pavel robert duin ph tn tudelft nl pattern recognition group faculty applied sciences delft university technology cj delft netherlands editor nello cristianini john shawe taylor robert williamson usually objects classified represented features 
discuss alternative object representation dissimilarity values 
distances separate classes nearest neighbor method offers solution 
dissimilarities practice usually far ideal performance nearest neighbor rule suffers sensitivity noisy examples 
show global classification techniques preferable nearest neighbor rule cases 
classification purposes different ways generalized dissimilarity kernels considered 
distances isometrically embedded pseudo euclidean space classification task performed 
second approach classifiers built directly distance kernels 
approaches described theoretically compared experiments different dissimilarity measures datasets including degraded data simulating problem missing values 
keywords dissimilarity embedding pseudo euclidean space nearest mean classifier support vector classifier fisher linear discriminant 
study design classifiers directly set dissimilarities objects generalized kernel approach 
kernels understood symmetric positive definite functions variables express similarity objects represented feature space 
propose address kernel general way proximity measure 
important difference kernel describes proximity relation objects may explicitly represented feature space may come different feature spaces 
example dissimilarity measure defined shapes contours images different sizes 
suitable choice variants hausdorff distance jain comparing sets points 
case need define common feature space objects represented 
means functional dependence specifically 
simply assume dissimilarity representation just 
lot attention paid similarity kernels devoted classification aspects dissimilarity distance kernels 
want emphasize importance recognition tasks dissimilarity kernels built directly el pavel robert duin 
images shapes 
feature space needs originally defined dissimilarities arise directly application 
examples jain zongker jacobs 

concerning notation generally refer objects printed bold emphasize feature vectors 
distance kernel represents information relative way pairwise dissimilarity relations objects 
goal learn relational data different computing distances necessary starting original representation 
question studied learn data dissimilarity kernels 
sch lkopf proposed treat kernels generalized distance measures strengthened link algorithms positive definite kernels support vector classifier svc vapnik sch lkopf kernel principal component analysis sch lkopf sch lkopf 
essential difference approach 
sch lkopf starts reasoning feature space representation case possible start learning task dissimilarity kernel 
elaborate discussion see section 
principle question dissimilarity kernel recognition problem tackled 
collected set methods researcher 
analyze problem illustrate possible solutions 
dissimilarity measure nearest neighbor nn classifier expected perform 
difficult build measure complex recognition problem 
case imperfect dissimilarities nn rule suffers sensitivity noisy examples global classifiers perform better 
design goal research 
distinct approaches classification tasks studied 
dissimilarity representation isometrically embedded feature space section 
possible finite representation space may pseudo euclidean goldfarb 
objects mapped space preserve structure data revealed original distances 
means dissimilarity measure defines classes bounded compact configuration underlying feature space reflect properties 
underlying feature space constructed process embedding 
purpose compactness hypothesis reverse hold see section 
dimensionality feature space low classification task easily performed 
second approach dissimilarity kernel interpreted mapping chosen representation set objects duin duin section 
formulation classifiers constructed directly dissimilarity kernels dissimilarity spaces 
organized follows 
section discusses dissimilarities light compactness hypothesis general 
section presents mathematical formulation linear embedding problem 
section gives details building classifiers embedded space 
section addresses classification problem direct dissimilarity kernels 
sections describe experiments conducted discuss results 
summarized section 
dissimilarity measures general classification problem solved called compactness hypothesis braverman duin states objects similar close representations 
effectively puts constraint dissimilarity measure small objects similar smaller similar objects objects different 
feature representations hold way entirely different objects may feature representation 
cause problem feature values improbable classes 
dissimilarity kernel reverse compactness hypothesis holds provided dissimilarity measure poses continuity 
demand objects identical implies belong class 
extended somewhat assuming objects small distance object positive sufficiently small similar belong class 
consequently dissimilarities objects consideration 
conclude dissimilarity representations satisfying continuity reverse compactness hypothesis holds objects similar representation similar reality belong class see duin 
result classes overlap classification error may zero large training sets 
order interpret hypothesis recall notion metric 
distance measure called metric conditions fulfilled reflectivity positivity distinct symmetry triangle inequality basically reflectivity positivity crucial define proper dissimilarity measure 
accept dissimilarity measure zero different objects violate compactness hypothesis 
hand negative dissimilarities difficult interpret 
reflectivity positivity conditions fulfilled 
distance measure metric assumption continuity fulfilled reverse compactness hypothesis means symmetry triangle inequality 
conditions necessary dissimilarity kernel 
non metric distances arise shapes objects images compared template matching type distances built computer vision jain jacobs 
argued tversky jacobs symmetry constraint strong especially dissimilarities come psychological judgments 
methods asymmetric distances permitted 
symmetric distance measures obeying triangle inequality proper metrics included 

linear embedding dissimilarities number ways embed dissimilarity data feature space 
interested faithful configuration non linear embedding performed distances preserved possible 
nonlinear projections require computational effort way projecting new points existing configuration straightforward defined linear mappings preferable 
isometric embeddings considered 
embedding euclidean distances representation set 
pn duin duin refer objects 
euclidean distance matrix objects distance preserving mapping euclidean space 
projection known literature classical scaling metric multidimensional scaling young householder borg groenen cox cox 
words dimensionality configuration squared euclidean distances preserved 
note having determined configuration rotation translation 
remove degree freedom loss generality mapping constructed origin coincides centroid mean vector configuration defined relation euclidean distances inner products 
proven borg groenen see appendix matrix square euclidean distances matrix inner products underlying configuration vector diagonal elements hand expressed centering matrix identity matrix 
projects data final configuration zero mean 
positive definite gram matrix 
factorization eigendecomposition diagonal matrix diagonal consisting non negative eigenvalues borg groenen ranked descending order followed zero values orthogonal matrix corresponding eigenvectors 
non zero eigenvalues dimensional representation qk qk rk qk matrix leading eigenvectors contains square roots corresponding eigenvalues 
note determined procedure unique 
possible arbitrary point centroid 
general projection achieved imposing weighted mean zero weight vector rotation centroid fixed orthogonal matrix xt xt note features uncorrelated estimated covariance matrix qk orthogonal 
cov xt embedding non euclidean distances qtk qk matrix positive definite distance matrix euclidean borg groenen gower 
non euclidean positive definite negative eigenvalues 
result constructed relies square roots eigenvalues see 
approaches possible address problem euclidean space positive eigenvalues taken account resulting dimensional con figuration qp distances approximate original ones 
distances positive largest negative eigenvalues magnitude smaller largest positive eigenvalues 
sum positive eigenvalues larger sum magnitudes negative ones 
justification choosing positive eigenvalues section issue noise influence discussed 
argue general directly measured distances may noisy may perfectly euclidean result small negative eigenvalues see illustration 
disregarding noise diminished 
known cox cox gower exists positive constant smallest negative eigenvalue new square euclidean distance matrix created adding diagonal elements new 
expressed euclidean space 
practice eigenvectors remain value added non zero eigenvalues giving new eigenvalue matrix equivalent regularizing covariance matrix configuration cov changing respectively 
approaches transform problem configuration defined euclidean space 
especially useful negative eigenvalues relatively small magnitude suggest hand original distance measure close euclidean 
cases negative eigenvalues interpreted noise contribution 
negative eigenvalues relatively large neglecting possibly important information rejected 
open question consequences classification tasks transforming problem euclidean space neglecting negative eigenvalues directly enlarging constant 
possibility exists problems euclidean space large 
goldfarb proposed project data pseudo euclidean space theoretical data embedded line dims embedded line dims example usefulness disregarding negative eigenvalues 
assume theoretical original data perfect line due measurement process somewhat distorted observed plot 
distance kernel computed distances dij xi xj nearly euclidean 
embedding process eigenvalues revealed negative largest negative magnitude equals 
suggest possible dimensional configuration significant positive eigenvalue indicates real intrinsic dimensionality data euclidean distance perfect line embedded configuration dimensional 
second plot shows projection dimensions configuration retrieved rotation 
plot presents projection st rd dimensions rd dimension corresponds largest magnitude negative eigenvalue embedding done case explained formula 
notice tiny change theoretical data euclidean distance simple problem enlarges number retrieved dimensions perfect case 
performed symmetric distance matrix 
pseudo euclidean space seen consisting euclidean spaces inner product operation positive definite space negative definite second see appendix 
simple illustration see 
embed data reasoning euclidean space applied 
essential difference refers notion inner product distance 
matrix inner products expressed see appendix matrix inner product operation pseudo euclidean space 
goldfarb write compare formula ip iq positive negative eigenvalues order positive eigenvalues decreasing values negative ones decreasing magnitude zero values 
represented pseudo euclidean space called signature goldfarb follows qk 
mv pseudo euclidean space 
length vector form zero 
orthogonal vectors mirrored lines oa oc 
vector defines plane mx space 
vector flipped version describes plane euclidean space perpendicular 
explains pseudo euclidean space inner product operation seen euclidean operation vector flipped general distances sign 
note uncorrelated features estimated pseudo euclidean covariance matrix goldfarb positive definite euclidean space cov xt 
means result mapping sense pca projection embedding procedure interpreted sort kernel pca sch lkopf sch lkopf approach kernel reproducing kernel pseudo euclidean feature space see section 
computing square distances pseudo euclidean space interpreted computing square euclidean distance positive space subtracting square euclidean distance negative space see appendix 
distances computed positive space overestimated purpose negative space correct non euclidean 
pseudo euclidean spaces going result embedding process positive distances contribution negative space distances smaller space case due construction relatively small feature values corresponding space practice confirms measured distances close euclidean giving relatively small negative eigenvalues embedding procedure 
note inner product interpreted traditional euclidean inner product mv 
having pseudo euclidean configuration linear classifier mx addressing standard euclidean case 
embedding new points having configuration pseudo euclidean space preserves pairwise distances new points added solving linear regression problem 
square distance matrix rs expressing dissimilarities novel objects representation set configuration xn sought pseudo euclidean space rk matrix bn rs inner products relating new objects objects see appendix bn centering matrix bn expressed xn bn space rk euclidean ip iq space rk pseudo euclidean xn mean square error solution xn bn xn bn knowing qk xn alternatively reduction dimensionality xn bn xn bn qk 
adding object representation set dissimilarity kernel practice point added finite pseudo euclidean space dimensionality vector representation increase goldfarb contrary euclidean case 
means outliers noise contribute significantly resulting dimensionality practice new points added projected space determined starting configuration reliability describes sufficiently sampled representation set plays essential role process representing new data consequently classification performance 
originally pseudo euclidean configuration distances preserved exactly dimensionality determined number non zero eigenvalues relatively small non zero eigenvalues compared large ones 
knowing dissimilarities noisy measurements small eigenvalues correspond non significant directions framework neglecting small eigenvalues stands reducing noise contribution see illustration 
means distances preserved approximately 
control dimensionality reduced vector representation 
basically dimensional ity reduction achieved orthogonal projection governed pca 
particular construction qk fact uncorrelated vector theoretical data noise noise noise influence eigenvalues leftmost plot presents theoretical banana data consisting points euclidean distance matrix computed 
second plot shows result embedding space note retrieved configuration rotation 
third plot presents projection dimensions data obtained embedding distorted distances dij dij sij sij non euclidean 
plot presents projection dimensions data obtained embedding consists theoretical data noisy features added 
note largest eigenvalues graphs non distorted distorted data gives practically results cases 
rejecting relatively small eigenvalues noise diminished 
representation cov stand form orthogonal pca projection see formula 
means reduction dimensionality performed simple way neglecting directions corresponding eigenvalues small magnitude 
reduced representation orthogonal projection deter mined significant positive eigenvalues significant magnitude negative eigenvalues 
rn qk diagonal matrix decreasing positive eigenvalues increasing negative eigenvalues qk matrix corresponding eigenvectors 

classification embedded data symmetric dissimilarity kernel seen description underlying vector representation determined euclidean space traditional classifier constructed space 
happens pseudo euclidean representation conventional classifiers redefined 
limit simple linear classification rules nearest mean classifier fisher linear discriminant support vector classifier advanced classifiers built 
generalized nearest mean classifier nearest mean classifier nmc simplest linear classifier assigns unknown object class nearest mean 
pseudo euclidean space decision rule pseudo euclidean distance 
assume class problem classes 
vector representation 
xn preserves originally considered squared distances mean vector class new object represented space zx classification rule defined assign iff zx zx assign defined 
embedding dissimilarity kernel avoided class mean vectors distances computed 
propose alternative approach 
similar classification process carried performing exact mapping 
result generalized nearest mean classifier obtained 
assume class represented distance matrix representation set 
pn 
new object represented distances set proximity class measured function defined vd pj vd pj pk vd generalized variability underlying feature space 
shown see appendix holds zx zx vd xj 
xn linear embedding introduced section zx representation object space dependencies important observations 
refers vd expressing variability dimensional vector vd coincides variance higher dimensional euclidean representation vd equal sum variances trace cov 
pseudo euclidean space vd stands generalized variability pseudo euclidean covariance matrix see 
second observation refers function measures distance point zx mean class represented space interesting point distance computed performing embedding process explicitly operates distances 
result allows define generalized nearest mean classifier follows assign min 
words assigned class nearest mean centroid centroid described underlying space defined distances 
note formulation classification rule holds number classes 
nmc pseudo euclidean space general identical classifiers 
nmc finds linear embedding feature space rk distance matrix dimensionality space determined class class distances 
operates class distances 
embedding performed directly works underlying feature spaces rk class separately 
may happen signatures feature spaces rk 
case performances nmc differ nmc unifies pseudo euclidean space signature classes treats separately allows describe properly 
distinct signatures accuracy expected higher problems classes behave differently 
fisher linear discriminant linear classifier separating hyperplane pseudo euclidean space defined follows goldfarb ip iq construct fisher linear discriminant fld notion pseudo euclidean covariance matrix needed 
representation defined goldfarb cov xi xi xi 
making definition goldfarb fld obtained maximizing ratio scatter scatter fisher criterion fukunaga class problem cw pooled class covariance matrix pseudo euclidean space cw pooled class covariance matrix computed euclidean case stand class means 
fld pseudo euclidean space constructed hyperplane mv 
illustration simple problem see 
support vector classifier points xi 
euclidean space point xi belongs classes described corresponding label yi 
goal nonoverlapping classes find optimal hyperplane maximizes margin vapnik burges alternatively minimizes 
non linearly separable classes non negative slack variables introduced allowing theoretical data embedding embedding embedding embedding illustration decision boundary fld embedded space 
leftmost plot presents theoretical artificial data 
training points marked circles 
objects taken training data perfectly embedded dimensions 
remaining points marked belong examples testing data illustrate new objects projected retrieved pseudo euclidean space 
plots show results embedding lp distance dij xik xjk 
lp distance metric positive smaller cases distances close objects emphasized 
fld determined circles original subplot embedded spaces drawn 
original theoretical data retrieved rotation 
classification errors soft margin linear support vector classifier svc vapnik burges solution quadratic programming procedure minimize wt yi xi 
term upper bound misclassification training samples regarded regularization parameter trade number errors width margin 
dual programming formulation follows maximize diag diag 
kernel matrix kij xi xj solving problem weight vector linear combination data vectors giving yi xi 
zero data points xi positive called support vectors 
contribute hyperplane equation 
result discrimination function terms inner products follows yi xi 
note training stage kernel matrix equals matrix inner products see 
linear svc inner products linear relation svc easily constructed underlying feature space performing embedding explicitly provided distances euclidean 
novel objects represented distances representation set svc immediately tested bn matrix inner products new objects objects embedded originally provided formula 
non euclidean dissimilarity matrix matrix inner products positive definite resulting pseudo euclidean space configuration linear classifier defined 
assign classifier treated euclidean space 
operation seen flipping values vector negative directions pseudo euclidean space 
suggested graepel equivalent flipping negative eigenvalues positive ones considering inner product euclidean space positive definite :10.1.1.48.236
summarizing dissimilarity kernel svc classifier built underlying feature space follows 
matrix computed 
positive definite matrix computed describes positive definite kernel construct svc 
note polynomial svc classifier vapnik built directly 
discussion kernel trick distances sch lkopf considered kernels generalized dissimilarity measures 
reasoning starts observation mercer kernel vapnik positive definite kernel seen nonlinear generalization similarity measure inner products 
possible kernel expressed inner product operation high dimensional feature space mapping image space idea sch lkopf considers generalization squared euclidean distance space called kernel trick allows express distance kernel explicitly performing mapping 
sch lkopf argues larger class kernels conditionally positive definite kernels 
symmetric function vectors xi fulfills kij xi xj called positive definite kernel 
inequality satisfied kernel called conditionally positive definite 
relation kernels established stands conjugate transpose 
simplest case ci equal formula read means euclidean distance matrix kernel 
kernel exists hilbert space real valued functions mapping 
supports fact euclidean distance kernel embedded euclidean space example hilbert space 
justified kernel 
means particular case mapping considered 
generally sch lkopf proves real valued symmetric kernel exists linear space hilbert space real valued functions endowed symmetric bilinear form mapping 
reproducing kernel wahba feature space definition pseudo euclidean space see appendix equipped non degenerate indefinite symmetric bilinear form seen generalized inner product 
justifies non euclidean distance kernel see formula reproducing kernel pseudo euclidean feature space sch lkopf argues kernels natural choice dealing translation invariant problem svc kernel pca example 
formula squared euclidean distance hilbert space 
summary sch lkopf provides new framework distance algorithms 
squared euclidean distance realized feature space suitable kernel function conditionally positive definite 
formula kernel transformed kernel kernel algorithms applied 
sch lkopf provides mathematical context approach embedding distances 
gives information relations kernels kernels words names class kernels isometrically embedded euclidean space 
sch lkopf starts feature space known dissimilarity measure 
assume distance kernel implicitly dataset knowing type measure computed 
approaches relies linear embedding particular case mapping considered sch lkopf 
sch lkopf focuses mathematical formulations try study methods practice 

classification dissimilarities second approach mentioned addresses dissimilarity kernel mapping defined representation set 
pn 
mapping defined 
pn notice expresses original feature space objects explicitly 
dimensionality theoretical data dissimilarity data theoretical data dissimilarity data simple illustration dissimilarity space 
third plots show theoretical artificial data quadratic classifier 
lp distance dij xik yjk computed data 
representation set consist objects 
second fourth plots dissimilarity spaces representative objects marked circles third plots 
note chosen linear classifier dissimilarity kernel separates data 
dissimilarity space controlled size formulation classifiers constructed directly dissimilarity kernels dissimilarity space 
justification construct classifiers dissimilarity space follows 
property distances small similar objects belonging class larger objects different classes gives possibility discrimination pi defined distances representative pi interpreted feature 
pi characteristic object particular class discrimination power pi large 
hand pi non typical object class pi may discriminate poorly 
defining discriminating dissimilarity measure non trivial recognition problem difficult 
hand distance measure derived solved classification problem 
challenge especially measure preferably incorporate invariances 
building measure equivalent defining features traditional classification problem 
measure nearest neighbor nn method expected perform provided metric nearly metric 
decision nn local neighborhoods general sensitive noise 
means nearest neighbors include best representatives class object assigned 
nn asymmetric distance measure perform badly dissimilarities strongly obey triangle inequality 
cases better generalization achieved classifier built dissimilarity space 
better include distances determine representatives training process 
instance linear classifier dissimilarity space weighted linear combination dissimilarities object representation set weights optimized training set large weights magnitude emphasize objects play essential role discrimination 
doing global classifier built sensitivity noisy representative examples reduced 
experience confirms linear quadratic classifier generalize better nn rule especially small representation set see duin 
linear classifier built dissimilarity kernel wj xj linear classifier dissimilarity kernel expressed wj xj 
essential difference separating hyperplanes 
linear relation square distance matrix matrix inner products classifier built fact quadratic classifier underlying space 
linear classifier constructed dissimilarity kernel general non quadratic nonlinear classifier underlying feature space nonlinear relation inner products kernel fisher linear discriminant fld general traditional classifier operating feature spaces dissimilarity kernels 
commonly dissimilarity measures sums differences measurements choice linear bayesian classifier assuming normal densities natural consequence central limit theorem applied 
principle quadratic bayesian classifier better requires training objects estimation class covariance matrices 
known duda fukunaga class problems equally probable classes classifier equivalent fisher linear discriminant fld obtained maximizing ratio scatter scatter fisher criterion fukunaga 
refer separating hyperplane fld 
fld constructed form 
starting point representation set consisting objects training set coincide case deal small sample size classification problem vectors xi dimensions 
proposed duin reduced representation set size especially importance distances expensive compute 
easiest way choose random selection 
objects chosen minimum distance maximized 
possibility greedy approach 
starting randomly chosen object iterative procedure object added dissimilar objects chosen 
done globally class separately 
case dissimilar objects chosen outliers positioned boundary 
exist ways determine reduced representation set investigated 
established linear classifier built 
new object distances set computed 
class problem prior probabilities linear bayesian classifier fukunaga constructed dissimilarity kernel follows log 
cw pooled class covariance matrix mi stands class mean dissimilarity space 
identical cw singular linear classifier built 
regularization yielding approximated covariance matrix cw classifier regularized covariance matrix called regularized linear discriminant rld 
linear programming lp machines properly defined objective function constraints dissimilarity kernel separating hyperplane obtained solving linear programming problem 
assume class problem classes cardinality respectively labels yi 
separating hyperplane built complete representation set 
simple optimization problem minimizing number misclassification errors defined minimize si yi xi 
si si ni depending class label yi 
argued bennett mangasarian formulation guarantee nontrivial solution mean vectors classes happen 
problem solved standard optimization methods simplex algorithm interior point methods 
constraints included hyperplane constructed ndimensional dissimilarity space 
possible impose sparse solution minimizing norm weight vector hyperplane wj wj 
order formulate minimization task terms lp problem eliminate absolute value wj objective function wj expressed nonnegative variables wj minimization problem minimize yi xi 
flexible formulation classification problem proposed graepel 
problem minimize basically means margin variable optimization problem 
imposing constant modified version introduced minimize yi xi 
approach sparse solution obtained means important objects selected nonzero weights original representation set resulting reduced set solution similar adaptation svc feature representations defined lp machines smola sch lkopf 
essential importance novel objects dissimilarities objects computed 
support vector classifier support vector classifier built dissimilarity kernel 
recall chosen representation set 
pn dissimilarity mapping defined 
pn linear decision function space support vector kernel consists elements kij xi xj stands euclidean inner product 
formulation linear support vector classifier see section matrix positive definite construction svc 
case sparse solution provided method obtained dissimilarity space 
means evaluation new objects dissimilarities training objects computed svc form 

experiments nist digits experiments section class problem recognition digits nist database wilson 
total data consist equally sampled binary images 
features describe images represented dissimilarity kernels 
aim illustrate potentials dissimilarity kernels studying behavior simple classifiers constructed kernels 
going perfect distance kernels constructed specific problem nn gives perfect result 
goal investigate done cases 
course ultimate goal build discriminating dissimilarities possible possible find discriminating features traditional classification tasks 
series experiments focuses comparison distance kernel approaches classification linear embedding dissimilarity spaces 
degradation degradation images handwritten digits 
plot shows examples binary digits experiments 
images degraded digits plots 
level degradation governed probability individual pixel set background 
different dissimilarity measures considered euclidean blurred images modified hausdorff jain digit contours 
chosen illustrate behavior kernel approaches respect distance properties 
distance metric second violates triangle inequality 
modified hausdorff distance applied digit contours useful template matching purposes jain 
measures difference sets 
ag 
bh defined max hm hm hm min 
calculate distance images digits detected dissimilarity computed respect boxes surrounding means shift invariant 
find second dissimilarity measure images blurred gaussian function standard deviation pixels similar distance transform image 
motivation preprocessing avoid sharp edges digits invariances robustness small tilting changed thickness incorporated 
euclidean distance computed blurred versions 
experiments performed times results averaged 
single experiment data randomly split training set testing set 
testing set consists samples class 
number different training sets chosen sizes varying objects class 
goal investigate different directions behavior generalized nearest mean classifier behavior fisher linear discriminant fukunaga representation sets classifiers constructed directly dissimilarity kernels 
table dissimilarity coefficients binary images similarity measure jaccard metric euclidean similarity srs dissimilarity drs srs simple matching srs drs srs yule ad bc srs ad bc drs srs second type experiments goal study usefulness dissimilarity kernels data missing values 
think dissimilarity kernels designed tackling types problems 
order study performance classifiers function number missing values simulated data randomly corrupting images 
level degradation governed probability particular image pixel set background 
different degradation levels experiments 
type procedure seen introducing extra noise simulates missing values problem assumed corrupted pixels originally unknown values images binary values just assigned background pixels 
simplify experiment computation distances expensive resampled dataset original binary images rescaled raster see illustration 
usual way compute dissimilarities binary data object construct similarity measure transform corresponding distance 
similarity measures binary ob jects variables reflecting object number elementary matches objects see 
instance variable reflects number cases similarity objects scored 
experiments mea binary images 
sures chosen yielding different properties jaccard simple matching yule similarities cox cox gower see summary table 
jaccard measure interest overlap ratio excluding non occurrences disregarding information matches background pixels 
contrary simple matching measure describing proportion matches total number instances pixels useful case 
comes fact counts matches background pixels considered unknown 
yule similarity different type cross product ratio measuring association pixels predictability 
aim compare behavior methods dissimilarities respect different degradation 
level degradation complete distance matrices computed 
assume training testing data degraded similar way 
training set fixed size samples class randomly chosen 
classifiers tested independent testing set containing samples class 
testing procedure repeated times results averaged 
error rate blurred euclidean distance nmc eucl emb 
nn training size class error rate error rate error rate modified hausdorff distance nmc eucl emb 
nmc ps eucl emb 
nmc eucl emb 
nn training size class nearest mean classifiers constructed embedded spaces blurred euclidean distance fld eucl emb 
svc eucl emb 
nn training size class error rate modified hausdorff distance fld eucl emb 
fld ps eucl emb 
fld eucl emb 
svc eucl emb 
svc ps eucl emb 
svc eucl emb 
nn training size class fld svc functions built embedded spaces blurred euclidean distance fld random rld lp sparse chosing lp svc nn training size class error rate modified hausdorff distance fld random rld lp sparse chosing lp svc nn training size class classifiers built dissimilarity kernels notion representation set comparison different classification methods blurred euclidean left column modified hausdorff right column dissimilarity kernels 
results discussion kernel approaches dissimilarities dissimilarity kernels modified hausdorff blurred euclidean experimental directions considered 
direction linear classifiers built embedded space second linear classifiers constructed dissimilarity spaces built directly dissimilarity kernels 
results nn rule commonly applied dissimilarity representations distance kernels nn rule best nn rules larger 
concerning embedded spaces decision functions nearest mean classifier fisher linear discriminant support vector classifier 
applied euclidean space positive eigenvalues pseudo euclidean space euclidean space obtained embedding corrected dissimilarity kernel see formula 
note classifier sense pseudo euclidean space 
euclidean dissimilarity measures euclidean pseudo euclidean spaces coincide 
case embeddings results pseudo euclidean case reported 
presents performance nearest mean classifiers nn rule 
distance measures observed classifiers complex problem give performance 
perform worse nn rule especially case modified hausdorff kernel 
suggests dissimilarity classes revealed embedded space gaussian shape 
concerning pseudo euclidean space reaches higher accuracy nmc nmc built euclidean space positive eigenvalues behaves nearly 
indicates negative directions pseudo euclidean space significance 
interesting observation nearest mean classifiers nearly improve accuracy increase training size larger objects class 
indicates number objects represents classes underlying feature space 
eigenvalues computed euclidean blurred distances eigenvalues computed modified hausdorff distances eigenvalues matrix inner products blurred euclidean left modified hausdorff right dissimilarity kernels size 
studied behavior fld svc constructed embedded spaces 
fld svc built reduced representations described section 
reduced representation achieved solution pca projection 
dimensionality assigned value training size blurred euclidean modified hausdorff distances respectively 
choices motivated visual exploration eigenvalues inner product matrix selecting number values significantly large magnitude see formulas embedding purpose compare 
results drawn blurred euclidean dissimilarity kernel svc fld constructed embedded space outperform nn rule 
performance especially fld indicates gaussian description classes overlapping revealed original distances 
modified hausdorff dissimilarity kernel linear classifier embedded space complex similar way nmc 
nn method outperforms decision rules considered 
note training size larger objects nn rule gives lower errors nn rule blurred euclidean kernel 
concerning behavior different variants fld svc modified hausdorff distances pseudo euclidean embedding allows reaching higher accuracy euclidean part described positive eigenvalues see formulas euclidean embedding enlarged dissimilarity kernel see formula 
shows negative directions sense 
embedding euclidean spaces fld performs worse svc pseudo euclidean space behave similarly 
experiments results depended training size dimensionality fixed 
additional experiment performed training size fixed objects class constructed dimensionality varied 
test size set samples class 
goal illustrate performance fld function retrieved dimensionality 
analyzing observations blurred euclidean dissimilarity kernel fld embedded euclidean space outperforms significantly fld built dissimilarity kernel dimensionalities smaller 
best result reached dimensionality range 
larger dimensionalities error increases 
judging left observe essential eigenvalues remaining ones significant contribution fld embedded space 
classifier behavior error curves indicates classes linearly separable 
modified hausdorff dissimilarity kernel fld embedded pseudo euclidean space performs better fld embedded euclidean space 
fld distance kernel reaches highest accuracy 
confirms classes separated best nonlinear way linear classifier built dissimilarity kernel interpreted nonlinear decision function underlying feature space 
second direction experiments focussed building linear classifiers dissimilarity space directly kernel importance representation set identical training set case error rate blurred euclidean distance fld pca reduced fld eucl emb 
dimensionality error rate modified hausdorff distance fld pca reduced fld eucl emb 
fld ps eucl emb 
fld eucl emb 
dimensionality performance fld blurred euclidean left modified hausdorff right dissimilarity kernels function retrieved dimensionality fixed training size objects class 
distances representation training objects computed evaluation new objects 
reduction size essential computational point view 
sections different approaches proposed 
possibility reduce selecting objects pre specified criterion 
simplicity random selection objects duin duin gives reasonable results 
experiments fld built kernel randomly reduced training size 
studied rld fixed regularization parameter constructed complete dissimilarity kernel 
possibility enforce sparse solution dissimilarity space way similar solving feature selection problem 
achieved linear programming schema bradley classification problem formulated terms sparse lp 
comparison lp non sparse solution described 
methods nn rule 
experimental results 
case blurred euclidean distances reduced representation sets lead lower accuracy comparison classifiers identical loss accuracy average gained training samples build fld training samples case lp formulation 
representation set chosen different manner classifiers 
case fld arbitrarily chosen 
lp contrary provided sparse solution corresponding optimization task 
classification error method provided case training objects minimization problem infeasible solution 
linear classifiers reach higher accuracy nn rule fld randomly chosen shows somewhat worse performance confirms building linear classifiers dissimilarity kernels may beneficial 
modified hausdorff distances linear classifiers perform worse case blurred distances 
worst results reported fld reduced randomly selected nn rule best training sizes larger objects class 
smaller training sizes outperformed especially rld non sparse lp 
summary series experiments conclude blurred euclidean dissimilarity kernel describes compact bounded classes modified hausdorff kernel 
linear classifiers underlying space dissimilarity kernel perform worse case 
probably caused fact modified hausdorff distance applied offer rotation invariance needed problem 
blurred euclidean distance hand partially robust tilting due initial blurring step 
nn method larger training sizes best classifiers modified hausdorff case 
rld non sparse lp perform smaller training sizes outperform nn rule 
results discussion kernel approaches dissimilarities missing values problem second track experiments studied applicability dissimilarity kernels data missing values 
order keep similar line reasoning previous part simulated missing values problem setting pixel values background 
training testing sets fixed sizes varying quantity level image degradation described section 
presents generalization error rate function increasing data degradation jaccard yule measures discussed groups kernel methods variants nmc fld methods representation set results yule measure simple matching dissimilarities similar chosen presentation yule distance kernel distance non euclidean non metric 
indication distinct results 
nearest mean classifiers surprisingly robust increase data deterioration see 
fld functions robust data degradation achieve higher accuracy nmc 
classifiers constructed directly dissimilarity kernel observed generally outperform nn rule 
robust degradation images 
comparing results nn rule deteriorates 
surprisingly performance classifiers deteriorate increasing image degradation 
best cases error achieved degradation see 
suggests simple dissimilarity kernels binary images highly robust missing information 
nmc variants accuracy classifiers simple binary dissimilarity kernels applied non degenerated data times higher error rate jaccard distance nmc eucl emb 
nn probability pixel degradation error rate error rate error rate yule distance nmc eucl emb 
nmc ps eucl emb 
nmc eucl emb 
probability pixel degradation nn nearest mean classifiers constructed embedded spaces jaccard distance fld eucl emb 
nn probability pixel degradation jaccard distance fld random rld lp sparse chosing lp nn probability pixel degradation error rate fld built embedded spaces error rate yule distance fld eucl emb 
fld ps eucl emb 
fld eucl emb 
probability pixel degradation nn fld random rld lp sparse chosing lp nn yule distance probability pixel degradation classifiers built dissimilarity kernels notion representation set comparison different classification methods jaccard left column yule right column dissimilarity kernels 
training size fixed objects class 
error rate fld random rld lp sparse chosing lp nn simple matching probability pixel degradation simple matching error rate fld random rld lp sparse chosing lp nn yule distance probability pixel degradation yule distance comparison methods notion representation set simple matching yule dissimilarities 
classifiers complicated dissimilarity measures blurred euclidean modified hausdorff compare figures training size objects 
reasons binary distances applied rescaled images digits aligned case original binary images 
surprisingly yule dissimilarity kernel performance sparse lp better non sparse lp level degradation higher see right 
jaccard dissimilarity kernel rld lp classifiers applied degraded images level perform comparably best results blurred euclidean modified hausdorff dissimilarities 
average jaccard dissimilarity kernel allows better separability classes yule distance observed 
case jaccard distance measure methods give identical solutions rld lp formulation 
methods achieve best classification results compared classifiers distance measures investigated error degraded jaccard distance compare training size objects 
interesting simple distance measure jaccard operating binary images digits outperforms modified hausdorff dissimilarity computed contours 
possible explanation case modified hausdorff implemented robust tilting stretching case rescaling images lower raster digits aligned 
binary dissimilarity measures considerably robust data degradation 
example rld lp classifiers images high level degradation achieve performance comparable best classifiers built intact blurred euclidean modified hausdorff dissimilarities 
summary conclude dissimilarity measures especially jaccard general robust degradation 
classifiers considered nn rule shows highest sensitivity data degradation expected due sensitivity noisy examples 
classifiers outperform nn rule variants nmc fld random yule simple matching measures 
proves point achieved constructing complex classifiers making dissimilarity kernels nn rule 
robust variants nmc especially nmc embedded euclidean space determined positive eigenvalues performance highest 
best results achieved rld lp 

experiments kimia dataset experiment want show nn result noisy prototypes outperformed types advanced classifiers constructed embeddings dissimilarity spaces 
purpose kimia dataset sebastian 
binary shapes 
sebastian 
proposed compare shapes computing edit distance medial axes 
type distance efforts put construction 
report dissimilarity measure allows performance nn 
dataset perfect dissimilarity measure modified hausdorff distance 
studied robust rotation 
group left group right binary images experiments 
kimia dataset consists categories category contains binary images shapes 
experiment illustrative feasible created groups images categories 
notice images different sizes supports idea starting feature space representations 
classes small number objects classification done leave procedure 
means dissimilarity measure built table leave results groups binary images 
average number support vectors size representation set parenthesis complete dissimilarity kernel 
note misclassification object gives error 
means error stands wrongly assigned objects respectively 
classifier group group nmc eucl 
emb 
nmc ps eucl 
emb 
nmc eucl 
emb fld eucl 
emb 
fld ps eucl 
emb 
svc eucl 
emb 
svc ps eucl 
emb 
fld dmax crit fld random rld lp sparse choosing lp svc nn nn nn nn time representation set consisting objects object time different testing 
procedure repeated times results averaged 
results classifiers table 
case support vector classifiers number different values parameters see formulas considered best results reported 
group example problem nn error relatively high smallest nn error equals means objects wrongly classified 
advanced classifiers outperform nn method 
best result fld constructed dissimilarity kernel representation set objects reduced randomly initialized dmax criterion operating dissimilarity kernel 
criterion looks objects distances maximized 
performance achieved lp classifier optimized objects 
results provided rld svc built dissimilarity space 
group example performance nn rule significantly better previous case deteriorates neighbors taken account 
case nn generalizes better nn larger type classifiers may difficulties outperform method 
improvement gained reached svc sparse formulation lp classifier dissimilarity space 
notice classifiers perform better nn nn nn proves point profit larger number objects able reduce noise local neighbors neighborhoods 
groups linear classifiers built embedded spaces perform worse linear classifiers dissimilarity kernel 
exclude embedded spaces useful case shows linear classifier best solution remember linear classifier dissimilarity kernel nonlinear classifier underlying space polynomial 
studying behavior classifiers important nearest mean classifiers generalize relatively poorly suggest classes elongated 
concluded observing images classes include rotated variants shapes 
dissimilarity robust rotation shapes differ significantly class 
fld svc perform better pseudo euclidean space euclidean space suggests employing space profitable 
lp classifier sparse formulation generalizes 
trained class classes optimized representation set large 
fld reduced representation set gives results smaller number objects decreases computational load evaluation novel objects 
summary imperfect dissimilarity measures nn method outperformed sophisticated classifiers account number representative objects global decisions 

objects usually described features 
alternative approach discussed objects represented mutual proximities 
allows extend notion kernel general proximity relation 
dissimilarity kernels discussed started sch lkopf 
framework advantageous situations feature representation straightforward example recognizing objects contours highly dimensional data hyperspectral images dealing missing information 
different ways building classifiers dissimilarity kernels discussed 
approach dissimilarities isometrically embedded pseudo euclidean space classification performed 
second approach classifiers built directly distance kernels 
conducted number experiments approaches 
illustrated class digit recognition problem various dissimilarity measures investigated class shape classification problems modified hausdorff distance 
experiments include randomly distorted digit images simulating problem missing values 
dissimilarity kernel separates classes nn method supposed achieve performance 
focus imperfect dissimilarity measures 
main point show cases advanced classifiers usually outperform nn rule 
example performance nn rule may improved techniques blurred euclidean distance computed nist digits 
nearly methods offer higher accuracy 
best classifiers non sparse lp formulation dissimilarity kernel fld embedded space 
methods give satisfactory results 
nearest mean classifiers fit structure data fld classifier randomly chosen conclude space improve performance nn rule global approaches distance measure sophisticated problem 
example modified hausdorff distance 
measure built specifically template matching purposes separates digit classes 
impossible improve performance nn method advanced approaches 
smaller training sizes accuracy non sparse lp rld higher 
absolute performance nn rule better modified hausdorff kernel blurred euclidean kernel larger training sets 
interesting non sparse lp classifier built blurred euclidean kernel outperforms nn method modified hausdorff shows advanced classifier constructed simpler dissimilarity representation may better simple nn rule applied sophisticated measure 
best illustrated analyzing digit recognition problem data 
performance nn rule seriously deteriorates increase data degradation 
time dissimilarities information simpler 
case global techniques better nn method 
simpler dissimilarity larger potential improvement global classifiers 
nearest mean classifiers embedded space fld randomly chosen provide better results high data degradation 
experiments kimia dataset confirms cases nn rule gives high error complex techniques built dissimilarity kernels achieve better results 
concerning pseudo euclidean spaces confirm goldfarb goldfarb reduction dimensionality essential diminish influence noise 
pseudo euclidean space advantageous addressing problem euclidean space observed digit kimia datasets 
building classifiers embedded space preferable constructing dissimilarity kernel training set consisting representative objects sufficiently large retrieved dimensionality small 
assures generalization novel objects observed case blurred euclidean distances classes linearly separable training objects 
linear classifiers operating complete dissimilarity kernels achieve higher accuracy methods reduced representation sets 
offer sparse solutions advantageous computational point view 
achieve results case kimia dataset 
linear classifiers fld svc generalize dissimilarity spaces provide better performance nn rule 
summary main dissimilarities separate classes nn general nn method outperformed advanced classifiers built embedded space distance kernels 
acknowledgments partly supported dutch organization scientific research nwo 
goes david tax valuable discussions art cutting things 
authors grateful encouragement criticism anonymous reviewers 
appendix pseudo euclidean spaces appendix meant provide additional information picture complete respect embedding problem pseudo euclidean spaces 
derivations borg groenen goldfarb gower 
pseudo euclidean space pseudo euclidean space real linear vector space equipped non degenerate indefinite symmetric bilinear function called inner product 
space interpreted composed euclidean subspaces dimensionality dimensionality inner product positive definite negative definite 
characterized signature goldfarb 
basis 
ep called orthonormal pseudo euclidean space ei ei 

ei ej known basis constructed 
orthonormal basis chosen inner product vectors ip iq note norm non zero vector pseudo euclidean space defined positive negative zero 
case non zero vector called light vector 
squared distance defined similar way euclidean space notion inner product general positive negative zero 
note different distance equal zero 
alternatively pseudo euclidean space expressed rp rq justifies formulas allowed express square distance rp note euclidean space special case pseudo euclidean space 
relation distances inner products assume vectors euclidean space 
xn 
definition euclidean distance properties inner product may write xi xj xi xj xi xj xi xi xj xj xi xj xi xj xi xj origin space 
dependence inner product xi xj expressed xi xj xi xj xi xj making known properties inner products formula distance xi expressed terms distances follows goldfarb xi xi xi xi xi xi xi xi xi xi xs xp xs xi xs xp xs xp xs xi xs xi xs xp xs stands mean computed column kernel 
mean value 
loss generality assume mean vector coincides origin 
implies xi xi formula plugging obtain xi xj xi xj xi xs xs xj xp xs representation vectors matrix inner products bij xi xj formula bij ij 
square euclidean distance matrix 
substituting xj 
xi mathematical operations formula alternatively expressed vector diagonal elements matrix note precisely reasoning follows pseudo euclidean space relation distances inner products spaces 
holds pseudo euclidean space inner product defined 
consequence matrix inner products consistently space expressed fact formulas remain true pseudo euclidean space 
adding new points embedded space rn configuration pseudo euclidean space pseudo euclidean space rs square distance matrix new objects 
vs objects representation set yi vector representation novel object projected space rk formula inner product new vectors original points yi xj yi xj yi xj making formula fact centroid coincides origin inner product yi xj yi xj xs xs xj xp xs xj yi xj bn rs matrix inner products new vectors original ones 
writing elementary matrix operations gets bn rp find configuration xn linear regression problem solved xn xt bn yielding generalized variability xn bn embedded euclidean pseudo euclidean space generalized variability configuration vectors 
xn defined trace covariance matrix sum variances follows vd xj note expresses norm defined space reflects geometry imposed distance matrix representation set 
pn possible express vd terms distances way vd pj pk show equivalence equivalent transformations 
making formula facts tr xj gets vd pj pk bt xj matrix inner products space case euclidean space 
note equivalence second equation proximity function proved reasoning arbitrary point zx 
braverman 
computers pattern recognition 
thompson washington 
bennett mangasarian 
robust linear programming discrimination linearly inseparable sets 
optimization methods software 
borg groenen 
modern multidimensional scaling 
springer verlag new york 
bradley mangasarian street 
feature selection mathematical programming 
informs journal computing 
burges 
geometry invariance kernel methods 
sch lkopf burges smola editors advances kernel methods support vector learning 
mit press 
cox cox 
multidimensional scaling 
chapman hall london 
jain 
modified hausdorff distance object matching 
th international conference pattern recognition volume pages 
duda hart stork 
pattern classification 
john wiley sons 
duin 
compactness complexity pattern recognition problems 
international symposium pattern recognition pierre devijver pages royal military academy brussels 
duin 
classifiers empty spaces 
th international conference pattern recognition volume pages barcelona spain 
duin 
complexity dissimilarity pattern classes 

duin de ridder 
relational discriminant analysis 
pattern recognition letters 
fukunaga 
statistical pattern recognition 
acad 
press 
goldfarb 
unified approach pattern recognition 
pattern recognition 
goldfarb 
new approach pattern recognition 
kanal rosenfeld editors progress pattern recognition volume pages 
elsevier science publishers 
gower 
euclidean distance geometry 
mathematical scientist 
gower 
metric euclidean properties dissimilarity coefficients 
journal classification 
graepel herbrich obermayer 
classification pairwise proximity data 
advances neural information system processing pages 
graepel herbrich sch lkopf smola bartlett ller obermayer williamson 
classification proximity data lp machines 
international conference artificial neural networks pages 

linear algebra 
springer verlag 
jacobs weinshall gdalyahu 
classification non metric distances image retrieval class representation 
ieee transactions pattern analysis machine intelligence 
jain zongker 
representation recognition handwritten digits deformable templates 
ieee transactions pattern analysis machine intelligence 
duin 
automatic pattern recognition similarity representations 
electronic letters 
sch lkopf 
support vector learning 
phd thesis verlag munich 
sch lkopf 
kernel trick distances 
neural information processing systems vancouver british columbia canada 
sch lkopf smola 
ller 
kernel principal component analysis 
sch lkopf burges smola editors advances kernel methods support vector learning pages 
mit press cambridge ma 
sch lkopf smola williamson bartlett 
new support vector algorithms 
neural computation 
sebastian klein kimia 
recognition shapes editing shock graphs 
international conference computer vision page appear 
smola sch lkopf 
semiparametric support vector linear programming machines 
kearns solla cohn editors advances neural information processings systems pages cambridge ma 
mit press 
tversky 
features similarity 
psychological review 
vapnik 
nature learning 
springer 
wahba 
support vector machines reproducing kernel hilbert spaces randomized 
sch lkopf burges smola editors advances kernel methods support vector learning pages 
mit press cambridge ma 
wilson 
character database 
technical report national institute standards technology february 
young householder 
discussion set points terms mutual distances 
psychometrika 
