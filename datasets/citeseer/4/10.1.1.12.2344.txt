taming aggressive replication pangaea wide area file system yasushi saito christos magnus karlsson mahalingam storage systems department hp labs palo alto ca usa christos karlsson hpl hp com pangaea wide area file system supports data sharing community widely distributed users 
built symmetrically decentralized infrastructure consists commodity computers provided users 
computers act autonomously serve data local users 
possible exchange data nearby peers improve system performance availability network economy 
approach realized aggressively creating replica file accessed 
presents design implementation evaluation pangaea file system 
pangaea offers efficient randomized algorithms manage highly dynamic potentially large groups file replicas 
applies optimistic consistency semantics replica contents offers stronger guarantees required users 
evaluation demonstrates pangaea outperforms existing distributed file systems large heterogeneous environments typical internet large corporate intranets 
pangaea wide area file system supports daily storage needs distributed community users 
platform ad hoc data sharing enables multinational corporations distributed groups collaborating users content management systems exchange data efficiently file system 
pangaea builds unified file system federation thousands widely distributed computers connected dedicated virtual private networks 
currently assume servers trusted relaxing trust relationship 
system faces continuous reconfiguration users moving companies restructuring computers added removed 
pangaea meet key goals speed hide wide area networking latency file access speed resemble local file system 
availability autonomy avoid depending availability specific node 
pangaea adapt automatically server additions removals failures network partitioning 
network economy minimize wide area networks 
nodes distributed uniformly nodes lan half way globe 
pangaea transfer data nodes physical proximity possible reduce latency save network bandwidth 
argue system follow symbiotic design achieve goals dynamic wide area environments 
system server functions autonomously allows reads writes files disconnected 
computers available system configuration changes servers dynamically adapt collaborate way enhances performance availability system 
pangaea realizes symbiosis pervasive replication 
aggressively creates replica file directory accessed 
single master replica file 
replica may read written time replicas exchange updates peer peer fashion 
pervasive replication achieves high performance serving data server close point access high availability letting server contain working set network economy transferring data close replicas 
sections introduce key strategies implement pervasive replication 
graph replica management pangaea replica management satisfy goals 
support large number replicas maximize availability 
second needs manage replicas file independently difficult predict file access patterns accurately wide area 
third needs support dynamic addition removal replicas nodes available 
pangaea addresses challenges maintaining sparse strongly connected randomized graph replicas file 
graph propagate updates discover replicas replica addition removal 
design offers important benefits available inexpensive membership management replica added connecting live replicas discovers matter replicas unavailable 
graph sparse adding removing replica involves constant cost regardless total number replicas 
available update distribution pangaea distribute updates live replicas file far graph connected 
redundant flexible nature graphs extremely disconnected multiple node link failures 
network economy random graph design facilitates efficient wide area network bandwidth system aggressive replication policy 
pangaea achieves clustering replicas physical proximity tightly graph creating spanning tree faster edges dynamically update propagation 
optimistic replica coordination distributed service faces inherently conflicting challenges high availability strong data consistency 
pangaea aims maximizing availability time users able read write replica system able create remove replicas blocking 
address challenge pangaea uses techniques replica management 
pushes updates replicas invalidating achieves higher availability wide area keeping date data locations 
approach may result managing unnecessary replicas wasting storage space networking bandwidth 
ameliorate problem pangaea lets node remove inactive replicas discussed section 
second pangaea manages replica contents optimistically 
lets node issue updates time propagates replicas background detects resolves conflicts happen 
pangaea supports eventual consistency guaranteeing user sees change user unspecified time 
studies reveal file systems face little concurrent write sharing users demand consistency window minutes 
pangaea actual window inconsistency seconds wide area show section 
addition pangaea provides option synchronously pushes updates replicas gives users confirmation update delivery section 
believe pangaea consistency semantics sufficient ad hoc data sharing pangaea targets 
pangaea support applications require strong consistency open close consistency locks synchronize directory operations lock files 
related traditional local area distributed file systems meet goals speed availability network economy 
systems xfs frangipani rely tight node coordination replica management overcome non uniform networking latencies frequent network partitioning typical wide area networks :10.1.1.110.7161:10.1.1.130.3029
pervasive replication resembles persistent caching client server file systems afs coda lbfs 
pangaea harness nodes improve system robustness efficiency 
provides better availability 
server crashes nodes providing access files hosted 
updates propagated live replicas servers unavailable 
decentralized nature pangaea allows node removed permanently transparently users 
second pangaea improves efficiency propagating updates nearby nodes client fixed server creating new replicas nearby existing replica 
sense pangaea generalizes idea fluid replication utilizes surrogate coda servers placed strategic fixed locations improve performance availability system 
pangaea replication follows optimistic approach similar mobile data sharing services lotus notes bayou roam :10.1.1.12.7323
systems lack replica location management rely polling usually humans discover exchange updates replicas 
pangaea keeps track replicas automatically distributes updates proactively transparently users 
systems replicate granularity database roam supports subset replicas 
contrast pangaea files directories replicated independently operations rename affect multiple files replicated different set nodes 
operations demand new protocol ensuring consistent outcome conflicts discuss section 
pangaea offers simple conflict resolution policy similar roam locus coda 
chose design sophisticated approaches bayou pangaea assumptions semantics file system operations 
farsite pangaea build unified file system federation nodes different objectives 
farsite goal build reliable service top untrusted nodes byzantine consensus protocols designed primarily local area networks 
pangaea assumes trusted servers dynamically replicates files edge minimize wide area networks 
peer peer data sharing systems built top load balanced fault tolerant distributed hash tables share properties pangaea 
systems cfs past employ heuristics exploit physical proximity locating data support concurrent place updates hierarchically structured data 
pangaea systems provides extra machinery conflict detection resolution discuss section 
oceanstore builds file system strong consistency routing updates small core replicas :10.1.1.115.4299
pangaea allows place updating replica centralized coordination maximize availability 
ivy peer peer file system lets data updated time exchanging operation logs replicas :10.1.1.119.567
replicas poll remote logs frequently supports stronger consistency pangaea 
log update propagation allows versatile conflict resolution pangaea 
ivy forces user read logs writers support small file system small number writers 
number companies active field widearea collaborative data sharing including scale webfs 
offer uniform seamless interface sharing files wide area network independent physical locations users data 
provide features intelligent location cached copy closest user 
centralized database keep track location files replicas 
design meet pangaea goals availability autonomy 
pangaea structural overview section overviews structure server major data structures maintains 
pangaea design follows symmetrically distributed approach 
pangaea server handles file access requests users 
assume user uses single server log session lasting say hours demand replication improves latency user may move servers time 
server maintains local hard disks store replicas files directories 
servers interact peer peer fashion provide unified filesystem 
definitions terms node server interchangeably 
nodes automatically grouped regions nodes region low round trip times rtt ms implementation 
pangaea uses region information optimize replica placement coordination 
pangaea replicates data granularity files treats directories files special contents 
term file refer regular file directory 
edge represents known connection replicas file updates file flow edges 
replicas file edges comprise strongly connected graph 
set replicas file called file replica set 
structure server pangaea server currently implemented loopback server 
server consists main modules nfs protocol handler receives requests applications updates local replicas generates requests replication engine 
built sfs toolkit provides basic infrastructure nfs request parsing event dispatching 
replication engine accepts requests nfs protocol handler replication engine running nodes 
creates modifies removes replicas forwards requests nodes necessary 
largest part pangaea server 
log module implements transaction semantics local disk updates redo logging 
server logs replica update operations service allowing survive crashes 
request application user space kernel nfs protocol handler replication engine nfs client pangaea server log membership inter node communication structure pangaea server 
membership module maintains status nodes including liveness available disk space locations root directory replicas list regions system set nodes region round trip time rtt estimate pair regions 
module runs extension van renesse protocol 
node periodically sends knowledge nodes status random node chosen live node list recipient merges list 
fixed nodes designated landmarks bootstrap newly joining nodes 
protocol shown disseminate membership information quickly low probability false failure detection 
region rtt information gossiped part membership information 
newly booted node obtains region information landmark 
polls node existing region determine belongs create new singleton region 
region node smallest ip address elects leader periodically pings nodes regions measure rtt 
membership tracking scheme especially rtt management key scalability bottleneck system network bandwidth consumption node configuration estimated bytes second node 
plan external services idmaps widely available 
structure file system pangaea replica set consistency management maintaining distributed graph replicas file 
shows example system files 
pangaea distinguishes types replicas gold bronze 
read written users time run identical update propagation protocol 
gold replicas play additional role maintaining hierarchical name space 
gold replicas act starting points bronze replicas path name traversal 
irr example directory joe file joe foo 
replica joe stores pointers gold replicas foo 
replica foo keeps backpointer parent directory 
bronze replicas connected randomly form strongly connected graphs 
bronze replicas uni directional links gold replicas file shown 
directory entry file lists file gold replicas 
second gold replicas perform tasks hard completely distributed way 
particular pivots keep graph connected permanent node failure maintain minimum replication factor file 
form clique file graph monitor tasks 
issues discussed detail section 
currently pangaea designates replicas created initial file creation gold fixes locations fail permanently 
replica stores backpointer indicates location file system name space 
backpointer includes parent directory id file name directory 
purposes resolve conflicting directory operations section keep directory entry date gold replica set file changes section 
shows key attributes replica 
timestamp ts version vector vv record time file modified 
described detail section 
uni directional links gold replicas file 
peers point neighboring gold bronze replicas file graph 
replica set management pangaea replica created user accesses file removed node runs disk space finds replica inactive 
operations frequent carried efficiently replica stores multiple backpointers file hard linked 
backpointer need remember locations parent directory replicas parent directory node due namespace containment property section 
struct replica fid fileid bit globally unique file id ts timestamp pair physical clock ip addr vv maps ip addr timestamp set nodeid set ip addresses peers set nodeid set fileid string pair fname 
struct fname string fid fileid set nodeid ts timestamp key attributes replica 
blocking nodes store replicas unavailable 
section describes algorithms random walks achieve goals 
file creation describe interactions modules system various data structures simple scenario user server creates file directory moment assume stores replica creates protocol described section determines location initial replicas gold replicas file typical value 
replica reside replicas chosen random different regions system improve expected availability file 
second creates local replica adds entry local replica replies client client start accessing file 
background disseminates types updates 
floods new directory contents directory replicas 
floods contents empty save attributes permissions owner gold replica nodes 
practice describe section deploy techniques reduce overhead flooding dramatically 
side effect propagation replicas point gold replicas discovered path name lookups 
replica addition protocol creating additional replicas file run user tries access file local node 
say user node wants read file read write request preceded directory lookup open request create replica replicate file parent directory 
recursive step may continue way root directory 
locations root replicas maintained membership service section 
pangaea performs short cut replica creation transfer data nearby existing replica 
create replica discovers file gold replicas directory entry path name lookup 
requests file contents gold replica closest say 
finds replica closest graph neighbors say may forwards request turn sends contents point replies user lets start accessing replica 
request forwarding performed directory knows gold replicas may bronze replica closer gold ones 
new copy integrated file replica graph able propagate updates receive updates replicas 
background chooses existing replicas adds edges requests add edges new replica selection peers satisfy goals include gold replicas choices short cut replica creation 
include nearby replicas updates flow fast network links 
sufficiently randomized high probability crash nodes disconnect file graph 
pangaea satisfies goals simultaneously replica multiple edges 
chooses types peers new replica 
adds edge random gold replica preferably different region give gold replica variety regions neighbor set 
second asks random gold replica say pick replica immediate graph neighbors closest third asks choose random replicas random walks start perform series rpc calls graph edges 
protocol ensures resulting graph edge node connected provided connected 
parameter trades availability performance 
small value increases probability graph disconnection probability replica exchange updates replicas node failures 
large value increases overhead graph maintenance update propagation causing duplicate update delivery 
offers balance prototype 
name space containment procedures file creation replica addition require file parent directory node 
pangaea fact demands file intermediate directories root replicated node 
name space containment requirement yields benefits 
naturally offers availability autonomy benefits island replication 
enables lookup access replica server disconnected allows node take backup file system locally 
quantify benefits section 
second simplifies conflict resolution directory operations discuss section 
hand requirement increases systemwide storage overhead compared idealized scheme directories stored node 
consider overhead reasonable users pay times storage cost replicating files place 
bronze replica removal section describes protocol removing bronze replicas 
gold replicas removed side effect permanent node loss 
discuss handling permanent failures section 
replica removed possible reasons node run disk space cost keeping replica outweighs benefits 
reclaim disk space pangaea uses randomized gd size algorithm 
examine random replicas kept node calculate merit values gd size function considers replica size access time 
replica minimum merit evicted replicas worst merit values added candidates examined round 
algorithm repeated frees space disk 
optionally server reclaim replicas worth keeping 
currently competitive updates algorithm purpose 
server keeps replica counter incremented time replica receives remote update reset zero replica read 
counter value exceeds threshold prototype server evicts replica 
due lack wide area file system traces analyzed storage overhead fresh file system redhat installed 
overhead mainly depends spatial locality accesses degree files directory accessed 
expect overhead practice closer spatial locality typical file system traces usually high 
remove replica server sends notices replica graph neighbors 
neighbor turn initiates random walk starting random gold replica uses protocol described section establish replacement edge live replica 
starting walk live gold replica ensures graph remains strongly connected 
similar protocol runs node detects node permanent death describe section 
summary graph pervasive replication algorithms described section offer fundamental benefits traditional approaches fixed set servers manage replica locations 
simple efficient recovery failures graphs definition flexible spanning edges replica graph incrementally robust efficient 
just type edges locate replicas propagate updates simplifies recovery permanent failures avoids system disruption graph reconfiguration 
decoupling directories files directory entries point gold replicas set gold replicas typically stable 
file parent directory act independently file created 
adding removing bronze replica file require change directory replicas 
adding removing gold bronze replica directory require change file replicas 
key properties system efficiency 
propagating updates section describes pangaea solutions challenges posed optimistic replication efficient reliable update propagation handling concurrent updates lack strong consistency guarantees 
efficient update flooding basic method propagating updates pangaea flooding graph edges shown 
replica modified server server pushes entire file contents graph neighbors turn forward contents neighbors replicas receive new contents 
simple flooding gold replica set kept part replica attributes see 
replica updated 
update newly issued log fid vv send fid vv data nodes peers fid vv neighbors reply 
update fid vv data received node update applied reply log apply update 
reply forward update peers 
neighbors reply 
simple flooding algorithm distribute updates 
code assumes updates issued time handling concurrent updates discussed section 
algorithm guarantees reliable update delivery long replica graph strongly connected 
sections introduce techniques improving efficiency basic flooding algorithm 
optimization delta propagation major drawback flooding propagates entire file contents byte modified 
delta propagation improves propagation efficiency maintaining logical simplicity flooding 
portion file changed adding entry directory pangaea propagates small semantic description change called delta 
deltas general applied order replica produce result 
ensure having delta carry timestamps old timestamp represents state replica just change new timestamp shows state replica change 
replica applies delta current timestamp matches delta old timestamp 
resorts full contents transfer potential conflict resolution described section 
practice updates handled exclusively deltas full state transfer happens concurrent writes node recovers crash 
pangaea reduces size updates delta merging akin feature implemented coda 
example file deleted right modified happens temporary files server modification sent replicas 
delta merging transparent users adds delay propagation 
optimization flooding guarantees reliable delivery propagating updates deltas full contents multiple links step algorithm 
consumes times optimal network bandwidth number edges replica 
eliminate redundant update deliveries 
pangaea uses phase protocol propagate updates exceed certain size kb 
phase small message contains timestamps update called harbinger flooded graph edges 
update bodies sent phase requested nodes 
node receives new harbinger asks sender harbinger immediate upstream replica flooding chain push update body 
simultaneously forwards harbinger neighbors graph 
node receives duplicate harbinger having received update body asks sender retry 
required sender earliest harbinger may crash sending update body 
node receives harbinger having received update body tells sender sending update 
chose harbinger threshold kb delta sizes follow bimodal distribution peak bytes representing directory operations flatter plateau kb representing bulk writes 
harbinger algorithm saves network usage shrinks effective window replica inconsistency 
user tries read file harbinger received waits actual update arrives 
harbinger propagation delay independent actual update size chance user seeing stale file contents greatly reduced 
optimization exploiting physical topology positive side effect 
favor fast links node requests body update sender harbinger receives 
unpredictable node link load may reduce benefit 
simple extension harbinger algorithm improves data propagation efficiency requiring coordination nodes 
pushing forwarding harbinger graph edge server adds delay proportional estimated speed edge rtt implementation 
way pangaea dynamically builds spanning tree shape closely matches physical network topology 
shows example 
section show technique drastically reduces wide area networks updating shared files 
pangaea batches nfs write requests flushes data disk replicas commit request 
size update grow larger typical write request size kb 
example update propagation file replicas thick edges represent fast links 
update issued 
sends harbinger fat edge forwards harbinger quickly 
forwards harbinger time sends harbinger spanning tree formed 
links tree backups tree links fail 
update body pushed tree edges 
practice steps proceed parallel 
conflict resolution optimistic replication concurrent updates inevitable rare 
combination version vectors writer wins rule resolve conflicts 
recall delta timestamps mismatch servers revert full state transfer 
version vectors separate true conflicts causes missing updates fixed simply overwriting replica 
simplifies conflict resolution 
conflicts contents regular file currently offer users options 
rule update timestamps attribute ts 
case clocks servers loosely synchronized ntp respect users intuitive sense update ordering 
second option concatenate versions file user fix conflict manually 
options application specific resolvers certainly possible implemented 
conflicts regarding file attributes directory entries difficult handle 
fall categories 
conflict directory update operations example alice mv foo alice foo bob mv foo bob foo concurrently 
want updates take effect 
second category conflict rmdir operation example alice mv foo alice foo bob rmdir alice 
problems difficult handle files may replicated different sets nodes node receive conflicting updates fail detect conflict place 
outline solution fully de scribed 
principle child file foo example parent alice bob dictate outcome conflict resolution writer wins rule 
file backpointer section define file location file system namespace 
implement directory operations mv rm change file backpointer 
replica receives change backpointer reflects change parents creating deleting modifying corresponding entries 
parent directory turn flood change replicas 
practice randomly delay patching subsequent flooding chance replicas file 
illustrates pangaea resolves conflict scenario 
policy resolve mv rmdir conflict replica detects absence directory entry corresponding backpointer re creates entry potentially involves re creating directory ancestor directories recursively way root 
directory pangaea effect merely copy backpointers children 
resolving conflicts directory contents done applying writer wins rule individual entries 
file removed directory directory keeps entry marks dead acts death certificate detect stale change entry arrives 
controlling replica divergence protocols described far provide hard guarantees degree replica divergence consistency achieved eventually 
alleviate problem pangaea introduces option called red button provide users confirmation update delivery 
red button pressed particular file sends pending updates neighboring replicas 
corresponding updates circulate replicas described section 
replica acknowledge harbinger graph neighbors forwarded harbinger acknowledge time avoid replica replies immediately receives harbinger twice 
user pressed red button waits operation fully acknowledged replicas time case user list unavailable replicas 
replica find replica parent directory node name space containment property 
ts foo alice bob bp foo ts bp alice ts bp bob ts mv foo alice foo ts foo alice bob bp foo ts bp alice ts foo bp bob ts mv foo bob foo ts foo alice bob bp foo ts bp alice ts bp bob ts foo update sent bob alice 
ts foo alice bob bp foo ts bp alice ts foo bp bob ts foo example conflict resolution involving files fileid foo fileid alice fileid bob fileid 
ts shows replica timestamp 
bp foo shows backpointer replica indicates file name foo directory 
foo means directory contains entry file foo id timestamp 
bold texts indicate changes previous step 
entries marked foo death certificates 
sites initially store contents 
alice mv foo alice foo 
bob concurrently mv foo bob foo node 
bob update newer timestamp ts alice ts want bob win alice 
alice node receives update bob replica file notice backpointer changed foo foo 
change triggers replica delete entry alice add entry bob 
option gives user confirmation updates delivered remote nodes allows take actions contingent stable delivery colleagues new contents 
red button guarantee single copy serializability prevent users changing file simultaneously 
failure recovery failure recovery pangaea simplified due properties randomized nature replica graphs tolerate operation disruptions idempotency update operations including nfs requests unified logging module allows operation re started 
distinguish types failures temporary failures permanent failures 
currently distinguished simply duration crash permanent node suspected failed continuously weeks 
vast majority failures temporary set different goals 
temporary failures try reduce recovery cost 
permanent failures try clean data structures associated failed node system runs node existed place 
recovering temporary failures temporary failures handled retrying 
node persistently logs outstanding remote operation requests contents update random walk edge addition 
node retries logged updates reboot detects node recovery 
recovery logic may create uni directional edges edges desired maintains important invariant graphs connected replicas reachable hierarchical name space 
pangaea reduces logging overhead flooding logging id modified file keeping deltas memory 
reduce memory footprint node finds deltas unresponsive node sender discards deltas falls back full state transfer 
recovering permanent failures permanent failures handled garbage collection gc module 
gc module periodically scans local disks discovers replicas edges permanently failed nodes 
gc module finds edge failed bronze replica replaces edge performing random walk starting gold replica section 
recovering permanent loss gold replica complex 
gold replica say detects permanent loss gold replica creates new gold replica live node chosen criteria described section 
gold replicas form clique section detect loss 
choice flooded replicas file protocol described section update uni directional links gold replicas 
simultaneously updates local replica parent directory ies backpointer reflect new gold replica set 
change flooded replicas directories 
rarely system transient state multiple gold replicas may initiate protocol simultaneously 
situation resolved writer wins policy described section 
recovering permanent node loss inherently expensive procedure data stored failed node eventually re created 
problem exacerbated pangaea central authority manage locations replicas surviving nodes scan disks discover replicas require recovery 
lessen impact gc module tries discover replicas needs recovery possible single disk scan 
set default gc interval nights reduces scanning overhead dramatically offering expected file availability order nines assuming gold replicas file mean server lifetime days 
system evaluation section evaluates design implementation pangaea 
investigate baseline performance overheads pangaea show performs competitively distributed file systems lan 
measure latency network economy availability pangaea wide area networking environment ways study latency pangaea workloads personal workload andrew benchmark bbs workload involving extensive data sharing 
personal workload show user sees local access latency node connected slow network roaming users benefit fetching personal data nearby sources 
second workload show file shared users pangaea progressively lowers access latency transferring data nearby clients 
demonstrate network economy studying updates propagated widely shared files 
show pangaea transfers data predominantly fast links 
demonstrate effect pervasive replication availability system analyze traces file server show pangaea users far traditional replication policies 
prototype implementation implemented pangaea user space nfs version server sfs toolkit 
prototype implements features described support recovery permanent failures section fragmentary 
pangaea currently consists lines code 
pangaea server maintains types files local file system data files metadata file intention log file 
data file created replica type cpu disk mem mhz quantum atlas wls mb ghz quantum atlas tw mb mhz seagate cheetah lw mb table type number pcs experiments 
cpus versions 
file directory 
node wide metadata file keeps extended attributes replicas stored server including graph edges version vectors 
data files directories metadata file implemented berkeley db library maintains hash table file 
intention log file implemented berkeley db record update operations survive node crash 
berkeley db files managed environments feature supports transactions low level logging 
architecture allows metadata changes multiple files flushed sequential write low level log 
experimental settings compare pangaea linux kernel nfs version server coda running linux ext native file system 
pangaea server serve clients node 
pangaea nfs flush buffers synchronously disk replying client required nfs specifications 
coda supports main modes operation strongly connected mode denoted coda provides open close semantics weakly connected mode denoted coda improves response time write operations asynchronously updates server 
mainly evaluate coda semantics closer pangaea table shows machines evaluation 
machines physically connected mb ethernet 
disks machines large replicas purged pangaea coda 
nfs coda configured single server type machine 
machines clients 
pangaea machines servers applications access files local servers 
workloads andrew type machine experiments 
experiments completely network bound insensitive cpu speeds 
wide area experiments built simulated wan evaluate pangaea reliably variety networking conditions 
routed packets type freebsd node included table running dummynet add artificial delays bandwidth restrictions 
router node fast bottleneck experiments 
baseline performance lan section evaluates pangaea performance lan sequential workload data sharing 
environment pangaea main target conducted study test pangaea ability serve people daily storage needs understand system behavior idealized situation 
created variation andrew benchmark simulates single person engineering oriented workload 
mix operations original andrew benchmark volume data expanded fold allow accurate measurements modern hardware 
benchmark denoted andrew tcl consists stages mkdir creating directories copy copying tcl source files directory stat doing ls source files grep doing du grep source files compile compiling source code 
averaged results runs system confidence interval numbers 
table shows time complete benchmark 
evaluation label pang stands pangaea system gold replicas file 
pangaea performance comparable nfs 
expected systems perform amount buffer flushing main source overhead 
pangaea substantially slower 
pangaea create berkeley db file new directory relatively expensive operation 
pangaea performance independent file replication factor optimistic replication replication processing happens background 
coda weakly connected mode coda fast 
due implementation differences pangaea nfs flush buffers disk update operation coda avoids intercepting low level file access vfs requests small kernel module 
shows network bandwidth benchmark 
overhead defined update messages turn duplicates 
pang involve network activity stores files local server 
numbers pang show effect pangaea harbinger algorithm conserving network bandwidth usage 
benchmark benchmark available www hpl hp com personal 
mb pang pang pang pang overhead received sent coda coda nfs network bandwidth consumed andrew benchmark 
overhead bars show bytes consumed duplicate updates 
numbers bars show percentage overhead 
replicas gold form clique pangaea consumed times bandwidth pang 
network usage nearoptimal bandwidth wasted 
table shows network bandwidth consumption common file system update operations 
operations creating file writing byte show high percentage overhead sent directly minor impact wasted bandwidth size small 
hand bulk writes majority traffic incur overhead 
seconds ms rtt mb ms rtt mb ms rtt mb ms rtt mb pang pang pang pang nfs coda compile grep stat copy mkdir andrew tcl benchmark results node slow network link 
labels bars indicate link speeds 
pangaea links servers nfs coda links clients server 
nfs took seconds mb network finish hours mb network 
performance personal workload wans ran andrew tcl benchmark study performance systems wans personal workload 
workload involves data sharing elapsed time depends latency capacity link client server 
shows time needed complete benchmark 
pangaea pang pang pang pang nfs coda coda ext mkdir copy stat grep compile total table andrew tcl benchmark results lan environment 
numbers seconds 
label pang shows pangaea performance creates replicas new file 
ext linux native local file system 
pang pang pang pang nfs coda coda bytes bytes overhead bytes overhead bytes overhead bytes bytes bytes create write write kb write mb table network bandwidth consumption common file system operations 
shows total number bytes transmitted nodes operation 
overhead shows percentage bandwidth duplicate updates 
coda totally hide network latency benchmark designed reads source data local disk systems propagate updates nodes background 
hand performance nfs degrades severely slow links 
roaming roaming single user moving different nodes important distributed file systems 
expect pangaea perform non uniform networks nodes connected networks different speeds 
simulated roaming nodes stores files initially server case coda type nodes 
run andrew tcl benchmark completion node delete files re run compilation stage benchmark node 
vary parameters link speed link speed seen performance depends parameters 
shows results 
shows network uniform nodes placed close far apart pangaea coda perform comparably 
non uniform networks pangaea achieves better performance coda transferring data nearby nodes 
contrast coda clients fetch data server 
pangaea performs slightly better uniformly slow networks 
surmise reason pangaea uses tcp data transfer coda uses udp protocol 
seconds mb mb mb pang coda mb mb mb mb mb mb mb result recompiling tcl source code 
mb mb example means link client nodes link right side picture mb link benchmark client server link mb speed links irrelevant experiment 
data sharing non uniform environments workload characteristics wide area collaboration systems known 
created synthetic benchmark modeled bulletin board system 
benchmark articles files continuously posted updated nodes chosen uniformly random randomly chosen nodes users fetch new articles read 
file system performance measured metrics mean latency reading file accessed server wide area network bandwidth consumption files updated 
numbers depend file size number existing replicas pangaea perform short cut creation order replicas created affects shape graph 
choose article size man lan ms rtt mb ms rtt mb ms rtt mb simulated network configurations modeled corporate network 
gray circle represents sf bay area metropolitan area network man upper bubble represents bristol uk bubbles represent india israel japan 
number circle shows number servers running lan 
kb size typical usenet 
try average final parameter creating reading random files sample point computing mean 
run article posters readers constant speed articles posted read second performance metrics independent request inter arrival time 
benchmark run multiple servers single physical node build configuration realistic size 
avoid overloading cpu disk choose run virtual servers type machine table virtual servers machines total servers physical nodes 
shows simulated geographical distribution nodes modeled hp corporate network 
logistical reasons coda compare versions pangaea pang pangaea gold replicas new file 
hub configuration replica management creating file gold replica server chosen available servers uniformly random 
bronze replicas connect gold replica 
updates issued replica routed gold replica 
roughly corresponds coda 
random configuration creates graph simple random walks considering gold replicas network proximity 
chosen test effect pangaea graph construction policy 
expect pangaea access latency reduced replicas added increases chance file contents transferred new replica nearby existing replica 
confirms prediction 
contrast hub configuration shows speedup matter replicas file exist fetches data central replica 
shows network bandwidth consumption file updates 
systems consume total amount traffic update mean access latency file replicas hub random pangaea average time needed read new file collaborative environment 
axis shows number existing replicas file 
axis shows mean latency access file node store replica file 
wan transfer replicas hub wan random wan pang wan wide area network bandwidth usage file updates 
axis shows percentage traffic routed indicated networks 
wan man shows traffic flowed non lan ms rtt wan shows traffic flowed networks ms rtt see 
replicas pangaea uses far widearea network traffic transfers data preferentially fast links dynamic spanning tree construction section 
trend accentuated replicas created 
shows time pang configuration took propagate updates replicas files experiment 
max lines show large fluctuations updates travel ms rtt links multiple times tcp 
numbers independent number replicas specific network configuration propagation delay depends graph diameter configuration 
believe seconds average seconds maximum delay propagating kb contents ms mb links reasonable 
fact time spent waiting constructing spanning tree section cutting delay parameter shrink propagation latency potentially worsen network bandwidth usage 
propagation latency replicas delta max harbinger max delta mean harbinger mean time needed propagate updates replicas 
dashed lines show time needed distribute replicas 
represent window inconsistency time users may observe old contents 
solid lines represent time needed distribute actual updates 
represent number seconds users wait seeing new contents 
mean lines show mean time needed update issued replica arrive replicas file specific number replicas 
max lines show maximum time observed update arrive replicas file 
performance network economy large scale previous section demonstrated pangaea ability fetch data nearby source distribute updates fast links small scale 
section investigates benefits hold truly large scale discrete event simulator runs pangaea graph maintenance update distribution algorithms 
extracted performance parameters real testbed previous section ran essentially workload 
test network configurations 
configuration called hp number nodes lan increased fold total nodes 
second configuration called keeps size lan nodes increases number regions connects regions ms rtt mb links 
figures show average file read latency network bandwidth usage configurations 
figures show trend differences configurations pronounced 
particular hp configuration pangaea propagates updates entirely local area network popular files crosses wide area links fixed number times regardless number replicas 
configuration pangaea saves bandwidth visibly replicas exist 
systems improve read latency accesses forced go wide area links pangaea shows improvement replicas 
mean access latency file replicas central random pang central hp random hp pang hp file reading latency simulated node system 
meaning numbers 
wan transfer replicas central random pang central hp random hp pang hp wide area network bandwidth usage file updates simulated node systems 
meaning numbers 
availability analysis section studies effects pervasive replication especially name space containment system availability 
pangaea server replicates just replicas accessed directly users intermediate directories needed look replicas 
expect pangaea disrupt users traditional approaches replicate files directories fixed number nodes 
perform trace analysis verify prediction 
types configurations compared pangaea gold replicas file system replicates entire file system contents nodes 
trace collected departmental file server contains users total accesses files 
simulate wide area workload single node trace assume user different node simulated configurations contain nodes 
configuration start empty file system feed half trace warm system 
artificially introduce remote node crashes wide area link failures 
simulate situation failure pang pang pang fixed fixed fixed fixed failures availability analysis file system trace users failed node move functioning node 
numbers parentheses show storage consumption normalized pang 
crash random nodes redirect accesses user failed node random node 
simulate link failures nodes isolated rest crash random nodes throw away activities users crashed nodes 
run second half trace observe users sessions complete successfully 
run simulation times configuration different random seeds average results 
shows results 
network partitioning pangaea wins huge margin shows near availability pervasive replication configurations rely remote servers file operations 
node failures differences smaller 
observe storage overhead pangaea offers better availability 
pangaea wide area file system targets needs data access sharing distributed communities users 
federates commodity computers provided users 
pangaea built design principles pervasive replication provide low access latency high availability randomized graph replica management adapts changes system conserves wan bandwidth optimistic consistency allows users access data time 
evaluation pangaea shows pangaea fast efficient distributed file systems lan 
benefits pervasive replication adaptive graph protocols clear heterogeneous environments typical internet large intranets 
environments pangaea outperforms exist define session directory operation unlink series system calls file including open close 
system calls fails consider session fail 
ing systems aspects access latency efficient usage wan bandwidth file availability 
shepherd peter druschel anonymous reviewers members group hp labs especially eric anderson mahesh kim keeton susan spence ram swaminathan john wilkes offering invaluable feedback improved quality 
atul adya william bolosky miguel castro gerald john douceur john howell jacob lorch marvin theimer roger wattenhofer 
farsite federated available reliable storage incompletely trusted environment 
th symp 
op 
sys 
design impl 
osdi boston ma usa december 
thomas anderson michael dahlin neefe david patterson drew roselli randolph wang 
serverless network file systems 
th symp 
op 
sys 
principles sosp pages copper mountain usa december 
william bolosky john douceur david ely marvin theimer 
feasibility serverless distributed file system deployed existing set desktop pcs 
conf 
measurement modeling comp 
sys 
sig metrics pages santa clara ca usa june 
callaghan pawlowski 
rfc nfs version protocol specification 
www faqs org rfcs rfc html june 
pei cao sandy irani 
cost aware www proxy caching algorithms 
st usenix symp 
internet tech 
sys 
usits monterey ca usa december 
frank dabek frans kaashoek david karger robert morris ion stoica 
wide area cooperative storage cfs 
th symp 
op 
sys 
principles sosp pages lake louise ab canada october 
alan demers daniel greene carl hauser wes irish john larson 
epidemic algorithms replicated database maintenance 
th symp 
princ 
distr 
comp 
podc pages vancouver bc canada august 
armando fox eric brewer 
harvest yield scalable tolerant systems 
th workshop hot topics operating systems hotos vi pages rio rico az usa march 
www csd uch gr markatos papers hotos ps 
francis jamin jin jin raz shavitt zhang 
idmaps global internet host distance estimation service 
ieee acm trans 
networking ton october 
richard golding darrell long john wilkes 
distributed bibliographic database system 
usenix winter tech 
conf san francisco ca usa january 
jim gray 
census tandem system availability 
ieee trans 
reliability october 
michel dubois 
implementation evaluation update cache protocols relaxed memory consistency models 
generation computer systems june 
john howard michael kazar menees david nichols satyanarayanan robert sidebotham west 
scale performance distributed file system 
acm trans 
comp 
sys 
tocs 
ji felten wang singh 
island file system highly available scalable internet services 
usenix windows systems symposium august 
leonard jr steven raymond irene greif 
replicated document management group communication system 
conf 
comp supported coop 
cscw chapel hill nc usa october 
kim cox brian noble 
safety visibility performance wide area file system 
usenix conf 
file storage sys 
fast monterey ca january 
usenix 
john kubiatowicz david bindel yan chen steven czerwinski patrick eaton dennis geels ramakrishna gummadi sean rhea hakim weatherspoon weimer chris wells ben zhao 
oceanstore architecture global scale persistent storage 
th int 
conf 
arch 
support prog 
lang 
op 
sys 
asplos ix pages cambridge ma usa november 
kumar satyanarayanan 
flexible safe resolution file conflicts 
usenix winter tech 
conf pages new orleans la usa january 
david mazi res 
toolkit user level file systems 
usenix annual tech 
conf boston ma usa june 
lily maria ebling satyanarayanan 
exploiting weak connectivity mobile file access 
th symp 
op 
sys 
principles sosp pages copper mountain usa december 
muthitacharoen chen david mazi res 
low bandwidth network file system 
th symp 
op 
sys 
principles sosp pages lake louise ab canada october 
muthitacharoen robert morris gil chen 
ivy read write peer peer file system 
th symp 
op 
sys 
design impl 
osdi boston ma usa december 
scott parker gerald popek gerard allen stoughton bruce walker evelyn walton johanna chow david edwards stephen charles kline 
detection mutual inconsistency distributed systems 
ieee trans 
software engineering se 
prabhakar 
randomized web cache replacement scheme 
infocom anchorage usa april 
david ratner 
roam scalable replication system mobile distributed computing 
phd thesis uc los angeles 
tech 
report 
ucla csd 
luigi rizzo 
dummynet info iet unipi 
luigi ip dummynet 
antony rowstron peter druschel 
storage management caching past large scale persistent peer peer storage utility 
th symp 
op 
sys 
principles sosp pages lake louise ab canada october 
yasushi saito christos 
replica consistency management pangaea wide area file system 
technical report hp labs 
published 
yasushi saito jeffrey mogul ben verghese 
usenet performance study september 
www research digital com wrl projects 
software 
berkeley database 
com 
susan spence erik riedel magnus karlsson 
adaptive consistency patterns sharing networked world 
technical report hpl ssp hp labs february 
douglas terry marvin theimer karin petersen alan demers mike spreitzer carl hauser 
managing update conflicts bayou weakly connected replicated storage system 
th symp 
op 
sys 
principles sosp pages copper mountain usa december 
thekkath timothy mann edward lee 
frangipani scalable distributed file system 
th symp 
op 
sys 
principles sosp pages st malo france october 
robbert van renesse yaron minsky mark hayden 
gossip style failure detection service 
ifip int 
conf 
dist sys 
platforms open dist 
middleware 
www cs cornell edu info people papers ps 
werner vogels 
file system usage windows nt 
th symp 
op 
sys 
principles sosp pages kiawah island sc usa december 
bruce walker gerald popek robert english charles kline greg thiel 
locus distributed operating system 
th symp 
op 
sys 
principles sosp pages woods nh usa october 
yu amin vahdat 
costs limits availability replicated services 
th symp 
op 
sys 
principles sosp pages lake louise ab canada october 
