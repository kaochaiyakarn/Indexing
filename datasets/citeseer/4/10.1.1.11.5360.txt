instance level constraints space level constraints making prior knowledge data clustering dan klein klein cs stanford edu kamvar kamvar stanford edu christopher manning manning cs stanford edu department computer science stanford university stanford ca usa improved method clustering presence limited supervisory information pairwise instance constraints 
allowing instance level constraints inductive implications able successfully incorporate constraints wide range data set types 
method greatly improves previously studied constrained means algorithm generally requiring half constraints achieve accuracy range real world data robust constrained 
additionally discuss active learning algorithm increases value constraints 

large datasets available online extensive hand labeling costly time consuming standard supervised learning algorithms infeasible 
part goal pattern discovery labeling instances may known 
cases gathering large amount unlabeled data cheap easy may able get small amount prior knowledge instance level constraints indicating particular items similar dissimilar 
consider types constraints introduced wagstaff cardie instances known class case say linked known different classes case say linked 
types constraints intuitively appealing task data clustering goal group similar instances 
natural way encode background knowledge class labels known priori 
instance task protein function prediction genome sequence data augmented knowledge functional links proteins eisenberg 
functional links experimental means phylogenetic profile method gene neighbor method complement similarity information automatically computed sequence data 
collaborative filtering user may wish modify recommendations knows priori books alike alike 
depending nature problem source background knowledge types constraints may 
task constrained clustering closely related problem semi supervised learning goal induce class labels data small training set 
important note information pairwise constraints explore weaker information labeled data 
class labels generate pairwise constraints pairwise constraints give information pairs instances partially label data sets 
idea background knowledge constrain clustering widely explored gordon wagstaff :10.1.1.20.7363
novel consideration spatial inductive interpretation constraints presentation active constraint selection strategy 

instance vs space level constraints important clustering algorithm satisfy known constraints equally important algorithm satisfy implications constraints 
example sets clusters satisfy diagonal link constraints clearly intuitive partitioning 
constraints suggest space level generalizations explicit instance level assertions points linked cluster points near points probably cluster 
links similar spatial generalizations 
data instance bias spatial bias 
effects adding diagonal link constraints data instance level inductive bias results single outliers stronger space level bias results qualitative changes clusters 
previous algorithms cop cobweb wagstaff cardie cop means wagstaff designed task clustering constraints failed show marked improvement unsupervised counterparts addition constraints :10.1.1.20.7363
constrained variant cobweb fisher incremental partitioning algorithm cop means ckm constrained incremental assignment variant standard means km clustering mcqueen 
algorithms check assignment see instance assigned linked linked previously assigned instance assignment accordingly 
major flaw algorithms fail utilize space level implications suggested constraints words mechanism propagating constraints 
exhibit outlier behavior seen 
clusters produce may consistent constraints consistent natural implications constraints 
important stress constrained clustering problem induction subject differing induction principles 
principle propose elements involved pairwise constraints generally representative local neighborhoods 
supplying constraints express purpose known outliers induction principle apply cases better algorithm cop means exhibits behavior 
bias unnatural starting point clustering naturally apply fixing mistakes existing clustering 
propose algorithm addresses problem distorting pairwise proximities instances reflect spatial information constraints 
proximity clustering done distorted proximities 
entire algorithm called constrained complete link ccl performs substantially better previously proposed algorithms empirical studies 

scenarios clustering 
distant tight clusters easily detected constraints 
odd shaped noncontiguous clusters detected easily constraints 
clusters separated feature space detected constraints 

constraint applicability presenting details algorithm helpful identify cases adding constraints useful pattern discovery classification 
data naturally form tight clusters need background knowledge reasonable clustering algorithm detect desired clusters 
likewise distinction classes feature space little useful information data constraints little 
background knowledge useful patterns partially data clustering algorithm detect correctly assistance 
situation arise ways 
focus case fine proximity structure feature space strongly correlated underlying similarity coarse proximity structure may misleading 
show extreme examples cases 
examples may contrived real data characteristics lesser degree method works real data examples 
goal take feature space proximities sparse collection pairwise constraints indicate ways feature space target similarity space cluster space generally feature space altered accommodate constraints 
alteration may involve radical change topology original space allows entirely new clusters detected involve small deformations improve boundaries correct clusters 
case similarity space constructed feature space global linear transformation feature weighting may appropriate way improve performance 
zhu 
describe heuristic method selecting feature weights pairwise constraints 

imposing propagating constraints outline general algorithm follows 
proximity matrix instances data set set constraints pairwise cluster decision assertions 
create new proximity matrix basis constraints implications 
supply new matrix proximity clustering algorithm 
distortion algorithm goals 
distorted proximities specific items known class close items different classes far apart 
adjusting feature space manner example increasing distance linked points called imposing constraints corresponds instance level view constraints 
secondly distort entries proximity matrix reflect intuitions 
points close points close close points far apart points close far adjusting proximities satisfy intuitions corresponds space level view constraints 
adjustment called propagating constraints illustrated 
notice intuitions trivially satisfied triangle inequality metric input proximity matrix constraints 
imposing constraints may lose 
way propagating constraints find way restoring proximities maintaining constraints imposed 
imagine various methods proximity distortion various clustering algorithms give concrete method sections pseudocode figures 

links case constraints link pairs imposing constraints involve shortening certain entries proximity matrix 
concretely interpret proximity matrix weights complete graph data points impose link constraints lowering distance linked pair zero 
original proximities metric arc directly connecting points shortest path points 
imposing constraints violated triangle inequality shortest path property specific 
constrained pairs implications nearby points 
close constraining away push constraining pull feature space similarity space 
clusters distant feature space brought similarity space propagated link constraint 
way points previously distance apart may closer path skips constrained pairs 
find new metric respects new constrained entries running pairs shortest paths algorithm imposed matrix 
resulting path length matrix metric faithful original distances sense 
method imposing constraints allows speed computation pairs shortest path lengths 
particular source goal shortest path points point involved link constraint 
trivial modification floyd warshall algorithm cormen allows pairs shortest paths computation time number points involved link constraint 
assume phase expensive proximity clustering algorithms 

links addition links complicates matters substantially 
find satisfying clustering pure links slightly superlinear time np complete determine satisfying assignment exists links 
practice harder devise satisfactory procedure link constraints included 
example founded ineffective procedure take input proximity matrix constrain linked entries zero constrain linked entries large number max ij allow entries vary 
search complete link clustering standardly runs log time priority queue implementation 
matrix defines metric space arg min metric jd norm 
constrained optimization problem large solve dozen points general purpose solvers permutation data points corresponds triangle inequality ac cb ab mixed constraints satisfying conceptually works practice 
add link constraints pairs shortest paths algorithm section 
gives metric matrix 
impose instance level links setting entries max ij 
explicitly restore choose proximity clustering algorithm indirectly propagate link constraints implicitly restoring time performs merge 
discuss section mention clustering phase effective propagating links 
way division propagation methods appropriate context mentioned satisfying links np complete clustering problem satisfied efficiently 
hard problem approximated heuristic clustering convenient approximate 

clustering complete link hierarchical agglomerative clustering see jain dubes clustering algorithm 
assume basic familiarity complete link cl clustering 
cl merges clusters order proximity closest clusters merged furthest clusters merged 
setting link entries proximity matrix link entries max ij achieve direct operational instance level interpretation constraints modification clustering algorithm 
propagation link constraints occurs merges 
merge cl creates reduced proximity matrix row column 
cl defines distance clusters maximum distance points cluster linked merging cause linked way cl achieves implicit propagation link constraints 

results discussion 
evaluation criteria methods exist cluster evaluation siegel castellan 
target classification known commonly index rand index rand 
rand index views clustering data linkage decision pair data points 
pair con matrix constraints matrix constraints ij ji matrix constraints ik jk matrix constraints ij matrix constraints done implicitly matrix constraints find valid intermediates modified floyd warshall ng ng ij minfd ij ik 
pseudocode constraining input proximity matrix 
matrix constraints matrix clusters fc point ig linkage starts empty distances ij choose closest arg min clusters add linkage merge cnew clusters clusters cnew maxf 
pseudocode constrained complete link clustering 
sidered correct proposed clustering agrees target clustering 
rand index ri correct decisions total decisions value lies perfect agreement 
wagstaff cardie modification rand index suitable constrained clustering 
adding constraints ensures correctness pairs fixed constraints closure 
confine evaluation decisions underdetermined constraints 
cri correct free decisions total free decisions natural evaluation criteria clustering pairwise constraints 
synthetic data sets target clusterings 
circles xor ccl active ccl ckm ccl active ccl ckm ccl active ccl ckm ccl active ccl ckm 
synthetic data sets number constraints vs accuracy 
ccl constrained cl random constraint selection ccl active constrained cl active selection ckm cop means 
circles xor ccl active ccl ckm ccl active ccl ckm ccl active ccl ckm 
real data sets number constraints vs accuracy 
iris crabs soybean 
facilitate comparison previous area 
term accuracy refer cri values 
follows ccl constrained algorithm ckm reimplementation cop means wagstaff :10.1.1.20.7363

synthetic data evaluate system synthetic real world data 
synthetic data designed highlight problems solved effectively ccl unconstrained cl constrained algorithms 
shows target clusterings synthetic sets circles difficult case spherical clustering methods cl km 
difficult common clustering algorithm centers equality proximal feature space 
xor difficult solution linearly separable solvable class km prior knowledge required distinguish target labeling alternate ones 
difficult case spherical clustering methods high 
shows ccl sets 
constraints added randomly choosing data pairs constraining pair target clustering examine active selection section 
case sizable improvement unconstrained accuracy constraints 
ccl spatial propagation allows substantially outperform ckm 
investigate qualitative behavior algorithms shows example results varying numbers randomly chosen constraints 

iris crabs data sets projected principal components 
consider representative 
constraints ccl ckm simply divide data roughly linearly half 
constraints cause ckm slightly alter chosen centers suggested earlier ckm satisfy instance level constraints assigning points different cluster close neighbors essentially creating outliers middle qualitatively unchanged clusters unconstrained data points assignment boundaries voronoi partitions feature space 
behavior persists large numbers constraints 
ccl deforms feature space way circles lie disjoint spheres proximity space 
data sets links able shape proximities way desired clusters easily 
worth pointing non spherical clustering method example clustering detect non spherical synthetic patterns 
section demonstrates algorithm effective real data sets completely ineffective 

real world data give results real world data sets shown 
soybean soybean large data set uci repository 
instances features different classes 
nominal hamming distance default metric 
instances represent different features represent qualitative measurements classes plant diseases 
iris classic iris data fisher 
instances features 
classes relatively separated non spherical 
instances represent different irises features structural dimensions classes iris species 
wagstaff 
reports results simpler smaller soybean small set omit set unconstrained cl yielded perfect clustering 
crabs crabs data campbell 
instances features classes 
instances represent different crabs features represent structural dimensions classes crab species 
data set difficult principal component essentially crab size irrelevant target classification 
shows accuracy ccl constraints added 
constraints improve performance substantially case 
ckm shown ccl outperforms substantially supporting hypothesis spatial induction principle appropriate real data sets 
note soybean example unconstrained cl algorithm performs worse unconstrained km 
ccl exploits constraints quickly ckm accuracy limited number constraints appears ineffective helping ckm algorithm 

constraint types results constraints selected randomly choosing pairs constraining pair target equality 
practice pairs linked 
argued section applications may links links mixes constraints available 
issue especially important context wagstaff 
suggest ckm best exploits link constraints 
test dependence mix shows behaviour ccl different constraint mixes soybean iris crabs data sets 
cases ccl accuracy improves quickly faster ckm constraints added 

active learning real world domain control pairs assay 
case choose pairs believe maximum impact accuracy 
claimed constraint propagation intended case local proximity structure feature space reliable global structure 
case want perform cl unconstrained fashion moderate number clusters remaining 
scientist supplying constraints simply harder top level decisions 
believe valuable intuition doing requires scientist supply quadratic number constraints 
propose gradually feeding constraints algorithm requires requiring small hopefully linear number constraints complete clustering algorithm begins request constraints 
ccl links equal proportion data proportion links ckm links 
equal proportion 
data proportion 
links 
legend 
constraints effective ccl wide range mix types including links links mixes equal proportion proportion relative number pair types data 
iris crabs soybean precisely implemented active learning scheme 
algorithm told allowed pairwise questions 
recall merged cluster distance increasing cl algorithm 
learner clusters data constraints determines distance cutoff asking questions expecting need questions 
clusters merge distance asks roots proposed merge belong 
response imposes constraint accordingly propagates reduced proximity matrix 
selects new merge needed continues 
keeps proposing bad merges exhaust questions single stage 
hand spatial contraction cause merges closer merges may occur question asked 
point questions left continues onward unsupervised 
figures show results active constraint selection 
cases actively chosen constraints effective passively chosen ones 
shows actual constraints chosen synthetic sets 
active selection converges correct structures quickly 

previously proposed algorithms constrained clustering treat pairwise constraints assertions individual instances fail exploit spatial implications constraints 
method inducing spatial effects pairwise constraints demonstrated substantially outperforms previous approaches exhibiting behavior quantitatively superior qualitatively natural 
active learning scheme dramatically decreases number constraints required achieve accuracy 
data sets small numbers constraints passively selected constraints outperformed active selection 
appears active strategy tends focus questions hard regions data cause unequal contraction space 
obstacle cl bias equal radius clusters 
wagstaff helpful discussion criticism anonymous reviewers suggestions corrections 
supported part national science foundation 
iis nsf graduate fellowship research collaboration ntt communication science laboratories telegraph telephone csli stanford university 
campbell 

multivariate study variation species rock crab genus 
australian journal zoology 
cormen leiserson rivest 

algorithms 
cambridge ma mit press 
eisenberg marcotte yeates 

protein function post genomic era 
nature 
fisher 

knowledge acquisition incremental conceptual clustering 
machine learning 
fisher 

multiple measurements problems 
annals 
gordon 

survey constrained classification 
computational statistics data analysis 
jain dubes 

algorithms clustering data 
englewood cliffs nj prentice hall 
mcqueen 

methods classification analysis multivariate observations 
proceedings fifth symposium math statistics probability pp 

rand 

objective criteria evaluation clustering methods 
journal american statistical association 
siegel castellan jr 

nonparametric statistics behavioral sciences 
new york mcgraw hill 
wagstaff cardie 

clustering instance level constraints 
seventeenth international conference machine learning pp 

wagstaff cardie rogers 

constrained means clustering background knowledge 
eighteenth international conference machine learning 
zhu chu zhu caruana 

heuristically inducing distance metric user preferences clustering 
ms carnegie mellon university www cs cmu edu academics ml ps gz 
ccl active selection accuracy accuracy accuracy accuracy ccl passive selection accuracy accuracy accuracy accuracy ckm accuracy accuracy accuracy accuracy constraints constraints constraints constraints ccl active selection accuracy accuracy accuracy accuracy ccl passive selection accuracy accuracy accuracy accuracy ckm accuracy accuracy accuracy accuracy constraints constraints constraints constraints 
examples clustering behavior circles xor iris crabs 
loops indicate link pairs dashed lines indicate link pairs 
