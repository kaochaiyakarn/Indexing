discovery linguistic relations lexical attraction submitted department electrical engineering computer science partial fulfillment requirements degree doctor philosophy massachusetts institute technology may massachusetts institute technology 
rights reserved 
author 
department electrical engineering computer science may certified 
patrick winston ford professor artificial intelligence computer science thesis supervisor accepted 
arthur smith chairman department committee graduate students discovery linguistic relations lexical attraction submitted department electrical engineering computer science may partial fulfillment requirements degree doctor philosophy motivated long term goals understand humans learn language build programs understand language 
representation relevant features explicit prerequisite successful learning understanding 
chose represent relations individual words explicitly model 
lexical attraction defined likelihood relations 
introduce new class probabilistic language models named lexical attraction models represent long distance relations words formalize new class models information theory 
framework lexical attraction developed unsupervised language acquisition program learns identify linguistic relations sentence 
explicitly represented linguistic knowledge program lexical attraction 
initial grammar lexicon built input raw text 
learning processing 
processor uses regularities detected learner impose structure input 
structure enables learner detect higher level regularities 
bootstrapping procedure program trained words associated press material able achieve precision recall finding relations content words 
knowledge lexical attraction program identify correct relations syntactically ambiguous sentences saw statue liberty flying new york 
thesis supervisor patrick winston title ford professor artificial intelligence computer science acknowledgments am grateful carl de marcken valuable insights sharing interest math endless patience 
am thankful mother father importance placed education 
am indebted advisors patrick winston teaching ai boris katz teaching language marvin minsky teaching avoid bad ideas 
contents language understanding acquisition case lexical attraction 
bootstrapping acquisition 
learning process simple sentence 
discovery linguistic relations demonstration long distance links 
complex noun phrase 
syntactic ambiguity 
lexical attraction models syntactic relations primitives language 
lexical attraction likelihood syntactic relation 
context word syntactic relations 
entropy determined syntactic relations 
summary 
bootstrapping acquisition processor 
memory 
results 
critical evaluation 
related 
contributions list figures semantic judgments help bootstrap syntax 
learning processing bootstrap acquisition 
accuracy relations content words 
discovering relations simple sentence 
phrase structure versus dependency structure 
discovering long distance relations 
structure complex noun phrase 
prepositional phrase attachment 
distinguishing syntactically identical sentences 
flying 
possible dependency structures words 
learning processing bootstrap acquisition 
content word link accuracy results 
improvement accuracy trained raw text 
phrase structure representation di cult get local maxima 
chapter language understanding acquisition motivated desire explain language learning hand build programs understand language 
believe goals intertwined 
areas human intelligence language proved amenable small models simple rule systems 
unlocking secrets learning language raw data open path robust natural language understanding 
believe humans learners sophisticated learning algorithms having right representations 
evolution provided cognitive transducers relevant features input explicit 
representational primitives language linguistic relations subject verb verb object 
standard phrase structure formalism indirectly represents relations side ects constituent grouping process 
adopted formalism takes relations individual words basic primitives 
lexical attraction gives likelihood relations 
built language program explicitly represented linguistic knowledge lexical attraction 
grammar lexicon parts speech 
program di erent stages learning processing 
learns processing gets better input 
possible feedback loop learner processor 
regularities detected learner enable processor assign structure input 
structure assigned input enables learner detect higher level regularities 
starting initial knowledge seeing raw text input program able bootstrap acquisition show significant improvement identifying meaningful relations words 
section presents lexical attraction knowledge solution problems language acquisition syntactic disambiguation 
second section describes bootstrapping procedure detail 
third section presents snapshots learning process 
chapter gives examples learning 
chapter explains computational mathematical linguistic foundations lexical attraction models 
chapter describes program results detail 
chapter summarizes contributions 
case lexical attraction lexical attraction measure nity words likelihood words related sentence 
chapter gives formal definition 
main premise thesis knowledge lexical attraction central language understanding acquisition 
questions addressed thesis formalize acquire lexical attraction knowledge 
section argues language acquisition syntactic disambiguation similar problems knowledge lexical attraction powerful tool solve 
language understanding syntax semantics play complementary roles language understanding 
order understand language needs identify relations words sentence 
cases relations may obvious meanings words 
syntactic markers relative positions words may provide necessary information 
consider examples saw statue liberty flying new york 
hit boy girl long hair hammer 
sentence subject object may doing flying 
common interpretation saw statue liberty flying new york 
sentence saw airplane flying new york people attribute flying airplane 
sentences syntactically similar decision words related 
sentence ends prepositional phrases 
phrases potentially modify subject verb object noun previous prepositional phrase subject certain constraints discussed chapter 
words syntax leaves question words related sentence open 
reader decides likelihood potential relations 
green ideas sleep 
contrast sentence classical example illustrate independence grammaticality meaningfulness words sentence go meaningful way tell relations syntactic clues 
examples illustrate syntax semantics independently constrain possible interpretations sentence 
cases syntax semantics get unique interpretation general need 
need semantics particular likelihood various relations words 
sentence chomsky chomsky 
sentence attributed lenat 
sentence schank schank colby 
language acquisition children start mapping words concepts full grasp syntax 
stage problem facing child disambiguation problem sentences 
cases listener trying identify relations words sentence syntax help 
case child syntactic rules known 
case ambiguous sentences syntactic rules di erentiate various possible interpretations 
similar problems call similar solutions 
just able interpret ambiguous sentences relying likelihood potential relations child interpret sentence unknown syntax way 
john ice cream eat john eats ice cream semantic judgments help bootstrap syntax 
illustrates language acquisition path 
exposure language input teaches child words map concepts 
experience world teaches likelihood certain relations concepts 
knowledge possible identify certain linguistic relations sentence complete syntactic analysis possible 
pre syntax identification linguistic relations syntactic acquisition bootstrapped 
sentence john eats ice cream john subject eating ice cream object 
english relies svo word order identify roles 
languages may di erent word ordering syntactic markers 
child identifies subject object semantically may able learn syntactic rule particular language uses 
syntactic rules child identify obvious relations sentence guess meanings unknown words syntactic role 
language acquisition disambiguation knowing words related central importance 
knowledge formalized concept lexical attraction 
bootstrapping acquisition learning encoding world experience computers turned challenging problem 
current common sense reasoning systems primitive stages 
suggests alternative large corpora gather information likelihood certain relations words 
large corpora presents chicken egg problem 
order gather information likelihood words related able detect related 
requires knowing syntax trying learn place 
learning processing bootstrap acquisition 
get loop learning program needs bootstrapping mechanism 
key bootstrapping lies learning processing 
illustrates feedback loop 
initial knowledge syntax processor starts making inaccurate analyses memory starts building crude lexical attraction knowledge 
knowledge eventually helps processor detect relations accurately results better quality lexical attraction knowledge memory 
idea built language learning program bootstraps initial knowledge reads examples free text learns discover linguistic relations form basis language understanding 
program evaluated accuracy relations content words nouns verbs adjectives adverbs 
accuracy measured precision recall 
precision defined percentage relations program correct 
recall defined percentage correct relations program 
program able achieve precision recall 
previous unsupervised language acquisition showed little improvement started zero knowledge 
shows improvement program shows 
detailed results chapter 
percentage number words trained procedure recording pairs selected processor precision recall accuracy relations content words learning process simple sentence shows program gradually discovers correct relations simple sentence 
denotes number words training 
words people want government money education people want government money education people want government money education people want government money education people want government money education discovering relations simple sentence 

symbol marks sentence 
links undirected 
chapter show directions links immaterial training process 
training program information links 
words program discovered period usually ends sentence word frequently starts 
words changed 
frequent collocation money discovered 
words link left marker 
notice want example starts sentence 
linked left marker links formed program able see longer distance correlations 
lack meaningful links point explained nature word frequencies 
typical word english frequency range 
word frequency formula zipf law rank word zipf shannon 
means words training program seen words twice determine correlations 
words program discovers interesting links 
word people related want modifies people modifies want 
link result having seen instances reason links word deserves explanation 
separate english words rough classes called function words content words 
function words include closed class words usually grammatical function prepositions conjunctions auxiliary verbs 
content words include words bearing actual semantic content nouns verbs adjectives adverbs 
function words typically frequent 
frequent function word seen time typically range 
means program discovers function word function word links period 
function word content word links discovered ones connecting 
content word content word links discovered 
words training program able discover correct links sentence 
verb connected subject object 
modifiers connected heads 
words money education related preposition linked 
chapter discovery linguistic relations demonstration chapter presents snapshots learning process 
underlying theory algorithm follows chapters 
examples chapter chosen illustrate handling various linguistic phenomena 
formal performance results critical evaluation program shortcomings chapter 
syntax represented dependency formalism 
contrasts phrase structure dependency representations sentence 
phrase structure representation forming higher order units combining words phrases 
dependency representation explicit representation relationships individual words 
chapter gives formal definition 
dt np john ate cake np vp john ate cake phrase structure versus dependency structure 
examples chapter program trained corpus associ ated press newswire material stopped various points training example sentences processing 
long distance links cause death friday cause death friday cause death friday discovering long distance relations 
links spanned words 
shows program capable handling longer distance relations 
sentence long noun phrase headed noun cause 
cause link spans length sentence 
words see interesting discovered 
words program able relate cause death longer distance relations missing 
words training attraction word cause word discovered correct link created 
ap newswire data tipster information retrieval text research collection 
complex noun phrase new york stock exchange composite index fell new york stock exchange composite index fell new york stock exchange composite index fell new york stock exchange composite index fell structure complex noun phrase 
di cult problems non lexicalized language systems analyze structure complex noun phrase 
noun phrase new york stock exchange composite index turns determiner adjective noun noun noun adjective noun seen just parts speech 
parts speech give information assign meaningful structure phrase 
program collects information individual words concept parts speech 
able discover structure complex noun phrase pieces noun phrase repetitively 
words discovers group new york 
words discovers stock exchange 
words discovers composite index 
words figures correct relations pieces 
syntactic ambiguity argued chapter need semantic judgments interpret syntactically ambiguous sentences 
specifically need information likelihood various relations words lexical attraction information 
section presents examples syntactic ambiguity demonstrate lexical attraction information helps resolve ambiguity 
shows prepositional phrase attachment problem 
sentence ends prepositional phrases starting word 
syntax uniquely determine attached 
words program decided final attachment 
words words learns relate died clashes clashes west september died 
note died west clashes september meaningful phrases 
links discovered program stronger attraction 
people died clashes west september people died clashes west september prepositional phrase attachment 
illustrates common type ambiguity related phrase 
english preposition particularly ambiguous semantic function quirk 
function similar genitive gravity earth earth gravity constructions bottle wine 
sentences syntactically identical 
phrase number people subject 
people doing second number increasing 
words training lexical attraction information su cient find correct subject 
number people number people increased distinguishing syntactically identical sentences 
presents final example analogous sentence previous chapter 
replaced words ones frequent corpus 
sentence ambiguous doing flying 
program able link pilot flying case airplane flying second case lexical attraction 
pilot saw train flying washington driver saw airplane flying washington flying 
chapter lexical attraction models probabilistic language model representation linguistic knowledge assigns probability distribution possible strings language 
chapter presents new class language models called lexical attraction models 
framework lexical attraction models possible represent syntactic relations form basis extraction meaning 
syntactic relations defined pairwise relations words identifiable surface form sentence 
lexical attraction formalized mutual information captured syntactic relation 
set syntactic relations sentence represented undirected acyclic planar graph show entropy lexical attraction model determined mutual information captured syntactic relations model 
prelude central idea chapter search low entropy language model leads unsupervised discovery syntactic relations 
syntactic relations primitives language defining property human language compositionality ability construct compound meanings combining simpler ones 
meaning sentence john kissed mary just list concepts john kiss mary 
part meaning captured way concepts brought syntactic relation links drawn words cross seen combining di erent way mary kissed john di erent meaning 
language gives ability describe concepts certain semantic relations placing corresponding words certain syntactic positions 
languages restricted small number function words xes word order expression di erent relations dimensional nature language 
number di erent syntactic arrangements limited 
define set relations words identifiable syntactic domain syntactic relations 
examples syntactic relations subject verb relation verb object relation prepositional attachments 
limited number possible syntactic relations mapping conceptual relations syntactic relations 
compare sentences saw book burnt book 
conceptual relation see book di erent burn book 
verb see action ect object burn 
expressed verb object relation 
di erent types syntactic relations constrain expressed distinguished language 
take syntactic relations representational primitives language 
phrase structure representation sentence reveals words phrases combine form higher order units 
actual syntactic relations words implicit groupings 
phrase structure formalism fact john subject kissed expressed saying john head noun phrase direct constituent sentence verb phrase headed verb kissed 
chose syntactic relations representational primitives reasons 
indirect representation phrase structure unsupervised language acquisition di cult 
second eventual goal extract meaning syntactic relations need phrase structure indirectly helps retrieve 
assuming syntactic relations indirect product phrase structure chose take syntactic relations basic primitives treat phrase structure caused trying express syntactic relations dimensional nature language 
linguistic formalism takes syntactic relations words basic primitives known dependency formalism 
mel cuk discusses important properties syntactic relations book dependency formalism mel cuk 
large scale implementation english syntax similar formalism sleator temperley uses di erent types syntactic relations subject verb verb object determiner noun sleator temperley :10.1.1.12.1238
di erentiate di erent types syntactic relations 
goal learning program described chapter correctly determine words sentence syntactically related 
learning di erentiate di erent types syntactic relations unsupervised manner discussed chapter 
lexical attraction likelihood syntactic relation order understand sentence needs identify words sentence syntactically related 
lexical attraction likelihood syntactic relation 
section formalize lexical attraction framework information theory 
shannon defines entropy discrete random variable log ranges possible values random variable probability value shannon cover thomas 
consider sequence tokens drawn independently discrete distribution 
order construct shortest description sequence token encoded log bits average 
log defined information content token entropy interpreted average information token 
english sentence information content word assuming words word probabilities estimated word corpus dominated news material 
independently selected 
note information content lower frequently occurring words 
ira fighting british rule northern ireland care encoding compression 
total information sentence sum information word bits 
mathematically equivalent statement probability seeing sentence product probabilities seeing word equivalence entropy probability assigned input 
probabilistic language model assigns probability distribution possible sentences language 
maximum likelihood principle states parameters model estimated maximize probability assigned observed data 
means language model achieves lowest entropy 
model achieve lower entropy account relations words sentence 
consider phrase northern ireland 
independent probability northern seen ireland time 
way saying northern carries bits information adds bits new information ireland 
dependency northern ireland encoded bits bits 
bit gain correlation words called mutual information 
lexical attraction measured mutual information 
basic assumption words high lexical attraction syntactically related 
context word syntactic relations northern ireland example shows information content word depends related words context 
context word turn determined language model 
section describe new class language models called lexical attraction models 
lexical attraction framework possible represent linguistically plausible context word 
choice context language model implies certain probabilistic independence assumptions 
example gram model defines context word words immediately preceding 
diagram gives information content words bigram model 
arrows show dependencies 
information content northern ireland di erent previous section di erent dependencies 
assumption word conditionally independent adjacent words 
information content word computed conditional probability previous word 
result encoding sentence reduced bits bits 
ira fighting british rule northern ireland words sentence completely independent 
fact beeferman report words continue show selectional influence window words beeferman berger la erty 
degree dependency falls exponentially distance 
justifies choice gram models relate dependency proximity 
previous words context linguistic intuition 
sentence man dog spoke selection spoke influenced man independent previous word dog 
follows context word better determined linguistic relations fixed pattern 
words direct syntactic relation strong dependencies 
chomsky defines dependencies selectional relations chomsky 
subject verb example selectional relation verb object 
subject object hand assumed chosen independently 
noted independence approximation 
sentences doctor examined patient lawyer examined witness show subject strong influence choice object 
second degree ects discussed chapter 
diagram gives information content words sentence direct syntactic relations ira fighting british rule northern ireland arrows represent head modifier relations words 
information content word computed conditional probability head 
marked verb governing auxiliary noun governing preposition may look controversial linguists 
information theory perspective mutual information content words higher function words 
model favor function word heads 
probabilities estimated counting occurrences pair relative position 
linguistic dependencies reduce encoding words sentence bits compared bits bigram model model certain independence assumptions number parameters prohibitively large learn 
choice independence assumptions determine context word 
assumption word depends word sentence necessarily adjacent word gram models 
define class language models assumption lexical attraction models 
lexical attraction models possible define context word terms syntactic relations 
numbers take account encoding dependency structure 
entropy determined syntactic relations define set probabilistic dependencies imposed syntactic relations sentence dependency structure sentence 
strength links dependency structure determined lexical attraction 
section formalize dependency structures markov networks show entropy language model determined mutual information captured syntactic relations 
markov network undirected graph representing joint probability distribution set variables pearl 
vertex corresponds random variable word case 
structure graph represents set conditional independence properties distribution variable probabilistically independent non neighbors graph state neighbors 
see interesting properties dependency structure diagram graph formed syntactic relations acyclic links connecting words cross 
section see directions links ect joint probability sentence links undirected 
discuss properties derive formula entropy model 
dependency structure acyclic syntactic relations sentence form tree 
trees acyclic 
linguistically word sentence unique governor head word governs sentence assume word probabilistically depends governor resulting dependency structure rooted tree diagram 
opposed bayesian networks directed 
see mel cuk discussion 
dependency structure planar sentences natural languages property syntactic relation links drawn words cross 
property called planarity sleator temperley projectivity mel cuk adjacency hudson various researchers 
examples illustrate planarity english 
sentence easily seen woman red dress meeting afternoon 
sentence interpretation possible 
fact plausible john red dress 
john met woman red dress afternoon john met woman afternoon red dress gaifman gave formal analysis dependency structures satisfy planarity condition gaifman 
gives natural correspondence dependency systems phrase structure systems shows dependency model characterized planarity context free 
sleator temperley show planar model context free allows cycles sleator temperley :10.1.1.12.1238
lexical attraction symmetric lexical attraction words symmetric 
mutual information matter direction dependency goes 
directly follows bayes rule 
obvious choice head word corresponding dependency directions imposes ect joint probability sentence 
joint probability determined choice pairs words linked 
dependency structures formalized markov networks undirected 
consider northern ireland example northern ireland northern ireland case conditional probability northern word ireland 
second case conditional probability ireland previous word northern 
cases encoding words bits fact log joint probability northern ireland 
natural representation link direction label shows number bits gained mutual information northern ireland generalize result representation sentence ira fighting british rule northern ireland theorem probability sentence dependency structure depend choice head word 
proof consider sentence 

denote words links respectively 
denote probability sentence having dependency structure assume head word word probabilistically depends governor 
joint probability sentence expression final expression plays special role starting head word arrived result 
choice head corresponding directions imposed links immaterial probability sentence 
encoding dependency structure linear factor represents prior probability language model assigns dependency structure 
gram models possible dependency structure 
probabilistic context free grammars probabilities assigned grammar rules impose probability distribution possible parse trees 
model assume uniform distribution possible dependency structures number possible dependency structures 
planarity condition number possible dependency structures word sentence cayley formula harary 
encoding dependency structure take log bits 
encoding planar dependency structures linear number words theorem shows 
theorem number possible dependency structures word sentence 
proof consider sentence words 
leftmost word connected rest sentence links 
links undirected impose direction head argument simpler 
leftmost child split rest sentence groups left descendants span right descendants span rest sentence spans groups empty 
notation denotes span words 




problem counting number dependency structures words headed split smaller versions problem count number structures headed headed headed decomposed recurrence relation numbers represent number words respectively 
recurrence fold convolution 
general expression recurrence fold convolution mn mn binomial coe cient graham knuth patashnik 

values 
shows possible dependency structures words 
upper bound number dependency structures obtained inequality cormen leiserson rivest possible dependency structures words logarithm value dividing see encoding planar dependency structure takes log bits word 
entropy determined syntactic relations results hand revealing look information theory perspective 
final expression rewritten log log log interpreted information sentence information words information dependency structure mutual information captured syntactic relations 
average information isolated word independent language model 
showed encoding dependency structure linear number words lexical attraction models 
terms constant contribution word entropy model completely determined mutual information captured syntactic relations 
summary part meaning language captured way words arranged sentence 
arrangement implies certain syntactic relations words 
view extraction meaning adopted linguistic representation takes syntactic relations basic primitives 
defined lexical attraction likelihood words related 
language model determines words related sentence 
example ngram models assume word depends previous words 
defined new class language models called lexical attraction models word depends word sentence necessarily adjacent provided link crossing cycles allowed 
possible represent linguistically plausible head modifier relationships framework 
showed entropy model determined lexical attraction related words 
examples chapter showed linguistic relations words lead lower entropy 
conversely search low entropy language model lead unsupervised discovery linguistic relations show chapter 
chapter bootstrapping acquisition learning processing bootstrap acquisition 
chapter presents bootstrapping mechanism learning identify linguistic relations 
key mechanism learning processing 
way structures built processor help learner detect higher level regularities 
depicts feedback loop memory processor 
training process processor build structure memory raw input 
memory detects low level correlations raw input processor uses information assign simple structure 
simple structure enables memory detect higher level correlations turn enables processor assign complex structure 
bootstrapping mechanism program starts reading raw text initial knowledge reaches precision recall content word links training 
describe implementation processor section memory second section 
followed quantitative results learning process critical evaluation shortcomings 
final section discusses related 
key insights di erentiate approach training words parts speech enables program learn common idiosyncratic usages words 
committing early generalizations prevent program making mistakes 
representation relevant features explicit simplifies learning 
believe humans learners sophisticated learning algorithms having right representations 
processor goal processor find dependency structure assigns sentence high probability 
chapter showed probability sentence determined mutual information captured syntactic relations 
problem find dependency structure highest total mutual information 
optimum solution time 
approximation algorithm 
main reason choice simplicity resulting processor speed 
simplicity important architecture input memory consists states actions processor raw input signal 
order learning easy processor simple small number possible states small number possible actions 
approximation algorithm optimum algorithm 
approximation algorithm possible actions simple processor words linked linked 
words consideration constitute state processor visible memory 
order words come processor attention determined simple control system 
control system reads words left right 
reading new word processor tries link previous words 
link crossing cycle detected weakest links conflict eliminated 
pseudo code algorithm link sentence length downto pop right links stack min right word mi mi mi mi mi stack unlink stack reset stack unlink link push left links stack input procedure link sentence sentence array words 
links created deleted link unlink 
function mi gets mutual information value link memory 
array stack detect cycles link crossings 
th element contains minimum valued link path exists 
moves leftward sentence right links popped left links pushed stack 
right link popped right side new 
link crossing detected new link created nonempty stack 
cycle detected new link created nonempty 
note strong new link crosses multiple weak links accepted weak links deleted new link weaker sum old links 
action results lower total mutual information implemented multiple weak links connected sentence prevented strong meaningful link created 
way directional bias approximation algorithm partly compensated 
get sense link crossing cycle constraints combined knowledge lexical attraction lead identification linguistic relations sequence snapshots processor running simple sentence 
note steps may skipped snapshots words read left right word checks words left possible links 
link consideration marked value square brackets 
accepted links square brackets people algorithm detects cycles eliminates weakest link cycle people negative links accepted people algorithm detects crossing links people want weaker conflicting link gets eliminated people want combination link crossing cycle constraints knowledge lexical attraction help eliminate links correspond syntactic relations 
diagram strongly attracts money result elimination meaningless link government people want government money want money strongly attracted link replaces people people want government money bad links eliminated 
people want government money cycle example new link rejected weak 
example new link strong eliminates old links cycle 
people want government money education final result people want government money education algorithm guaranteed find linkage 
leave words disconnected 
section argues limiting factor performance program accuracy learning process due representational limitations 
marginal gain optimal algorithm significant 
algorithm section performs reasonably average speed simplicity candidate training 
optimal algorithm viterbi style algorithm viterbi finds dependency structure highest mutual information designed decomposition proof theorem 
relevant reproduced convenience 


dependency structure gives highest mutual information span dominated word outside span 
convenience assume leftmost word dominates sentence theorem 
optimal algorithm find 
probability assigns span dominated spans length values spans length algorithm compute spans length follows max max algorithm compute values bottom starting shorter spans computing longer spans previous values 
value corresponding structure recorded 
structure gives answer 
recursive computation takes steps 
length computed span length possible head span means computations 
total computation 
memory memory store lexical attraction information 
lexical attraction word pair computed frequency pair comes processor attention 
information processor deciding link words 
section describe di erent procedures updating memory 
section performance results procedures 
memory may record pairs processor certain state ignore pairs appear di erent states 
di erent memory updating procedures created choosing di erent states record 
diagram illustrates procedure 
memory records pairs adjacent input 
lines show di erent time steps windows show words processor trying link 
kick ball kick ball ball kick basic data structures memory words word pairs counts 
word may appear window processor tries link di erent words 
keep consistent count word keeps track times seen left window times seen right window addition times appeared text 
lexical attraction words estimated mi log log log mi mutual information wild card matching word probability seeing left window seeing right window count total number observations windows 
significant percentage syntactic relations adjacent words 
procedure program discover syntactic relations generally seen adjacent words 
example relate determiners adjectives nouns learn collocations new york times 
diagram illustrates program learns relate determiner noun 
kick ball ball ball throw kick ball technique program learn relationship kick ball 
determiner intervenes 
relations words practically seen 
example prepositional phrase modifying transitive verb hidden object 
second procedure memory updating record word pairs encountered matter far apart sentence 
procedure guaranteed see related word pair point records lot unrelated pairs 
section shows improves recall precision drops expected 
procedures actual structure identified processor 
fact feedback loop processor memory real learning processing 
memory gathers information looking raw data feeds way processor 
third procedure feedback idea 
memory records subset pairs selected structure identified processor 
structure memory behaves procedure recording adjacent pairs 
axb sequence words pairs recorded 
words linked form group 
sense act single word 
ax 
yb sequence words linked processor tries linking words group link crossing rule words linked addition adjacent pairs attracted strongly able break link 
processor attempts link 
memory records pairs 
kick ball kick ball ball kick kick ball kick ball ball kick ball kick diagram shows third procedure leads discovery relation kick ball 
left program uses rule previous paragraph updates pairs kick ball 
kick ball pair frequently reinforced high mutual information detected 
pair seen expected mutual information stay low 
right illustrates long distance link formed correlation identified 
section shows learning processing improves precision recall 
results section presents results accuracy program finding relations content words 
chose base evaluation content word links essential extraction meaning 
content words words convey meaning nouns verbs adjectives adverbs opposed function words convey syntactic structure prepositions conjunctions 
mistakes content word links significantly important mistakes links 
sentences illustrate di erence 
sentence mistake result choosing wrong subject flying 
second sentence program detected relation money education way word links important 
saw mountains flying new york people want money education shows results procedures described previous section 
programs trained words associated press material 
training sentences test data precision recall measured 
sample sentences hand parsed 
total words averaging words sentence content word links test set 
choice content word links evaluation metric significant 
people agree content words related sentence professional linguists argue link function words 
sources mistakes program 
links unknown word sentence 
mistakes failure processor learning paradigm 
order isolate test sentences restricted vocabulary frequent words account words seen corpus 
accuracy measured precision recall 
recall defined percentage content word links human parsed test set recovered program 
precision defined percentage content word links program human parsed test set 
help reader judge quality results give numbers comparison 
algorithm described processor consults memory lexical attraction words time considering new link 
program modified numbers supplied randomly precision recall gives lower bound 
percentage number words trained procedure recording adjacent pairs precision recall number words trained procedure recording pairs precision recall percentage number words trained procedure recording pairs selected processor precision recall content word link accuracy results program relies assumption syntactically related words statistically correlated 
pairs words linked humans test set positive lexical attraction gives upper bound recall 
critical evaluation section qualitative analysis program shortcomings suggest 
mistakes program traced back main reasons di erentiation di erent types syntactic relations 
program represent learn argument structures words 
mechanism categorization generalization 
link types second degree models consider sentences architect worked building 
architect worked building 
relation building sentences di erent 
marked di erent prepositions 
general language word order function words morphology represent di erent relations 
program represent di erence di erent link types independence assumptions permit representation relations involving words 
examples relation words mediated third word preposition 
distribution building di erent just building 
natural extension approach relax independence assumptions step look relationships modifiers words influence lexical attraction mark type link 
argument structure history related source mistakes program link strongly attracted words matter links 
representation di erent link types program distinguish complements mandatory arguments adjuncts optional arguments 
link direct objects verb determiner modify nouns simultaneously 
learns represent di erent link types solution usage history word learn argument structure information constrain relations 
solution help opposite problem 
program link words seen 
chomsky sentence ultimate example green ideas sleep 
program able find relations sentence matter training goes comes linguistics textbooks 
real data sentence quite bad relations words positive lexical attraction pointed section 
learning usage history words knowledge familiar situations solution problem 
categorization generalization stayed away word categories partly think done harm past 
existence word categories regulatory role syntactic constructions denied believe need finer system type hierarchy current subcategorization frames dictionary features allow 
learning word individually advantages 
matter data words seen times 
way estimate argument structure lexical attraction information rare words identify category examples seen generalize properties category rare word 
addition providing solution low sample problem categorization words interesting right 
lot useful semantic information gathered reading free text 
example reading years wall street journal program discover category corresponding concept politician members include president prime minister governor frequent actions include visiting meeting giving press releases attempts cluster words usage see example pereira tishby brown lee 
success limited partly due lack model identify syntactic relations free text 
approach particularly suitable line 
related unsupervised language acquisition date framework originally developed finite state systems 
hidden markov models speech recognition 
pillars approach algorithms training processing probabilistic language models 
viterbi algorithm selects probable analysis sentence model viterbi 
baum welch algorithm estimates parameters model sequence training data baum 
algorithms generalized probabilistic context free grammars addition hmm baker 
baum welch called forward backward algorithm context hmm inside outside algorithm context pcfg 
detailed description algorithms see rabiner juang lari young charniak 
general approach define space context free grammars improve rule probabilities training part speech tagged bracketed corpus 
di erent search spaces starting points training methods investigated various researchers 
early focused optimizing parameters hand built grammars jelinek fujisaki jelinek sharman jelinek mercer 
lari young inside outside algorithm grammar induction artificially generated language lari young 
algorithm practical small category sets scale realistic grammar natural language 
carroll charniak incremental approach new rules generated existing rules fail parse sentence carroll charniak 
method tested small artificial languages worked grammar space fairly restricted 
briscoe started partial initial grammar achieved results training corpus text briscoe 
pereira schabes started possible chomsky normal form rules restricted number nonterminals trained air travel information system spoken language corpus pereira schabes 
achieved results training bracketed corpus program showed improvement accuracy trained raw text 
entropy improved bracketing accuracy stayed raw text 
gives accuracy entropy results 
improvement accuracy trained raw text 
general approaches fail unsupervised acquisition large size search space complexity estimation algorithm problem local maxima 
charniak provides detailed review charniak 
focused improving ciency training methods stolcke chen de marcken 
cp ap bp cp ap bp ap bp bp cp ap ap bp ap cp bp phrase structure representation di cult get local maxima 
de marcken de marcken excellent critique current approaches unsupervised language learning fail 
main observation phrase structure representation di cult get local maxima 
observation early generalizations commit learning programs mistakes 
illustrates impact representation learning process 
di erent structures string abc 
modifies modifies 
learner chosen wrong nonterminals probable rules simultaneously switched fix mistake 
contrasts approaches important respects 
previous uses part speech information words 
high performance probabilistic parsers confirm detailed lexical information needed high coverage accurate parsing magerman collins charniak 
second standard formalism focused learning probability distribution possible tree structures 
simply assumes uniform distribution admissible trees concentrates learning relationships words 
chapter contributions motivated desire understand human language learning ability build programs understand language 
design decisions view extraction meaning 
representing syntactic relations words directly consequence having goal 
primitive operation standard phrase structure formalism group words phrases form higher order constituents 
meaningful relations words indirect outcome grouping process 
primitive operation finding meaningful relations words pilot saw train flying washington driver saw airplane flying washington likelihood words related defined lexical attraction 
knowledge lexical attraction words play important role language processing acquisition 
developed language program lexical attraction explicitly represented linguistic knowledge 
contrast language processing acquisition program grammar lex icon word categories 
chapter formalizes lexical attraction context information theory northern ireland northern ireland northern ireland program starts processing raw language input initial knowledge 
able discover meaningful relations words processes language 
bootstrapping achieved learning processing 
processor uses regularities detected learner impose structure input 
structure enables learner detect higher level regularities di cult see raw input 
chapter discusses bootstrapping process starting knowledge training raw data program able achieve precision recall finding relations content words 
significant result previous unsupervised language acquisition demonstrated little improvement started zero knowledge 
percentage number words trained procedure recording pairs selected processor precision recall key insights di erentiate approach training words parts speech enables program learn common idiosyncratic usages words 
committing early generalizations prevent program making mistakes 
representation relevant features explicit simplifies learning 
believe humans learners sophisticated learning algorithms having right representations 
potential applications semantic categorization information extraction 
importantly may shed light humans able learn language raw data easily understand syntactically ambiguous sentences 
baker 
trainable grammars speech recognition 
speech communication papers th meeting acoustical society 
baum 
inequality associated maximization technique statistical estimation probabilistic functions markov processes 
inequalities 
beeferman berger la erty 
model lexical attraction repulsion 
acl eacl 
briscoe 
robust stochastic parsing algorithm 
aaai workshop probabilistically natural language processing techniques 
brown 
class gram models natural language 
computational linguistics 
carroll charniak 
learning probabilistic dependency grammars labeled text 
probabilistic approaches natural language papers aaai fall symposium 
carroll charniak 
experiments learning probabilistic dependency grammars corpora 
workshop notes statistically nlp aaai 
charniak 
statistical language learning 
mit press 
charniak 
statistical parsing context free grammar word statistics 
aaai 
chen 
building probabilistic models natural language 
ph dissertation harvard university 
chomsky 
syntactic structures 
mouton 
chomsky 
aspects theory syntax 
mit press 
collins 
new statistical parser bigram lexical dependencies 
proceedings th annual meeting acl 
cormen leiserson rivest 
algorithms 
mit press mcgraw hill 
cover thomas 
elements information theory 
john wiley sons de marcken 
unsupervised acquisition phrase structure grammars 
third workshop large corpora 
de marcken 
unsupervised language acquisition 
ph dissertation mit 
fujisaki jelinek 
probabilistic parsing method sentence disambiguation 
proceedings st international workshop parsing technologies 
gaifman 
dependency systems phrase structure systems 
information control 
graham knuth patashnik 
concrete mathematics 
addison wesley edition 
harary 
graph theory 
addison wesley 
hudson 
word grammar 
blackwell 
jelinek 
markov source modeling text generation 
ed impact processing techniques communications 


lari young 
estimation stochastic context free grammars inside outside algorithm 
computer speech language 
lee 
similarity approaches natural language processing 
ph dissertation harvard university 
magerman 
statistical decision tree models parsing 
proceedings rd annual meeting acl 
mel cuk 
dependency syntax theory practice 
suny 
pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
pereira schabes 
inside outside reestimation partially bracketed corpora 
proceedings th annual meeting association computational linguist 
pereira tishby 
distributional similarity phase transitions hierarchical clustering 
probabilistic approaches natural language papers aaai fall symposium 
quirk greenbaum leech 
comprehensive grammar english language 
longman 
rabiner juang 
hidden markov models 
ieee assp magazine 
schank colby 
computer models thought language 
freeman 
shannon 
mathematical theory communication 
bell system technical journal 
shannon 
prediction entropy printed english 
bell system technical journal 
sharman jelinek mercer 
generating grammar statistical training 
proceedings third darpa speech natural language workshop 
sleator temperley 
parsing english link grammar 
technical report cmu cs cmu 
sleator temperley 
parsing english link grammar 
third international workshop parsing technologies 
stolcke 
bayesian learning probabilistic language models 
ph dissertation university california berkeley 
viterbi 
error bounds convolutional codes asymptotically optimal decoding algorithm 
ieee transactions information processing 
zipf 
human behavior principle ort 
addisonwesley 

