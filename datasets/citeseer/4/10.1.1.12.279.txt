cache prefetching stefan berg department computer science engineering university washington seattle wa technical report uw cse cache prefetching memory latency hiding technique attempts bring data caches occurrence central aspect cache prefetching techniques ability detect predict particular memory patterns :10.1.1.55.4004
introduce compare done speci memory patterns identi ed 
applications contain di erent memory patterns discuss prefetching techniques combined mechanism deal larger number memory patterns 
discuss applicable currently prefetching techniques multimedia processing system 
keywords prefetching cache memory multimedia 
gap memory processor speed widely known phenomena computer architects 
exists unable build memories fast keep processor memories big hold working set typical applications 
due inverse relationship size access time computer memories 
memory hierarchy register le caches main memory disks exploits facts smaller memories faster applications spatial temporal locality 
temporal locality critical component memory access pattern randomly distributed go larger slower memories little gained fast memories 
temporal locality allows place data way applications vast majority go faster smaller memories 
data placement key component achieving goal 
intelligent movement data faster memories closer functional units important data movement slower memories 
functional unit operates data available fast memory operation delayed data transfered slow memory 
happens frequently functional units may underutilized due unavailability data 
comparison 
data moved slower memories usually stored write bu er scheduled write back time need complete operation immediately 
cache prefetching data transfered lower level memory hierarchy register risc functional unit cisc explicit memory instruction 
computation memory address scheduling memory instruction limited control ow data ow dependencies 
data may available functional units needed 
cache prefetching mechanism speculatively move data higher levels cache hierarchy anticipation instructions require data 
prefetching performed architectures issuing load instructions non existent register example register zero architectures register hardwired zero 
case prefetching controlled compiler called software prefetching 
hardware prefetching alternative case hardware controller generates prefetching requests information obtain run time memory cache addresses 
generally software prefetchers compile time pro ling information hardware prefetchers run time information 
advantages ective 
cache prefetching reduces cache rate eliminates demand fetching cache lines cache hierarchy 
called latency hiding technique attempts hide long latency transfers lower levels higher levels memory hierarchy periods time processor executes instructions 
remainder organized follows 
section discuss metrics evaluating prefetching techniques 
section compares di erent prefetching techniques forming main part 
concluding discuss section prefetching applies domain multimedia processing 

metrics prefetching aims reduce average memory latency decrease execution time 
apart execution time number metrics discussing ectiveness various prefetching techniques 
section introduce discuss metrics de ne consistent vocabulary remainder 
assume uniprocessor environment cache prefetching certainly applied multiprocessors 

coverage accuracy joseph grunwald de ne prefetching coverage fraction removed prefetching accuracy fraction prefetches useful 
discussion assume useful prefetch contain data processor replaced 
de nition coverage intuitive show de nition 
need de ne terminology 
place store prefetched data cache 
prefetch accuracy timeliness perfect storing prefetched data cache risks replacement useful data useless data 
alternative approach stores prefetched data prefetch bu er :10.1.1.55.4004:10.1.1.55.4004
hit prefetch bu er data transfered prefetch bu er cache 
happens replacement scheme remove useless data prefetch bu er eventually 
prefetch timing see section prefetched data transfered soon cache small prefetch bu er typically sucient 
np sequence addresses exhibited particular application running architecture prefetch mechanism 
architecture prefetch bu ers markov prefetcher joseph grunwald misses np serviced prefetch bu er appear fully hidden processor :10.1.1.55.4004:10.1.1.55.4004:10.1.1.55.4004
removing hidden cache misses np arrive pb sequence addresses application running architecture prefetching bu er 
consider sequence addresses architecture prefetches directly cache 
possible prefetch useless data cache may depending associativity replacement scheme cache introduce new misses np fact pseudo random replacement scheme may little resemblance np line able determine misses removed added need associate actual sequence memory addresses includes cache hits cache misses 
computationally expensive task number memory quite large 
storing actual memory non prefetching prefetching simulation run parallel 
usually prefetching techniques coverage de ned terms prefetching simulation avoiding need correlate misses non prefetching prefetching simulations 
introducing alternate de nition coverage need de ne terms 
late early early total number misses constituents de ned follows late number cache misses prefetch request issued data arrived cache bu er 
reduced time associated misses 
early number cache misses data prefetched cache bu er replaced 
early number cache misses data replaced prefetch counted prefetched data accessed time cache misses caused early prefetching occur prefetching directly cache 
practice prefetch bu er cache checked parallel ensure prefetch bu er hit serviced just fast hit 
number remaining cache misses 
cold misses replacement misses counted early early overhead useless early late hit total number prefetch addresses generated prefetch algorithm 
number consists quantities overhead number prefetch addresses cache bu er discarded 
mowry label prefetches unnecessary 
particular concern software prefetchers issue slots wasted 
useless number prefetch addresses loaded cache bu er fall classes 
program execution address referenced 
prefetched data may eventually replaced cache bu er 
prefetched data referenced eventually replaced prefetched 
subsequence prefetch may useless fall category 
early early number prefetch addresses loaded cache bu er replaced nally missed 
late late number prefetch addresses referenced processor data stored cache bu er 
hit prefetch addresses 
exactly prefetched cache subsequently referenced cache hit 
note prefetch counted hit may early prefetch sense may replaced data referenced prefetched data needed 
happen cache way associative 
resultant counted early de ne prefetch coverage coverage joseph hit hit late early early prefetching methods utilize prefetch bu er de nition exactly joseph grunwald hit equals removed prefetching denominator equation equals number misses base architecture prefetching 
simulations architectures prefetch cache de nition coverage relatively easy compute attempt correlate particular misses base architecture misses prefetching architecture 
mowry di erent de nition coverage remember early equals zero prefetching bu er 
coverage luk hit late early hit late early early de nition early late misses counted successful prefetch 
idea addresses computed accurately timing wrong 
disagreement coverage sensitive prefetch timing 
observed late counted fractional fraction completion time relative full latency 
prefetching cache prefetch bu er equation coverage shortcoming 
misses early misses introduced side ect prefetching 
happens frequently denominator coverage equation increased coverage decreases 
poor prefetch timing poor prefetch accuracy reduce numerical value coverage coverage quite intuitive sense removal misses 
aware prefetching results coverage de nition excludes early misses 
counting quantities early early useless add signi cant amount complexity simulation code 
prefetching cache similar technique works prefetching bu er early useless non zero data structure remember cache line prefetched cache lines replaced 
quantities computed overhead 
give equation accuracy accuracy late hit overhead useless early late hit late prefetch counted useful 
coverage accuracy inversely related 
typically coverage improved aggressive speculation prefetching addresses turn reduce accuracy 
illustrated graphically nicely joseph grunwald :10.1.1.55.4004:10.1.1.55.4004:10.1.1.55.4004

timeliness timeliness prefetching requests precisely de ned numeric value 
perfectly timed prefetching request complete shortly data needed instruction 
perfect time prefetch issue instance time range negative ects incurred 
implies cases early late prefetching 
early prefetch occurs prefetched data arrives long needed 
replace data going referenced prefetched data 
processor incur counted part early furthermore associativity cache small prefetched data may replaced referenced 
subsequent data counted early early prefetching increase number cache misses bus trac 
late prefetch occurs data prefetched arrives needed processor 
late prefetches fully hide cache provide bene reducing cache time 
counted late avoid negative impact early late prefetching prefetching algorithms incorporate methods controlling time prefetch request issued 
timing dicult achieve 

load latency average load latency roth show bene prefetching compared base architecture prefetching 
similar comparing rates properly measures reduced latency late misses 
expose excessive prefetching particular memory system 
excessive prefetching bus bandwidth bottleneck latencies increase 
requires accurate modeling memory bus 
excessive prefetching slow cache hits cache misses access cache tags blocked frequently prefetch engine 
prefetch techniques tag memories idle issue prefetches memory bus 

bus bandwidth prefetching increases utilized bus bandwidth accuracy perfect 
bus bandwidth generally valuable resource limit performance bus bandwidth increases due prefetching frequently measured 
aspect bus bandwidth dicult improve performance prefetching applications fully utilize processor memory bandwidth 
prefetching hides cache misses eliminate memory transfers 
reordering memory accesses improve available bandwidth exploiting page mode accesses caches lower levels hierarchy ects studied exploited cache prefetching mechanisms 
bus bandwidth computing upper bound performance improvement prefetching technique 
performance increase memory bandwidth fully utilized 
alternate upper bound achievable performance improvement prefetching assume cache rate 
due fact prefetching reduce bus bandwidth bound accurate lower bus bandwidth close fully utilized 
assuming cache rate computing upper bound provide accurate result functional units close fully utilized 

prefetching techniques di erent prefetching techniques 
discussed prefetch cache prefetch separate bu ers 
prefetching techniques compiler insert prefetch instructions code rely hardware issue prefetch requests 
prefetching done compiler called software prefetching pro ling code analysis identify data prefetched 
hardware prefetching typically uses data cache addresses compute prefetching addresses techniques look instruction addresses information determine data prefetch 
important distinguishing factor prefetching techniques memory pattern designed recognize prefetch 
call set patterns prefetching technique optimized prefetch domain 
exception markov prefetching prefetching techniques designed handle particular memory pattern unable prefetch memory patterns 
prefetch domain important characteristic prefetching technique data accessed pattern outside prefetch domain usually prefetched 
limits achievable prefetch coverage 
furthermore unsupported memory patterns confuse prefetch algorithm reduce prefetch accuracy 

memory patterns elementary memory patterns identi ed literature 
cache prefetchers designed prefetch data recognized memory patterns 
stride memory isolated addresses related addresses 
example local global variable accesses array constant index certain members structure 
caches generally ective memory 
prefetch techniques able preload memory caches addresses dicult predict 
stride call memory pattern accesses strictly increasing decreasing adjacent memory address locations stride 
array stride 
early prefetching techniques jouppi prefetch bu ers designed prefetch data category memory 
stride memory addresses strictly increasing decreasing separated constant stride 
natural extension stride pattern exploited numerous prefetching techniques 
typical data structure accessed stride pattern array 
linked linked memory pattern data pointed address compute subsequent address 
notation occurs statement ptr ptr executed loop 
set zero rst element structure pointed ptr 
linked lists tree data structures accessed linked memory pattern 
number techniques proposed prefetch linked memory 
irregular memory patterns fall previous categories irregular 
example consider triangular memory pattern stride increases constant access irregular 
prefetching methods able deal irregular memory patterns 
notable exception prefetching markov predictors handle patterns fall category :10.1.1.55.4004:10.1.1.55.4004:10.1.1.55.4004
stride stride stride previously de ned baer chen 
linked category documented roth 
luk mowry refer data structures accessed linked memory patterns recursive data structures 

prefetching loops common abstraction look prefetching context loop shown 
refer loop loop 
variable element linked data structure case function return 
pointer array function return zero computation nished 
assume element single cache line 

view loop iteratively processes data structure elements prefetch algorithm able bring close processor function called indicated 
assume discussion prefetch request issued hardware software 
hardware prefetching technique prefetch statement thought representing action taken hardware 
prefetch 
loop non overlapping prefetching 
discuss possibilities ect prefetching statement terms hardware capabilities relative speed functional units memory 

faster memory memory latency time takes complete memory latency fully hidden 
processor incur cache 
slower memory overlap consider happens memory latency larger time complete 
processor incur cache iteration loop 
cache reduced service time data transit lower level memory hierarchy 
memory operations overlapped pipelined done hide memory latency 
prefetching able reduce cache latency somewhat fully hide 
reasons may possible pipeline memory request 
may hardware able pipeline memory requests 
increase instruction level parallelism ilp increasing gap memory processor performance modern processor architectures able pipeline memory requests issue multiple memory requests cycle 
data dependences impossible pipeline memory requests 
commonly case linked data structures linked list prefetch executed completed 
possible prefetching technique able look ahead iteration pipeline memory requests data dependencies 

slower memory overlap array linked data structures prefetching techniques exist overlap memory requests 
hide memory latency 
shows general approach memory latency twice long time complete 
prefetch 
loop overlapping prefetching 
array data structure equivalent 
linked data structures luk mowry proposed techniques obtain address rst having 
discuss technique shortly 
case memory system pipeline stages able handle memory requests parallel 
memory latency continues lag processor speeds increasingly important memory systems handle multiple requests simultaneously prefetching methods exploit techniques prefetch data iterations ahead current iteration 
:10.1.1.55.4004
pure prefetching methods prefetching methods designed exploit non zero strided linked memory patterns generally combination 
call methods pure prefetching methods discuss section 
section introduce hybrid prefetching methods combination pure prefetching methods 

consecutive prefetchers prefetch techniques targeting cover stride pattern consecutive prefetchers 
oldest group prefetching techniques hardware complexity low feasible 
smith early technique block lookahead obl 
obl prefetch issued cache line address space brought directly cache 
prefetch triggered memory prefetch cache prefetch 
prefetch brute force method prefetches right data stride memory generate large number overhead prefetch requests overhead 
overhead prefetches undesirable cache tag lookup done discarded 
prefetch avoids overhead prefetch half stride pattern 
memory result cache prefetch issue prefetch prefetch hit 
slight extension obl tagged prefetch uses tag bit cache line issue prefetch rst cache line 
obl tagged prefetch coverage similar prefetch method generates far fewer overhead prefetches 
jouppi jouppi proposed prefetch separate prefetch bu er called stream bu er 
technique uses obl prefetch issuing single prefetch consecutive cache lines prefetched stored stream bu er 
stream bu er organized queue lled prefetching requests emptied cache requests 
data copied cache cache supplied processor 
stream bu er ushed prefetching restarted new address item popped stream bu er match cache single stream bu er works instruction cache data cache 
sequence cache misses generated data cache rarely consecutive long period time cause stream bu er restarted frequently 
jouppi multiple stream bu ers operating parallel works better data cache 
stream bu ers major contributions prefetching 
separate bu er avoids pollution cache data referenced early 
second absence non blocking caches order execution stream bu ers overlap multiple prefetches single able hide memory latency better section obl techniques 

stride prefetchers consecutive prefetchers able eciently prefetch stride memory pattern especially larger addition exception ltering technique proposed palacharla kessler consecutive prefetchers naively assume memory consecutive 
ects hurt prefetch accuracy consecutive prefetchers 
generally stride prefetchers scan memory nd candidates stride prefetching 
candidate prefetching initiated 
improves prefetching accuracy stride prefetchers relative consecutive prefetchers 
comparison prefetch accuracies variety prefetching techniques joseph grunwald show ect nicely 
stride prefetchers appear perfect accuracy number useless misses consecutive prefetchers typically exceeds number cache misses :10.1.1.55.4004:10.1.1.55.4004:10.1.1.55.4004
stride prefetchers fall general categories 
software prefetching technique uses compiler analyze array program loops inserts prefetch instructions needed 
uses hardware component detects repeated execution particular memory instruction issues stride memory uses information predict prefetch 
mowry lam gupta mowry developed software prefetching technique array indices ane linear functions loop indices 
memory pattern need stride pattern typically described small number stride patterns 
technique requires hardware support form prefetch instruction works similar load instruction data referenced stored register 
prefetch instructions allowed cause exceptions page fault 
indiscriminate version compiler insert prefetches ane array 
software pipelining carefully place prefetches early prefetch completes just time array 
selective version inserts prefetches compiler predicts array instruction cause considering reuse loop iteration counts relative cache size 
loop splitting insert prefetches iterations compiler determines misses 
importance selective software prefetching technique indiscriminate technique reduce overhead prefetches overhead 
mowry observed reduced average percentage overhead prefetches small loss prefetch coverage :10.1.1.55.4004
overhead prefetches costly software prefetchers prefetch occupies instruction slot instruction bandwidth executing useful 
hardware prefetching techniques main cost overhead prefetches increased pressure cache tag memory overcome banking tag memory 
fu patel janssens hardware stride prefetchers hardware structure shown fig 
nding stride memory 
simplest implementation scheme proposed fu 
named hardware structure stride prediction table spt 
spt uses valid bit state information times stride entries 
memory instructions looked spt new entry created 
entry di erence previous memory address current memory address stride predictor 
stride predictor added current memory address candidate prefetch address 
previous memory address updated current memory address 
instruction address tag previous memory address state indexed instruction address times stride 
general form data structure hardware stride prefetchers detect stride memory 
candidate prefetch address issued prefetch memory system memory instruction causes memory instruction causes hit 
generally prefetch problem prefetch obl technique 
memory prefetched 
furthermore techniques mentioned far issue prefetch memory ahead unable issue deeply pipelined memory requests section 
chen baer chen baer proposed scheme adds signi cant features :10.1.1.117.6719
store known stride prediction table rpt name spt bit state eld single valid bit 
state eld contains information stable stride past ects choice prefetch addresses 
prefetches issued strides memory addresses memory instruction di erent 
second innovation lookahead program counter la pc updated cycle uses branch prediction table bpt predict direction branches 
la pc designed run ahead pc 
lookups prediction table rpt done pc la pc 
rpt updates controlled lookups pc prefetches issued lookups la pc 
additional entry times rpt fig 
stores number iterations la pc ahead pc particular memory instruction 
candidate prefetch address computed adding product times stride previous memory address 
added state information designed improve prefetch accuracy 
la pc designed better control prefetch distance reduce early late prefetches 
view loop section la pc allows prefetches issued loop iteration ahead memory consequently aggressive pipeline memory 
smallest number memory cycles instruction mcpi occurs distance la pc ahead pc allowed grow number cycles needed access level memory hierarchy 
larger distance increase number early prefetches 
pinter superscalar processors architecture design choice general purpose processors new challenge recognized cache prefetching :10.1.1.55.4004
superscalar processor execute instructions cycle issue possibly multiple memory requests cycle 
increases pressure cache tags determine memory hit cache 
consequently fewer idle cycles available discarding overhead prefetch requests fewer prefetches issued interfering normal processor execution 
pinter addressed problem tango architecture lter cache caches accesses tag memory 
proposed new la pc mechanism able run ahead pc rapidly able nd memory instructions cycle scheme chen baer :10.1.1.117.6719
new la pc referred pre pc enabled extending branch target bu er btb 
pre pc advances branch branch instruction instruction done la pc advance rapidly 
having advanced branch specially indexed prediction table superscalar processors searched associatively pre pc memory instructions current branch branch determine prefetch candidates 
simulated returns memory instructions cycle 
memory instruction prefetch issued state machine logic introduced chen baer :10.1.1.117.6719
memory processed pre pc advanced branch 
just done chen baer distance pre pc pc limited roughly memory latency avoid early prefetching 
apart early prefetching predictions multiple branches reduce prefetch accuracy 
allow branch branch updates pre pc additional entries needed btb shown fig 

key entries allow updates pre pc entry nt entry elds 
contain indices back btb rst branch taken taken paths respectively 
btb accessed regular pc normal branch prediction elds dual ported shown fig 

addition btb replacements costly btb entries point replaced btb entry entry nt entry elds updated 
pinter solved performing fully associative search entry nt entry elds btb replacement 
update done locally btb entry time penalty tolerable certainly increased hardware cost full associativity 
identical chen baer additional requirement 
goal able pre pc nd memory single cycle implement fully associative search btb index current pre pc 
fully associative search return potentially memory instructions pre pc previous branch index added address prediction entry nt entry size nt size target branch pc tag dual ported searched associatively information 
branch target bu er btb advancing pre pc 
tag limits cycle 
reason limit reduce hardware cost having lookup issue larger number prefetches 
second key innovation tango architecture lter cache 
fifo queue keeps address cache lines produced hit cache 
prefetch address queue overhead prefetch discarded having cache tag lookup 
lter cache cut number cache tag lookups overhead prefetches half 

recursive prefetchers recursive prefetchers prefetch linked memory primary challenges 
stride prefetchers recursive prefetchers able detect memory pattern prefetched 
second overlapped prefetching dicult due data dependences inherent linked memory 
key technique detecting linked memory pattern looking statement commonly linked list traversals 
executed loaded address 
executed previous address serves base address loading address 
important relationship base address load produced load previous loop 
di erent strided memory pattern base address computed base address constant memory 
recursive prefetchers look dependencies di erent load instructions nd linked memory pattern 
look hardware prefetching methods rst software prefetching approach afterward 
mehrotra harrison linked list detection scheme proposed mehrotra harrison simplest design 
extended prediction table chen baer elds shown fig :10.1.1.117.6719

linear stride eld traditional eld detecting stride pattern 
indirect stride previous data loaded elds added detecting linked memory 
linked memory detected technique generated self referential updates statement 
previous data loaded eld holds actual memory contents loaded previous execution load instruction 
cycle indirect stride value updated di erence previous data loaded value memory address current execution load instruction 
statement example equate constant set eld structure pointed linked memory pattern detected indirect stride eld constant multiple executions load instruction 
instruction address tag previous memory address indexed instruction address previous data loaded stride linear indirect stride state 
prediction table detecting stride linked memory patterns 
roth sohi simple easily implemented method mehrotra harrison disadvantage considering self referential memory instructions 
roth proposed di erent scheme able detect linked memory producer base address consumer base address example shown fig 

takes separate hardware tables accomplish 
shows tables 
values loaded memory instructions cached potential producer window possible candidates memory instructions load produce base address 
memory instruction lookup done indexing base address current load 
match pc current memory instruction considered consumer pc memory instruction matching entry considered producer 
producer consumer pcs stored correlation table ct template consumer memory instruction fig 

template necessary generating candidate prefetch addresses 
typical memory instructions template include set memory instruction 

example control structure rds traversals 
prefetching addresses generated information stored ct pc load instruction hits ct known producer past 
template information ct set memory instruction equal set eld structure pointed indexed loaded value memory address instruction memory address instruction instruction address producer producer consumer inst 
addr 
indexed producer consumer instruction template potential producer window correlation table address 
potential producer window correlation table ct detecting linked memory 
data returned current load instruction produce memory address prefetch address 
prediction correct consumer recorded ct load prefetch address point 
lacking timing information prefetch address immediately entered prefetch requests queue prq issued memory system idle cycle 
jouppi stream bu ers prefetched data stored bu er reduces penalty early prefetching 
prefetched data may producers looked ct generate additional prefetches 
roth perform additional prefetches increases risk early prefetches 
argued may little bene prefetch ahead 
understand need consider loop encompassing memory instruction generates prefetches 
memory latency time execute iteration loop memory latency fully hidden bene prefetching ahead 
memory latency longer time execute iteration memory instruction prefetch complete time di erence triggers subsequent prefetch 
way look impossible overlap prefetches due data dependencies 
places limit prefetches hidden section 
noted argument generating additional prefetches ignores cases may simple loop model 
example loops may equal length bene cial prefetch ahead ectively borrow time loop loop 
luk mowry essentially approach nding linked memory taken software prefetcher 
addresses known compile time compiler looks data types 
linked memory require data type structure contains element points data type structure 
recursive prefetcher software prefetcher proposed luk mowry calls data types recursive data structures rds 
software prefetcher recognizes recursive data structures set control structures traverse rds 
example control structure 
rds function function explicit dereference 
scheme detect complicated control structures 
example detect rds traversals recursive function calls allow multiple recursive calls sites needed tree traversals 
addition algorithm detect traversals recursive data structures varying types shown fig 

detecting control structures rds traversals comparable nding producers base addresses scheme proposed roth 
scheme appears certain cases nding base address producers may able detect 
example fig 
shows control structure supported software prefetcher :10.1.1.55.4004
hardware scheme detect dependence producer consumer addition base address set eld embedded memory load 
consumer instruction template speci ed correlation producer consumer 
furthermore size limited dependencies far apart may detected 
hardware scheme provider false matches correlation producer consumer occurs chance actual relationship existing 
hand hardware scheme may able detect certain base address producers software scheme misses programming style match control structures rds traversals recognized compiler optimization stage software scheme 
:10.1.1.55.4004
example control structure rds traversal 
soon base address computed prefetch instruction inserted element 
shown tree traversal example fig 

luk mowry call greedy prefetch scheduling functionally similar hardware prefetcher proposed roth 
primary di erence detection base address producers previously discussed placement prefetch calls 
hardware prefetcher issue prefetches instantaneously software prefetcher usually issue prefetch top loop iteration function call done fig 

remaining di erences generally separate hardware software prefetching schemes 
hardware schemes higher hardware cost software schemes additional instruction overhead 
technique roth prefetches bu er luk mowry scheme prefetches directly cache 
software scheme vulnerable early prefetching particular concern greedy prefetch scheduling due lack prefetch timing control 
functionally similar luk mowry motivate greedy prefetch scheduling di erent perspective roth 
diculty recursive prefetchers dependencies successive prefetches impossible overlap prefetches limiting amount memory latencies hidden 
mentioned section order overlap prefetches computed having load intermediate elements data structure 
appears impossible luk mowry introduced software prefetching mechanisms support varying degrees success 
rst prefetch left prefetch right test data test data left left right right null null 
left side shows code right side insertion prefetch calls 
greedy prefetch scheduling 
greedy prefetching overlap memory linked list traversals 
tree traversals overlap 
particular ary tree prefetches issued parallel illustrated fig 
assuming pointers located cache line 
contrary argument roth luk mowry conditions bene cial prefetched data initiate prefetches 
duplicates gure luk mowry shows sequential ordering prefetches binary tree greedy prefetching 
nodes bold covered greedy prefetching result prefetch hit long evicted cache due early prefetching con icts 
nodes resulted cache misses 
prefetch node causes prefetches nodes node missed subsequently 
ect dicult achieve luk mowry greedy prefetcher initiating prefetches prefetches done software prefetcher 
possible prefetched proposed roth :10.1.1.55.4004
ordering prefetches greedy prefetching pre order traversal binary tree 
bold tree nodes prefetched result prefetch hit 
nodes missed 
remaining question prevent explosion prefetches occur due fanout tree 
mechanism needed prevent prefetches issued certain depth 
problems early prefetching 
example node fig 
prefetched tree traversal referenced half way tree traversal assuming pre order traversal 
prefetch bu er small chances may high node replaced referenced 
lack better timing control prefetches possibility prefetching data referenced luk mowry greedy prefetching relatively bad prefetching accuracy 
coverage generally applications larger number misses caused scalar array 
poor coverage important reason roughly half applications reduction execution time 
half reductions execution time 
luk mowry proposed additional prefetching techniques targeted improving prefetching accuracy greedy prefetching 
rst history pointer prefetching keeps track traversal ordering rds stores information rds adding new pointer 
node rds history pointer provides value assuming history depth linear traversal linked list 
takes full traversal rds history pointers initialized 
traversal ordering change subsequent traversals prefetch history pointer ectively break dependence chain possible prefetch having visit intermediate nodes 
history pointers updated subsequent traversals accommodate slight changes traversal ordering 
drastically di erent traversal orderings cause wrong data prefetched time 
depth prefetches pipelined 
cost approach time space required maintain history pointers 
luk mowry tested performance history pointer prefetching application change tree traversal course program execution 
compiler technique currently automated applied hand 
conditions history prefetching performs signi cantly better greedy prefetching 
coverage accuracy improved signi cant number overhead prefetches data indiscriminately prefetched 
locality analysis strided software prefetcher developed mowry applied recursive data structure making selective prefetching dicult 
nal technique data linearization prefetching 
luk mowry demonstrated potential automated compiler optimization technique 
technique applied applications 
basic idea map recursive data structures sequence memory traversed 
reduces problem prefetching sequentially accessed array 
solution ered handle di erent traversal orders 
luk mowry optimized application dominant traversal order 
discuss handle accesses rds simple forward traversal order 
compared greedy prefetching data linearization prefetching lower coverage applications tested 
dominant traversal order optimized :10.1.1.55.4004
overhead prefetches removed execution time improved greedy prefetching 

markov prefetcher markov prefetcher remembers past sequences cache misses :10.1.1.55.4004:10.1.1.55.4004:10.1.1.55.4004
nds matches remembered sequence issue prefetches subsequent misses sequence 
gives markov prefetching ability prefetch sequence memory long observed 
means markov prefetcher deal dynamic memory patterns appear programming languages frequently allocate deallocate memory java scheme 
joseph grunwald implemented hardware prefetcher approximates markov model :10.1.1.55.4004:10.1.1.55.4004:10.1.1.55.4004
main data structure state transition table stab indexed address stores number prefetch address predictors modeled 
data structure large 
evaluated prefetcher mbyte stab mbyte cache 
compared performance architecture prefetching mbyte cache 
program execution history past addresses maintained 
new encountered actions take place 
oldest address history list misses past index stab current stored new prefetch address predictor 
entries stab replaced lru scheme 
parameter adjust prefetch distance represents maximum overlap prefetches achieved 
second current address looked stab 
prefetch address predictors table entry sent prefetch request queue 
prefetch address predictor di erent priority position stab 
lower priority requests may discarded prefetch request queue full 
markov prefetcher uses prefetch bu er store prefetches 
cache hits prefetch bu er data copied cache removed prefetch bu er 
data prefetch bu er moved head 
prefetch bu er managed fifo prefetched data arriving cache replace entry 
keeping data transferred cache prefetch bu er ect similar victim cache 
data evicted soon cache repeatedly restored prefetch bu er 
unfortunately evaluation done show ective jouppi organization data copied cache removed stream bu er 
number prefetch address predictors modeled greatly ects coverage accuracy 
accuracy worse quickly number prefetch address predictors increased coverage increases steadily 
coverage markov prefetcher obtained benchmark applications prefetch address predictors 
perfect coverage dicult achieve markov prefetcher sequence misses seen rst predicted 
disadvantage consecutive strided recursive prefetchers 

hybrid prefetching methods amdahl law tells limitations cache prefetching 
application speedup limited portion execution time caused cache stalls 
pure prefetching methods cache misses covered prefetch domain hidden 
limits potential speedup 
clearly rst limitation limitation really reality prefetching performance improving technique rst place 
second limitation clearly limits pure prefetching techniques 
quite processor architect choose stride prefetcher recursive prefetcher knowing really needed designs 
example stride prefetcher able prefetch misses caused linked memory patterns linked data structure laid sequentially memory order memory accesses 
needed techniques ective cache misses methods combining pure prefetching techniques 
markov prefetcher example rst class markov prefetchers learn generally ective prefetching strided linked memory patterns pure prefetching techniques 
call second class hybrid prefetchers 
section give examples describe techniques building hybrid prefetchers 

instruction prefetchers instruction rates traditional thought important processor performance data rates 
research maynard shows may just important important data :10.1.1.55.4004:10.1.1.55.4004
memory instructions treated just data prefetching perspective exhibit predictable access pattern exploited 
fortunately caches separated data instructions making easier incorporate di erent prefetch techniques data instructions 
instructions generally fetched memory stride memory access pattern 
consecutive prefetcher 
trouble occurs branches instruction sequence may continue di erent location 
luk mowry introduced nice example hybrid prefetcher designed prefetch instructions consecutively handles branches 
cooperative prefetcher uses separate prefetchers 
rst prefetcher line hardware prefetcher similar obl fetching cache line fetches cache lines typically range 
line prefetcher may issue useless prefetches crosses branch points 
avoid useless prefetches luk mowry designed prefetch lter cache recognizes prefetched cache lines referenced 
prefetch lter prefetching cache lines accesses 
second prefetcher software prefetcher 
compiler insert prefetch instructions ahead branch points covered line prefetching estimated cause cache misses 
luk mowry compare hybrid technique line prefetching markov prefetching 
line prefetching performs sequential instruction sequences markov prefetching handle sequential sequences branch points 
markov prefetching learn sequence prefetch 
benchmark applications cooperative prefetching generally reduces execution time 
better line prefetching markov prefetching improve execution time 

data prefetchers mehrotra harrison came idea extending stride prediction table able detect linked 
discussed prefetcher section 
linked memory detection scheme general proposed roth luk simple integrates stride linked prefetching little overhead 
unfortunately results available evaluate ectiveness technique 
joseph grunwald introduce general approach creating hybrid prefetchers 
idea multiple unmodi ed pure hardware prefetchers 
de ne modes operation serial parallel 
parallel operation prefetchers get access system resources issue prefetches independently prefetchers 
modi cation prevent prefetchers issuing identical prefetches memory system 
serial operation accurate prefetcher allowed issue prefetch rst 
issue prefetch accurate prefetcher queried 
prefetcher accuracy determined statically joseph grunwald consider stride prefetchers accurate markov prefetchers nally consecutive prefetchers 
appears little needed ectively serial hybrid prefetcher 
pure prefetchers able access memory cache misses information need able build database contents 
joseph grunwald describe technique method 
furthermore clearly de ned means prefetcher able issue prefetch 
stride prefetcher hit spt rpt state indicating stride pattern 
recursive prefetcher nding actual base address producer constitute able prefetch 
markov prefetching hit stab gives little certainty accuracy prefetch address predictors 
conjunction hybrid prefetching scheme may bene cial prefetch address predictor 
consecutive prefetchers claim prefetch address 
clearly problem multiple prefetching techniques put series rst issue prefetch 
addition stride prefetcher certainly question bene consecutive prefetcher 
joseph grunwald remarkable improvement prefetch coverage parallel serial hybrid prefetchers compared stride markov consecutive prefetchers 
unfortunately parallel prefetcher highly inaccurate 
surprising result 
serial prefetcher mixed results prefetch accuracies ranging nearly bad parallel prefetcher better prefetch accuracies markov consecutive prefetchers 

multimedia processing multimedia applications important workload modern personal computers various embedded applications 
give examples multimedia applications fall categories graphical user environments modern television communication systems computer games medical visualization diagnosis systems 
restrict discussion image processing domain area author familiar area tends focus image processing 
basic image processing operations convolution lters discrete cosine transformations widely areas multimedia processing audio processing discussion may apply equally areas 
look basic classes multimedia architectures general purpose processors multimedia extensions dedicated 
hardwired architectures perform particular multimedia processing task systems programmable contain special purpose memory systems 
general purpose processors multimedia extensions dedicated multimedia functional unit 
basic characteristics multimedia processing created need new functional unit 
multimedia processing contains large amount data level parallelism 
example pixel wise image addition theory pixels pairs source images added parallel create destination image 
second multimedia applications precision bits sucient 
multimedia functional unit exploits parallelism bundling multiple data elements wide machine word operating data elements parallel style single instruction single data simd processing 
lee refers type architecture architecture argues cost ective parallel architecture choice multimedia processing 
multimedia functional units appear ective con dence memory system 
generally memory system remains unchanged general purpose processor extended multimedia processing 
take somewhat di erent approach describe 
motivate caches key component memory system general purpose processor perfect match multimedia processing 
image addition function example nd memory access pattern temporal locality abundance spatial locality 
little temporal locality quite common multimedia functions input data needed brief period time 
implies program execution large amount data loaded cache 
cache exploits spatial locality quite lling multimedia data referenced ecient cache space 
addition useful data local variables constant data evicted cache multimedia data 

general purpose processors multimedia extensions high performance general purpose processors extended multimedia processing plans announced dnow 
altivec max mmx vis 
extending general purpose processor multimedia processing means multimedia functional units added processor 
multimedia functional units may operate general purpose registers oating point registers set multimedia registers 
memory hierarchy generally remains unchanged 
multimedia data loaded processor memory instructions passes memory hierarchy memory 
ective multimedia function units dicult current compilers general purpose processors automatically multimedia function units 
programmer rewrite program call special assembly libraries utilize multimedia function units 
best performance improvements parts program rewritten assembly explicit multimedia instructions :10.1.1.55.4004:10.1.1.55.4004
study ranganathan showed common image video processing kernels exhibit cache stall times equal half execution time general purpose processors enhanced multimedia extensions 
cache prefetching designed hide cache misses provide quite signi cant improvements execution time 

dedicated usually organized vliw processors 
compared superscalar design reduces hardware complexity instruction scheduling shifted hardware compiler 
reduced hardware complexity translates reduced cost power consumption 
reduced cost power consumption important predominantly targeted embedded market personal computer market 
addition functional units uncommon making scheduling particularly complex task 
contain traditional integer function units usually contain larger percentage multimedia function units multimedia extended general purpose processors 
typically support multiple memory operations cycle despite low cost relatively large chip memories 
memory hierarchy general purpose processors typically ord single cache level 
replace caches addressable chip memories 
important distinguishing factor addition direct memory access dma engine scheduling data transfers parallel computation 
double bu ered transfer pattern common form program dma engines 
block data input transfered bu er chip memory current block stored second bu er computation 
processing current block completed bu ers switched 
technique overlapping memory transfers computation ective hiding memory latency eciently chip memory area 
double bu ering multi way associative chip caches 
special hardware feature allows replacements disabled section cache set 
programmer maps bu ers areas cache programs dma engine transfer data directly cache 
remaining cache area continues available caching local variables data 
far argument cache prefetching unnecessary 
dma engines ective hiding memory latency 
problem dma engines programmability 
programming dma engines quite challenging programmer write programs computation data ow properly synchronize 
applications portable trained programmers easily moved project di erent 
dicult companies develop products 
performance improvements cache prefetching promises reduce programming complexity making unnecessary write data ow program 
cache prefetching multimedia extended general purpose processors primary challenge properly detecting ectively prefetching memory patterns exhibited multimedia applications 
requirement di erent cache prefetching expected general purpose processors 
secondary focus ective utilization available chip memory area challenging due low temporal locality multimedia applications 

data prefetching rst observation recursive prefetchers suited typical multimedia memory pattern 
important memory pattern multimedia processing generated lookup tables 
lookup tables example generalized warping position pixel image changed result table lookup pixel coordinates 
lookup tables popular computing transcendental functions 
table lookups memory pairs fall producer consumer category recursive prefetchers chains producers consumers 
number producers consumers larger separation short 
prefetching hard apply 
may course possible linked list tree data structure exists form multimedia application aware application group accesses categorized dominant multimedia pattern 
large chip memory requirements markov prefetcher probably infeasible today cost critical factor 
memory size may issue 
aware studies evaluated markov prefetcher typical multimedia applications 
table lookups involved sequence misses quite random may ectively prefetched markov prefetcher 
hand data ow multimedia applications typically quite static may translate repeated sequence cache misses course program execution 
cases markov prefetching quite ective 
prefetchers evaluated context multimedia applications consecutive stride prefetchers 
zucker evaluate ectiveness prefetching techniques mpeg encoding decoding applications running general purpose processor 
stream bu er remove order cache misses 
stride prefetcher able remove misses cache sizes larger kbyte smaller cache sizes stride prefetcher better consecutive prefetcher 
problem observed useless prefetches removing useful data cache 
modi ed stride prefetcher prefetch bu er problem went away 
zucker proposed prefetch bu er directly supply data functional units copying cache 
supposed overcome cache polluting ect observed multimedia applications 
number cache misses removed change compared regular stride prefetcher 
unfortunately results show comparisons execution time quantitative results prefetch accuracy judge possible ects copying prefetched data cache 
ranganathan evaluate software prefetching technique proposed mowry insert prefetching instructions hand perform analysis insert 
simulation environment multimedia enhanced general purpose processor 
reductions execution time image processing applications cases eliminating cache stall times 
hand jpeg mpeg coding applications showed little bene applications small component execution time spent waiting cache misses 
illustrates shortcoming study zucker model memory latency instruction execution 
comparison ranganathan model order processor shows applications memory latency impact performance severely 

improve performance prefetching broad class applications hybrid prefetching scheme employed 
reviewed prefetching techniques clever design combine prefetching technique ability prefetch wide variety memory patterns 
reviewed technique joseph grunwald describes general way combine di erent hybrid prefetching techniques 
promising appears serial prefetching method prefetchers placed series accurate rst 
interesting question answer stride prefetcher recursive prefetcher perform furthermore kind cache misses remain 
simple memory pattern emerges exploited pure prefetcher 
important ective hardware software integration multiple prefetch methods mehrotra harrison prefetcher nice example performance impact unproven 
apart hybrid prefetchers performance pure prefetching techniques remains important performance directly ects performance hybrid prefetchers 
stride prefetchers mature accurate schemes detect stride patters sophisticated techniques accurately time prefetches 
linked prefetchers fairly accurate lack prefetch timing making prefetch bu er popular choice 
particular inability overlap prefetches may limit far cache misses hidden superscalar processors 
illustrated potential prefetcher roth ective tree data structures current design permits 
additional prefetch techniques may necessary cover memory patterns fall stride linked category 
markov prefetcher may ll hardware cost expensive choice slow reaction time diculty dynamic memory access patterns may limit 
important evaluate point point diminishing return reached 
execution time due cache stalls penalty hidden stride linked prefetchers additional improvements execution time hard achieve :10.1.1.55.4004
prefetching competes current practice writing data ow programs dma engines may important feature reducing programmer complexity 
method improve performance signi cantly prefetching currently explored 
promising prefetching technique multimedia applications stride prefetcher 
needed evaluating multimedia applications 
little temporal locality multimedia applications may bene cial develop prefetching techniques isolate prefetched multimedia data separate chip memory region prevent cache pollution 

patterson hennessy computer architecture quantitative approach morgan kaufmann publishers san francisco ca :10.1.1.55.4004

joseph grunwald prefetching markov predictors proceedings th annual international symposium computer architecture pp 
june :10.1.1.55.4004

callahan kennedy porter eld software prefetching proceedings th annual international conference architectural support programming languages operating systems pp 
april :10.1.1.55.4004


chen 
baer ective hardware data prefetching high performance processors ieee transactions computers pp 
:10.1.1.55.4004


luk mowry compiler prefetching recursive data structures proceedings th annual international conference architectural support programming languages operating systems pp 
october :10.1.1.55.4004

mowry lam gupta design evaluation compiler algorithm prefetching proceedings th annual international conference architectural support programming languages operating systems pp 
october :10.1.1.55.4004

pinter tango hardware data prefetching technique superscalar processors proceedings th annual international symposium microarchitecture pp :10.1.1.55.4004
december :10.1.1.55.4004

zucker lee flynn automated method software controlled cache prefetching proceedings st hawaii international conference system sciences pp 
january :10.1.1.55.4004
:10.1.1.55.4004
joseph grunwald prefetching markov predictors ieee transactions computers pp 
:10.1.1.55.4004

jouppi improving direct mapped cache performance addition small fully associative cache prefetch bu ers proceedings th annual international symposium computer architecture pp 
may :10.1.1.55.4004

palacharla kessler evaluating stream bu ers secondary cache replacement proceedings st annual international symposium computer architecture pp 
april :10.1.1.55.4004

roth sohi dependence prefetching linked data structures proceedings th annual international conference architectural support programming languages operating systems pp 
october :10.1.1.55.4004


baer 
chen ective chip preloading scheme reduce data access penalty proceedings conference supercomputing pp :10.1.1.55.4004
november :10.1.1.55.4004

fu patel janssens stride directed prefetching scalar processors proceedings th annual international symposium microarchitecture pp 
december :10.1.1.55.4004

mehrotra data prefetch mechanisms accelerating symbolic numeric computation 
phd thesis university illinois urbana il :10.1.1.55.4004

lipasti schmidt software prefetching pointer call intensive environments proceedings th annual international symposium microarchitecture pp 
november :10.1.1.55.4004

smith decoupled access execute computer architectures proceedings th annual international symposium computer architecture pp :10.1.1.55.4004
july :10.1.1.55.4004

bu er block prefetching method ibm technical disclosure bulletin pp 
:10.1.1.55.4004
:10.1.1.55.4004
maynard donnelly contrasting characteristics cache performance technical multi user workloads proceedings th annual international conference architectural support programming languages operating systems pp 
october :10.1.1.55.4004


luk mowry cooperative prefetching compiler hardware support ective instruction prefetching modern processors proceedings st annual international symposium microarchitecture pp 
november :10.1.1.55.4004

mowry 
luk compiler hardware support automatic instruction prefetching cooperative approach tech 
rep cmu cs school computer science carnegie mellon university june :10.1.1.55.4004

mehrotra harrison examination memory access classi cation scheme pointer intensive numeric programs proceedings international conference supercomputing pp :10.1.1.55.4004
may :10.1.1.55.4004

lee eciency architectures index mapped data media processors spie proceedings vol 
pp 
:10.1.1.55.4004

weber ju favor amd dnow 
technology microprocessors proceeding notebook hot chips pp 
august :10.1.1.55.4004

phillip second generation simd microprocessor architecture proceeding notebook hot chips pp 
august :10.1.1.55.4004

lee subword parallelism max ieee micro pp 
:10.1.1.55.4004

peleg weiser mmx technology extension intel architecture ieee micro pp 
:10.1.1.55.4004

tremblay connor narayanan vis speeds new media processing ieee micro pp 
:10.1.1.55.4004
:10.1.1.55.4004
bhargava john evans radhakrishnan evaluating mmx technology dsp multimedia applications proceedings st annual international symposium microarchitecture pp 
november :10.1.1.55.4004

ranganathan adve jouppi performance image video processing general purpose processors media isa extensions proceedings th annual international symposium computer architecture pp 
may :10.1.1.55.4004

berg sun kim kim critical review programmable architectures spie proceedings vol 
pp 
:10.1.1.55.4004

kim kim image computing library generation vliw multimedia processor spie proceedings vol 
pp 
:10.1.1.55.4004

zucker flynn lee comparison hardware prefetching techniques multimedia benchmarks proceedings international conference multimedia computing systems pp 
june :10.1.1.55.4004
:10.1.1.55.4004
