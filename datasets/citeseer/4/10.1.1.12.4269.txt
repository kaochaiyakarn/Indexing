integrating experimentation guidance relational reinforcement learning extended kurt driessens sa zeroski department computer science leuven celestijnenlaan leuven belgium kurt driessens cs kuleuven ac department intelligent systems jo stefan institute ljubljana slovenia saso dzeroski ijs si full topic appears proceedings nineteenth international conference machine learning 
learning form reinforcement learning optimal policy learned implicitly form function takes state action pair input outputs quality action state 
optimal action state action greatest value 
dealing large state spaces learning encounters major problems 
rst limitations standard learning related number di erent state action pairs may exist 
function principle represented table entry state action pair 
states actions characterised parameters number pairs grows combinatorially number parameters easily large making infeasible represent function tabular form learn accurately convergence function happens state action pair visited times 
problem typically solved integrating qlearning algorithm inductive learner learns function generalises state action pairs 
reasonable estimates value stateaction pair having visited 
examples include neural networks nearest neighbour methods regression trees 
relational learner employed relational reinforcement learning rrl 
rrl uses rst order representations states actions learns rst order regression tree maps structural descriptions real numbers 
rst order representations gives rrl broader application domain classical learning approaches 
examples relatively complex applications described detail include learning solve simple planning tasks blocks world learning play certain computer games tetris 
structural domains state space typically large relational learner provide right level abstraction learn domain remains problem rewards may distributed sparsely state space 
random exploration search space rewards may simply encountered 
application domains mentioned prohibits rrl nding solution 
plenty exploration strategies exist deal problems exploration start learning process 
exactly problem faced rrl setting 
approach followed success consists guiding learner examples reasonable strategies provided teacher 
mix classical unsupervised learning supervised behavioural cloning obtained 
suitability approach context rrl explore 
discuss guidance incorporated learning approach done rrl algorithm 
demonstrate feasibility approach domains blocks world computer games tetris 
di erent forms guidance considered traces action sequences generated hand coded policies traces generated policies learned rrl traces human performing task hand 
cases guidance followed experimentation improves performance experimentation terms performance level achieved convergence speed 
observe careful supplying learning algorithm perfect guidance right start 
coupled generalisation engine providing optimal actions allow learn distinguish optimal non optimal actions 
guidance show learning algorithm optimal non optimal actions great variety states 
restricting visited states guidance limiting guidance policy take correct actions negative uence ectiveness ered guidance 
variety visited states probably hardest achieve constructing guidance traces 
driessens zeroski 
integrating experimentation guidance relational reinforcement learning 
proceedings th international conference machine learning icml pages 
smart kaelbling 
practical reinforcement learning continuous spaces 
proceedings th international conference machine learning pages 
morgan kaufmann 
watkins 
learning delayed rewards 
phd thesis king college cambridge 
wiering 
explorations ecient reinforcement learning 
phd thesis university amsterdam 
