acquiring core meanings words represented jackendoff style conceptual structures correlated streams linguistic non linguistic input jeffrey mark siskind artificial intelligence laboratory technology square room ne cambridge ma internet ai mit edu describes operational system acquire core meanings words prior knowledge category meaning words encounters 
system input description sequences scenes sentences describe events place scenes unfold produces output lexicon consisting category meaning word input allows sentences describe events 
argued main components system parser linker inference component linguistically cognitively plausible assumptions innate knowledge needed support tractable learning 
discusses theory underlying system representations algorithms implementation semantic constraints support heuristics necessary achieve tractable learning limitations current theory implications language acquisition research 
natural language systems reported learn meanings new words 
systems particular learn new meanings expectations arising morphological syntactic se supported bell laboratories ph scholarship 
part research performed author visiting xerox parc research intern consultant 
mantic pragmatic context unknown word text processed 
example system encounters sentence yesterday turned alarm clock took shower breakfast conclude noun represents type food 
systems succeed learning new words context offers sufficient constraint narrow possible meanings acquisition unambiguous 
accordingly theory accounts type learning arises adult encounters unknown word reading text comprised known words 
explain kind learning young child performs early stages language acquisition starts knowing meanings words 
new theory account language learning child exhibits 
theory learner training session consisting sequence scenarios 
scenario contains linguistic non linguistic visual information 
nonlinguistic information scenario consists time ordered sequence scenes depicted conjunction true negated atomic formulas describing scene 
likewise linguistic information scenario consists time ordered sequence sentences 
initially learner knows words comprising sentences training session lexical category meaning 
correlated sources input linguistic non linguistic learner infer set possible lexicons possible categories meanings words linguistic input allow linguistic input describe account non linguistic input 
inference accomplished applying compositional semantics linking rule reverse performing constraint satisfaction 
theory implemented working computer program 
program succeeds tractable small number judicious semantic constraints small number heuristics order eliminate search 
explains general theory implementation details 
addition discusses limitations current theory prevents converging single definition words 
background rayner describe system determine lexical category word corpus sentences 
observe original formulation definite clause grammar normally defines argument predicate parser sentence tree lexicon represented directly clauses grammar alternative formulation allow lexicon represented explicitly additional argument parser relation yielding argument predicate parser sentence tree lexicon 
argument relation learn lexical category information technique summarized 
query formed containing conjunction calls parser sentence corpus 
calls share common lexicon call tree left unbound 
lexicon initialized entry word appearing corpus lexical category initial entry left unbound 
purpose initial lexicon enforce constraint word corpus assigned unique lexical category 
restriction constraint play important role describe 
result issuing query example lexicon instantiated lexical categories lexical entry lexicon words corpus parsed 
note lexicons produced backtracking 
extend results rayner learning representations word meanings addition lexical category information 
theory implemented operational computer program called maimra 
rayner system corpus sentences input maimra correlated streams input linguistic non linguistic modeling visual context uttered 
intended closely model kind learning exhibited child prior lexical knowledge 
task faced maimra illustrated 
maimra attempt solve perception problem linguistic non linguistic input symbolic form maimra 
session maimra input pairs cup john cup mary cup mary cup john cup slid john mary 
cup mary cup bill cup bill cup mary cup slid mary bill 
maimra attempts infer category meaning information input 
architecture maimra operates collection modules mutually constrain various mental representations 
organization modules illustrated 
conceptually modules module simply constrains values may appear concurrently inputs 
parser enforces relation time ordered sequence sentences corresponding time ordered sequence syntactic structures parse trees licensed lexical category information lexicon 
linker imposes compositional semantics parse trees produced parser relating meanings individual words lexicon meanings entire utterances mediation syntactic structures consistent parser 
inference component relates time ordered sequence observations non linguistic input time ordered sequence semantic structures sense explain non linguistic input 
non directional collection modules maimra word word 
lexicon entry entry cup entry slid entry entry john entry entry mary entry bill parser cup slid john mary lexicon parser cup slid mary bill lexicon parser cup slid bill john lexicon 
lexicon entry det entry cup entry slid entry entry john entry entry mary entry bill 
technique rayner acquire lexical category information corpus sentences 
input scenario scene scene john john mary mary cup slid john mary cup john cup mary cup mary cup john scenario scene scene cup slid mary bill session mary bill bill mary cup mary cup bill cup bill cup mary output det cup thing cup slid event go path path place path place john thing john mary thing mary bill thing bill sample learning session maimra 
maimra scenarios input 
scenario comprises linguistic information form sequence sentences non linguistic information 
non linguistic information sequence conceptual structure state descriptions describe sequence visual scenes 
maimra produces output lexicon allows linguistic input explain non linguistic input 
parser linking inference sentences lexicon categories meanings syntactic structures conceptual structures observations cognitive architecture maimra 
ways 
lexicon sequence sentences input architecture produce output sequence observations predicted sentences 
corresponds language understanding 
likewise lexicon sequence observations input architecture produce output sequence sentences explain observations 
corresponds language generation 
sequence observations sequence sentences input architecture produce output lexicon allows sentences explain observations 
alternative corresponding language acquisition interests 
mental representations maimra externally visible linguistic input non linguistic input lexicon 
syntactic semantic structures exist internal maimra externally visible 
cognitive architecture learning values mental representations sentences observations deterministic fixed input 
remaining representations may nondeterministic may multiple lexicons syntactic structure sequences semantic structure sequences consistent fixed input 
general modules provides limited constraint possible values mental representations 
taken significant nondeterminism introduced module isolation 
taken modules offer greater constraint mutually consistent values mental representations reducing amount nondeterminism 
success maimra hinges efficient ways representing nondeterminism 
conceptually maimra implemented techniques similar rayner system 
naive implementation directly reflect architecture illustrated 
predicate maimra represent conjunction constraints introduced parser linker inference modules ultimately constraining mutually consistent values sentence observation sequences lexicon 
learning lexicon accomplished forming conjunction queries maimra scenario single lexicon shared conjoined queries 
lexicon list lexical entries form entry word category meaning 
constraint enforced initializing lexicon contain single entry word entry having unbound category meaning slots 
result processing query bindings category meaning slots allow sentences explain observations 
naive implementation inefficient practical 
inefficiency results sources inefficient representation nondeterministic values non directional computation 
nondeterministic mental representations expressed naive implementation backtracking 
expressing nondeterminism way requires substructure shared different alternatives mental representation multiplied 
example maimra input sequence sentences sentence parses second parses theta distinct values parse tree sequence produced parser sentence sequence 
parse tree sequence represented distinct backtrack possibility naive implementation 
actual implementation represents nondeterminism explicitly trees additionally factors shared common substructure reduce size mental representations time needed process 
noted previously individual modules offer little constraint mental representations 
sentence sequence corresponds parse tree sequences turn corresponds greater number semantic structure sequences 
filtered inference component correspond non linguistic input 
modules operate non directed sets constraints direction specific algorithms tailored producing factored mental representations efficient order 
inference component called produce semantic maimra sentences lexicon observations parser sentences lexicon linker trees lexicon inference observations 
lexicon entry entry cup entry slid entry entry john entry entry mary entry bill maimra cup slid john mary lexicon cup john cup mary cup mary cup john maimra cup slid mary bill lexicon cup mary cup bill cup bill cup mary 
lexicon entry det entry cup cup entry slid go entry entry john john entry entry mary mary entry bill bill 
naive implementation cognitive architecture techniques similar rayner 
structure sequences correspond observation sequence 
parser called produce syntactic structure sequences correspond sentence sequence 
linking component run reverse produce meanings lexical items correlating syntactic semantic structure sequences previously produced 
details factored representation algorithms create discussed section 
mental representations maimra require method representing semantic information 
chosen jackendoff theory conceptual structure model semantic representation 
stressed represent conceptual structure decomposition primitives way schank schank jackendoff claim particular decompositional theory adequate basis expressing entire range human thought meanings words lexicon 
clearly human experience formalization current state art knowledge representation 
concerned representing learning meanings words describing simple spatial movements objects visual field learner 
limited task primitive decompositional theory jackendoff adequate 
conceptual structures appear mental representations maimra 
semantic structures produced linker meanings entire utterances represented conceptual structure state event descriptions 
second observation sequence comprising non linguistic input represented conjunction true negated state descriptions 
state descriptions appear observation sequence 
function inference component infer possible event descriptions account observed state sequences 
meaning components lexical entries represented fragments conceptual structure contain variables 
conceptual structure fragments combined linker filling variables fragments produce variable free conceptual structures representing meanings utterances meanings constituent words 
learning constraints modules implements linguistic cognitive theory accordingly assumptions knowledge innate learned 
additionally module currently implements simple theory limitations linguistic cognitive account 
section discusses assumptions limitations module greater detail 
parser maimra learn lexical category information required parser parser fixed context free grammar assumed innate 
fixed grammar maimra shown 
glance unreasonable assume grammar innate 
closer look reveals particular context free grammar entirely arbitrary motivated theory linguists take innate 
grammar derived theory follows 
start version theory allows non binary branching nodes maximal projections carry xp 
fix parameters head spec yield prototype rule xp complement second instantiate rule lexical categories viewing det aux making pspec degenerate 
third add rules stipulating maximal projection 
fourth declare maximal projections valid complements 
add derivation english auxiliary system 
particular context free grammar little instantiating theory english lexical categories english parameters head spec english auxiliary system 
claim syntactic theory implemented maimra complete 
linguistic phenomena remain unaccounted grammar agreement tense aspect adjectives adverbs negation coordination quantifiers wh words pronouns 
grammar motivated gb theory components gb theory implemented theory theory 
theory enforced linking rule discussed subsection 
may increase scope accuracy syntactic theory incorporated maimra current limited grammar offers sufficiently rich framework investigating language acquisition 
severe limitation lack subcategorization grammar allows nouns principled way deriving rules theory np vp np vp pp aux context free grammar maimra 
grammar motivated theory 
head rule enclosed box 
head information linker 
verbs prepositions take number complements kind 
causes grammar severely results high degree non determinism representation syntactic structure 
interesting despite highly ambiguous grammar combination parser linker inference component non linguistic context provide sufficient constraint system learn words quickly training scenarios 
gives evidence constraints normally assumed imposed syntax result interplay multiple modules broad cognitive system 
linker linking component maimra implements single linking rule assumed innate 
rule best illustrated way example 
linking proceeds bottom fashion leaves parse tree root 
node parse tree annotated fragment conceptual structure 
annotation leaf nodes comes meaning entry word lexicon 
non leaf node distinguished daughter called head 
knowledge daughter node head phrasal category assumed innate 
grammar maimra information indicated categories enclosed boxes 
annotation non leaf node formed copying annotation head daughter node may contain variables filling variable slots annotation remaining non head daughters 
note nondeterministic process stipulation variables get linked complements 
nondeterminism associated lexicon parse tree 
addition linking ambiguity existence multiple lexical entries different meanings word cause meaning ambiguity 
variable may appear multiple times fragment conceptual structure 
linking rule stipulates variable linked argument instances variable get linked argument 
additionally linking rule maintains constraint annotation root node node sister head variable free 
violate constraint discarded 
distinct variables conceptual structure annotating head sisters head 
insufficient variables head partial linking discarded 
may means annotation parent contain variables 
acceptable parent sister head 
maimra imposes additional constraints linking process 
meanings lexical items semantic content simply variable 
second functor conceptual structure fragment variable 
words possible fragment john link produce john 
constraints help reduce space possible lexicons support search pruning heuristics learning faster 
summary linking component pieces knowledge assumed innate 

linking rule 

head category associated phrasal category 

requirement root semantic structure variable free 

requirement conceptual structure fragments associated sisters heads variable free 

requirement lexical item empty semantics 

requirement conceptual structure fragment contain variable functors 
limitations theory linking discussed 
attempt give adequate semantics categories det aux comp 
currently linker assumes nodes labeled categories conceptual structure annotation 
furthermore det aux comp nodes sisters head linked variable conceptual structure annotating head 
second linking rule account predication account semantics adjuncts 
shortcoming results just limitations linking rule fact jackendoff conceptual structure unable represent adjunct information 
inference component inference component imposes constraint linguistic input explain non linguistic input 
notion explanation assumed innate comprises principles 
sentence describe subsequence scenes 
teacher says true current non linguistic context learner 
teacher say false unrelated visual field learner 
second teacher constrained making true statements visual field learner teacher required state true non linguistic data may go 
third order linguistic description match order occurrence non linguistic events 
necessary language fragment handled maimra support tense aspect 
adds substantial constraint learning process 
sentences describe non overlapping scene sequences 
principles reasonable 
third accordance evidence children acquire tense aspect language learning process 
fourth principle questionable 
motivation fourth principle enables inference algorithm discussed section 
scope suggests different inference algorithm require principle 
learning principles notion sentence describing sequence scenes 
notion description expressed set inference rules 
rule enables inference event state description right hand side sequence state descriptions match pattern left hand side 
example rule states sequence scenes divided concatenated subsequences scenes gamma gamma gamma gamma gamma gamma gamma gamma gamma go cup john mary np cup det cup vp go john mary go pp john np john john pp mary np mary mary cup slid john mary example linking rule maimra showing derivation conceptual structure sentence cup slid john mary conceptual structure meanings individual words syntactic structure sentence 
subsequence contains scene scene subsequence scene second subsequence describe entire sequence scenes saying went path rule stipulate things true scenes embodying event type go just minimum conditions right hand side hold scene sequence 
general observation may entail multiple descriptions describing subsequence scenes may overlap descriptions 
maimra currently assumes inference rules innate 
tenable rules low level probably implemented vision system 
current focusing removing requirement rules inference component 
severe limitation current set inference rules lack rules describing causality incorporated cause primitive conceptual functions 
method considered rules cause go states caused move location start motion 
clearly unsatisfactory 
incorporate accurate notion causality discussed 
unfortunately jackendoff conceptual structures expressive support complex notions causality 
area 
implementation mentioned previously maimra uses directed algorithms non directed constraint processing produce lexicon 
processing scenario maimra applies inference component non linguistic input produce semantic structures 
applies parser linguistic input produce syntactic structures 
applies linking component reverse syntactic structures semantic structures produce lexicon output 
process best go go go go stay stay go ext go ext go ext orient orient orient orient orient orient orient orient inference rules inference component maimra infer events states 
illustrated way example 
consider input scenario 
cup john cup mary cup john cup mary cup bill cup mary cup slid john mary cup slid mary bill 
scenario contains scenes sentences 
frame axioms applied scene sequence yielding sequence scene descriptions containing true state descriptions pertaining scenes true state descriptions 
cup john cup mary cup mary cup bill scenario sentences scenes find possible ways partitioning scenes sequences partitions partitions contain contiguous subsequence scenes partitions overlap need contiguous 
abbreviate sequence scenes partitioning scenario containing sentences produces disjunction apply inference rules partition resulting disjunctive formula replacing partition disjunction events states describe partition 
example results replacements 
disjunction remains replacements describes possible sequences comprised events states explain input scene sequence 
notice non determinism managed factored representation produced directly algorithm 
inference component produces semantic structure sequences corresponding non linguistic input parser produces syntactic structure sequences corresponding linguistic input 
variant cky algorithm produce factored parse trees 
linker applied reverse corresponding parse tree semantic structure pair 
inverse linking process termed 
recursive process applied parse tree fragment conceptual structure fragment 
step conceptual structure fragment assigned root node parse tree fragment 
root node parse tree non head daughters compute possible ways extracting variable free subexpressions conceptual cup john cup mary cup bill go cup john mary go cup john go cup mary go cup cup mary stay cup mary go cup mary bill go cup mary go cup bill go cup replacements resulting application inference rules example text 
structure fragment assigning daughters leaving distinct variables place holders 
residue subexpression extraction assigned head daughter 
applied recursively conceptual structures assigned daughters root node parse tree fragment annotations 
results recursive calls conjoined 
disjunction formed possible way performing subexpression extraction 
process illustrated example 
consider conceptual structure fragment go john mary vp node head daughter labeled sister daughters labeled pp 
produces set possible extractions shown 
recursion terminates lexical item 
returns lexical entry triple comprising word category representation meaning 
result process monotonic boolean formula definition triples concisely represents set possible lexicons allow linguistic input scenario explain non linguistic input 
factored lexicon arising processing scenario similar second scenario training session illustrated 
disjunctive lexicon produced process may contain lexicons assign meaning word 
incorporate constraint rule lexicons 
conceptually done converting factored disjunctive lexicon disjunctive normal form removing lexicons contain lexical entry word 
computationally efficient way accomplishing task view factored disjunctive lexicon monotonic boolean formula phi propositions lexical entries 
conjoin phi conjunctions form ff ff ff ff distinct lexical entries word appear phi 
resulting formula longer monotonic 
satisfying assignments formula correspond conjunctive lexicons meet constraint 
satisfying assignments known constraint satisfaction techniques truth maintenance systems 
problem finding satisfying assignments boolean formula sat np complete experience practice sat problems generated maimra easy solve process generating sat problems takes far time solving 
constraint may bit restrictive 
relaxed somewhat allowing alternate meanings word conjoining conjunctions form ff ff distinct lexical entries go john mary go mary john go john mary go mary john go john mary go mary john go john mary go mary john go john mary go mary john go john mary go mary john go john mary go mary john go john mary go mary john go john mary go mary john conjunction disjunction recursive step process illustrating possible subexpression extractions conceptual structure fragment text assignments non head daughters 
center column contains fragments annotating pp rightmost column contains fragments annotating second pp 
leftmost column shows residue annotates head 
row distinct possible extraction 
definition cup cup definition mary mary definition definition mary mary definition definition john john definition definition john john definition definition slid go path definition john john definition definition slid go path definition mary mary definition definition john john definition definition john john definition definition slid go path definition john john definition definition slid go path portion disjunctive lexicon results processing scenario similar second scenario training session 
word appear phi pairwise conjunctions previously 
discussion training session maimra converges unique lexicon scenarios minutes cpu time 
able converge unique meaning word enter scenarios form john outside john room john room john outside john entered room 
turns way force maimra realize sentence describes entire scenario just scene 
maimra rule possibility enter mean 
reason maimra successful session empty semantics constraint rules associating sentences just scene semantic structures representing scene subsequences little semantic material distribute words sentence 
way problem maimra attempt choose lexicon maximizes amount non linguistic data accounted 
investigate issue 
claims result 
demonstrates combination syntactic semantic pragmatic modules incorporating cognitively plausible assumptions offers sufficient constraint learning word meanings prior lexical knowledge context non linguistic input 
offers general framework explaining meaning acquisition 
second appropriate choices representation algorithms allow efficient implementation general framework 
claim strictly required theory implementation currently incorporate training session maimra initial lexicon telling john mary bill nouns prepositions determiner 
reduce combinatorics generating ambiguous parses 
category information words meaning information words occurring training session 
theory possible efficiently bootstrap categories words longer training session containing shorter sentences constrain possible categories words 
done 
children employ mechanisms described construct useful engineered systems learn language 
third claim bold 
language acquisition research operates tacit assumption children acquire individual pieces knowledge language experiencing single short stimuli isolation 
extended assumption knowledge language acquired discovering distinct cues input cue elucidating parameter setting parameterized linguistic theory 
call assumption local learning hypothesis 
contrast approach knowledge language acquired finding data consistent longer correlated sessions 
approach requires learner puzzle solving constraint satisfaction 
normally believed approach cognitively plausible 
evidence children short input buffers 
limited size input buffers taken imply short isolated stimuli take part inferring new language fact 
maimra demonstrates despite short input buffer ability retaining scenario time possible produce disjunctive representation supports constraint solving multiple scenarios 
believe cross scenario constraint solving impossible account meaning acquisition local learning hypothesis wrong 
approach offers viable alternative local learning hypothesis consistent observed short input buffer effect 
related prior meaning acquisition focuses contextual learning scanning texts notable pursued path similar described attempting learn correlated linguistic non linguistic input 
describes system called moran 
non linguistic component scenario moran consists sequence exactly scenes scene described conjunction atomic formula 
linguistic component scenario case frame analysis single sentence describing state change occurring scenes 
scenario isolation moran infers calls claiming puzzle solving conscious 
constraint satisfaction done children adults low level subconscious cognitive function subject introspective observation 
conceptual meaning structure cms attempts capture essence meaning verb sentence 
cms subset scenes identifying portion scenes referred sentence arguments atomic formula linked noun phrases replaced variables labeled syntactic positions noun phrases fill sentence 
process inferring involves processes reminiscent tasks performed maimra ground distinction inference component suggests possible subsets non linguistic input referred linguistic input distinct part referred process verb meanings constructed extracting arguments sentence meanings 
moran variants tasks simpler analogous tasks performed maimra 
ground distinction easier scenario moran contains single sentence pair scenes 
moran need subsequence scenes corresponds sentence 
second linguistic input comes moran relies preexisting knowledge lexical categories words sentence 
moran acquire category information furthermore deal ambiguity arise parsing process ground distinction 
training session moran relies subtle implicit link objects world linguistic tokens refer 
part difficulty faced maimra discerning linguistic token john refers conceptual structure fragment john 
moran information priori lacking formal distinction notion linguistic token conceptual structure 
information process trivial 
moran exhibit cross scenario correlational behavior attributed maimra fact learns verb meaning just single training scenario 
implausible model child language acquisition 
contrast maimra moran able learn polysemous senses verbs scenario provided verb 
moran focuses extracting common substructure polysemous meanings attempting maximize commonality different word senses build catalog higher level conceptual building blocks task attempted maimra 
pustejovsky describes system called operates fashion similar maimra moran learning word meanings pairs linguistic non linguistic input 
moran linguistic input scenario single parsed sentence 
nonlinguistic input parsed sentence predicate calculus description parts single event middle 
input derives thematic mapping index data structure representing roles borne arguments main predicate 
moran task faced simpler faced maimra unambiguous parsed input correspondence nouns referents correspondence single sentence semantic representation event described sentence 
learn lexical categories determine ground partitioning non linguistic input learns verb meanings single scenarios cross scenario correlation 
multiple scenarios verb cause generalize common generalization individual instances 
goes maimra trying account acquisition variety features roles including maimra system successfully learns word meanings prior lexical knowledge words 
works applying syntactic semantic pragmatic constraints correlated linguistic nonlinguistic input 
doing accurately reflects type learning performed children contrast previous lexical acquisition systems focus learning unknown words encountered reading texts 
module implements weak theory isolation offers limited constraint possible mental representations collective constraint provided combination modules sufficient reduce nondeterminism manageable level 
demonstrates reasonable set assumptions innate knowledge combined appropriate representations algorithms tractable learning possible short training sessions limited processing 
may disagreement linguistic cognitive plausibility assumptions particular syntactic semantic pragmatic theories currently incorporated maimra may approxima tions reality general framework shows promise explaining children acquire word meanings 
particular offers viable alternative local learning hypothesis explain children acquire meanings require correlation experience input scenarios limited size input buffers 
attempt address potential shortcomings focus supporting robust acquisition broader class word meanings 
acknowledgments peter patrick winston victor zue giving freedom embark project encouraging elaborate bell laboratories supporting ph scholarship johan kris everybody xerox parc listening half baked versions prior completion bob berwick barbara grosz david mcallester george lakoff interesting discussions ron rivest pushing complete 
robert berwick 
learning word meanings examples 
proceedings eighth international joint conference artificial intelligence pages 
noam chomsky 
lectures government binding volume studies generative grammar 
foris publications 
noam chomsky 
concepts consequences theory government binding volume linguistic inquiry monographs 
press cambridge massachusetts london england 
noam chomsky 
barriers volume linguistic inquiry monographs 
press cambridge massachusetts london england 
richard granger jr foul program figures meanings words context 
proceedings fifth international joint conference artificial intelligence pages 
ray jackendoff 
semantics cognition 
press cambridge massachusetts london england 
paul jacobs uri zernik 
acquiring lexical knowledge text case study 
proceedings seventh national conference artifical intelligence pages august 
kasami 
efficient recognition syntax algorithm context free languages 
scientific report air force cambridge research laboratory bedford ma 
george lakoff mark johnson 
metaphors live 
university chicago press 
david allen mcallester 
solving sat problems dependency directed backtracking 
unpublished manuscript received directly author 
david allen mcallester 
outlook truth maintenance 
memo artificial intelligence laboratory august 
fernando pereira david warren 
definite clause grammars language analysis survey formalism comparison augmented transition networks 
artificial intelligence 
james pustejovsky 
acquisition lexical entries perceptual origin thematic relations 
proceedings th annual meeting association computational linguistics pages july 
james pustejovsky 
constraints acquisition semantic knowledge 
international journal intelligent systems 
rayner asa 
logic grammar learn lexicon 
technical report swedish institute computer science 
sharon 
inferring conceptual graphs 
cognitive science 
sharon 
inferring building blocks knowledge representation 
wendy lehnert martin editors strategies natural language processing chapter pages 
lawrence erlbaum associates 
roger schank 
fourteen primitive actions inferences 
memo aim stanford artificial intelligence laboratory march 
younger 
recognition parsing context free languages time 
information control 
