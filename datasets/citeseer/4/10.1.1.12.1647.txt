case staged database systems stavros computer science department carnegie mellon university pittsburgh pa usa stavros cs cmu edu traditional database system architectures face rapidly evolving operating environment millions users store access terabytes data 
order cope increasing demands performance high dbms employ parallel processing techniques coupled plethora sophisticated features 
widely adopted centric thread parallel execution model entails shortcomings limit server performance executing workloads changing requirements 
monolithic approach dbms software lead 
introduces staged design high performance evolvable dbms easy tune maintain 
propose break database system modules encapsulate self contained stages connected queues 
staged data centric design remedies weaknesses modern dbms providing solutions hardware software engineering level 
advances processor design storage architectures communication networks explosion web allowed storing accessing terabytes information online 
dbms play central role today volatile landscape 
responsible executing operations supporting increasing base millions users 
cope high demands mod permission copy fee part material granted provided copies distributed direct commercial advantage vldb copyright notice title publication date appear notice copying permission large data base endowment 
copy republish requires fee special permission endowment proceedings cidr conference ailamaki computer science department carnegie mellon university pittsburgh pa usa cmu edu ern database systems centric multithreaded multi process execution model parallelism employ multitude sophisticated tools server performance usability 
techniques boosting performance functionality introduce hurdles 
threaded execution model entails shortcomings limit performance changing workloads 
uncoordinated memory concurrent queries may cause poor utilization memory hierarchy 
addition complexity modern dbms poses software engineering problems difficulty introducing new functionality predicting system performance 
furthermore monolithic approach designing building database software helped view database center world additional front back ends mediators wie add communication cpu overhead :10.1.1.11.8981
database researchers indicated need departure traditional dbms designs cw sz due changes way people store access information online 
research mdo shown increasing processor memory speed gap hp affects commercial database server performance engineering scientific desktop applications 
database workloads exhibit large instruction footprints tight data dependencies reduce instruction level parallelism incur data instruction transfer delays ad kp 
systems expected deeper memory hierarchies adaptive programming solution necessary best utilize available hardware resources sustain high performance massive concurrency 
introduces staged database system design high performance evolvable dbms easy tune maintain 
propose break dbms software multiple modules encapsulate self contained stages connected queues 
stage exclusively owns data structures sources independently allocates hardware resources scheduling decisions 
staged data centric approach improves current dbms designs providing solutions hardware level optimally exploits underlying memory hierarchy takes direct advantage smp systems software engineering level aims highly flexible extensible easy program monitor tune evolve platform 
contributions threefold provides analysis design shortcomings modern dbms software ii describes novel database system design initial implementation efforts iii presents new research opportunities 
rest organized follows 
section reviews related database operating systems community 
section discusses modern commercial dbms problems arise exploitation memory resources complex software design 
section staged database system design scheduling trade analysis initial implementation effort 
section shows staged database system design overcomes problems section section summarizes contributions 
related research efforts past decades database research new software designs proposed 
earliest prototype relational database systems ingres sw consisted stages processes enabled pipelining reason breaking dbms software main memory size limitations 
staging known improve cpu performance mid seventies awe 
section discusses representative pieces broad scope research databases operating systems computer architecture 
parallel database systems dg chm exploit inherent parallelism relational query execution plan apply dataflow approach designing highperformance scalable systems 
gamma database machine project de relational operator assigned process processes parallel achieve pipelined parallelism operators series streaming output input partitioned parallelism input data partitioned multiple nodes operators split independent ones working part data 
extensible dbms ch goal facilitate adding combining components new operator implementations 
parallel extensible database systems employ modular system design desirable properties notion cache related interference multiple concurrent queries 
database research focuses data processing model input data arrives multiple continuous time varying streams bb 
relational operators treated parts chain scheduling objective minimize queue memory response times providing results acceptable rate sorted importance uf 
avnur propose eddies query processing mechanism continuously reorders pipelined operators query plan tuple basis allowing system adapt fluctuations computing resources data characteristics user preferences ah 
operators run independent threads central queue scheduling 
aforementioned architectures optimize execution engine throughput changing invocation relational operators exploit cache related benefits 
example eddies may benefit repeatedly executing different queries operator increasing tuple processing granularity discuss similar trade offs sections 
cache conscious dbms optimizes query processing algorithms index manipulation cgm clh gl data placement schemes ad 
techniques improve locality request limited effects locality requests 
context switching concurrent queries destroy data instruction locality caches 
instance running workloads consisting multiple short transactions misses occur due conflicts threads working sets replace cache jk rb 
os research introduced staged server programming paradigm lp divides computation stages schedules requests stage 
cpu processes entire stage queue traversing stages going forward backward 
authors demonstrate approach improves performance simple web server publish subscribe server reducing frequency cache misses application operating system code 
experiments successful significant scheduling trade offs remain unsolved 
instance clear circumstances policy delay request locality benefit disappears 
thread scalability limited building highly concurrent applications ous 
related suggests inexpensive implementations context switching ab bm proposes event driven architectures limited thread usage mainly internet services 
welsh propose staged event driven architecture seda deploying highly concurrent internet services 
seda decomposes event driven application stages connected queues preventing resource demand exceeds service capacity 
seda optimize memory hierarchy performance primary bottleneck data intensive applications 
time sharing thread concurrency model cpu time breakdown load query state load execute optimizer load execute parser optimize parse optimize parse cpu uncontrolled context switching lead poor performance 
pitfalls thread concurrency computer architecture research addresses increasing processor memory speed gap hp exploiting data code locality minimizing memory stalls 
modern systems employ mechanisms ranging larger deeper memory hierarchies sophisticated branch predictors software hardware prefetching techniques 
affinity scheduling explicitly routes tasks processors relevant data caches sl se 
frequent switching threads program interleaves unrelated memory accesses reducing locality 
address memory performance single application point view improving locality threads 
summarize research databases proposed cache conscious schemes optimize query execution algorithms modular pipelined designs parallelism extensibility continuous query performance 
os community proposed techniques efficient threading support event driven designs scalability locality aware staged server designs 
applies staged server programming paradigm sophisticated dbms architecture discusses design challenges highlights performance scalability software engineering benefits 
problems current dbms design motivated observations received attention date 
prevailing thread execution model yields poor cache performance presence multiple clients 
processor memory speed gap demand massive concurrency increase memory related delays context switch overheads hurt dbms performance 
second monolithic design today dbms software lead complex systems difficult maintain extend 
section discusses problems related observations 
cpu context switching points optimize parse optimize parse optimize par se optimize modern database systems adopt thread concurrency model executing coexisting query streams 
best utilize available resources dbms typically pool threads processes incoming query handled threads depending complexity number available cpus 
thread executes blocks synchronization condition event predetermined time quantum elapsed 
cpu switches context executes different thread ibm thread takes different task sql server lar 
typically relies generated events program structure query current state 
model intuitive shortcomings 
single number preallocated worker threads yields optimal performance changing workloads 
threads waste resources threads restrict concurrency 

preemption oblivious thread current execution state 
context switches occur middle logical operation evict possibly larger working set cache 
suspended thread resumes execution wastes time restoring evicted working set 

round robin thread scheduling exploit cache contents may common set threads 
selecting thread run scheduler ignores different thread benefit fetched data 
shortcomings depicted 
hypothetical execution sequence concurrent queries handled worker threads pass optimizer parser single cpu database server 
example assumes takes place 

choice threads processes depends underlying operating system 
choice implementation detail affect generality study 
par se thread thread thread thread time cpu resumes execution query spends time loading fetching main memory thread private state 
module execution cpu spends time loading data code shared average queries executing module shown separate striped box context switch overhead 
subsequent invocation different module evict data structures instructions previous module replace ones 
performance loss example due large number worker threads takes place worker thread sufficient preemptive thread scheduling optimization parsing single query interrupted resulting unnecessary reloads working set round robin scheduling optimization parsing different queries scheduled modules keep replacing data code cache 
shortcomings trade offs challenges discussed paragraphs 
choosing right thread pool size multithreading efficient way mask network latencies fully exploit multiprocessor platforms researchers argue thread scalability ous 
related studies suggest maintaining thread pool continuously picks clients network queue avoid cost creating thread client arrival adjusting pool size avoid unnecessarily large number threads 
modern commercial dbms typically adopt approach ibm lar 
database administrator dba responsible statically adjusting thread pool size 
trade dba faces large number threads may lead performance degradation caused increased cache tlb misses thread scheduling overhead 
hand threads may restrict concurrency threads may block system perform 
optimal number threads depends workload characteristics may change time 
complicates tuning illustrate problem performed experiment predator slr research prototype dbms ghz pentium iii server mb ram linux 
created workloads designed wisconsin benchmark 
quoting db performance tuning manual ibm agents implemented threads processes run decision support environment applications connect concurrently set num pool agents small value avoid having agent pool full idle agents 
run transaction processing environment applications concurrently connected increase value num pool agents avoid costs associated frequent creation termination agents max attainable throughput different workloads perform differently number threads changes 
de 
workload consists short msec selection aggregation queries incur disk workload consists longer join queries secs tables fit entirely main memory needed logging purposes 
modified execution engine predator added queue front 
converted thread client architecture pool threads picks client queue works client exits execution engine puts exit queue picks client input queue filling input queue parsed optimized queries measure throughput execution engine different thread pool sizes 
shows throughput achieved workloads different thread pool sizes percentage maximum throughput possible workload 
workload throughput reaches peak stays constant pool threads 
fewer threads os completely overlap idle cpu time resulting slightly lower throughput hand workload throughput severely degrades threads hide higher number longer queries interfere pool size increases 
challenge discover adaptive mechanism low overhead thread support performs consistently frequently changing workloads 
preemptive context switching workload workload server code typically structured series logical operations procedure calls 
procedure query parsing typically includes sub procedures symbol checking semantic checking query rewriting 
furthermore logical operation typically 
convert system non preemptive threads cooperating ones alarm timer causing context switches roughly msec 

user level threads low context switch cost 
systems processes kernel threads db increased context switch costs greater impact throughput 
consists loops iterate single token sql query symbol table look ups 
client thread executes global procedure loop variables data structures frequently accessed symbol table consist thread working set 
context switches occur middle operation evict working set higher levels cache hierarchy 
result resumed thread suffers additional delays re populating caches evicted working set 
replacing preemption cooperative scheduling cpu yields boundaries logical operations may lead unfairness hurt average response time 
challenge find points thread yield cpu build mechanism take advantage information sure execution path holds cpu long 
round robin thread scheduling thread scheduling policy factor affects memory affinity 
currently selection thread run typically done round robin fashion equal priority threads 
scheduler considers thread statistics properties unrelated memory access patterns needs 
way coordinating accesses common data structures different threads order increase memory locality 
table shows intuitive classification commonality data code database server 
private exclusive specific instance query 
shared data code accessible query different queries may access different parts 
lastly common accessed majority queries 
current schedulers opportunity exploit shared common increase performance choosing thread find largest amount data code fetched higher levels memory hierarchy 
order quantify performance penalty performed experiment 
measured time takes similar simple selection queries pass table data code queries classification data code private query execution plan client state intermediate results shared tables indices operator specific code nested loop vs sort merge join common catalog symbol table rest dbms code parser predator scenarios query finishes parsing cpu works different unrelated operations optimize scan table switches parsing second query second query starts parsing immediately query parsed query suspends execution exiting parser 
setup query improves parsing time second scenario finds part parser code data structures server cache 
shown simulation results section modest average improvement server modules results response time improvement running multiple concurrent queries high system load full simulation results described ha :10.1.1.13.4633
thread scheduling policy suspends execution certain queries order best memory resources may hurt average response times 
trade decreasing cache misses scheduling threads executing software module increasing response time queries need access different modules 
challenge find scheduling policies exploit module memory resources improving throughput response time 
pitfalls monolithic dbms design extensibility 
modern dbms difficult extend evolve 
commercial database software offers sophisticated platform efficiently managing large amounts data rarely stand service 
typically deployed conjunction applications services 
common usage scenarios data streams different sources different form xml web data pass translators middleware act interface dbms 
different applications require different logic built system programmer top dbms 
scenarios may deter administrators dbms may necessary simple purposes may worth time spent configurations 
compromise plain file servers cover needs lack features 
dbms require rest services applications communicate coordinate accesses database 
system performance degrades unnecessary cpu computation communication latency data path 
alternative extending dbms handle data conversions application logic difficult process typically defined api exported functionality limited due security concerns 
connect parse optimize execute disconnect new client authenticate new syntactic semantic check graph construct type check query rewrite statistics create plans eval 
plans delete state disconnect 
stage threads scheduling thread class stage class packet class queue struct enqueue packet struct dequeue struct init staged database system design stage queue thread support 
new queries queue stage encapsulated packet pass stages shown top 
packet carries query state private data 
inside execution engine query issue multiple packets increase parallelism 
tuning 
database software complexity difficult identify resource bottlenecks properly tune dbms heavy load conditions 
dba relies statistics system reports tune dbms clear view different modules resources 
example optimizer may need separate tuning reduce search space disk read ahead mechanism may need adjustment 
current database software monitor resource component utilization coarse granularity total disk traffic table accesses concurrent demand lock table 
solely information difficult build automatic tuning tools ease dbms administration 
furthermore client requests exceed database server capacity overload conditions new clients rejected experience significant delays 
receive fast service need cached tuple 
maintainability 
desirable property software system ability improve performance extend functionality releasing software updates 
new versions software may include example faster implementations algorithms 
complex piece software dbms challenging process isolate replace entire software module 
difficult programmer previous knowledge specific module implementation 
difficulty may rise coding style extended global variables module interdependencies 
testing debugging 
large software systems inherently difficult test debug 
test case combi 
execution engine table table 
table table table 
table join nested loop sort merge hash join aggr 
min max group nations different software components possible inputs practically countless 
errors detected difficult trace bugs millions lines code 
furthermore multithreaded programs may exhibit race conditions need concurrent access resource may lead deadlocks 
tools automatically search program structure run time expose possible race conditions sb may slow executable increase time software release 
monolithic software design difficult develop code deadlock free accesses shared resources may contained single module 
staged approach dbms software section describes staged design high performance scalable dbms easy tune extend 
section presents design overview section describes performance improvement opportunities scheduling trade offs 
results drawn experiments simulated staged database server 
section discusses current status ongoing implementation top existing prototype dbms section outlines additional design issues 
staged database system design sort send staged database system consists number selfcontained modules encapsulated stage 
stage independent server queue thread support resource management communicates interacts stages defined interface 
stages accept packets carrying query 
state private data query perform packets may enqueue newly created packets stages 
class citizen query enters stages needs 
stage centered exclusively owned degree possible server code data 
levels cpu scheduling local thread scheduling stage global scheduling stages 
design promotes stage autonomy data instruction locality minimizes usage global variables 
divide top level actions database server performs query execution stages see connect parse optimize execute 
execute stage typically represents largest part query lifetime decomposed stages described section 
break objective keep accesses data structures keep instruction loops single stage minimize query 
example connect disconnect execute common code related client server communication update server statistics create destroy client state private data 
likewise parser operates string containing client query performs frequent lookups common symbol table 
design general apply modern relational dbms minor adjustments 
example commercial dbms support precompiled queries bypass parser optimizer 
design query route connect stage directly execute stage 
shows certain operations performed inside stage 
depending module data footprint code size stage may divided smaller stages encapsulate operation subsets better match cache sizes 
key elements proposed system stage definition capabilities stage communication data exchange redesign relational execution engine incorporate staged execution scheme 
discussed 
stage definition stage provides basic operations enqueue dequeue queue incoming packets 
stage specific server code contained dequeue 
proposed system works exchange packets stages 
packet represents server perform specific query stage 
enters stage queue enqueue operation waits dequeue operation removes 
query current state restored stage specific code executed 
depending stage query new packets may created enqueued stages 
eventually stage code returns destroying packet done query specific stage ii forwarding packet stage parse optimize iii packet back stage queue client needs wait condition 
queries packets carry state private data 
stage responsible assigning memory resources query 
optimization shared memory system packets carry pointers query state data structures kept single copy 
stage employs pool worker threads stage threads continuously call dequeue stage queue thread reserved scheduling purposes scheduling thread 
threads stage help mask events executing stage packets queue 
threads happen suspend stage time quantum stage level scheduling policy specifies stage execute 
enqueue causes stage queue overflow apply back pressure flow control suspending enqueue operation subsequently freeze query execution thread stage 
rest queries output blocked stage continue run 
staged relational execution engine design relational operator assigned stage 
assignment operator physical implementation functionality 
group operators small portion common shared data code avoid stage scheduling overhead separate operators access large common code base common data take advantage stage affinity processor caches 
dashed box shows execution engine stages currently considering discussed section 
control flow operators stages uses packets top level dbms stages data exchange execution unit exhibits significant peculiarities 
firstly stages execute sequentially anymore 
secondly multiple packets different operators involved issued active query 
control flow packet happens query stage operators stages activated 
activation occurs bottom fashion respect operator tree init stage enqueues packets leaf node stages similarly push model gra aims avoiding early thread invocations 
dataflow takes place intermediate result buffers data exchange producer consumer type operator stage communication 
poisson module cpu 
production line model staged servers 
scheduling trade module time load module mean service time module loaded staged architectures exhibit fundamental scheduling trade mentioned section hand requests executing batch module benefit fewer cache misses 
hand completed request suspends progress rest batch finishes execution increasing response time 
section demonstrates appropriate solution scheduling trade translates significant performance advantage staged dbms design 
summarize previous ha generalized context simulated single cpu database server follows production line operation model requests go series stages order :10.1.1.13.4633:10.1.1.13.4633:10.1.1.13.4633
compare alternative strategies forming scheduling query batches various degrees locality developed simple simulated execution environment analytically tractable 
submitted query passes stages execution contain server module see 
module data structures instructions shared average queries accessed loaded cache subsequent executions different requests module significantly reduce memory delays 
model behavior charge query batch additional cpu demand quantity 
model assumes loss generality shared average requests fit cache total eviction set takes place cpu switches different module 
prevailing scheduling policy processor sharing ps fails reuse cache contents switches query query random way respect query current execution module 
execution flow model purely sequential reducing search space scheduling policies combinations parameters number queries form batch module time receive service completion cutoff value module visiting order scheduling alternatives described evaluated ha :10.1.1.13.4633
compares query mean response time server consisting modules equal service mean response time secs system load gated gated non gated fcfs ps execution time spent fetching common data code mean response times system load 
time breakdown configurations alter results 
graph shows performance ps come serve proposed policies system load various module loading times time takes modules fetch common data structures code cache quantity 
quantity varies percentage mean query cpu demand mean query service time corresponds private data instructions adjusted accordingly ms 
value viewed percentage execution time spent servicing cache misses attributed common instructions data default server configuration ps 
results show proposed algorithms outperform ps module loading times account query execution time 
response times twice fast improve module load time significant 
referring experiment section improvement execution time second query corresponds time spent query fetching parser common data code 
shows system consisting modules similar code data overlap improve average query response time 
implementing staged database system currently building staged mechanism top predator slr single cpu multi user clientserver object relational database system uses shore ca storage manager 
reasons choosing particular system modular code design documented currently actively maintained 
approach includes steps identifying stage boundaries base system modifying code follow staged paradigm relatively straightforward changes transformed base code series procedure calls discussed adding support stage 
section discusses design applies smps 
aware thread scheduling techniques implementing page dataflow queue control flow schemes inside execution engine 
thread scheduling 
shore provides non preemptive user level threads typically restrict degree concurrency system client thread yields events 
behavior explicitly control points cpu switches thread execution 
incorporated proposed affinity scheduling schemes section system thread scheduling mechanism rotating thread group priorities stages 
example cpu shifts parse stage stage threads receive higher priority keep executing dequeue queue empty global scheduler imposes gate queue working threads blocked event 
thread scheduler tries overlap events possible stage 
execution engine 
implementation currently maps different operators distinct stages see file scan index scan accessing stored data sequentially index respectively sort join includes join algorithms fifth stage includes aggregate operators min max average 
stages replicated separately attached database tables 
way queries access tables take advantage relations lie higher levels memory hierarchy 
implemented producer consumer type operator stage communication intermediate result buffers data exchange 
activation operators stages operator tree query starts leaves continues bottom fashion increase code locality essentially page push model 
operator fills page result tuples join output tuples read scan checks parent activation places page buffer parent node stage 
parent stage responsible consuming input children keep producing pages 
stage thread momentarily continue execution output page buffer full input empty enqueues current packet stage queue processing 
current status 
system current state new lines code predator lines supports primary database functionality original system 
currently integrating implementation updates inserts deletes transactions data definition language support persistent client sessions 
considering finer granularity stage definition order find optimal setting different workloads system con 
expect high level design remain existing stages add new routing information bypassing optimizer ddl statement additional stages include new functionality create delete table stage inside execution engine 
similar approach apply process staging commercial dbms far complicated prototype 
wizards tools statistic collection mechanisms may assigned new stages complicated stages optimizer may break smaller stages 
additional design issues stage granularity 
trade offs involved stage size amount server code data structures attached 
system may consist just high level stages top part 
break requires minimum redesign existing dbms may fail fully exploit underlying memory hierarchy large stage code base data structures entirely fit cache 
furthermore scheme resembles original monolithic design 
depending characteristics server caches system may consist fine granularity modules log manager concurrency control tree module 
case stage attached data structure making query execution totally data driven 
scheme requires total redesign dbms 
furthermore large number self executing stages may cause additional overheads related queueing delays resource 
initial approach addressing trade create self contained modules decide assignment stages tuning process discussed 
self tuning 
plan implement mechanism continuously monitor automatically tune parameters staged dbms number threads stage 
choice entails trade discussed section smaller scale 
example easier effective stage responsible logging monitor os adjust accordingly number threads doing dbms 
stage size terms server code functionality 
assuming staged dbms broken fine grain self contained modules tuning mechanism dynamically merge split stages reassigning various server tasks 
different hardware system load configurations may lead different module assignments 
page size exchanging intermediate results execution engine stages 
parameter affects time stage spends working query switches different 
choice thread scheduling policy 
different scheduling policies prevail different system loads ha :10.1.1.13.4633
benefits staged dbms design section discusses benefits advantages staged dbms design traditional database architectures 
section shows staged execution framework solve thread concurrency problems discussed 
section describe software engineering benefits design stemming modular self containing nature 
section discuss additional opportunities research 
solutions thread problems staged dbms design avoids pitfalls traditional threaded execution model described section mechanisms 
stage allocates worker threads functionality frequency number concurrent clients 
way targeted thread assignment various database execution tasks finer granularity just choosing thread pool size system 

stage contains dbms code logical operations 
preempting current execution thread random point code time quantum elapses stage thread voluntarily yields cpu stage code execution 
way thread working set evicted cache shrinking phase time restore greatly reduced 
technique 

thread scheduler repeatedly executes tasks queued stage exploiting stage affinity processor caches 
task execution fetches common data structures code higher levels memory hierarchy subsequent task executions experience fewer cache misses 
type scheduling easily apply existing systems require annotating threads detailed application logic 
solutions software complexity problems flexible extensible evolvable design 
stages provide defined api easy 
replace module new faster algorithm develop plug modules new func 
tionality 
programmer needs know stage api limited list global variables 
route packets modules basic functionality different complexity 
facilitates run time functionality decisions 
example 
important transactions may pass module sophisticated recovery mechanism 
debug code build robust software 
independent teams test correct code single stage looking rest code 
existing systems offer sophisticated development facilities staged system allows building intuitive easier development tools 

encapsulate external wrappers translators stages integrate dbms 
way avoid communication latency exploit commonality software architecture external components 
example unified buffer manager avoid cost subsequent look ups component cache 
defined stage interface enables dbms control distribution security privileges 
easy tune 
stage provides monitoring self tuning mechanism 
utilization system hardware resources software components stage granularity exploited self tuning process 
stage responsible adjusting parameters number threads buffer space slice cpu time scheduling policies routing rules 
parameters tune traditional dbms easier build auto tuning tools 
stage autonomy eliminates interdependencies facilitates performance prediction 
furthermore overload conditions staged design quickly push system requests easily served request cached tuple respond routing packets alternative faster stages trading accuracy speed momentarily increase server capacity 
back pressure packet flow techniques ensure modules reach near maximum utilization 
multi processor systems high dbms typically run clusters pcs multi processor systems 
database software runs different process cpu single process multiple threads assigned different processors 
case single cpu handles query random part 
staged system naturally maps stages dedicated cpu 
stages may migrate different processors match workload requirements 
single query visits cpus different phases execution 
data code locality benefits higher single cpu server fewer stages exclusively single processor cache 
shared memory systems query state private data remain copy packets routed different processors 
non shared memory systems stage mapping incorporates overhead copying packets pointers client private data 
scheme resembles architecture parallel shared architectures gamma de operator assigned processor parallel processing techniques employed order minimize overhead shipping data different cpus 
multiple query optimization multiple query optimization sel rs extensively studied past fifteen years 
objective exploit subexpression commonality set concurrently executing queries reduce execution time reusing fetched computed input tuples 
design complements past approaches providing staged infrastructure naturally groups accesses common data sources phase query lifetime input tuples intermediate results 
queries forced repeatedly execute defined stages example perform join operation scan specific table new scheduling algorithms take advantage infrastructure 
information collected optimizer combined run time queue information execution engine stage increase data overlap queries phase execution 
query arrives stage finds ongoing computation common subexpression reuse results 
way burden multiple query optimization move optimizer run time execution engine stages 
furthermore optimizer spend time waiting sufficient number incoming queries common subexpressions newly arrived queries exploit common data queries inside execution engine 
discussed issues modern database architectures introduced new staged dbms design desirable properties 
modern database servers suffer high processor memory data instruction transfer delays 
despite ongoing effort create locality aware algorithms interference caused context switching execution multiple concurrent queries results high penalties due additional conflict compulsory cache misses 
furthermore current threaded execution model commercial systems susceptible suboptimal performance caused inefficient thread allocation mecha nism 
looking software engineering point view years dbms software development lead monolithic complicated implementations difficult extend tune evolve 
fresh ideas os community lp applying complex context dbms server suggested departure way database software built 
proposal staged dbms design remedies weaknesses modern commercial database systems providing solutions hardware level optimally exploits underlying memory hierarchy takes direct advantage smp systems software engineering level aims highly flexible extensible easy program monitor tune maintain evolve platform 
contributions threefold provides analysis design shortcomings modern dbms software ii describes novel database system design initial implementation efforts iii presents new research opportunities 
david dewitt goetz graefe jim gray paul larson anonymous cidr reviewers valuable comments 
philippe bonnet josef burger technical support stavros christodoulakis manolis koubarakis petrakis providing equipment authors technical university crete 
supported part national science foundation iis ccr ibm lilian foundation graduate student fellowships 
grateful support 
ad ailamaki dewitt hill 
weaving relations cache performance proc 
vldb 
ad ailamaki dewitt hill wood 
dbmss modern processor time go proc 
vldb 
ab anderson bershad lazowska levy 
scheduler activations effective kernel support user level management parallelism proc 
sosp pp 
ah avnur hellerstein 
eddies continuously adaptive query processing proc 
sigmod 
awe asynchronous elements ibm ims team 
comments anonymous reviewer oct 
bb babcock babu datar motwani widom 
models issues data stream systems invited 
proc 
pods 
bm banga mogul 
scalable kernel performance internet servers realistic loads proc 
usenix 
bernstein asilomar report database research sigmod record vol 
dec 
ch carey haas 
extensible database management systems sigmod record vol dec 
ca carey persistent applications proc 
sigmod 
cw chaudhuri weikum 
rethinking database system architecture self tuning risc style database system proc 
vldb 
chm chekuri hasan motwani 
scheduling problems parallel query optimization proc 
pods pp 

cgm chen gibbons mowry 
improving index performance prefetching proc 
sigmod 
clh chilimbi larus andm hill making pointer data structures cache conscious ieee computer dec 
de dewitt gamma database machine project ieee tkde pp 
mar 
de dewitt 
wisconsin benchmark past benchmark handbook gray ed morgan kaufmann pub san mateo ca 
dg dewitt gray 
parallel database systems high performance database systems communications acm vol 
june 
gl graefe larson 
tree indexes cpu caches proc 
icde pp 

gra graefe 
iterators schedulers distributed memory parallelism software practice experience vol pp 
apr 
ha ailamaki :10.1.1.13.4633
affinity scheduling staged server architectures tr cmu cs carnegie mellon university mar 
hp hennessy patterson 
computer architecture quantitative approach nd ed morgan kaufmann 
jk kumar 
thread cache analysis modified tpc workload proc 
nd workshop 
ibm ibm db universal database manuals 
administration guide volume performance ftp ftp software ibm com ps products db info vr pdf letter db pdf kp keeton patterson raphael baker 
performance characterization quad pentium pro smp oltp workloads proc 
isca pp 

lp larus parkes 
cohort scheduling enhance server performance proc 
usenix 
mdo maynard 
contrasting characteristics cache performance technical multi user commercial workloads proc 
asplos 
lar paul larson 
personal communication june 
ous ousterhout 
threads bad idea purposes invited talk usenix tech 
conf 
slides available home net jan 
pai druschel andw zwaenepoel flash efficient portable web server proc 
usenix 
rb rosenblum bugnion herrod witchel gupta 
impact architectural trends operating system performance proc 
sosp pp 
rs roy seshadri sudarshan 
efficient extensible algorithms multi query optimization proc 
sigmod 
sb savage burrows nelson sobalvarro anderson 
eraser dynamic race detector multi threaded programs proc 
sosp pp 

sel sellis 
multiple query optimization acm transactions database systems mar 
slr seshadri livny ramakrishnan 
case enhanced data types proc 
vldb 
kant naughton 
cache conscious algorithms relational query processing proc 
vldb pp 

sz silberschatz zdonik strategic directions database systems breaking box acm computing surveys vol pp 
dec 
sl lazowska 
processor cache affinity information shared memory multiprocessor scheduling ieee vol 
feb 
sw stonebraker wong kreps held 
design implementation ingres acm tods 
se subramaniam eager 
affinity scheduling unbalanced workloads proc supercomputing 
uf urhan franklin 
improving interactive query performance proc 
vldb 
welsh culler brewer 
seda architecture conditioned scalable internet services proc 
sosp 
wie wiederhold 
mediators architecture information systems ieee computer pp 
mar 
