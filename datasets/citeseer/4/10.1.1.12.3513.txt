modeling dependencies protein dna binding sites gal nir friedman tommy kaplan school computer science engineering hebrew university jerusalem israel dept 
molecular genetics biotechnology hebrew university medical school jerusalem israel nir tommy cs huji ac il availability genome sequences high throughput genomic assays opens door silico analysis transcription regulation 
includes methods discovering characterizing binding sites dna binding proteins transcription factors 
common representation transcription factor binding sites specific score matrix pssm 
representation strong assumption binding site positions independent 
explore bayesian network representations binding sites provide different tradeoffs complexity number parameters richness dependencies positions 
develop formal machinery learning models data estimating statistical significance putative binding sites 
evaluate ramifications richer representations characterizing binding site motifs predicting genomic locations 
show richer representations improve pssm model tasks 
categories subject descriptors pattern recognition structural computer applications life medical sciences genetics general terms algorithms keywords bayesian networks dna sequence motifs transcription factors binding sites 
key issue modem molecular biology understanding mechanisms transcriptional regulation 
aspects transcription regulation involve dna binding proteins called transcription factors 
factors modulate expression genes binding specific positions nearby genomic regions 
transcription factors bind specific dna subsequences biological assays 
database contact author nir cs huji ac il permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
recomb april berlin germany 
copyright acm 
contains hundreds biologically validated binding sites 
assays labor intensive identify binding sites transcription factor 
availability complete genomic sequences including intergenic regions motivates attempts understand regulatory mechanisms silico analysis 
binding site identification involves main tasks 
task predict potential binding sites known transcription factor genomic scale 
uses examples biologically verified binding sites aims find similar sites intergenic regions gene promoter regions 
second task discover sequence motif putative sites collection relatively long intergenic sequences suspected bound factor 
example task examining promoter regions set genes common functional annotation expressed 
case discovered motif indicates possibly unknown factor regulates set genes 
works years proposed different schemes handle task :10.1.1.121.7056
tasks require describe motif characterizes sequences appear binding sites transcription factor 
biological literature suggests relevant sequences relatively short bp long 
binding sites quite conserved show variability 
sequence motif represent multiple allowed preferred subsequences binding site 
commonly representation motifs position specific score matrix pssm 
pssm records preference potential dna nucleotide binding site position 
representation inherently assumes positions motif independent 
open question strong independence assumption reasonable 
results indicate specific cases dependence positions 
take pragmatic approach issue 
aim test modeling dependencies leads better performance computational tasks binding site annotation motif discovery 
result positive suggests dependencies positions transcription factors 
main technical question face relax assumption independence 
purpose need model joint distribution positions 
naive representation requires large number parameters exponential motif length 
full independence model pssm full dependence model exponentially large joint distribution represent ends spectrum 
choice representation leads tradeoff expressive models represent complex dependencies succinct representation learned robustly examples 
expressive models represent complex dependencies involve parameters require larger number examples learning 
examine models explore middle ground spectrum 
models language bayesian networks 
describe procedures learn representa tions data allow carry tasks described 
demonstrate flexible set models generalizes better pssm model biologically verified sites 
show models learned precise predicting putative binding sites sense achieving better false positives vs false negatives tradeoff genome wide cerevisiae localization assays 

modeling binding site motifs consider model sequence motif representing binding sites transcription factor 
want represent commonalities different binding sites 
way going represent probability distribution sequences assign high probability sequences binding site 
assume binding sites length 
want represent probability distribution pos sible mers appear binding site 
formally need distribution xx xi random variable represents nucleotides th position mer 
naive representation distribution simply lists probabilities assignments 
representation estimate data 
interested succinct ways representing 
review pssm representation richer models task 
position score matrix pssm common way representing binding site assume nucleotide position independent nucleotides positions 
assumption implies marginal nucleotide events fo 
cl distributions fo pssm models 
pssm model requires peters describe distribution nucleotides position 
pssm capture specific preferences position different levels specificity positions 
pssms suppose factor sever types binding 
slightly different physical protein binding site somewhat different preferences 
bind sequence fits 
model case assume additions describes pe binding 
prior prob ability possibly uniform pe binding assume pe positions independent just pssm case 
requires nucleotide th position value joint probability positions require sum possible values xl distribution pssm representations mixing probabilities determined 
te pssm refers log odd score xi distribution 
nition pssm equivalent combined xi 
examples different bayesian network models motif positions 
model show example bayesian network structure corresponding representation joint distribution 
pssm tree non tree mixture pssms mixture trees 
mixture model benefits terms representation terms semantic interpretation 
number parameters fairly small parameters cc parameters conditional probabilities xi 
second suggested model offers important representational concept 
nucleotides dependent unknown independent observed 
plays role hidden biological mechanism renders positions independent 
understanding interaction hidden mechanism nucleotides position provide insights physical protein dna interactions 
bayesian networks mixtures pssms capture broad dependencies positions variable 
alternative approach describe dependencies consider position depends 
example particular nucleotide position cause conformation particular amino acid side chain change 
turn affects conformation amino acids binding site may effect preference binding position 
representation designed capture local dependencies language bayesian networks 
representation directed acyclic graph represent dependencies 
vertices correspond random variables 
pa describes conditional distribution variable immediate parents corresponding joint probability distribution decomposes product form 
xk pat pa possibly empty set parents xi fig ure show examples bayesian networks associated form probability distribution 
formal semantics bayesian networks terms conditional independence statements position xi independent non descendants parents 
general edges complex dependencies positions 
simplest network edges 
easy see case distribution simply pssm 
general case number parameters networks depends number edges conditional distribution xi pa requires parameters 
tree bayesian networks sub class bayesian network want single class tree bayesian networks 
tree models position parent making forest 
example networks tree networks 
tree networks generalize order markov chains 

provide flexible language modeling dependencies limiting number parameters 
important benefit class models efficient algorithms learn best tree structure :10.1.1.30.9978:10.1.1.30.9978
mixture trees cases tree structured network limited 
possible approach enriching represen tation combine benefits tree structure added richness hidden mechanism 
leads natural extension similarly mixture pssms mixture trees 
xi parents hidden variable nucleotide 
network example model 
intuitively unobserved variable enhances ability tree model additional dependencies multiplying number parameters factor 
important advantage mixture trees similarly trees exist efficient algorithms learning best structure :10.1.1.30.9978

learning motif models learning setup suppose want learn motif models data 
assume input set aligned binding sites transcription factor 
task learn probabilistic model captures common features sequences 
instance studied problem learning bayesian networks data sketch main issues going details 
interested reader find details :10.1.1.30.9978
assume training dataset aligned binding sites 
denote ri value xi th example 
clarify discussion conceptually easier think input learning problem terms measures frequency events training examples 
distribution indicator function value con dition argument true 
represent distribution explicitly convenient refer marginal probabilities distribution discussion 
easy compute marginal distribution input examples 
parameter learning key component learning assigning parameters conditional distribution 
models pssm part need learn 
models learn structure structure consider need estimate parameters 
task find parameters maximize average log probability sample data 
want maximize log likelihood function logp 
models hidden variable learning parameters straightforward 
maximum hood parameters pa simply matching condi tional distributions 
set pa pa 
parameter learning case reduces estimating marginal probability 
usually small number training examples smooth maximum likelihood estimates :10.1.1.156.9918
amounts adding small number experiments pseudo instances distributed background distribution 
models hidden variable parameter estimation somewhat complex 
need perform iterative procedure expectation maximization find local maximum likelihood function 
structure learning addition estimating parameters want learn dependency structure edges include 
performing structure learning need take account richer models ones edges achieve higher likelihoods 
runs risk learning model training data performs badly new instances 
maximizing likelihood function attempt maximize statistical score bayesian considerations :10.1.1.112.8434:10.1.1.156.9918
score thought likelihood penalized term accounts differences model complexity 
designed estimate performance model new unseen instances 
bdeu score :10.1.1.156.9918
models hidden variable finding best graph combinatorial optimization problem 
tree networks problem reduced maximum weighted forest problem solved efficiently :10.1.1.30.9978:10.1.1.30.9978
general bayesian networks problem intractable resort heuristic search 
models hidden variable situation complex 
need decide cardinality structure score intractable need resort approximation 
cheeseman stutz approximation bdeu score 
choose cardinality evaluate score cardinality 
mixture trees model face problem structure learning similarly trees efficient maximum weighted forest algorithm :10.1.1.30.9978
experimental evaluation evaluate extent richer models described beneficial representing transcription factors binding sites performed experiment extracting datasets aligned binding sites database examined transcription factors sites 
transcription factor evaluated ability representations describe distribution sequences binding site factor 
get objective evaluation performed fold cross validation tests dataset 
test learn model select structure parameters data evaluate remaining instances 
repeated learning step times instance evaluated exactly test case 
performance representation dataset summarized average log probability lv pssm 
tree pos pos count mix mix transcription factors comparison dependency models pssm learned aligned binding sites database 
example evident dependency bindings sites abf motifs 
pssm model mixture pssms model components dependency structure tree model middle positions shown occurrence table positions difference average log instance test data best dependency model pssm model axis transcription factors axis 
asterisks mark statistically significant results paired test instance evaluated part test set 
specific example consider sites identifier abf arabidopsis aba responsive element binding factor associated dataset consists binding sites bp long 
pssm learned dataset see shows position uninformative position weakly informative 
learn mixture pssms get mixture pssms shown 
see pssms differ positions 
learn tree bayesian network get dependency structure shown 
strongest dependency position 
see dependency examining occurrence table positions 
see position position high probability position position high probability 
cross validation test methods achieves log probability bits instance pssms mixture pssms trees respectively 
qualitatively say dependency model able detect real phenomenon data smoothed simplistic pssm model 
results evaluation datasets appear supplementary information table 
summarize results comparing best dependency model pssm model 
comparison individual methods see supplementary information item 
evaluated statistical significance differences paired test log probability particular instances 
cases pssm model performed better 
cases differences statistically significant 
cases noticeable differences learned dependency models cases show weak correlation 
cases dependency models better pssms 
case improvement statistically significant 
consider individual methods tree networks better cases significant mixtures pssms better cases significant mixtures trees better cases significant 
results give strong support claim cases modeling dependencies binding sites positions significantly improve generalization performance new unseen binding sites 

binding sites identification important usage learned binding site motifs identifying putative binding sites new sequences 
suppose learned model representing joint distribution xx binding site 
new promoter sequence want check contains binding site 
naively scan look probable mer model 
mer may probable background distribution 
minimize number false identifications want consider mers probable model improbable background distribution 
statistically speaking background distribution sequences natural score log odds ratio probability mer model probability background distribution score xx log xx suppose find mer score attach statistical significance finding want compute value score probability finding score high random mers sampled background distribution 
ways compute values 
naive approach uses empirical estimate value sampling mers scores 
unfortunately estimation high significance levels requires vast amounts samples 
alternatively gaussian approximation score distribution 
requires computing mean variance score distribution 
models term computed closed form analytical solution 
settings approach suffers high inaccuracies especially extreme scores region interest see supplementary information details item 
cope problem method importance sampling estimate score distribution 
sampling mers sample alternative distribution formally step justified manipulation po score xk ep score 
xx 
xk eq score xk 
po xk 
indicator function 
get estimate value weighted samples weight sample ratio po probability 
success approach depends choice intuitively want distribution mass interesting regions distribution score high 
useful proposal distribution ensure sampled distribution precise regions particularly region interest 
important sample distributions span range background model model interest defining mix ture form ci qi represents interpolation see item details 
evaluate statistical significance different mers keep mind testing multiple hypotheses 
searching promoter sequence length evaluate mers twice searching reverse strand 
sequence contain real binding site expect best mer sequence receive score significance 
control bonferroni threshold 
want significance level evaluate positions need check best scoring mer value effect achieved multiplying values number tested mers sequence called bonferroni corrected values 
summarize sequence suspected regulation cal discriminative score mers 
mer calculate bonferroni corrected significance report candidate binding site value 

motif discovery unaligned promoter sequences point examined question modeling dependencies binding sites positions models identify potential binding sites 
cases genomic sequences suspected regulation asked construct binding site model explain regulation 
want model binding site common promoters sequences regulated promoters rare promoters 
main difficulty actual binding position transcription factor sequence unknown 
modeling regulated sequence preamble learning motif models describe model regulated sequences 
generarive approach describe probabilistic processes generated promoter sequences 
model similar model probabilistic approaches pssm learning meme important difference allow general binding site model described section :10.1.1.121.7056
model assumes sequence generated ways 
regulated transcription factor contains single binding site model regulated 
extension deal multiple binding site model meme straightforward 
random variable denote cases 
event denotes contains binding site event de notes complementary event 
probability generating particular sequence motif model sum possible events 
shorthand notation event respectively 
sequence contain binding site model background distribution 
estimate distribution promoter regions genes genome 
model background distribution order markov chain 
probability sequence generated background probability sl il po 
time invariant order markov model 
consider case regulated assume binding site generated probability described rest promoter generated background distribution 
describe probability manner depend details model focus effects probability sequence 
basic problem computing probability regulated sequence know location binding site 
introduce random variable denotes location average possible values take prior probability possi ble binding positions 
take uniform distribution note prior knowledge promoter sequence organization incorporated distribution 
specific value probability rf se im fractional term log odds ratio probability mer motif model probability background distribution 
see se quence probable contains mer higher probability model jm background distribution 
having defined probabilistic model bayes rule compute posterior probability regulation posterior probability specific met binding site motif model rl sl incorporating biological observations model left choice free parameter 
parameter represents prior probability sequence regulated see actual sequence 
training want introduce additional knowledge mark specific sequences regulated 
simplistic way doing assume part training data observe 
case training data consists pairs contain sequence regulated 
example cluster expressed genes set promoter sequences genes cluster sequences 
problem simplistic approach confident training data 
cluster expressed genes contain false positive non regulated genes appear cluster similarly false negative genes regulated included cluster 
capture considerations take probabilistic approach allow learning algorithm view promoter sequences genes cluster having high probability regulated sequences having low probability regulated 
deal case introduce random variable denotes observation gene dependent regulated status gene promoter sequence 
assumption probability observation sequence view noisy sensor underlying biological regulation 
readily calculate probability regulation observation bayes rule 
crucial detail lies choice 
observa tion expressed genes genes similar functional annotation set distribution reflect fact regulated genes appear regulated cluster 
similarly want distribution reflect non regulated genes appear cluster 
interesting case involves chip localization data 
case observation value sequence enriched assay 
significant localization value indication sequence bound assay target transcription factor 
model dependence localization value attribute noisy sensor model segal :10.1.1.19.2946
model encodes value small generated regulated sequence 
value grows probability decays exponentially value sufficiently large generated non regulated sequence 
learning tools necessary describe learning procedure 
algorithm input dataset con sists promoter sequences associated observations 
want learn motif model maximizes log likelihood data log assume background distribution known fixed 
task amounts estimating structure parameters mo model 
unfortunately due fact unknown simple estimation procedure task 
expectation maximization em approach 
em algorithm uses current model complete probability hidden values 
completion longer missing values maximum likelihood model computed analytically 
new model completing data forth 
procedure iterates local maximum 
procedure form hill climbing guaranteed improve likelihood iteration 
structural expectation maximization sem algorithm generalizes idea learn structure 
models structure fixed learn maximum likelihood pssms mixtures pssms define em progressing sequence models arg logp useful log decomposed sum terms 
eq 
tes involve find log problem fo eq 
consider mer input sequences training sample 
essen mer account mt proportion current belief binding site 
achieved weighing mer te sire exactly probability regulation previous model 
face problem bayesian score ide procedure ee networks mixture trees details complex 
final upshot 
consists phases step mt compute weight mer input sequences 
step set mt model proce des section weighted dataset step 
note freedom choosing model section consider network step 
model fin issue choice model 
em algorithm sensitive bad stating points get trapped inferior local maximum 
choice reasonable albeit perfect stating point success algorithm 
genera finding motifs dna sequences mers suspected binding sites define initial tion 
favors motifs simple efficient vant described bash 
uses projections subsequences described buhler tompa 
having chosen projection check input sequence 
projected mers scored vue enrichment sequences 
repeat test projections choose significant projected mer find 
subsequences length match projected word samples training initial model 
method described general previous em methods meme :10.1.1.121.7056
differs meme sever aspects 
foremost plug gener network les model representation choice trees mixtures pssms 
second process tes account table performance evaluation synthetic data 
method report sensitivity specificity statistical significant set sequences predicted contain motif 
compared known planted sequences 
left hand side table datasets created pssm model dependencies right hand side datasets created tree network 
column reports different setting training data parameters number true positive sequences tp vs false positive ones fp training data 
line method true reports performance model sampled binding sites 
learned model pssm tree mix pssms mix trees true tp fp pssm generated tp fp tp fp tp fp tree generated tp fp tp fp pssm tree evaluation training synthetic data 
roc curves showing ability identify binding sites test data 
binding sites generated pssm model 
binding sites generated tree model 
axis shows false positive rate fp fp tn axis shows true positive rate tp tp fn 
curve shows performance model true model generated data learned pssm model tree learned tree model mix ms learned mixture pssms 
graphs training data consisting true positive sequences length bp 
sequence 
allows combine sequences different strengths support regulated 
third method incorporates rich background model learning postprocessing step 
starting point discriminative combinatorial search method 
search method attempts find initial solutions best distinguish sequences believed regulated believed regulated 

experimental results stated fundamental questions dependencies positions evident biological data learning models position dependencies improve de hove binding site discovery 
discussed question section turn second 
synthetic data evaluating methods synthetic data 
task built datasets consisting positive promoters sequences planted binding site motifs negative ones 
simulate underlying biological problem accurately possible motifs sampled models trained known binding sites human lun transcription factor database 
setting created parallel sets sampled tree network contains position dependencies pssm model 
promoter sequences sampled order markov model background distribution trained human promoter regions 
simulate noise contaminated datasets group false positive promoters motif planted 
set observation model positive sequences negative ones 
tested methods variety settings changing promoters lengths bp composition positive promoters true positives false ones true positives false ones 
synthetic datasets available site item 
applying methods synthetic training data tested unseen test data similarly generated 
test promoter assigned bonferroni corrected value best scoring mer see section 
scores discriminate putative positive promoters negative ones 
results dataset shown roc curves 
curves compare false positive rate true positive rate changing value threshold 
evident methods perform similarly data generated pssm comparable true model generated data 
data generated tree network shows difference performances learned tree network pssm mixture pssms models incapable modeling underlying dependencies perform worse 
learned tree network closely tracks performance true model 
practice want retrieve sequences contain learned motif 
selecting sequences bonferroni corrected value prespecified threshold 
experi ments chose threshold 
rationale strict threshold follows 
genome wide scans examine thousands sequences expected targets transcription factor question 
setting strict significance level control number false positives retrieved sequences 
table report quality selection procedure measures 
sensitivity positives sequences retrieved positive ones specificity true positives retrieved significance retrieved sequences hypergeometric value called specificity score 
score probability retrieving positive sequences random set sequences size 
see data false positives sequences models perform roughly equivalently data generated pssm 
tree generated data see pssm worse mixture pssms slightly better tree network roughly equivalent true model 
increase noise ratio problem harder 
pssm model large extent tree model fairly robust high degrees noise 
hand mixture models msn sensitivity tree 
swi ypd pssm mix trees true binding position detecting gal motif ga promoter sensitivity sensitivity mixture pssms mixture trees results chip localization data lee 
roc curves example motifs 
example detection motif upstream region 
position axis show bonferroni corrected value axis assigned model 
cross validation evaluation different methods 
point represents localization experiment shows difference performance learned models learned pssm sensitivity axis specificity axis 
susceptible noise performance decays 
learning shorter sequences length bp qualitative reappear data shown see item 
chip localization data evaluate methods real life data dataset genome wide chromatin chip localization measurements yeast transcription factors experiments 
assay measures binding affinities target transcription factor promoter regions 
experimental protocol assigns value promoter sequence 
sequence value considered bound factor 
stringent threshold aimed reduce false positive identifications genome wide screening 
assay provides valuable information binding specificity transcription factors 
important keep mind pinpoint exact binding location 
test prediction gene regulation events match biological predictions localization experiment 
stress predictions solely sites identifications de learned motifs 
focused experiments genes localization value value objective evaluation ability learned motif detect sequences transcription factor binds 
crucial test performance sequences seen training 
achieve performed fold cross validation test 
runs yeast genes training data tested learned motif remaining genes 
sequence scored bonferroni corrected value declared contain occurrence motif value smaller 
evaluate success different methods tested set genes lee consider bound transcription factor localization value 
note set conservative nature expect contain truly bound sequences 
provides objective test data 
discussion treat sequences true 
figures show roc curves examples msn swi ypd 
see examples models capture dependencies clearly superior pssm model 
differences performances due increased expressiveness richer models 
illustrates scan promoter region search statistically significant putative binding site 
shown promoter region ga models learned localization assay ga 
see mixture trees model assigns significant bonferroni corrected value true binding position pssm model 
evaluated accuracy learned motifs scan binding sites 
method compared putative set regulated sequences original set lee sensitivity specificity measures 
results available supplementary information table 
summary results appears show differences sensitivity specificity methods pssm models 
points upper right quadrant represent experiments richer model performed better terms sensitivity specificity points bottom left quadrant ones pssm model performed better points quadrants ones methods achieve different tradeoffs sensitivity specificity 
see experiments methods perform better pssm models 
tree network shows modest improvement experiments slight decrease 
mixture pssms sensitivity alignace comparison pssms learned method ones learned alignace gene 
difference pssms learned method alignace sensitivity axis specificity axis 
comparison hypergeometric value pssms learned alignace axis ones method axis 
somewhat better better worse mixture trees significantly better virtually experiments better worse 
evaluate learned models respect known underlying biological context compared pssms learned known yeast transcription factor binding sites see table 
experiments pssm tested transcription factor 
experiments pssm learn matches known 
clusters yeast genes rich collection datasets genes collected church lab 
clusters genes functional annotations expression known targets transcription factors 
originally analyzed alignace 
analysis included multiple runs alignace followed filtering quality motifs 
best pssms reported cluster 
gauge quality baseline method compared pssms learned procedures ones learned reported 
task training data done alignace examined learned motifs group comparing sensitivity specificity hypergeometric value 
see method improved pssm performance cases reduced 
main observation different tradeoff sensitivity specificity learning techniques 
examples pssm obtained significant hypergeometric value shown 
results show pssm learning procedure comparable alignace terms motif quality significance 
evaluated different methods dataset 
group having genes repeated procedures 
due noisier nature data set regulation prior genes inside cluster 
methods differ expressiveness number parameters representations comparison set sequences learning misleading 
comparing performance different models training data richer models particularly mixtures pssms trees better see supplementary information table 
get realistic assessment fold cross validation protocol described 
see summary results see supplementary information tables details 
see tree networks perform similarly pssms mixture models perform poorly cases 
understand phenomenon examined pssm results 
clusters sensitivity specificity small suggests clusters contain false positives false negatives genes making problem harder 
recall synthetic results show mixture models robust presence noise simpler models 
suspicion tend overfit spurious signals false positives sequences suitable domain 
discuss pos sible solutions section 

discussion expanded probabilistic representation dna motifs language bayesian network 
framework allows model spanning range position independent pssm full dependency model 
described meth ods learn models limited data showed types dependency models trees mixtures pssms mixture trees generalize better pssm unseen real life data 
methods discovering putative binding sites bayesian network model described effective approach evaluating statistical significance candidate sites 
showed perform de novo discovery motifs unaligned genomic sequences suspected regulation 
thorough empirical evaluation compared effectiveness dependency models discovering statistically significant transcription factors real life clusters 
model dependencies positions biological sequence motifs 
agarwal bafna suggested tree network model discussed algorithms learning 
related problem modeling splice junctions works examined order markov models tree bayesian networks 
works learned models aligned binding sites detect splice junctions new sequences 
bayesian networks model dependencies positions protein motifs aligned structure 
domain introduces layer complications due large alphabet size amino acids 
best knowledge presents general framework learning bayesian network motif model de novo discovery transcription factors 
show section ability lead dramatic improvements learned motifs 
extended directions 
ad vantage able model motif bayesian network suggests exploration different types models general unrestricted models 
include representational extensions geared complexity vs expressiveness issue context specific dependency models 
second framework particular assumption type binding sites readily adapted discover sequence motifs splicing factors 
third important challenge integrate method additional data 
noted section prior information binding site location 
interesting challenge combine method types information gene expression :10.1.1.19.2946
biggest question posed results automatically select different dependency models including pssm model 
natural measure predicting performance probabilistic models test data bayesian score 
unfortunately experiments bayesian score successful selecting best models 
analysis focused mainly statistical sig results 
results interest lo lo sensitivity tree sensitivity sensitivity mixture pssms mixture trees evaluation different methods clusters church lab 
point represents shows difference cross validated performance learned models learned pssm sensitivity axis specificity axis 
ing implications protein dna interactions 
challenge relate dependencies protein structure function 
purpose need able estimate confidence discovered dependencies bootstrap bayesian methods relate dependencies dimensional conformations protein dna complexes 
acknowledgments doug brutlag hillel fleischer margalit dana pe er simon ilan useful discussions relating 
supported part israel science foundation isf israeli ministry science 
supported fellowship 
kaplan supported horowitz fellowships 
friedman supported alon fellowship harry abe sherman senior computer science 

agarwal andv bafna 
detecting non adjacent correlations signals dna 
recomb 

tl bailey gribskov 
combining evidence values application sequence searches 
bioinformatics 
tl bailey elkan :10.1.1.121.7056
fitting mixture model expectation maximization discover motifs biopolymers 
smb 


simple hyper geometric approach discovering putative transcription factor binding sites 


andt kaplan 
supplementary information modeling dependencies protein dna binding sites 
www cs huji ac il lab andn 
context specific bayesian clustering gene expression data 
comp 
bio 
pv lapedes ds fields gd stormo 
statistical algorithm modeling interaction energies 
psb 

buhler tompa 
finding motifs random projections 
recomb 

ml pl gm church binding sites exert interdependent effects binding affinities transcription factors 
nuc 
acids res 
cai delcher ka kasif splice sites bayes networks 
bioinformatics 
dm chickering heckerman 
efficient approximations marginal likelihood bayesian networks hidden variables 
mach 
learn 
ck cn liu discrete dependence trees 
eee trans 
info 
theo ry 
ap dempster nm laird db rubin 
maximum likelihood incomplete data em algorithm 
roy 
star 
soc 
efron rj tibshirani 
bootstrap 

belief presence missing values hidden variables 
cml 

bayesian structural em algorithm 
ual 

geiger goldszmidt 
bayesian network classifiers 
mach 
learn 
friedman goldszmidt 
learning bayesian networks local structure 
learning graphical models 

friedman goldszmidt wyner 
data analysis bayesian networks bootstrap approach 
ual 

friedman koller 
bayesian bayesian network structure bayesian approach structure discovery bayesian networks 
mach 
learn 
jb carlin hs stern db rubin 
bayesian data analysis 
heckerman 
tutorial bayesian networks 
learning graphical models 

heckerman geiger dm chickering :10.1.1.156.9918
learning bayesian networks combination knowledge statistical data 
mach 
learn 
gz hertz gd stormo 
identifying dna protein patterns statistically significant alignments multiple sequences 
bioinformatics 
jd hughes pe tavazoie gm church 
computational identification cis elements associated groups functional related genes saccharomyces cerevisiae 
mot 
bio 
tm dl 
discovering structural correlations helices 
prot 
sci 
ds 
transcription factors 

sl lauritzen 
em algorithm graphical association models missing data 
comp 
star 
data analysis 
ti lee 
transcriptional regulatory networks saccharomyces cerevisiae 
science 
lp lira cb burge 
computational analysis sequence features involved recognition short introns 
pnas 
liu dl js liu 
discovering conserved cha motifs upstream regulatory regions expressed genes 
psb 

meila mi jordan 
estimating dependency structure hidden variable 
nips 

werner 
fast versatile tools detection consensus matches nucleotide sequence data 
nuc 
acids res 
ren 
genome wide location function dna binding proteins 
science 
fp roth pw hughes jd gm church 
finding dna regulatory motifs unaligned noncoding sequences clustered genome mrna 
nat 
segal simon koller :10.1.1.19.2946
promoter sequence expression probabilistic framework 
recomb 

simon 
serial regulation transcriptional regulators yeast cell cycle 
cell 
tavazoie jd hughes mj campbell rj cho gm church 
systematic determination genetic network architecture 
nat 
genet 
robinson ukkonen 
mining putative regulatory elements yeast genome gene expression data 
smb 

system gene expression regulation 
nuc 
acids res 
