local linear projection llp xiaoming huo chen school industrial system engineering georgia institute technology atlanta ga dimensionality reduction important applications exploratory data analysis 
method local linear projection llp proposed 
advantage method robust uncertainty 
statistical analysis applied estimate parameters 
results synthetic data promising 
preliminary experiment applying method microarray data reported 
results show llp identify significant patterns 
propose tasks perfect method 

dimensionality reduction plays significant role exploratory data analysis 
real applications data may high dimensions typically embedded manifolds subspaces substantially lower dimensions 
identifying manifolds subspaces critical understanding data 
important applications data visualization modeling 
communities statistics machine learning artificial intelligence substantial amount techniques developed 
give quick review works directly related 
embedded structures linear subspaces linear techniques principal component analysis pca singular value decomposition svd identify embedded linear subspaces 
pca second order statistics variances covariances data considered researchers find directions variances maximized 
svd works data 
finds linear subspace best preserves information data 
pca svd embedded structure globally linear 
applications condition restrictive 
multi dimensional scaling especially metric mds close pca svd 
pca svd find significant linear subspaces 
metric mds workers try map data partially supported seed center graphics visualization usability georgia institute technology darpa lockheed martin stanford university contract 
low dimensional space time keeping distances 
philosophical points seemingly different underlying linear algebra similar 
global linearity condition abandoned methods focused finding local embedded structures proposed example principal curves 
paid attention methods dedicated identifying local hidden manifolds example isomap local linear embedding lle :10.1.1.111.3313
isomap consider distance data points consider geodesic distance length shortest path resides embedded manifold 
implementations idea realized considering nearest neighbors 
order achieve better numerical performance researchers proposed variations curvilinear distance analysis cda 
lle data point represented convex combination nearest neighbors data mapped low space time convex combinations called embedding preserved best possibility 
examples shown illustrate ideas 
examples swiss rolls open boxes cylinders 
instructive 
horizontal view bird eyes view fig 

hurricane data embedded structure 
order help readers visualize type problem trying solve provide exemplary data 
data apparent embedded structure 
due human genome project availability microarray technology microarray data poses new challenge data analysts 
microarray technology allows workers measure levels gene expression tens thousands genes simultaneously 
dimensionality microarray data definitely high 
urgent develop efficient dimension reduction tools 
matter fact previously mentioned tools applied microarray data example researchers svd interpolate missing values microarray data 
isomap understand structure microarray data 
pca summarize microarray experiments 
lot examples 
evidence illustrate importance dimension reduction microarray data consider clustering genes 
clustering genes group genes associated identical functionalities 
nice survey clustering methods microarray datasets 
associated software described 
studies reported 
due space enumerate 
dimension reduction help improving clustering result 
project data points embedded low dimensional manifold compute inter distances projections 
inter distances faithful inter distance computed directly data 
dimension reduction tool preprocessing tool clustering algorithm 
dimension reduction tool help visualize data 
visualize data reduce global dimensionality data 
little bit different reducing local dimensionality data 
appending post processing method visualize data 
example look local structure data 
study synthetic data give demo idea 
works seen far observed shortcomings 

methods isomap cda lle nearest neighbor methods statistical model assumed 
difficult quantitatively measure success failure method 
difficult describe domain methods 

existing methods algorithms clear described implementing parameters example number nearest neighbors dimension embedded manifold 
analysis choose fully reported 
believe answers problems statistical analysis specifically analysis variance anova 
statistical model introduced model phenomenon locally embedded manifold noisy data 
proposed model propose local linear projection method identify embedded manifold 
preliminary computational statistical analysis carried determine choose values parameters model 
method works synthetic data expected 
provide preliminary results microarray data 
rest organized follows 
section statistical model embedded manifolds described 
section describe idea algorithm llp 
section parameter estimation strategies 
section report findings synthetic data microarray data 
section questions analyzed listed final remarks 

model assume additive noise model 
suppose observations denoted yn denote dimension observation 
yi assume underlying piecewise smooth function yi xi variable xi lower dimensional space noises follow multivariate normal distribution ip unknown 
model underlying function locally regular specifically function approximated linear function locally linear projection applied extract information 

local linear projection llp applied extract local low dimensional structure 
step neighboring observations identified 
second step svd pca estimate local linear subspace 
observation projected subspace 
algorithm llp observation yi 
find nearest neighbors yi 
neighboring points denoted yk 

pca svd identify linear subspace contains information vectors yk 
suppose linear subspace ai pai denote projection vector subspace 
denote assumed dimension embedded manifold subspace ai viewed linear subspace spanned vectors associated singular values 

project yi linear subspace ai yi denote projection yi pai yi 

output llp yi faithful underlying structure exists original observations 
justification step 
previous model yk wehave yk xk ip xi yi viewed random vectors mean vectors low dimensional subspace 
lowdimensional subspace extracted svd vectors yk 
dimension linear subspace estimated analyzing variances 
computational complexity llp roughly constant function dimension data dimension embedded linear subspace number nearest neighbors 
reasoning follows 
identify nearest neighbors distance matrix observations need computed costs pn operations 
row column distance matrix need sorted costs log order complexity quick sort algorithm multiply number rows operations 
words sorting takes log operations 
suppose svd step takes constant amount operations projection step 
order complexity llp 
estimating model parameters key parameters llp 
number nearest neighbors dimension local underlying subspace 
ideal number linearity assumption holds 
parameter ideal 

number nearest neighbors notations section fixed data point yi nearest neighbors yj linearization model true squared distances di yi yj approximately follow distribution 
distances ordered 
calculate differences going observe big ones decreases small ones 
distributed random variables sequence differences order statistics going mentioned pattern 
decreasing pattern differences help identify appropriate number nearest neighbors 
due space postpone detailed statistical analysis publications 

dimension linear subspace notations section version matrix fixed case computed svd analysis appropriate dimension linear subspace falls domain analysis variance anova 
case analysis complicated 
model matrix computed data 
intuitively dimension subspaces increases people expect quick drop variances relatively steady decreasing 
postpone detailed analysis publications 

simulations experiments reported 
synthetic signal 
second microarray data 

synthetic data hurricane twelve dimensional signal produced 
underlying function sin cos 
dimension pattern dimensions 
signal intrinsically 
illustration noisy data generated function limited dimensions 
creating noisy data standard deviation noise chosen 
analysis differences squared distances choose number nearest neighbors 
results applying llp shown 
horizontal view bird eyes view horizontal view bird eyes view fig 

result applying llp synthetic data 
limited rst dimensions 

microarray data llp applied microarray dataset 
dataset downloadable web 
number nearest neighbors 
result applying llp shown 
original llp fig 

result applying llp microarray data 
view color 
postpone detailed discussion result publications 

works llp useful identifying locally low dimensional embedded subspaces 
optimal dimension reduction tool 
preprocessing tool data analysis techniques 
plan carry detailed analysis approaches described 
eisen spellman brown botstein 
cluster analysis display genome wide expression patterns proc 
natl acad 
sci 
usa vol 
hastie stuetzle 
principal curves journal american statistical association june 
hastie tibshirani friedman 
elements statistical learning data mining inference prediction springer series statistics new york 
lee lendasse verleysen 
curvilinear distance analysis versus isomap submitted esann bruges 

computational analysis microarray data nat rev genet 
stuart altman 
principal components analysis summarize microarray experiments application time series 
pac 
symp 


stanford raftery 
finding curvilinear features spatial point patterns principal curve clustering noise ieee trans 
pami june 
roweis saul :10.1.1.111.3313
nonlinear dimensionality reduction locally linear embedding science vol 

genesis cluster analysis microarray data 
tamayo 
interpreting patterns gene expression self organizing maps methods application proc 
natl acad 
sci 
usa vol 
tenenbaum silva langford 
global geometric framework nonlinear dimensionality reduction science vol 
cantor sherlock brown hastie altman 
missing value estimation methods dna microarrays bioinformatics vol 
young 
multidimensional scaling theory methods applications academic press new york 
