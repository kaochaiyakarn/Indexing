uw cse technical report bilinear model sparse coding david grimes rajesh rao department computer science engineering university washington seattle wa cs washington edu october algorithms sparse coding independent component analysis ica demonstrated localized features learned natural images 
approaches take image transformations account 
result produce image codes redundant feature learned multiple locations 
describe algorithm sparse coding bilinear generative model images 
explicitly modeling interaction image features transformations bilinear approach helps reduce redundancy image code provides basis vision 
results demonstrating bilinear sparse coding natural images 
explore extension model capture spatial relationships independent features object providing new framework parts object recognition 
algorithms redundancy reduction ecient coding subject considerable attention years :10.1.1.134.6077:10.1.1.57.3911:10.1.1.164.7690
basic ideas traced early attneave barlow techniques independent component analysis ica sparse coding helped formalize ideas demonstrated feasibility ecient coding redundancy reduction 
techniques produce ecient code attempting minimize dependencies elements code appropriate constraints 
appear advances neural information processing systems 
mit press cambridge ma 
successful applications ica sparse coding area image coding 
olshausen field showed sparse coding natural images produces localized oriented basis lters resemble receptive elds simple cells primary visual cortex :10.1.1.134.6077
bell sejnowski obtained similar results algorithm ica :10.1.1.57.3911
approaches take image transformations account 
result oriented feature learned di erent locations yielding redundant code 
presence feature multiple locations prevents complex features learned leads combinatorial explosion attempts scale approach large image patches hierarchical networks 
propose approach sparse coding explicitly models interaction image features transformations 
bilinear generative model learn independent features image transformations 
approach extends tenenbaum freeman bilinear models learning content style casting problem probabilistic sparse coding framework 
prior bilinear models global decomposition methods svd approach emphasizes extraction local features removing higherorder redundancies sparseness constraints 
show natural images approach produces localized oriented lters translated di erent amounts account image features arbitrary locations 
results demonstrate image factored set basic local features transformations providing basis transformation invariant vision 
conclude discussing approach extended allow parts object recognition object modeled collection local features parts relative transformations 
bilinear generative models considering standard linear generative model algorithms ica sparse coding dimensional input vector image dimensional basis vector scalar coecient :10.1.1.134.6077:10.1.1.57.3911
linear generative model goal ica learn basis vectors independent possible goal sparse coding distribution highly equation 
linear generative model equation extended bilinear case independent sets coecients equivalently vectors ij coecients jointly modulate set basis vectors ij produce input vector study coecient regarded encoding presence object feature image values determine image transformation 
terminology tenenbaum freeman describes content image encodes style 
equation expressed linear equation xed ij likewise xed obtains linear equation de nition bilinear xed factor model linear respect factor 
power bilinear models stems rich non linear interactions represented varying simultaneously 
learning sparse bilinear models learning bilinear models goal learn image data appropriate set basis vectors ij ectively describe interactions feature vector transformation vector commonly approach unsupervised learning minimize sum squared pixel wise errors images ij jjz ij jj ij ij jj 
jj denotes norm vector 
standard approach minimizing function gradient descent alternate minimization respect fx yg minimization respect ij unfortunately optimization problem stated underconstrained function local minima results simulations indicate convergence dicult cases 
di erent ways represent image making dicult method converge basis set generalize ectively 
related approach tenenbaum freeman 
gradient descent method estimates parameters directly computing singular value decomposition svd matrix containing input data corresponding content class style approach regarded extension methods principal component analysis pca applied bilinear case 
svd approach avoids diculties convergence plague gradient descent method faster practice 
unfortunately learned features tend global non localized similar obtained pca methods second order statistics 
result method unsuitable problem learning local features objects transformations 
underconstrained nature problem remedied imposing constraints particular cast problem probabilistic framework impose speci prior distributions higher probabilities values achieve certain desirable properties 
focus class sparse prior distributions reasons forcing coecients zero input sparse priors minimize redundancy encourage statistical independence various various growing evidence sparse representations brain distribution neural responses visual cortical areas highly cell exhibits little activity inputs responds vigorously inputs causing distribution high peak near zero long tails previous approaches sparseness constraints obtained encouraging results enforcing sparseness encourages parts local features shared objects learned imposing sparseness allows object transformations explained terms small set basic transformations 
bilinear sparse coding assume priors normalization constants parameters sparseness function 
study log 
probabilistic framework squared error function summed images interpreted representing negative log likelihood data parameters log ij see example 
priors marginalize likelihood obtain new likelihood function ij ij 
goal nd ij maximize equivalently minimize negative log certain reasonable assumptions discussed equivalent minimizing optimization function input images ij jjz ij jj gradient descent derive update rules components feature vector transformation vector respectively image assuming xed basis ij dx dt aq ij dy dt qb ij training set inputs values image convergence update basis set ij batch mode dw ab dt ab ij suggested olshausen field order keep basis vectors growing bound adapted norm basis vector way variances maintained xed desired level 
results training paradigm tested algorithms bilinear sparse coding natural image data 
natural images distributed olshausen field code algorithm 
training set images consisted patches randomly extracted source images 
images pre whitened equalize large variances frequency speed convergence 
choose complete basis large number transformations including transformation case 
sparseness parameters set 
order assist convergence learning occurs batch mode batch consisted image patches 
step size gradient descent equation set 
transformations chosen translations range pixels axes 
bilinear sparse coding natural images shows results training natural image data 
comparison learned features linear generative model equation bilinear model provided 
show simple localized oriented features bilinear method able model features di erent transformations 
case range horizontal translations training bilinear model 
provides example bilinear sparse coding model encodes natural image patch patch translated 
note vectors sparse 
shows model account localized feature di erent locations varying vector 
shown column gure translated local feature generated linearly combining sparse set basis vectors ij 
bilinear basis linear basis patch translated natural patch image ij basis images representing natural images transformations sparse bilinear model 
comparison learned features standard linear model bilinear model trained sparseness priors 
rows bilinear case depict translated object features see equation translations pixels 
representation example natural image patch patch translated left 
note bar plot representing vector sparse having signi cant coecients 
code style vectors canonical patch translated likewise sparse 
ij basis images shown dimensions non zero coecients translating learned feature multiple locations 
rows images represent individual basis vectors ij values values selected transformations shown bar plots 
denotes translation pixels cartesian plane 
column shows resulting basis vectors translation 
parts object recognition bilinear generative model equation uses set transformation values features model appropriate global transformations apply entire image region shift pixels image patch global illumination change 
consider problem representing object terms constituent parts 
case able transform part independently parts order account location orientation size part object image 
standard bilinear model extended address need follows ij note object feature set transformation values double summation longer symmetric 
note standard model equation special case equation conducted preliminary experiments test feasibility equation set object features learned standard bilinear model 
fig 
shows results 
results suggest allowing independent transformations di erent features provides rich substrate modeling images objects terms set local features parts individual transformations 
modeling independently transformed features 
shows standard bilinear method generating translated feature combining basis vectors ij set values di erent features 
shows examples images generated allowing di erent values di erent features 
note signi cant di erences resulting images obtained standard bilinear model 
summary fundamental problem vision simultaneously recognize objects transformations 
bilinear generative models provide tractable way addressing problem factoring image object features transformations bilinear equation 
previous approaches unconstrained bilinear models produced global basis vectors image representation 
contrast research image coding stressed importance localized independent features derived metrics emphasize higherorder statistics inputs 
introduces new probabilistic framework learning bilinear generative models idea sparse coding 
results demonstrate bilinear sparse coding natural images produces localized oriented basis vectors simultaneously represent features image transformation 
showed learned generative model translate basis vector di erent locations reducing need learn basis vector multiple locations traditional sparse coding methods 
proposed extension bilinear model allows feature transformed independently features 
preliminary results suggest approach provide exible platform adaptive parts object recognition objects described set independent shared parts transformations 
importance parts methods long recognized object recognition view ability handle combinatorially large number objects combining parts transformations 
methods exist learning representations object parts transformations directly images 
ongoing orts focused deriving ecient algorithms parts object recognition combination bilinear models sparse coding 
acknowledgments research supported nsf sloan research fellowship 
attneave 
informational aspects visual perception 
psychological review 
barlow 
possible principles underlying transformation sensory messages 
editor sensory communication pages 
cambridge ma mit press 
bell sejnowski :10.1.1.57.3911
independent components natural scenes edge lters 
vision research 
hinton ghahramani 
generative models discovering sparse distributed representations 
philosophical transactions royal society 
lewicki sejnowski 
learning overcomplete representations 
neural computation 
olshausen field 
emergence simple cell receptive eld properties learning sparse code natural images 
nature 
olshausen field 
sparse coding overcomplete basis set strategy employed 
vision research 
rao ballard 
development localized oriented receptive elds learning translation invariant code natural images 
network computation neural systems 
rao ballard 
predictive coding visual cortex functional interpretation extra classical receptive eld ects 
nature neuroscience 
rao ruderman 
learning lie groups invariant visual perception 
advances neural information processing systems pages 
cambridge ma mit press 
schwartz simoncelli 
natural signal statistics sensory gain control 
nature neuroscience august 
tenenbaum freeman 
separating style content bilinear models 
neural computation 

