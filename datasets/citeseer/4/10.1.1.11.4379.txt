run time adaptation grid environments ammar computer engineering department kuwait university box kuwait ammar eng edu kw study general mapping problem set independent tasks compete shared resources grid environment 
tasks resource allocation requirements 
task requires multiple different resources allocated simultaneously 
run time task may release allocated resources execution completion time 
objective minimize schedule length submitted tasks satisfying resource sharing constraints 
develop phase mapping approach solving problem 
phase approach line planning phase schedule plan gives scheduling order resource assignments tasks generated compile time 
second phase run time adaptation phase 
goal second phase improve performance schedule plan adapting run time changes early release resources variation computation communication costs 
adaptation may involve changing scheduling order resource assignments original schedule plan 
experimental results demonstrate effectiveness approach compared baseline algorithm performs adaptation run time dynamic algorithm performs planning compile time 
phase mapping approach outperforms algorithms respect schedule length 

grid computing emerging new computing paradigm exploits wide variety geographically distributed resources supercomputers devices supported darpa ito quorum program naval postgraduate school subcontract number 
raghavendra viktor prasanna department ee systems university southern california los angeles ca usc edu storage systems special devices enable construction high performance systems 
grid systems offer consistent inexpensive access resources irrespective physical location 
computational grid simply grid called heterogeneous computing hc system 
key features grid systems domain autonomy scalability security interoperability systems 
major challenge systems effectively available resources 
complexities associated management usage resources multiple administrative domains need 
grid environment different resources controlled diverse organizations diverse policies widely distributed locations need 
mapping applications computational grids problem 
mapping algorithms literature static assume perfect estimation computation communication costs available compiletime 
run time computation communication costs may differ estimated costs may greatly affect performance static algorithms 
dynamic algorithms proposed 
static dynamic algorithms consider compute resources 
case systems application requires multiple resources different types allocated simultaneously 
example interactive data analysis application may require simultaneous access storage system holding copy data supercomputer analysis network links data transfer display device interaction 
applications allocation required resources necessary 
general problem resource allocation problem 
run time applications may hold allocated resources entire execution time 
resources may released task finishes execution task longer needs resources 
example data repository needed task get input data may released input data retrieved 
scarce expensive resources stages task execution supercomputer needed process analyze data early stage released soon task finishes 
cases runtime adaptation needed account variation computation communication costs take advantage early release resources 
study general mapping problem set independent tasks compete shared resources computational grid 
tasks resource allocation requirements task requires multiple different resources allocated simultaneously 
run time task may release allocated resources execution completion time 
early release resources predicted compile time 
objective minimize schedule length submitted tasks satisfying resource sharing constraints 
solve mapping problem develop phase mapping approach 
phase approach line planning phase schedule plan generated compile time 
schedule plan gives scheduling order resource assignments tasks schedule length minimized resource sharing constraints satisfied 
second phase approach run time adaptation phase 
goal phase improve performance schedule plan generated compile time adapting run time changes 
develop fast simple window adaptation algorithm accounts run time changes early release resources variation computation communication costs 
mapping event adaptation algorithm applied subset waiting tasks scheduling order schedule plan 
adaptation may involve advancing execution tasks changing resource assignments specified original schedule plan 
experimental results demonstrate effectiveness phase mapping approach 
results show advantages approach compared baseline algorithm performs adaptation run time 
adaptation algorithm improves schedule length schedule plan generated compile time 
compare approach dynamic algorithm performs planning compile time mapping decisions run time 
approach improvement schedule length dynamic algorithm 
experiments vary quality estimated computation communication costs order simulate different run time scenarios 
quality estimated values high actual values close estimated values 
rest organized follows 
section define mapping problem 
section discuss phase phase mapping approach line planning phase 
second phase run time adaptation phase discussed section 
experimental results section 
section gives 

problem definition system model consider computational grid compute resources machines fm set non compute resources fr rrg 
compute resources hpc platforms workstations personal computers non compute resource rk data repository input output device assume task access resource compute resource time 
system resources interconnected heterogeneous communication links 
estimated communication costs comm matrix comm rk mj gives estimated cost transferring byte data resource rk machine mj 
ma mj gives list time slots machine mj available ra rk gives list time slots resource rk available 
mapping proceeds lists updated 
application model system consideration set independent tasks ft tng compete shared resources system 
assume complete set tasks mapped known priori 
assume task ti needs concurrent access set resources compute resource number additional non compute resources specified resource requirements set ti ti amount data transferred task ti resource rk rk ti data ti rk 
say task ti task tj compatible ti tj ti tj incompatible 
incompatible tasks executed concurrently due resource sharing constraints 
task ti start execution required resources available 
required resources allocated ti execution 
assume required resources acquired time 
resources available tasks released ti execution completes execution 
information assumed available compile time early release resources 
assume estimate computation time task ti machine mj available compiletime 
estimated computation times estimated computation time ect matrix 
ect ti mj gives estimated computation time task ti machine mj 
task ti executed machine mj ect ti mj set infinity 
execution time task ti machine mj exec ti mj depends computation time ti mj data transfer times mj resources ti needs access execution 
example systems assume computation communication overlapped exec ti mj defined exec ti mj ect ti mj rk ti data ti rk comm rk mj term gives total time transfer required data mj resource rk ti 
exec ti mj defined different ways consider overlapping computation communication communication models 
average execution time task ti exec ti defined mx exec ti exec ti mj st ti mj ft ti mj earliest estimated start time finish time task ti machine mj respectively ti mapped mj 
st ti mj equal earliest time mj available duration exec ti mj 
ft ti mj defined ft ti mj st ti mj exec ti mj objective function objective minimize schedule length makespan submitted tasks satisfying implied resource sharing constraints 
objective optimize performance individual task 
formally define objective function minimize max ime ti ime ti completion time task ti total number submitted tasks 

line planning phase phase approach solving general mapping problem defined section offline planning phase 
goal phase generate valid schedule plan minimizes schedule length submitted tasks satisfying resource sharing constraints 
schedule plan gives scheduling order resource assignments tasks 
specifies estimated start finish times task required resources 
start finish times resource assignments specified static mapping algorithm phase estimated computation communication costs 
phase assume required resources held task entire execution time early release resources predicted compile time 
developed static algorithms mapping applications represented directed acyclic graphs dags resource allocation requirements hc systems computational grids 
compatibility graph capture implied resource sharing constraints tasks vertex vi denotes task ti edge eij exists ti tj incompatible 
idea algorithms selecting maximal sets independent tasks execute concurrently 
independent set undirected graph defined set vertices vertices set adjacent 
independent set called maximal independent set independent set contains 
maximal independent set compatibility graph represents maximal set tasks precedence resource sharing constraints executed concurrently 
independent tasks representation considered special case dag representation idea algorithms developed line planning phase generate schedule plan 
shows pseudo code line planning algorithm 
algorithm highest average execution time algorithm developed 
additional details algorithm 

run time adaptation second phase phase mapping approach run time adaptation phase 
goal phase improve performance schedule plan generated phase adapting run time changes 
run time actual computation communication costs may differ estimated costs line planning phase generate schedule plan 
tasks may release allocated resources executions finish times opposed assumption phase 
need adapt run time changes may need modify original schedule plan scheduling order resource assignments tasks order improve scheduling length submitted tasks 
develop fast simple window algorithm run time adaptation 
scheduling order resource line planning algorithm 
initialize 
submitted tasks ed sch plan 
length 
construct compatibility graph 
pick task tc highest exec tc 
find maximal independent set tasks tc 
empty 
sort tasks non increasing order exec ti 

task ti order 
assign compute resource mj ti order minimize finish time ft ti mj 

add ti start time finish time resource assignments sch plan 

update ma mj ra rk rk ti 

ft ti mj length length ft ti mj 

add tasks ed remove 
tx allocated task lowest finish time ft tx mj 

remove tx ed 

set candidate tasks allocated 

remove tasks incompatible allocated task 


pick task tc highest exec tc 
find maximal independent set tasks tc 

pseudo code line planning algorithm assignments original schedule plan adaptation algorithm 
algorithm starts executing maximal independent set tasks ordered schedule plan 
algorithm proceeds considering subset waiting tasks mapping event order adapt run time changes 
adaptation may involve changing scheduling order resource assignments selected subset tasks 
mapping event defined time runtime adaptation applied selected set tasks 
mapping events repeated fixed time intervals seconds time task finishes execution time resource available 
general frequent mapping events leads better adaptation 
hand cost adaptation increases frequency mapping events increases 
frequency mapping events modified adjusted dynamically runtime 
approach choose mapping events occur time resource available 
reason selecting subset tasks window tasks considered mapping event minimize cost adaptation 
considering waiting tasks expensive adversely affects original schedule plan 
want avoid thrashing ensure performance adaptation worse adaptation approach 
pseudo code run time adaptation algorithm shown 
algorithm starts executing maximal independent set tasks selected offline planning algorithm steps 
start time tasks original schedule plan equal 
tasks executed assigned machines schedule plan 
adaptation algorithm applied set tasks information available variation computation communication costs early release resources 
adaptation algorithm proceeds follows tasks executed 
subset tasks selected mapping event starting waiting task scheduling order schedule plan step 
size equal pre selected value window size 
step tasks executed mapping event removed tasks considered execution scheduling order schedule plan 
task ti find machine mb gives best finish time ti mapping event 
compare mb machine mj assigned ti line planning algorithm specified schedule plan 
comparison decide execute ti machine mb mapping event 
comparison cases 
mb mj machine mb machine selected line planning algorithm task ti best suited machine ti 
approach case execute ti mb mapping event 

mb mj machine mb different machine selected line planning algorithm task ti 
case compare performance machines estimated execution time ti machine follows exec ti mb exec ti mj approach case execute task ti machine mb mapping event mb performance mj 
exec ti mb exec ti mj case clear machine mb better suited task ti mj line algorithm assign ti 
opportunity time improve finish time ti executing mb 
approach execute ti mb mapping event 
exec ti mb exec ti mj case experiments show idea execute ti mb especially exec ti mb exec ti mj 
mb expected best machine ti time schedule length suffer actual execution time ti mb greater exec ti mb 
small difference actual estimated value execution time large effect schedule length exec ti mb exec ti mj 
approach case execute ti mb mapping event exec ti mb exec ti mj exec ti mj delta value 
complexity run time adaptation algorithm function number tasks number mapping events window size 

experimental results conducted extensive simulations evaluate performance phase mapping approach 
define grid system number machines machines number resources resources software simulator inputs 
estimated communication costs resources selected randomly uniform distribution mean equal ave comm 
workload consists independent tasks randomly generated follows 
total number tasks run time adaptation algorithm 
sch plan schedule plan generated line planning phase 

counter 
run maximal independent set tasks ordered sch plan 
scheduling order resource assignments sch plan 
update counter execution 

counter points task run time time 

counter total number tasks 
mapping event 
select subset tasks size equals window size starting sch plan counter 

remove tasks run time preserve scheduling order 

task ti scheduling order 
find best machine mb execute ti time 

execute ti mb exec ti mb exec ti mj exec ti mj 
mj machine assigned ti sch plan 

counter 


run time adaptation algorithm tasks average computation cost task ave comp inputs 
estimated computation cost task machine randomly selected uniform distribution mean equal ave comp 
resource requirements task randomly selected available resources 
number required resources randomly selected uniform distribution mean equal resources 
amount data transferred resource resource requirements set randomly selected uniform distribution mean equal ave data size 
actual run time values computation communication costs randomly selected uniform distribution range pe pe estimated values pe percentage error value 
perfectly estimated values correspond pe 
experiments machines set resources set 
tasks generated ave comp ave data size kbyte 
pe set set window size window size set total number tasks 
performance metrics evaluate performance mapping approach 
metrics schedule length adaptation cost time sun enterprise gbyte memory 
compare approach baseline algorithm performs adaptation runtime 
baseline algorithm called adapt algorithm 
data point figures section average distinct runs 
compares approach adapt algorithm different numbers tasks ranging tasks tasks increments tasks 
approach improvement schedule length adapt algorithm 
clearly shows advantage approach adapt algorithm performs adaptation run time 
shown cost adaptation algorithm small compared expected execution times applications hc systems computational grids 
cost adaptation second tasks 
comparison approach adapt algorithm different window size values shown 
total number tasks figures tasks 
changed value window size tasks increments tasks 
corresponds changing window size values total number tasks increments 
shows improvement schedule length approach adapt algorithm increases window size increases 
hand cost adaptation algorithm increases window size increases shown 
window size increases tasks examined algorithm mapping event 
experiment performance approach relatively window size tasks total number tasks 
due resource sharing constraints prevent tasks run time 
increasing window size help finding tasks run concurrently resource sharing tasks high 
effect pe value performance approach shown set tasks 
changed value pe increments 
shown improvement schedule length approach adapt algorithm increases pe value increases 
clearly shows advantage run time adaptation approach increases quality estimated computation communication costs decreases 
adapt algorithm uses estimated values adapt run time changes greatly affected quality estimated costs 
shows effect values defined section performance mapping approach set tasks 
changed value increments 
shown improvement schedule length approach adapt algorithm increases value increases 
performance approach decreases 
recall run time adaptation algorithm compare performance machines order decision regarding advancing execution changing machine assignment task ti 
value high depends pe value small difference actual estimated computation communication costs large effect schedule length explained section 
experiments adaptation may lead worse results adapt algorithm total number tasks small difference estimated actual computation communication costs small quality estimated values high 
order demonstrate effectiveness line planning algorithm compare approach dynamic algorithm performs planning compile time 
initially algorithm called plan algorithm randomly orders tasks 
examines subset tasks mapping mapping event 
size equal window size 
task executed mapping event mapped best machine time 
shows comparison approach plan algorithm respect schedule lengths 
frequency mapping events window size approaches 
shown approach improvement schedule length plan algorithm 
clearly shows advantage line planning algorithm 
schedule length approach adapt number tasks 
comparison schedule lengths different number tasks adaptation cost sec approach number tasks 
adaptation cost different number tasks improvement adapt approach window size tasks 
improvement schedule length approach adapt algorithm different window sizes adaptation cost sec approach window size tasks 
adaptation cost different window sizes improvement adapt approach percentage error pe 
improvement schedule length approach adapt algorithm different pe values improvement adapt approach delta 
improvement schedule length approach adapt algorithm different values 
developed phase approach mapping set independent tasks grid environment 
tasks resource allocation requirements task requires multiple different resources allocated simultaneously 
phase approach line planning phase schedule plan generated compile time objective minimizing schedule length submitted tasks satisfying resource sharing constraints 
second phase run time adaptation phase run time changes variation computation communication costs early release resources considered order improve performance schedule plan generated compile time 
evaluated performance phase mapping approach simulation results 
experimental results showed advantages run time adaptation algorithm compared baseline algorithm performs adaptation run time 
adaptation algorithm improved schedule length schedule plan generated compile time 
results showed approach improvement schedule length dynamic algorithm performs planning compile time 
showed advantage line planning algorithm 
general experimental results demonstrated effectiveness phase mapping approach 
ahmad 
kwok 
parallelizing multiprocessor scheduling problem 
ieee trans 
parallel distributed systems april 
schedule length approach plan number tasks 
comparison plan algorithm 
unified mapping framework heterogeneous computing systems computational grids 
phd thesis university southern california 
prasanna raghavendra 
unified resource scheduling framework heterogeneous computing environments 
th heterogeneous computing workshop hcw pages april 
prasanna raghavendra 
framework mapping resource allocation heterogeneous computing systems 
th heterogeneous computing workshop hcw pages may 
braun siegel beck maheswaran robertson yao hensgen freund 
comparison study static mapping heuristics class meta task heterogeneous computing systems 
th heterogeneous computing workshop hcw pages april 
buyya chapin 
architectural models resource management grid 
st ieee acm international workshop grid computing pages bangalore india december 
christofides 
graph theory algorithmic approach 
academic press 
foster kesselman editors 
grid blueprint new computing infrastructure 
morgan kaufmann san francisco ca 
foster kesselman lee lindell nahrstedt roy 
distributed resource management architecture support advance reservations allocation 
intl 
workshop quality service 
freund carter watson keith 
generational scheduling heterogeneous computing systems 
intl 
conf 
parallel distributed processing techniques applications pdpta pages august 
iverson 
dynamic competitive scheduling multiple dags distributed heterogeneous environment 
th heterogeneous computing workshop hcw pages march 
iverson 
parallelizing existing applications distributed heterogeneous environment 
th heterogeneous computing workshop hcw pages april 
prasanna wang 
heterogeneous computing challenges opportunities 
ieee computer june 
potter scott 
dynamic task mapping algorithms distributed heterogeneous computing environment 
th heterogeneous computing workshop hcw pages april 
maheswaran ali siegel hensgen freund 
dynamic matching scheduling class independent tasks heterogeneous computing systems 
th heterogeneous computing workshop hcw pages april 
maheswaran siegel 
dynamic matching scheduling algorithm heterogeneous computing systems 
th heterogeneous computing workshop hcw pages march 
watson flann freund 
genetic simulated annealing scheduling data dependent tasks heterogeneous environment 
th heterogeneous computing workshop hcw pages april 
lee 
compile time scheduling heuristic interconnection constrained heterogeneous processor architectures 
ieee trans 
parallel distributed systems feb 
catlett 
metacomputing 
communications acm june 

wu 
task scheduling algorithms heterogeneous processors 
th heterogeneous computing workshop hcw pages april 
wang siegel roychowdhury 
task matching scheduling heterogeneous computing environments genetic algorithm approach 
journal parallel distributed computing november 
biographies ammar assistant professor computer engineering department kuwait university 
received degree computer engineering kuwait university ph degrees computer engineering university southern california 
main research interest resource management task mapping grids environments heterogeneous computing systems 
member ieee ieee computer society acm 
raghavendra research professor department electrical engineering systems university southern california 
received ph degree computer science university california los angeles 
september december faculty electrical engineering systems department university southern california los angeles 
january july boeing chair professor computer engineering school electrical engineering computer science washington state university 
received presidential young investigator award ieee fellow 
subject area editor journal parallel distributed computing editor chief special issues new journal called cluster computing kluwer publishers program committee member networks related international conferences 
viktor prasanna prasanna kumar received bs electronics engineering bangalore university ms school automation indian institute science 
obtained ph computer science pennsylvania state university 
currently professor department electrical engineering department computer science university southern california los angeles 
associate member center applied mathematical sciences usc 
served division director computer engineering division 
research interests include parallel distributed computation computer architecture vlsi computations high performance computing signal image processing vision 
dr prasanna published extensively consulted industries areas 
served organizing committees international meetings vlsi computations parallel computation high performance computing 
steering chair international parallel distributed processing symposium merged ieee international parallel processing symposium ipps symposium parallel distributed processing spdp steering chair international conference high performance computing 
serves editorial boards journal parallel distributed computing ieee transactions computers ieee transactions parallel distributed systems 
founding chair ieee computer society technical committee parallel processing 
fellow ieee 
