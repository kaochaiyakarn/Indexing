polytechnic cluster delta compression collection files zan memon torsten suel cis department polytechnic university december tr tr cis cluster delta compression collection files zan memon torsten suel cis department polytechnic university brooklyn ny delta compression techniques commonly succinctly represent updated version file respect earlier 
study delta compression somewhat different scenario wish compress large collection related files performing sequence pairwise delta compressions 
problem finding optimal delta encoding collection files pairwise deltas reduced problem computing branching maximum weight weighted directed graph solution inefficient scale larger file collections 
motivates propose framework cluster delta compression uses text clustering techniques prune graph possible pairwise delta encodings 
demonstrate efficacy approach experimental results collections web pages 
experiments show cluster delta compression collections provides significant improvements compression ratio compared individually compressing file tar gzip moderate cost efficiency 
shorter version appears proceedings rd international conference web information systems engineering wise december 
project supported intel wireless internet center advanced technology polytechnic university 
torsten suel supported nsf career award nsf ccr 
delta compressors software tools compactly encoding differences files strings order reduce communication storage costs 
examples tools utilities computing edit sequences files vdelta tools compute highly compressed representations file differences 
tools number applications various networking storage scenarios see detailed discussion 
communication scenario typically exploit fact sender receiver possess file similar transmitted file transmitting difference delta files requires significantly smaller number bits 
storage applications version control systems deltas orders magnitude smaller compressed target file 
delta compression techniques studied detail context world wide web consecutive versions web page differ slightly pages site share lot common html structure 
particular considers possible improvements caching sending delta respect previous version page similar page located client proxy cache 
study delta compression slightly different scenario 
applications delta compression performed respect previous version file easy identify file interested delta compression better compress large collections files obvious efficiently identify appropriate target files 
approach reduction optimum branching problem graph theory proposed clustering techniques finding similar files 
focus collections web pages sites 
applications mind efficient downloading storage collection web pages line browsing improved archiving massive terabyte web collections internet archive seehttp archive org 
techniques study applicable scenarios lead new general purpose tools exchanging collections files improve currently gzip tools 
contributions study problem compressing collections files focus collections web pages varying degrees similarity files 
approach efficient delta compressor particular compressor achieve significantly better compression obtained compressing file individually tools collection 
main contributions problem obtaining optimal compression collection files specific delta compressor solved finding optimal branching directed graph nodes edges 
implement algorithm show achieve significantly better compression current tools 
hand algorithm quickly inefficient collection size grows files due quadratic complexity 
general framework called cluster delta compression efficiently com puting near optimal delta encoding schemes large collections files 
framework combines branching approach proposed hash techniques clustering files similarity :10.1.1.24.779:10.1.1.17.6054:10.1.1.38.249
framework evaluate number different algorithms heuristics terms compression running time 
results show compression close achieved optimal branching algorithm achieved time small multiplicative factor time needed tools 
note limitations study results preliminary expect additional improvements running time compression results 
particular believe narrow gap speed gzip best algorithms 
secondly restrict case target file compressed respect single file 
additional significant improvements compression achievable file cost additional algorithmic complexity 
consider problem compressing entire collection allow individual files added retrieved collection 
rest organized follows 
subsection lists related 
section discuss problem compressing collection files delta compression describe optimal algorithm computing maximum weight branching directed graph 
section provides framework called cluster delta compression outlines approaches framework 
section experimental results 
section provides open questions concluding remarks 
related overview delta compression techniques applications see 
delta compression techniques originally introduced context version control systems see discussion 
main delta compression algorithms today diff vdelta 
find difference files compress difference simple widely way perform delta compression provide compression files slightly similar vdelta hand relatively new technique integrates data compression data differencing 
refinement tichy block move algorithm generalizes known lempel ziv technique delta compression 
compressor shown achieve compression running time 
issue appropriate distance measures files strings studied extensively different measures proposed 
note diff related symmetric edit distance measure vdelta lempel ziv type delta compressors related copy distance files 
studies measure called lz distance closely related performance lempel ziv type compressing schemes 
refer protocols estimating file similarities communication link 
fast algorithms optimum branching problem described 
aware previous uses optimum branchings compress collections files previous applications quite similar 
particular tate uses optimum branchings find optimal scheme compressing multispectral images adler mitzenmacher compress graph structure world wide web 
adler mitzenmacher show natural extension branching problem hypergraphs model delta compression files np complete indicating efficient optimal solution 
types hash clustering techniques technique quadratic complexity called min wise independent hashing proposed broder see manber wu similar technique nearly linear time technique called hashing proposed indyk motwani applied web documents :10.1.1.24.779:10.1.1.17.6054:10.1.1.38.249
chan woo observe case web pages similarities url provide powerful heuristic identifying files delta compression 
web page close subdirectory web server shares lot content structure page principle provide useful heuristic file collections hierarchical file system 
follow approach highly dependent nature collection 
practice useful combine approaches way 
delta compression optimum branchings delta compressors provide efficient way encode difference similar files 
collection files faced problem succinctly representing entire collection appropriate delta encodings target files 
observe problem finding optimal encoding scheme collection files pairwise deltas reduced computing optimum branching appropriately constructed weighted directed graph problem reduction formally branching directed graph defined set edges incoming edge node contains contain cycle 
weighted directed graph maximum branching branching maximum edge weight 
collection files construct complete directed graph node corresponds file directed edge corresponding weight represents reduction bytes obtained delta compressing file respect file addition nodes graph includes extra null node corresponding empty file model compression savings file compressed zlib empty file 
formulation difficult see maximum branching graph gives optimal delta encoding scheme collection files 
condition definition branching expresses constraint file compressed respect file 
second condition ensures cyclical dependencies prevent decompressing collection 
manner weights assigned maximum branching results compression scheme optimal benefit uncompressed case 
shows weighted directed graph formed collection files 
example node null node nodes represent files 
weights edges node nodes compression savings obtained target files example directed weighted complete graph 
optimal branching graph consists edges data set pages average total cat gzip cat gzip opt branch opt branch size size ratio time ratio time cbc kb mb kb mb kb mb kb mb ebay kb mb thomas dist kb mb sites kb mb table compression ratios collections files 
compressed 
weights edges represent compression savings file compressed file 
optimal sequence compression file compressed files compressed computing delta respect file file compressed computing delta respect file experimental results implemented delta compression optimal branching algorithm described dense graphs takes time proportional number edges 
table shows compression results times collections web pages collected crawling limited number pages site breadth crawler 
results indicate optimum branching approach give significant improvements compression tar followed gzip outperforming factor major problem optimum branching approach inefficient soon number files grows dozens 
ca data set pages took hour perform computation multiple hours needed set sites 
plots running time seconds optimal branching algorithm different numbers files set files software distribution 
time plotted logarithmic scale accomodate curves time spent computing edge weights upper curve time spent actual branching computation weights graph determined calls lower curve 
curves grow quadratically vast majority time spent computing appropriate edge weights graph tiny amount spent actual branching computation 
time seconds speed performance computing complete graph optimum branching number files running time optimal branching algorithm order compress larger collections pages need find techniques avoid computing exact weights edges complete graph sections study techniques clustering pages pruning approximation edges 
note limitation branching approach support efficient retrieval individual files compressed collection addition new files collection 
problem applications require interactive access address 
cluster delta compression shown previous section delta compression techniques potential significantly improved compression collections files 
optimal algorithm maximum branching quickly bottleneck increase collection size mainly due quadratic number pairwise delta compression computations performed 
section describe basic framework called cluster delta compression efficiently computing near optimal delta compression schemes larger collections files 
basic framework describe general approach leads different algorithms implemented 
nutshell basic idea prune complete graph sparse subgraph find best delta encoding scheme subgraph 
precisely general steps collection analysis perform clustering computation identifies pairs files similar candidates delta compression 
build sparse directed sub graph containing edges similar pairs 
assigning weights compute estimate appropriate edge weights maximum branching perform maximum branching computation delta encoding 
determine assignment weights second step done precisely performing delta compression remaining edge approximately estimates file similarity produced document analysis step 
note weights computed precisely delta compressor resulting compressed files saved actual delta compression step consists simply removing files corresponding unused edges assuming sufficient disk space 
primary challenge step need efficiently identify small subset file pairs give delta compression 
solve problem sets known techniques document clustering set proposed broder see similar idea set proposed indyk motwani applied document clustering haveliwala gionis indyk :10.1.1.24.779:10.1.1.17.6054:10.1.1.38.249
techniques developed context identifying near duplicate web pages finding closely related pages web 
problems clearly closely related scenario number differences nontrivial apply techniques delta compression discuss issues 
file similarity measures compression performance delta compressor pair files depends details precise locations lengths matches internal compressibility target file windowing mechanism performance internal huffman coder 
number formal measures file similarity edit distance block moves copy distance lz distance proposed provide reasonable approximations see discussion 
simplified measures easy compute clustering techniques simpler similarity measures refer shingle intersection shingle containment :10.1.1.24.779
formally file integer define shingle set gram set multiset substrings length called shingles occur files define shingle define shingle intersection containment respect 
note shingle containment symmetric 
measures assign higher similarity scores files share lot short substrings intuitively expect correlation delta compressibility files similarity measures 
fact relationship shingle intersection edit distance measure easily derived files edit distance shingle size refer proof similar result case edit distance block moves 
similar relationship derived shingle containment copy distances 
shingle intersection shingle containment related edit distance copy distance measures models corresponding classes edit delta compression schemes 
discussion supports shingle similarity measures scenario practice relationship measures achieved delta compression ratio quite noisy 
efficiency reasons approximate measures introducing additional potential error 
clustering min wise independent hashing describe set techniques called min wise independent hashing proposed broder :10.1.1.24.779
similar technique described manber wu :10.1.1.17.6054
simple idea technique approximate shingle similarity measures sampling small subset shingles file 
order obtain estimate samples drawn independently file obtained coordinated fashion common set random hash functions map shingles length integer values 
select file smallest hash values obtained way 
refer reader detailed analysis :10.1.1.24.779
note number different choices implementing schemes choice hash functions class simple linear hash functions analyzed indyk 
sample sizes option fixed number samples say file independent file size 
alternatively sample constant rate say resulting sample sizes proportional file sizes 
hash functions way select samples file hash functions include minimum value hash function sample 
alternatively select random hash function select smallest values hash function 
selected second method significantly efficient requiring hash function computation shingle 
shingle size shingle size bytes results reported 
experimented achieved slightly worse results 
selecting sample estimate shingle intersection shingle containment measures intersecting samples pair files 
phase takes time quadratic number files 
decide edges include sparse graph independent choices similarity measure intersection containment measure 
threshold versus best neighbors keep edges certain similarity threshold say graph 
file keep promising incoming edges constant edges coming nearest neighbors estimated similarity measure 
detailed discussion various implementation choices outlined impact running time compression experimental section 
total running time clustering step min wise independent hashing number files average size file roughly average size sample 
main advantage optimal algorithm edge performing delta compression step files size kilobyte perform simpler computation samples small size say 
results significant speedup optimal algorithm practice algorithm eventually inefficient due quadratic complexity 
clustering locality sensitive hashing second set techniques proposed indyk motwani applied document clustering haveliwala gionis indyk extension set results linear running time :10.1.1.38.249
particular techniques avoid pairwise comparison files performing number sorting steps specially designed hash signatures directly identify similar files 
step technique identical min wise independent hashing technique fixed sample size 
select file fixed number min wise independent hash values different random hash functions 
file th hash function 
main idea called locality sensitive hashing hash values construct file signatures consist concatenation hash values concatenate bit hash values bit signature 
files agree signature strong evidence intersection threshold 
formally shown repeating process number times depends chosen threshold find pairs files shingle intersection threshold avoiding pairs threshold 
formal description technique refer 
resulting algorithm consists steps sampling extract fixed number hash values different hash functions 
locality sensitive hashing repeat times file different hash functions additional factor added term 
value selected collection randomly select indexes 
file construct signature concatenating hash values 
sort resulting signatures scan sorted list find pairs files signature identical 
pair add edges directions running time method constants range depending choice parameters 
discuss parameter settings consequences detail experimental section 
note limitations 
implementation identifies pairs fixed similarity threshold 
allow determine best neighbors node provide estimate precise similarity pair significantly slightly threshold 
second method shingle intersection shingle containment 
addressing limitations issue 
experimental evaluation section perform experimental evaluation cluster compression schemes implemented framework previous section 
introduce algorithm experimental setup 
subsection show naive methods thresholds give results 
subsections look different techniques resolve problem subsection presents results best algorithms larger data set 
due space constraints large number options give selection results 
refer reader complete evaluation 
algorithms implemented number different algorithms variants 
particular options basic scheme mh vs lsh 
number hash function single hash vs multiple hash 
sample size fixed size vs fixed rate 
similarity measure intersection vs containment 
edge pruning rule threshold vs best neighbors vs heuristics 
edge weight exact vs estimated 
note combination choices sense 
example lsh implementations support containment best neighbors require fixed sample size 
hand observe benefit multiple hash functions mh scheme assume single hash function case 
note implementations samples treated sets multi sets frequently occurring string 
intuitively appropriate goal modeling delta compression performance 
algorithm sample threshold remaining branching benefit total size edges size edges zlib size zlib mb tar gzip mb optimal mb mh mb intersect mb mb mb mh mb intersect mb mb mb mh mb contain mb mb mb table number remaining edges number edges final branching compression benefit threshold clustering schemes different sampling techniques threshold values 
algorithms implemented compiled gcc solaris 
experiments run sun enterprise server ultrasparc cpus mhz gb ram 
cpu experiments data read single scsi disk 
note large amount memory fast disk minimize impact running times 
data sets medium data set consists union web page collections section files total size large data set consists html pages crawled poly edu domain total size pages crawled breadth crawl attempted fetch pages reachable poly edu homepage subject certain pruning rules avoid dynamically generated content cgi scripts 
threshold methods experiments look performance mh lsh techniques try identify retain edges certain similarity threshold 
table look optimum branching method different algorithms fixed threshold select edges considered similar different thresholds 
method show number similar edges number edges included final branching total improvement obtained method compared compressing file individually 
results demonstrate fundamental problem arises threshold methods high thresholds vast majority edges eliminated resulting branching poor quality compared optimal 
low thresholds obtain compression close optimal number similar edges high problem determines cost subsequent computation 
number edges included unfortunately numbers indicate real sweet spot threshold gives small number remaining edges compression data set 
note result due precision sampling methods holds threshold lsh algorithms 
simplified explanation data sets contain different clusters various similarity low threshold keep clusters intact dense graphs edges high threshold disconnect clusters resulting inferior compression 
leads study techniques overcoming problem best neighbors retaining best incoming edges node mh algorithm keep number edges bounded estimating weights way improve efficiency threshold mh algo rithms directly similarity estimate provided mh schemes edge weight subsequent branching 
pruning heuristics experimented heuristics decreasing number edges lsh algorithms described 
summary fixed threshold followed optimal branching remaining edges result trade compression running time 
distance measures investigate assumption underlying algorithms shingle intersection shingle inclusion measures correlate delta compressibility 
plots correlation follows axis edges complete directed graph files data set sorted left right intersection containment measure respectively split groups files 
leftmost coordinate axis corresponds directed edges graph smallest intersection containment similarity measure 
group files plot axis minimum maximum average multiplicative benefit compression provided means compressed twice terms compression ratio 
measures clearly visible plots different bands points 
note maximum minimum values top bottom relationship quite noisy 
considering middle band showing averages see containment provides somewhat better model delta compressibility 
surprising relationship copy distance containment fact modern delta compressors copy 
reason decided limit study mh algorithms containment measure uses fixed sampling rate 
example compute exact weight edge threshold case fixed sample size perform calls cost 
relationship similarity measures delta compression benefits 
note potential drawback containment measure 
consider large collection small files significant containment single large file 
case may decide compress small files large file file resulting scans large file smaller files achieve compression ratio 
general intersection metric assigns low similarity value pairs different size containment metric assigns high similarity value pairs file quite large potentially resulting slow compression decompression time 
experiments running time significantly impacted 
best neighbors look case limit number remaining edges mh algorithm keeping similar edges node proposed 
clearly limits total number edges reducing cost subsequent computations 
table shows running times various phases compression benefit function number neighbors sampling rate 
clustering time course depends heavily sampling rate smallest sampling rate gives reasonable compression observe significant impact compression rate file sizes 
time computing weights graph grows approximately linear compression rate grows small get results maximum benefit 
results time actual branching computation negligible 
fact large file potentially result worse compression pointer window movement mechanisms delta compressor may able fully capture common substrings distributed large file 
sample cluster weighing branching benefit size time time time zlib table running time compression benefit neighbor schemes 
cluster branching benefit time time time zlib table running time compression benefit neighbor schemes sampling rate estimated edge weights 
estimated weights containment measure values computed mh clustering weights remaining edges decrease running time shown table 
time building weighted graph essentially reduced zero 
extra step perform actual compression edges independent cost computing exact weights looking achieved benefit see optimum total cost seconds versus seconds hours optimum branching 
lsh pruning heuristic lsh algorithms experimented simple heuristic reducing number remaining edges sorting file signatures keep subset edges case files identical signatures 
particular building complete graph files connect files simple linear chain directed edges 
somewhat arbitrary heuristic started bug results significantly decreased running time slight cost compression shown table 
currently looking principled approaches thinning tightly connected clusters edges 
threshold edges branching benefit size zlib table number remaining edges compression benefit lsh scheme pruning heuristic 
algorithm running time size uncompressed mb zlib mb cat gzip mb fast mh mb best mh mb fast lsh mb best lsh mb table comparison best mh lsh schemes gzip 
best results large data set results best schemes identified large data set pages poly edu domain 
note results preliminary probably significantly improved optimizations 
unable compute optimum branching set due size expect result slightly lower achieved best method 
mh algorithms neighbors estimated exact edge weights respectively 
lsh algorithms thresholds respectively pruning heuristic previous subsection 
fast mh running time spent clustering scales eventually bottleneck final compression step 
spent computing exact weights remaining edges 
optimizations currently implementing result significant reductions running time 
particular combine methods running lsh low threshold mh remaining edges choose neighbors estimate weights 
working optimizations sampling hashing phase algorithms 
concluding remarks investigated problem delta compression obtain compact representation cluster files 
described problem optimally encoding collection delta compression single file reduced problem computing maximum weight branching 
providing superior compression algorithm scale larger collections motivating propose faster cluster delta compression framework 
studied file clustering heuristics performed extensive experimental comparisons 
preliminary results show significant compression improvements obtained gzip moderate additional computational costs 
open questions remain 
additional optimizations possible lead improvements compression running time including faster sampling better pruning heuristics lsh methods 
second cluster framework proposed uses pairwise deltas file compressed respect single file 
shown multiple files result significant improvements compression fact partially exploited tar gzip files 
discussed polynomial time optimal solution multiple files finding schemes practice challenging 
final goal create general purpose tools distributing file collections improve significantly gzip 
related studying apply delta compression techniques large web repository store billions pages network workstations 
note scenario fast insertions lookups crucial significant changes approach necessary 
early prototype system currently evaluated 
window small adler mitzenmacher 
compressing web graphs 
proc 
ieee data compression conference dcc march 
banga douglis rabinovich 
optimistic deltas www latency reduction 
usenix annual technical conference anaheim ca pages january 
broder :10.1.1.24.779
resemblance containment documents 
compression complexity sequences sequences pages 
ieee computer society 
fratta 
note finding optimum branchings 
networks 
chan woo 
cache compaction new technique optimizing web transfer 
proc 
infocom march 
cormode paterson vishkin 
communication complexity document exchange 
proc 
acm siam symp 
discrete algorithms january 

transparent caching delta transfer system web objects 
may 
unpublished manuscript 
douglis feldmann krishnamurthy mogul 
rate change metrics live study world wide web 
proc 
usenix symp 
internet technologies systems pages berkeley december 
usenix association 
similar internet archive athttp www archive org 
gravano ipeirotis jagadish koudas muthukrishnan srivastava 
grams dbms approximate string processing 
ieee data engineering bulletin december 
haveliwala gionis indyk 
scalable techniques clustering web 
proc 
webdb workshop dallas tx may 

system web browsing wireless environment 
proc 
nd acm conf 
mobile computing networking pages november 
hunt vo tichy 
delta algorithms empirical analysis 
acm transactions software engineering methodology 
indyk 
small approximately min wise independent family hash functions 
proc 
th symp 
discrete algorithms january 
indyk motwani :10.1.1.38.249
approximate nearest neighbors removing curse dimensionality 
proc 
th acm symp 
theory computing pages may 
korn 
vo 
engineering differencing compression data format 
proceedings usenix annual technical conference pages june 
macdonald 
file system support delta compression 
ms thesis uc berkeley may 
manber wu :10.1.1.17.6054
glimpse tool search entire file systems 
proc 
winter usenix conference pages january 
mogul douglis feldmann krishnamurthy 
potential benefits data compression 
proc 
acm sigcomm conference pages 
memon suel 
delta encoding related web pages 
proc 
ieee data compression conference dcc march 
memon suel 
cluster delta compression collection files 
technical report tr cis polytechnic university cis department october 
suel memon 
algorithms delta compression remote file synchronization 
editor lossless compression handbook 
academic press 
appear 
tarjan 
finding optimum branchings 
networks 
tate 
band ordering lossless compression multispectral images 
ieee transactions computers 
tichy 
string string correction problem block moves 
acm transactions computer systems november 
tichy 
rcs system version control 
software practice experience july 
memon suel 
simple delta compression tool 
technical report tr cis polytechnic university cis department june 
ziv lempel 
universal algorithm data compression 
ieee transactions information theory 

