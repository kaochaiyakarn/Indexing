multiagent learning variable learning rate michael bowling manuela veloso computer science department carnegie mellon university pittsburgh pa learning act multiagent environment difficult problem normal definition optimal policy longer applies 
optimal policy moment depends policies agents creates situation learning moving target 
previous learning algorithms shortcomings depending approach 
converge policy may optimal specific opponents policies may converge 
article examine learning problem framework stochastic games 
look number previous learning algorithms showing fail criteria 
contribute new reinforcement learning technique variable learning rate overcome shortcomings 
specifically introduce wolf principle win learn fast varying learning rate 
examine technique theoretically proving convergence self play restricted class iterated matrix games 
empirical results variety general stochastic games situations self play demonstrating wide applicability method 
key words multiagent learning reinforcement learning game theory research multiagent systems includes investigation algorithms provide behaviors functions select actions multiple agents coexisting environment 
multiagent systems increasingly relevant artificial intelligence software robotic agents email addresses 
cmu edu michael bowling 
cmu edu manuela veloso 
preprint submitted artificial intelligence may prevalent 
robotic soccer disaster mitigation rescue automated driving information commerce agents examples challenging multiagent domains 
automation trend continues need robust algorithms coordinating multiple agents effectively responding external agents 
multiagent domains require determining course action agent just single agent domains 
machine learning powerful tool finding successful course action greatly simplify task specifying appropriate behaviors agent 
particular learning agent discover exploit dynamics environment adapt unforeseen difficulties task 
benefits caused learning studied extensively single agent problems stationary environment 
multiagent environments learning important difficult selection actions take place presence agents 
consider multiagent domains agents forced interact agents may independent goals assumptions algorithms conventions 
interested approaches agents learn adapt agents behavior 
assume agents ability adapt behavior face difficult learning problem moving target 
optimal course action changing agents adapt 
external adapting agents violate basic stationary assumption traditional techniques behavior learning 
new techniques need considered address multiagent learning problem 
furthermore multiagent learning strong connection game theory players select actions maximize payoffs presence payoff maximizing players 
efforts contributed new approaches multiagent learning problem successfully demonstrating algorithms learn optimal policies specific assumptions 
article overview algorithms providing parallel game theory multiagent learning 
analysis previous algorithms leads introduce desirable properties multiagent learning algorithms rationality convergence 
interestingly note previous algorithms offer properties 
article contribute new learning technique variable learning rate 
introduce concept provide specific principle adjust learning rate wolf principle standing win learn fast 
successfully develop apply wolf principle different learning approaches 
novelty wolf principle face challenge determining wolf learning algorithm rational convergent introduced properties multiagent learning algorithms 
show rationality property contribute theoretical proof convergence wolf gradient ascent restricted class iterated matrix games 
show empirical results convergence rationality extended wolf algorithm compare performance variety game situations previously learning algorithms 
article organized follows 
section describe stochastic game framework description multiagent learning problem 
examine previous techniques crucial shortcomings 
section describe variable learning rate technique wolf principle 
analyze theoretically restricted class games proving overcomes shortcomings previous algorithms 
section describe practical algorithm extends technique general class stochastic games 
section show results demonstrating applicability effectiveness algorithms introduced 
stochastic games learning section give overview stochastic game framework 
introduce desirable properties multiagent learning algorithms rationality convergence 
examine previous learning algorithms specifically comparing respect properties 
stochastic game framework presenting formal definition stochastic game examine related models 
examining markov decision processes multiple state framework 
examine matrix games single state framework 
introduce stochastic game framework seen merging mdps matrix games 
due background game theory agent systems terms agent player interchangeably 
markov decision processes markov decision process mdp tuple set states set actions transition function reward function jr transition function defines probability distribution states function current state agent action 
reward function defines reward received selecting action state 
solving mdps consists finding policy mapping states actions maximize discounted reward discount factor mdps focus reinforcement learning rl 
crucial result forms basis existence stationary deterministic policy optimal 
policy target rl algorithms 
matrix games matrix game strategic game tuple number players set actions available player joint action space ri player payoff function ir 
players select actions available set receive payoff depends players actions 
called matrix games ri functions written dimensional matrices 
matrix games players finding strategies maximize payoff 
pure strategy selects action deterministically 
mixed strategy selects actions probability distribution available actions 
strategy player denoted ai pd probability distribution set actions available player 
pure strategy distributions assigns action probability 
notation refer joint possibly mixed strategy players excluding player pd 
example matrix games shown table 
table shows matrices simple player game called matching pennies 
game player may select heads tails 
choices player takes dollar player 
different player gives dollar player 
matrices represent payoffs players row columns corresponding actions heads tails 
table shows game rock scissors 
game players select action winner determined rules beats rock scissors beats rock beats scissors 
winner takes dollar loser 
table shows coordination game players actions 
players receive payoff select action players different preferences actions prefer agree 
mdps difficult define means solve matrix game 
strategy evaluated players strategies known 
illustrated matching pennies game table 
game player going play heads player optimal strategy matching pennies table rock scissors coordination game example matrix games 
games zero sum games general sum game 
play heads player going play tails player optimal strategy play tails 
optimal pure strategy independent opponent 
similarly opponent independent mixed strategy optimal 
exist opponent dependent solution set solutions 
called best response 
definition game best response function player bri set strategies optimal player play joint strategy major advancement driven development matrix games game theory notion best response equilibrium nash equilibrium 
definition nash equilibrium collection strategies players ai player better changing strategies players continue follow equilibrium strategy 
notion equilibrium compelling matrix games nash equilibrium may 
types matrix games 
matrix games usefully classified structure payoff functions 
common classes games purely collaborative purely competitive games 
purely collaborative games team games agents payoff function action best interest agent best interest agents 
purely competitive games agents payoff function negative 
games table table examples game 
purely competitive games called zero sum games payoff functions sum zero equivalently constant 
games including purely collaborative games called general sum games 
table example game 
appealing feature zero sum games contain unique nash equilibrium 
equilibrium solution relatively simple linear program 
finding equilibria general sum games requires difficult quadratic programming solution 
stochastic games stochastic game framework combining mdps matrix games 
stochastic game tuple number players set states set actions available player joint action space transition function ri reward function ith agent ir 
looks similar mdp framework multiple players selecting actions state rewards depend joint action players 
important notice player separate reward function 
interested determining course action player environment 
specifically want learn stationary possibly stochastic policy pd maps states probability distribution actions 
goal find policy maximizes player discounted reward discount factor non trivial result proven shapley zero sum games fink general sum games exist equilibria solutions stochastic games just matrix games 
stochastic games natural extension mdps multiple agents 
extension matrix games multiple states 
state stochastic game viewed matrix game payoff player joint action state determined ri 
playing matrix game receiving payoffs players transitioned state matrix game determined joint action 
see stochastic games contain mdps matrix games subsets framework 
second connection mdps stochastic games 
player stochastic game play fixed possibly stochastic policy problem remaining agent reverts back mdp 
fixing agents policies multiple equilibria equal payoffs interchangeable 
value equilibrium solution linear program mina rl cra 
stochastic transitions markovian depending remaining player actions 
types stochastic games 
classification matrix games stochastic games 
purely collaborative games team games ones agents reward function 
purely competitive games zero sum games player games player reward negative 
matrix games zero sum stochastic games unique nash equilibrium finding equilibrium longer trivial 
shapley existence proof value iteration algorithm examined variety techniques 
properties section properties desirable multiagent learning algorithms 
examine previous algorithms achieved properties 
intuitively properties formalize idea learner learn best response possible 
learner guarantee convergence 
define properties formally 
property rationality players policies converge stationary policies learning algorithm converge policy players policies 
fairly basic property requiring learner learn play policy players play stationary policies case best response policy exist 
observed earlier players play stationary policies stochastic game simply mdp 
property requires algorithm finds optimal policy mdp 
property convergence learner necessarily converge station ary policy 
property usually conditioned agents algorithm class learning algorithms 
convergence property requires class players learning algorithms ideally class encompassing useful algorithms learner policy converge 
example refer convergence respect players stationary policies convergence respect rational players 
focus convergence case self play 
players learning algorithm players policies converge 
crucial difficult step convergence general classes players 
addition ignoring possibility self play naive assumption players way inferior identical algorithm 
relationship equilibria 
properties explicitly relate notions equilibria strong relationship 
players rational learning algorithms policies converge converged equilibrium 
seen considering rational players players converge stationary policy converge best response rational 
true players resulting policies equilibrium player playing best response players 
equilibria fixed points rational learning algorithms 
addition players learning algorithm rational convergent self play players guaranteed converge nash equilibrium 
convergent property guarantees convergence previous result converge equilibrium 
previous algorithms stochastic games studied years number algorithms proposed solving games 
algorithms come game theory community reinforcement learning community 
algorithms differ assumptions known control exerted agents algo rithms striking similarity 
algorithms consist mdp solving mechanism form temporal differencing matrix game solving mechanism linear programming 
table summarizes number algorithms game theory reinforcement learning categorizing components 
examine closely specific algorithms reinforcement learning community 
addition presenting algorithm examine light properties section 
algorithm probably commonly applied learning algorithm multiagent systems 
just application single agent learning algorithm learning completely ignoring agents 
examine specifically multiagent learning algorithms minimax opponent modelling 
algorithms selected represent single agent learning included table strict sense stochastic game algorithm 
attempt address agents matrix game solving component 
matrix game solver mdp solver stochastic game solver mg mdp stochastic game solver game theory reinforcement learning lp td lp td lp td qp td fp td van der wal fictitious play shapley minimax avi hu wellman opponent modeling lp qp linear quadratic programming fp fictitious play td temporal differencing table summary algorithms solve stochastic games 
algorithm contains matrix game solving component mdp solving component 
game theory algorithms assume transition reward functions known 
reinforcement learning algorithms receive observations transition reward functions 
emphasized techniques described section basic approaches learning stochastic games 
algorithm hu wellman extension minimax general sum games 
algorithm requires restrictive assumptions order guar converge 
examine algorithm article similar properties minimax 
learning learning single agent learning algorithm specifically designed find optimal policies mdps 
spite original intent widely multiagent learning success 
theoretical merits 
observed players play stationary strategy stochastic game mdp learning learn play optimal response players 
learning rational 
hand learning play stochastic policies 
prevents learners convergent self play 
reason learners converge re rational converge nash equilibrium 
games equilibria mixed equilibria matching pennies rock scissors described section learners possibly converge 
single agent learning techniques capable initialize arbitrarily set learning rate 
repeat current state find equilibrium matrix game acz select action ai distribution exploration 
observing joint action reward state aca 
table algorithm minimax player value operator computes value zero sum matrix game expected payoff player players play nash equilibrium 
playing stochastic policies 
techniques mainly address issues partial observability function approximation single agent decision problems 
general solve problem show learners capable playing stochastic policies converge self play see sections 
minimax littman examine stochastic games framework multiagent reinforcement learning 
extended traditional learning algorithm zero sum stochastic games 
algorithm shown table 
notion function extended maintain value joint actions backup operation computes value states differently replacing max operator value operator 
value operator computes expected payoff player players played unique nash equilibrium 
interesting note basically policy reinforcement learning equivalent original value iteration algorithm stochastic games 
algorithm uses game theoretic concept equilibria order estimate value state 
value update value states transition state 
computation minimax algorithm learns player part nash equilibrium strategy 
requirement players execute actions infinitely completely explore states actions 
true initialize arbitrarily 
repeat state select action ai maximizes ns ai observing agents actions reward state iv ai max ai ai 
table algorithm opponent modeling learning player agent converge part nash equilibrium provides opponent independent method learning equilibrium solution 
algorithm guaranteed converge self play 
hand algorithm rational 
consider opponent rock scissors playing exclusively rock playing scissors small probability 
minimax find equilibrium solution randomizing actions equally best response playing situation best response 
opponent modelling final algorithm examine opponent modelling joint action learners 
algorithm shown table 
idea learn explicit models players assuming playing accord ing stationary policy 
algorithm estimate players select joint action past play 
player plays optimal response estimated distribution 
uther veloso investigated algorithm zero sum games claus boutilier examined fully collaborative matrix games 
algorithm essentially fictitious play reinforcement learning context 
fictitious play game theory algorithm proven find equilibria certain types games 
basically fictitious play algorithm players selecting action iteration received highest total payoff played exclusively past 
fictitious play played players proven converge nash equilibrium games iterated dominance solvable 
games iteratively removing dominated actions actions payoffs lower strategy regardless opponent play leave single action set equivalent actions 
addition games players empirical distribution actions played converge game nash equilibrium actual strategies played may 
behavior opponent modelling algorithm similar results formal proofs 
single agent learners opponent modelling rational 
eventually player estimates opponent policy converge true policy 
finds best response policies estimates eventually converge best response policy opponent true policy 
single agent learning convergent 
reason identical plays pure policies converge games mixed equilibria 
discussion summary single agent learners joint action learners rational guarantee convergence 
minimax guaranteed converge equilibrium guarantee best response actual opponent 
minimax rational 
rational convergent properties encompass desirable learning technique interesting simultaneously achieving properties difficult 
rest article look new technique exactly rational convergent learning algorithm 
idea variable learning rate rational learning algorithms convergent 
section look theoretical analysis variable learning rates restricted class iterated matrix games 
section develop technique general stochastic game learner show empirical results algorithm 
theoretical analysis section examining gradient ascent technique learning simple player action general sum repeated matrix games 
look theoretical analysis algorithm observes algorithm fails converge 
follow introducing concept variable learning rate prove concept fact causes gradient ascent converge 
gradient ascent singh kearns mansour examined dynamics gradient ascent player action iterated matrix games 
represent problem matrices player selects action set determines rewards payoffs players 
row player selects action column player selects action row player receives payoff rij column player receives payoff cij 
action game strategy probability distribution available actions represented single value 
strategy row player corresponds probability player selects action probability player selects second action 
similarly strategy column player 
consider joint strategy point constrained unit square 
pair strategies write expected payoffs row column player receive 
vr vc expected payoffs respectively 
vr rn ra ra ra raa ra vc cn 
caa aa 
cll 
player consider effect changing strategy expected payoff 
computed just partial derivative expected payoff respect strategy gradient ascent algorithm player adjust strategy iteration increase expected payoffs 
means player move strategy direction current gradient step size 
ck strategies kth iteration players gradient ascent new strategies gradient move strategy valid probability space unit square gradient projected back probability space 
occur boundaries probability space 
question consider expect happen players gradient ascent update strategies 
notice algorithm rational properties defined section 
fixing player strategy causes player gradient constant eventually force player converge optimal pure strategy response 
hand algorithm con shown 
despite fact algorithm play mixed strategies 
analysis singh colleagues gradient ascent examines dynam ics learners case infinitesimal step size lim 
call algorithm infinitesimal gradient ascent iga 
observe algorithm appropriately decreasing step size properties iga 
section briefly outline analysis 
analysis main singh kearns mansour theorem 
theorem players follow infinitesimal gradient ascent iga strategies converge hash equilibrium average payoffs time converge limit expected payoffs hash equilibrium 
proof theorem proceeds examining dynamics strat egy pair 
ane dynamical system dynamics defined differential equation define multiplicative matrix term diagonal values classify dynamics system properties dynamical systems theory invertible qualitative forms dynamics system depending purely real purely imaginary eigenvalues 
results cases invertible purely real eigenvalues purely imaginary eigenvalues 
qualitative forms different cases shown 
analysis proceeded examining case geometrically 
important consideration basic forms unconstrained dynamics dynamics projects gradient unit square 
basically requires considering possible positions unit square relative dynamics shown 
crucial aspect analysis points zero gradient constrained dynamics show correspond nash equilibria 
discussed lemma 
unconstrained dynamics exist point zero gradient called center denoted 
point mathematically setting equations zero solving notice center may inside unit square 
addition invertible point zero gradient unconstrained invertible real imaginary ues eigenvalues fig 

qualitative forms iga dynamics 
dynamics 
constrained dynamics gradients boundaries unit square projected unit square additional points zero gradient may exist 
iga converges points zero gradient 
theorem exciting result convergence results rational multiagent learning algorithm 
notion convergence weak 
fact may players policies converge playing gradient ascent expected payoffs may converge 
furthermore moment time expected payoff player arbitrarily poor 
difficult evaluate learner potentially disastrous applied temporal differencing multiple state stochastic games assumes expected payoffs past predict expected payoffs 
section examine method addressing convergence problem 
prove new method stronger notion convergence players converge nash equilibrium 
variable learning rate introduce concept study impact variable learning rate 
gradient ascent algorithm steps taken direction gradient constant 
allow vary time changing update rules idea average payoffs converge means period arbitrarily low payoffs corresponding period past arbitrarily high payoffs 
min max kth iteration algorithm takes step size direction gradient 
notice restrictions require strictly positive bounded bounding step sizes 
specific method varying learning rate contributing wolf win learn fast principle 
essence method learn quickly losing cautiously winning 
intuition learner adapt quickly doing poorly expected 
doing better expected cautious players change policy 
heart algorithm determine player winning losing 
analysis section player select nash equilibrium compare expected payoff payoff receive played selected equilibrium strategy 
equilibrium strategy selected row player equilibrium strategy selected column player 
notice requirement players choose equilibrium strategy pair may nash equilibrium 
formally min winning ma losing min vc vc winning max losing variable learning rate consider case infinitesimal step size limn 
call algorithm wolf iga section show wolf adjustment interesting effect convergence algorithm 

analysis wolf iga prove result 
theorem person action iterated general sum game players follow wolf iga algorithm max min strate gies converge nash equilibrium 
notice standard notion convergence strictly stronger true basic iga 
proof theorem follow closely proof theorem singh colleagues examining possible cases dynamics learners 
write differential equations define system infinitesimal step size ec ec call multiplicative matrix diagonal entries depends learning rates time 
time qualitative form dynamics determined matrix summarized general cases invertible purely real eigenvalues purely imaginary eigenvalues 
thing note cases depend lemma stronger 
lemma invertible defined iga section invertible 
purely imaginary eigenvalues purely imaginary eigenvalues 
purely real eigenvalues purely real eigenvalues 
proof 
positive trivial see uu greater equal zero uu greater equal zero respectively 
exact conditions invertibility purely real imaginary eigenvalues lemma true 
satisfy case general dynamics iga variable learning rate 
sections follow examining cases separately 
proofs cases proceed identically proof iga 
fact proof rely particular learning rate adjustment 
final sub case final case forced deviate arguments 
due fact variable learning rates general change direction gradient sign partial derivatives 
proof iga convergence depends signs derivatives arguments 
cases abbreviated proof convergence illustrate variable learning rate affect arguments 
recommend iga analysis thorough examination including helpful diagrams 
remaining sub case iga shown converge show case wolf iga converge nash equilibrium 
liberal crucial lemma proof iga 
lemma implies algorithms converge strategies converge nash equilibrium 
lemma iga wolf iga limt ac nash equilibrium 
proof 
proof iga shows algorithm converges projected gradient zero strategy pairs nash equilibrium 
wolf iga notice algorithm converges projected gradient zero true projected gradient iga zero 
point nash equilibrium 
examine individual cases 
invertible case dynamics strategy pair qualitative form shown 
lemma invertible iga learning rate adjust ment leads strategy pair converge point boundary nash equilibrium 
proof 
notice invertible zero 
loss generality assume zero gradient column player constant 
column player strategy converge zero depending gradient positive negative 
point row player gradient constant converge zero depending sign gradient 
joint strategy converges corner lemma nash equilibrium 
real eigenvalues case dynamics strategy pair qualitative form shown 
lemma real eigenvalues iga learning rate adjust ment leads strategy pair converge point hash equilibrium 
proof 
loss generality assume 
dynamics show 
consider case center inside unit square 
notice strategy pair quadrant gradient right 
strategy pair region eventually converge upper right corner unit square 
likewise strategies quadrant converge bottom left corner 
consider strategy pair quadrant gradient left strategy eventually exit quadrant entering quadrant possibly hitting center 
center gradient zero converged 
enters quadrants shown converge upper right lower left corner 
strategies converge lemma point nash equilibrium 
cases center unit square boundary unit square shown converge similar analysis discussed 
imaginary eigenvalues case dynamics strategy pair qualitative form shown 
case broken sub cases depending unit square relation center 
center inside unit square 
case argument iga 
lemma imaginary eigenvalues center inside unit square iga learning rate adjustment leads strategy pair converge point boundary hash equilibrium 
proof 
cases consider 
unit square lies entirely single quadrant 
case direction gradient constant right quadrant 
strategies converge appropriate corner bottom right corner quadrant 
second case unit square entirely neighboring quadrants 
consider case lies entirely quadrants gradient points right strategy eventually hit right boundary point quadrant gradient pointing downward 
case converge bottom right corner 
similarly show convergence pairs quadrants 
third final case center boundary unit square 
case points boundary projected gradient zero 
similar arguments strategy converge boundary points 
see diagram explanation 
cases strategy pairs converge lemma converged nash equilibrium 
center inside unit square 
final sub case point dynamics iga wolf iga qualitatively differ 
show iga converge case wolf iga 
proof identify areas strategy space players winning losing show trajectories piecewise elliptical way spiral center 
lemmas sub section implicitly assume imaginary eigenvalues center inside unit square 
lemma considers dynamics fixed learning rates 
lemma learning rates remain constant trajectory strategy pair elliptical orbit center axes ellipse proof 
just result dynamical systems theory mentioned imaginary eigenvalues 
need critical lemma identifies areas strategy space players constant learning rate 
notice corresponds areas players winning losing 
lemma player winning player strategy moving away center 
proof 
notice sub case imaginary eigenvalues center unit square game single nash equilibrium center 
players selected equilibrium strategies wolf principle center ae 
consider row player 
player winning current expected payoffs larger expected payoffs play selected equilibrium 
written write left hand side equation 
simplify substituting center equilibrium strategies ov notice positive factors sign 
true player strategy greater center increasing smaller center decreasing 
player winning strategy moving away center 
shown column player 
corollary quadrant learning rate constant 
combining lemmas find trajectories piece wise elliptical orbits center pieces correspond quadrants defined center 
prove convergence limited number starting strategy pairs 
lemma prove convergence initial strategy pairs 
lemma initial strategy pair sufficiently close center strategy pair converge center 
sufficiently close means elliptical trajectory point defined players learning rate lies entirely unit square 
proof 
loss generality assume shown 
max consider initial strategy 
fixed learning rates players trajectory forms ellipse fig 

trajectory learning rates wolf iga imaginary eigenvalues center inside unit square 
centered ratio radius radius equal trajectory piecewise elliptical consider ellipse trajectory follows quadrant 
shown graphically 
trajectory travels quadrant lemma observe row player winning column player losing 
min max ratio ellipse axes ellipse cross quadrant point similarly quadrant row player losing column player winning ratio ellipse axes rl ellipse cross quadrant point 
continue return axis trajectory began 
strategy pair point 
orbit center decrease distance center factor trajectory converge center 
reason identically sufficiently close initial strategies axes 
lemma imaginary eigenvalues center inside unit square wolf iga leads strategy pair converge center hash equilibrium 
proof 
proof just extension lemma 
consider largest ellipse players learning rates fits entirely unit square 
ellipse touch boundary unit square boundary quadrants 
consider initial strategy pair 
strategy pair follow piecewise elliptical orbits move unit square boundary eventually hit boundary quadrants 
point inside largest ellipse defined players learning rate apply lemma trajectory converge center 
initial strategy pair trajectory converge center nash equilibrium 
lemmas combine prove theorem 
summary wolf principle strengthens iga convergence result 
self play wolf iga players strategies expected payoffs converge nash equilibrium strategies payoffs matrix game 
contributes reinforcement algorithm provably rational convergent self play restricted class iterated matrix games 
result generalized self play corollary 
corollary person action iterated general sum game players follow wolf iga algorithm different min max strategies converge nash equilibrium rmi min max specifically wolf iga max min versus iga max min converge hash equilibrium 
proof 
proof identical theorem 
deviation imaginary eigenvalue case center inside unit square 
case proof lemma amended 

argument trajectory quadrants revolution center new position 
lq rmin cmin mx trajectory converges center 
remainder proof identical theorem 
return examining wolf outside self play section examine situation empirically complex domain 
discussion final points result 
justification wolf principle learning related problems 
second short discussion determining player winning 
look knowledge wolf iga algorithm requires 
requirements relaxed practical algorithm section 
wolf 
apart theoretical result wolf principle may appear just unfounded heuristic 
studied form areas notably considering adversary 
evolutionary game theory adjusted dynamics scales individual growth rate inverse success population 
cause populations composition change quickly population performing poorly 
form appears modification ran weighted majority algorithm 
algorithm expert mistake portion weight loss redistributed experts 
algorithm placing large weights mistaken experts algorithm losing larger portion weights redistributed algorithm adapts quickly 
defining winning wolf principle adjusting learning rate learn faster losing slowly winning 
places great deal emphasis determine player winning 
description wolf iga row player considered winning essentially player winning prefer current strategy playing equilibrium strategy player current strategy 
possible choice determining player winning ex pected payoff currently larger value game equilibrium equilibrium multiple exist 
consider final subcase sec tion mathematically correspond interesting note zero sum games mixed strategy equilibria rules identical 
general sum games necessarily case 
situation bring light differences methods 
exist general sum player action games points strategy space player receiving lower expected payoff equilibrium value lower expected payoff equilibrium strategy player 
essentially player doing poorly strategy poor play player 
point gradient moving strategy away equilibrium rule determine winning player move away equilibrium quickly discourage convergence 
requirements gradient ascent algorithm wolf gradient ascent strict requirements knowledge available player 
specifically basic gradient ascent requires known player payoff matrix rr row player actual distribution actions player playing row player 
requirements strong particularly 
payoffs known example matrix game rr re strategy point 
nash equilibrium 
vr vr vr rules disagree 
need learned experience ac tion selected player known player distribution actions 
wolf gradient ascent proceeds add requirement nash equilibrium known 
case really extra knowledge computed known payoffs 
want algorithm general need addressed algorithm determines winning losing equilibrium known payoffs known 
section look algorithm removes knowledge constraints give empirical results general stochastic games just matrix games 
practical algorithm general algorithm wolf principle vary learning rate 
describing simple rational algorithm policy hill climbing phc converge 
algorithm similar gradient ascent require knowledge 
describe wolf phc varies learning rate approximate notion winning 
section algorithms examined number empirical examples matrix games zero sum general sum stochastic games 
policy hill climbing simple rational learning algorithm capable playing mixed strategies 
simple extension learning shown table 
algorithm essence performs hill climbing space mixed policies 
values maintained just normal learning 
addition algorithm maintains current mixed policy 
policy improved increasing probability selects highest valued action learning rate 
notice algorithm equivalent learning step policy moves greedy policy executing highest valued action probability modulo exploration 
technique learning rational converge optimal policy players playing stationary strategies 
proof follows proof learning guarantees values converge suitable exploration policy 
converge policy algorithms article assume exploration policy suitable learning rates 
initialize repeat state select action mixed strategy suitable exploration 
observing reward state ma ax update constrain legal probability distribution 
argmax table policy hill climbing algorithm phc player greedy converging converge best response 
despite fact rational play mixed policies doesn show promise convergent 
show examples convergence failures section 
wolf policy hill climbing introduce modification naive policy hill climbing algorithm encourages desired convergence property sacrificing rational property 
algorithm uses variable learning rate wolf win learn fast principle 
required changes policy hill climbing shown table 
reminder wolf principle aids convergence giving time players adapt changes player strategy appear beneficial allowing player adapt quickly players strategy changes harmful 
practically algorithm requires learning parameters 
parameter update policy depends agent currently determined winning losing 
determination done comparing online learning 
learning rates 
initialize 
repeat phc table phc table update estimate average policy va update constrain legal probability distribution argmax table wolf policy hill climbing algorithm wolf phc player current expected value greater current expected value average policy 
current expected value lower agent losing larger learning rate 
average policy intended take place unknown equilibrium policy 
games averaging greedy policies fact approximate equilibrium driving mechanism fictitious play 
summary introduces learning rates ii determination winning losing average policy approximation equilibrium policy 
wolf policy hill climbing rational speed learning altered 
convergence properties quite different 
section show examples technique converges best response policies number variety stochastic games 
results show results applying policy hill climbing wolf policy hillclimbing number different games 
domains include matrix games help show algorithms demonstrate empirically theoretical results section predicted 
algorithms applied multi state stochastic games 
general sum grid world domain hu wellman 
zero sum soccer game introduced littman 
examine final matrix game involves players 
briefly look performance wolf non self play experiments 
set experiments involves training players learning algorithm 
phc wolf phc rational know converge converged nash equi see section 
matrix game experiments stochastic game results aggressive 
cases learning rates decreased factor inversely proportionate number times state visited exact proportion varied domains 
learning rates course set hand little fine tuning 
practical application setting learning rate decay identical setting learning rate learning 
learning rate decay requires amount care set appropriate value learning rate don want take larger steps learning rate update values 
results show policy trajectories correspond single training run algorithm showing prototypical trajectory time 
training runs performed domain similar trajectories shown article 
matching pennies rock scissors algorithm applied zero sum matrix games matching pennies rock scissors see table 
games nash equilibrium mixed policy consisting executing actions equal probability 
large number trials small ratio learning rates order illustrate algorithm learns converges 
shows results applying policy hill climbing wolf policy hill climbing matching pennies game 
wolf phc quickly begins oscillate equilibrium decreasing amplitude 
wolf modification oscillates equilibrium appear ance converging 
obvious game rock 
results show trajectories players strategies policy space steps 
policy hill climbing circles equilibrium policy hint converging wolf policy hill climbing nicely spirals equilibrium 
behavior nearly identical theoretically proven behavior wolf iga 
lf phc pr phc pr heads le iterations fig 

results matching pennies matrix game policy players probability distribution learning phc wolf phc 
player policy looks similar 
gridworld examined hu gridworld domain shown 
agents start corners trying reach goal square opposite wall 
players compass actions cases deterministic 
players attempt move square moves fail 
game interesting force players interact initial starting position north action uncertain moves player north probability 
optimal path agent move laterally square move move north goal players move laterally actions fail 
nash equilibria game 
involve player lateral move trying move north 
players coordinate actions 
wolf policy hill climbing successfully converges equilibria 
shows example trajectory players strategies initial player pr rock climbing player pr rock wolf policy hill climbing fig 

results rock scissors matrix game trajectories play ers policies learning phc wolf phc iterations 
state learning steps 
example players converged equilibrium player moves east player moves north initial state 
evidence wolf policy hill climbing case unmodified phc learn equilibrium general sum game multiple equilibria 
sl pr west player pr east fig 

gridworld game 
dashed walls represent actions uncertain 
results show trajectories players policies initial state learning wolf phc 
soccer final domain comparatively large zero sum soccer game introduced littman demonstrate minimax 
shows example initial state game player possession ball 
goal players carry ball goal opposite side field 
actions available compass directions moving 
players select actions simultaneously executed random order adds non determinism actions 
player attempts move square occupied opponent stationary player gets possession ball move fails 
grid world domain nash equilibrium game requires mixed policy 
fact deter policy learned single agent learner jal defeated 
experimental setup resembles littman appropriate comparison reported results minimax 
player trained steps 
training player policy fixed challenger learning trained player 
determines learned policy worst case performance gives idea close minimax wolf wolf phc phc fig 

soccer game 
results show percentage games won specifically trained worst case opponent steps training 
closer percentage closer learned policy equilibrium 
error bars shown relative ordering performance statistically significant 
player equilibrium policy perform worse losing half games challenger 
minimax wolf phc phc generally oscillate target solution points time may close equilibrium short time far 
order account training continued steps evaluated steps 
worst performing policy considered value policy learning run 
shows percentage games won different players playing 
minimax represents minimax trained results taken 
wolf represents wolf policy hill climbing trained 
phc phc represent policy hill climbing respectively 
wolf represents wolf policy hill climbing trained twice training steps 
performance policies averaged train ing runs standard deviations shown lines bars 
relative ordering performance statistically significant 
wolf phc performing equivalently minimax table player matching pennies 
game players selecting heads tails 
payoffs player shown table player selects row second selects column third selects left right table 
amount training continues improve training 
exact effect adjustment seen performance phc larger smaller learning rate 
shows success wolf simply due changing learning rates changing learning rate appropriate time encourage convergence 
player matching pennies previous domains involve players 
examined wolf multiple player matrix game player matching pennies 
game players actions heads tails 
player receives payoff action matches player 
similarly player reward matching player player rewarded selecting action different player 
table shows complete set payoffs players 
game single nash equilibrium corresponding players randomizing equally actions 
shows results players wolf phc domain 
results shown different learning rate ratios 
notice large ratio wolf converges equilibrium just experiments lower ratio converge 
domain examined ratio learning rates important wolf 
due fact takes extra time player strategy change propagate extra player affecting player interest strategy 
interesting wolf converges sophisticated technique smooth fictitious play fails converge game 
results directly comparable due different decay learning rate 
minimax uses exponential decay decreases quickly wolf phc 
note training minimax proven converge equilibrium win half games versus challenger 
results show wolf moving closer equilibrium training 
player player player nash iii player il player player nash ooooo fig 

results wolf phc self play player matching pennies game 
line corresponds player strategy time 
notice different convergence properties depending learning rate ratio 
results self play previous experiments examining wolf phc self play players algorithm 
corollary section gives theoretical evidence wolf may learn learners 
section results examining empirically 
matrix game experiment examining wolf situations outside self play situation described corollary 
examine learning wolf phc unmodified phc rock scissors 
shows trajectories players 
corollary predicted convergence proof gave intuition convergence rate depend ratio learning rates players 
experiment convergence nash equilibrium convergence slower wolf learners see analysis predicted 
wolf pi ic player phc player pr rock fig 

results wolf phc versus phc rock scissors policy trajectories players prototypical run iterations 
soccer littman soccer game see complex game examined far consider wolf converge domain different opponents 
opponent unmodified policy hill climbing second opponent learning 
opponents convergent self play rational 
play going converge nash equilibrium 
experimental setup exactly section 
training performed steps resulting policy frozen challenger trained find policy worst case performance 
closer policy winning half games challenger closer policy equilibrium 
shows results 
show percentage games won challenger measure distance equilibrium policy 
results shown steps training twice amount training unmodified phc learning 
important observations 
learned policy comparatively close equilibrium result phc self play shown comparison 
second appears training move policy closer equilibrium giving evidence convergence 
lo phc phc wolf wolf wolf phc wolf phc fig 

soccer game 
results show percentage games won specifically trained worst case opponent non identical opponent steps training 
closer percentage closer learned policy equilibrium 
notice added improvement training 
phc self play included comparison purposes 
course results proves convergence situations 
results give evidence variable learning rate wolf principle encourage convergence non convergent rational learning algorithm 
specifically wolf phc shown variety domains effectively converge best response policies despite wild non stationarity learning agents different learning algorithms 
multiagent learning powerful needed technique building multiagent systems 
framework stochastic games helps provide model learning problem difficulties attempting learn essentially moving target 
previous techniques adequately address difficulties shortcomings rational play best responses exist necessarily converge 
contribute concept learning variable learning rate shown able overcome shortcomings 
contribute wolf principle define vary learning rate 
altering learning rate wolf principle rational algorithm convergent 
proven restricted class iterated matrix games modifying gradient ascent wolf learning rate 
modify general rational learners 
wolf modification policy hill climbing rational stochastic game learning algorithm 
demonstrated algorithm empirically number single state zero sum general sum player multi player stochastic games 
bellman dynamic programming princeton university press 
howard dynamic programming markov processes mit press 
kaelbling littman moore reinforcement learning survey journal artificial intelligence research 
sutton barto reinforcement learning mit press 
osborne rubinstein course game theory mit press 
owen game theory academic press 
nash jr equilibrium points person games pnas reprinted 
mangasarian stone person nonzero sum games quadratic programming journal mathematical analysis appli cations 
shapley stochastic games pnas reprinted 
fink equilibrium stochastic person game journal science hiroshima university series 
stochastic games finite state action spaces cwi tracts 
competitive markov decision processes springer ver lag new york 
bowling veloso analysis stochastic game theory mul reinforcement learning technical report cmu cs com puter science department carnegie mellon university 
littman markov games framework multi agent reinforce ment learning proceedings eleventh international conference machine learning morgan kaufman pp 

hu wellman multiagent reinforcement learning theoretical framework algorithm proceedings fifteenth international conference machine learning morgan kaufman san francisco pp 

robinson iterative method solving game annals mathematics reprinted 
uther veloso adversarial reinforcement learning tech 
rep carnegie mellon university unpublished 
claus boutilier reinforcement learning operative multiagent systems proceedings fifteenth national conference artificial intelligence aaai press menlo park ca 
bowling convergence problems general sum multiagent reinforce ment learning proceedings seventeenth international conference machine learning morgan kaufman stanford university pp 

watkins learning delayed rewards ph thesis king college cambridge uk 
tan multi agent reinforcement learning independent vs cooperative agents proceedings tenth international conference machine learning amherst ma pp 

sen hale learning coordinate sharing formation 
jaakkola singh jordan reinforcement learning algorithm partially observable markov decision problems advances neural information processing systems 
baird moore gradient descent general reinforcement learning advances neural information processing systems mit press 
singh kearns mansour nash convergence gradient dynamics general sum games proceedings sixteenth conference uncertainty artificial intelligence morgan kaufman pp 

reinhard differential equations foundations applications mc hill text 
weibull evolutionary game theory mit press 
blum burch line learning metrical task system prob lem tenth annual conference computational learning theory nashville tn 
singh jaakkola littman convergence re sults single step policy reinforcement learning algorithms machine learning 
hu learning dynamic noncooperative multiagent systems ph thesis electrical engineering computer science university gan ann arbor mi 
fudenberg levine theory learning games mit press 
bowling veloso rational convergent learning stochastic games proceedings seventeenth international joint conference artificial intelligence seattle wa appear 
bowling veloso variable learning rate convergence gradient dynamics proceedings eighteenth international conference machine learning williams college appear 
kuhn ed classics game theory princeton university press 

