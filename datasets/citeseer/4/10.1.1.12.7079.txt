ecient learning equilibrium ronen brafman moshe tennenholtz computer science department industrial eng 
management ben gurion university technion beer israel haifa israel mail brafman cs bgu ac il mail technion ac il march introduce ecient learning equilibrium ele normative approach learning non cooperative settings 
ele learning algorithms required equilibrium 
addition learning algorithms arrive desired value polynomial time deviation prescribed ele irrational polynomial time 
prove existence ele desired value expected payo nash equilibrium pareto ele objective maximization social surplus repeated games perfect monitoring 
show ele exist imperfect monitoring case 
discuss extension results general sum stochastic games 
reinforcement learning context multi agent interaction attracted attention researchers cognitive psychology experimental economics machine learning arti cial intelligence related elds quite time 
uses repeated games stochastic games models interactions 
literature learning games game theory mainly concerned understanding learning procedures adopted di erent agents converge equilibrium corresponding game 
game may known idea show simple dynamics lead rational behavior prescribed nash equilibrium 
learning algorithms required satisfy rationality requirement converge adopted agents equilibrium 
facing uncertainty game played game theorists adopt bayesian approach 
typical assumption approach exists probability distribution possible games common knowledge 
notion equilibrium extended context games incomplete information treated appropriate solution concept 
context agents assumed rational agents adopting corresponding bayes nash equilibrium learning issue 
major claim game theoretic approach line goals multi agent reinforcement learning research ai modi ed 
bayesian approach model partial information line common approach theoretical computer science computational learning dealing uncertainty 
second descriptive motivation underlying learning research game theory di ers considerably normative motivation learning research ai di erences important rami cations 
explain issues detail 
consider bayesian model partial information 
date machine learning particular single agent reinforcement learning taken di erent approach motivated largely online algorithms computer science 
distribution assumed uncertain entities goal approach behavior agent complete information closely quickly 
ai researchers adopted non bayesian approach learning games looking algorithms converge appropriate equilibrium game class relevant games follow suit 
researches multi agent reinforcement learning choose adopt assumptions game theorists despite fact di erences fundamental 
learning games started descriptive motivation mind 
goal show people simple heuristic rules updating behavior multi agent setting game eventually adopt behavior corresponds appropriate equilibrium behavior 
case economic models equilibria concepts sense justi ed 
assumption agents learning rule justi ed fact agents involved people designed similarly 
ai concerned descriptive models human behavior interested designing arti cial agents 
case cooperative systems reason believe agents designed di erent designers employ learning algorithms 
view designer choice learning algorithm agent fundamental decision follow normative criteria 
ai perspective choice learning algorithm basic action take game play agent designers 
related point 
game theorists adopting descriptive stance concerned quickly learning rule leads convergence ages evolve behavior 
agent designer wants agent learn quickly 
care agent springs 
ai speed convergence paramount importance 
better align research methodology multiagent reinforcement learning ai perspective non bayesian normative approach learning games 
approach assumptions distribution possible games may played making re ective setting studied machine learning ai spirit line algorithms computer science treats choice learning algorithm game 
speci cally adopt framework repeated games view learning algorithm strategy agent repeated game 
strategy takes action stage previous observations initially information identity game played 
natural requirements learning algorithms provided agents 
individual rationality learning algorithms equilibrium 
irrational agent deviate learning algorithm long agents stick algorithms regardless actual game 
eciency deviation learning algorithm single agent stick algorithms irrational lead situation payo improved polynomially stages 
agents stick prescribed learning algorithms expected payo obtained agent polynomial number steps close value obtained nash equilibrium agents known game outset 
tuple learning algorithms satisfying properties class games said ecient learning equilibrium ele 
notice learning algorithms satisfy desired properties game class despite fact actual game played initially unknown 
assumptions typical machine learning 
borrow game theory literature criterion rational behavior multi agent systems 
take individual rationality associated notion equilibrium 
take equilibrium actual initially unknown game benchmark success wish obtain corresponding value initially know game played 
idea constitutes major conceptual contribution 
remaining sections formalize notion ecient learning equilibrium show devoid content prove existence ele general class games class repeated games perfect monitoring 
show classes games ele exist 
generalize results context pareto ele wish obtain maximal social surplus 
discuss extension results general sum stochastic games 
technically speaking results prove rely novel combination called folk theorems economics novel ecient algorithm punishment games initially unknown 
ecient learning equilibrium de nition section develop de nition ecient learning equilibrium context player repeated games 
generalization player repeated games immediate requires additional notation 
extension stochastic games appears section 
game model multi agent interaction 
game set players chooses action perform set actions 
result players combined choices outcome obtained described numerically form payo vector vector values players 
common description player game bi matrix 
called game strategic form 
rows matrix correspond player actions columns correspond player actions 
entry row column game matrix contains rewards obtained players player plays th action player plays th action 
simplifying assumption size action set players identical 
extension sets di erent sizes trivial 
repeated game rg players play game repeatedly 
view repeated game respect game consisting nite number iterations players select action game playing iteration players receive appropriate payo dictated game matrix move iteration 
ease exposition normalize players payo game non negative reals positive constant max denote interval possible payo max 
perfect monitoring setting set possible histories length set possible histories union sets possible histories empty history 
history time consists history actions carried far corresponding payo obtained players 
perfect monitoring setting player observe actions selected payo obtained past know game matrix start 
imperfect monitoring setup player observe performance action payo obtained action selected player 
player observe player payo constrained setting strict imperfect monitoring player observe action payo 
de nitions possible histories agent imperfect monitoring settings follow naturally 
rg policy player mapping set possible histories set possible probability distributions policy determines probability choosing particular action possible history 
notice learning algorithm viewed instance policy 
de ne value player policy pro le policy player policy player expected average reward criterion follows rg natural number denote expected iterations undiscounted average reward player players follow policy pro le 
de nition player similar 
de ne lim inf 
policy pro le learning equilibrium game matrix de ned set actions possible payo 
rst requirement learning algorithms treated strategies 
order individually rational best response 
addition rapidly obtain desired value 
identity desired value may parameter 
take natural candidate nash equilibrium game 
appealing alternative discussed 
assume consider games actions fa repeated game nash equilibrium shot game associated denote nv expected payo obtained agent equilibrium 
policy pro le ecient learning equilibrium ele exists polynomial game matrix corresponding rg nv nash equilibrium player deviates iteration probability failure similarly player 
notice deviation considered irrational increase expected payo 
spirit equilibrium game theory done order cover case expected payo nash equilibrium equals probabilistic maximin value 
cases de nition replaced requires deviation lead decreased value obtaining similar results 
chosen order remain consistent game theoretic literature equilibrium stochastic contexts 
notice deviation considered irrational detrimental ect deviating player average reward manifest near exponentially far 
captures insight normative approach learning non cooperative setting 
assume initially game unknown agents learning algorithms rapidly lead values players obtained nash equilibrium known game 
mentioned earlier learning algorithms equilibrium 
probabilistic maximin value player de ned max min range set policies players respectively 
de nition player similar 
ecient learning equilibrium existence de nition ele lesser interest provide interesting examples ele instances 
section prove constructive result theorem exists ele perfect monitoring setting 
describe concrete algorithm property 
said earlier combination called folk theorems economics novel ecient punishment mechanism ensures eciency approach 
folk theorems see extended discussion basic idea strategy pro le leads payo greater equal security level probabilistic maximin values agents guarantee obtained directing agents prescribed strategies telling agent punish agent turns deviate behavior punishment remains threat followed equilibrium result desired strategy pro le executed 
order idea setting need technique punishing initially knowing payo matrix need devise ecient punishment procedure setting 
recall consider repeated game iteration played 
follows term agent denote player algorithm question term adversary denote player 
players set fa possible actions 
consider algorithm termed ele algorithm 
ele algorithm player performs action time times parallel player performs sequence actions times 
players behave nash equilibrium revealed game computed players behave corresponding strategies point 
nash equilibria exist selected shared selection algorithm 
players refer adversary deviates player refer agent acts follows agent replaces payo complements max adversary payo agent treat game constant sum game aim minimize adversary payo notice payo unknown 
refer modi ed game describe agent go minimizing adversary payo initialize construct model repeated game game replaced game entries game matrix assigned rewards max 
addition associate boolean valued variable joint action 
variable initialized value assumed 
repeat compute act compute optimal probabilistic maximin execute 
observe update joint action follows action agent performed adversary action 
performed rst time update reward associated observed mark known 
claim ele algorithm adopted players ele 
proof theorem non trivial rests showing agent ability punish adversary quickly 
details appendix 
imperfect monitoring ele algorithm previous section uses agent ability view adversary actions payo natural question ability required existence ele 
section show general perfect monitoring required special classes games ele exists imperfect monitoring 
start general case theorem ele exist imperfect monitoring setting 
proof order see consider games 

payo obtained joint action identical player di erent player 
equilibrium players play second action leading 
equilibrium players play rst action leading 
unique equilibria obtained removal strictly dominated strategies assume ele exists look corresponding policies players equilibrium 
notice order ele visit entry times game visit entry times game player resp 
player obtain high value resp 
payo resp 
lower 
rational player deviate pretend game behave suggested equilibrium policy tells case 
game player tell di erence player able lead playing second action players times game increasing payo contradicting ele 
approach bayesian exclude possibility agent knows participating game particular class 
may classes repeated games ele exists 
particular consider class repeated common interest games 
repeated games underlying game common interest game game players receive identical payo setting de nition imperfect perfect monitoring denote setting player knows payo knows adversary payo 
examine case strict imperfect monitoring recall setting player knows action payo theorem exists ele class common interest games strict imperfect monitoring 
proof idea quite simple surprisingly proposed complex ecient approaches proposed 
require knowledge number actions available agent polynomial bound 
algorithm works follows agents go series random play 
suciently times ensure probability joint actions played greater phase agent maintains information best payo obtained far action payo rst obtained 
exploration phase agent plays best action repeatedly 
learning equilibria average reward learning strategy leads maximal average reward agent 
agent motivation deviate 
ele polynomial number steps required attain average reward see deviation immediately reduce average reward agent 
need polynomial number steps approximately obtain maximal average reward need 
log steps random play ensure joint actions played probability follows 
large get probability trials agents play previously joint action approximated get log probability learn outcome associated new joint action approximated repeating process times get desired result 
pareto ele previous sections dealt ele perfect imperfect monitoring settings 
cases interested having learning procedure enable agents obtain expected payo ones obtained nash equilibrium known game 
ambitious objective 
denote payo player game question player plays player plays say pair actions economically ecient max 
total payo agents maximized 
easy see agents choose action general way guarantee agents behave economically ecient manner 
due fact may case economically ecient behavior performing resp 
agent resp 
irrational resp 
may lower probabilistic maximin value agent resp 
guarantee 
classical approach economics dealing economic eciency introducing side monetary payments 
formally part strategy agent function agent instructed pay certain amount money agent part strategy 
agent reward paid positive negative zero utility assumed type utility function termed quasi linear 
sum agents monetary payments result agents turn strategies maximize economically ecient 
de ne notion pareto ele 
pareto ele similar nash ele aim agents behavior economically ecient 
distinctive aspects pareto ele 
require agents able get close ecient outcome 

allow side payments part agents behavior 
de nition perfect monitoring case 
de nition imperfect monitoring case similar 
suppose considering games actions 
repeated game economically ecient joint action shot game associated denote pv payo obtained agent joint action 
policy pro le allows side payments pareto ecient learning equilibrium exists polynomial game matrix de ned actions corresponding rg pv pv player deviates iteration probability failure similarly player 
theorem exists pareto ele perfect monitoring setting 
proof sketch consider algorithm de nes policies side payments agents 
player performs iterations action parallel player performs sequence actions times 
game known agents compute probabilistic maximin values agent agent 
denote probabilistic maximin value agent payo gets economically ecient solution loss generality choose 
player paid player ecient solution played player total payo high probabilistic maximin 
easy see examining cases 
agents adopt ecient behavior side payments iterations 
economically ecient behaviors exist predetermined selection algorithm 
case players adversary deviates exploration stage state player agent punish case nash ele 
play payo game complements max adversary payo proof follows steps proof existence ele 
case imperfect monitoring result respect nash ele hold 
theorem pareto ele exist imperfect monitoring setting 
stochastic games stochastic games provide general model repeated multi agent interactions 
stochastic game players may nitely states fs state associated game strategic form 
joint action state determines payo determine stochastically identity state agents reach 
formally fa set actions available agents 
state game associated associates payo agent joint action 
addition probability state joint action denoted 
multiple games policy agent associates possibly mixed action state potentially payment agent 
policy function history states agent visited payo observed 
section assume perfect monitoring setting impossibility result imperfect monitoring repeated games immediately rules existence ele general context stochastic games 
stochastic games provide realistic technically challenging setting 
try understand issues involved 
rst obstacle face lack general results existence nash equilibrium average reward stochastic games 
restrict attention case pareto ele 
conceptually required generalization straightforward learning algorithm quickly lead economically ecient policy agents policy maximizes average sum rewards deviations quickly lead lower reward 
case repeated games equated quick polynomial size game approximation parameters situation stochastic games complicated 
parameter typically assess speed convergence learning algorithm stochastic games return mixing time 
intuitively return mixing time policy expected time take agent uses policy converge value close value policy 
ideally learning algorithm attain optimal value time polynomial return mixing time optimal policy 
formally assume xed stochastic game policy pro le denote step average reward policy pro le agent starting state de ne 
denote return mixing time minimal states 
policy pro le executed steps longer agents expected average sum rewards close long term average sum rewards 
policy pro le maximizes min 
mix return mixing time 
de nition pareto ecient learning equilibrium stochastic games identical repeated games polynomial mix 
note game irreducible xed policy pro le induced markov chain ergodic depend show theorem assumptions pareto ele stochastic games exists agents perfect monitoring mix known 
proof intuitive idea algorithm identical case repeated games elaborate new issues 
agents run algorithm nding policy pro le maximize 
run algorithm nding best accomplish assuming agent trying minimize average payo 
point run policy pro le adjusted appropriate side payments agent receives best accomplish case repeated games 
point agent deviates agent plays goal minimize agent average reward 
note learning algorithm pareto optimal 
long term average sum rewards algorithm close optimal average sum rewards desired 
agent incentive deviate stage side payment structure guarantees attain value attain 
show algorithm pareto ele need show value attained eciently punishment performed eciently 
resorting results ecient learning xed sum stochastic games common interest stochastic games 
compute policies maximizes 
algorithm described 
refer reader details 
need note algorithm learns required policy pro le polynomial time 
compute values agent attain max :10.1.1.159.617
max appropriate learning xed sum game 
xed sum game rewards agent rewards respect xed sum game rewards agent rewards 
note value input max learn step policy time polynomial game parameters 
policy optimal policies mix time shall take mix average reward policy compute side payments structure case repeated games 
case average reward policy pro le suitably modi ed include side payments lower value agent receive 
agent deviate know mix steps attain lower average reward punishment carried eciently 
note standard imperfect technique removing knowledge mix simply guess progressively higher values mix refer reader implications approach :10.1.1.159.617
discussion previous learning games ts paradigms 
study learning rules lead nash equilibrium solution concept game 

study learning rules predict human behavior non cooperative interactions ones modeled repeated games 
approach taken signi cant merit descriptive purposes normative approach learning go recommending behavior eventually lead desired solution 
major issues needs face 
learning algorithms agents individually rational 

learning algorithms eciently converge desired values employed agents 

deviation desired learning algorithm irrational short period time 
concepts introduced address issues 
ele pareto ele provide new basic tools learning non cooperative settings 
able show constructive existence results ele pareto ele context repeated games perfect monitoring 
able show relax perfect monitoring assumption desired properties impossible obtain general case 
pareto ele appealing concept context stochastic games able extend results context 
concepts results provide rigorous normative approach learning general non cooperative interactions 
useful contrast approach important line related features algorithms guarantee agent attain value approximately equal value attained known advance adversary play 
algorithms line appear special attention issue eciency 
result truly spirit line algorithms goal online able line 
case attempt react online adversary behavior manner similar terms average payo best done known adversary behavior hand 
results highly valuable readers may notice subtle crucial point treat adversary policy xed sequence mixed strategies probabilistic actions contrary spirit game theory 
reality adversary adjust policy response agent behavior 
imagine example instance known prisoner dilemma game consider adversary policies agent initially plays row denoted cooperate adversary play column denoted cooperate 
agent initially plays row denoted defect adversary play column denoted defect 
agent initially plays cooperate adversary play defect agent initially plays defect adversary play cooperate 
clear agent guarantee best response value adversary approach limited view adversary pre determined sequence mixed strategies 
bottom line despite practical theoretical importance results replace concepts notion equilibrium 
related normative guidelines design learning algorithms 
bowling veloso suggest criteria learning algorithms 
rst call rationality stipulates player policies converge stationary policy learning algorithm converge best response policy 
second call convergence stipulates learner necessarily converge stationary policy 
criteria attractive notion nash equilibrium learning strategies deeper notion rationality best response convergence 
convergence de nitely desirable ignores issue convergence rate 
convergence speci ed de ned 
bowling veloso consider special de ned case convergence self play agents algorithm 
standard notion convergence adopted learning games uses 
fact particular context self play investigated bowling veloso requirements equivalent requirement algorithm converge correlated equilibrium common property pursued learning algorithms game theory literature 
concept ele provides rigorous notion individually rational learning strategies 
believe ecient polynomial time convergence rate integral part de nition rationality 
settings happens exponential number iterations great interest 
applies judgment 
agent irrational choice leads increased reward near decreased reward exponential number steps irrational 
bowling veloso 
rational learning stochastic games 
proc 
th ijcai pages 
brafman tennenholtz 
max general polynomial time algorithm near optimal reinforcement learning 
ijcai 
brafman tennenholtz :10.1.1.159.617
max general polynomial time algorithm near optimal reinforcement learning 
journal machine learning research 
brafman tennenholtz 
learning coordinate eciently model approach 

claus boutilier 
dynamics reinforcement learning cooperative multiagent systems 
proc 
workshop multi agent learning pages 
erev roth 
predicting people play games reinforcement learning games unique strategy equilibrium 
american economic review 
yoav freund robert schapire 
adaptive game playing multiplicative weights 
games economic behavior 
fudenberg levine 
theory learning games 
mit press 
fudenberg 
folk theorem repeated games discounting incomplete information 
econometrica 
fudenberg tirole 
game theory 
mit press 
hart mas colell 
reinforcement procedure leading correlated equilibrium 
debreu editors economic essays festschrift werner pages 
springer 
hu wellman 
multi agent reinforcement learning theoretical framework algorithms 
proc 
th icml 
kaelbling littman moore 
reinforcement learning survey 
journal ai research 
kearns singh 
near optimal reinforcement learning polynomial time 
int 
conf 
machine learning 
littman 
markov games framework multi agent reinforcement learning 
proc 
th icml pages 
shapley 
stochastic games 
proc 
nat 
acad 

usa volume pages 
ele existence proof claim ele algorithm adopted players ele 
proof refer algorithm agent playing adversary order guarantee adversary obtain maximin value repeated max algorithm version max algorithm :10.1.1.159.617
proof repeated max leads near optimal value takes steps 
game matrix contains entries 
number iterations agent learns new information update model means agent needs compute strategy times 
computation probabilistic maximin strategy requires polynomial time carried linear programming 
shall show small number phases agent learns new information expected payo high value probabilistic maximin strategy 
average payo high 
actual number iterations agent learns new information may smaller depends adversary plays 
adversary attempt prevent agent learning various aspects game 
agent guaranteed obtain payo precisely iteration agent learn new information expected payo payo probabilistic maximin strategy model high probabilistic maximin value real game 
course discussion deals expected values 
wish guarantee actual value probability proceed follows lemma assume expected payo strategy 
probability average payo iterations rmax bounded proof payo iteration rmax notice jy 
cherno bound implies rob implies average return iterations rmax lower probability follows choose rmax 
holds max max ln 
note polynomial max precise value determined polynomial max required 
assume adversary uses xed arbitrary strategy denote value probabilistic maximin strategy agent 
step show lemma agent current model game real game 
probabilistic maximin policy respect agent plays iterations probability agent receive payo rmax learn value new entry 
proof denote agent actual average payo iterations 
rmax done 
assume rmax shall show probability agent learn new entry game matrix 
mg denote probabilistic maximin value game de nition mg lower bound expected payo expected value respect 
know jr mj rmax probability 
assume rmax follows rmax payo high corresponding values means mg de ne new game payo di erence corresponding payo de nition non negative 
strictly positive exactly entries marked unknown 
expected value exactly di erence expected value know expected value mg expected value rmax standard cherno bound analysis lemma follows iteration follows actual average value positive probability 
happen unknown entry played 
nish proof showing agent punish eciently choose zk zr max rmax satis ed rmax polynomial max polynomial max de ne claim probability average payo obtained agent lower probabilistic maximin value game 
know probability iterations average payo rmax iteration average value game positive learning occur 
know average payo remaining iterations max de nition average value desired 
note agent need aware max simply plays algorithm 
conclude existence proof notice iterations agents explore unknown game matrix 
player deviates exploration stage lemma guarantees deviation irrational factor desired probability success 
agents follow exploration stage deviation selecting appropriate strategy determined nash equilibrium irrational de nition nash equilibrium 

