pattern discovery sequences markov assumption information computer science university california irvine ca usa ics uci edu investigate general problem discovering recurrent patterns embedded categorical sequences 
important real world problem nature motif discovery dna sequences 
investigate fundamental aspects data mining problem discovery easy hard general framework characterizing learning context deriving bayes error rate problem markov assumption 
bayes error framework demonstrates certain patterns harder discover 
explains role different parameters pattern length pattern frequency sequential discovery 
demonstrate bayes error calibrate existing discovery algorithms providing lower bound achievable performance 
discuss number fundamental issues characterize sequential pattern discovery context variety empirical results complement verify theoretical analysis apply methodology real world motif discovery problems computational biology 

data sets form categorical sequences defined finite alphabet symbols frequently occur variety real world applications 
examples applications include computational biology dna rna protein sequences telecommunication networks alarm message sequences user modeling sequences web page requests 
important data mining problem context unsupervised discovery recurrent patterns sequences detection discovery relatively short relatively rare possibly noisy repeated substrings data 
problem difficult relatively little known priori patterns may look typically combinatorially large number possible patterns 
permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
sigkdd edmonton alberta canada copyright acm 
padhraic smyth information computer science university california irvine ca usa smyth ics uci edu simplest models patterns noisy pattern substrings fixed length position dominant symbol noise symbols substituted part noise process 
example consider letter alphabet pattern length designated symbol locations may substituted symbols noise probability 
consider case multiple noisy copies pattern embedded background process uniformly distributed independent occurrences symbols dac 
actual occurrences patterns sequence enclosed brackets 
note second occurrence pattern slightly noisy version basic pattern symbol substituted second position 
note locations sequence true pattern partial match background subsequence starts symbols sequence 
long sequence may spurious background matches leading false detections making detection discovery quite difficult 
note different levels problems easiest hardest 
pattern detection sequence known model patterns detection algorithm 

parameter estimation parameters pattern model known functional form pattern markov model known locations patterns 
effect supervised learning 
pattern discovery sequence set sequences locations parameters pattern unknown known form patterns assumed 
unsupervised learning problem problem primary interest 
focus characterizing general nature class sequential pattern discovery problems theoretically empirically 
particular interested determining problem hard learning viewpoint 
effect pattern discovery alphabet size 
sequence length 
pattern frequency 
long tradition statistical pattern recognition machine learning providing mathematical bounds difficulty learning problems function fundamental problem characteristics 
known examples approach multivariate classification problems include bayes error rate lower bound average error rate possible classifier set features chow duda hart mclachlan chapter particular ripley risk minimization framework upper bounding test error rates vapnik 
prior bayes error rate led fundamental important insights nature classification multivariate feature spaces 
particular bayes error rate provides theoretical target terms lowest achievable average error rate performance classifier problem 
bayes error rate computed exactly know true conditional densities distributions class probabilities classification problem assume classes multivariate gaussian 
practical problems course true distributions known theoretical results provide fundamental insight nature multivariate classification problems quantify role problem dimensionality class separation forth see chapter duda hart quantification dimensionality affects bayes error rate gaussian model 
apply general bayes error rate framework sequential pattern discovery problems 
prior bayes error rate need assume data generated specific type model order compute bayes error 
particular order markov framework base model 
analogy role multivariate gaussian models classification order markov model viewed restrictive scope sequential data gaussians multivariate case 
markov model useful baseline framework characterizing pattern discovery problem focusing particular class markov patterns modeled hidden markov model certain constraints 
important understand nature learning baseline model understand detection discovery complex pattern structures 
example learning hard case correct functional form model assumed known reasonable infer real world problems may able assume knowledge correct form pattern harder 
sequential pattern discovery applications computational biology single application general class problems received attention prior 
certainly important motivator describe 
focus understanding general class pattern discovery problems sequences view understanding fundamental nature challenging unsupervised learning problem 
primary novel contributions follows provide approximate expression bayes error rate pattern discovery markov assumption experimentally demonstrate resulting expression closely matches true bayes error rate 
knowledge previous bayes error rate sequential pattern discovery detection problems 
illustrate different factors alphabet size pattern length pattern frequency pattern autocorrelation directly affect bayes error rate turn increase decrease difficulty pattern discovery problem 
empirically investigate known algorithms pattern discovery markov context demonstrate far away bayes error rate practice significant finding algorithms relatively far away optimal bayes performance large training data sets available 
apply ideas motif finding problems computational biology demonstrate theoretical framework bayes error rate shed light reliability automated motif discovery algorithms real data 

markov model sequential patterns notation denotes size observable alphabet number unique symbols occur sequence 
denotes length pattern case fixed length patterns 
consensus pattern string symbols ith symbol symbol appear position pattern denotes probability substitution error pattern positions 
probability consensus symbol appearing position 
ns denotes expected number substitutions pattern computed 
denotes frequency pattern occurrence sequences expected number patterns sequence length model patterns form fixed length plus noise consensus pattern fixed length allowed probability 
assumption pattern structure quite simple proven useful model motif discovery computational biology see liu 
pevzner sze buhler tompa benchmark problems detailed discussions 
motifs thought relatively short highly conserved regions dna sequence 
dna letter alphabet typical motifs range base pairs symbols long 
motif discovery may prior knowledge number example hmm state transitions pattern instantiation uniform background sequence 
motifs sequence exact expected lengths typically little knowledge motifs occur sequence symbols contain 
model embedding patterns background sequence hidden markov models hmms hidden state position pattern states patterns length background state model background 
think hmm generative process method simulating sequences generating patterns length particular consensus pattern noise level frequency specifically assume markov process states consisting background state pattern states pl inthe simplest case fixed length patterns assume strictly linear state transition matrix background state transition pattern state pattern state pi transition state pi pl transition back background state viewed similar product multinomial model block motifs suggested liu 

assumption occurrences pattern differ due substitution errors 
relax assumption allow insertion deletion states 
model generate markov state sequence states 
state sequence generated manner directly observed hidden nature markov model 
hj denote value hidden state variable position hj 
pl 
generative model produces observed symbol hidden state value hj sequence 
symbols produced multinomial distribution 
background state multinomial distribution symbols corresponds frequency symbol appears background uniform distribution default 
pattern states pi multinomial probability consensus symbol position non consensus symbols 
pattern state output multinomial distribution typically tuned specific symbol position 
shows model generating patterns length consensus pattern state hmm 
background state characterized high entropy distribution background frequencies symbols sequences 
emissions pattern states low entropy probability mass concentrated consensus symbol corresponding position pat tern 
transition probability background pattern governs frequency patterns observed 
generally patterns assumed generated hidden stochastic finite state machine pattern states arbitrary transition probabilities exit transitions background state allowing generation variable length patterns 
alternatively include specialized insertion deletion states usually done computational biology multiple sequence alignment see example baldi 
eddy 
consider pattern discovery problem hmm assumption assumed length pattern known number states hmm case pattern discovery problem reduced learning parameters correct hmm problem 
exist known techniques fitting models expectationmaximization em procedure see real world problems complex learning right model difficult 
follows characterize mathematical terms easy hard pattern discovery tasks hmm context deriving bayes error rate problems nature 

bayes error rate motivation difficulty learning particular pattern characterized multiple dimensions 
example affected size observable alphabet length pattern variability pattern characterized substitution probability pattern occurrence similarity pattern background distributions amount available training data 
characterizing learnability dimensions look single characteristic bayes error rate fundamentally quantifies difficulty detecting pattern 
denote sequence observed symbols oj jth elements sequence 
think pattern detection problem class classification problem wish classify location sequence coming background class pattern class hj denotes background hj pl denotes disjunction pattern states 
bayes error rate known concept classification minimum error rate problem achieved true model optimal bayes decision class variable observed feature vector 
state sequence memoryless independent draws distribution state values markov standard non sequential classification problem bayes error rate defined min state value sum different values symbol take alphabet 
definition intuitive sense value optimal bayes decision choose maximum probability value probability making error probability minimum probability event occurred averaged different possible values categorical data bayes error function close pattern multinomial distribution symbols background multinomial distribution 
re introduce markov dependence optimal decision class location consider observation oj sequence observed values adopt directly analogous definition bayes error rate defined requires sum infinite number possible observation sequences introduces number technical issues steady state probabilities direct interest 
reason operational definition bayes error rate practice asymptotically equivalent theoretical cousin easier understand directly estimated empirically lim min hi hi intuitively symbol bayes error rate corresponds fraction symbols infinitely long realization hidden markov process misclassified optimal bayes decision rule 
optimal bayes decision rule location sequence calculation hi andp hi computed time linear length sequence known forward backward algorithm hmms rabiner 
intuitively classification mistakes occur background symbol looks similar pattern state background state context vice versa 
bayes error rate principle indicates difficult perfect model knowledge case isolate occurrences pattern sea background characterizes difficulty original unsupervised learning pattern discovery problem 
analytical expressions bayes error rate particular hmm structure estimate bayes error rate ways 
seek closed form expression defined equation empirically estimate long sequences true states known counting average number classification errors forward backward algorithm 
disadvantage empirical approach reveal functional form dependence bayes error parameters model 
useful pursue closed form analytical expressions little prior deriving closed form expressions markov assumption 
related aware chu lee provide bounds bayes error markov context sub optimal decision rules 
deriving expression bayes error rate function hidden markov model parameters appears difficult impossible general case 
special case linear transition structure hmm assumed certain simplifying assumptions derive approximate expression bayes optimal decision classifies symbol background pattern class depends observed sequence induces dependence decisions respect symbol 
useful approximation ignore dependence consider simplified iid problem observed sequence 
classify position independently pattern position subsequent symbols oi 
oi 
problem simpler reduction context observable sequence adjacent characters possible position 
simplification lawrence 
liu 

practice motif applications example posterior class probabilities implied iid assumption close ones obtained hmm ing iid bayes error rate serve tight upper bound true error rate iid tions patterns periodic internal structure consider example pattern 
cases optimal decision depends symbols outside context study effect detail section 
possible evaluate iid exactly closed form pattern 
derivations corresponding expressions complex refer reader smyth details 
simplify analysis obtain close approximations iid ignoring fact state sequence generates particular substring length contain background pattern states 
assume entire substring generated run pure background sequence pattern states 
call model iid pure associated error rate iid see assumption pure state sequences leads close approximation iid long marginal probability pattern states small 
omit derivation details due space limitations see smyth details state iid pure assumption bayes error rate iid iid min look various interpretation limiting cases expression section 
general bayes error vary zero probability minority class purposes practice useful bring problems different pattern frequencies pattern lengths scale 
done considering normalized bayes error rate fraction patterns misclassified fraction symbols 
naturally normalized bayes error varies value corresponds decision rule classifying symbol background 
experiments shown equation approximates closely true variety problems inter normalized error rate iid pure error rate iid error rate empirical error rate expected number substitution errors analytical empirical estimates normalized probability error symbol substitution probability varies 
normalized error rate log pattern frequency iid pure error rate iid error rate empirical error rate analytical empirical estimates normalized probability error pattern frequency varies ns 
est 
shows empirical analytical estimates normalized probability error change vary expected number substitution errors 
dots correspond empirical evaluations bayes error sequences length dotted solid lines plotted correspond analytical approximation iid iid pure assumptions respectively 
shows normalized bayes error vary pattern frequency pattern length substitution probability fixed 
note empirical estimates noisy probability patterns approaches zero accurate empirical evaluation bayes error expensive computationally due lengths sequences required get accurate results 
solid line corresponds analytical approximation correctly captures non linearity dependence 
switching seen plots occurs substrings substitution relative consensus pattern recognized patterns 
see iid pure approximation dotted line starts deviate empirical results marginal pattern bayes error rate errors errors errors pattern length normalized bayes error rate sets problems set characterized exactly expected percentage substitution errors 
probability increases consistent theory expect approximation break 
heuristic measures previously computational biology characterize difficulty particular motif discovery problem 
example sze gelfand pevzner expected percent errors pattern 
cases metric fails differentiate easy hard problems 
example consider set problems defined alphabet having substitution probability pattern frequency 
suppose problems set different pattern lengths expected percentage errors problems set constant 
illustrate sets lines corresponds different set normalized vary metric sze gelfand remains 
equation provides principled way combining different parameters problems single summary characteristic problem difficulty level 
insights provided analytical expression analytical expression equation shows functional dependence bayes error rate parameters model allows gain insight behavior bayes error vary parameters problem 
section realistic example biological domain illustrate qualitatively difficulty pattern discovery problem 
suppose trying discover patterns motifs length dna sequence alphabet size patterns appear probability 
assume needed average number substitutions pattern 
equation allows address questions directly error associated optimal recognition procedure probability substitutions symbol noise pattern goes zero 
bayes error approaches min plugging specific values hypothetical problem optimal detection algorithm incorrectly misclassify order pattern symbols substitutions pattern model 
naturally allowing substitutions increase error rate 
pattern length pattern frequency substitution probability optimal procedure misses patterns classifies background 
patterns recognized background hypothetical problem corresponds substitution probability 
equivalently average number substitutions greater ns optimal procedure patterns classify background 
particular value pattern frequency optimal procedure misses patterns classifies background 
occurrences pattern classified background example pattern frequency optimal procedure misses patterns classifies background 
fix pattern frequency pattern length probability substitution find maximum number substitutions substrings substitutions recognized patterns classified background 
number expression ln ln ln ln hypothetical example occurrences consensus pattern single substitution error classified background 
expected false positive false negative rate optimal procedure 
quantity computed calculations 
toy problem normalized bayes error rate equal 
approximately errors due false negatives due false positives optimal detection procedure going overestimate substantially number patterns true model known 
dominance false positives bayes error expression typical problems provides theoretical explanation observed empirical fact pattern detection algorithms biological sequences tend systematically suffer high false positive rates see 
normalized error rate insertions insertions log probability insertion normalized probability error function probability insertion ns 
extend model handle arbitrary insertions deletions consecutive pattern positions introduce additional insertion states deletion states original pattern states 
standard way modeling insertions deletions hmm models model biological sequences baldi 
experiment fixed parameters model empirically evaluated bayes error rate problems probability insertions ranging 
horizontal line bottom plot indicates probability error model insertions 
experiment related experiments shown introducing insertions increase bayes error problem significantly 
analysis modified handle risk functions loss implicitly counting classification errors 
addition represent presence multiple consensus patterns potentially different length frequency embedded common background extend analysis bayes error handle multiple patterns 
simplest case patterns distinct confused background 
denote model contains single pattern pi frequency fi mi model contains frequencies bayes error satisfies ol si number distinct strings length recognized pattern pi model mi small number compared number strings length andp ol probability random produced background 
see smyth general cases exact derivations 

effect pattern structure analytical analysis bayes error rate suggests iid assumption hold true patterns periodic structure example patterns 
may counter intuitive periodic patterns posterior posterior posterior posterior posterior probability pattern state estimated hmm model ns 
axis represents position sequence 
higher bayes error rate order magnitude 
quantify exactly type pattern structure violates iid assumption leads higher bayes error notion pattern autocorrelation vector 
autocorrelation vector binary vector length pattern th position equal th prefix pattern coincides th suffix 
autocorrelations studied extensively literature past years see regnier szpankowski pevzner 
demonstrate bayes error increases periodic patterns table set simulated problems 
pattern structure varied random patterns zero autocorrelation vector patterns period autocorrelation vector positions 
parameters symbols comprising pattern problem pattern length frequency substitution probability 
table see expected error example normalized bayes error rate random pattern increases pattern highest autocorrelation 
substitution noise added higher number expected substitution errors noise effect starts dominate bayes error difference random periodic patterns smaller bayes error rates higher case 
general detection structured patterns markov context presents difficult learning problem detection random patterns 
findings provide direct explanation results observed van 
patterns clear periodic structure practical complications 
intuitively unlabeled data true boundaries periodic patterns harder determine precisely learning difficult 
characteristic behavior periodic patterns visualized plots posterior pattern probability sequence 
illustrates posterior probability pattern state changes neighborhood true known boundary pattern marked vertical line different patterns note long runs back ground cut plot 
hmm algorithm learning parameters locations patterns 
difference random structured patterns quite dramatic 
algorithm discover boundaries random pattern better non random ones corresponding bayes error rates quite different 
fortunately patterns significant autocorrelation structure relatively rare motif discovery randomly generated patterns 
useful aware potential problems associated discovering patterns 

pattern discovery algorithms section bayes error framework help better understand characteristics specific learning algorithms data sets 
analyze performance number known motif discovery algorithms set simulation motif finding problems similar challenge problems pevzner sze 
bayes error rate provides performance bound algorithm problems analysis demonstrates contribution learning error introduced separate effects knowing positions patterns sequence knowing parameters true data generating model positions known 
widely algorithms motif discovery motif sampler algorithm proposed liu 
meme algorithm bailey elkan 
underlying generative model similar discussed 
background modeled single multinomial distribution pattern represented product multinomial distribution 
algorithms assume simplified iid version problem memberships substring length background pattern treated iid variables 
simplification allows faster inference learning ignoring dependence consecutive letters context fixed length learning pattern discovery carried differently algorithms meme uses em restarts clever heuristic initialization techniques find unknown parameter values motif sampler uses gibbs sampling estimate parameters model bayesian setting 
versions algorithms see liu example include extensions allow handle multiple occurrences multiple patterns higherorder background models experiments publicly available motif sampler code described liu 
implemented version em algorithm quite similar meme algorithm 
refer iid gibbs iid em respectively 
addition comparison include performance hmm model described section trained standard em algorithm hmms referred linear hmm algorithm 
iid em linear hmm em algorithm pattern discovery differ nature underlying model 
iid gibbs uses iid model problem iid em uses gibbs sampling locate patterns learn parameters em 
experiments ran iid em iid gibbs linear hmm algorithms sets table empirical estimates normalized bayes error rate types pattern structure random period period period 
errors random normalized error rate training data size true error rate known locations linear hmm iid em iid gibbs normalized probability error learned models function training set size ns 
simulated problems 
shifting heuristic restarting em algorithm runs iid em hmm avoid partial solutions see liu 
bailey elkan smyth details 
performed experiments data simulated known true hmm models class challenge problems pevzner sze 
performance models measured normalized error rate relatively long sequences unseen data provide stable estimates error rate 
note error rate exceed value estimated models error rate models higher simply assigning symbols background class normalized error rate definition 
studied effect increasing size training data problems different bayes error rates 
length training sequence varied symbols 
training set size estimated model differ original true model due noise actual training patterns ambiguity locating boundaries patterns training data 
results allow explore contribution factors algorithm performance 
figures illustrate results experiments 
point plots obtained averaging different training sets size true model 
plots lowest curve shows normalized bayes error rate problem true model lower bound probability error normalized error rate training data size true error rate known locations linear hmm iid em iid gibbs normalized probability error learned models function training set size ns weak prior pattern frequency 
classifier 
intermediate curve shows performance linear hmm model known true locations patterns supervised learning problem parameter estimation problem 
uppermost curves show normalized error rate different algorithms iid em linear hmm iid gibbs 
plots generated strong prior pattern frequency sense true pattern frequency algorithms 
case learning algorithms roughly performance function training set size 
see different behavior specified correct weak prior pattern frequency weak sense smaller equivalent sample size 
case gibbs strategy significantly outperforms em algorithms 
fact em algorithms greatly overestimate frequency pattern occurrences allowed deviate prior omit plots due space limitations 
attribute effect batch nature updates em algorithm accumulates pattern probability observed sequence making update 
contrast gibbs sampler changes line single occurrence pattern step 
incorrect pattern frequency mismatched data specified prior iid gibbs algorithm better able handle outperforming em algorithms plots shown 
uncertainty pattern frequency iid gibbs appears reliable dis covering patterns 
lower curves worth discussing 
lowest bayes error rate estimated empirically best discovery algorithm possibly problem true model infinite amount training data 
curve performance hmm algorithm told locations patterns estimated empirically standard supervised hmm training algorithm 
difference lower curves effect contribution error rate simply parameter estimation multinomials small sample sizes told patterns get noisy estimates pattern parameters contributes additional error 
call contribution error parameter estimation error real algorithms course discover parameter locations naturally error curve systematically higher known location curve 
fact distance real algorithm curve known location curve considered contribution error coming knowing parameters pattern location error allows decompose total error algorithm additive components basic bayes error additional error due noise parameter estimates difference bayes error known location curves additional error knowing patterns located difference known location curves real algorithm curves 
tells example expect pattern discovery algorithm high accuracy terms pattern detection error terms close zero pattern easily detectable known bayes error close parameter estimation noise low typically implying large amount data prior knowledge algorithm efficiently discover patterns constraints met 
curves unsupervised algorithms quite close different values training set size indicating iid model quite adequate patterns em gibbs learning algorithms equally successful finding pattern locations simulated problems 
practical viewpoint interesting note algorithms require significantly different computation times learning 
iid gibbs fit model seconds hmm take minutes due complex forward backward calculations 

finding real motifs consider application bayes error rate framework actual motif finding problems dna sequences 
evaluation bayes error rate requires knowledge true data generating model possible real world problems 
locations instances pattern known experimental data consider supervised version problem construct corresponding model instances 
reasonable number instances pattern expect error rate approximate bayes error true model 
normalized error rate experimental patterns normalized error rate learning binding sites different coli dna binding proteins 

reported experimentally confirmed instances binding sites different coli dna binding protein families 
constructed map estimates parameters hmm models instances 
dirichlet prior matching letter frequencies estimated sample size regularize emission probabilities 
shows empirically evaluated bayes error rate function number instances pattern instances problem problems total approximately experimentally verified binding sequences 
normalized bayes error varies near independent number training patterns indicating presence significant variability difficulty discovery problems 
note relatively high values bayes error due high pattern ambiguity small alphabet size 
sze gelfand pevzner evaluated performance motif sampler meme consensus see stormo algorithms problems 
performance metrics different classification error correlated 
table see reported errors increase non linearly estimated bayes error rate demonstrating bayes error directly influences learnability motif discovery 
relatively high bayes error rates real motif problems suggests motif discovery sequence information hard problem 
note particular bayes error rate reduced seeking better learning algorithms larger data sets 
bayes error principle reduced provide additional features motif classifier 
suggests profitable direction motif discovery computational biology combine additional information outside sequence gene expression measurements protein structure sequence information 

pattern discovery categorical sequences important challenging data mining problem 
table error metrics motif sampler meme consensus algorithms known motif problems different bayes error rates 
family normalized bayes error rate motif sampler meme consensus pur crp demonstrated bayes error rate framework analyzing problems nature markov context 
particular bayes error provides insight different factors influence learnability certain types patterns 
particular problem motif discovery discovered simulated real data sets bayes error rate quite high 
directions include multivariate extensions design application new algorithms pattern finding 
supported national science foundation iis iis nasa jet propulsion laboratory national institute standards technology lawrence livermore national laboratory uci cancer center microsoft research ibm faculty partnership award 
bailey elkan 
unsupervised learning multiple motifs biopolymers expectation maximization 
machine learning journal pp 

baldi chauvin mcclure 
hidden markov models biological primary sequence information 
proceedings national academy science pp 

buhler tompa 
proceedings fifth annual international conference computational molecular biology recomb montreal canada pp chow 
recognition method neighbor dependence ire trans 
elect 
comput vol 
ec pp 

chu 
error bounds contextual recognition procedure 
ieee trans 
elect 
comput vol 

smyth 
pattern discovery sequences markov assumption technical report ics tr university california irvine 
duda hart 
pattern recognition scene analysis new york ny john wiley 
eddy 
multiple alignment hidden markov models 
intelligent systems molecular biology pp 

van vides 
extracting regulatory sites upstream region yeast genes computational analysis oligonucleotide frequencies 
journal molecular biology pp lee 
bounds approximations error probabilities character recognition 
proceedings international conference cybernetics society pp 

lawrence boguski liu neuwald 

detecting subtle sequence signals gibbs sampling strategy multiple alignment 
science liu brutlag liu 
discovering conserved dna motifs upstream regulatory regions expressed genes 
pac symp 

liu neuwald lawrence 
bayesian models multiple local sequence alignment gibbs sampling strategies 
journal american statistical association pp 

mclachlan 
discriminant analysis statistical pattern recognition new york ny john wiley sons 
pevzner 
computational molecular biology algorithmic approach 
cambridge ma mit press 
pevzner sze 
combinatorial approaches finding subtle signals dna sequences 
proceedings th international conference intelligent systems molecular biology ismb pp 

regnier szpankowski 
approximate pattern occurrences text 
compression complexity sequences pp 

ripley 
pattern recognition neural networks cambridge uk cambridge university press 
mcguire church 
comprehensive library dna binding site matrices proteins applied complete escherichia coli genome 
journal molecular biology 
stormo proc 
natl 
acad 
sci sze gelfand pevzner finding weak motifs dna sequences 
pacific symposium biocomputing pp 
vapnik 
statistical learning theory new york ny john wiley 
