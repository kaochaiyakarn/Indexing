aggregate mixed order markov models statistical language processing consider language models size accuracy ate different order gram models 
types models studied partic ular 
aggregate markov models class bigram models map ping words classes tic 
mixed order markov models combine bigram models predictions con different words 
types models trained expectation maximization em algorithms maxi mum likelihood estimation 
examine smoothing procedures mod els interposed different order grams 
significantly re duce perplexity unseen word combi nations 
purpose statistical language model sign high probabilities word sequences low probabilities ones 
challenge arises combinatorially large number possibilities fraction observed 
general language models learn recognize word sequences functionally similar lexically distinct 
learning problem generalizing sparse data particularly acute large sized vocabularies jelinek mercer roukos 
simplest models natural language gram markov models 
models prob ability word depends words precede 
problems estimating ro bust models form documented 
number parameters transition probabilities scales vocabulary size 
typical models num ber exceeds orders magnitude total number words feasible training corpus 
lawrence saul fernando pereira pereira research att 
com labs research park ave florham park nj transition probabilities gram models estimated counts word combinations training corpus 
maximum likelihood ml esti mation leads zero valued probabilities unseen grams 
practice adjusts smoothes chen goodman ml estimates language model generalize new phrases 
smoothing done ways example introducing artificial counts backing lower order models katz combining models interpolation jelinek mercer 
great deal information lost smoothing procedure 
due great dis gram models different order 
goal investigate models intermediate size accuracy different order gram models 
show models intervene different order grams smoothing procedure 
experimentally find significantly reduces perplexity unseen word combinations 
language models evalu ated arpa north american business news nab corpus 
experiments vo words including tokens punctuation sentence boundaries un known word token standing vocabulary words 
training data consisted approxi mately words sentences test data words half sentences 
sentences drawn randomly replacement nab corpus 
perplexity figures com puted combining sentence probabilities prob ability sentence wow wn yin lp wi wn start sentence markers respectively 
reported confirmed results vary significantly different ran drawn test sets size 
organization follows 
section examine aggregate markov mod els class bigram models brown mapping words classes probabilistic 
describe iterative algo rithm discovering soft word classes expectation maximization em procedure maximum likelihood estimation dempster laird rubin 
features algo rithm attractive large vocabulary language mod eling tuning parameters converges mono log likelihood handles proba constraints natural way 
number classes small large depending constraints modeler 
varying number classes leads models intermediate tween unigram bigram models 
section examine sort inter mediate model arises combinations non adjacent words 
language models combinations proposed huang 
ney essen kneser rosen 
consider specifically skip transition matrices wt wt predictions conditioned kth previous word sentence 
value determines words skips back predic tion 
predictions conditioned sin gle previous word sentence inherently weaker conditioned previous words 
combining predic tions form different values create model intermediate size ac bigram trigram models 
mixed order markov models express predic tions wt wt wt wt convex com bination skip transition matrices wt wt 
derive em algorithm learn mixing elements transition matrices 
number transition probabilities models scales mv opposed mixed order models powerful trigram models stronger predic tions bigram models 
reason quite immediately preceding word pre value earlier words sentence 
section aggregate mixed order models improve probability estimates grams 
done interposing models different order grams smoothing procedure 
compare results baseline tri gram model backs bigram unigram models 
intermediate models reduce perplexity unseen word combina tions 
section discuss extensions models open problems research 
conclude aggregate mixed order models provide compelling alternative language models exclusively grams 
aggregate markov models section consider construct class bigram models brown 
problem naturally formulated hidden variable density estimation 
denote probability word wl mapped class likewise denote probability words class followed word 
class bigram model predicts word wl followed word probability wl total number classes 
hidden variable problem class label unknown word wl 
note eq 
represents elements transition matrix wa terms cv elements 
expectation maximization em algorithm dempster laird rubin iterative procedure estimating parameters hidden variable models 
iteration consists steps step computes statistics hidden variables step updates param eters reflect statistics 
em algorithm aggregate markov models particularly simple 
step compute bigram training set posterior probability wl lc lwl eq 
gives probability word wl assigned class observation followed word 
step uses posterior probabilities re estimate model parameters 
updates aggregate markov models wl wc wl wl ew wl denotes number counts training set 
updates guar increase log likelihood wl lnp wl iteration 
general converge local global maximum log likelihood 
perplexity related log likelihood total number words processed 
algorithms brown pereira tishby lee proposed iteration em iteration em plots training test perplexity versus number iterations em algorithm aggregate markov model classes 
train test table perplexities aggregate markov models training test sets number classes 
case corresponds ml unigram model ml bigram model 
winning assignment probability histogram winning assignment probabilities maxc commonly occurring words 
goo performing decomposition eq 
worth noting em algorithm directly optimizes log likelihood eq 

ob vious advantages goal finding word classes improve perplexity language model 
em algorithm handles probabilistic constraints natural way allowing words belong class increases likelihood 
approach differs important ways hidden markov models hmms class language modeling jelinek 
hmms hidden variables represent word classes dynamics fundamentally dif ferent 
hmms hidden state time predicted state transition matrix hidden state time hand gate markov models hidden state time predicted matrix ct word time state state versus word state dynamics lead different learning algorithms 
example baum welch algorithm hmms requires forward backward passes training sentence em algorithm 
trained aggregate markov models classes 
shows typical plots training test set perplexities versus number iterations em algorithm 
clearly curves close monotonic decrease test set perplexity strongly suggests lit tle overfitting number classes small compared number words vocabulary 
table shows final perplexities iterations em various ag markov models 
results confirm aggregate markov models intermediate accu racy unigram bigram models 
aggregate markov models observed discover meaningful word classes 
table shows aggregate model classes las cents take ago day friday monday month quarter reported said thursday trading tuesday wednesday 
get days home months years 
nineteen called san start sentence bank board chairman group members number office part percent price prices rate sales shares dollar old long year may just economic high interest tax united president business california case companies dollars incorporated industry law money time today war week unknown government market foreign international major new oil soviet stock west world say think including back expected going plan way told don people fifteen half oh bush court department officials spokesman second third twelve zero american big city federal general house national party political state union york chief exchange news public service trade table probable assignments frequent words aggregate markov model classes 
class absent probable class selected words 
probable class assignments hun commonly occurring words 
precise class listed words arg 
shows histogram winning assignment probabilities words 
note winning assignment probabilities distributed broadly inter val 
demonstrates utility allowing soft membership classes words max imum likelihood estimates corre winner take assignment method assigns word single class hard clustering brown 
ney essen kneser lose information 
conclude section final com ments overfitting 
models trained iterations em allowing nearly com plete convergence log likelihood 
implement constraints probabilities 
experiments ml aggregate markov lit worth noting regard individual zeros matrices wl nec give rise zeros matrix wt computed eq 

models assigned non zero probability bi grams test set 
suggests large vocabularies useful regime aggregate models suffer overfitting 
regime aggregate models relied compute probabilities unseen word combinations 
return point section consider smooth gram language models 
mixed order markov models drawbacks gram models size grows rapidly order 
section consider predictions con vex combination pairwise correlations 
leads language models size grows linearly number words prediction 
ski transition matrix wt wt predicts current word kth previous word sentence 
mixed order markov model combines information matrices different values denote number bigram models combined 
probability distribution models form wt fi ak wt mk wt wt ii aj 
terms equation simple tion 
matrices mk eq 
de fine skip stochastic dependency position position parameters ak mixing coefficients weight predic tions different dependencies 
value ak interpreted probability model seeing word wt looks back prediction singer 
model predicts wt probability wt wt probability wt wt 
included eq 
cosmetic reasons parameters am fixed unity model looks words back 
view eq 
hidden variable model 
imagine adopt strategy pre dict word time starting previous word toss coin bias ai wt see word high predictive value 
answer predict skip transition matrix ml wt wt 
shift word tothe left repeat process 
tosses settled pre diction resort prediction mm wt wt 
hidden variables process outcomes coin tosses unknown word wt 
viewing model way derive em algorithm learn mixing coefficients ak transition matrices mk 
step algorithm compute word training set posterior probability generated mk wt wt 
denoting probabilities ck ck aa wt mk wt wt wt iw denominator eq 

step algorithm update parame ters ak mk reflect statistics eq 

updates mixed order markov models wt note ml estimates mk depend raw counts separated bigrams coupled values mixing coef aa 
particular em algorithm adapts matrix elements weighting word combina tions eq 

raw counts separated bigrams give initial estimates 
iteration em plot training set perplexity versus number iterations em algorithm 
re sults mixed order markov model 
train missing table results ml mixed order models de notes number bigrams mixed prediction 
column shows training set 
ec nd shows fraction words test set assigned zero probability 
case corresponds ml bigram model 
mk wt wt sums sentences training set iff 
trained mixed order markov models 
shows typical plot train ing set perplexity function number iterations em algorithm 
table shows final perplexities training set iter ations em 
mixed order models directly test set predict zero probability unseen word combinations 
standard gram models number unseen word combinations decreases order model 
reason mixed order models assign finite probability grams wn separated bigrams observed training set 
illustrate point table shows fraction words test set assigned zero proba bility mixed order model 
expected fraction decreases monotonically number bigrams mixed prediction 
clearly success mixed order models de ability gauge predictive value word relative earlier words sentence 
see plays low unknown officials prices go way earlier tuesday foreign quarter federal don days friday wednesday thursday monday half part united years going nineteen months 
cents san ago percent 

table words low high values mixed order model 
second order model table 
model small value indicates word typically carries information word precedes 
hand large value indicates word highly predictive 
ability learn relationships confirmed results table 
common words table shows lowest highest values 
note low values associated prepositions mid sentence punctuation marks conjunctions high values associated words sentence markers 
particularly interesting dichotomy arises forms indefinite article lat ter precedes word begins vowel inherently predictive 
results underscore importance allowing coefficients depend context opposed context independent ney essen kneser 
smoothing smoothing plays essential role language models ml predictions unreliable rare events 
gram modeling common adopt re cursive strategy smoothing bigrams unigrams trigrams bigrams 
adopt similar strategy th mixed order model smooth ruth 
root smoothing procedure lies uni gram model aggregate markov model classes 
shown section models assign finite probability word combinations observed training set 
legitimately replace unigrams base model smoothing procedure 
examine impact replacing uni gram models aggregate models root validation test unseen table perplexities bigram models smoothed aggregate markov models different numbers classes 
smoothing procedure 
held inter algorithm jelinek mercer smooth ml bigram model gate markov models section 
smoothing parameters row bigram transi tion matrix estimated validation set size test set 
table gives final validation set test set unseen bigrams test set 
note smooth ing aggregate markov model nearly halved perplexity unseen bigrams compared smoothing unigram model 
examine recursive mixed order models obtain smoothed probability esti mates 
held interpolation algorithm smooth mixed order markov models section 
ruth mixed order model mv smoothing parameters corresponding rows skip transition matrix 
mth mixed order model smoothed discount ing weight skip prediction fill ing leftover probability mass lower order model 
particular discounted weight skip prediction wt lak wt hi wt leaving total mass fi wt th mixed order model 
note mixed order model corresponds ml bigram model 
table shows perplexities smoothed mixed order models validation test sets 
aggregate markov model classes base model smoothing proce dure 
row corresponds bigram model smoothed aggregate markov model second row corresponds mixed order model smoothed ml bigram model smoothed aggregate markov model third row corresponds validation test table perplexities smoothed mixed order mod els validation test sets 
mixed order model smoothed mixed order model smoothed ml bi gram model significant decrease ity occurs moving smoothed mixed order model 
hand difference perplexity higher values dra matic 
experiment looked smoothing trigram model 
baseline ml trigram model backed bigrams nec essary unigrams katz backoff procedure katz 
procedure predictions ml trigram model discounted amount determined turing coefficients left probability mass filled backoff model 
compared trigram model backed model table 
handled slight variant katz procedure dagan pereira lee mixed order model substituted backoff model 
advantage smoothing procedure straightforward assess performance dif ferent backoff models 
backoff models consulted unseen word combinations perplexity word combinations serves reasonable merit 
table shows perplexities smoothed trigram models baseline backoff 
mixed order smoothing reduce perplexity unseen word combinations 
shown table perplexities entire test set 
perplexity decreased significant amount considering predictions involved unseen word com required backing trigram model 
models table constructed grams observed training data 
grams occur infrequently natural question truncated models omit low frequency grams training set perform ones 
ad vantage truncated models need store nearly non zero parameters un truncated models 
results table ob backoff procedure lation avoid estimation trigram smoothing parameters 
backoff test unseen baseline mixed table perplexities smoothed trigram mod els test set subset unseen word combinations 
baseline model backed bi grams unigrams backed model table 
baseline mixed trigrams missing table effect truncating trigrams occur times 
table shows baseline mixed order perplexities test set num ber distinct trigrams counts fraction trigrams test set required backing 
tained dropping trigrams occurred times training corpus 
row cor responds models table 
observation table omitting low frequency trigrams decrease quality mixed order model may fact slightly improve 
contrasts standard backoff model truncation causes significant increases perplexity 
discussion results demonstrate utility language mod els intermediate size accuracy tween different order gram models 
models considered hidden vari able markov models trained em algorithms maximum likelihood estimation 
combinations intermediate order models investigated rosenfeld 
experiments word vocabulary wall street journal corpus pre nab corpus 
trained maximum entropy model consisting unigrams bigrams tri grams skip bigrams trigrams selecting long distance bigrams word triggers words model tested held thou sand word sample 
rosenfeld reported test set perplexity reduction plexity baseline trigram backoff model 
experiments perplexity gain mixed order model ranged depending amount truncation trigram model 
rosenfeld results di rectly comparable demonstrate utility mixed order models 
worth discussing different approaches combining infor mation non adjacent words 
max imum entropy approach allows com non independent features calls careful markovian decomposition 
rosenfeld ar length naive linear combinations favor maximum entropy methods 
arguments apply reasons 
large number context dependent mixing parameters optimize likelihood combined model 
weighting eq 
en sures skip predictions invoked context appropriate 
second adjust predictions skip transition matrices em match contexts invoked 
count models way consistent eventual 
training efficiency issue evaluating language models 
maximum entropy method requires long training times cpu days rosenfeld experiments 
methods re quire significantly example trained smoothed mixed order model start finish cpu hours larger training corpus 
accounting differ ences processor speed amounts signifi cant mismatch training time 
mention open problems research 
aggregate markov models viewed approximating full bigram tran sition matrix matrix lower rank 
eq 
clear rank class transition matrix bounded num ber classes 
interesting parallels expectation maximization em minimizes approximation error mea kl divergence singular value de composition svd minimizes approxi mation error measured norm press 
svd finds global minimum error measure em finds local 
clearly desirable improve understanding fundamental problem 
focused bigram models ideas algorithms generalize straight forward way higher order grams 
aggregate models higher order grams brown able capture multi word struc tures noun phrases 
likewise trigram mixed order models useful complements gram gram models mon large vocabulary language modeling 
final issue needs addressed scaling performance mod els depends vocabulary size amount training data 
generally expects sparser data helpful models intervene different order grams 
nev interesting see exactly relationship plays aggregate mixed order markov models 
acknowledgments michael kearns yoram singer ful discussions anonymous reviewers ques tions suggestions helped improve don hindle help language modeling tools build baseline models considered 
brown della pietra desouza lai mercer 

class gram models natural language 
computational linguistics 
chen goodman 

empirical study smoothing techniques language modeling 
proceedings th meeting association computational linguistics 
dagan pereira lee 

similarity estimation word occurrence ties 
proceedings nd annual meeting association computational linguistics 
dempster laird rubin 

max imum likelihood incomplete data em algorithm 
journal royal statistical society 
huang hon 
hwang 
lee rosenfeld 

sphinx speech recognition system overview 
computer speech language 
jelinek mercer 

interpolated es markov source parameters sparse data 
proceedings workshop pattern recognition practice 
jelinek mercer roukos 

princi ples lexical language modeling speech recogni tion 
furui sondhi eds 
advances speech signal processing 
mercer dekker katz 

estimation probabilities sparse data language model component speech recognizer 
ieee transactions assp 
ney essen kneser 

ing probabilistic dependences stochastic language modeling 
computer speech language 
pereira tishby lee 

distribu tional clustering english words 
proceedings th annual meeting association computational linguistics 
press flannery teukolsky vet 

numerical recipes cambridge university press cambridge 
rosenfeld 

maximum entropy approach adaptive statistical language modeling 
com puter speech language 


dimensions meaning 
pro ceedings 
minneapolis mn 
singer 

adaptive mixtures probabilistic transducers 
touretzky mozer hasselmo eds 
advances neural information processing systems 
mit press cam bridge ma 

