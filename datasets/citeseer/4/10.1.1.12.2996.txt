enriching knowledge sources maximum entropy part speech tagger kristina toutanova dept computer science gates bldg serra mall stanford ca usa kristina cs stanford edu presents results maximum entropy part speech tagger achieves superior performance principally enriching information sources tagging 
particular get improved results incorporating features extensive treatment tion unknown words ii features disambiguation tense forms verbs iii features disambiguating particles prepositions adverbs 
best resulting accuracy tagger penn treebank previously unseen words 
numerous systems automatic assignment parts speech tagging employing different machine learning methods 
top performing methods hidden markov models brants maximum entropy approaches ratnaparkhi transformation learning brill 
overview approaches manning ch 

methods largely information sources tagging features consequence offer similar levels performance 
stands contrast manually built tagger achieves better performance lexical contextual information sources generalizations available statistical taggers samuelsson voutilainen demonstrate 
dan klein michael saunders useful discussions anonymous reviewers helpful comments 
christopher manning depts computer science linguistics gates bldg serra mall stanford ca usa manning cs stanford edu explores notion automat ically built tagger performance improved expanding knowledge sources available tagger 
pay special attention unknown words markedly lower accuracy unknown word tagging means area significant performance gains possible 
adopt maximum entropy approach allows inclusion diverse sources information causing frag mentation necessarily assuming independence predictors 
maxi mum entropy approach applied part speech tagging ratnaparkhi approach ability incorporate non local non hmm tagger type evidence fully explored 
describes models developed experiments performed evaluate 
baseline maximum entropy model started maximum entropy tagger uses features similar ones proposed ratnaparkhi 
tagger learns loglinear conditional probability model tagged text maximum entropy method 
model assigns probability tag set possible tags word context usually def med sequence words tags preceding word 
model estimating probability tag sequence tn sentence wn wl 
ii ti 
usual tagging process assigning maximum likelihood tag sequence string words 
idea maximum py modeling choose probability distribution highest entropy distributions satisfy certain set constraints 
constraints restrict model behave accordance set statistics collected training data 
statistics expressed expected values appropriate functions defined contexts tags particu lar constraints demand expectations features model match empirical expectations features training data 
example want constrain model tag verb noun frequency empirical model induced training data define features fl iff wi nn iff vb commonly statistics part speech tagging certain word tagged certain way tags appeared sequence tags appeared sequence 
look lot statistics markov model 
maximum entropy framework possible easily define incorporate complex statistics restricted gram sequences 
constraints model expectations features joint distribution equal expectations features empirical training data distribution ep fi 
having defined set constraints model accord proceed find model satisfying constraints maxi conditional entropy model assumes apart satisfy constraints 
berger 
approxi mate joint distribution contexts tags product empirical distribution histories conditional distribution 
lh 
example constraints approximation enable efficient computation 
expectation fea ture te space possible contexts predicting part speech tag contexts contain sequences words tags information space huge 
approximation sum just smaller space observed contexts training sample empirical prior zero unseen contexts model solution constrained optimization task exponential equivalently loglinear model para metric form ea denominator normalizing term referred partition function 
parameters correspond weights features discuss detail tics model parameter estimation procedure improved iterative scaling 
extensive discussion maximum entropy methods see berger 
jelinek 
note pa rameter estimation algorithm directly uses equa tion 
ratnaparkhi suggests approximation summing training data sum possible tags ti hi hi ti believe passage error estimate ineffective iterative scaling algorithm 
note expectations form appear ratnaparkhi 
features baseline model baseline model context available predicting part speech tag word wi sentence words wl wn tags tl ti tin wi wi 
features define constraints model obtained instantiation feature templates ratnaparkhi 
special feature templates exist rare words training data increase model capacity unknown words 
actual feature templates model shown table 
subset features ratnaparkhi 
feature type template 
general wi ti 
general tl ti 
general tia ti ti 
general wi ti 
rare suffix wi 
rare prefix ipi ti 
rare contains number 
rare wi contains uppercase character 
rare contains hyphen ti table baseline model features general feature templates instantiated arbitrary contexts rare feature tem plates instantiated histories current word wi rare 
rare words defined words appear certain number times training data value 
order able throw features give misleading statistics due sparse ness noise data different cutoff values general rare feature templates implementation respectively 
seen table features conjunctions boolean function history boolean function tag features conjuncts true corresponding threshold number histories training data included model 
feature templates ratnaparkhi left ones look previous word word positions current word positions current 
features form template table look words different positions 
motivation leaving features results experiments successively adding feature templates 
adding template model incorporated general feature templates rare feature templates significantly increased accuracy development set 
addition feature template looked preceding word current tag resulting model slightly reduced accuracy 
testing performance model trained tested part speech tagged wsj section penn treebank 
data divided contiguous parts sections training sections development test set sections final test set 
data set sizes shown numbers unknown words 
data set tokens unknown training development test table data sizes testing procedure uses beam search find tag sequence maximal probability sentence 
experiments beam size 
increasing beam size result improved accuracy 
preceding tags word sentence regarded having pseudo tag na 
way information word word sentence available tagger 
special sentence symbol 
tag dictionary known words testing 
built tags training data augmented capture basic systematic tag ambiguities english 
regular verbs ed form vbd vbn similarly stem form vbp vb 
words occurred tags training data included possible assignment 
results test set baseline model shown table 
model unknown word accuracy accuracy baseline ratnaparkhi table baseline model performance table shows results reported ratnaparkhi convenience 
accuracy model higher lower unknown words 
may stem differences models feature templates thresholds approxi mations expected values features discussed section may just reflect differences choice training test sets precisely specified ratnaparkhi 
differences great justify definite statement different feature templates particularities model estimation 
draw additional word features ratnaparkhi looking words position away current appear helping performance models 
discussion problematic cases large number words including common words syntactic category 
introduces lot ambiguities tagger resolve 
ambiguities easier taggers resolve harder 
significant confusions baseline model test set seen table 
row labels table signify correct tags column labels signify assigned tags 
example num ber nn jj position number words nns incorrectly assigned jj category 
particular sions shown table account large percentage total error 
table shows part baseline model confusion matrix just unknown words 
table shows baseline model assignment accuracies different parts speech 
example accuracy nouns greater accuracy adjectives 
accuracy plural proper nouns surprisingly low 
tag accuracy tag jj nn rb nnp vbn vbd rp vb vbp accuracy table accuracy assignments different parts speech baseline model 
tagger errors various types 
result inconsistency labeling training data ratnaparkhi usually reflects lack linguistic clarity determination correct part speech context 
instance status various noun premodifiers chief maximum nn jj word ing acting jj vbg type 
errors nn nnp nns largely reflect difficulties unknown words 
cases vbn vbd vb vbp nn represent matic tag ambiguity patterns english fight answer invariably clear context general structural contextual clues able 
class cases prominent probably rp rb ambiguity words linguistic distinctions having sound empirical basis see baker quite subtle require semantic intuitions 
syntactic cues correct tag human taggers infrequently errors 
classification greatest hopes tagging improvement appear come minimizing errors second third classes classification 
sections discuss include additional knowledge sources help assignment tags forms verbs capitalized unknown words particle words accuracy part speech assignments 
improving unknown words model accuracy baseline model markedly lower unknown words previously seen ones 
case taggers reflects importance lexical information taggers best accuracy figures punished corpus taggers known word accuracy unknown word accuracy 
experiments examined ways additional features improve accuracy tagging unknown words 
previ ously discussed possible improve accuracy capitalized words proper nouns word sentence jj nn nnp rp vb vbd vbn vbp total jj nn nnp rb rp vb vbd vbn vbp total accuracy test set table confusion matrix baseline model showing top confusion pairs jj nn nnp nns vbn total jj nn nnp nns vbn total table confusion matrix baseline model unknown words showing top confusion pairs unknown words accuracy test set accuracy development set unknown words accuracy development set baseline model model model capitalization verb forms particles table accuracies models test development sets baseline model model model capitalization verb forms particles 
current word 
previous tag 
previous tags 
word 
suffixes 
prefixes 
contains uppercase character 
contains number 
contains hyphen 
capitalized mid 
sentence 
letters uppercase 
feature 
feature 
particles type 
particles type total table number features different types example error proper noun category nnp accounts significantly larger percent total error unknown words known words 
baseline model unknown word error due words nnp assigned category category assigned nnp 
percentage type error known words 
incorporation feature schemas greatly improved nnp accuracy feature looks letters word uppercase 
feature looked capitalization cf 
table feature activated word contains uppercase character 
turns notable distinction example titles wsj data words uppercase distribution tags words different distribution words contain uppercase character 
feature activated word contains uppercase character start sentence 
word tokens different tag distribution distribution tokens contain uppercase character 
conversely empirically prefix features rare words having net negative effect accuracy 
explanation phenomenon 
addition features removal prefix features considerably improved accuracy unknown words accuracy 
results test set adding features shown accuracy unknown word accuracy table accuracy adding capitalization features removing prefix features 
unknown word error reduced compared baseline model 
important note composed information known tagger sense 
feature viewed conjunction features baseline model negation feature existing baseline model words sentence preceding tag pseudo tag na feature looking preceding tag 
maximum entropy model require independence predictors provides free simple combination feature weights additional interaction terms needed model non additive interactions log space terms features 
features disambiguating verb forms significant sources classifier errors vbn vbd ambiguity vbp vb ambiguity 
seen table vbn vbd confusions account total word error 
vbp vb confusions smaller errors 
cases easy people taggers determine correct form 
example infinitive modal directly preceding vb vbp ambiguous word form certainly non finite 
modal positions away current position obvious human sight baseline model 
help resolve vb vbp ambiguity cases add feature looks preceding words chosen threshold verb activates modal verb form help verbs frequently take bare infinitive complement 
having separate feature look preceding position define feature looks chosen number positions left 
increases scope available history tagger provides better statistic avoids fragmentation 
added similar feature resolving vbd vbn confusions 
activates auxiliary form preceding positions value implementation 
form feature templates motivated structural rules english induced training data possible look predictors certain parts speech preceding words sentence example computing association strengths 
addition feature schemas helped reduce vb vbp vbd vbn con fusions 
performance test set resulting model features disam verb forms added model section 
number vb vbp confusions reduced compared base line 
number vbd vbn confusions reduced 
accuracy unknown word accuracy table accuracy extended model features particle disambiguation discussed section task determining rb rp tags words difficult particular examples local syntactic indicators 
instance find exact sequence parts speech particle prepositional 
consequently accuracy rarer rp particles category low baseline model cf 
table 
kim took monster 
kim sat monster 
tried improve tagger capability resolve ambiguities adding infor mation verbs preferences take specific words particles adverbs prepositions 
verbs take particles particular words particle context verb words ambiguous tags 
added different feature templates capture information consisting usual predicate history condition tag predicate true current word particle verb positions left known chance current word particle 
verb particle pairs known system common collected analysis training data preprocessing stage 
second feature template form verb current word tagged particle current tag verb pseudo symbol na verb previous positions 
features help reducing rb rp confusions 
accuracy rp category rose 
confusions class reduced errors increased example number ins classified rbs rose slightly 
considerable room improve results attainable accuracy limited accuracy distinctions marked penn treebank quick informal study accuracy 
table shows final performance test set 
unknown word accuracy table accuracy final model ease comparison accuracies models test development sets shown table 
note accuracy lower development set 
presumably corre sponds charniak observation section penn treebank easier 
table shows different number feature templates kind instantiated different models total number features model 
seen features help disambiguate verb forms look capital ization feature templates particles small number compared features kinds 
improvement classification accuracy comes price adding parameters maximum entropy model result increased model complexity 
accuracy figures corpus part speech taggers start look extremely similar possible move performance levels 
explored just information sources addition ones usually tagging 
progress slow new feature applies limited range cases improvement accuracy compared previous results noticeable particularly individual decisions focused 
potential maximum entropy methods previously fully exploited task assignment parts speech 
incor maximum entropy tagger linguistically sophisticated features non local look just particular positions text 
added features model interactions previously employed predictors 
changes led modest increases tagging accuracy 
initial experiments improving tagger accuracy additional information sources 
hope explore automatically discovering information sources profitably incorporated maximum entropy part speech prediction 
baker 
english syntax 
cambridge ma mit press nd edition 
adam della pietra stephen della pietra vincent 
maximum entropy approach natural language processing 
compu tational linguistics 
brants thorsten 

tnt statistical part speech tagger 
proceedings sixth applied natural language processing conference anlp seattle wa pp 
brill eric 

advances transformation part speech tagging 
proceedings vol 
pp 

charniak eugene 

maximum entropy inspired parser 
proceedings st meeting north american chapter association computational linguistics pp 

jelinek frederick 

statistical methods speech recognition 
cambridge ma mit press 
manning christopher hinrich 

foundations statistical natural language processing 
cambridge ma mit press 
mikheev andrei 

periods capitalized words ms university edinburgh 
available ltg ed ac html ratnaparkhi adwait 

maximum entropy model part speech tagging 
proceedings conference empirical methods natural language processing university pennsylvania pp 

ratnaparkhi adwait 

maximum entropy models natural language ambiguity tion 
phd thesis university pennsylvania 
samuelsson christer voutilainen 

comparing linguistic stochastic tagger 
proceedings th annual meeting association computational linguistics pp 


