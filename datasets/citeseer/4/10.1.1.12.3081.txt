challenges web search engines monika henzinger rajeev motwani craig silverstein october article presents high level discussion problems information retrieval unique web search engines 
goal raise awareness stimulate research areas 
web search engines faced number dicult problems maintaining enhancing quality performance 
problems unique domain novel variants problems studied literature 
goal writing article raise awareness problems believe bene increased study research community 
deliberately ignore interesting dicult problems subject active research 
consider techniques improve text retrieval support natural language queries query non text corpora image audio collections search ectively multiple languages forth 
high level description problems describe detail subsequent sections 
spam 
users web search engines tend examine rst page search results 
silverstein showed queries rst result screen requested 
inclusion rst result screen usually shows top results lead increase trac web site exclusion means small fraction users see link web site 
commercially oriented web sites income depends trac google parkway mountain view ca 
mail monika google com 
department computer science stanford university stanford ca 
mail rajeev cs stanford edu 
part done author visiting google supported part nsf iis research foundation veritas 
google parkway mountain view ca 
mail google com 
interest ranked top results query relevant content web site 
achieve goal web authors try deliberately manipulate placement ranking order various search engines 
result process commonly called search engine spam 
simply refer spam 
achieve high rankings authors text approach link approach cloaking approach combination thereof 
web ranking optimization services fee claim place web site highly search engine 
unfortunately spamming prevalent commercial search engine take measures identify remove spam 
measures quality rankings su ers severely 
traditional research information retrieval ir deal problem malicious content corpora 
quite certainly problem benchmark document collections ir researchers past collections consist exclusively high quality content newspaper scienti articles 
similarly spam problem context intranets web exists 
current research ir increased focus search retrieval web relatively little attention paid worsening problem content created mislead 
content quality 
spam problem exist troubling issues concerned quality content web 
web full noisy low quality unreliable contradictory content 
reasonable approach ir relatively high quality content assume document collection authoritative accurate design techniques context tweak techniques incorporate possibility content 
democratic nature content creation web leads corpus fundamentally noisy poor quality useful information emerges statistical sense 
designing high quality search engine start assumption typical document trusted isolation synthesis large number low quality documents provides best set results 
rst step direction outlined extremely helpful web search engines able identify quality web pages independent user request 
link approaches instance pagerank estimating quality web pages :10.1.1.109.4049
pagerank uses link structure web estimate page quality 
better estimate quality page requires additional sources information page reading level page di erent pages correlation content 
quality evaluation 
evaluating quality di erent ranking algorithms notoriously dicult problem 
commercial search engines bene large amounts user behavior data help evaluate ranking 
users usually ort give explicit feedback leave implicit feedback information results clicked 
research issue exploit implicit feedback evaluate di erent ranking strategies 
web conventions 
creators web pages follow simple rules anybody imposing rules 
example anchor text link provide succinct description target page 
authors behave way refer rules web conventions formalization standardization rules 
search engines rely web conventions improve quality results 
consequently webmasters violate conventions confuse search engines 
main issue identify various conventions evolved develop techniques accurately determining conventions violated 
duplicate hosts 
web search engines try avoid crawling indexing duplicate pages add new information search results clutter results 
problem identifying duplicates set crawled pages studied 
search engine avoid crawling duplicate content rst place gain larger 
general predicting page duplicate crawled page problem tractable limit nding duplicate hosts hostnames serve content 
ways duplicate hosts arise artifact domain name system dns hostnames resolve physical machine 
preliminary duplicate hosts problem 
vaguely structured data 
degree structure data strong uence techniques search retrieval 
extreme database community focused highly structured relational data ir community concerned essentially unstructured text documents 
late movement middle database literature considering imposition structure structured data 
similar vein ir systems document management systems accumulated metainformation introduce structure 
emergence xml led research involving extraction imposition maintenance partially structured data 
web pages html fall middle continuum structure documents close free text structured data 
html markup provides limited structural information typically control layout providing clues semantic information 
layout information html may limited utility especially compared information contained languages xml tag content fact particularly valuable source meta data unreliable corpora web 
value layout information stems fact visible user meta data user visible particularly susceptible spam techniques layout information dicult spam ecting user experience 
initial partly related vein 
believe exploitation layout information lead direct dramatic improvement web search results 
spam web authors try deliberately manipulate placement rankings various search engine 
resulting pages called spam 
traditional ir collections contain spam 
result research making search algorithms resistant spam techniques 
web search engines hand consistently developing improving techniques detecting ghting spam 
search engine techniques developed new spam techniques developed response 
search engines publish anti spam techniques avoid helping spammers circumvent 
historical trends indicate variety spam continue increase 
challenging research issues involved detecting spam developing ranking algorithms resistant spam 
current spam falls broad categories text spam link spam cloaking 
spammer combination 
text spam search engines evaluate content document determine ranking search query 
text spam techniques modify text way search engine rates page particularly relevant modi cations increase perceived relevance human reader document 
ways try improve ranking 
concentrate small set keywords try improve perceived relevance set keywords 
instance document author repeat keywords bottom document hoped disturb user 
text small type rendered invisible written page background color accomplish 
technique try increase number keywords document perceived relevant search engine 
na approach include subset dictionary bottom web page increase chances page returned obscure queries 
na approach add text di erent topic page appear main topic page 
example porn sites add names famous personalities pages order pages appear user searches personalities 
link spam advent link analysis search engines accompanied ort spammers manipulate link analysis systems 
common approach author put link farm bottom page site link farm collection links points page site site author controls 
goal manipulate systems raw counts incoming links determine web page importance 
completely linked link farm easy spot sophisticated techniques pseudo web rings random linkage member group 
problem link farms distract reader pages legitimate content 
sophisticated form link farms developed called doorway pages 
doorway pages web pages consist entirely links 
intended viewed humans constructed way search engines discover 
doorway pages thousands links including multiple links page 
text spam equivalent doorway pages text links analyzed search engines page basis 
link farms doorway pages ective link analysis sensitive absolute number links 
techniques concentrate quality links pagerank particularly vulnerable techniques :10.1.1.109.4049:10.1.1.107.7614
cloaking cloaking involves serving entirely di erent content search engine crawler users 
result search engine content page scores page ways human observer arbitrary 
cloaking intent help search engines instance giving easily text version page heavy multimedia content provide link access database normally accessible forms search engines navigate 
typically cloaking deceive search engines allowing author achieve bene ts link text spam human readers web page 
defending spam general text spam defended heuristic fashion 
instance common sites hide text writing white text white background ensuring human readers ected search engines misled content 
result search engine companies detected text ignored 
reactive approaches obviously optimal 
pro active approaches succeed 
approaches combined search engine crawler program downloads web pages purpose including search engine results 
typically search engine download number pages crawler process pages create data structures service search requests 
steps repeated continuously ensure search engine searching date content possible 
possible search engine notice pages change response launch new anti spam heuristic consider pages potential spam pages 
typically link spam sites certain patterns links easy detect patterns mutate way link spam detection techniques 
heuristic approach discovering link spam required 
possibility case text spam global analysis web merely local page level site level analysis 
example cluster sites suddenly sprout thousands new interlinked webpages candidate link spam site 
ravi kumar nding small bipartite clusters web rst step direction 
cloaking discovered crawling website twice client believes search engine client believes search engine 
web pages typically di er downloads legitimate reasons changing news headlines 
content quality attempts deliberately mislead search engines web replete text intentionally human readers 
example webpage claims falsely 
thomas je erson rst president united states 
websites purposefully contain misleading medical information 
sites contain information correct date example sites giving names elected 
great deal research determining relevance documents issue document quality accuracy received attention web search forms information retrieval 
instance trec conference explicitly states rules considers document relevant mention accuracy reliability document 
understandable typical research corpora including ones trec corporate intranets consist document sources deemed reliable authoritative 
web course corpus techniques judging document quality essential generating search results 
successful approach heuristically approximating quality web link analysis instance pagerank hits :10.1.1.109.4049:10.1.1.107.7614
techniques start practice ample room improvement 
interesting aspect problem document quality speci hypertext corpora web evaluating quality anchor text 
anchor text text typically displayed underlined blue web browser annotate hypertext link 
typically web search engines bene including anchor text analysis scoring function 
study showed reputable medical sites contain contradictory information di erent pages site particularly dicult content quality problem 
little research anchor text analysis due spam methodologies avoiding pitfalls 
instance kinds low quality pages anchor text high quality 
possible judge quality anchor text independently quality rest page 
possible detect anchor text intended editorial purely descriptive 
addition fundamental issues remain open application anchor text determination document quality content 
case documents multiple topics anchor text analysis identify themes 
promising area research combine established link analysis quality judgments text judgments 
text analysis instance judge quality thomas je erson page noting rst president united states web corpus attribute role george washington 
quality evaluation search engines easily improve ranking algorithms running tests compare quality new ranking technique old 
performing comparisons human evaluators quite intensive runs danger correctly re ecting user needs 
best users perform evaluation task know needs best 
users typically reluctant give direct feedback 
web search engines collect implicit user feedback log data position clicks search time spent click 
data incomplete 
instance user clicks search result search engine know pages user visits user returns search engine 
hard tell user clicking page ends nding page relevant useful 
incomplete nature information experimental setup college implicit user data particularly important 
click data collected 
metrics computed data 
approach simply collect click data subset users users ranking algorithm 
experimenter compute metrics percentage clicks top results number clicks search 
joachims suggested experimental technique involves merging results ranking algorithms single result set 
way user performs comparison algorithms 
joachims proposes number clicks quality metric shows weak assumptions clickthrough ranking higher clickthrough retrieves relevant links web conventions web grown developed evolution conventions authoring web pages 
search engines assume adherence conventions improve search results 
particular conventions assumed relating anchor text hyperlinks meta tags 
discussed section fact anchor text meant descriptive web convention exploited scoring function search engine 
search engines typically assume web page author includes link page author believes readers source page nd destination page interesting relevant 
way people usually construct web pages assumption usually valid 
prominent exceptions instance link exchange programs web page authors agree reciprocally link order improve connectivity rankings advertisement links 
humans adept distinguishing links included primarily commercial purposes included primarily editorial purposes 
search engines 
complicate matters utility link binary function 
instance pages links allowing download latest version adobe acrobat reader 
visitors acrobat reader link useful certainly useful downloaded program 
similarly sites terms service link bottom page 
user rst enters site link useful user browses webpages site link usefulness immediately decreases 
third web convention concerns meta tags 
tags currently primary way include metadata html 
theory meta tags include arbitrary content conventions arisen meaningful content 
meta tag particular importance search engines called content meta tag web page authors describe content document 
convention dictates content meta tag contains short textual summary page brief list keywords pertaining content page 
abuse meta tags common attempt deceive break convention ignorance 
instance webpage author include summary entire site meta tag just individual page 
author include keywords general page warrants meta description cars sale web page sells particular model car 
general correctness meta tags dicult search engines analyze visible users constrained useful visitors 
web page authors meta tags correctly 
web search engines correctly judge usefulness text meta tag search results potentially improved signi cantly 
applies content normally displayed alt text associated image tag 
link analysis increasingly important technique web information retrieval research di erent types links web 
research try distinguish commercial editorial links links relate metainformation site site best viewed start link browser link links relate actual content site 
extent existing research link analysis helpful authors highly visible web pages established web conventions 
clearly sucient 
instance highly visible pages include advertisements average page 
understanding nature links valuable enables sophisticated treatment associated anchor text 
potential approach text analysis anchor text combined meta information url link conjunction information obtained web graph 
duplicate hosts web search engines try avoid crawling indexing duplicate near duplicate pages pages increase time crawl contribute new information search results 
problem nding duplicate near duplicate pages set crawled pages studied 
research identifying duplicate near duplicate directory trees called mirrors 
mirror detection individual page detection try provide complete solution problem duplicate pages simpler variant reap bene ts requiring computational resources 
simpler problem called duplicate host detection 
duplicate hosts single largest source duplicate pages web solving duplicate hosts problem result signi cantly improved web crawler 
host merely name domain name system dns arise fact dns names resolve ip address 
companies typically reserve name dns increase visibility protect domain name 
fact necessary resolve ip address just resolve webserver 
technically necessary minimum requirement resolve computers serve content hostnames question 
instance currently com com resolve ip address result sites www com www com display identical content 
unfortunately duplicate ip addresses necessary sucient identify duplicate hosts 
virtual hosting result di erent sites sharing ip address round robin dns result single site having multiple ip addresses 
merely looking content small part site homepage equally ine ective 
domain names resolve website homepages di erent instance page includes advertisement dynamic content 
hand unrelated sites web identical construction home page 
problem means solved problem 
diculty solution needs expensive brute force approach compares pair hosts 
instance approach download page hosts look graph isomorphism 
defeats purpose project download pages sites 
furthermore web crawls complete link structure approach robust missing pages 
speci cally transient network problem problem server downtime may keep crawler crawling page host pair 
likewise due increasing amount dynamic content web text approaches check exact duplicates 
hand problem simpler general problem detecting mirrors 
algorithms take advantage fact urls similar di ering hostname component 
furthermore need worry content reformatting common problem mirror sites 
trivial matter analysis bene semantic knowledge dns 
instance candidate foo com foo uk things equal candidates foo com bar com 
vaguely structured data ir corpora tended low structure database content structured 
obviously led major di erence elds evolved years 
instance practical consequence di erence databases permit richer complex set queries text query languages general restricted 
database content generally structured data started exposed web interfaces developed third class data called semi structured data 
web context semi structured data typically content webpage part webpage contains structured data longer contains unambiguous markup explicating structure schema 
considerable research recovering full structure semi structured data example mannila nestorov abiteboul motwani 
examples cover points continuum structured data 
web pages fall categories fall fourth category call vaguely structured data 
information web page structured database sense typically closer prose data structure unintentional exhibited html markup 
say html markup provides unintentional structure typically intent webpage author describe document semantics 
author uses html control document layout way document appears readers 
interesting note original purpose html meant document description language page description language 
give example html tag intended mark glossary entries 
common browsers caused text indented particular way glossary tag context author wants text indented manner 
rarely context involve actual glossary 
course markup serves layout semantic purpose 
html header tags instance produce large font bold text useful breaking text time indicate text marked probably summary description smaller font text follows 
markup provides reliable semantic information prove valuable search engine 
give just example users grown accustomed ignoring text periphery web page cases consists navigational elements advertisements 
search engines positional information expressed layout code adjust weight various sections text document 
addition layout classify pages 
instance pages image upper left page personal homepages 
pages regular markup structure lists search engines may wish analyze di erently pages running text 
markup meta analyzed 
plausible pages mistakes markup lower quality pages mistakes 
patterns markup may allow search engine identify web authoring tool create page turn useful recovering amount structure page 
markup particularly useful clustering web pages author authors template pages write 
course html tags analyzed semantic information inferred 
addition header tags mentioned tags control font face bold italic size color 
analyzed determine words document author thinks particularly important 
advantage html markup language maps closely content displayed opportunity abuse dicult html markup way encourages search engines think marked text important users appears unimportant 
instance xed meaning tag means text context appear prominently rendered web page safe search engines weigh text highly 
reliability html markup decreased cascading style sheets separate names tags representation 
research extracting information structure html possess 
instance chakrabarti created dom tree html page information increase accuracy topic distillation link analysis technique 
research addressing fact html markup primarily descriptive usually inserted ect way document appears viewer 
research bene studies human perception people view changes font size face ecting perceived importance text people pay attention text top page bottom forth 
newspaper publishers long known layout conveys semantic information trivial extract 
turning html markup challenge 
possible render page course computationally expensive 
way gure say piece html text middle rendered html page rendering 
course html text example vaguely structured data 
kinds content exists unstructured data semi structured data terms quantity annotation 
di er html text 
matter continuum structure mapped 
techniques appropriate unstructured data equally vaguely structured data 
techniques semi structured data 
techniques improved data gets structured way map improvements structured forms data structural data doesn correspond intuitive idea structure 
challenging problems faced current web search engines 
years ir research community started proper methodology evaluate web ir systems 
establishment web trec conference greatly contributed improved understanding issues involved appropriate methodology far away 
fruitful areas research related web search engines touch 
instance challenging systems issues arise hundreds millions queries billions web pages serviced day time inexpensively possible 
furthermore interesting user interface issues user interface confuse novice users clutter screen fully empowers experienced user 
ways mine collection web pages provide useful service public large 
resources resources research community stanford webbase project www stanford edu testbed doc webbase distributes content web pages 
web term document frequency available berkeley web term document frequency rank site cs berkeley edu index html 
mannila 
generating grammars sgml tagged texts lacking dtd 
workshop principles document processing 
www cs helsinki fi publications html 
elliott morales broder mu 
lara watkins yang 
health information internet accessibility quality readability english spanish 
journal american medical association 
bharat broder dean henzinger 
comparison techniques find mirrored hosts world wide web 
journal american society information science 
brin davis garc molina 
copy detection mechanisms digital documents 
proceedings acm sigmod international conference management data pages 
brin page :10.1.1.109.4049
anatomy large scale hypertextual web search engine 
proceedings th international world wide web conference www pages 
appeared computer networks 
brin page motwani winograd 
web pocket 
bulletin technical committee data engineering 
broder 
resemblance containment documents 
proceedings compression complexity sequences ieee computer society pages 
chakrabarti 
enhanced topic distillation text markup tags hyperlinks 
proceedings acm sigir conference research development information retrieval 
chakrabarti 
integrating document object model hyperlinks enhanced topic distillation information extraction 
proceedings th international world wide web conference www 
cho shivakumar garcia molina 
finding replicated web collections 
proceedings acm sigmod international conference management data pages 
craswell hawking robertson 
ective site finding link anchor information 
proceedings acm sigir conference research development information retrieval 

attending web pages 
chi extended abstracts poster pages 
kleinberg 
authoritative sources hyperlinked environment 
proceedings th annual acm siam symposium discrete algorithms pages 
joachims 
evaluation search engines clickthrough data 
appear 
nestorov abiteboul motwani 
extracting schema semistructured data 
proceedings acm sigmod conference management data pages 
ravi kumar raghavan rajagopalan tomkins 
trawling emerging automatically 
proceedings th international world wide web conference www 
silverstein henzinger marais 
analysis large altavista query log 
sigir forum 
world wide web consortium 
web style sheets www org style 

