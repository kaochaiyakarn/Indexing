ieee transactions pattern analysis machine intelligence vol 
april inducing features random fields stephen della pietra vincent della pietra john lafferty member ieee technique constructing random fields set training samples 
learning paradigm builds increasingly complex fields allowing potential functions features supported increasingly large subgraphs 
feature weight trained minimizing kullback leibler divergence model empirical distribution training data 
greedy algorithm determines features incrementally added field iterative scaling algorithm estimate optimal values weights 
random field models techniques introduced differ common computer vision literature underlying random fields non markovian large number parameters estimated 
relations learning approaches including decision trees 
demonstration method describe application problem automatic word classification natural language processing 
keywords random field kullback leibler divergence iterative scaling maximum entropy em algorithm statistical learning clustering word morphology natural language processing 
method incrementally constructing random fields 
method builds increasingly complex fields approximate empirical distribution set training examples allowing potential functions features supported increasingly large subgraphs 
feature assigned weight weights trained minimize kullback leibler divergence field empirical distribution training data 
features incrementally added field top greedy algorithm intent capturing salient properties empirical sample allowing generalization new configurations 
general problem methods propose address discovering structure inherent set sample patterns 
fundamental aims statistical inference learning problem central wide range tasks including classification compression prediction 
illustrate nature approach suppose wish automatically characterize spellings words statistical model application develop section 
field features simply uniform distribution ascii strings take distribution string lengths 
conspicuous feature english spellings commonly comprised lower case letters 
induction algorithm observation constructing field 
gammaz gammaz 
indicator function weight gammaz associated feature character lower case chosen approximately 
means string lowercase letter position times stephen vincent della pietra renaissance technologies stony brook ny 
mail com john lafferty computer science department school computer science carnegie mellon university pittsburgh pa 
mail lafferty cs cmu edu string lowercase letter position 
collection strings generated resulting field gibbs sampling 
examples shown sample generated annealing concentrate distribution probable strings 
jz gsr wq vf ga pcp io ijv emx mlj jp ag cy jpd lv ik se hp lb fz xr pk fl second important feature algorithm adjacent lower case characters extremely common 
second order field 
gammaz gammaz gammaz gammaz 
ij gammaz gammaz 
weight gammaz gammaz associated adjacent lower case letters approximately 
features algorithm induces include strings ly ing character string character denotes 
addition features include regular expressions weight weight gamma addition features 
set strings obtained gibbs sampling resulting field shown homes thing ther qut best examples discussed detail section 
induction algorithm parts feature selection parameter estimation 
greediness algorithm arises feature selection 
step feature pool candidate features evaluated estimating reduction kullback leibler divergence result adding feature field 
reduction approximated function single parameter largest value function called gain candidate 
approximation key elements approach making practical evaluate large number candidate features stage induction algorithm 
candidate largest gain added field 
parameter estimation step parameters field estimated iterative scaling algorithm 
algorithm new statistical estimation algorithm ieee transactions pattern analysis machine intelligence vol 
april call improved iterative scaling 
improvement generalized iterative scaling algorithm darroch ratcliff require features sum constant 
improved algorithm easier implement darroch ratcliff algorithm lead increase rate convergence increasing size step taken maximum iteration 
section give simple self contained proof convergence improved algorithm kuhn tucker theorem machinery constrained optimization 
proof rely convergence alternating projection csiszar proof darroch ratcliff procedure 
feature selection step parameter estimation step require solution certain algebraic equations coefficients determined expectation values respect field 
applications expectations computed exactly involve sum exponentially large number configurations 
true application develop section 
cases possible approximate equations solved monte carlo techniques compute expectations random variables 
application uses gibbs sampling compute expectations resulting equations solved newton method 
method viewed terms principle maximum entropy instructs assume exponential form distributions parameters viewed lagrange multipliers 
techniques develop apply exponential models general 
formulate approach terms random fields provides convenient framework main application naturally cast terms 
method differs common applications statistical techniques computer vision natural language processing 
contrast applications computer vision involve free parameters typical application method involves estimation thousands free parameters 
addition methods apply general exponential models random fields underlying markov assumption 
contrast statistical techniques common natural language processing typical applications method probabilistic finite state push automaton statistical model built 
section describe form random field models considered general learning algorithm 
section discuss feature selection step algorithm briefly address cases equations need estimated monte carlo methods 
section improved iterative scaling algorithm estimating parameters prove convergence algorithm 
section application inducing features spellings section discuss relation methods learning approaches possible extensions method 
ii 
learning paradigm section basic algorithm building random field elementary features 
basic idea incrementally construct increasingly detailed field approximate distribution typically distribution obtained empirical distribution set training examples 
establishing notation defining form random field models consider training problem statement equivalent optimization problems 
discuss notions candidate feature gain candidate 
give statement induction algorithm 
form random field models finite graph vertex set edge set finite alphabet 
configuration space set labelings vertices letters ae configuration denotes configuration restricted random field probability distribution set random fields simplex probability distributions support written supp smallest vertex subset ae having property 


consider random fields gibbs distributions form 
vc 
vc functions supp field markov vc clique totally connected subset property expressed terms conditional probabilities arbitrary vertices 
assume path connected subset vc 

delta 

say values parameters field functions features field 
convenient notation disregards dependence features parameters vertex subset expressing field form 

deltaf 
random field form field markovian obtained completing edge set ensure subgraph generated vertex subset supp totally connected 
impose constraint parameters say parameters tied 
tied write 


non binary feature 
general collapse number tied parameters single parameter della pietra della pietra lafferty inducing features random fields associated non binary feature 
having tied parameters natural particular problem presence nonbinary features generally estimation parameters difficult 
automorphism oe graph permutation vertices takes edges edges oev random field said homogeneous features feature automorphism oe graph feature oe 

addition field said homogeneous 
roughly speaking homogeneous feature contributes weight distribution matter graph appears 
homogeneous features arise naturally application section 
methods describe apply exponential models general essential underlying graph structure 
convenient express approach terms random field models described 
optimization problems suppose initial model distribution set features fn 
practice case empirical distribution set training samples 



ffi 
number times configuration appears training samples 
wish construct probability distribution accounts data sense approximates deviate far measure distance probability distributions kullback leibler divergence 
log 

notation 

expectation function respect probability distribution function distribution notation ffi denote generalized gibbs distribution 
ffi 


note usual partition function 
normalization constant determined requirement ffi 
sums written expectation natural sets probability distributions determined data set distributions agree expected value feature function fp second set generalized gibbs distributions feature function delta ffi denote closure respect topology inherits subset euclidean space 
natural criteria choosing element sets ffl maximum likelihood gibbs distribution 
choose distribution maximum likelihood respect ml arg min ffl maximum entropy constrained distribution 
choose distribution maximum entropy relative arg min criteria different determine distribution ml 
distribution unique element intersection discuss detail section appendix empirical distribution set training examples minimizing equivalent maximizing probability field assigns training data 


gamman kp sufficiently parameters simple matter construct field arbitrarily small 
classic problem training 
idea method proposed incrementally construct field captures salient properties incorporating increasingly detailed collection features allowing generalization new configurations resulting distributions absolutely continuous respect empirical distribution training sample 
maximum entropy framework parameter estimation training problem basic problem remains scope 
random field induction paradigm 
inducing field interactions supposing set atomic features atomic ae fg gamma 
supp supported single vertex 
atomic features incrementally build complicated features 
definition specifies shall allow field incrementally constructed induced 
ieee transactions pattern analysis machine intelligence vol 
april definition suppose field delta ffi features called active features feature candidate atomic form 

atomic feature active feature supp psi supp set candidate features denoted 
words candidate features obtained conjoining atomic features existing features 
condition supports ensures feature supported path connected subset candidate feature call parameter family random fields ffg ffg ffi induction define ff gamma ffg think ff improvement feature brings model weight ff 
show section ff convex ff 
suggestive notation convex convex place mnemonic concave convex terminology 
define greatest improvement feature give model keeping features parameters fixed sup ff ff refer gain candidate incremental construction random fields describe algorithm incrementally constructing fields 
field induction algorithm 
initial data distribution initial model output field active features arg min 
algorithm set 
candidate compute gain 
fn arg max feature largest gain 
compute arg min 
set go step 
induction algorithm parts feature selection parameter estimation 
feature selection carried steps feature yielding largest gain incorporated model 
parameter estimation carried step parameters adjusted best represent distribution 
computations discussed detail sections 
iii 
feature selection feature selection step induction algorithm approximation 
approximate improvement due adding single candidate feature measured reduction kullback leibler divergence adjusting weight candidate keeping parameters field fixed 
general estimate may adding feature require significant adjustments parameters new model 
computational perspective approximating improvement way enable simultaneous evaluation thousands candidate features algorithm practical 
section explain feature selection step detail 
proposition ff defined approximate improvement obtained adding feature parameter ff field constant ff strictly convex ff attains maximum unique point ff satisfying ffg proof definition kullback leibler divergence write ff 
log gamma ffg ffg 



gamma ffg 
gamma log theta ffg delta ff gamma log theta ffg ff ff gamma ge ffg ffg gamma ffg ff ff ge ffg ffg gamma ffg ffg gammaq ffg gamma gamma ffg delta ff ff ff convex ff 
constant ff ff minus variance respect ffg strictly negative ff strictly convex 
binary valued gain expressed particularly nice form 
stated proposition proof simple calculation 
proposition suppose candidate binary valued 
ff maximized ff log gamma gamma value ff della pietra della pietra lafferty inducing features random fields bernoulli random variables gamma gamma features binary valued take values non negative integers parameter ff solves maximizes ff general determined closed form 
case tied binary features applies application describe section 
cases convenient rewrite slightly 
fi ff ff fi fi 

ffi total probability assigned event feature takes value fi fi log fi gamma fi fi equation lends numerical solution 
general shape curve fi 
fi fi log fi shown 
fig 

derivative gain limiting value fi log fi fi fi gamma solution equation newton method practice converges rapidly functions 
configuration space large coefficients calculated summing configurations monte carlo techniques may estimate 
important emphasize set random configurations estimate coefficients candidate simultaneously 
discuss details monte carlo techniques problem refer extensive literature topic 
obtained results standard technique gibbs sampling problem describe section 
iv 
parameter estimation section algorithm selecting parameters associated features random field 
algorithm generalization generalized iterative scaling algorithm darroch ratcliff 
reduces darroch ratcliff algorithm features sum constant new algorithm restriction 
section hold set features initial model distribution fixed simplify notation accordingly 
particular write fl ffi fl delta ffi fl assume 


condition commonly written equivalent pk 
description algorithm requires additional piece notation 


features binary 
total number features configuration 
improved iterative scaling 
initial data distribution initial model non negative features fn output distribution arg min algorithm set 
fl gamma unique solution fl set fl ffi 
converged set terminate 
go step 
words algorithm constructs distribution lim fl ffi fl fl fl determined solution equation 

fl 


th iteration field induction algorithm candidate feature fn added field choose initial distribution ffg ff parameter maximizes gain practice provides starting point iterative scaling 
fact view distribution result applying iteration iterative proportional fitting procedure project ffg linear family distributions marginals constrained 
main result section proposition suppose sequence determined improved iterative scaling algorithm 
decreases monotonically converges arg min arg min 
remainder section self contained proof convergence algorithm 
key idea proof express incremental step algorithm terms auxiliary function bounds log likelihood objective function 
technique standard means analyzing em algorithm previously applied iterative scaling 
analysis iterative scaling different simpler previous treatments 
particular contrast csiszar proof darroch ratcliff ieee transactions pattern analysis machine intelligence vol 
april procedure proof rely convergence alternating projection 
formulating basic duality theorem states maximum likelihood problem gibbs distribution maximum entropy problem subject linear constraints solution 
turn task computing solution 
introducing auxiliary functions general setting apply method prove convergence improved iterative scaling algorithm 
finish section discussing monte carlo methods estimating equations size configuration space prevents explicit calculation feature expectations 
duality duality maximum likelihood maximum entropy problems expressed proposition 
proposition suppose exists unique satisfying arg min arg min 
properties determines uniquely 
result known quite packaging 
language constrained optimization expresses fact maximum likelihood problem gibbs distributions convex dual maximum entropy problem linear constraints 
property called pythagorean property resembles pythagorean theorem imagine square euclidean distance vertices right triangle 
include proof result appendix self contained carefully address technical issues arising fact closed 
proposition true replaced fact empty 
proof elementary rely kuhn tucker theorem machinery constrained optimization 
auxiliary functions turn task computing 
fix log likelihood objective function gammad definition function theta auxiliary function fl fl ffi fl fl continuous fl dt tfl dt tfl ffi auxiliary function construct iterative algorithm maximizing start recursively define fl ffi fl arg max fl fl clear property definition step procedure increases proposition implies fact sequence reach maximum proposition suppose sequence fl ffi fl satisfies fl sup fl fl increases monotonically max converges arg max 
equation assumes supremum sup fl fl achieved finite fl 
appendix slightly stronger assumptions extension allows components fl take value gamma 
proposition construct practical algorithm determine auxiliary function fl fl satisfying required condition determined efficiently 
section choice auxiliary function yields improved iterative scaling updates 
prove proposition prove lemmas 
lemma cluster point fl fl proof sub sequence converging fl fl fl gamma gamma inequality follows property fl nk second third inequalities consequence monotonicity 
lemma follows limits fact continuous 
lemma cluster point dt tfl ffi fl proof previous lemma fl fl 
means fl maximum fl dt tfl dt tfl ffi lemma sequence cluster point converges proof suppose 
exists open set containing subsequence nk compact nk cluster point contradicts assumption fq unique cluster point 
della pietra della pietra lafferty inducing features random fields proof proposition suppose cluster point follows lemma dt tfl ffi lemma appendix point proposition 
follows lemma converges 
appendix prove extension proposition allows components fl equal gamma 
extension assume components feature function non negative 

practical restriction replace gamma min 

improved iterative scaling prove monotonicity convergence improved iterative scaling algorithm applying proposition particular choice auxiliary function 
assume component feature function non negative 
fl define fl fl delta gamma 
fl 


easy check extends continuous function gamma theta lemma fl extended auxiliary function 
key ingredient proof lemma convexity logarithm convexity exponential expressed inequalities ff ff log gamma proof lemma extends continuous function gamma theta suffices prove satisfies properties definition 
prove property note fl ffi gamma fl delta gamma log 
fl deltaf 
fl delta gamma 
fl deltaf 
fl delta gamma 
fl 
fl equality simple calculation 
inequality follows inequality 
inequality follows definition jensen inequality 
property definition straightforward verify 
proposition follows immediately lemma extended proposition 
easy check fl defined proposition achieves maximum fl satisfies condition proposition appendix monte carlo methods improved iterative scaling algorithm described previous section suited numerical techniques features take non negative values 
iteration algorithm necessary solve polynomial equation feature express equation form fi largest value 



ffi gamma field th iteration fi fl equation solution precisely 
efficiently solved newton method coefficients non negative 
monte carlo methods configuration space large coefficients simultaneously estimated generating single set samples distribution application word morphology word clustering algorithms useful natural language processing tasks 
algorithm called mutual information clustering construction simple bigram language models maximum likelihood criterion :10.1.1.13.9919
algorithm gives hierarchical binary classification words variety purposes including construction decision tree language parsing models sense disambiguation machine translation 
fundamental shortcoming mutual information word clustering algorithm takes fundamental word spellings :10.1.1.13.9919
increases severity problem small counts virtually statistical learning algorithm 
example word appears word corpus collect bigrams clustering experiments described :10.1.1.13.9919
clearly insufficient evidence base statistical clustering decision 
basic motivation feature approach querying features spellings clustering algorithm notice word begins capital letter ends ism contains ian profit features words similar contexts 
section describe applied random field induction algorithm discover morphological features words sample results 
application demonstrates technique gradually probability mass enormous set possible configurations case ascii strings set configurations increasingly similar training sample 
achieves introducing positive features training samples exhibit negative features appear sample appear rarely 
description resulting features ieee transactions pattern analysis machine intelligence vol 
april improve mutual information clustering scope refer reader detailed treatment topic :10.1.1.13.9919
section formulate problem terms notation results sections 
section describe field induction algorithm carried application 
section explain results induction algorithm presenting series examples 
problem formulation discover features spellings take configuration space set strings ascii alphabet construct probability distribution 
predicting length predicting actual spelling 

length distribution spelling distribution 
take length distribution 
model spelling distribution delta strings length random field 
configuration space ascii strings length ascii character 
reduce number parameters tie features described section feature weight independent appears string 
natural view graph regular gon 
group automorphisms graph set rotations resulting field homogeneous defined section 
field homogeneous addition tie features fields different values weight feature independent introduce dependence length feature applies string adopt artificial construction 
take graph gon gon label distinguished vertex length keeping label held fixed 
complete description fields induced need specify set atomic features 
atomic features allow fall types 
type class features form 

ascii character denotes arbitrary character position string 
second type atomic features involve special vertex carries length string 
features 

atomic feature introduces dependence string characters lies string atomic features introduce dependence length string 
tie length dependence long strings introduce atomic feature strings length greater 
final type atomic feature asks character lies sets denoting arbitrary lowercase letters uppercase letters digits punctuation 
example atomic feature 
tests character lowercase 
illustrate notation suppose features active field ends ism string characters capital letter contains ian 
probability word ian ism parameters appropriate features characters denote string common regular expression notation 
notation means string characters begins capital letter corresponding feature adjacent positions string recalling definition require support feature connected subgraph 
similarly ism means ends ism corresponds feature fw adjacent positions string ian means contains ian corresponding feature fw description algorithm random field induction algorithm model assigns uniform strings 
incrementally add features random field model order minimize kullback leibler divergence field unigram distribution vocabulary obtained training corpus 
length distribution taken lengths words empirical distribution training data 
improvement model candidate feature evaluated reduction relative entropy respect unigram distribution adding new feature yields keeping parameters model fixed 
learning algorithm incrementally constructs random field describe features spellings informative 
stage induction algorithm set candidate features constructed 
fields homogeneous set candidate features viewed follows 
active feature expressed form 
substring appears string extended alphabet ascii characters macros della pietra della pietra lafferty inducing features random fields length labels ff set active features including ffl empty string representation set candidate features precisely set ff deltas deltaa delta denotes concatenation strings 
required definition candidate increases support active feature single adjacent vertex 
model assigns probability arbitrary word strings partition function computed exactly smallest string lengths compute feature expectations random sampling algorithm 
specifically gibbs sampler generate spellings random lengths 
computing gain candidate feature spellings estimate probability candidate feature occurs times spelling see equation example feature occurs times string solve corresponding fi newton method candidate feature 
emphasized single set random spellings needs generated set estimate candidate adding best candidate field feature weights improved iterative scaling algorithm 
carry algorithm random spellings generated time incorporating new feature yielding monte carlo estimates coefficients recall expected number times feature appears substring representation homogeneous features string total active features see equation 
estimates coefficients newton method solve equation complete single iteration iterative scaling algorithm 
convergence kullback leibler divergence inductive step complete new set candidate features considered 
sample results began uniform field field features 
field ascii strings length equally lengths drawn fixed distribution 
sample strings drawn distribution mo zp tll ks cm tl tc os lw lj du lew soc ade np oh cof cr mv vz nu xj fx cy hu comes surprise feature induction algorithm chooses simply observes characters lowercase 
maximum likelihood maximum entropy weight feature fi 
means string lowercase letter position times string lowercase letter position 
draw strings new distribution annealing concentrate distribution probable strings obtain spellings primarily lowercase letters certainly resemble english words jz gsr wq vf ga pcp io ijv emx mlj jp ag cy jpd lv ik se hp lb fz xr pk fl table show features algorithm induced associated parameters 
things worth noticing 
second feature chosen denotes adjacent lowercase characters 
third feature added letter common letter 
weight feature fi 
feature introduces dependence length string denotes feature character word lowercase letter 
notice feature small weight corresponding intuition words uncommon 
similarly features uncommon receive small weights 
appearance feature explained fact vocabulary corpus restricted frequent spellings words receive unknown word spelling frequent 
sentence marker appearance spelling 
feature fi feature fi shown spellings obtained gibbs sampling resulting collection fields 
ye ee mp ore tee ee gre om ec ter iu ec vt eets see pi tt eot ke ov inducing features model begins concentrated spellings resemble actual words extent particularly short strings 
point algorithm discovered example common letter word words ed long words ion 
sample features induced appropriate weights shown table 
ed ion ent ant id ieee transactions pattern analysis machine intelligence vol 
april sample features induced shown table randomly generated spellings 
notice example feature appears surprisingly high weight 
due fact string contains digit contain digits 
digits relatively rare general feature assigned small weight 
model lowercase letter followed uppercase letter rare 
ing ed er ity ent qu ex ae ment ies homes thing ther qut best visit state model inducing features describe words 
point model making refined judgements regarding considered word 
appearance features explained fact preparing corpus certain characters assigned special macro strings 
example punctuation characters represented corpus 
sampled spellings demonstrate model point recognized existence macros discerned proper 
ic sys ally qui iz ib ation said comment agents said resting ibm recall ments clearly model learn point compiled significant collection morphological observations traveled long way goal statistically characterizing english spellings 
vi 
extensions relations approaches section briefly discuss relations incremental feature induction algorithm random fields statistical learning paradigms 
possible extensions improvements method 
conditional exponential models carries general setting conditional exponential models including improved iterative scaling algorithm 
general conditional may field features defined binary functions general approach applicable 
feature induction method conditional exponential models demonstrated problems statistical machine translation terms principle maximum entropy :10.1.1.103.7637
decision trees feature induction paradigm bears various methods growing classification regression trees 
decision trees method builds top classification refines features 
decision trees correspond constructing features disjoint support 
explain recall decision tree determines partition context random variable order predict actual class context represented random variable leaf tree corresponds sequence binary features root denotes parent node feature question splits negation fn question asked sibling node 
distribution assigned leaf simply empirical distribution determined training samples theta leaf characterized conjunction features different leaves correspond disjoint support 
contrast feature induction algorithm generally results features overlapping support 
criterion evaluating questions terms amount reduce conditional entropy corresponds criterion maximizing reduction kullback leibler divergence candidate features field modifying induction algorithm way obtain algorithm closely related standard methods growing binary decision trees 
considering parameter family fields determine best candidate af consider parameter family fields features disjoint support improvement obtained adding byg 
general resulting distribution absolutely continuous respect empirical distribution 
random variable take values ym standard decision tree algorithm obtained th stage della pietra della pietra lafferty inducing features random fields add disjoint features ffi fn ffi maximum likelihood training parameters features recovers empirical distribution data node extensions mentioned section approach differs common applications statistical techniques computer vision typical application method involves estimation thousands free parameters 
induction technique may scale large dimensional image problems 
potential difficulty degree polynomials improved iterative scaling algorithm quite large difficult obtain reliable estimates coefficients monte carlo sampling exhibit sufficiently instances desired features 
extent significant problem primarily empirical issue dependent particular domain method applied 
random field induction method definitive possible variations basic theme incrementally construct increasingly detailed exponential model approximate distribution basic technique greedy algorithm course ways improving search set features 
algorithm section respects simple possible general framework 
computationally intensive 
natural modification add top candidates stage 
increase speed induction algorithm potentially result redundancy features top candidates correlated 
modification algorithm add best candidate step carry parameter estimation new features added field 
natural establish bayesian framework prior distribution features parameters incorporated 
enable principled approach deciding feature induction complete 
natural class conjugate priors class exponential models problem incorporating prior knowledge set features challenging 
appendix duality appendix prove proposition restated 
proposition suppose exists unique satisfying arg min arg min 
properties determines uniquely 
proof proposition lemmas 
lemmas state proof 
lemma non negative extended real valued function theta 

strictly convex separately 
lemma map fl 
fl ffi smooth fl theta 
derivative ffi respect dt ffi delta gamma lemma nonempty 
proof define property proposition arg min 
see sense note identically continuous strictly convex function closed attains minimum unique point show closed action ffi definition minimum function ffi 
derivatives respect lemma conclude 
lemma proof straightforward calculation shows gamma gamma delta gamma ffi follows identity continuity gamma gamma lemma follows 
proof proposition choose point exists lemma 
satisfies property definition satisfies property lemma 
consequence property satisfies properties 
check property instance note point 
remains prove properties determines uniquely 
words need show point satisfying properties 
suppose satisfies property 
property 
follows 
satisfies property argument reversed proves 
suppose satisfies property 
ieee transactions pattern analysis machine intelligence vol 
april second equality follows property 

satisfies property similar proof shows 
ii 
dealing appendix prove extension proposition allows components fl equal gamma 
extension assume components feature function non negative 

assumed loss generality replace gamma min 

necessary 
gamma denote partially extended real numbers usual topology 
operations addition exponentiation extend continuously gamma 
open subset gamma theta defined fl fl deltaf 
observe theta dense subset map fl 
fl ffi point defined finite fl extends uniquely continuous map 
condition fl ensures normalization definition fl ffi defined fl finite 
definition call function gamma extended auxiliary function restricted theta ordinary auxiliary function sense definition addition satisfies property definition fl fl finite 
note ordinary auxiliary function extends continuous function extension extended auxiliary function 
extension proposition proposition suppose feature function satisfies non negativity condition suppose extended auxiliary function proposition continues hold condition fl replaced fl fl fl fl proof lemma valid altered condition fl fl satisfies property definition fl consequence lemma valid proof proposition goes change 
iii 
part research carried authors ibm thomas watson research center yorktown heights new york 
stephen della pietra vincent della pietra partially supported arpa 
john lafferty partially supported nsf arpa iri 
almeida variational method estimating parameters mrf complete incomplete data annals applied probability 
moura noncausal gauss fields parameter structure estimation ieee transactions information theory july 
berger della pietra della pietra maximum entropy approach natural language processing computational linguistics :10.1.1.103.7637
breiman friedman olshen stone classification regression trees wadsworth belmont 
brown note approximations discrete probability distributions information control 
brown della pietra de souza lai mercer class gram models natural language computational linguistics :10.1.1.13.9919
brown cocke della pietra della pietra lafferty mercer 
statistical approach machine translation computational linguistics 
iterative technique reconstruction ary images pattern recognition 
csiszar divergence geometry probability distributions minimization problems annals probability 
csiszar geometric interpretation darroch ratcliff generalized iterative scaling annals statistics 
csiszar information geometry alternating minimization procedures statistics decisions supplement issue 
darroch ratcliff generalized iterative scaling log linear models ann 
math 
statist 

dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society 

diaconis conjugate priors exponential families ann 
statist 

ferrari convergence partially parallel gibbs samplers annealing annals applied probability 
hwang younes optimal spectral structure reversible stochastic matrices monte carlo methods simulation markov random fields annals applied probability 
geman geman stochastic relaxation gibbs distributions bayesian restoration images ieee trans 
pattern anal 
machine intell 

thomson carlo maximum likelihood dependent data discussion royal stat 
soc 

jaynes papers probability statistics statistical physics rosenkrantz ed reidel publishing dordrecht holland 
lafferty mercer automatic word classification features spellings proceedings th annual conference university waterloo centre new oed text research oxford university press oxford england 
potamianos partition function estimation gibbs random field images monte carlo simulations ieee transactions information theory july 
younes estimation annealing fields ann 
inst 
poincare probab 
statist 

stephen della pietra received physics princeton university ph physics harvard university 
post doctoral fellow university texas austin 
member mathematics department institute advanced study princeton nj 
research staff member ibm thomas watson research center yorktown heights hawthorne ny project leader natural language understanding group 
primary research ibm machine translation natural language understanding 
working statistical methods modeling stock market renaissance technologies stony brook ny 
della pietra della pietra lafferty inducing features random fields vincent della pietra received mathematics princeton university ph mathematical physics harvard university 
member institute advanced study princeton nj 
research staff member ibm thomas watson research center yorktown heights hawthorne ny project leader machine translation group 
primary research ibm machine translation natural language understanding 
working statistical methods modeling stock market renaissance technologies stony brook ny 
john lafferty studied mathematics undergraduate massachusetts institute technology received ph degree mathematics princeton university member program applied 
assistant professor benjamin pierce lecturer mathematics harvard university visiting assistant professor institute mathematics china 
joined computer sciences department ibm thomas watson research center yorktown heights ny research staff member 
member faculty computer science department carnegie mellon university primary research interests include speech language information theory statistical learning algorithms 
