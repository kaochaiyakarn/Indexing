considering decision cost learning feature weights wolfgang wilke ralph bergmann university kaiserslautern centre learning systems applications lsa department computer science box kaiserslautern germany mail informatik uni kl de 
new algorithm called learning feature weights cbr systems classification 
algorithms known far considers profits correct cost wrong decision 
need algorithm motivated real world applications cost profits decisions play major role 
introduce representation accuracy cost profits decisions define decision cost classification system 
compare accuracy optimization cost optimization tested optimizes classification accuracy conjugate gradient algorithm 
second optimizes decision cost cbr system respecting cost profits classifications 
experiments algorithms real application demonstrate usefulness approach 
developing case reasoning system real world application requires lot effort 
example features selected represent cases types features defined similarity measure instances attributes selected expert 
step task development process deals determination feature weights 
weights acquired expert determined learning algorithm tries extract importance features set cases 
learning algorithms required natural weighting domain weights expert need improvement 
different learning algorithms feature weights 
focus algorithms feedback learning 
example salzberg salzberg proceed follows correct classification occurs weights matching features incremented mismatching features decremented new query fixed amount 
incorrect classification occurs matching feature weights decremented mismatching feature weights incremented 
approaches vdm stanfill waltz ib aha relief kononenko wess wess vsm lowe gamma nn vsm wettschereck wettschereck aha differ way feature weights modified share main criterion changing weights correctness classification 
consequence algorithms optimize classification accuracy 
argue appropriate learning goal real world applications decision cost play major role domains 
view problem motivated experiences applications credit scoring goal task decide business bank customer 
available cases consist main balance items balance sheet financial index numbers relevant enterprise data credit rating 
cbr system classifies new business customers decide enterprise class class 
application profits cost right wrong decision asymmetric 
system classifies customer class class bank rejects credit customer bank looses interest income 
hand class customer classified member class bank looses credit sum considerably higher loss 
decision cost wrong decision depends strongly predicted correct class customer 
diagnosing cases poisoning project althoff built cbr system diagnosing cases poisoning 
cases consist attributes identified useful diagnosis task experts 
decision classes describe different kinds possible therapies treating poisoning 
course different therapies cause different effects patient constitution 
worst case wrong therapy mortal patient 
important come right diagnosis quick possible 
number diagnoses respective therapy identical similar 
making wrong diagnosis leading nearly therapy correct diagnosis problem 
application decision costs differ significantly 
examples show decision cost may play major role optimization cbr system 
purpose developed feature weight learning algorithm knn cost optimizes feature weights regard cost profits decisions case classification system 
compare knn cost algorithm optimizes classification accuracy introduce knn acc learning algorithm uses mechanism application data provided toxicology information advisory centre russian federation ministry health medical industry knn cost respect accuracy optimization criterion 
organized follows section give short neighbor classification illustrate definition accuracy cost classification system 
section introduces algorithms knn acc knn cost describe empirical evaluation algorithms credit scoring application section 
add discussion give outlook 
basics case classification neighbors cbr system classification tasks case fn consists describing features desired class set ft denotes possible classes domain 
case base cb defined set known cases past 
new query fq ng case base cb similar cases retrieved predict real class query similarity sim query case case base defined sim sim weight feature sim local similarity measure attribute assume sim features yielding similarity query case sim 
cbr system predicts class query retrieving neighbors fr applying majority vote method denote probability query member class defined ffi sim sim ffi defined follows ffi ae denotes class case prediction cbr system class highest probability calculated set neighbors 
voting algorithms neighbor classification numerator definition pq pq set 
transformation error function function similarities squared 
example single majority vote michie pp weiss kulikowski considers frequencies different classes set nearest neighbors 
weighted majority vote main advantage distance neighbors taken account prediction 
accuracy decision cost common representation classification accuracy system confusion matrix weiss kulikowski 
set cases entry matrix counts number classifications cases class member predicted class system set cases 
predicted class correct class table 
sample confusion matrix credit scoring application approach modify entries confusion matrix respect probability equation classify query member class matrix contains probabilities possible decision system set cases 
measuring accuracy cbr system learning leave test weiss kulikowski training set case query classified cbr system including cases query query probabilities retrieved nearest neighbors equation calculated done case resulting probabilities combined entry confusion matrix 
normalize row confusion matrix respect occurrences correct classes training set 
resulting confusion matrix represents probabilities different outcomes classification training set matrix approximated confusion matrix probabilities decisions counting occurrence different decisions table shows example matrix credit scoring application different classes predictions 
probability correct predictions class diagonal matrix percent 
entries represent probabilities errors particular type misclassification percent 
simplification term confusion matrix represent cost weiss kulikowski michie similar matrix called cost matrix non diagonal entries represent cost specific misclassification fields diagonal represent benefits correct classification explicitly set 
extend approach allowing positive negative entries representing cost profits single matrix 
result decision value matrix represents cost profits possible decision system 
table shows example decision value matrix credit scoring application 
values predicted class correct class table 
sample decision value matrix credit scoring application represent relation possible errors occur 
cost misclassifying bad customer times higher misclassifying case bank looses credit volume second case interest rate lost 
consequently profit detecting bad times higher correctly classifying decision value matrix represents cost profits different decisions classification bank application 
representation accuracy confusion matrix cost decision value matrix define decision cost classification system cost decision probability confusion matrix corresponding entries decision value matrix 
definition akin utility decisions bayesian decision theory berger 
incremental learning feature weights section describe general algorithm learning feature weights conjugate gradient algorithm 
subsections specialize algorithm algorithms knn acc knn cost see relation entries cb ca table 
see entries cb ca table 
learning weights guided conjugate gradient conjugate gradient method generate test search algorithm feedback test procedure 
algorithm tries optimize system respect error function adjusting weights fw 
optimization realized iterative search local minimum error function chosen learning goal 
learning rate influence step width learning step direction conjugated gradient 
basic algorithm follows 
initialize weight vector learning rate 
compute error initial system 
gamma criterion learning step gamma wa compute 
output weights initialized random values set constant expert 
calculation initial algorithm number learning steps depending criterion 
learning step successful weights modified guide direction local minimum 
learning rate decreased 
algorithm terminates outputs weight vector result learning 
conjugated gradient methods feature weight learning common disciplines backpropagation neural networks 
choice learning rate choice value difficult depends unknown domain properties 
quite small learning steps needed find local minimum neighborhood starting point 
guaranteed algorithm finds respecting initial starting point 
large misleading steps may happen algorithm decrements value improves possible learning step large leads improvement improvement different minimum 
algorithm finds local minimum respecting initial weights 
behavior illustrated 
dotted line shows learning steps lead value large 
solid line shows learning step sufficiently small learning rate 
leads step forward local minimum respects expert initialization 
learning rate large behavior algorithm respecting initialization founded minimum related weights expert 
choice criterion criterion ensure termination algorithm 
possible criteria algorithm fixed number learning steps expert weight initialization lambda sufficient lambda large fig 

effects different values learning rate minimal change error function gamma ffl minimal change feature weights wa ffl minimal learning rate ffl possible combine criteria 
choice different value amount nearest neighbors taken account leads different probabilities classifications 
optimal value computed simple generate test procedure 
optimal means minimal value decision cost equation 
fixed experience domain learning procedure vsm lowe 
wettschereck aha calculate optimal prior learning step fix rest learning procedure 
computational expensive approach calculates new optimal learning step 
necessary general new weights learning step effect optimality old value new calculation optimal step costly lead best results 
knn acc optimizing classification accuracy specialize basic algorithm conjugated gradient knn acc optimizes classification accuracy 
done defining special error functions purpose required derivation wa learning leave test described section calculate value 
knn acc similar vsm lowe gamma nn vsm wettschereck aha algorithms 
determinate feature weights conjugate gradient algorithm 
approaches similarity measure quantify distance cases 
measure fixed different types features 
error function acc optimizing classification accuracy acc defined acc cb ffi gamma learning step conjugate gradient method need derivation acc respect weights equation acc gamma cb ffi gamma derivation weights ffi gamma sim sim sim knn acc algorithm conjugate gradient algorithm error function derivation 
minimizing error function means maximizing probabilities prediction correct classes minimizing probabilities misclassification training set 
confusion matrix see table knn acc tries maximize diagonal minimizes non diagonal entries 
knn cost optimizing decision cost idea knn cost minimize decision cost defined equation 
done integrating decision value matrix table error function 
probability error function judged respective cost decision value matrix 
resulting error function cost defines error respective decision cost cost cb sgn entry decision value matrix sgn sign entry 
derivation wa calculate new weights computed follows cost cb sgn pq wa formula equation 
algorithm knn cost conjugate gradient algorithm part section error function cost derivation wa error function please note equations similar shown wettschereck replacement distances similarities algorithm minimizes decision cost cbr system 
representation accuracy cost means knn cost tries minimize maximize products entry confusion matrix corresponding entry decision value matrix depending cost profits empirical evaluation knn acc knn cost results empirical evaluation knn acc knn cost algorithms implemented part cbr shell 
goal tests verify reject hypothesis classification weights learned knn cost leads lower decision cost classification weights learned knn acc experimental settings experiments credit scoring domain cases 
case consists different features class description 
numeric attributes remaining symbolic values 
percent attributes unknown case description 
test learning algorithms independent runs 
run divided case base training set percent randomly selected cases remaining percent test set 
run training set case base cbr system weights initialized randomly 
run learned feature weights knn acc knn cost algorithms 
learning process stopped change gamma occured 
initial learning rate set gamma tests took nearest neighbors account 
results compare decision cost initial cbr systems resulting systems learning feature weights algorithms 
show calculate decision cost 
classify cases test set initial cbr systems 
left side table shows confusion matrix classification accuracy averaged initial systems 
contrary matrix introduced table entries denote average occurrence classifications cases test set 
choose different representation classifications entailed cost equal minimizing decision cost computed sum single products 
squares equation introduced speed learning process 
esprit contract project partners prime contractor france germany irish medical systems ireland university kaiserslautern germany name systems initial cbr systems predicted class accuracy cost correct class decision cost table 
average decision cost cbr systems learning systems explicit 
entries multiplied respective cost cost matrix table 
resulting cost possible decision shown right side table 
bottom line table shows average decision cost initial systems result sum entries decision value matrix comparing knn acc knn cost compare initial systems systems learning feature weights knn acc knn cost learning phase test resulting systems test set 
learning algorithms calculate average decision cost resulting systems 
table summarizes entire decision cost initial systems resulting systems learning 
learning algorithms lead decision cost learning learning learning table 
decision cost cbr systems learning feature weights improvement decision cost 
average decision cost systems weights obtained knn acc gamma lower decision cost learning 
associated profits right cost wrong decisions 
improvement represents benefit better classification accuracy learning 
weights extracted knn cost lead improvement gamma decision cost 
especially decrease decision cost gamma obtained knn cost compared results obtained knn acc remarkable 
negative value denotes profit classification systems classification accuracy systems weights learned knn acc weights learned knn cost nearly 
reason improvement decision cost knn cost prefers classify costly cases correctly knn acc hypothesis classification weights learned knn cost leads lower decision cost classification weights learned knn acc empirically verified domain 
summary discussion feature weight learning algorithms optimizing classification accuracy optimizing decision cost cbr system 
algorithms conjugate gradient optimize feature weights different criteria 
empirically evaluated algorithms domain credit scoring real bank customer data 
evaluation verify hypothesis cost optimization profitable classification accuracy optimization 
pazzani algorithms proposed optimize cost decision lists decision trees rule expert systems 
discussion focus algorithms integrate cost cbr system 
possible integrate cost algorithms learning feature weights 
especially extension algorithms feedback easy cases 
feedback denotes amount change weight learning step improve system 
stated salzberg changes feature weights fixed value depending correct incorrect classification occured feature matches mismatches 
introduce cost value vary decision system 
disadvantages sensitive order presentation examples weights simultaneously changed fixed amount learning step 
relief kira rendell selects random training case similar case class similar case different class 
new feature weights calculated gamma difference difference approach introduce decision cost feedback relief equation judge differences desired cost training case classified class similar case class original version relief limited class problems restriction removed kononenko relief 
takes different classes feedback account 
extension decision cost quite similar 
problem algorithms sensitivity order presentation examples 
ib aha aha takes distribution different classes case base account changing weights 
amount change judged factor gamma delta represents observed frequency different classes 
promising approach optimizing classification accuracy 
introduce decision cost approach difficult weights optimized conflicting criteria 
costly class low frequency benefits criteria disappear 
cost criterion argues massive change feature weights frequency criterion argues moderate modification 
factor change highly frequent class low cost contrary 
feedback criteria compensate criteria accuracy cost stated contradictory 
learning algorithms improving cbr systems modified order take decision cost account 
example instance learning algorithm ibl aha aha extended 
roughly speaking ibl cases rated effects arbitrary classifications misclassifications 
cases cause wrong classifications seldom ensure right classification removed case base 
goal keep cases case base ensure correct classification 
integrate cost possible rate cases classification rating respective entry cost matrix 
topic research area 
price approach decision cost acquired additionally 
experts know cost profits different outcomes classification 
authors prof michael richter dr klaus dieter althoff harald useful discussions implementation 
partially funded european communities esprit contract ii information knowledge reengineering reasoning cases project partners prime contractor france daimler benz germany germany irish medical systems ireland university kaiserslautern germany partially funded innovation fur 
aha 

incremental instance learning graded concepts 
proceedings th international workshop machine learning pages 
aha 

study instance algorithms supervised learning 
phd thesis university california irvine 
aha 

case learning algorithms 
bareiss editor proceedings cbr workshop pages 
morgan kaufmann publishers 
althoff bergmann wess 

integration induction casebased reasoning critical decision support tasks medical domains approach 
centre learning systems applications technical report 
berger 

statistical decision theory bayesian analysis 
springer verlag 
kira rendell 

practical approach feature selection 
proccedings ninth international conference machine learning aberdeen scotland 
morgan kaufmann 
kononenko 

estimating attributes analysis extensions relief 
proceedings european conference machine learning pages 
springer verlag 
lowe 

similarity metric learning variable kernel classifier 
neural computation 
michie spiegelhalter taylor 

machine learning neural statistical classification 
ellis horwood 
pazzani murphy ali hume brunk 

reducing misclassification costs 
proccedings th international conference machine learning pages 
morgan kaufmann 
hinton williams 

learning internal representations error propagation chapter pages 
mit press 
salzberg 

nearest hyperrectangle learning method 
machine learning 
stanfill waltz 

memory reasoning 
communications acm 
weiss kulikowski 

computer systems learn classification prediction methods statistics neural nets machine learning expert systems 
morgan kaufmann 
wess 

patdex und von der 

deutsche xps hamburg 
springer verlag 
wess 

systemen zur und 
phd thesis university kaiserslautern 
wettschereck 

study distance bases machine learning algorithms 
phd thesis oregon state university 
wettschereck aha 

weighting features 
veloso aamodt editors case reasoning research development pages 
springer 
article processed macro package llncs style 
