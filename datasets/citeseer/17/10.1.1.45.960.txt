tree structured neural network real time adaptive control alois heinz institut fur informatik universitat freiburg am freiburg germany email heinz informatik uni freiburg de tree structured neural network tsnn described meets special requirements real time adaptive modeling control 
shown tsnn capable arbitrarily accurate approximation function derivatives certain conditions 
evaluation tsnn function jacobian extremely efficient due usage lazy evaluation function inversion 
tsnn constructed learned training data derived line adaptable version linear weight vector allows application appropriate theories prove convergence stability 
real time adaptation implemented efficiently consequence lazy evaluation scheme 
demand intelligent controllers consumer products industrial applications permanently increasing just autonomy inherent built intelligence systems increases 
modern controllers cope complex nonlinear time varying processes due unpredictable changes environment disturbances process 
increased controller flexibility required compensate unavailable prior knowledge process reduce design costs 
sophisticated controllers need self organizing able learn environment 
suppose control architecture predictive model plant maintained practical evidence control signal computed measured values desired plant response inverse plant model derived help jacobian plant model 
details different neural fuzzy logic control architectures reader may refer 
requirements real time adaptive modeling divided representational learning quality issues 
model flexible able approximate function practical importance 
able incorporate prior expert knowledge sparse coding scheme local generalization 
evaluation model jacobian line adaptation extremely efficient 
desirable model function linear adjustable variables allows easy convergence stability analysis 
may difficult meet mentioned requirements easy way propose strategy tree structured neural networks tsnn appears practical situations topology initial parameters tsnn created initialized available prior knowledge representative training data set constructive learning algorithm 
phase network receptive fields evolved shaped 
operating mode topology certain sensitivity parameters frozen tsnn remains adaptable wide range possible functions 
subject convergence stability analysis linear dependence remaining adjustable parameters 
section give short tree structured neural networks explain characteristic modeling abilities explain arithmetic operations applied show tsnn universal approximators functions derivatives certain conditions 
section describe efficient algorithms evaluation tsnn function jacobian line adaptation 
derive estimates algorithm average case runtimes logarithmic root network size 
simple example demonstrates representational learning abilities tsnn section 
section contains 
modeling abilities tsnn simplicity due limited space discussion tsnn restricted real functions real arguments networks extended easily represent category vector valued functions numerical symbolic arguments 
restricted tsnn input dimension labeled binary tree 
inner node equipped weight vector threshold positive radius leaf labeled real value decision function ir ir associated inner node oe gamma gamma delta oe ir sigmoid function oe ae sign jxj gamma jxj 
evaluation node respect input vector recursively defined ae leaf gamma qr gamma delta theta denote left right descendants respectively 
easy see min qr max qr 
evaluation tree respect defined evaluation root root come closed formula define indicator gamma leaf belongs left right sub tree inner node respectively undefined 
decision contribution pair respect defined gamma 
evaluation respect rewritten leafs vl dl dl defined dl ancestors 
easy realize leafs dl ir proof replace leaf values show nodes evaluate 
set fdl leafs set kernel functions form partition unity 
radii tsnn close zero resulting function resembles piece wise constant functions usual decision trees 
positive radii dl individually shaped basis function 
ll consider arithmetic representational properties tsnn functions 
lemma sum tsnn functions tsnn function 
proof consider tsnn construct follows take replace leaf copy leaf values increased leafs vl dl leafs vl dl leafs leafs vl dl dl leafs leafs vl dl dl leafs leafs dl dl lemma product tsnn functions tsnn function 
proof tsnn construct theta follows take replace leaf copy leaf values multiplied delta leafs dl delta leafs dl leafs leafs vl vl dl dl theta theorem continuous function ir ae ir compact uniformly approximated sequence tsnn desired accuracy 
proof theorem direct consequence stone theorem follows lemmas additionally consider constants tsnn functions tsnn functions different values pair distinct points 
theorem ir ae ir compact continuous function continuous derivatives 
tsnn capable simultaneous uniform approximation derivatives 
sketch proof tsnn built decomposes dimensional grid 
leaf belongs small hyper rectangle hl ae dl implies hl leaf replaced tsnn tl inner node leafs function approximates hyperplane hl value derivatives center hl increasing radii half size grid width smoothes tsnn function 
decreasing grid width leads better approximation 
tsnn seen constructed expert designers local decomposition arithmetic operators sum product 
tsnn constructed automatically topology enhancing parameter modifying procedure training set 
case height restricted logarithmic function tree size 
efficient parallel evaluation tsnn transformed usual feed forward neural network layers 
input vector fed input layer hidden layer containing copies inner nodes 
evaluations fed layer holding copies leafs compute values dl transmit vl weighted connections linear output element 
product operations elements replaced sums logarithmic exponential transformations transfer functions second layer respectively 
efficient tsnn evaluation adaptation efficient evaluation modeled function jacobian extremely important especially control applications emphasized 
basis efficient tsnn evaluation particular 
fact oe different interval gamma allows effective lazy evaluation 
evaluation node descendant nodes 
evaluation algorithm upper part fig 

derive upper bound estimate average case runtime function number tree nodes assume uniform probability event 
vector node height evaluated probability gamma delta value decreases increasing may assume loss generality tree smallest possible height dlog gamma 
shown average case number tree nodes inspected evaluation tree dlog gamma log log log eval proc node vector real leaf return fi oe gamma return eval elif return eval return gamma theta eval theta eval fi proc node vector leaf return fi gamma oe return elif return return gamma theta theta gamma theta theta gamma theta theta fi procedure eval evaluates tsnn function recursively 
procedure operates fashion returns dimensional vector tsnn function value partial derivatives respect input values 
components vector numbered operator denote sequence 
algorithms take advantage lazy evaluation 
log gamma average case runtime order root tree size 
example average case evaluation runtime proportional square root number tree nodes easy see linear logarithmic 
jacobian real valued function consists vector partial derivatives respect input variables 
derivative node evaluation respect leaf zero inner node gamma qr qr gamma gamma oe ae jxj gamma jxj 
note zero delta zero computation derivative requires inspection descendant 
case descendant evaluations derivatives needed 
efficient way compute jacobian tsnn function recursive procedure computes returns vector node function value derivatives lazy evaluation 
algorithm derived pure evaluation algorithm methods algorithmic differentiation lower part fig 

algorithm visits nodes manner pure evaluation algorithm performs constant number operations node average case runtime order 
opposed tree construction algorithms line adaptation algorithm leaf values tsnn 
tsnn adapted computed value differs observed probabilities observed experiments ranged maximal 
learned sine function leafs radii splits retrained leafs radii retrained leafs retrained sin cos cos leafs radii splits retrained leafs radii retrained leafs retrained learned sine function function functions retrained complete input space functions retrained half interval parameters retrained radii leaf values retrained leaf values retrained 
value 
gradient squared error respect leaf value vl gamma dl 
leaf nodes visited evaluation may need adaptation 
line gradient descent adaptation algorithm puts pointers leafs visited evaluation phase list modifies values error computed 
fewer leaf nodes visited nodes average case runtime line adaptation algorithm order evaluation algorithm 
simple tsnn example describe simple experiment demonstrate adaptation abilities tsnn 
construction algorithm build tsnn training data originated sine function interval 
tsnn inner nodes constructed function fig 
maximal aberration sine function 
tsnn retrained data function sin cos cos fig 
different conditions 
fig 
shows results adaptation training data total interval fig 
depicts results data half interval 
fig 
tsnn parameters fig 
radii leaf values fig 
leaf values 
experiment shows adaptation flexible parameters subject changes fig 

case tsnn function may altered large extend regions new data appears fig 
severe drawback 
leaf values adapted tsnn able change function new information available remember learned function regions modifications required fig 

course sensitivity case changed increased 
oscillation increased region line adapted tsnn try interpolate new values possible 
described architecture tsnn network shown flexible approximate function derivatives certain practical conditions 
tsnn constructed expert designer application specific knowledge spatial functional decomposition help arithmetic operators 
simpler faster automatic construction process available training data 
described efficient algorithms evaluation tsnn function jacobian line adaptation algorithm 
estimates average case runtime order root network size algorithms 
efficiency direct consequence utilized lazy evaluation scheme tsnn especially applicable real time adaptive modeling control tasks 
line version tsnn network function linear adjustable variables desirable property models control 
parameters controls amplitude individually shaped receptive field input space similar radial basis functions contrast simple radial basis function networks curse dimensionality 
line adaptation tsnn able learn modified functional relation quickly continue remember implemented knowledge regions new evidence 
tsnn suitable real time adaptive modeling control 
bishop 
neural networks pattern recognition 
oxford university press oxford 
breiman friedman olshen stone 
classification regression trees 
wadsworth statistics probability series 
chapman hall new york 
brown harris 
adaptive modelling control 
systems control engineering 
prentice hall international london uk 

second course mathematical analysis 
cambridge university press cambridge england 
heinz 
class constructible neural networks 
fogelman gallinari editors icann proceedings international conference artificial neural networks paris france volume pages paris la france oct 
ec cie 
jager 
fuzzy logic control 
phd thesis technische universiteit delft delft netherlands june 
narendra 
stable adaptive systems 
prentice hall englewood cliffs nj 
quinlan 
programs machine learning 
machine learning 
morgan kaufmann publishers san mateo california 
rall 
automatic differentiation techniques applications volume lecture notes computer science 
springer verlag 
hunt nski murray smith 
review advances neural adaptive control systems 
technical report esprit project tp glasgow university daimler benz research 
