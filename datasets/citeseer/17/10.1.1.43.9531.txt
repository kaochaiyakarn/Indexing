incorporating prior information machine learning creating virtual examples niyogi girosi poggio mit center biological computational learning cambridge ma 
september key problems supervised learning insufficient size training set 
natural way intelligent learner counter problem successfully generalize exploit prior information may available domain learned prototypical examples 
discuss notion prior knowledge creating virtual examples expanding effective training set size 
show contexts idea mathematically equivalent incorporating prior knowledge regularizer suggesting strategy motivated 
process creating virtual examples real world pattern recognition tasks highly non trivial 
provide demonstrative examples object recognition speech recognition illustrate idea 
learning examples machine learning techniques increasingly popular alternative knowledge approaches artificial intelligence problems variety fields 
hope automatic learning examples eliminate need laborious domain specific knowledge task hand 
analyses complexity learning problems suggest hope overly optimistic number examples needed solve problem prohibitive 
clearly middle ground needed useful direction research study incorporate prior world knowledge task learning examples framework 
current deals subject 
providing background problem learning examples usually formulated 
section discuss briefly complexity learning problem absence prior knowledge require large number examples learn 
section introduce idea virtual examples creating additional examples current set examples specific knowledge task hand 
framework similar learning hints abu mostafa emphasis describe specific non trivial transformations allow create virtual examples real world pattern recognition problems 
show section certain function learning contexts framework virtual examples equivalent imposing prior knowledge regularizer 
idea virtual examples ad hoc strategy 
discuss section specific examples computer vision speech recognition 
conclude main points section 
background learning function approximation problem learning examples usefully modeled trying approximate unknown target function pairs consistent function modulo noise 
target function belongs target class functions denoted learner access data set consisting say pairs picks function chosen hypothesis class basis data set 
hope examples drawn learner hypothesis sufficiently close target resulting successful generalization novel unlabelled examples learner encounter 
numerous problems pattern recognition speech vision handwriting finance robotics cast framework research typically focuses different kinds hypothesis classes different ways choosing optimal function class training algorithms 
multilayer perceptrons rumelhart hinton radial basis function networks poggio girosi moody darken decision trees breiman etal correspond different choices hypothesis classes popular learning machines 
similarly different kinds gradient descent schemes backpropagation em algorithm correspond ways choosing optimal function class finite data set 
varying choices hypothesis classes training algorithms profusion learning paradigms emerged 
significant issue interest learning paradigms generalize new unseen data 
section discuss factors generalization performance learning machine depends 
prior information problem sample complexity learning examples system number examples needs collected successful generalization key issue 
sample complexity typically characterized theory vapnik chervonenkis describes general laws probabilistically learning machines obey 
turns learner picks hypothesis basis example set number examples needs order generalize order vc dimension class combinatorial measure complexity hypothesis class 
roughly speaking vc dimension see vapnik details measure different kinds functions example parametric class univariate polynomials degree vc dimension 
case vc dimension related simple way number parameters need true general 
think classes parameters having small vc dimension vice versa 
vc dimension better direct measure learning complexity simply number parameters 
general large complex hypothesis classes accomodate different data sets higher vc dimension smaller restricted hypothesis classes 
see number examples needed proportional vc dimension sense effective size hypothesis class 
consequently interest small hypothesis classes learning machines 
small hypothesis class 
recall target function belongs hypothesis class small choose best function distance target generalization error high 
appreciate point better consider situation learning neural networks squares setting 
recall ideally learn target function expectation respect true probability distribution generating data arg min gamma practice don know true distribution compute true expectation typically minimize class example consider typical situation neural networks learn function draw finite data set pairs construct empirical approximation objective function minimize class neural networks finite number parameters 
collected data points minimized neural network nodes hidden layer say effect computing function arg min hn emp gamma min hn gamma attempt learn function finite amount data points hypothesis class finite number parameters function obtain practice function predict unknown values naturally know function far function true target 
general show generalization error gamma decomposed approximation component estimation component gamma app est approximation error app due finite size hypothesis class 
number hidden nodes increases representational power hypothesis class increases approximation error goes zero 
estimation error est hn due finite amount data available learner 
monotonically decreasing function depends vc dimension hypothesis class amount data 
number hidden nodes increases vc dimension increases consequently estimation error increases keeping data fixed 
approximation error small need large sized networks large estimation error small need small sized networks small 
trade approximation error estimation error arises learning paradigms investigated number different hypothesis classes ranging multilayer perceptrons barron radial basis functions niyogi girosi 
theorem states canonical result radial basis functions fig 
describes generalization error surface function number parameters number data 
theorem niyogi girosi class gaussian radial basis function networks input nodes hidden nodes gamma oe element bessel potential space order class 
assume data set obtained randomly sampling function presence noise noise distribution compact support 
ffi probability greater gamma ffi bound generalization error defined set functions written gm stands convolution operation lp gm bessel macdonald kernel function fourier transform gm ksk holds kf gamma nk ln nl gamma ln ffi 
generalization error number examples number basis functions function 
conclude 
unconstrained hypothesis classes number examples needed low estimation error certainly prohibitive 
hand highly constrained hypothesis classes approximation error high successful generalization 
way dilemma target class small precisely prior information problem solved 
prior information target function correspond smaller target class problems poor generalization ameliorated 
essence prior information idea 
mathematics generalization free lunch theorems wolpert statistical theory vapnik point thing incorporation prior knowledge way learning machines tractably generalize finite data 
way incorporating prior knowledge idea virtual examples utilizing prior information target function generate novel examples old data enlarging effective data set 
discuss section 
virtual examples framework prior information discussed significant problem learning examples large amount data examples needed adequate learning 
consequently crucial exploit form prior knowledge task hand 
known technique incorporating prior information restrict class hypotheses reduce data requirements vapnik chervonenkis theory discussed previous section 
alternative expand set available examples fashion learner access effectively larger set examples resulting accurate learning 
additional examples created existing ones application prior knowledge referred virtual examples introduced poggio vetter different notion virtual examples introduced abu mostafa discussed 
lay general framework virtual examples section 
course virtual examples way incorporating prior information briefly review alternate methods relationship approach section 
idea virtual examples ad hoc show section connection virtual examples approach regularization technique incorporating prior information 
example discussed possible prove techniques yield solution 
heart virtual examples idea involves actual creation virtual examples main focus discuss substantive real world practical demonstrations approach sections 
general framework discussed earlier primary goal learner approximate unknown target function examples pairs function 
unknown target function real valued multivariate function sec 
characteristic defined manifold sec 

absence prior information learner attempt fit function data set predict values 
suppose prior knowledge set transformations allow obtain new examples old 
example target function invariant respect particular group transformations 
simple case target function known odd 
complex case target function characteristic function defined manifold objects 
correspondingly cases obtaining new examples easy cases case object recognition quite difficult 
suppose know transformation valid example valid example 
invariant transformation identity mapping 
general course relation depends prior knowledge problem quite complex 
set examples knowledge transformation generate set virtual examples tx gamma 
interesting cases prior knowledge problem allow define group transformations create new virtual examples old data set 
example rotations image plane define group object recognition problems 
creation virtual examples allows expand example set consequently move better generalization 
techniques prior information related research needless say idea virtual examples possible way incorporating prior information 
discuss section various ways researchers tried utilize prior knowledge 

prior knowledge choice variables features prior knowledge choice variables features represent problem 
consider case object recognition 
simple form prior knowledge rotated version object represents object 
think input network features invariant rotations image plane 
case rotation invariant recognition achieved just example 
approach limited vision applications difficult find features invariant interesting transformations 
example find features face images invariant respect rotation space apart trivial ones colour person hair 
details possibilities approach see mundy 
kind prior knowledge certain features appear conjunction disjunction certain variables linked certain form 
case explicitly add new variables set original variables making learning task easier 
example robotics known certain mechanical systems relation torques state variables represented certain combinations trigonometric functions 
explicitly adding sine cosine transformation state space variables usually problem easier solve 
technique uncommon statistics new variables created means nonlinear transformation original ones 

prior knowledge learning technique way incorporate prior knowledge embed learning technique 
examples transformation distance technique introduced simard le cun denker 
idea underlying technique suppose pattern classification problem solved know outcome classification scheme invariant respect certain transformation set parameters example rotation angle image plane object recognition 
means input pattern manifold sx output constant 
desire classification technique nearest neighbors notion distance distance patterns euclidean distance euclidean distance manifolds 
quantity computed analytically general simard le cun denker show estimate local approximation manifold tangent plane experimentally computed 
case prior knowledge embedded definition distance learning technique choice variables described 
case prior knowledge embedded learning technique regularization theory set mathematical tools introduced tikhonov order deal ill posed problems tikhonov tikhonov arsenin wahba poggio girosi girosi jones poggio 
regularization theory ill posed problem transformed posed prior knowledge 
common form prior knowledge smoothness role theory learning examples investigated length poggio girosi 
forms prior knowledge framework regularization theory 
topic investigated verri poggio gave sufficient conditions constraint embedded regularization framework 
examples prior knowledge considered include monotonicity convexity positivity 

generating new examples prior knowledge form utilising prior knowledge learning idea generating new examples existing data set 
idea virtual examples poggio vetter consider 
example similar technique pomerleau alvinn autonomous navigation vehicle learns drive highway 
system consists camera mounted vehicle neural network takes image road input produces output set steering commands 
examples acquired recording actions human driver 
humans keeping vehicle right lane images road look alike examples action take vehicle unusual attitude far right left 
network able give correct answers finds kinds situations examples 
pomerleau prior knowledge geometry problem order create examples case unusual attitudes 
knowing location camera respect vehicle examples belonging data set created human driver able create images road look vehicle unusual attitude say close centerline 
new images corresponding locations vehicle computed steering command creating new set images containing examples unusual attitudes allowing system achieve excellent performance 

incorporating prior knowledge hints technique proposed abu mostafa 
list briefly main points concept hints 
approach overlaps extent completely ideas virtual examples 
consider function learned domain range hypothesis provided learning process say regularization network approximation functional measuring error 
hint hm test satisfy 
generates example hint measures amount error example hint odd chooses uses gammax 
total disagreement hm em em examples hints ffl invariance hint certain examples 
associated error gamma ffl monotonicity hint certain examples associated error gamma 
ffl example hint set examples treated hint abu mostafa describes represent hints virtual examples 
important distinguish notion virtual examples abu mostafa 
abu mostafa virtual example typically pair related way hint 
minimization done virtual examples 
hand abu mostafa introduces notion duplicate examples 
pairs traditional sense created knowledge hint 
associated invariant sets essentially virtual examples 
abu mostafa focuses learning mechanism kind adaptive minimization scheme hint represented creation virtual examples duplicate examples focus actual creation virtual examples non trivial learning problems 
virtual examples regularization showing idea virtual examples lead solution identical obtained incorporating prior knowledge regularizer 
related results obtained leen bishop 
regularization theory rbf suppose set theta rg random noisy sample multivariate function problem recovering function data ill posed formulated framework regularization theory tikhonov wahba poggio girosi 
framework solution minimizing functional form gamma oe positive number usually called regularization parameter oe cost functional constrains space possible solutions form prior knowledge 
common form prior knowledge smoothness words ensures inputs close corresponding outputs close 
consider general class rotation invariant smoothness functionals girosi jones poggio defined oe ds indicates fourier transform positive radial function tends zero ksk high pass filter 
consider simplicity subsequent notations case fourier transform positive definite conditionally positive definite micchelli bell shaped function 
possible show see girosi jones poggio sketch proof function minimizes functional classical radial basis functions approximation scheme micchelli moody darken gamma vector coefficients satisfies linear system identity matrix defined vector output values matrix ij gamma 
classical examples basis functions include gaussian exp inverse multiquadric kxk gamma 
section show embed prior knowledge radial symmetry framework derive corresponding solution 
regularization theory presence radial symmetry standard regularization theory approach minimization functional usually done space functions phi oe finite 
additional knowledge solution known constrain space solutions 
know solution function radial symmetry restrict minimize phi set radial functions 
problem solve min phi min phi gamma oe notice function uniquely defines dimensional function follows kxk notation standard results fourier theory represent elements hankel transform lions gamma ds gamma known number gamma bessel function kind defined ksk 
functional eq 
thought functional solution minimization problem imposing stationarity condition ffih ffi 
lengthy calculations solution approximation problem written form kxk kx defined kxk kx gamma ds sj gamma gamma basis function friendly look notice similarity solution standard solution 
cases final approximating function linear superposition basis functions basis function data point 
computational point view cases coefficients solving linear system difference case matrix ij eq 
replaced matrix ij kx kx 
clear standard solution obtained placing bump function data point interpretation evident solution 
example shows similar thing happens clearer section discuss creation virtual examples 
example consider common case basis function gaussian 
case fourier transform gaussian exp gammas 
integral eq 
performed obtain form kxk kx gamma kxk kx gamma gamma bessel function kind imaginary argument par 

plot function dimensions set kx 
clear function radial bump function bump concentrated circle radius kx radial section function looks gaussian function centered kx providing local radially symmetric form approximation 
radial symmetry virtual examples section prior knowledge generate new virtual examples existing data set 
theta rg data set assume know function underlying data radial symmetry 
means possible rotation matrices dimensions 
gamma dimensional vector parameters represents point sigma gamma surface dimensional unit sphere 
property implies example points sigma gamma examples call additional points virtual examples 
circle radius basis function kxk kx 
consider standard radial basis functions approximation technique form 
suppose moment function invariant respect finite number rotations example generate virtual examples included expansion regular examples 
trivial see invariance property coefficients basis functions corresponding virtual examples equal coefficients corresponding original example 
result eq 
replaced ff gamma ff defined relax assumption function invariant respect finite number rotations allow span entire surface sigma gamma equation suggests replace eq 
sigma gamma omega gamma gamma omega gamma uniform measure sigma gamma hankel representation radial function eq 
integral sigma gamma performed provides result kxk kx kxk kx precisely expression 
derivation clear basis function kxk kx infinite superposition gaussian functions centers uniformly cover surface sphere radius kx creating virtual examples sense right thing leading result gets principled sophisticated approach regularization theory 
appealing feature virtual examples technique fact applied general cases impossible derive analytical results derived section 
virtual examples vision speech goal suggest creation virtual examples technique incorporate prior information machine learning problems 
previous section shows creating virtual examples equivalent incorporation prior information regularizer framework function learning 
virtual example strategy heuristic 
turn attention real world problems arise computer vision speech recognition give examples generate virtual examples certain conditions 
examples consider non trivial part virtual example strategy identifying set legal transformations allow new valid example created 
previous treatments machine learning problem concentrated function learning legal transformations typically simple easily create examples 
example function odd radial symmetry easy deal 
imagine interested object recognition 
generate new example 
certain obvious cases 
example translating image image plane image scale transformation generate trivial cases new examples 
non obvious ones rotation depth changing expression face significantly harder realize 
section discuss problem object recognition view function learning paradigm generate non trivial virtual examples 
virtual views object recognition consider problem recognizing objects images 
particular class objects cars cubes faces defined terms pointwise features put correspondence 
example features feature represented location coordinate system say coordinates particular view particular object represented point note points correspond valid views objects 
trying learn object class regarded trying learn characteristic function function form gamma 
ae view projected view represented characteristic function problems rarely specify simple mathematical constraints radial symmetry characteristic functions 
recognition problem particularly challenging 
consider example face recognition problem studied beymer 
goal recognize faces different people variety views 
approach collect large number views person train classifier recognize 
shown fig 
fifteen views particular face collected training examples face 
relatively straightforward approach works usually requires large number training examples 
contrast alternative strategy kind prior knowledge class faces order generate virtual examples virtual views 
train view independent system basis virtual examples 
raises question examples images belonging class generate new examples images belonging class 
order need uncover set legal transformations allow take elements come elements prior knowledge class objects allow uncover set valid transformation 
pose invariant view face recognizer uses views model person face 
beymer 
symmetry prior information poggio vetter examined particular case bilateral symmetry certain objects faces 
suppose model view object pair symmetric points view 
purposes define object bilaterally symmetric transformation view pair symmetric points object yields legal view pair orthographic projection rigid rotation object dx pair pair pair pair gammax gammax gamma gamma notice symmetric pairs elementary features situations points lying symmetry plane degenerate cases symmetric pairs 
geometrically simply means bilaterally symmetric objects simple transformations view yield views legal 
transformations similar mirroring view axis image plane shown top left image mirrored right correspond bilaterally symmetric object proper rotations rigid object orthographic projection image plane 
transformation equation additional view generated model view 
views linearly independent resort views theorem notation introduced earlier set defines space valid image views particular object 
views theorem states essentially regarded dimensional vector space 
furthermore basis computed linearly independent views 
details see poggio vetter 
single view upper left new view upper right generated assumption bilateral symmetry 
views sufficient verify novel view second row corresponds object 
compute basis spans space object 
allows compute recognition function just true view 
bilateral symmetry face recognition systems beymer poggio psychophysical evidence supports human visual system schyns bulthoff bulthoff vetter poggio bulthoff 
general transformations linear object classes flexible way acquire information images objects certain class change pose illumination transformations learn possible pattern variabilities class specific deformations representative training set views generic prototypical objects class faces 
particular objects belong behaved class known linear object class transformations easily learned 
manner prior knowledge object class linear utilized effectively generate novel views incorporated training process 
approach linear classes originates proposal poggio vetter countering curse dimensionality applications supervised learning techniques powerful versions developed 
techniques non linear learning networks developed beymer shashua poggio beymer poggio 
purposes provide brief overview technique linear classes generating novel views objects 
objects projections linear classes consider view dimensional object defined terms pointwise features poggio vetter 
view represented vector coordinates feature points 
assume linear combination views objects dimensionality ff consider linear operator associated desired uniform transformation instance specific rotation 
define lx rotated view object linearity group uniform linear transformations follows ff view object represented weighted sum views objects rotated view linear combination rotated views objects weights 
course arbitrary view projection view decomposition general imply decomposition rotated views necessary sufficient condition 
linear classes natural question ask conditions projections objects satisfy equation 
answer clearly depend types objects projections allow 
series articles poggio vetter vetter poggio notion linear classes introduced developed provide definition set views objects fx linear object class linear projection px equivalent saying minimal number basis objects necessary represent object allowed change projection 
note linear projection restricted projections may drop occluded points 
assume px px projections elements linear object class ff px constructed knowing ff equation px objects 
ff implications relations described earlier suggest prototypical views projections basis linear object class known transformations synthesize operator transform view new view object linear combination prototypes 
words compute new view object knowing explicitly dimensional structure 
notice knowledge correspondence equation equation necessary rows linear equation system exchanged freely 
technique require compute correspondence views different viewpoints 
fact points may occluded 
fig 
shows simple example linear object class construction new view object 
dimension class cuboids cuboid represented linear combination prototypical cuboids 
class linear orthographic projections preserve dimensions 
dimensional objects differ shape texture 
truly apply linear class idea gray level images need derive object representations incorporate texture 
done developing shape texture vector representations correspondence linear class idea 
output input test examples orientation learning image transformation rotation threedimensional cuboids orientation upper row new orientation lower row 
test cuboid upper row right represented linear combination dimensional coordinates example cuboids upper row 
linear combination example views lower row coefficients evaluated upper row results correct transformed view test cuboid output lower row right 
notice correspondence views different orientations needed different points object may occluded different orientations 
learning transformation complete story discussing transformation view novel view learned 
proceed introduce helpful change coordinate systems equations 
absolute coordinate system represent views difference view object class 
subtracting projection object sides equations deltax ff deltax deltax ff deltax change coordinate system equation evaluates new difference vector rotated view 
new view object constructed adding difference view 
steps constructing novel view step compute coefficients ff optimal decomposition sense square 
decompose initial field deltax new object initial fields deltax prototypes minimizing jj deltax gamma ff deltax jj rewriting deltax phi matrix formed vectors deltax arranged column wise ff column vector ff coefficients minimizing equation gives ff phi deltax step observation previous section implies operator transforms deltax deltax deltax deltax deltax phi ff phi phi deltax phi phi learned example pairs deltax deltax 
case layer linear network compare poggio learn transformation transform view novel object class 
examples linearly independent phi phi phi phi gamma phi cases equation solved svd algorithm 
step analogous steps taken deal textures 
decomposing new texture example textures textures mapped common basis typically image correspondences 
representation decomposition textures performed described 
step final step image rendering 
ff coefficients computed texture shape vectors applied prototype examples second orientation 
correspondence fields new image combined image forward warping wolberg generate novel image 
procedure generate novel views images prior knowledge image belongs linear class 
fig 
shows case new view generated class cuboids linear class assumption correct 
interestingly fig 
shows novel views face generated prototypical views linear class technique 
faces theoretically guaranteed constitute linear class practice assumption turns quite example shows 
collecting fifteen example patterns training generate virtual examples techniques described train combination real virtual examples 
system successfully implemented described beymer poggio 
face recognition system single real view plus virtual views person achieved recognition rate correct system real views person achieved database 
systems significantly better system real view person 
notice approach outlined correspondence plays key role 
interestingly correspondence prototypes required views viewpoint 
correspondence required real image prototypes computed automatically optical flow techniques beymer poggio 
different approach see jones poggio require explicit correspondence real image prototypes 
vetter propose technique may allow automatic correspondence prototype images 
virtual examples speech recognition example potential utility virtual example technique incorporating prior information provided context speech recognition 
production speech humans important source prior information lies physical constraints vocal tract speech articulators speech producing apparatus necessarily obey 
example different sounds phonemes produced speaker share common characteristics related properties real view center surrounded virtual views derived technique related linear class technique simpler 
correspondence real view prototypes computed analytically 
details required process see beymer poggio 
formant hz second formant hz second formants beat palm boot 
values female speaker plotted italics 
individual speaker vocal tract 
speaker high pitch case phonemes speaker produces 
consider fig 
compares formant values vowel sounds beat palm boot male female speaker 
data obtained peterson barney 
speech recognition distinguish different vowels basis certain vowel features formants 
notice broad pattern formants speakers actual formant values differ female speaker consistently higher formants general 
turns formant values related length vocal tract people longer tracts lower formants vice versa 
sort prior information capture attempting solve speech recognition problem way invariant systematic speaker differences 
roughly speaking speech produced air pumped vocal tract exciting corresponding acoustic waves transmitted air hearer 
vocal tract including nasal tract modulates excitations produced resulting speech 
acoustic standpoint vocal tract modeled non uniform tube resonances correspond formants described earlier 
length tube inversely related frequency resonances humans longer vocal tracts formants lower frequencies 
different sound corresponds different articulatory configuration correspondingly different vocal tract shape non uniform tube different shape correspondingly different set formant values 
speakers producing roughly configuration order approximation speaker differences come due variations scale vocal tracts shape vocal tract large man longer child 
length example quantity preserved sounds speaker 
extracted automatically speaker scale canonical speaker produce novel 
acoustic problem modeled electrical circuits common formulation taken source filter point view 
vocal tract viewed filter shaping input provided electrical sources 
sources roughly types periodic sources correspond glottal vibrations voiced speech aperiodic sources correspond various kinds turbulent sources produced unvoiced partially voiced speech fricatives shown fig 
schematic view speech production apparatus 
vocal tract filter shown parameterized kinds parameters models shape tract depends phonetic identity sound models things scale depends specific characteristics speaker 
similarly voiced periodic source parameterized set parameters denoted speaker specific don change phoneme phoneme 
typical examples speaker specific parameters pitch voice quality approach virtual examples idea obtain number examples novel speaker sounds 
sounds know phoneme dependent parameter values depends phoneme common speakers parameters estimated directly data collected speaker 
novel speaker sounds estimate speaker specific parameters generate new instances speaker drive speech production model phoneme specific parameters derived speaker speaker specific parameters voiced source periodic unvoiced source turbulent switch speech vocal tract filter schematic view speech production apparatus 
derived test speaker 
note virtual example strategy depend large part fidelity speech production model place 
simpler direct strategy learn mapping collecting examples number different speakers 
doing convert speaker novel speaker functional mapping learned 
examine fig 
shows relationship ae bat beat speaker 
data speakers collected speakers grouped speaker classes 
weighted spectral measure computed speaker ae sounds 
mean values measure speaker groups plotted 
notice strong correlation mean value group ae mean value group suggesting strong predictability 
idea successfully incorporating speaker information speech recognition system niyogi zue niyogi 
problem described analogous vision example described 
different poses face different sounds speaker equivalent different poses vocal tract 
object recognition knowledge relationship different poses prototypes create novel pose new speaker 
speech recognition knowledge relationship different sounds prototypical speakers create novel sound new speaker 
particular strategy creating virtual examples rarely speech recognition particularly useful wishes adapt speech ae weighted spectral average measure speaker speaker ae plotted 
data speakers collected grouped classes class average sounds plotted 
notice strong correlation suggesting possible predict ae sound sound group speaker belongs 
recognition system new speaker extremely limited amounts adaptation data 
introduced idea virtual examples possible strategy incorporating prior knowledge target function learning examples paradigm 
motivated importance virtual examples discussing issue sample complexity machine learning 
specifically general results vapnik chervonenkis argued number examples needed successful generalization prohibitive adequate constraints hypothesis class 
successful learning result constraints properly reflected prior knowledge problem solved 
creation virtual examples way dilemma 
showed idea virtual examples mathematically equivalent incorporating prior knowledge regularizer function learning certain restricted domains 
substantive real world problems rare prior knowledge directly implemented elegant regularization constraint 
cases creation virtual examples straightforward 
say virtual examples easily created 
particular spent significant portion time discussing details creation virtual examples object recognition speech recognition 
results obtained far suggest promising way incorporate prior knowledge example learning framework leading systems generalize limited amounts data typically available real world problems 
abu mostafa 
method learning hints 
hanson jack cowan lee giles editors advances neural information processings systems san mateo ca 
morgan kaufmann publishers 
abu mostafa 
learning hints neural networks 
journal complexity 
abu mostafa 
hints vc dimension 
neural computation 
abu mostafa 
hints 
neural computation 
barron 
approximation estimation bounds artificial neural networks 
machine learning 

regularization methods linear inverse problems 
editor inverse problems 
springer verlag berlin 
beymer poggio 
face recognition example view 
proceedings international conference computer vision cambridge ma june 
beymer poggio 
image representations visual learning 
science june 
beymer shashua poggio 
example image analysis synthesis 
memo artificial intelligence laboratory massachusetts institute technology 
david beymer 
face recognition varying pose 
proceedings ieee conf 
computer vision pattern recognition seattle wa 
bishop 
training noise equivalent tikhonov regularization 
neural computation 
breiman friedman olshen stone 
classification regression trees 
wadsworth belmont ca 
lions 
mathematical analysis numerical methods science technology volume 
springer verlag berlin 
wolpert ed 
mathematics generalization 
addison wesley reading ma 
girosi jones poggio 
regularization theory neural networks architectures 
neural computation 

table integrals series products 
academic press new york 
poggio 
synthesizing color algorithm examples 
science 
jones poggio 
model matching line drawings linear combination prototypes 
proceedings international conference computer vision pages 
ieee june 
leen 
data distributions regularization invariant learning 
neural computation 
micchelli 
interpolation scattered data distance matrices conditionally positive definite functions 
constructive approximation 
moody darken 
learning localized receptive fields 
hinton sejnowski editors proceedings connectionist models summer school pages palo alto 
moody darken 
fast learning networks locally tuned processing units 
neural computation 

methods solving incorrectly posed problems 
springerverlag berlin 
joseph mundy andrew zisserman 
geometric invariance computer vision 
mit press cambridge ma 
niyogi 
modelling speaker variability imposing speaker constraints phonetic classification 
technical report tr mit laboratory computer science 
niyogi girosi 
relationship generalization error hypothesis complexity sample complexity radial basis functions 
neural computation 
niyogi zue 
correlation analysis vowels application speech recognition 
proceedings eurospeech genoa italy 
peterson barney 
control methods study vowels 
journal acoustical society america 
poggio girosi 
networks approximation learning 
proceedings ieee september 
poggio vetter 
recognition structure model view observations prototypes object classes symmetries 
memo artificial intelligence laboratory massachusetts institute technology 
pomerleau 
alvinn autonomous land vehicle neural network 
advances neural information processing systems san mateo ca 
morgan kaufman 
pomerleau 
efficient training artificial neural networks autonomous navigation 
neural computation 
rumelhart hinton williams 
parallel distributed processing 
mit press cambridge ma 
schyns bulthoff 
conditions viewpoint dependent face recognition 
memo mit artificial intelligence lab cambridge ma august 
simard lecun denker 
efficient pattern recognition new transformation distance 
advances neural information processing systems pages san mateo ca 
morgan kaufmann publishers 
simard lecun denker 
tangent prop formalism specifying selected invariances adaptive network 
touretzky editor advances neural information processing systems iv pages san mateo ca 
morgan kaufmann publishers 
tikhonov 
solution incorrectly formulated problems regularization method 
soviet math 
dokl 
bulthoff 
face recognition varying pose role texture shape 
submitted 
vapnik 
nature statistical learning theory 
springer new york 
vapnik 
estimation dependences empirical data 
springer verlag berlin 
verri poggio 
regularization theory shape constraints 
memo artificial intelligence laboratory massachusetts institute technology 
vetter jones poggio 
bootstrapping algorithm learning linear models object classes 
proceedings computer society conference computer vision pattern recognition cvpr pages june 
vetter poggio bulthoff 
importance symmetry virtual views dimensional object recognition 
current biology 
wahba 
splines models observational data 
series applied mathematics vol 
siam philadelphia 
georg wolberg 
image warping 
ieee computer society press los alamitos ca 
ae 
