feature selection discretization huan liu rudy setiono discretization turn numeric attributes discrete ones 
feature selection eliminate irrelevant redundant attributes 
chi simple general algorithm uses statistic discretize numeric attributes repeatedly inconsistencies data 
achieves feature selection discretization 
handle mixed attributes multiclass data remove irrelevant redundant attributes 
keywords discretization feature selection pattern classification feature selection eliminate irrelevant redundant attributes 
relevant features classification algorithms general improve predictive accuracy shorten learning period form simpler concepts 
abundant feature selection algorithms 
methods principle component compose smaller number new features select subset original attributes 
considers virtues serves indicator kind data selected features collected 
category feature selection algorithms divided terms data types 
basic types data nominal attribute color may values red green yellow ordinal attribute winning position values attribute salary values 
feature selection algorithms shown effectively discrete data strictly binary data binary class value 
order deal numeric attributes common practice algorithms discretize data conducting feature selection 
provides way select features directly numeric attributes discretizing 
numeric data common real world problems 
classification algorithms require training data contain discrete attributes better discretized binarized data 
numeric data automatically transformed discrete ones classification algorithms readily disposal 
chi effort goal discretize numeric attributes select features 
problem attack data sets numeric attributes irrelevant redundant department information systems computer science national university singapore kent ridge singapore nus sg range numeric attribute wide find algorithm automatically discretize numeric attributes remove irrelevant redundant ones 
closely related kerber chimerge discretizes numeric attributes statistic 
chimerge consists initialization step bottom merging process intervals continuously merged termination condition determined significance level ff set manually met 
improvement obvious simple methods equal width intervals divides number line minimum maximum values intervals equal size equal frequency intervals interval boundaries chosen interval contains approximately number training examples 
defining width frequency threshold easy attribute knowing chimerge requires ff specified ideally ff attribute 
big small ff discretize attribute 
extreme example discretization continuous attribute 
discretization introduce inconsistencies nonexistent change characteristics data 
short easy find proper ff chimerge 
ideal data determine value ff take 
leads chi algorithm 
naturally discretization continue long inconsistencies generated original data attribute discretized maximum attributes may discretized interval 
attributes removed affecting discriminating power original data 
describe chi algorithm experiments various aspects turn 
ii 
chi algorithm chi algorithm summarized applies statistic conducts significance test relationship values attribute categories 
consists phases 
phase begins large significance level ff numeric attributes discretized 
attribute sorted values 
attribute performed 
calculate value equation inconsistency mean patterns match belong different categories 
pair adjacent intervals number intervals equals number distinct values attribute 
merge pair adjacent intervals lowest value critical value 
merging continues pairs intervals values exceeding parameter determined ff initially corresponding value degree freedom 
process repeated decreased ff discretized data inconsistency rate exceeds ffi phase matter fact generalized version chimerge kerber 
specifying threshold phase chi wraps chimerge loop automatically increments threshold equivalently decreases ff 
consistency checking introduced stopping criterion sure discretized data set accurately represents original 
new features chi automatically determines proper threshold keeps fidelity original data 
phase finer process phase 
starting ff determined phase attribute associated takes turns merging 
consistency checking conducted attribute merging 
inconsistency rate exceeded decreased attribute round merging attribute involved merging 
process continued attribute values merged 
round robin discretization achieves objectives removal irrelevant redundant attributes better coordination discretized attributes 
chi algorithm phase att attribute set ff data ffi numeric att sort att data sort data att chi sq init att data refresh data chi sq calculation att data merge data ff ff ff ff phase set ff att att merged mergeable att sort att data sort data att chi sq init att data refresh data chi sq calculation att data merge data data ffi att mergeable function returns inconsistency rate discretized data 
function merge returns true false depending concerned attribute merged 
function decreases significance level level implemented table 
function chi sq init prepares computation 
formula computing value ij gamma ij ij number classes ij number patterns ith interval jth class number patterns ith interval ij number patterns jth class ij total number patterns ij expected frequency ij ij set 
degree freedom statistic number classes 
inconsistency rate dataset calculated follows instances considered inconsistent match class labels matching instances considering class labels inconsistency count number instances minus largest number instances class labels example matching instances instances belong label label label largest inconsistency count gamma inconsistency rate sum inconsistency counts divided total number instances 
purpose phase implementation chi twofold direct comparison chimerge 
sense phase chi automated version chimerge consideration computational efficiency discussed section iv 
phase attribute merged value simply means attribute needed representing original data set 
result discretization ends feature selection accomplished 
iii 
experiments sets experiments conducted 
set experiments real world data evaluation done indirectly classifier 
want establish chi helps improve predictive accuracy chi properly effectively discretizes data eliminates irrelevant redundant attributes explains predictive accuracy classifier improved 
purposes 
reasons choice works problems known requiring description selects relevant features tree branching benchmark verify effects chi 
second set experiments directly examine chi ability discretizing feature selection introducing synthetic data sets adding noisy attributes real world data set 
ex periments controlled data sets better understand effective chi real world data data sets experiments iris wisconsin breast cancer heart disease different types attributes 
iris data continuous attributes breast cancer data ordinal discrete ones heart disease data mixed attributes numeric discrete 
data sets described 
iris data contains patterns classes iris setosa iris iris virginica 
pattern described numeric attributes length petal length petal width 
originally odd numbered data selected training patterns rest testing patterns 
breast cancer data contains samples breast collected university wisconsin hospital 
discrete attributes valued scale 
class value benign malignant 
data set split randomly sets patterns training testing 
heart disease data contains medical cases heart diseases 
contains numerically valued features nominally valued numerically valued attributes 
class values healthy diseased heart 
removing patterns missing attribute values patterns third randomly chosen testing rest training 
controlled data extra data sets designed test various noisy attributes removed 
synthetic third iris data added noisy attributes 
synthetic data consists items described attributes attribute determines item class label 
values attribute generated uniform distribution lower bound upper bound item class label determined follows class class class 
add irrelevant attributes values generated normal distribution mean standard deviation oe 
values generated normal distributions oe oe respectively values distribution 
values generated uniform distribution synthetic data set contains irrelevant redundant attributes items 
attributes similarly constructed irrelevant 
value attribute obtained multiplying corresponding value obtained university california irvine machine learning repository anonymous ftp ics uci edu 
mean attributes related class values 
constant factor 
determine item class label redundant 
third data set modified version iris data 
noisy attributes added iris training data corresponding original attributes 
values noisy attribute determined normal distribution ave oe max gamma min ave max min average maximum minimum values original attribute 
total attributes items 
empirical results real world data show discretization number attributes decreases data sets table 
iris data number attributes reduced petal length petal width values 
breast cancer data attributes removed original attributes 
remaining attributes discrete values respectively 
heart disease data discrete attributes left discretization feature selection consistency checking 
continuous attributes attributes remain suggested chi having discrete values respectively 
breast cancer heart disease data ffi set iris data ffi 
table change number attributes iris heart breast second run original data sets discretized ones 
run default setting 
chi discretizes training data generates mapping table testing data discretized 
shown tables predictive accuracies tree sizes data sets 
predictive accuracy improves tree size drops half breast cancer heart disease data 
iris data accuracy tree size remain attributes values 
way shows works pretty chi small data set 
table change predictive accuracy iris heart breast table change size decision tree iris heart breast empirical results controlled data purpose experimenting controlled data verify effective chi removing noisy attributes discretization 
necessary see chi discretize relevant attribute properly remove irrelevant attributes remove irrelevant redundant attributes 
synthetic data chi merged discrete values corresponding classes merged attributes value phase 
stays noisy attributes removed 
synthetic data phase chi merged discrete values irrelevant attribute value 
phase chi merged redundant value 
irrelevant redundant attributes removed 
modified iris data phase chi merged discrete values discretized attributes value 
recall attributes added irrelevant attributes 
phase attributes merged value 
attributes remained discrete values respectively identical experiment original data 
set controlled experiments shown chi effectively discretizes numeric attributes removes irrelevant redundant attributes 
redundant attributes removed phase 
iv 
discussions merge chi algorithm reduces number intervals worst case different values merged value innermost loop requires gamma times calling function number patterns training data sort needs logn 
attributes reimplemented chimerge requires mn logn 
consider worst case checking data consistent refer chi algorithm takes mn 
outermost loop determined number incremental steps value 
computational complexity phase km logn kmn log 
similar complexity obtained phase 
complexity result gives guideline long take run chi data set 
phase implementation due concern efficiency 
phase mainly designed improve efficiency specially large 
due consistency checking takes saving gamma outermost loop implementing phases 
chi discretize data select features supervised learning tasks class information vital statistic 
chi works ordinal attributes 
mixed nominal ordinal attributes chi specified operate ordinal attributes discretization feature selection 
chi attempting discover order single attribute class correlations perform correctly second order correlation upper limit number values attribute take sample data 
corresponding order correlation 
feature weighting methods helpful higher order correlation data considered 
issue determine initial ff 
large ff chi run longer 
final ff values numeric attributes remain different initial ff values ff set small instance 
addition ff threshold required tolerable rate inconsistency ffi default value assuming data set consistent reset value 
reasonable approximation rate inconsistency training data difficult compute 
chi simple general algorithm automatically select proper critical value test determine intervals numeric attribute select features removing irrelevant redundant attributes characteristics data 
inconsistency criterion guarantees fidelity training data remain chi applied 
empirical results real world data controlled data shown chi useful reliable tool discretization feature selection numeric attributes 
almuallim dietterich 
learning boolean concepts presence irrelevant features 
artificial intelligence november 
catlett 
changing continuous attributes ordered discrete attributes 
european working session learning 
fayyad irani 
attribute selection problem decision tree generation 
aaai proceedings ninth national conference artificial intelligence pages 
aaai press mit press 
kerber 
chimerge discretization numeric attributes 
aaai proceedings ninth national conference artificial intelligence pages 
aaai press mit press 
kira rendell 
feature selection problem traditional methods new algorithm 
aaai proceedings ninth national conference artificial intelligence pages 
aaai press mit press 
liu wen 
concept learning feature selection 
proceedings australian new zealand conference intelligent information systems 
murdoch barnes 
statistical tables science engineering business studies 
macmillan press 
quinlan 
programs machine learning 
morgan kaufmann 
rendell 
lookahead feature construction learning hard concepts 
machine learning proceedings seventh international conference pages 
morgan kaufmann pub 
san mateo california 
liu setiono 
probabilistic approach feature selection filter solution 
proceedings th international conference machine learning pages 
sethi 
hierarchical classifier design mutual information 
ieee trans 
pami vol pages july 
dubes jain 
critical evaluation intrinsic dimensionality algorithms 
gelsema kanal editors pattern recognition practice pages 
morgan kaufmann publishers 

