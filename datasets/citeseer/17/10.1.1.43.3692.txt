proceedings tenth conference uncertainty artificial intelligence 
seattle wa morgan kaufmann 
induction selective bayesian classifiers pat langley langley stanford edu stephanie sage sage stanford edu institute study learning expertise high street palo alto ca examine previous naive bayesian classifier review limitations include sensitivity correlated features 
respond problem embedding naive bayesian induction scheme algorithm carries greedy search space features 
hypothesize approach improve asymptotic accuracy domains involve correlated features reducing rate learning ones 
report experimental results natural domains including comparisons decision tree induction support hypotheses 
closing discuss approaches extending naive bayesian classifiers outline directions research 
years growing interest probabilistic methods induction 
techniques number clear attractions accommodate flexible nature natural concepts inherent resilience noise solid grounding theory probability 
experimental studies probabilistic methods revealed behaviors competitive best inductive learning schemes 
probabilistic induction anderson cheeseman fisher yun mckusick langley focused unsupervised learning basic approach applies equally supervised learning tasks 
supervised bayesian methods long field pattern recognition duda hart past years received attention machine learning community clark niblett kononenko langley iba thompson 
describe technique designed improve impressive behavior simplest approach probabilistic induction naive bayesian classifier 
review representational performance learning assumptions underlie method situations lead problems 
central assumption naive approach attributes independent class harm classification process violated 
response drawback describe revised algorithm selective bayesian classifier deals highly correlated features incorporating attributes final decision process 
experimental evidence scheme improves asymptotic accuracy domains naive classifier fares poorly hurting behavior domains compares induction algorithms 
close comments related directions research 
naive bayesian classifier straightforward widely tested method probabilistic induction known naive bayesian classifier scheme represents class single probabilistic summary 
particular description associated class probability base rate specifies prior probability observe member class description associated set conditional probabilities specifying probability distribution attribute 
nominal domains typically stores discrete distribution attribute description 
jc term specifies probability value instance class numeric domains represent continuous probability distribution attribute 
requires assume general form model common choice normal distribution conveniently represented entirely terms mean variance 

borrowed term kononenko common names method include simple bayesian classifier langley idiot bayes buntine 
selective bayesian classifiers classify new instance naive bayesian classifier applies bayes theorem determine probability description instance ji conjunction values expand expression jc jc denominator sums classes jc probability instance class calculating quantities description algorithm assigns instance class highest probability 
order expression operational specify compute term jc 
naive bayesian classifier assumes independence attributes class lets equality jc jc values jc represent conditional probabilities stored class 
approach greatly simplifies computation class probabilities observation 
bayesian framework lets specify prior probabilities class conditional terms 
absence domain specific knowledge common scheme uninformed priors assign equal probabilities class values attribute 
specify weight give priors relative training data 
example anderson dirichlet distribution initialize probabilities give priors influence single training instance 
clark niblett describe approach explicit priors estimates jc directly proportions training data 
instances value observed replace zero probability number training cases 
learning naive bayesian classifier trivial matter 
simplest implementation increments count time encounters new instance separate count class time observes instance class 
counts classifier estimate class nominal value algorithm updates count 
technique solid basis probability theory avoids arbitrary parameters approximates approaches instances implementations 
class value pair 
second count lets classifier estimate jc 
numeric attribute method retains revises quantities sum sum squares compute mean variance normal curve uses find jc 
domains missing attributes include fourth count class attribute pair 
contrast induction methods naive bayesian classifier carry extensive search space possible descriptions 
basic algorithm choices partition data direction move weight space resulting probabilistic summary completely determined training data prior probabilities 
order training instances effect output basic process produces description operates incrementally 
features learning algorithm simple understand quite efficient 
bayesian classifiers appear advantages induction algorithms 
example collection class conditional probabilities inherently robust respect noise 
statistical basis scale domains involve irrelevant attributes 
langley iba thompson analysis factors effect algorithm behavior specific class target concepts 
experimental literature consistent expectations researchers reporting naive bayesian classifier gives remarkably high accuracies natural domains 
example cestnik kononenko bratko included method straw man experiments decision tree induction fared sophisticated techniques 
clark niblett reported similar results finding naive bayesian classifier learned rule induction decision tree methods medical domains 
langley 
obtained stronger results simple probabilistic method outperformed decision tree algorithm natural domains 
naive bayesian classifier relies important assumptions 
simple scheme posits instances class summarized single probabilistic description sufficient distinguish classes 
represent attribute value feature may absent closely related assumption linear separability early neural networks 
encodings lead complex story effect nearly 
perceptrons bayesian classifiers selective bayesian classifiers typically limited learning classes separated single decision boundary 
addressed limitation langley focus 
important assumption naive bayesian classifier class probability distributions attributes independent 
model attribute dependence bayesian framework pearl determining dependencies estimating limited training data difficult 
independence assumption clear attractions 
unfortunately unrealistic expect assumption hold natural world 
correlations attributes domain common 
example domain medical diagnosis certain symptoms common older patients younger ones regardless ill correlations introduce dependencies probabilistic summaries degrade naive bayesian classifier accuracy 
illustrate difficulty consider extreme case redundant attributes 
domain features numerator saw earlier jc jc jc include fourth feature perfectly correlated redundant features obtain jc jc jc twice influence values 
emphasis redundant information reduces influence features produce biased prediction 
example consider linearly separable target concept predicts class features predicts class 
naive classifier easily master concept single redundant feature consistently misclassify possible instances matter training cases encounters 
surprisingly domains naive bayesian classifier performs appear contain significant dependencies 
evidence comes part holte studies show level decision trees nearly full decision trees domains 
addition langley sage behavior simple nearest neighbor algorithm suffer domains 
main exception involves numeric domains duda hart simple situation decision boundaries emerge normal distributions 
expect irrelevant attributes 
attribute sufficient high accuracy remaining ones degrade nearest neighbor method attributes appear highly correlated 
strong performance naive bayesian method despite violation independence assumption intriguing 
suggests revised method circumvents dependencies outperform naive algorithm domains dependencies occur performing equally cases 
section discuss variant bayesian algorithm selects uses subset known features attempt exclude highly correlated attributes 
continue convenient assumption independence minimizing detrimental effects classification accuracy 
selective bayesian classifier goal modify naive bayesian classifier achieve improved accuracy domains redundant attributes 
selective bayesian classifier variant naive method uses subset attributes making predictions 
words performance component algorithm computes jc product conditional probabilities jc selected attributes original feature set 
learning component selective classifier augments original algorithm ability exclude attributes introduce dependencies 
process consists search space attribute subsets 
number choices designing search process 
direction search proceed forward backward manner 
forward selection method start empty set successively add attributes backward elimination process full set remove unwanted ones 
potential problem backward search attributes correlated removing may improve performance redundant information exist 
chose forward selection immediately detect dependencies harmful redundant attribute added 
second decision dealt organization search 
clearly exhaustive search space impractical possible subsets attributes 
realistic approach commonly machine learning algorithms greedy method traverse space 
point search algorithm considers local changes current set attributes best selection choice 
gives worst case time complexity 
selective bayesian classifiers number training instances predictive accuracy pruning naive bayes selective bayes number training instances predictive accuracy pruning naive bayes selective bayes 
learning curves selective bayesian classifier naive bayesian classifier pruning congressional voting records mushroom domain 
error bars represent confidence intervals sided test 
third needed metric evaluate alternative subsets attributes 
considered technique estimating accuracy training set accurate method cross validation 
applied efficiently bayesian classifier simply subtract instance stored attribute frequencies measure accuracy resulting classifier add instance back 
spite opted simply measure accuracy entire training set achieved better results method preliminary studies 
considered criteria halting search process 
adding attributes alternatives improves classification accuracy adopt conservative strategy continuing select attributes long degrade accuracy 
argument approach higher dimensional spaces allow separation classes single decision boundary favors inclusion attributes 
initial experiments favored scheme incorporated system 
summarize algorithm initializes subset attributes empty set accuracy resulting classifier simply predicts frequent class saved subsequent comparison 
iteration method considers adding unused attribute subset trial basis measures performance resulting classifier training data 
attribute improves maintains accuracy permanently added subset ties broken randomly 
algorithm terminates addition attribute results reduced accuracy point returns probabilistic summaries current attribute set 
experiments bayesian classifiers previous comparative studies shown naive bayesian classifier outperforms sophisticated methods decision tree induction domains performs significantly worse langley 
hypothesized result reflects decision trees reliance splits poorly mimic actual decision boundaries domains 
contrast posited naive bayesian classifier poorly domains containing redundant attributes 
selective classifier suffer problem predicted improve performance naive classifier domains accuracy decision tree methods remaining superior domains 
test idea compared behavior selective bayesian classifier naive bayesian classifier quinlan algorithm domains uci repository machine learning databases 
knew naive classifier outperforms soybean disease breast cancer dna promoter domains reverse true mushroom congressional voting chess endgame domains 
domains provide testbed evaluating new algorithm 
data set contains set classified instances described terms numeric nominal attributes 
example soybean disease data consists instances described terms climate conditions crop history plant symptoms labeled disease classes 
congressional voting domain describes members th congress votes key issues labeled selective bayesian classifiers number training instances predictive accuracy pruning naive bayes selective bayes number training instances predictive accuracy pruning naive bayes selective bayes 
learning curves selective bayesian classifier naive bayesian classifier pruning chess endgames breast cancer 
democrat republican 
breast cancer data includes instances malignant benign tissue samples described numeric attributes clump thickness marginal adhesion 
detailed information domains available uci repository anonymous ftp ics uci edu 
domain randomly generated sets separate training test cases 
dependent variable experiment classification accuracy test cases processing sample training cases averaged runs 
classification accuracy algorithm percentage test cases correctly predicts class 
interested rate improvement asymptotic accuracy algorithms measured accuracy different numbers training samples 
resulting learning curves congressional voting mushroom domains respectively confidence intervals shown point 
cases asymptotic accuracy selective bayesian classifier noticeably higher naive method approaching level voting domain remaining slightly lower mushroom data 
shows greater increase accuracy domain chess endgames selective classifier quite reach level 
experimental results domains different picture 
shows selective algorithm reproduces superior performance naive bayesian classifier decision tree induction breast cancer domain 
analogous results appear soybean dna promoter data 
odd behavior soybean data occurs pruning non pruning versions program 
results confirm predictions comparative behavior algorithms 
domains naive classifier exhibits low asymptotic accuracy apparently due presence redundant attributes selective bayesian classifier shows marked improvement 
time simple classifier domains outperforms decision tree induction 
selective bayesian classifier appears overcome weaknesses algorithms 
related bayesian induction years seen growing interest probabilistic approaches induction research genre typically followed paths 
briefly approach focuses new features creation explicit dependency links emphasizes clustering instances taxonomic hierarchies 
framework attempts improve naive bayesian classifier extending basic induction algorithm significant ways 
kononenko describes example approach tests dependencies attributes creates new features conjunctions correlated values 
semi naive bayesian classifier uses training data compute conditional probabilities joint features classify test cases original ones 
experimental comparisons algorithm naive bayesian classifier revealed differences medical domains slight improvement data sets 
schlimmer stagger constructed features analogous reasons similar manner operated different probabilistic framework 
selective bayesian classifiers number training instances predictive accuracy pruning naive bayes selective bayes number training instances predictive accuracy pruning naive bayes selective bayes 
learning curves selective bayesian classifier naive bayesian classifier pruning small soybean domain dna promoters 
selective bayes incorporates attributes soybean data giving identical curve naive method 
research induction bayesian networks pearl generalizes basic approach handling attribute dependence 
cooper herskovits algorithm carries greedy search space bayesian networks requires user specify ordering attributes introduce new features 
connolly restriction probabilistic clustering method generate hidden attributes render observable ones conditionally independent 
kononenko explicitly compared accuracy technique naive approach natural domains usefulness methods increased sophistication remains open question 
langley describes straightforward example hierarchy building approach 
recursive bayesian classifier uses naive algorithm generate probabilistic summary class 
summaries correctly classify training set method halts 
calls naive method recursively class instances classes assigned cases assigned class training data 
method continues recurse correctly classifies training data gains improvement organizes resulting classifiers hierarchy probabilistic descriptions uses sort novel test cases 
experiments artificial domains showed algorithm induce concepts naive bayesian classifier handle studies natural domains showed significant differences methods 
induction probabilistic concept hierarchies builds directly fisher cobweb deals unsupervised training data 
incremental algorithm uses information theoretic evaluation function determine incorporate training case existing category create entirely new category 
gennari langley fisher yun mckusick langley explored similar approaches 
anderson adapted basic idea strict bayesian framework method creates flat set categories hierarchy 
unfortunately experiments compare clustering schemes naive bayesian classifier rare tell sophistication necessary 
clearly approach taken differs frameworks probabilistic induction 
assuming sophisticated knowledge structure requiring complex methods acquiring knowledge selective bayesian classifier retains simplicity naive approach ignores attributes reduce classification accuracy 
assumption independence motivate idea prove useful domains irrelevant features 
course basic idea restricting attributes prediction new greedy approaches searching attribute space 
kittler refers scheme sequential forward selection refers search opposite direction sequential backward elimination 
brodley utgoff methods multivariate decision trees john kohavi pfleger press caruana freitag press skalak press langley sage press similar schemes determine relevant features decision tree nearest neighbor meth selective bayesian classifiers ods 
contribution lies extending idea bayesian classifiers typically take attributes account prediction 
superficially approach similar michie sequential bayesian classifier inspects attribute time classification selecting informative step halting probability class exceeds threshold 
method behavior better viewed constructing decision tree probabilistic evaluation function 
technique common approach reported kubat pfurtscheller decision tree induction select predictive attributes naive bayesian classifier 
report promising results method eeg classification task parallel findings uci data sets 
concluding remarks experimental results encouraging remain preliminary variety related approaches suggests possibilities additional comparative studies 
example determine extent techniques inducing bayesian networks probabilistic concept hierarchies provide benefits simple selection scheme 
carry systematic studies explore effect design decisions implementing selective bayesian classifier 
addition consider usefulness selection techniques kubat method compare technique frameworks similar representational power rely independence assumption lms algorithm related techniques widrow winter 
simplicity selective bayesian classifier lend average case analyses langley compare experimental results theoretical ones synthetic domains 
summary simple modification naive bayesian classifier forward selection attributes estimated accuracy increases asymptotic accuracy separate test sets domains harm accuracy 
selection algorithm appears beneficial domains involve significant correlations predictive attributes bias decisions naive bayesian classifier removed 
result 
warner toronto stephenson earliest arguments favor removing correlated features naive bayesian classifier carried process manually 
technique improves robust algorithm extends repertoire methods probabilistic induction 
owe george john ronny kohavi igor kononenko karl pfleger jeff schlimmer miroslav kubat discussions various ideas reported ray mooney provided modified code producing learning curves 
stanford university robotics laboratory provided space facilities greatly aided research 
supported part office naval research 
review naive bayesian classifier section repeats material langley 
anderson 

explorations incremental bayesian algorithm categorization 
machine learning 
brodley utgoff 

multivariate versus univariate decision trees coins technical report 
amherst university massachusetts department computer information science 
buntine 

theory learning classification rules 
dissertation department computer science university technology sydney 
caruana freitag 
press 
greedy attribute selection 
proceedings eleventh international conference machine learning 
new brunswick nj 
cestnik 

estimating probabilities crucial task machine learning 
proceedings ninth european conference artificial intelligence pp 

stockholm sweden 
cestnik kononenko bratko 

assistant knowledge elicitation tool sophisticated users 
bratko lavrac eds 
progress machine learning 
sigma press 
cheeseman kelly self stutz taylor freeman 

autoclass bayesian classification system 
proceedings fifth international conference machine learning pp 

ann arbor morgan kaufmann 
clark niblett 

cn induction algorithm 
machine learning 
connolly 

constructing hidden variables bayesian networks conceptual clustering 
proceedings tenth international conference machine learning pp 

amherst ma morgan kaufmann 
selective bayesian classifiers cooper herskovits 

bayesian method induction probabilistic networks data 
machine learning 
duda hart 

pattern classification scene analysis 
new york john wiley 
fisher 

knowledge acquisition incremental conceptual clustering 
machine learning 
gennari langley fisher 

models incremental concept formation 
artificial intelligence 
yun 

concept formation incremental conceptual clustering 
proceedings eleventh international joint conference artificial intelligence pp 

detroit mi morgan kaufmann 
holte 

simple classification rules perform data sets 
machine learning 
john kohavi pfleger 
press 
irrelevant features subset selection problem 
proceedings eleventh international conference machine learning 
new brunswick nj 
kittler 

feature selection extraction 
young fu eds handbook pattern recognition image processing new york academic press 
kononenko 

comparison inductive naive bayesian learning approaches automatic knowledge acquisition 
wielinga 
eds current trends knowledge acquisition 
amsterdam ios press 
kononenko 

semi naive bayesian classifier 
proceedings sixth european working session learning pp 

porto portugal pittman 
kubat pfurtscheller 

discovering patterns eeg signals comparative study methods 
proceedings european conference machine learning pp 

vienna springer verlag 
langley 

induction recursive bayesian classifiers 
proceedings european conference machine learning pp 

vienna springer verlag 
langley iba 

average case analysis nearest neighbor algorithm 
proceedings thirteenth international joint conference artificial intelligence pp 

chambery france 
langley iba thompson 

analysis bayesian classifiers 
proceedings tenth national conference artificial intelligence pp 

san jose ca aaai 
langley sage 
press 
oblivious decision trees cases 
proceedings aaai workshop case reasoning seattle wa aaai press 
mckusick langley 

constraints tree structure concept formation 
proceedings twelfth international joint conference artificial intelligence pp 

sydney morgan kaufmann 
michie 

sequential bayes class probability trees 
michie eds machine intelligence oxford oxford university press 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
san mateo ca morgan kaufmann 
quinlan 

programs machine learning san mateo ca morgan kaufmann 
schlimmer 

incremental adjustment representations learning 
proceedings fourth international machine learning workshop pp 

irvine ca morgan kaufmann 
skalak 
press 
prototype feature selection sampling random mutation hillclimbing algorithms 
proceedings eleventh international conference machine learning 
new brunswick nj 
warner toronto stephenson 

medical diagnosis application heart diseases 
journal american medical association 
widrow winter 

neural nets adaptive filtering adaptive pattern recognition 
ieee computer march 
