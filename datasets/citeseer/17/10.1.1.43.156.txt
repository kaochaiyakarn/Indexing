probably approximately correct learning david haussler haussler saturn ucsc edu center computer engineering information sciences university california santa cruz ca surveys theoretical results efficiency machine learning algorithms 
main tool described notion probably approximately correct pac learning introduced valiant 
define learning model look results obtained 
consider criticisms pac model extensions proposed address criticisms 
look briefly models proposed computational learning theory 
dangerous thing try formalize enterprise complex varied machine learning subjected rigorous mathematical analysis 
tractable formal model simple 
inevitably people feel important aspects activity left theory 
course right 
advisable theory machine learning having reduced entire field bare essentials 
hoped aspects phenomenon brought clearly focus tools mathematical analysis new insights gained 
light wish discuss results obtained years called pac probably approximately correct learning theory 
valiant introduced theory get computer scientists study computational efficiency algorithms look learning algorithms 
simplified notions statistical pattern recognition decision theory combining approaches computational complexity theory came notion learning problems feasible sense polynomial time algorithm solves analogy class feasible problems standard complexity theory 
supported onr valiant successful efforts 
theoretical computer scientists ai researchers obtained results theory complained proposed modified theories 
field research includes pac theory relatives called computational learning theory 
far monolithic mathematical sits base machine learning unclear theory possible desirable 
argue insights gained varied computational learning theory 
purpose short monograph survey reveal insights 
definition pac learning intent pac model successful learning unknown target concept entail obtaining high probability hypothesis approximation 
name probably approximately correct 
basic model instance space assumed set possible assignments boolean variables attributes concepts hypotheses subsets notion approximation defined assuming probability distribution defined instance space giving probability instance 
error hypothesis fixed target concept denoted error clear context defined error deltac delta denotes symmetric difference 
error probability disagree instance drawn randomly hypothesis approximation target concept error small 
obtain hypothesis 
simplest case looking independent random examples target concept example consisting instance selected randomly label instance target concept positive example gamma neg ative example 
training testing distribution noise phase 
learning algorithm computational procedure takes sample target concept consisting sequence independent random examples returns hypothesis 
cn set target concepts instance space fcn gn hn defined similarly 
define pac learnability follows concept class pac learnable hypothesis space exists polynomial time learning algorithm polynomial delta delta delta target concepts cn probability distributions instance space ffl ffi ffl ffi algorithm ffl ffi independent random examples drawn probability gamma ffi returns hypothesis hn error ffl 
smallest polynomial called sample complexity learning algorithm intent definition learning algorithm process examples polynomial time computationally efficient able produce approximation target concept high probability reasonable number random training examples 
model worst case requires number training examples needed bounded single fixed polynomial target concepts distributions instance space 
follows fix number variables instance space confidence parameter ffi invert sample complexity function plot error ffl function training sample size get usually thought learning curve fixed confidence upper envelope learning curves fixed confidence obtained varying target concept distribution instance space 
needless say curve observed experimentally 
usually plotted experimentally error versus training sample size particular target concepts instances chosen randomly single fixed distribution instance space 
curve lie curve obtained inverting sample complexity 
return point 
thing notice definition target concepts concept class may learned hypotheses different class gives flexibility 
cases interest 
target class hypothesis space 
case say properly pac learnable 
imposing requirement hypothesis class may necessary included specific knowledge base specific inference engine 
see learning difficult 
case don care hypothesis space long hypotheses evaluated efficiently 
occurs goal accurate computationally efficient prediction examples 
able freely choose hypothesis space may learning easier 
concept class exists hypothesis space hypotheses evaluated instances polynomial time pac learnable say simply pac learnable 
variants basic definition pac learnability 
important variant defines notion syntactic complexity target concepts classifies concept cn syntactic complexity 
usually syntactic complexity concept taken length number symbols shortest description fixed concept description language 
variant pac learnability number training examples allowed grow polynomially syntactic complexity target concept 
variant concept class specified concept description language represent boolean function example discussing learnability dnf disjunctive normal form formulae decision trees 
variants model algorithm request examples separate distributions drawing positive negative examples randomized coin flipping algorithms 
shown variants equivalent model described modulo minor technicalities concept classes pac learnable model pac learnable 
model easily extended non boolean attribute instance spaces instance spaces structural domains blocks world 
instances defined strings finite alphabet learnability finite automata context free grammars investigated 
outline results basic pac model number fairly sharp results notion proper pac learnability 
summarizes results 
precise definitions concept classes involved reader referred literature cited 
negative results complexity theoretic assumption rp np 

conjunctive concepts properly pac learnable class concepts form disjunction conjunctions properly pac learnable class existential conjunctive concepts structural instance spaces objects 

linear threshold concepts perceptrons properly pac learnable boolean realvalued instance spaces class concepts form conjunction linear threshold concepts properly pac learnable 
holds disjunctions linear thresholds linear thresholds multilayer perceptrons hidden units 
addition weights restricted threshold arbitrary linear threshold concepts boolean instances spaces properly pac learnable 

classes dnf cnf decision lists properly pac learnable fixed unknown classes dnf functions cnf functions decision trees properly pac learnable 
difficulties proper pac learning due computational difficulty finding hypothesis particular form specified target class 
example boolean threshold functions weights properly pac learnable boolean instance spaces rp np pac learnable general boolean threshold functions 
concrete case enlarging hypothesis space computational problem finding hypothesis easier 
class boolean threshold functions simply easier space search class boolean threshold functions weights 
similar extended hypothesis spaces classes mentioned properly pac learnable 
turns classes pac learnable 
known classes dnf functions cnf functions decision trees multilayer perceptrons hidden units pac learnable 
stronger result show concept class pac learnable show properly pac learnable result implies class pac learnable reasonable hypothesis space 
results obtained important concept classes including class boolean formulae boolean expressions general class multilayer perceptrons multiple fixed number hidden layers class deterministic finite automata 
results assume certain widely cryptographic postulates place weaker postulate rp np 
methods proving pac learnability formalization bias positive learnability results obtained 
showing efficient algorithm finds hypothesis particular hypothesis space consistent sample concept target class 
sample complexity algorithm polynomial 
consistent mean hypothesis agrees example training sample 
algorithm finds hypothesis exists called consistent algorithm 
size hypothesis space increases may easier find consistent hypothesis require random training examples insure hypothesis accurate high probability 
limit subset instance space allowed hypothesis trivial find consistent hypothesis sample size proportional size entire instance space required insure accurate 
fundamental tradeoff computational complexity sample complexity learning 
restriction particular hypothesis spaces limited size form bias explored facilitate learning 
addition cardinality hypothesis space parameter known vapnik chervonenkis vc dimension hypothesis space shown useful quantifying bias inherent restricted hypothesis space 
vc dimension hypothesis space denoted cdim defined maximum number instances labeled positive negative examples possible ways labeling consistent hypothesis 
hypothesis space target class cn hn 
shown consistent algorithm learning sample complexity ffl gamma ffl cdim hn ln ffl ln ffi improves earlier bounds may considerable overestimate 
terms cardinality hn denoted shown sample complexity ffl ln ffi hypothesis spaces boolean domains second bound gives better bound 
linear threshold functions notable exception vc dimension class linear logarithm cardinality quadratic 
hypothesis spaces real valued attributes infinite bound applicable 
criticisms pac model criticisms leveled pac model ai researchers interested empirical machine learning 
worst case emphasis model unusable practice 
notions target concepts noise free training data restrictive practice 
take turn 
aspects worst case nature pac model issue 
worst case model measure computational complexity learning algorithm definition sample complexity worst case number random examples needed target concepts target class distributions instance space 
address issue 
pointed worst case definition sample complexity means calculate sample complexity algorithm exactly expect overestimate typical error hypothesis produced function training set size particular target concept particular distribution instance space 
compounded fact usually calculate sample complexity algorithm exactly relatively simple consistent algorithm 
forced fall back upper bounds sample complexity hold consistent algorithm previous section may contain constants 
upshot basic pac theory predicting learning curves 
variants pac model come closer 
simple variant distribution specific define analyze sample complexity learning algorithm specific distribution instance space uniform distribution boolean space 
potential problems 
finding distributions analyzable indicative distributions arise practice 
second bounds obtained may sensitive particular distribution analyzed reliable actual distribution slightly different 
refined bayesian extension pac model explored 
bayesian approach involves assuming prior distribution possible target concepts training instances 
distributions average error hypothesis function training sample size function particular training sample defined 
gamma ffi confidence intervals pac model defined 
experiments model small learning problems encouraging needs done sensitivity analysis simplifying calculations larger problems analysed 
distribution specific learning provides increasingly important counterpart pac theory 
variant pac model designed address issues probability mistake model explored 
worst case model designed specifically help understand issues incremental learning 
looking sample complexity defined measure performance probability learning algorithm incorrectly guesses label tth training example sequence random examples 
course algorithm allowed update hypothesis new training example processed grows expect probability mistake example decrease 
fixed target concept fixed distribution instance space easy see probability mistake example average error hypothesis produced algorithm gamma random training examples 
probability mistake example exactly plotted empirical learning curves plot error versus sample size average runs learning algorithm sample size 
comparisons worst case probability mistake tth example possible target concepts distributions training examples probability mistake tth example target concept selected random prior distribution target class examples drawn random certain fixed distribution bayesian approach 
call worst case probability mistake call average case probability mistake 
results summarized follows 
concept class dn cdim cn 
concept class consistent algorithm hypothesis space worst case probability mistake example dn ln dn furthermore particular consistent algorithms concept classes worst case probability mistake example omega gammas dn ln best said general arbitrary consistent algorithms 
second concept class exists learning algorithm necessarily consistent computationally efficient worst case probability mistake example dn gamma 
extra factor appears bound 
removed 
addition learning algorithm worst case probability mistake example omega gamma dn 
furthermore particular concept classes particular prior probability distributions concepts classes particular distributions instance spaces classes average case probability mistake example omega gamma dn learning algorithm 
results show interesting things 
certain learning algorithms perform better arbitrary consistent learning algorithms worst case average case restricted setting definitely learning just finding consistent hypothesis appropriately biased hypothesis space 
second worst case worse average case 
experiments learning perceptrons multilayer perceptrons shown cases dn predictor actual average case learning curves backpropagation synthetic random data 
overestimate natural data domains learning conjunctive concepts uniform distribution 
distribution algorithm specific aspects learning situation taken account 
general concur extensions pac model required explain learning curves occur practice 
amount experimentation distribution specific theory replace security provided distribution independent bound 
second criticism pac model assumptions defined target concepts training data unrealistic practice 
certainly true 
pointed computational hardness results learning described having established simple noise free case hold general case 
pac model advantage allowing state negative results simply strongest form 
positive learnability results strengthened applicable practice extensions pac model needed purpose 
proposed see 
definitions target concepts random examples hypothesis error pac model just simplified versions standard definitions statistical pattern recognition decision theory reasonable thing go back established fields general definitions developed 
probability misclassification measure error general loss function defined pair consisting guessed value actual value classification gives non negative real number indicating cost charged particular guess particular actual value 
error hypothesis replaced average loss hypothesis random example 
loss guess wrong right discrete loss get pac notion error special case 
general loss function choose false positives expensive false negatives viceversa useful 
loss function allows handle cases possible values classification 
includes problem learning real valued functions choose guess loss functions 
second assuming examples generated selecting target concept generating random instances labels agreeing target concept assume random instance randomness label 
instance particular probability drawn instance possible classification value particular probability occurring 
random process described making independent random draws single joint probability distribution set possible labeled instances 
target concepts attribute noise classification noise kinds noise modeled way 
target concept noise distribution instance space bundled joint probability measure labeled examples 
goal learning find hypothesis minimizes average loss examples drawn random joint distribution 
pac model disregarding computational complexity considerations viewed special case set discrete loss function added twist learning performance measured respect worst case joint distributions entire probability measure concentrated set examples consistent single target concept particular type 
pac case possible get arbitrarily close zero loss finding closer closer approximations underlying target concept 
possible general case ask close hypothesis produced learning algorithm comes performance best possible hypothesis hypothesis space 
unbiased hypothesis space known bayes optimal classifier 
pac research general framework 
quadratic loss function mentioned place discrete loss kearns shapire investigate problem efficiently learning real valued regression function gives probability classification instance 
shown vc dimension related tools originally developed vapnik chervonenkis type analysis applied study learning neural networks 
restrictions whatsoever placed joint probability distribution governing generation examples notion target concept target class eliminated entirely 
theoretical learning models number theoretical approaches machine learning computational learning theory 
total mistake bound model 
arbitrary sequence examples unknown target concept fed learning algorithm seeing instance algorithm predict label instance 
incremental learning model probability mistake model described assumed instances drawn random measure learning performance total number mistakes prediction worst case sequences training examples arbitrarily long target concepts target class 
call quantity worst case mistake bound learning algorithm 
interest case exists polynomial time learning algorithm concept class worst case mistake bound target concepts cn polynomial pac model mistake bounds allowed depend syntactic complexity target concept 
perceptron algorithm learning linear threshold functions boolean domain example learning algorithm worst case mistake bound 
bound comes directly bound number updates perceptron convergence theorem see 
worst case mistake bound perceptron algorithm polynomial linear number boolean attributes target concepts conjunctions disjunctions concept expressible weights arbitrary threshold 
variant perceptron learning algorithm multiplicative additive weight updates developed significantly improved mistake bound target concepts small syntactic complexity 
performance algorithm extensively analysed case examples may mislabeled 
shown polynomial time learning algorithm target class polynomial worst case mistake bound pac learnable 
general methods converting learning algorithm worst case mistake bound pac learning algorithm low sample complexity 
total mistake bound model unrelated pac model 
fascinating transformation learning algorithms weighted majority method 
method combining incremental learning algorithms single incremental learning algorithm powerful robust component algorithms 
idea simple 
component learning algorithms run parallel sequence training examples 
example algorithm prediction predictions combined weighted voting scheme determine prediction master algorithm 
receiving feedback prediction master algorithm adjusts voting weights component algorithms increasing weights correct prediction decreasing weights guessed wrong case multiplicative factor 
shown method combining learning algorithms produces master algorithm worst case mistake bound approaches best worst case mistake bound component learning algorithms resulting algorithm robust regard mislabeled examples 
weighted majority method conjunction conversion mentioned design better pac learning algorithms 
pac total mistake bound models extended significantly allowing learning algorithms perform experiments queries teacher learning 
simplest type query membership query learning algorithm proposes instance instance space told instance member target concept 
ability membership queries greatly enhance ability algorithm efficiently learn target concept mistake bound pac models 
shown polynomialtime algorithms polynomially membership queries polynomial worst case mistake bounds learning 
monotone dnf concepts disjunctive normal form negated variables 
formulae boolean formulae variable appears 
deterministic finite automata 
horn sentences propositional prolog programs 
addition general method converting efficient learning algorithm membership queries polynomial worst case mistake bound pac learning algorithm long pac algorithm allowed membership queries 
concept classes listed pac learnable membership queries allowed 
contrasts evidence cryptographic assumptions classes pac learnable random examples 
brief survey able cover small fraction results obtained computational learning theory 
glimpse results refer reader 
hope convinced reader insights provided line investigation difficulty searching hypothesis spaces notion bias effect required training size effectiveness majority voting methods usefulness actively making queries learning effort worthwhile 
amsterdam 
valiant learning model extensions assessment 
master thesis mit department electrical engineering computer science jan 
angluin 
learning regular sets queries counterexamples 
information computation nov 
angluin 
queries concept learning 
machine learning 
angluin frazier pitt 
learning conjunctions horn clauses 

manuscript 
angluin hellerstein karpinski 
learning read formulas queries 
jacm 
appear 
angluin laird 
learning noisy examples 
machine learning 
baum 
nearest neighbor back propogation accurate feasible sized sets examples 
snowbird conference neural networks computing 
unpublished manuscript 
itai 
learnability fixed distributions 
proc 
workshop comp 
learning theory pages morgan kaufmann san mateo ca 
bergadano saitta 
error boolean concept descriptions 
proceedings european working session learning pages 
blum rivest 
training neuron neural net np complete 
proceedings workshop computational learning theory pages published morgan kaufmann san mateo ca 
blumer ehrenfeucht haussler warmuth 
learnability dimension 
jacm 
blumer ehrenfeucht haussler warmuth 
occam razor 
information processing letters 
buntine 
theory learning classification rules 
phd thesis university technology sydney 
forthcoming 
cover 
geometrical statistical properties systems linear inequalities applications pattern recognition 
ieee trans 
electronic computers ec 
duda hart 
pattern classification scene analysis 
wiley 

linear function neurons structure training 
biol 
cybern 
haussler 
generalizing pac model neural net learning applications 
information computation 
appear 
haussler 
learning conjunctive concepts structural domains 
machine learning 
haussler 
quantifying inductive bias ai learning algorithms valiant learning framework 
artificial intelligence 
haussler kearns littlestone warmuth 
equivalence models polynomial learnability 
information computation 
appear 
haussler littlestone warmuth 
predicting functions randomly drawn points 
proceedings th annual symposium foundations computer science pages ieee 
haussler pitt editors 
proceedings workshop computational learning theory 
morgan kaufmann san mateo ca 
john shawe taylor biggs 
bounding sample size vapnik chervonenkis dimension 
technical report csd tr university london surrey england 
kearns li 
learning presence malicious errors 
th acm symposium theory computing pages chicago 
kearns li pitt valiant 
learnability boolean formulae 
th acm symposium theory computing pages new york 
kearns schapire 
efficient learning probabilistic concepts 

manuscript 
kearns valiant 
cryptographic limitations learning boolean formulae finite automata 
st acm symposium theory computing pages seattle wa 
littlestone 
line batch learning 
proceedings nd workshop computational learning theory pages published morgan kaufmann 
littlestone 
learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 
littlestone 
mistake bounds logarithmic linear threshold learning algorithms 
phd thesis university calif santa cruz 
littlestone warmuth 
weighted majority algorithm 
th annual ieee symposium foundations computer science pages 
mitchell 
need biases learning generalizations 
technical report cbm tr rutgers university new brunswick nj 
natarajan 
learning sets functions 
machine learning 
pitt 
inductive inference dfas computational complexity 
technical report illinois urbana champaign 
pitt valiant 
computational limitations learning examples 
acm 
rivest haussler warmuth editors 
proceedings workshop computational learning theory 
morgan kaufmann san mateo ca 
rivest 
learning decision lists 
machine learning 
rumelhart 

personal communication 
pazzani 
average case analysis empirical explanation learning algorithms 
technical report uc irvine 
tesauro cohn 
experimental tests statistical learning theories 
snowbird conference neural networks computing 
unpublished manuscript 
valiant 
learning disjunctions conjunctions 
proc 
th ijcai pages los angeles august 
valiant 
theory learnable 
comm 
acm 
vapnik 
estimation dependences empirical data 
springer verlag new york 
