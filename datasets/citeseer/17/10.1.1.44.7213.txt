kluwer academic publishers boston 
manufactured netherlands 
comparison prediction accuracy complexity training time old new classi cation algorithms lim wei yin loh limt stat wisc edu loh stat wisc edu department statistics university wisconsin madison wi yu shan shih math ccu edu tw department mathematics national chung cheng university taiwan received january revised may editor william cohen 
decision tree statistical neural network algorithms compared datasets terms classi cation accuracy training time case trees number leaves 
classi cation accuracy measured mean error rate mean rank error rate 
criteria place statistical spline algorithm called top statistically signi cantly di erent algorithms 
statistical algorithm logistic regression second respect accuracy criteria 
accurate decision tree algorithm quest linear splits ranks fourth fth respectively 
spline statistical algorithms tend accuracy require relatively long training times 
example third terms median training time 
requires hours training compared seconds algorithms 
quest logistic regression algorithms substantially faster 
decision tree algorithms univariate splits ind cart quest best combinations error rate speed 
tends produce trees twice leaves ind cart quest 
keywords classi cation tree decision tree neural net statistical classi er 

current research machine learning statistics communities algorithms decision tree classi ers 
emphasis accuracy algorithms 
study called statlog project compares accuracy decision tree algorithms non decision tree algorithms large number datasets 
studies smaller scale include brodley brown mingers shavlik mooney towell 
comprehensibility tree structures received attention 
comprehensibility typically decreases increase tree size complexity 
supported part army research ce zer university wisconsin 
supported part fellowship republic china national science council 

lim 
loh 
shih trees employ kind tests prediction accuracy fewer leaves usually preferred 
aha survey methods tree simpli cation improve comprehensibility 
third criterion largely ignored relative training time algorithms 
statlog project nds algorithm uniformly accurate datasets studied 
algorithms possess comparable accuracy 
algorithms excessive training times may undesirable 
purpose extend results statlog project ways 
addition classi cation accuracy size trees compare training times algorithms 
training time depends somewhat implementation turns large di erences times seconds versus days di erences attributed implementation 

include decision tree algorithms included statlog project plus tree oc quest 

include newest spline statistical algorithms 
classi cation accuracy may benchmarks comparison algorithms 

study ect adding independent noise attributes classi cation accuracy appropriate tree size algorithm 
turns possibly algorithms adapt noise quite 

examine scalability promising algorithms sample size increased 
experiment compares decision tree algorithms classical modern statistical algorithms neural network algorithms 
datasets taken university california irvine repository machine learning databases uci 
fourteen datasets real life domains arti cially constructed 
datasets statlog project 
increase number datasets study ect noise attributes double number datasets adding noise attributes 
yields total datasets 
section brie describes algorithms section gives background datasets 
section explains experimental setup study section analyzes results 
issue scalability studied section recommendations section 
algorithms short description algorithm 
details may cited 
algorithm requires class prior probabilities proportional training sample sizes 
comparison classification algorithms 
trees rules cart version cart implemented cart style ind package gini index diversity splitting criterion 
trees se se pruning rules denoted ic ic respectively 
software obtained address ic www arc nasa gov ic projects bayes group ind ind program html 
plus tree variant cart algorithm written language 
described clark pregibon 
employs deviance splitting criterion 
best tree chosen fold cross validation 
pruning performed tree function library statlib archive lib stat cmu edu 
se se trees denoted st st respectively 
release default settings including pruning www cse unsw edu au quinlan 
tree constructed rule induction program produce set rules 
trees denoted rules 
fact fast classi cation tree algorithm described loh 
employs statistical tests select attribute splitting node uses discriminant analysis nd split point 
size tree determined set stopping rules 
trees univariate splits splits single attribute denoted linear combination splits splits linear functions attributes denoted ftl 
fortran program obtained www stat wisc edu loh 
quest new classi cation tree algorithm described loh shih 
quest univariate linear combination splits 
unique feature attribute selection method negligible bias 
attributes uninformative respect class attribute approximately chance selected split node 
fold cross validation prune trees 
univariate se se trees denoted qu qu respectively 
corresponding trees linear combination splits denoted ql ql respectively 
results version program 
software obtained www stat wisc edu loh quest html 
ind due buntine 
version default settings 
ind comes standard prede ned styles 
compare bayesian styles bayes bayes opt mml mml opt denoted ib ibo im imo respectively 
opt methods extend non opt methods growing di erent trees storing compact graph structure 
time memory intensive opt styles increase classi cation accuracy 

lim 
loh 
shih oc algorithm described murthy kasif salzberg 
version www cs jhu edu salzberg announce oc html compare styles 
rst denoted ocm default uses mixture univariate linear combination splits 
second option denoted ocu uses univariate splits 
third option denoted ocl uses linear combination splits 
options kept default values 
algorithm described brodley 
constructs decision tree multivariate tests linear combinations attributes 
tree denoted lmt 
default values software ecn purdue edu brodley software html 
cal fraunhofer society institute information data processing germany 
version 
cal designed speci cally numerical valued attributes 
procedure handle categorical attributes mixed attributes numerical categorical included 
study optimize parameters control tree construction 
prede ned threshold signi cance level randomly split training set parts strati ed classes construct tree third validation set choose optimal parameter con guration 
employ shell program comes cal package choose best parameters varying steps 
best combination values minimize error rate validation set chosen 
tree constructed records training set chosen parameter values 
denoted cal level decision tree classi es examples basis split single attribute 
split categorical attribute categories produce leaves leaf reserved missing attribute values 
hand split continuous attribute yield leaves number classes leaf reserved missing values 
software obtained www csi ca holte learning sites html 

statistical algorithms lda linear discriminant analysis classical statistical method 
models instances class normally distributed common covariance matrix 
yields linear discriminant functions 
qda quadratic discriminant analysis 
models class distributions normal estimates covariance matrix corresponding sample covariance matrix 
result discriminant functions quadratic 
details lda qda statistics textbooks johnson comparison classification algorithms 
sas proc discrim implementation lda qda default settings 
nn sas proc discrim implementation nearest neighbor method 
pooled covariance matrix compute mahalanobis distances 
log logistic discriminant analysis 
results obtained logistic regression see agresti fortran routine written rst author www stat wisc edu limt 
fda exible discriminant analysis generalization linear discriminant analysis casts classi cation problem involving regression 
mars nonparametric regression procedure studied 
plus function fda mda library statlib archive 
models additive model degree denoted fm model containing rst order interactions degree penalty denoted fm 
pda form penalized lda 
designed situations highly correlated attributes 
classi cation problem cast penalized regression framework optimal scoring 
pda implemented plus function fda method gen ridge 
mda stands mixture discriminant analysis :10.1.1.10.3633
ts gaussian mixture density functions class produce classi er 
mda implemented plus library mda 
pol algorithm 
ts logistic regression model linear splines tensor products 
provides estimates conditional class probabilities predict class labels 
pol implemented plus function poly fit library statlib archive 
model selection done fold crossvalidation 

neural networks lvq learning vector quantization algorithm plus class library statlib archive 
details algorithm may kohonen 
percent training set initialize algorithm function 
training carried optimized learning rate function fast robust lvq algorithm 
additional ne tuning learning performed function lvq 
number iterations times size training set lvq 
default values learning rate parameter lvq respectively 

lim 
loh 
shih rbf radial basis function network implemented sas tnn sas macro feedforward neural networks www sas com :10.1.1.130.4558
network architecture speci ed arch rbf argument 
study construct network hidden layer 
number hidden units chosen total number input output units hidden units dna dna datasets hidden units tae tae datasets memory storage limitations 
macro perform model selection choose optimal number hidden units utilize capability taken long datasets see table 
results reported algorithm regarded lower bounds performance 
hidden layer fully connected input output layers direct connection input output layers 
output layer class represented unit value particular category category 
avoid local optima preliminary conducted best estimates subsequent training 
details radial basis function network bishop ripley 

datasets brie describe sixteen datasets study modi cations experiment 
fourteen real domains arti cially created 
fifteen datasets available uci 
wisconsin breast cancer bcw 
breast cancer databases uci collected university wisconsin wolberg 
problem predict tissue sample taken patient breast malignant benign 
classes numerical attributes observations 
sixteen instances contain single missing attribute value removed analysis 
results records 
error rates estimated fold cross validation 
decision tree analysis subset data fact algorithm reported wolberg tanner loh wolberg tanner loh 
dataset analyzed linear programming methods 
method choice cmc 
data taken national indonesia prevalence survey 
samples married women pregnant know pregnant time interview 
problem predict current method choice long term methods short term methods woman demographic socio economic characteristics 
classes numerical attributes categorical attributes records 
error rates estimated fold cross validation 
dataset available uci 
comparison classification algorithms statlog dna dna 
uci dataset molecular biology statlog project 
splice junctions points dna sequence super uous dna removed process protein creation higher organisms 
problem recognize sequence dna boundaries exons parts dna sequence retained splicing introns parts dna sequence spliced 
classes categorical attributes having categories 
categorical attributes represent window nucleotides having categories 
middle point window classi ed exon intron boundaries intron exon boundaries 
examples database divided randomly training set size test set size 
error rates estimated test set 
statlog heart disease hea 
uci dataset cleveland clinic foundation courtesy 
problem concerns prediction presence absence heart disease results various medical tests carried patient 
classes numerical attributes categorical attributes records 
statlog project employed unequal misclassi cation costs 
equal costs algorithms allow unequal costs 
error rates estimated fold cross validation 
boston housing bos 
uci dataset gives housing values boston 
classes twelve numerical attributes binary attribute records 
loh classes created attribute median value owner occupied homes follows class log median value class log median value class 
error rates estimated fold crossvalidation 
led display led 
arti cial domain described breiman friedman olshen stone 
contains boolean attributes representing light emitting diodes classes set decimal digits 
attribute value zero corresponding light digit 
attribute value percent probability having value inverted 
class attribute integer zero inclusive 
program uci generate records training set records test set 
error rates estimated test set 
bupa liver disorders 
uci dataset donated forsyth 
problem predict male patient liver disorder blood tests alcohol consumption 
classes numerical attributes records 
error rates estimated fold cross validation 
pima indian diabetes pid 
uci dataset contributed 
patients dataset females years old pima 
lim 
loh 
shih indian heritage living near phoenix arizona usa 
problem predict patient test positive diabetes number physiological measurements medical test results 
classes numerical attributes records 
original dataset consists records numerical attributes 
attributes notably serum insulin contain zero values physically impossible 
remove serum insulin records impossible values attributes 
error rates estimated fold cross validation 
statlog satellite image sat 
uci dataset gives multi spectral values pixels neighborhoods satellite image classi cation associated central pixel neighborhood 
aim predict classi cation multi spectral values 
classes numerical attributes 
training set consists records test set consists records :10.1.1.130.4558
error rates estimated test set 
image segmentation seg 
uci dataset statlog project 
samples database outdoor images 
images hand segmented create classi cation pixel sky foliage cement window path grass 
classes nineteen numerical attributes records dataset 
error rates estimated fold cross validation 
algorithm handle dataset modi cation program requires large amount memory 
algorithms discretize attribute attributes categories 
attitude smoking restrictions smo 
survey dataset obtained lib stat cmu edu datasets csb 
problem predict attitude restrictions smoking workplace prohibited restricted unrestricted related smoking related covariates 
classes numerical attributes categorical attributes 
divide original dataset training set size test set size 
error rates estimated test set 
thyroid disease thy 
uci ann train dataset contributed werner 
problem determine patient 
classes normal functioning numerical attributes fteen binary attributes 
training set consists records test set records 
error rates estimated test set 
statlog vehicle silhouette veh 
uci dataset originated turing institute glasgow scotland 
problem classify silhouette types vehicle set features extracted silhouette 
comparison classification algorithms vehicle viewed angles 
model vehicles double decker bus van saab opel manta 
classes eighteen numerical attributes records 
error rates estimated fold cross validation 
congressional voting records vot 
uci dataset gives votes member house representatives th congress sixteen key issues 
problem classify democrat republican sixteen votes 
classes sixteen categorical attributes categories records 
error rates estimated fold cross validation 
waveform wav 
arti cial class problem waveforms 
class consists random convex combination waveforms sampled integers noise added 
description generating data breiman friedman olshen stone program available uci 
numerical attributes records training set 
error rates estimated independent test set records 
ta evaluation tae 
data consist evaluations teaching performance regular summer teaching assistant ta assignments statistics department university wisconsin madison 
scores grouped roughly equal sized categories low medium high form class attribute 
predictor attributes ta native english speaker binary ii course instructor categories iii course categories iv summer regular semester binary class size numerical 
dataset rst reported loh shih 
di ers datasets categorical attributes large numbers categories 
result decision tree algorithms cart employ exhaustive search usually take longer train algorithms 
cart evaluate splits categorical attribute values 
error rates estimated fold cross validation 
dataset available uci 
summary attribute features datasets table 
experimental setup algorithms designed categorical attributes 
cases categorical attribute converted vector attributes 
categorical attribute takes values fc replaced dimensional vector 
vector consists zeros 
ected algorithms statistical neural network algorithms tree algorithms ftl ocu ocl ocm lmt 

lim 
loh 
shih table 
characteristics datasets 
columns give number type added noise attributes dataset 
number values taken class attribute denoted notation denotes standard normal distribution ui uniform distribution integers inclusive uniform distribution unit interval 
abbreviation stands ui 
original attributes noise attributes num 
categorical tot 
numerical 
set size bcw ui cmc dna hea bos led pid sat ui seg smo thy veh vot wav tae order increase number datasets study ect noise attributes algorithm created sixteen new datasets adding independent noise attributes :10.1.1.130.4558
numbers types noise attributes added right panel table 
name new dataset original dataset addition symbol 
example bcw dataset noise added denoted bcw 
dataset di erent ways estimate error rate algorithm 
large datasets size larger test set size test set estimate error rate 
classi er constructed records training set tested test set 
twelve datasets analyzed way 
remaining datasets fold cross validation procedure estimate error rate 
dataset randomly divided disjoint subsets containing approximately number records 
sampling strati ed class labels ensure subset class proportions roughly dataset 
comparison classification algorithms 
subset classi er constructed records 
classi er tested withheld subset obtain cross validation estimate error rate 

cross validation estimates averaged provide estimate classi er constructed data 
algorithms implemented di erent programming languages languages available platforms types unix workstations study 
workstation type implementation language algorithm table 
relative performance workstations spec marks table 
oating point spec marks show task takes second dec take seconds sparcstation ss sparcstation ss respectively 
enable comparisons training times reported terms dec equivalent seconds training times recorded ss ss divided respectively 
table 
hardware software platform algorithm 
workstations dec alpha model dec sun sparcstation model ss sun sparcstation ss 
algorithm platform algorithm platform tree rules st plus tree se dec qu quest 
se dec lmt linear dec qu quest 
se dec cal cal ss ql quest linear se dec single split dec ql quest linear se dec fact univariate dec statistical ftl fact linear dec lda linear discriminant anal 
dec sas trees dec qda quadratic discriminant anal 
dec sas rules dec nn nearest neighbor dec sas ib ind bayes style ss log linear logistic regression dec ibo ind bayes opt style ss fm fda degree ss im ind mml style ss fm fda degree ss imo ind mml opt style ss pda penalized lda ss ic ind cart se ss mda mixture discriminant anal 
ss ic ind cart se ss pol ss ocu oc univariate ss ocl oc linear ss neural network ocm oc mixed ss lvq learning vector quantization ss st plus tree se dec rbf radial basis function network dec sas 
results report summary results analysis 
fuller details including error rate training time algorithm dataset may obtained id nl mach ml htm 

lim 
loh 
shih table 
spec benchmark summary workstation specfp specint source dec dec model spec newsletter mhz vol 
issue june ss sun sparcstation spec newsletter model mhz vol 
issue june ss sun sparcstation spec newsletter mhz vol 
issue june 
exploratory analysis error rates formal statistical analysis results helpful study summary table 
mean error rate algorithm datasets second row 
minimum maximum error rates plurality rule dataset columns 
denote smallest observed error rate row dataset 
algorithm error rate standard error consider close best indicate table 
standard error estimated follows 
independent test set denote size test set 
cross validation estimate denote size training set 
standard error estimated formula algorithm largest error rate row indicated total numbers marks algorithm third fourth rows table 
may drawn table 
algorithm pol lowest mean error rate 
ordering algorithms terms mean error rate upper half table 
algorithms ranked terms total number marks 
criterion accurate algorithm pol fteen marks marks 
eleven algorithms marks 
ranked increasing order number marks parentheses ftl ocm st fm mda fm ocl qda nn lvq 
excluding remaining algorithms rank order decreasing number marks parentheses pol log ql lda pda ql ocu qu qu ibo rbf imo im ic st ic cal ib lmt 
top algorithms rank top upper half table 
comparison classification algorithms table 
minimum maximum naive plurality rule error rates dataset 
mark indicates algorithm error rate standard error minimum dataset 
mark indicates algorithm worst error rate dataset 
mean error rate algorithm second row 
decision trees rules statistical algorithms nets error rates qu qu ql ql ftl ib ibo im imo ic ic ocu ocl ocm st st lmt cal lda qda nn log fm fm pda mda pol lvq rbf min max naive mean bcw pp bcw pp cmc pp cmc pp dna dna hea hea bos pp bos led pp ppp pp led pp pid pp pid pp sat sat seg seg smo pp pp pp smo pp pp thy ppp ppp pp thy ppp pp veh veh vot pp pp vot pp pp pp pp pp wav wav tae tae pp :10.1.1.130.4558
columns table show algorithms accurate plurality rule 
nn cmc cmc smo qda smo thy thy ftl tae st tae 

easiest datasets classify bcw bcw vot vot error rates lie 

lim 
loh 
shih table 
ordering algorithms mean error rate mean rank error rate mean pol log mda ql lda ql pda ic fm ibo imo error rate lmt im qu qu ocu ic ib ocm st st ftl fm rbf ocl lvq cal nn qda mean pol log fm fm ql lda qu imo mda pda rank ibo ql ic im ftl ocu qu ic st st error rate lmt ocm ib rbf qda lvq ocl cal nn 
di cult classify cmc cmc tae minimum error rates greater 
di cult datasets smo smo 
case smo marginally lower error rate plurality rule 
algorithm lower error rate plurality rule smo 

datasets largest range error rates thy thy rates range 
maximum due qda 
qda ignored maximum error rate drops 

datasets mark 
pol sat lvq sat fm seg ibo veh veh qda times 

addition noise attributes appear increase signi cantly error rates algorithms 

statistical signi cance error rates 
analysis variance statistical procedure called mixed ects analysis variance test simultaneous statistical signi cance di erences mean error rates algorithms controlling di erences datasets 
assumption ects datasets act random sample normal distribution quite robust violation assumption 
data procedure gives signi cance probability null hypothesis algorithms mean error rate strongly rejected 
simultaneous con dence intervals di erences mean error rates obtained tukey method 
procedure di erence mean error rates algorithms statistically signi cant level di er 
comparison classification algorithms visualize result plots mean error rate algorithm versus median training time seconds 
solid vertical line plot units right mean error rate pol 
algorithm lying left line mean error rate statistically signi cantly di erent pol 
algorithms seen form clusters respect training time 
clusters roughly delineated horizontal dotted lines correspond training times minute minutes hour 
shows magni ed plot eighteen algorithms median training times minutes mean error rate statistically signi cantly di erent pol 

analysis ranks avoid normality assumption analyze ranks algorithms datasets 
dataset algorithm lowest error rate assigned rank second lowest rank average ranks assigned case ties 
lower half table gives ordering algorithms terms mean rank error rates 
pol rst 
note mean rank pol 
shows far uniformly accurate datasets 
comparing methods ordering table seen pol log ql lda algorithms consistently performance 
algorithms perform criterion mda fm fm 
case mda low mean error rate due excellent performance datasets veh veh wav wav algorithms poorly 
domains concern shape identi cation datasets contain numerical attributes 
mda generally rest datasets reason tenth place ranking terms mean rank 
situation fm fm quite di erent 
low mean rank indicates fm usually performer 
fails miserably seg seg datasets reporting error rates fty percent algorithms error rates percent 
fm robust algorithms 
fm appears lack robustness lesser extent 
worst performance bos dataset error rate percent compared percent algorithms 
number marks algorithm table predictor erratic poor performance 
mda fm fm mark 
friedman test standard procedure testing statistical significance di erences mean ranks 
experiment gives signi cance probability null hypothesis algorithms equally accurate average rejected 
di erence mean ranks greater statistically signi cant level 
pol statistically signi cantly di erent algorithms mean rank equal 
shows plot median training time versus mean rank 
algorithms lie left vertical line statistically signi cantly di erent pol 
magni ed plot subset algo 
lim 
loh 
shih mean error rate qu qu ql ql ftl ib ibo im imo ic ic ocu ocl ocm st st lmt cal lda qda nn log fm fm pda mda pol lvq rbf hr min min methods mean error rate qu qu ql ql ftl ib im ic ic ocu lmt lda log pda mda min min min accuracy sig 
different pol 
plots median training time versus mean error rate 
vertical axis log scale 
solid vertical line plot divides algorithms groups mean error rates algorithms left group di er signi cantly simultaneous signi cance level pol minimum mean error rate 
plot shows algorithms statistically signi cantly di erent pol terms mean error rate median training time minutes 
comparison classification algorithms rithms signi cantly di erent pol median training time minutes 
algorithms di er statistically signi cantly pol terms mean error rate form subset di er pol terms mean ranks 
rank test appears powerful analysis variance test experiment 
fteen algorithms may recommended applications accuracy short training time desired 

training time table gives median dec equivalent training time algorithm relative training time datasets 
owing large range training times order relative fastest algorithm dataset reported 
fastest algorithm indicated 
algorithm times slow indicated value example case dna dataset fastest algorithms requiring seconds 
slowest algorithm fm takes seconds days times slow 
columns table give fastest slowest times dataset 
table gives ordering algorithms fastest slowest median training time 
fastest algorithm followed closely ftl lda 
reasons superior speed compared decision tree algorithms 
splits categorical attribute subnodes number categories 
wastes time forming subsets categories 
second pruning method require cross validation increase training time fold 
classical statistical algorithms qda nn quite fast 
expected decision tree algorithms employ univariate splits faster linear combination splits 
slowest algorithms pol fm rbf spline neural network 
ic ic st st claim implement cart algorithm ind versions faster plus versions 
reason ic ic written st st written language 
reason ind versions heuristics buntine personal communication greedy search number categories categorical attribute large 
apparent tae dataset categorical attributes categories 
case ic ic take seconds versus half hours st st 
results table indicate ind classi cation accuracy adversely ected heuristics see aronis provost possible heuristic 
level tree may appear surprising faster algorithms produce multi level trees 
reason splits continuous attribute intervals number classes 
hand splits continuous attribute intervals 
spend lot time search intervals 

lim 
loh 
shih mean rank qu qu ql ql ftl ib ibo im imo ic ic ocu ocl ocm st st lmt cal lda qda nn log fm fm pda mda pol lvq rbf hr min min methods mean rank qu qu ql ql ftl im ic ic ocu lda log pda mda min min min accuracy sig 
different pol 
plots median training time versus mean rank error rates 
vertical axis log scale 
solid vertical line plot divides algorithms groups mean ranks algorithms left group di er signi cantly simultaneous signi cance level pol 
plot shows algorithms statistically signi cantly di erent pol terms mean rank median training time minutes 
comparison classification algorithms table 
dec equivalent training times relative times algorithms 
second third rows give median training time rank algorithm 
entry subsequent rows indicates algorithm times slower fastest algorithm dataset 
fastest algorithm denoted entry 
minimum maximum training times columns 
denote seconds minutes hours days respectively 
decision trees rules statistical algorithms nets cpu time qu qu ql ql ftl ib ibo im imo ic ic ocu ocl ocm st st lmt cal lda qda nn log fm fm pda mda pol lvq rbf min max time rank bcw cmc dna hea bos led pid sat seg smo thy veh vot wav tae table 
ordering algorithms median training time ftl lda qda nn ib im ocu ic ic pda lvq mda qu qu log lmt ql ql ocm st ocl st fm ibo imo cal pol fm rbf 
size trees table gives number leaves tree algorithm dataset noise attributes added 
case error rate obtained fold crossvalidation entry mean number leaves cross validation trees 
table shows number leaves changes addition noise 
lim 
loh 
shih attributes 
mean median number leaves classi er columns tables 
ibo imo clearly yield largest trees far 
apart necessarily short design algorithm shortest trees average ql followed closely ftl ocl 
ranking algorithms univariate splits increasing median number leaves ic st qu ic st ocu qu 
algorithm tends produce trees leaves algorithms 
reason may due pruning error rates quite 
binary tree algorithms splits categorical attribute nodes number categories 
addition noise attributes typically decreases size trees cal tend grow larger trees imo wildly 
results complement oates jensen looked ect sample size number leaves decision tree algorithms signi cant relationship tree size training sample size 
observed tree algorithms employ cost complexity pruning better able control tree growth 

scalability algorithms di erences mean error rates pol algorithms statistically signi cant clear error rate sole criterion pol method choice 
unfortunately pol algorithms 
see training times increase sample size small scalability study carried algorithms qu ql ftl ic lda log fm pol 
training times measured algorithms training sets size 
datasets generate samples sat smo tae new large uci dataset called adult classes continuous categorical attributes 
rst datasets large experiment bootstrap re sampling employed generate training sets 
samples randomly drawn replacement dataset 
avoid getting replicate records value class attribute sampled case randomly changed value probability 
new value selected pool alternatives equal probability 
bootstrap sampling carried adult dataset records 
nested training sets obtained random sampling replacement 
times required train algorithms plotted log log scale 
exception pol fm log logarithms training times increase linearly log 
non monotonic behavior pol fm puzzling due randomness cross validation model selection 
erratic behavior log adult dataset caused convergence problems model tting 
comparison classification algorithms table 
number leaves including means medians decision tree algorithms 
numbers bcw cmc hea bos pid seg veh vot tae means fold cross validation experiments 
dataset alg 
bcw cmc dna hea bos led pid sat seg smo thy veh vot wav tae mean med 
qu qu ql ql ftl ib ibo im imo ic ic ocu ocl ocm st st lmt cal mean med :10.1.1.130.4558

lim 
loh 
shih table 
di erences number leaves decision tree algorithms datasets noise 
negative di erence means tree noise shorter noise 
dataset alg 
bcw cmc dna hea bos led pid sat seg smo thy veh vot wav tae mean med 
qu qu ql ql ftl ib ibo im imo ic ic ocu ocl ocm st st lmt cal mean med :10.1.1.130.4558
comparison classification algorithms lda ftl ic qu log ql pol fm sat sample size cpu time sec 
smo sample size tae sample size cpu time sec 
adult sample size 
plots training time versus sample size log log scale selected algorithms 
lines roughly parallel 
suggests relative computational speed algorithms fairly constant range sample sizes considered 
ql exceptions 
cohen observed scale 

lim 
loh 
shih 
results show mean error rates algorithms su ciently similar di erences statistically insigni cant 
di erences probably insigni cant practical terms 
example mean error rates top ranked algorithms pol log ql di er 
small di erence important real applications user may wish select algorithm criteria training time interpretability classi er 
error rates huge di erences training times algorithms 
pol algorithm lowest mean error rate takes fty times long train accurate algorithm 
ratio times roughly equivalent hours versus minutes shows maintained wide range sample sizes 
large applications time factor may advantageous quicker algorithms 
interesting old statistical algorithm lda mean error rate close best 
surprising designed attributes categorical attributes transformed vectors prior application lda ii expected ective class densities multi modal 
fast easy implement readily available statistical packages provides convenient benchmark comparison algorithms 
low error rates log lda probably account performance better algorithms 
example pol basically modern version log 
enhances exibility log employing spline functions automatic model selection 
strategy computationally costly produce slight reduction mean error rate bring top pack 
performance ql may similarly attributable lda 
quest linear split algorithm designed overcome di culties encountered lda multi modal situations 
applying modi ed form lda partitions data partition represented leaf decision tree 
strategy higher mean error rate ftl shows 
fact algorithm precursor quest 
major di erence quest fact algorithms employs cost complexity pruning method cart 
results suggest form bottom pruning may essential low error rates 
purpose constructing algorithm data interpretation decision rules trees univariate splits su ce 
exception cal di erences mean error rates decision rule tree algorithms statistically signi cant pol 
ic lowest mean error rate qu best terms mean ranks 
far 
algorithms provide classi cation accuracy 
fastest far tends yield trees twice leaves ic qu 
fastest shows scale comparison classification algorithms 
ic slightly faster trees slightly fewer leaves qu 
loh shih show cart algorithms ic liable produce spurious splits certain situations 
acknowledgments indebted auer brodley buntine hastie holte murthy quinlan taylor help advice installation computer programs 
grateful providing national indonesia prevalence survey data 
cohen provost reviewers helpful comments suggestions 

agresti 
categorical data analysis 
john wiley sons new york ny 

aronis provost 
increasing ciency data mining algorithms breadth rst marker propagation 
heckerman mannila pregibon uthurusamy editors proceedings third international conference knowledge discovery data mining pages menlo park ca 
aaai press 

auer holte maass 
theory applications agnostic pac learning small decision trees 
prieditis russell editors proceedings twelfth international conference machine learning pages san francisco ca 
morgan kaufmann 

becker chambers wilks 
new language 
wadsworth 

bishop 
neural networks pattern recognition 
oxford university press new york ny 

breiman friedman olshen stone 
classi cation regression trees 
chapman hall new york ny 

aha 
simplifying decision trees survey 
knowledge engineering review 

brodley multivariate versus univariate decision trees 
technical report department computer science university massachusetts amherst ma 

brodley multivariate decision trees 
machine learning 

brown 
comparison decision tree classi ers backpropagation neural networks multimodal classi cation problems 
pattern recognition 

bull 
analysis attitudes workplace smoking restrictions 
lange ryan billard brillinger conquest greenhouse editors case studies pages 
john wiley sons new york ny 

buntine 
learning classi cation trees 
statistics computing 

buntine caruana 
ind version recursive partitioning 
nasa ames research center mo field ca 

clark pregibon 
tree models 
chambers hastie editors statistical models pages 
chapman hall new york ny 

cohen 
fast ective rule induction 
prieditis russell editors proceedings twelfth international conference machine learning pages san francisco ca 
morgan kaufmann 

mingers 
neural networks decision tree induction discriminant analysis empirical comparison 
journal operational research society :10.1.1.130.4558

lim 
loh 
shih 
friedman 
multivariate adaptive regression splines discussion 
annals statistics 

friedman 
ranks avoid assumption normality implicit analysis variance 
journal american statistical association 

hand 
construction assessment classi cation rules 
john wiley sons chichester england 

harrison rubinfeld 
hedonic prices demand clean air 
journal environmental economics management 

hastie buja tibshirani 
penalized discriminant analysis 
annals statistics 

hastie tibshirani 
discriminant analysis gaussian mixtures 
journal royal statistical society series 

hastie tibshirani buja 
flexible discriminant analysis optimal scoring 
journal american statistical association 

hollander wolfe 
nonparametric statistical methods 
john wiley sons new york ny nd edition 

holte 
simple classi cation rules perform commonly datasets 
machine learning 

johnson 
applied multivariate statistical analysis 
prentice hall englewood cli nj rd edition 

kohonen 
self organizing maps 
springer verlag heidelberg 

bose stone 
regression 
journal american statistical association 

lerman 
determinants method service point choice 
secondary analysis national indonesia prevalence survey volume fertility family planning honolulu hi 
east west population institute 


loh 
shih 
split selection methods classi cation trees 
statistica sinica 


loh 
tree structured classi cation generalized discriminant analysis discussion 
journal american statistical association 

mangasarian wolberg 
cancer diagnosis linear programming 
siam news 

merz murphy 
uci repository machine learning databases 
department information computer science university california irvine ca 
www ics uci edu mlearn mlrepository html 

michie spiegelhalter taylor editors 
machine learning neural statistical classi cation london 
ellis horwood 

ruppert miller jr simultaneous statistical inference 
springer verlag new york nd edition 

uller wysotzki 
automatic construction decision trees classi cation 
annals operations research 

uller wysotzki 
decision tree algorithm cal statistical approach splitting algorithm 
taylor editors machine learning statistics interface pages 
john wiley sons new york ny 

murthy kasif salzberg 
system induction oblique decision trees 
journal arti cial intelligence research 

wasserman 
applied linear statistical models 
irwin boston ma rd edition 

oates jensen 
ects training set size decision tree complexity 
fisher jr editor proceedings fourteenth international conference machine learning pages san francisco ca 
morgan kaufmann 

quinlan 
programs machine learning 
morgan kaufmann san mateo ca 
comparison classification algorithms 
quinlan 
improved continuous attributes 
journal arti cial intelligence research 

ripley 
pattern recognition neural networks 
cambridge university press cambridge 
:10.1.1.130.4558

neural networks statistical models 
proceedings nineteenth annual sas users groups international conference pages cary nc 
sas institute 
ftp ftp sas com pub neural neural ps 

sas institute sas stat user guide version volume 
sas institute cary nc 

shavlik mooney towell 
symbolic neural learning algorithms empirical comparison 
machine learning :10.1.1.130.4558

ripley 
modern applied statistics plus 
springer new york ny nd edition 

wolberg tanner 
loh 
diagnostic schemes ne needle breast masses 
analytical quantitative 

wolberg tanner 
loh 
fine needle aspiration breast mass diagnosis 
archives surgery 

wolberg tanner 
loh 
statistical approach ne needle aspiration diagnosis breast masses 
acta 
