penalized likelihood estimation iterative kalman smoothing non gaussian dynamic regression models ludwig fahrmeir stefan ludwig maximilians universit nchen address correspondence prof dr ludwig fahrmeir tel institute statistics fax 
ii email ua aa 
de munich germany part supported deutsche forschungsgemeinschaft analyse 

dynamic regression state space models provide flexible framework analyzing non gaussian time series longitudinal data covering example models discrete longitudinal observations 
non gaussian random coefficient models direct bayesian approach leads numerical integration problems intractable complicated data sets 
markov chain monte carlo methods avoid repeated sampling approximative posterior distributions open questions sampling schemes convergence 
article consider simpler methods inference posterior modes equivalently maximum penalized likelihood estimation 
point view approach interpreted nonparametric method smoothing time varying coefficients 
efficient smoothing algorithms obtained iteration common linear kalman filtering smoothing way estimation generalized linear models fixed effects performed iteratively weighted squares estimation 
algorithm combined em type method cross validation estimate unknown hyper smoothing parameters 
approach illustrated applications binary time series longitudinal data set 
keywords 
discrete observations hyperparameter estimation non gaussian longitudinal data smoothing state space models time varying coefficients 

dynamic regression state space models relate time series observations sequence unknown states parameters typically including trend component time varying coefficients covariates 
observations estimation filtering smoothing unknown sequence primary interest 
gaussian linear state space models relationship observation design matrix appropriate dimension 
supplemented linear transition equation usual assumptions gaussian noise processes 
due linearity normality posterior distribution normal linear kalman filter smoother provides posterior means posterior covariances optimal estimates computationally efficient way 
non gaussian time series longitudinal data linear observation model replaced appropriate non gaussian model 
broad class generalized dynamic regression exponential family state space models obtained observation model form generalized linear model predictor important class non exponential family models robust models heavy tailed error distributions resistant additive outliers 
closed form updating formulas similar linear kalman filtering linear gaussian model available special models appropriate conjugate prior posterior distributions 
article gaussian linear transition equation retained allowing simultaneous modelling estimation stochastic trends seasonal components time varying covariate effects 
corresponds common assumption gaussian random effects generalized linear mixed models clayton 
direct bayesian approaches involve high dimensional integrations generally intractable complicated problems 
markov chain monte carlo methods avoid drawing repeated samples approximative posterior distributions carlin polson stoffer carter kohn 
problems concerning choice computationally efficient sampling schemes convergence sampling equilibrium 
simpler approximative methods useful alternative supplement exploratory data analysis provide initial solutions methods fr 
clayton generalized linear mixed models fahrmeir kaufmann fahrmeir dynamic generalized linear models estimation posterior modes equivalently maximum penalized likelihood estimation green 
point view approach interpreted dropping bayesian smoothness prior imposed transition model starting directly penalized likelihood criterion method yields efficient procedure discrete spline smoothing time varying coefficients compare hastie tibshirani 
show maximum penalized likelihood smoothing estimates obtained iterative application linear kalman filtering smoothing working model similarly fisher scoring static generalized linear models performed iteratively weighted squares applied working observations 
convenient result allows computationally efficient available version linear kalman filters smoothers iteration steps 
exponential family models related algorithm derived different arguments contained durbin koopman 
advantages iterative kalman filtering smoothing comparison common nonparametric procedures avoids additional inner backfitting loop directly provides error covariance matrices elements smoother matrix combined em type algorithm cross validation estimate unknown hyper smoothing parameters 
organized follows dynamic exponential family models dealt section including specific models simulations illustrative applications section 
penalized likelihood estimation iterative kalman smoothing developed section 
extensions general non gaussian dynamic regression models section 
exponential family state space models consider case time series observations 
extension longitudinal data population units section 
sequel responses states dimension respectively rewrite gaussian linear observation equation var covariance matrix obvious modification non gaussian exponential family observations specify observation model dimensional distribution natural exponential family type exp natural parameter function known functions 
simplicity assume unknown nuisance parameter 
properties exponential families mean variance functions vary 
static generalized linear models mean related linear predictor ir ir times continuously differentiable response function matrix may depend covariates past responses 
case densities means understood conditionally past responses 
exponential family assumption mean specification replaces observation equation linear gaussian models 
supplemented state transition model 
retain assumption gaussian linear transition equation transition matrix gaussian white noise initial state specify models completely terms densities conditional independence assumptions added conditional current responses independent past states assumption implied gaussian linear state space models assumption mutual independence error sequences design matrix contains past responses covariates stochastic understood conditionally 
sequence markovian scalar responses univariate dynamic generalized linear models obtained 
counts loglinear poisson models standard choice po exp 
linear predictor may chosen simple structural time series models gaussian observations states unobserved stochastic trend seasonal components possibly time varying effects covariates simple nonstationary models trend time varying effects second order random walk models resp 
appropriate definition models put state space form 
seasonal component period modelled see section 
course loglinear poisson model appropriate choices negative binomial may considered 
number counts time limited say binomial regression models logit probit models appropriate response function linking predictor exp exp obtains logit model probit model 
common way modelling binary time series 
extensions time series multinomial responses proceed similar lines number categories responses described vector tq components 
observation tj category observed tj 
corresponding categorical response models completely determined response probabilities tq tj tj 
independent repeated responses tq multinomial parameters tj absolute frequency observations category example dynamic multivariate logistic model trend covariates specified tj tj tr tj tj tj exp exp transition model trend covariate components 
simplest models ordered categories dynamic cumulative models 
derived threshold mechanism underlying linear dynamic model 
resulting conditional response probabilities tj tj linear predictors tj tj ordered threshold parameters tq known distribution function logistic 
dynamic cumulative models written state space form previous lines 
simplest case threshold covariate effects obey firstorder random walk partly constant time 
tq response function appropriately defined 
dynamic versions models ordered categories see fahrmeir ch designed analogous reasoning 
models extended longitudinal data time series observed unit population size specify individual observation models form 
design matrices constructed may appropriate functions covariates states interpreted population parameters 
common longitudinal data assume individual responses time conditionally independent covariates past responses 
collecting individual responses observation vector time models easily extended time series 

posterior mode smoothing penalized likelihood estimation 
fisher scoring iterative kalman smoothing subsection derive smoothing algorithms assuming hyperparameters known 
estimation hyperparameters dealt subsection 
ease presentation consider time series data suppose covariates deterministic 
furthermore denote histories responses states set posterior mode smoother tt ir defined tt argmax mode posterior distribution entire sequence 
aim maximize repeated application bayes theorem yields depend logarithms inserting gaussian densities transition model obtain penalized log likelihood function pl ir ir pl ln densities defined exponential family observation model 
tt argmax pl maximizing equivalent maximize penalized log likelihood respect may interprete bayesian smoothness prior defined transition model 
nonparametric point view may consider fixed unknown sequence states parameters 
term measures goodness fit obtained linear predictor second term penalizes roughness fit equivalently smoothness sequence 
complete analogy spline smoothing generalized additive models gam cf 
hastie tibshirani easily seen simple example binary dynamic logit model log trend timevarying effect assumed obey order random walks pl log log neglecting priors simplicity 
term measures goodness fit terms deviance terms penalize roughness trends time varying effects 
compared spline smoothing smoothing trends seasonal components covariate effects covariate functions 
variances general components play role smoothness parameters 
relationship pointed hastie tibshirani 
linear gaussian observation model log likelihood term specializes ln penalized squares criterion nonlinear maximization problem reduces quadratic programming problem 
posterior modes means coincide linear gaussian state space models optimization problem solved common linear kalman filters smoothers 
exploit special dynamic structure penalized squares criterion efficiently resulting recursive algorithms complexity 
find solution general case exponential family observation model nonlinear optimization code principle 
statistical purposes gauss newton fisher scoring advantage just static glm 
case linear gaussian models algorithms take account special dynamic structure penalized loglikelihood criterion 
derive single fisher scoring step analogy static generalized linear models show performed applying linear kalman filtering smoothing working observations resulting algorithmic solution complexity 
rewrite compact matrix notation pl ln penalty matrix symmetric block tridiagonal blocks easily obtained tt tt 
describe fisher scoring step matrix notation convenient introduce vector observations augmented correspondingly define vector expectations augmented block diagonal covariance matrix diag block diagonal design matrix diag ir unit matrix block diagonal matrix diag derivative response function evaluated properties score function components 
weight matrix diag diagonal blocks expected information matrix diag diagonal blocks block diagonal 
derivative pl pl ka block tridiagonal expected information matrix pl single fisher scoring step current iterate ir say iterate ir rewritten working observation components 
similar formula penalty matrix contains information transition model obtained iteratively weighted squares estimate applied working observations static glm 
assume special case linear gaussian state space model defined 
za identity matrix score function diag cov weight matrix reduces working observation actual observation za tt vector smoothed estimates 
remarked earlier classical linear kalman filter smoother solves efficiently explicitly inverting block tridiagonal matrix comparing conclude order solve carry single fisher scoring step exponential family case apply convenient version linear kalman filtering smoothing replacing 
call working kalman filter smoother 
algorithm numerical approximations filtered predicted smoothed values corresponding approximate error covariance matrices 
working kalman filter smoother initialization fa fv 
smoothing may classical fixed interval smoother computationally efficient version yielding tt remarks note specializes generalized extended kalman filter smoother fahrmeir 
implicitly chooses reasonable starting vector stops iteration step 
ii applying matrix inversion lemma anderson moore considering shown correction step written scoring form working score function ho want solve iterate solution previous iteration starting vector loop iteratively weighted kalman filter smoother initialization compute tt 
set iteration index step starting compute application 
step convergence criterion fulfilled set go step 
complete fisher scoring algorithm efficient block tridiagonal form explicit inversion avoided 
convergence obtain posterior mode smoother error covariances computed convergence curvatures pl diagonal blocks cf 
fahrmeir kaufmann need extra computational effort get 
convenient result hyperparameter estimation seen subsection 
iterative process suitably initialized require starting vector estimation approach easily extended longitudinal data 
due conditional independence individual responses log likelihood sum ln individual log likelihood contributions score functions information matrices sums individual contributions additional index 
estimation hyperparameters outline methods data driven hyperparameter estimation 
way estimate em type algorithm similarly linear gaussian dynamic models suggested fahrmeir fahrmeir goss 
procedure joint estimation summarized follows em type algorithm 
choose starting values set iteration index 
smoothing compute un known parameters replaced current estimates 
em step compute fa fa id fb fv defined fixed interval smoother 

termination criterion reached set go 
way adopt principle cross validation proposed kohn linear state space models mentioned hastie tibshirani fahrmeir static generalized additive models context 
simplicity consider univariate responses summarize hyperparameters vector approximative solution obtained vector extending generalized cross validation criterion static dynamic generalized linear models define gcv tr smoother hat matrix 
obtained arguments static glm see fahrmeir ch convergence fisher scoring step form estimated linear predictor za zu suppressing information connected initial prior smoother matrix obtained omitting row column diagonal blocks approximate error covariance matrices computed convergence diagonal blocks 
tr obtained additional computational effort 
unknown hyperparameters estimated minimizing gcv numerically 

approximate posterior mean analysis subsection smoothing estimate entire state vector defined derived posterior mode inverse information matrices approximate error covariance matrices 
experience simulated real data sets indicate satisfactory approximation quality practical purposes 
simulation results fahrmeir provide evidence posterior approximately gaussian posterior mode associated error covariance matrices reasonable useful approximations posterior mean 
give additional informal argument approximate posterior normality 
taylor expansion sampling log likelihood mode posterior neglecting cubic higher order terms laplace approximation tierney kadane clayton 
carrying expansion obtain higher order terms 
remainder term natural link functions expectation general link functions 
omitting higher order terms rearranging get za za independent za approximated gaussian sampling log likelihood mean za covariance matrix observations maximizing approximate penalized likelihood yields solution 
comparing see corresponds solution convergence 
obtained linear kalman smoother 
posterior approximately gaussian mean approximately equal mode 
accuracy approximation depends data situation sample size 
longitudinal data approximation justified asymptotically fixed arguments laplace method tierney kadane 
question approximation quality difficult small particular pure time series situation 
simulation results subsection indicate satisfactory behaviour sparse data situation 
rigorous asymptotic theory small interesting topic theoretical research 

illustrative applications time series rainfall data application analyzed kitagawa fahrmeir comparison 
example simulation study carried get insight estimation quality 
second application analyze larger longitudinal data set ordinal responses micro economics 

binary rainfall data data number occurences rainfall tokyo area calendar day years 
obtain smooth estimate probability occurence rainfall calendar day kitagawa chose dynamic binomial logit model february exp exp rain day parametrized constant corresponding penalized log likelihood pl ln ln shows corresponding smoothed estimates data points 
random walk variance estimated em type algorithm gcv 
methods provide estimate example lead pattern estimated probability rainfall calendar days 
tokyo rainfall data computed rw 
take second order random walk transition model shows gcv function dependent computed 
observe local minima approximately tokyo rainfall data 
values gcv dependent computed rw 
corresponding estimates 
dependent starting value em type algorithm yields estimates 
tokyo rainfall data computed rw different smoothing parameters provide insight estimation properties carried monte carlo experiment estimated probabilities true probabilities rainfall day replications binomial time series generated model 
replication smoothed estimates approximate error variances transformed variances computed combined em type algorithm 
monte carlo experiment true probabilities estimated probabilities 
confidence band time series 
monte carlo experiment empirical mean 
confidence band true probabilities displays true probabilities solid line time series replicate estimated probabilities pointwise confidence bands true curve compared empirical mean smoothed estimates corresponding pointwise confidence intervals 
figures indicate bias associated high curvature tendency 
average true curve covered pointwise confidence bands 
seen pointwise coverages replicates plotted 
monte carlo experiment nominal value 
points pointwise coverages intervals 
clear evidence low coverage associated high curvature agreement simulations gu context non gaussian spline smoothing 
average coverage probability 
indicates approximate error variances obtained tend larger exact error variances example 
confirmed comparing mean empirical variances obtained smoothed estimates replicates 
monte carlo experiment empirical variances mean transformed variances 
business test ordinal longitudinal data application subset monthly business microdata collected ifo institute munich 
questionnaire contains questions tendency successive change realizations plans expectations variables production orders hand demand answers categorical ordinal categories increase decrease change 
currently firms various industry branches participate survey voluntary basis 
analyze data collected industrial branch und period january december 
firms branch manufacture initial products building trade industry 
response variable formed production plans firm th month 
conditional distribution supposed depend covariates expected business condition orders hand compared previous month production plans previous month interaction effects included 
variable described dummy variables category 
stand responses respectively 
relevant dummies abbreviated cumulative logit model stochastic trends yearly seasonal components thresholds global covariate effects specified pr pr pr pr stand probability increasing nondecreasing production plans logistic distribution function 
trends covariate effects modelled independent random walks order seasonal components obey autoregressive transition models order 
unknown hyperparameters estimated em type algorithm 
business test data 
estimated seasonal component threshold parameter computed 
business test data 
estimated seasonal component second threshold parameter computed 
gives estimated trend parameters obtained 
trends comparably smooth stable slightly time varying 
seasonal components figures distinct pattern clear year corresponding lows autumn coinciding new season building trade industry busy months winter 
seasonal component additional local peak appears july august indicating plans increased production summer vacations 
displays smoothed estimates covariate parameters 
business test data 
covariate effects computed 
business test data 
estimated effect pointwise confidence bands computed 
compared remaining effects parameter corresponding increase category expected development business remarkable temporal variation 
exhibits clear decline minimum distinct increase period coincides months new german government autumn elections german parliament 
growing positive effect positive state business increase category production plans indicates positive reactions firms change government 

general non gaussian dynamic regression section smoothing algorithms derived state space models observation densities exponential family 
leads mathematically convenient expressions restriction removed admitting general non exponential family densities piecewise continuous second derivatives 
broad class non gaussian models obtained assume observation density general form parameter specific interest example mean parameterized possibly nonlinear function state vector important subclass robust models errors observation equation come heavy tailed distribution student distribution 
incorporation heavy tail error distribution model robust additive outliers 
denote corresponding log likelihood contribution 
score function contribution expected information matrix contribution 
defining augmented vector block diagonal matrix diag block diagonal weight matrix diag obtain similarly section augmented score vector expected information matrix proceeding section fisher scoring step written wm evaluated working observation 
comparing seen fisher scoring step carried identifying german science foundation dfg financial support 
anderson moore 
optimal filtering 
new jersey prentice hall 
clayton 
approximate inference generalized linear mixed models 
jasa 
carlin polson stoffer 
monte carlo approach nonnormal nonlinear state space modelling 
jasa 
carter kohn 
gibbs sampling state space models 
biometrika 
durbin koopman 
kalman filtering smoothing non gaussian time series 
discussion london school economics political science 
fahrmeir 
posterior mode estimation extended kalman filtering multivariate dynamic generalized linear models 
jasa 
goss 
filtering smoothing dynamic models categorial longitudinal data 
statistical modelling eds 
van der jansen francis 
amsterdam north holland pp 

kaufmann 
kalman filtering posterior mode estimation fisher scoring dynamic exponential family regression 


multivariate statistical modelling generalized linear models 
new york springer 
fr 
applied state space modelling non gaussian time series integration kalman filtering 
statistics computing forthcoming 
gu 
penalized likelihood regression bayesian analysis 
statistica sinica 
hastie tibshirani 
generalized additive models 
london chapman hall 
hastie tibshirani 
varying coefficient models 
journal royal statistical society 
kitagawa 
non gaussian state space modeling nonstationary time series 
jasa 
kohn 
fast algorithm signal extraction influence cross validation state space models 
biometrika 

integration kalman filtering dynamic generalized linear trend model 
computational statistics data analysis 
tierney kadane 
accurate approximations posterior moments marginal densities 
jasa 
