lazy learning local modeling control design mauro birattari hugues bersini iridia cp universit libre de bruxelles av 
franklin roosevelt bruxelles belgium email ulb ac october presents local methods modeling control discrete time unknown nonlinear dynamical systems limited amount input output data available 
propose adoption lazy learning memory technique local modeling 
modeling procedure uses query approach select best model configuration assessing comparing different alternatives 
new recursive technique local model identification validation enhanced statistical method model selection 
methods design controllers local linearization provided lazy learning algorithm described 
method lazy technique returns forward inverse models system compute control action take 
second indirect method inspired self tuning regulators recursive squares estimation replaced local approximator 
third method combines linearization provided local learning techniques optimal linear control theory control nonlinear systems regimes far equilibrium points 
simulation examples identification control nonlinear systems starting observed data 
problem modeling process limited amount observed data object disciplines nonlinear regression machine learning system identification 
literature dealing problem main opposing paradigms emerged local memory versus global methods 
global modeling builds single functional model dataset 
traditionally approach taken neural network modeling form nonlinear statistical regression 
available dataset learning algorithm produce model mapping dataset discarded model kept 
local memory algorithms defer processing dataset receive request information prediction local modeling 
classical nearest neighbor method original approach local modeling 
database observed input output supported european union tmr 
mauro birattari supported program belgium 
data kept estimate new operating point derived interpolation neighborhood query point 
local techniques old idea time series prediction farmer classification cover hart regression cleveland 
idea local approximators alternative global models originated non parametric statistics epanechnikov rediscovered developed machine learning field aha bottou vapnik 
lazy learning just time learning gave new impetus adoption local techniques modeling atkeson control problem tolle schaal atkeson atkeson 
new promising feature local paradigm adoption enhanced statistical procedures identify local approximator 
example press statistic myers simple founded economical way perform leave cross validation efron tibshirani assess performance generalization local linear models 
aim extend idea local memory learning directions 
propose model identification methodology iterative optimization procedure select best local model set different candidates 
classical local modeling amount options designed data analyst heuristic criteria priori assumptions 
propose automatic selection procedure searches optimal model configuration returning candidate model parameters statistical description generalization properties 
introduce new algorithm estimate recursive way model performance cross validation 
particular propose technique recursive squares methods compute press incremental way 
powerful wellfounded statistical test compare performance alternative candidates basis cross validation error sampling distributions 
contribution control domain set nonlinear control design techniques extensively analysis design tools linear control 
idea employing linear techniques nonlinear setting new control literature gained new popularity methods combining multiple estimators controllers different operating regimes system murray smith johansen 
gain scheduling athans fuzzy inference systems takagi sugeno local model networks johansen foss known examples control techniques nonlinear systems linear control 
comparative review approaches provided section 
propose methods discretetime control local linear description returned lazy learning approximator 
method example gradient controller exploits nice properties lazy learning algorithm nonlinear approximator 
inspired neural controllers combines inverse forward model select control action available dataset supposed produce desired output controlled system 
control algorithm time step horizon implemented gradient optimization algorithm lazy model computes value gradient cost function minimized 
algorithm introduced atkeson tested simply static control task 
analyze dynamic stability properties compare proposed local control techniques 
second controller derived self tuning regulator str architecture combines discrete time conventional control generalized minimum variance pole placement local model identification 
control system thought composed loops 
inner consists process feedback regulator 
parameters regulator adjusted outer loop case adaptive lazy learning estimator 
main differences conventional adaptive control techniques lie parameter estimation scheme 
str identification performed recursive parameter estimator updates linear model new input output sample observed 
approach global linear model description time step system dynamics linearized neighborhood current operating regime 
adaptive feature lazy model due sequential parameter estimation simply database updating 
third control system propose designed optimal control techniques parameterized values returned linear local estimator 
combination linear quadratic regulators lqr local modeling new literature tanaka atkeson 
papers authors strong assumptions analytical description locus equilibrium points available system supposed evolve sufficiently restricted neighborhood desired regime 
idea combination local estimator time varying optimal control take account nonlinearity system wider range conventional linearized quadratic regulators 
propose receding horizon controller returns sampling period action optimal sequence gradient optimization procedure 
optimization problem formulated minimization quadratic cost function cost returned forward simulation identified model gradient returned discrete time riccati equation linear time varying systems 
worth saying focus algorithmic computational issues 
subject reader invited refer literature dealing efficient implementations memory algorithms nene nayar moore 
remainder organized follows 
section introduce modeling technique iterative selection procedure 
section recursive algorithm press statistic computation 
section method statistical comparison models assessed recursive press evaluation introduced 
algorithmic details theoretical analysis algorithms local control section 
section simulation examples identification control 
local modeling optimization problem modeling data involves integrating human insight learning techniques 
real cases analyst faces situation limited set data available accurate prediction required 
information order structure set relevant variables missing reliable 
process learning consists trial error procedure model properly tuned available data 
lazy learning approach estimation value unknown function performed giving attention region surrounding point estimation required 
consider unknown mapping 
set samples phi psi examples collected matrix phi dimensionality theta vector dimensionality theta 
specific query point prediction value computed follows 
sample weight computed function distance query point point row phi multiplied corresponding weight creating variables phi wy diagonal matrix having diagonal elements ii locally weighted regression model lwr fitted solving equation fi prediction value obtained evaluating model query point gamma analysis method terms approximation properties see cybenko 
focus mainly procedural aspects modeling technique 
typically data analyst adopts local regression approach take set decisions related model number neighbors weight function parametric family fitting criterion estimate parameters 
extend classical approach method automatically selects adequate configuration 
aim simply import tools techniques field linear statistical analysis 
important tools press statistic myers simple founded economical way perform leave cross validation efron tibshirani assess performance generalization local linear models 
due short computation time allows intensive key element approach modeling data 
assessing performance linear model alternative configurations tested compared order select best 
selection strategy exploited select training subset neighbors various structural aspects features treat degree polynomial local approximator 
general ideas approach summarized follows 

task learning input output mapping decomposed series linear estimation problems 

single estimation treated optimization problem space alternative model configurations 

estimation ability alternative model assessed cross validation performance computed press statistic 
order operations effective propose innovative algorithms lazy learning method ffl recursive algorithm parametric estimation cross validation local model 
method avoids having restart model evaluation scratch decreases noticeably computational cost ffl rigorous statistical test compare performance alternative candidate models 
test just consider average values cross validation errors sampling distributions 
sections discuss algorithms detail 
press statistic recursive method focus leave cross validation procedure efron tibshirani returns reliable estimation prediction error define th error cv difference prediction local model centered fitted examples available th 
estimation prediction error average errors cv weighted distance 
considering local linear model leave oneout cross validation performed recalculating regression parameter excluded example local version press statistic atkeson mse cv gamma gamma gamma gamma gamma gamma gamma gamma gamma cv delta modeling procedure performance model cross validation criterion adopted choose best local model configuration 
important parameters tuned local model configuration size region surrounding function delta conveniently approximated linear local model 
parameter related number training examples fall region linearity 
task identifying region linearity akin task finding examples available number neighbors local regression fit 
consider different models fitted different number examples leave cross validation compare select predicted error smaller 
procedure faster avoid repeating model parameter press computation adopt incremental approach recursive linear techniques 
recursive algorithms developed model identification adaptive control literature goodwin sin identify linear model data available observed sequentially 
employ methods obtain parameters model fitted nearest neighbors updating parameters model examples 
furthermore leave errors cv obtained exploiting partial results square method require additional computational overload 
adopted weighting kernel indicator function assigns examples fit model recursive lazy learning algorithm described follows gamma fl gamma fi fi fi fl cv gamma fi 
initialize parameter local model conventional initialization fi large 

add example gamma delta parameter fi updated vector cv evaluated equation 

check departure local linear region comparing new model previously accepted 
new model significantly worse goto accept model goto consider adding examples 

recursive identification procedure return parameters model accepted 
procedure adopted identify recursively local linear model define largest region linearity 
gamma delta th nearest neighbor query point recursive approximation matrix gamma fi denotes optimal squares parameters model fitted nearest neighbor cv vector cv leave errors 
vector available formula easily computed 
value weighted average cross validated errors simplest statistic describe performance model defined neighbors 
problem assessing right dimension linearity region limited number samples affected noise requires powerful statistical procedure 
section discuss detail method adopt 
statistical test model selection recursive method described previous section returns size neighborhood vector cv leave errors 
order select best model procedure described detail fig 
consists increasing number neighbors considered identifying local model model performance deteriorates departure region local linearity detected 
requires statistical test evaluate enlarged model significantly worse considered 
terms hypothesis testing formulate null hypothesis cv cv belong distribution 
evaluate hypothesis nonparametric permutation test siegel castellan require assumptions normality homogeneity variance shape underlying distribution 
adopt paired version permutation algorithm see appendix correlation error vectors permutation test shows main advantage local modeling procedure low computational effort possible return prediction linear local parameters statistical description uncertainty affecting results 
property result useful prediction control problems 
lazy learning control design illustrate different approaches lazy learning control design define notations single input single output siso case 
consider class discretetime dynamic systems equations motion expressed form gamma gamma gamma ny gamma gamma gamma nu gamma gamma ne delta denotes time system output input zero mean disturbance term relative degree delta nonlinear function 
model known model billings 
assume physical description function delta limited amount pairs theta available 
defining information vector gamma theta gamma gamma ny gamma gamma gamma nu gamma gamma ne system written form gamma demonstrated models describe input output behavior general state space nonlinear dynamic systems state vector zero mean disturbance noise 
nonlinear functions 
lazy learning local linear control comparative analysis nonlinearity characterizes real control problems methods analysis control design considerably powerful theoretically founded linear systems nonlinear ones 
nonlinear control literature find approaches extension linear techniques nonlinear problems 
short survey techniques comparison lazy approach provided 
linearization equilibrium point 
point called equilibrium point plant time step gamma gamma term information vector 
assuming delta continuously differentiable linearize equation performing multi variable taylor series expansion 
outcome linear time invariant system describes locally nonlinear dynamics 
conditions li linear model provides informations local stability properties global system 
furthermore starting parametric form linear controller designed stabilize major drawback procedure consists accurate modeling system operating away equilibrium point 
alternative provided linearizing trajectory 
linearization trajectory 
idea study behavior system near prescribed trajectory 
denote specific trajectory nonlinear system gamma information vector time assuming delta continuously differentiable system may approximated near trajectory linear time varying system 
time varying property control design process difficult 
approach requires knowledge trajectory advance condition unfortunately satisfied 
gain scheduling 
widely successfully applied technique design nonlinear controllers 
method breaks control design process steps 
designs local linear controllers linearizations different equilibrium points 
scheme implemented interpolating scheduling parameters intermediate conditions 
formal analysis approach see 
adaptive control 
identification algorithm rls operates time update linear approximation system dynamics parameters adjust coefficients controller goodwin sin 
approximation provide satisfactory performance local linearization changes slowly 
variants approach forgetting factor recursive algorithm sensitive data order better track variations plant transfer function 
local model networks lmn 
approach extends concept operating point introducing notion operating regime 
operating regime set operating points system behaves approximately linearly johansen foss johansen foss 
validity region local description system behavior associated 
function delta approximated set interpolated local models 
local model networks identification control proposed authors murray smith hunt 
major advantage approach possibility integrate priori knowledge parametric learning procedures 
related nonlinear modeling approaches takagi sugeno fuzzy inference systems radial basis functions moody darken 
multiple model adaptive control 
lmn model control strategy uses weighted combination model controller pairs narendra balakrishnan 
main difference lies procedure combine models 
lmn operating domain partitioned considering priori knowledge preprocessing observed data approach combines local model controllers local measures accuracy maximum posteriori probability horizon error tracking estimated line control process 
addition weighting measures select local models update incoming sample 
discuss main differences mentioned approaches lazy learning technique 
lazy learning vs linearization 
linearization approach requires priori knowledge system order analytical characterization equilibrium lazy rls recursive vs lazy learning identification points 
lazy learning require analytical model returns best linear approximation extracted observed data 
linearization techniques return local linear model range validity restricted neighborhood linearization points 
memory techniques adaptively change local description function current system state 
lazy learning vs adaptive control 
standard recursive procedure embedded adaptive controller estimates linear model best linear approximation basis past observations 
adoption forgetting factor aims algorithm able deal nonlinear time varying configurations giving weight data 
contrary lazy approach treats separately nonlinear time varying systems 
set data considered estimate local regression parameters set nearest data input space domain information vector space case 
allows different local models different operating regimes prevents problem data interference jacobs stability plasticity dilemma carpenter grossberg 
consider illustrative example simple nonlinear dynamics gamma plot samples fig 

numbers represent temporal order samples observed 
system identified forgetting factor recursive approach example encountered estimated model dotted line lost memory dynamics existing neighborhood points 
result accuracy rls approximation dotted line poor 
hand lazy approach affected interference phenomenon data returns better local approximation solid line 
lazy technique deal time varying configurations minor changes 
sufficient extend input space adding input features current time variable prediction required nearest samples space time candidate neighbors 
lazy learning vs gain scheduling 
remarks linearization approach valid 
major difficulty gain scheduling approach selection appropriate scheduling variables 
lazy learning selects input features basis information vector represents operating condition system 
lazy learning vs local model networks 
approaches share common idea decomposing difficult problem simpler local problems 
main differences concern model identification procedure 
local model networks aim estimate global description cover system operating domain memory techniques focus simply current operating point 
lmn result time consuming identification faster prediction 
new data observed model update may require perform lmn modeling process scratch 
matter lazy learning takes advantage absence global model new input output example observed update database stores set input output pairs 
lazy learning vs multiple model adaptive control 
lazy learning situated middle ground lmn multiple model control 
lmn lazy learning model scheduling obeys criterion locality space state variables performance measure preceding time instants 
multiple model control lazy learning model sequentially adapted observed data done indirectly updating database 
lazy learning gradient controller discussed lazy learning approximate complex nonlinear mappings limited amount data 
idea lazy gradient controller property solve step horizon control problem optimization problem 
consider dynamic system set observed input output samples available 
clarity assume 
suppose system formulated form required reach time time step value ref time gamma value gamma available introduce vector gamma theta gamma gamma ny gamma gamma nu gamma gamma ne subset information vector 
lazy model predict response system control action gamma gamma gamma ny gamma gamma nu gamma gamma ne delta addition linearization returned local description provides estimate gradient system output dy du respect control action atkeson 
control problem formulated constraint gradient optimization problem opt arg min arg min ref gamma 
initialization algorithm value provided inverse mapping 

prediction outcome system forced input 
computation gradient vector dj du 
updating control sequence optimization step performed constrained gradient algorithm implemented matlab function constr grace 

minimum reached goto goto 
control action execution 
action gamma opt applied real system 

updating database storing new input output observation 
repeat steps sampling period 
lazy gradient controller algorithm order speed optimization resolution algorithm initialized value returned model inverse dynamics jordan rumelhart inv gamma ref gamma gamma ny gamma gamma nu gamma gamma ne delta detailed version lazy gradient control algorithm fig 

lazy gradient control analysis known parameterization plant extremely important prove stability 
powerful stability results obtained control linear systems lyapunov method techniques applied generic nonlinear case 
method discussed lazy learning black box approximator input output representation nonlinear dynamical system 
result nonlinear controller stability closed loop feedback system guaranteed theoretically generic nonlinear plant 
instability consequence non minimum phase configurations 
case inverse dynamics unstable methods described prevent control signal growing limit making closed loop system unstable 
problem demonstrated simulation examples sections 
technique successfully control complex minimum phase nonlinear system able regulate system linearization operating regimes non minimum phase 
concept minimum phase significantly complex nonlinear case linear 
possible solution may come adoption linear techniques solving locally non minimum phase problem 
section control technique avoid instability phenomena conventional linear techniques nonlinear context 
lazy learning self tuning controller section propose hybrid architecture indirect control nonlinear discrete time plants observed input output behavior 
indirect control scheme astrom wittenmark narendra combines parameter estimator computes estimate unknown parameters control law implemented function plant parameters 
adaptive version estimator generates estimate sampling period processing observed input output behavior 
estimate assumed specification real plant compute control law certainty equivalence paradigm 
conventional adaptive control theory problem analytically tractable plant assumed linear time invariant system unknown parameters 
approach combines local learning identification procedure described section conventional linear control techniques 
adopt minimum variance mv pole placement pp control technique borrowed linear self tuning controllers astrom wittenmark 
mv control algorithm formulated astrom 
mv technique practical applications significant theoretical developments 
reasons mv popularity lie simplicity ease interpretation implementation 
consider linear discrete time process described input output form equation gammad suppose want regulate ref 
mv control problem stated finding control law minimizes variance output 
mv controlled closed loop system stable roots inside unit circle minimum phase 
complex formulations available case tracking problem case non minimum phase systems generalized mv 
cases possible select properly controller parameters order closed loop system asymptotically stable 
pole placement design alternative technique deal non minimum phase configurations 
procedure requires choose desired closed loop pole positions calculate appropriate controller 
design techniques require model formulation form 
approach linearization performed configurations far equilibrium locus 
consequence relation input output gammad offset term 
requires slight modification formulas pp controller design see appendices 
proposed control algorithm described detail fig 

note selection neighbors considering subset vector information vector 

acquisition vector 

linearization function delta 
linearization computed lazy learning algorithm 

derivation polynomials offset linearized model 

design pp controller satisfies required properties stability accuracy speed closed loop behavior 

computation control signal 

updating database storing new input output observation 
repeat steps sampling period 
lazy self tuning controller algorithm fact gamma available expected outcome procedure 
anyway local weighted regression performed space complete information vector 
lazy self tuning control analysis lazy self tuning regulator nonlinear plant parameterized linear system parameters changing observable state 
means nonlinear model written linear model parameters vary state system 
configuration recalls linear parameter varying lpv configuration introduced athans analytical analysis gain scheduling controllers state dependent models priestley 
models represented follows gammad assume exists lpv model represents sufficiently accurate manner nonlinear system 
hypothesis complete controllability observability closed loop system may put state space form gamma delta representation allows analyze stability controller 
regulator designed eigenvalues stable system asymptotically stable fixed value frozen time stability 
sufficient condition uniform asymptotic stability system 
consider set values assumed matrix sufficient condition uniform stability tanaka sugeno exists common matrix pf gamma pole placement technique impose stable closed loop transfer function follows exists matrix satisfies equation 
assumptions ffl system completely controllable observable ffl system put form ffl approximation error lazy learning identifier negligible equilibrium nonlinear system controlled lazy pp self tuning controller globally asymptotically stable 
lazy learning optimal controller consider optimal control problem nonlinear system finite horizon time 
quadratic cost function solution optimal control problem control sequence minimizes fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi weighting terms designed priori 
analytic results available generic nonlinear configuration optimal control theory provides solution linear case 
nonlinear problem linear time varying setting 
consider trajectory dynamical system forced input sequence theta gamma assume system linearized state trajectory 
neglecting residual errors due order taylor series approximation behavior linear system generic trajectory behavior linear time varying system state equations written form gamma delta gamma delta gamma delta parameters system linearized query point 
offset term equals zero equilibrium points 
term requires slight modification linear controller formulation 
order simplify notation neglect constant term 
optimal control theory provides solution linear time varying system 
time step optimal control action gamma gamma delta gamma gamma delta solution backward riccati equation 
gamma gamma delta gamma delta gamma gamma delta having final condition piecewise constant optimal solution obtained solving euler lagrange equations necessary sufficient conditions optimality final time fixed 
adjoint term augmented cost function hamiltonian gamma delta euler lagrange equations hold nonlinear systems 
anyway system represented form formula compute derivative cost function respect control sequence requires time matrices obtained linearizing system dynamics trajectory forced input sequence 
discussed section modeling procedure performs system linearization minimum effort priori knowledge reduced amount data 
propose algorithm nonlinear optimal control formulated gradient optimization problem local system linearization 
algorithm searches sequence input actions opt arg min minimizes finite horizon cost function steps 
cost function generic sequence computed simulating forward steps model identified local learning method 
gradient respect returned 
basic operations optimization procedure described detail fig 
executed time control action required ffl forward simulation lazy model forced finite control sequence dimension ffl linearization simulated system resulting trajectory ffl computation resulting finite cost function ffl computation gradient cost function respect simulated sequence ffl updating sequence gradient algorithm 
search algorithm returns opt action sequence applied real system receding horizon control strategy clarke 
lazy learning model twofold role algorithm fig 
step estimator predicts behavior system forced generic input sequence ii step returns linear approximation system dynamics 

initialization algorithm random sequence actions 
forward simulation system forced sequence theta gamma denotes action applied simulated system time system behavior predicted model identified local learning method 

formulation nonlinear system time varying form 
parameters gamma returned local model identification 

backward resolution discrete time riccati equation resulting time varying system 

computation cost function 

computation gradient vector theta gamma formula 

updating control sequence optimization step performed constrained gradient algorithm implemented matlab function constr grace 

minimum reached goto goto 
control action execution 
action opt sequence opt applied real system 

updating database storing new input output observation 
repeat steps sampling period 
lazy learning optimal controller algorithm lazy optimal control analysis atkeson 
tanaka applied lqr regulator nonlinear systems linearized lazy learning neuro fuzzy models 
drawback approaches equilibrium point trajectory required 
strong assumption state system remain indefinitely neighborhood linearization point 
discussed advantage proposed approach requirements need satisfied 
lazy learning able linearize system points far equilibrium 
second time varying approach possible linear control strategy system operates different linear regimes 
demonstrated stability properties linear time varying optimal control 
respect approaches control strategy presents nice properties 
firstly easily deal mimo multi input multi output systems shown simulation example 
secondly allows control policies longer time horizon simple relative degree system 
formalism keep consideration uncertainty affecting model 
controller assumption parameters returned local models real description local behavior certainty equivalence principle 
restricting assumption requires sufficient degree accuracy approximation 
optimal control theory represent possible solution limitation 
fact stochastic optimal control theory provides formal solution problem parameter uncertainty control systems dual control fel 
furthermore modeling procedure return additional cost statistical description estimated parameters see section 
focus extension technique stochastic control case 
simulation studies identification nonlinear discrete time system approach applied identification complex nonlinear benchmark proposed narendra li 
discrete time equations system sin gamma delta cos gamma delta gamma cos gamma delta sin gamma delta sin gamma delta state input output accessible 
system modeled input output form gamma gamma gamma gamma delta initial empty database updated identification 
identification performed time steps test input sin gamma delta sin gamma delta plot fig 
shows model system output points plot fig 
shows identification error 
obtain performance modeling nonlinear system identification results system solid model dotted outputs identification error complex system 
results outperform obtained narendra li wider training set points complex architecture layer feed forward neural network 
lazy gradient control nonlinear discrete time system simulation consider control plant described difference equations control algorithm described fig 

system represented minimum phase input output form gamma gamma gamma gamma delta output ref ref sin sin initial empty database updated identification 
system controlled time steps 
plot fig 
shows model system output points plot fig 
shows control action 
results outperform obtained narendra li steps line adjustments complex architecture layer feed forward neural network 
lazy self tuning control nonlinear discrete time system simulation consider control nonlinear siso system described difference equation gamma gamma gamma gamma gamma delta gamma gamma gamma system represented input output form gamma gamma gamma gamma delta output ref periodic square wave 
lazy gradient algorithm able control system trajectory 
contrary self tuning regulator pole placement algorithm able track trajectory 
initialize lazy learning database set points gradient system control solid system dotted outputs control action lazy self tuning control noise solid system dotted outputs measurement input noise solid system dotted output 
collected exciting system random uniform input 
database updated line time new input output pairs returned simulated system 
plot fig 
shows system output plot fig 
shows effect adding band limited white noise peak oe plant output manipulated variable 
fig 
plot value zero open loop system identified lazy learning simulation 
worthy noting system non minimum phase absolute value zero greater system variable neighborhood gamma see time interval gamma 
region gradient controller fails control system making feedback loop unstable 
lazy optimal control consider third control example system known benchmark nonlinear control miller bersini 
tank containing water biological cells 
cells introduced zero open loop identified system 
tank mix 
state process characterized number cells amount 
equations motion dc dt gammac gamma fl dc dt gammac gamma fl fi fi gamma fi fl 
experiment goal stabilize multi variable system unstable state ref ref performing control action seconds 
control algorithm described fig 

horizon control algorithm set 
initial state conditions set random initialization procedure defined miller 
initialize lazy learning database set points collected exciting system random uniform input 
database updated line time new input output pair returned simulated system 
plot fig 
shows output controlled state variables plot fig 
shows control action 
considered challenging problem nonlinearity small changes value parameters cause unstable 
results show local techniques possible control complex systems wide nonlinear range limited amount examples priori knowledge underlying dynamics 
developments local memory techniques powerful techniques learning limited amount data gaining insight local behavior nonlinear systems 
furthermore required prediction parametric description return statistical distribution uncertainty affecting information 
modeling proposed innovative algorithm improve performance memory techniques recursive version cross validation statistical model selection 
control illustrated analyzed control systems extensive control results system outputs solid dotted control action discrete time step corresponds sec 
local modeling 
lazy gradient control system inspired neural control forward inverse approximations system dynamics select control action 
neuro control properties stability guaranteed general case 
showed approach obtains performances accurate neural networks smaller set training examples 
lazy self tuning architecture adopts linear control technique eases analysis terms stability properties provides useful insight dynamic properties nonlinear system 
addition computational point view expensive proposed methods 
lazy optimal controller extends local approaches control strategies extended time horizons 
algorithm suited regulation mimo systems 
developments mainly concern problem model uncertainty control 
aim extend analysis robust stochastic dual control 
fact lazy learning algorithms nonlinear modeling returns statistical description uncertainty affecting prediction model parameters minimal additional computational cost 
concerns lazy learning model real systems fast size database grows 
treat issue researchers exploring fast ways find relevant data local approximation efficient software algorithms special purpose hardware 
integration methods modeling control techniques lazy learning promising tools nonlinear control 
appendix paired permutation test consider null hypothesis vector leave errors cv cv elements vector cv theta cv belong distribution 
paired permutation test assumes observed values assigned vectors cv cv probability 
means true difference cv gamma cv paired errors just negative positive 
null hypothesis tested computing value assuming instance random variable sampling distribution randomization procedure cohen computer intensive statistical method derive sampling distribution statistic simulating process sample extraction 
permutation test done creating high number pseudo samples derived actual sample substituting randomly difference gammad sampling distribution generated tailed test determines null hypothesis rejected 
rejection region right tail consisting extreme values leave vectors assumed extracted distribution 
generalization error linear model neighbors assumed significantly greater model fitted neighbors 
generalized minimum variance design offset term clarke developed generalized minimum variance controller introducing signal control variable performance index gamma ref pn pd qn qd suppose data generated model 
multiplying sides obtain pn pd pn pd gamma setting py ref ref gamma qu pda pn pn pn gammad ce polynomial solution diophantine equation gammad multiplying sides gives ce ce optimal control law ref gamma qn qd equivalent pd qd ref gamma qn qd qd qd plant polynomials 
control law ref gamma gamma qd qn qd result basic minimum variance controller obtained setting pn pd qd qn 
pole placement design offset term pole placement formulation desired closed loop function hm bm am regulator output inputs signal ref measured output general structure regulator may represented ref gamma gamma polynomials forward shift operator input output relationship closed loop system obtained eliminating equations 
ar ref gamma gb ar cr ar gammad requiring input output relation equivalent gives ar bs bm am gb pole placement design problem offset term equivalent conventional additional requirement satisfied 
aha 
incremental instance learning independent graded concept descriptions 
pages sixth international machine learning workshop 
san mateo ca morgan kaufmann 
astrom 
computer control machine 
application linear stochastic control theory 
ibm res 
dev 
astrom 
theory applications adaptive control survey 
automatica 
astrom wittenmark 
computer controlled systems theory design 
prentice hall international editions 
atkeson moore schaal 
locally weighted learning 
artificial intelligence review 
atkeson moore schaal 
locally weighted learning control 
artificial intelligence review 

non parametric estimation regression functions 
journal royal statistical society series 
bersini 
simplification back propagation algorithm optimal 
ieee trans 
neural networks 
bottou vapnik 
local learning algorithms 
neural computation 
carpenter grossberg 
art adaptive pattern recognition self organising neural network 
ieee computer 
clarke 
advances model predictive control 
oxford university press 
clarke 
self tuning controller 
proceedings ieee 
cleveland 
robust locally weighted regression smoothing scatterplots 
journal american statistical association 
cohen 
empirical methods artificial intelligence 
cambridge ma mit press 
cover hart 
nearest neighbor pattern classification 
proc 
ieee trans 
inform 
theory 
cybenko 
just time learning estimation 
pages 
eds identification adaptation learning 
science learning models data 
nato asi series 
springer 
efron tibshirani 
bootstrap 
new york ny chapman hall 
epanechnikov 
non parametric estimation multivariate probability density 
theory probability applications 
farmer 
predicting chaotic time series 
physical review letters 
fel 
optimal control systems 
new york ny academic press 
goodwin sin 
adaptive filtering prediction control 
prenticehall 
grace andrew october 
optimization toolbox matlab 
mathworks jacobs jordan barto 
task decomposition competition modular connectionist architecture vision tasks 
cognitive science 
johansen foss 
constructing models models 
international journal control 
johansen foss 
semi empirical modeling nonlinear dynamic systems identification operating regimes local models 
pages hunt irwin warwick 
eds neural network engineering dynamic control systems 
springer 
jordan rumelhart 
forward models supervised learning distal teacher 
cognitive science 
billings 
input output parametric models non linear systems 
international journal control 
miller sutton werbos 
eds 
neural networks control 
mit press 
moody darken 
fast learning networks locally tuned processing units 
neural computation 
moore schneider deng 
efficient locally weighted polynomial regression predictions 
proceedings international machine learning conference 
morgan kaufmann publishers 
ftp www cs cmu edu papers html 
murray smith hunt 
local model architectures nonlinear modelling control 
pages hunt irwin warwick 
eds neural network engineering dynamic control systems 
springer 
murray smith johansen 
eds 
multiple model approaches modelling control 
taylor francis 
myers 
classical modern regression applications 
boston ma pws kent 
narendra 
stable adaptive systems 
englewood cliffs nj prentice hall 
narendra balakrishnan 
adaptive control multiple models 
ieee transactions automatic control 
narendra li 
neural networks control systems 
chap 
pages paul smolensky mozer rumelhart 
eds mathematical perspectives neural networks 
lawrence erlbaum associates 
nene nayar 
simple algorithm nearest neighbor search high dimensions 
ieee transactions pattern analysis machine intelligence 

fuzzy control 
pages levine 
ed control handbook 
ieee press 
priestley 
non linear non stationary time series analysis 
academic press 

analytical framework gain scheduling 
ieee control systems 

tolerating concept sampling shift lazy learning prediction error context switching 
artificial intelligence review 
schaal atkeson 
robot juggling implementation memory learning 
ieee control systems february 

multiple model adaptive control 
pages murray smith johansen 
eds multiple model approaches modeling control 
taylor francis 
athans 
gain scheduling potential hazards possible remedies 
ieee control systems magazine june 
siegel castellan jr 
non parametric statistics behavioral sciences 
nd edn 
mcgraw hill international 
li 
applied nonlinear control 
prentice hall international 

stochastic optimal control theory application 
new york ny john wiley sons 
gustafsson ljung 
just time models dynamical systems 
pages th ieee conference decision control 
takagi sugeno 
fuzzy identification systems applications modeling control 
ieee transactions system man cybernetics 
tanaka 
stability fuzzy neural linear control systems 
ieee transactions fuzzy systems 
tanaka sugeno 
stability analysis design fuzzy control systems 
fuzzy sets systems 
tolle parks 
learning control interpolating memories general ideas design lay theoretical approaches practical applications 
international journal control 
