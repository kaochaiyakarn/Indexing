multi agent reinforcement learning approach agent internal model shin ishii kenji doya nara institute science technology nara japan aist nara ac jp advanced telecommunications research institute international seika soraku kyoto japan doya ctr atr jp crest japan science technology seika soraku kyoto japan application reinforcement learning multiagent systems attracted attention 
multi agent environment agent action depends agents actions 
traditional reinforcement learning methods assume stationary environments hard take account agent actions may change due learning 
article consider cooperation problem propose multi agent reinforcement learning method estimation agent actions 
learning method agent estimates agent action internal model agent 
internal model acquired observation agent actions 
experiments demonstrate cooperative behaviors achieved learning method 

realization cooperative behaviors multiagent systems interesting topic viewpoint engineering cognitive science 
particular reinforcement learning cooperative behaviors attracted attention adaptability dynamic environments 
reinforcement learning applied multi agent problems pursuit games soccer prisoner dilemma game coordination games :10.1.1.138.2589:10.1.1.135.717
traditional reinforcement learning methods developed single agent environment modeled markov decision process mdp 
mdp transition environmental states defined transition probability function change time 
considering multi agent environment agents autonomously learn behavior agent changes due learning 
behavior agents included environment transition function changes time 
environment modeled mdp 
multi agent reinforcement learning studies reinforcement learning methods mdp applied modification 
words approaches discriminate stationary environment learning agents 
agent perceives way agent determines behavior way called internal model agent 
studies internal model learning agent explicitly considered :10.1.1.138.2589:10.1.1.135.717
littman proposed mini max qlearning method player zero sum games 
method agent estimates action agent learns estimation 
estimation done agent internal model function 
agent predicts agent action minimizes expected values agent function 
hu wellman proposed multi agent reinforcement learning method player general sum games 
agent gain necessarily agent loss general sum game mini max learning method appropriate 
method hu wellman agent estimates agent internal model conducts learning estimated function agent 
method assumes agents conduct learning 
furthermore order estimate agent function agent observe agent actions actual rewards received environment know parameters learning agent 
article propose multi agent reinforcement learning method estimation agent internal model 
estimation done observation agent past actions 
learning method learning agent estimates agent policy function 
need observe agent actual rewards received environment know parameters agent uses learning 
article consider multi agent environment learning agents 
assume synchronously execute actions actions observable agents 
inter agent communication 
prepare variant pursuit problem multi agent environment experimentally evaluate learning method 

markov decision process qlearning multi agent reinforcement learning method learning 
learning originally defined deal mdp model agent interacting stationary environment 
mdp described tuple set finite states environment set finite actions state transition function reward function 
time step agent observes state executes action receives reward environment shown 
dynamics environment modeled state transition function probabilistic specified state action reward stationary environment 
mdp model terms set stationary transition probabilities ss pr js pr js probability environment changes new state agent executes action state reward random variable determined reward function learning incremental reinforcement learning method 
learning agent selects action action value function called function defines expected sum discounted reward attained executing action state determining subsequent actions current policy updated agent experience 
learning flow follows 
current state agent executes action probability js ak ak called temperature determines randomness stochastic policy 
methods selecting action proposed adopt boltzmann selection method 

agent executes action selected step environment changes new state agent receives reward environment updates function follows ff ff fl max ff ff learning rate fl fl discount factor 
action reward state multi agent environment consists stationary environment learning agents 

multi agent environment considered article environment consists stationary environment learning agents 
environment formulated mdp 

new state satisfies terminal condition single episode ends 
go back step 
temperature appropriately decreased environment modeled mdp transition environment state ss learning procedure converges optimal function obtained 

multi agent reinforcement learning study consider multi agent environment consists stationary environment learning agents shown 
multi agent environment formulated mdp agent transition probability environment changes time due learning agents 
case original learning appropriate 
article intend extend learning deal multi agent environment 
propose multi agent reinforcement learning method estimation agents policy change due learning 
article consider multi agent environment learning agents 
assume synchronously execute actions actions observable agents 
inter agent communication 

estimation internal model order explicitly express dependency agent action function written self 
self environment state self denote states agent consideration agent respectively 
self actions agent agent respectively 
sets actions possible agent agent respectively 
definition function clarifies agent action hidden variable selecting action self agents synchronously execute actions inter agent communication observation actions 
reinforcement learning method agent action estimated function estimated observation agent past actions 
called internal model function 
agent observes agent executes action state agent updates internal model function action executable state follows ae parameter controls effect actual action 
reinforcement learning process propose multi agent learning method estimation agent internal model 
learning flow follows 
agent consideration observes current state estimates agent action selects action self stochastic policy js ak ak self expected function value respect self omega self ff self 
agent executes action self selected step 
time agent executes action environment changes new state agent receives reward hunter hunter prey hunter 
pursuit problem grid world arrows denotes directions prey hunters move 
environment 
agent updates equation updates self follows self ff self ff fl max self self argmax 

new state satisfies terminal condition single episode ends 
go back step 
experiments results 
pursuit problem order evaluate multi agent reinforcement learning method prepare variant wellknown pursuit problem typical testbed multi agent cooperation algorithms 
ffl toroidal grid world hunter agents prey exist shown 
episode placed random 
ffl discrete time step episode prey hunters synchronously executes actions moving left right current position staying current position 
ffl hunters perceive current state represented location prey relative 
examples goal state hunter 
example state shown represented relative locations hunter hunter prey respectively 
number possible 
ffl hunters capture prey episode ends 
capture achieved hunters positioned sides prey shown 
ffl prey actions probabilistic independent hunters positions 
probability moving right probability actions moving left staying 
action definition implies state transition stationary environment stochastic 

experimental results centralized learning cq method defined order evaluate proposed learning method 
cq assumes centralized learning component able observe grid world control hunter agents 
learning scheme usual learning 
flow described section action actions hunter hunter respectively 
learning flow follows 
centralized learning component observes current state selects action pair policy js 
hunters execute environment changes new state ss number learning episodes learning randomly action estimation learning proposed action estimation average time steps centralized learning 
time steps needed capture prey centralized learning method vs multi agent learning method action estimation 
learning component receives reward environment updated ff ff fl max 
new state satisfies terminal condition single episode ends back step 
cq transition environment state stochastic stationary 
environment modeled mdp 
show average time steps needed capture prey 
figures abscissa denotes number learning episodes ordinate denotes average time steps capture prey 
learning episodes evaluation episodes vary initial placements done figures show average time steps 
label learning proposed action estimation denotes proposed multiagent reinforcement learning method label qlearning randomly action estimation denotes multi agent reinforcement learning method learning conducted random estimation hunter action 
learning parameters learning methods ff fl num ep parameter num ep num ep number number learning episodes average time steps learning proposed action estimation 
time steps needed capture prey hunters reward functions different 
learning episodes 
decreased number learning episodes increases 
value chosen 
reward designed hunters capture prey 
initial values function initial values internal model function 
shows cooperative behaviors acquired cq 
show experimental result multi agent reinforcement learning method hunters reward functions different 
hunter receives captures prey 
hunter receives capturing 
parameters previous experiment 
noted difficult cq deal heterogeneous agents 
label learning self model action estimation denotes multi agent reinforcement learning method action estimation self policy ajs internal model function 
agent estimates agent action policy 
agent self policy ajs agent policy self state replaced agent 
environment symmetric respect hunters scheme expected 
cooperation problem consider symmetric structure 
shows effective earlier learning phase number learning episodes learning proposed action estimation learning self model action estimation 
time steps needed capture prey homogeneous environment proposed multi agent learning method vs multi agent learning method action estimation self model 
learning episodes performance learning methods learning episodes 
shows experimental results heterogeneous environment 
experimental condition identical expect definition goal capture different hunter 
hunters positioned sides prey goal achieved hunter episode ends 
case hunter positioned right prey goal achieved hunter goal reward received 
goal definition different hunters difference implies task symmetric agents 
shows exhibits better learning result 

discussion comparing advantage action estimation method obvious 
hunters learn approach prey learn cooperative capture movements 
capture occurs chance 
comparing cq learning performance similar cq 
cq expected learn effectively cq conventional learning stationary environment 
appropriate scheduling tem learning proposed action estimation learning self model action estimation number learning episodes average time steps 
time steps needed capture prey heterogeneous environment proposed multi agent learning method vs multi agent learning method action estimation self model 
parameter algorithm converges 
hand exists hidden variable agent action 
environment represented mdp 
estimation hidden variable learning proceed 
similarity learning performance cq implies estimation method works effectively 
noted learning internal model function agent explicitly depend agent policy 
folded learning algorithm considered 
learning phase fixed internal model function 
learning internal model function fixed policy 
phase converges iterative algorithm converges 
discussion implies convergence proposed algorithm conduct careful scheduling 
multi agent problem symmetric agents self model help estimation agent policy 
seen 
human beings self model primitive model agent behavior especially lack knowledge agents 
mini max reinforcement learning realization intuition 
actual multi agent environments heterogeneous symmetric agents 
consider heterogeneous environment produce agents non uniform roles 
called self organization 
im portant factor heterogeneity difference reward functions goal states 
heterogeneous environment proposed learning method works seen figures 
accordingly say proposed method emerges agent role 
note environment hard considered centralized reinforcement learning method 
estimation method works degree seen 
self model accurate case result worse proposed method 
includes estimation method combines self model action observation 

article proposed multi agent reinforcement learning method estimation internal model agents 
internal model estimated observation agent past actions 
applied variant pursuit problem experimental results showed cooperative behavior achieved new learning method 
furthermore showed new learning method applied heterogeneous environments reward functions goal states different agents 
multi agent learning difficult problem general results obtained may depend specific attributes problem 
encouraging experimental results suggest new learning method promising emerging adaptive behaviors multiple autonomous agents 
arai miyazaki kobayashi 
methodology multi agent reinforcement learning approaches learning profit sharing 
journal japanese society artificial intelligence 
japanese 
balch 
learning roles behavioral diversity robot teams 
collected papers aaai workshop multiagent learning 
aaai press 
benda 
optimal cooperation knowledge sources 
technical report bcs boeing ai center 
claus boutilier 
dynamics reinforcement learning cooperative multiagent systems 
proceedings fifteenth national conference artificial intelligence aaai pages 
gasser hill lieb 
distributed artificial intelligence volume chapter pages 
morgan kaufmann 
howard 
dynamic programming markov processes 
mit press 
hu wellman 
multiagent reinforcement learning theoretical framework algorithm 
proceedings fifteenth international conference machine learning icml pages 
tokoro 
adaptive architecture modular learning 
proceedings fifteenth international joint conference artificial intelligence ijcai pages 
littman 
markov games framework multiagent reinforcement learning 
proceedings eleventh international conference machine learning icml pages 
ono fukumoto 
multi agent reinforcement learning modular approach 
proceedings second international conference multi agent systems icmas pages 
wiering schmidhuber 
learning team strategies soccer case studies 
machine learning 
sandholm crites 
multiagent reinforcement learning iterated prisoner dilemma 
biosystems 
sen hale 
learning coordinate sharing information 
proceedings twelfth national conference artificial intelligence aaai pages 
sutton barto 
reinforcement learning 
mit press 
tan 
multi agent reinforcement learning independent vs cooperative agents 
proceedings tenth international conference machine learning icml pages 
watkins dayan 
technical note learning 
machine learning 

