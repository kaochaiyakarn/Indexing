policy search density estimation andrew ng computer science division berkeley berkeley ca ang cs berkeley edu ronald parr computer science dept stanford university stanford ca parr cs stanford edu daphne koller computer science dept stanford university stanford ca koller cs stanford edu propose new approach problem searching space stochastic controllers markov decision process mdp partially observable markov decision process pomdp 
authors approach searching parameterized families policies example gradient descent optimize solution quality 
trying estimate values derivatives policy directly indirectly estimates probability densities policy induces states different points time 
enables algorithms exploit techniques efficient robust approximate density propagation stochastic systems 
show techniques applied deterministic propagation schemes mdp dynamics explicitly compact form stochastic propagation schemes access generative model simulator mdp 
empirical results variants complex problems 
years growing interest algorithms approximate planning exponentially infinitely large markov decision processes mdps partially observable mdps pomdps 
large domains value functions complicated difficult approximate may simple compactly representable policies perform 
observation led particular interest direct policy search methods attempt choose policy restricted class policies 
setting class policies smoothly parameterized value differentiable gradient ascent methods may find locally optimal estimating values associated gradient far trivial 
simple method estimating value involves executing monte carlo trajectories average empirical return algorithms executing single trajectories allow gradient estimates 
methods standard approach policy search fairly 
propose somewhat different approach value gradient estimation problem 
estimating quantities directly estimate probability density states system induced different points time 
time slice densities completely determine value policy density estimation easy problem utilize existing approaches density propagation allow users specify prior knowledge densities shown theoretically empirically provide robust estimates time slice densities :10.1.1.119.6111:10.1.1.119.6111
show direct policy search implemented approach different settings planning problem access explicit model system dynamics allowing provide explicit algebraic operator implements approximate density propagation process 
second access generative model dynamics allows sample provide explicit representation state distributions 
show techniques combined gradient ascent order perform policy search somewhat subtle argument case sampling approach 
empirical results variants complex domains 
problem description markov decision process mdp tuple possibly infinite set states start state finite set actions reward function 
rmax transition model 
gives probability landing state action state stochastic policy map 
probability action state ways defining policy quality value 
horizon discount factor finite horizon discounted value function defined 
infinite state space summation replaced integral 
define optimality criteria 
finite horizon total reward horizon 
infinite horizon discounted reward discount lim 
infinite horizon average reward avg lim assume limit exists 
fix optimality criterion goal find policy high value 
discussed assume restricted set policies wish select 
assume set policies parameterized continuously differentiable simple example may dimensional state action mdp sigmoidal probability choosing action state exp 
note framework encompasses cases family consists policies depend certain aspects state 
particular pomdps restrict attention policies depend observables 
restriction results subclass stochastic memory free policies 
introducing artificial memory bits process state define stochastic limited memory policies 
value specified 
find best policy search maximizes 
compute approximate algorithms find local maximum 
nelder mead simplex search confused simplex algorithm linear programs require ability evaluate function optimized point 
compute estimate gradient respect variety deterministic stochastic gradient ascent methods 
write rewards assume single start state initial state distribution simplify exposition minor extensions trivial 
densities value functions optimization algorithms require method computing gradient 
real life mdps doing exactly completely infeasible due large infinite number states 
consider approach estimating quantities density reformulation value function expression 
policy induces probability distribution states time letting initial distribution giving probability define time slice distributions recurrence easy verify standard notions value defined earlier reformulated terms 

dot product operation equivalently expectation respect 
somewhat subtly case infinite horizon average reward avg 
limiting distribution exists 
reformulation gives alternative approach evaluating value policy compute time slice densities compute value 
unfortunately modification resolve difficulty 
representing computing probability densities large infinite spaces easier representing computing value functions 
results indicate representing computing high quality approximate densities may quite feasible :10.1.1.119.6111:10.1.1.119.6111
general approach approximate density propagation algorithm time slice distributions restricted family 
example continuous spaces set multivariate gaussians 
approximate propagation algorithm modifies equation maintain time slice densities 
precisely policy view defining operator takes distribution 
returns 
current policy rewrite 
cases closed approximate density propagation algorithms alternative operator properties hopefully close 
denote approximation denote 
selected carefully case close standard contraction analysis stochastic processes show proposition assume 
exists constant 
cases arbitrarily small case proposition meaningless 
systems reasonable independent 
furthermore empirical results show approximate density propagation track exact time slice distributions quite accurately 
approximate tracking applied planning task 
optimality criterion expressed define approximation replacing 
accuracy guarantees approximate tracking induce comparable guarantees value approximation guarantees performance policy optimizing possible proposition assume fixed jv 
proposition arg max arg max 
max jv 
differentiating approximate densities section discuss different techniques maintaining approximate density approximate propagation operator show combined gradient ascent perform policy search 
general assume family distributions parameterized example set dimensional multivariate gaussians diagonal covariance matrices dimensional vector specifying mean vector covariance matrix diagonal 
consider task doing gradient ascent space policies optimality criterion say 
differentiating relative get 
avoid introducing new notation denote associated vector parameters parameters function 
internal gradient term represented jacobian matrix entries representing derivative parameter relative parameter gradient computed simple recurrence chain rule derivatives summand jacobian derivative transition operator relative policy parameters 
second product terms derivative relative distribution parameters result previous step recurrence 
deterministic density propagation consider transition operator simplicity omit dependence 
idea approach try get close possible subject constraint 
specifically define projection operator takes distribution returns distribution closest sense define 
order ensure gradient descent applies setting need ensure differentiable functions 
clearly instantiations idea assumption holds 
provide examples 
consider continuous state process nonlinear dynamics mixture conditional linear gaussians 
define set multivariate gaussians 
operator takes distribution mixture gaussians computes mean covariance matrix 
easily computed parameters simple differentiable algebraic operations 
different example algorithm approximate density propagation dynamic bayesian networks dbns 
dbn structured representation stochastic process exploits conditional independence properties distribution allow compact representation 
dbn state space defined set possible assignments set random variables xn transition model described bayesian network fragment nodes fx xn node represents represents nodes network forced roots parents associated conditional probability distributions 
node associated conditional probability distribution cpd specifies parents 
transition probability defined parents 
dbns support compact representation complex transition models mdps 
extend dbn encode behavior mdp stochastic policy introducing new random variable representing action taken current time 
parents variables state action allowed depend 
cpd may compactly represented function approximation distribution actions defined different contexts 
discrete dbns number states grows exponentially number state variables making explicit representation joint distribution impractical 
algorithm defines set distributions defined compactly set marginals smaller clusters variables 
simplest example set distributions xn independent 
parameters defining distribution parameters multinomials 
projection operator simply distributions individual variables differentiable 
useful corollary analysis decay rate structured higher decay rate multiple applications converge rapidly stationary distribution property useful approximating optimize relative avg stochastic density propagation settings assumption direct access strong 
weaker assumption access generative model black box generate samples appropriate distribution generate samples 
case different approximation scheme 
operator stochastic operator 
takes distribution generates number random state samples 
action generate sample transition distribution 

sample hs assigned weight compensate fact actions selected equal probability 
resulting set samples weighted input statistical density estimator uses estimate new density assume density estimation procedure differentiable function weights reasonable assumption 
clearly compute approximate value 
gradient computation far trivial 
particular compute derivative consider behavior perturbed say applied originally 
case entirely different set samples probably generated possibly leading different density 
hard see differentiate result perturbation 
propose alternative solution importance sampling 
change samples modify weights reflect change probability generated 
specifically fitting define sample hs weight compute derivatives respect parameters required 
vector parameters 
chain rule term derivative estimated density relative sample weights matrix 
second derivative weights relative parameter vector jacobian easily computed 
lane fact lane fact lateral action lateral action function evaluations actual avg 
cost estimated avg 
cost driving task dbn model policy search optimization results experimental results tested approach different domains 
average reward dbn mdp problem shown task find policy changing lanes driving moderately busy lane highway slow lane fast lane 
model bat dbn result separate effort build model driver behavior 
simplicity assume car speed controlled automatically concerned choosing lateral action change lane drive straight 
observables shown clearance car lane close medium far 
agent pays cost step blocked meaning driving close car front pays penalty step staying fast lane 
policies specified action probabilities possible observation combinations 
reasonably small number parameters simplex search algorithm described earlier optimize 
process mixed quite quickly fairly approximation fully factored representation joint distribution single cluster observables 
evaluations averages monte carlo trials steps 
shows estimated actual average rewards policy parameters evolved time 
algorithm improved quickly converging natural policy car generally staying slow lane switching fast lane necessary overtake 
second experiment bicycle simulator 
actions corresponding leaning left center right applying negative zero positive torque dimensional state includes variables bicycle tilt angle orientation angle 
bicycle tilt exceeds falls enters absorbing state 
policy search space selected twelve simple manually chosen fine tuned features state actions chosen softmax probability action exp 
exp 

problem comes generative model complicated nonlinear noisy bicycle dynamics stochastic density propagation version algorithm stochastic gradient ascent 
distribution mixture singleton point consisting absorbing state multivariate gaussian 
task domain balance reliably bicycle 
horizon discount samples density propagation step quickly achieved 
trying learn ride goal radius away succeeded finding policies reliably 
formal evaluation difficult sufficiently hard problem finding solution considered success 
slight parameter sensitivity best results obtained picked fit care part data earlier successful trials representative fairly rider state distribution algorithm able obtain solutions median riding distances km goal 
significantly better results obtained learning planning setting value function approximation solution reported larger riding distances goal km single best trial km 
new variants algorithms performing direct policy search deterministic stochastic density propagation settings 
empirical results shown methods working large problems 

kevin murphy help bayes net toolbox bicycle simulator 
ng supported berkeley fellowship 
koller parr supported aro muri program integrated approach intelligent systems darpa contract subcontract iet onr contract darpa hpkb program sloan foundation powell foundation 
baird moore 
gradient descent general reinforcement learning 
nips 
boutilier dean hanks 
decision theoretic planning structural assumptions computational leverage 
artificial intelligence research 
boyen koller 
tractable inference complex stochastic processes 
proc 
uai pages 
forbes huang kanazawa russell 
bayesian automated taxi 
proc 
ijcai 
koller 
learning approximation stochastic processes 
proc 
icml pages 
meuleau peshkin 
kim kaelbling 
learning finite state controllers partially observable environments 
proc 
uai 

learning drive bicycle reinforcement learning shaping 
proc 
icml 
williams singh 
experiments algorithm learns stochastic memoryless policies pomdps 
nips 
williams 
simple statistical gradient algorithms connectionist reinforcement learning 
machine learning 
experiments learning accomplished faster simulator integration delta time constant training 
shaping reinforcements chosen reward progress goal training bike infinitely distant goal 
balancing experiments sampling fallen portion distributions obviously inefficient samples samples drawn non absorbing state portion gaussian tails corresponding tilt angles greater truncated weighted accordingly relative absorbing state portion 
