maximum entropy text classification kamal nigam cs cmu edu john lafferty lafferty cs cmu edu andrew mccallum zy mccallum com school computer science carnegie mellon university pittsburgh pa just research henry street pittsburgh pa proposes maximum entropy techniques text classification 
maximum entropy probability distribution estimation technique widely variety natural language tasks language modeling part speech tagging text segmentation 
underlying principle maximum entropy external knowledge prefer distributions uniform 
constraints distribution derived labeled training data inform technique minimally non uniform 
maximum entropy formulation unique solution improved iterative scaling algorithm 
maximum entropy text classification estimating conditional distribution class variable document 
experiments text datasets compare accuracy naive bayes show maximum entropy significantly better worse 
remains results indicate maximum entropy promising technique text classification 
variety techniques supervised learning algorithms demonstrated reasonable performance text classification non exhaustive list includes naive bayes lewis mccallum nigam sahami nearest neighbor yang support vector machines joachims dumais boosting schapire singer rule learning algorithms cohen singer slattery craven single technique proven consistently outperform domains 
explores maximum entropy text classification alternative previously text classification algorithms 
maximum entropy widely variety natural language tasks including language modeling chen rosenfeld rosenfeld text segmentation beeferman part speech tagging ratnaparkhi prepositional phrase attachment ratnaparkhi maximum entropy shown viable competitive algorithm domains 
maximum entropy general technique estimating probability distributions data 
riding principle maximum entropy known distribution uniform possible maximal entropy 
labeled training data derive set constraints model characterize class specific expectations distribution 
constraints represented expected values features real valued function example 
improved iterative scaling algorithm finds maximum entropy distribution consistent constraints 
text classification scenario maximum entropy estimates conditional distribution class label document 
document represented set word count features 
labeled training data estimate expected value word counts class class basis 
improved iterative scaling finds text classifier exponential form consistent constraints labeled data 
experimental results show maximum entropy technique warrants investigation text classification 
data set example maximum entropy reduces classification error compared naive bayes 
data sets basic maximum entropy perform naive bayes 
evidence basic maximum entropy suffers overfitting poor feature selection 
prior applied maximum entropy performance improved cases 
performs better naive bayes data sets 
areas investigation exist may improve performance 
include appropriate feature selection bigrams phrases features adjusting prior sparsity data 
proceeds follows 
section presents general framework maximum entropy estimating conditional distributions 
specific application maximum entropy text classification discussed section 
related section 
experimental results section 
section discusses plans 
maximum entropy motivating idea maximum entropy prefer uniform models satisfy constraints 
example consider way text classification task told average documents word professor faculty class 
intuitively document professor say chance faculty document chance classes 
document professor guess uniform class distribution 
model exactly maximum entropy model conforms known constraint 
calculating model easy example constraints satisfy rigorous techniques needed find optimal solution 
csisz ar provides tutorial maximum entropy techniques 
general formulation estimate probability distribution 
interested classification limit discussion learning conditional distributions labeled training data 
specifically learn conditional distribution class label document 
constraints features training data set constraints conditional distribution 
constraint expresses characteristic training data learned distribution 
real valued function document class feature 
maximum entropy allows restrict model distribution expected value feature seen training data stipulate learned conditional distribution cjd property jdj cjd practice document distribution unknown interested modeling 
training data class labels approximation document distribution enforce constraint jdj jdj cjd maximum entropy step identify set feature functions useful classification 
feature measure expected value training data take constraint model distribution 
parametric form constraints estimated fashion guaranteed unique distribution exists maximum entropy 
shown della pietra distribution exponential form cjd exp feature parameter estimated simply normalizing factor ensure proper probability exp constraints estimated labeled training data solution maximum entropy problem solution dual maximum likelihood problem models exponential form 
additionally guaranteed likelihood surface convex having single global maximum local maxima 
suggests possible approach finding maximum entropy solution guess initial exponential distribution correct form starting point perform hillclimbing likelihood space 
local maxima converge maximum likelihood solution exponential models global maximum entropy solution 
improved iterative scaling section briefly outline derivation improved iterative scaling iis hillclimbing algorithm calculating parameters maximum entropy classifier set constraints 
describe algorithmic details procedure 
complete description derivation improved iterative della pietra presentation follows berger iis performs hillclimbing parameter log likelihood space 
set training data calculate log likelihood exponential model equation jd log jd gamma log exp step iis find incrementally set parameters 
likelihood function convex guarantee iis succeeds improving likelihood know converge globally optimal set parameters maximum likelihood solution parametric form solution maximal entropy 
start initial vector parameters step improve setting equal delta higher likelihood 
step want find delta difference likelihoods positive gamma jd inequality gamma log gamma jensen inequality bound expression auxiliary function call gamma jd ffi gamma cjd exp ffi sum features training instance guarantee increase likelihood find delta positive 
find best delta differentiating respect change parameter ffi turn solving maxima ffi gamma cjd exp ffi note straightforward set equal zero solve increment ffi parameter case constant experiments solved closed form 
solved numeric root finding procedure newton method 
case polynomial guaranteed positive root 
analysis shows find hillclimbing step changes improve model likelihood 
likelihood convex hillclimbing guaranteed converge global maximum 
foundation improved iterative scaling algorithm outlined table 
step iteration need estimate class labels cjd documents current model 
class labels calculate improved model parameters iterate 
ffl inputs collection labeled documents set feature functions ffl set constraints equation 
feature estimate expected value training documents 
ffl initialize zero 
ffl iterate convergence ffl calculate expected class labels document current parameters cjd equation 
ffl parameter ffl set ffi solve ffi equation 
ffl set ffi ffl output text classifier takes unlabeled document predicts class label 
table outline improved iterative scaling algorithm estimating parameters maximum entropy 
gaussian prior maximum entropy suffer overfitting 
constraints estimated labeled training data learning algorithms data sparse overfitting occur 
little data expected value feature training data may far true value 
introducing prior model overfitting reduced performance improved 
integrate prior maximum entropy maximum posteriori estimation exponential model maximum likelihood estimation 
gaussian prior model mean zero diagonal covariance matrix 
prior favors feature weightings closer zero extreme 
prior probability model just product gaussian feature value variance oe oe exp gamma oe integrating prior improved iterative scaling requires adding single term derivative equation ffi ffi gammaoe gamma cjd exp ffi new formula easily solved maximum numeric root finding procedure newton method 
chen rosenfeld shown introducing gaussian prior improves performance language modeling tasks sparse data causes overfitting 
derives update rule equation 
maximum entropy text classification order apply maximum entropy domain need select set features setting constraints 
text classification maximum entropy word counts features 
specifically word class combination instantiate feature ae number times word occurs document number words representation word occurs class expect weight word class pair higher word paired classes 
natural language tasks maximum entropy features naturally binary features 
text classification expect features accounting number times word occurs improve classification 
example naive bayes implementations counts outperform implementations mccallum nigam note scaled counts features simple counts 
initially choose representation computational efficiency lets perform iis iteration closed form 
implications choice discussed section 
especially pleasing aspect maximum entropy suffer independence assumptions 
example consider phrase buenos aires words occur rarely occur 
naive bayes evidence phrase 
maximum entropy hand discount features weight classification appropriately reduced half 
constraints expectations counts 
implication freedom independence assumptions bigrams phrases easily added features maximum entropy worry features overlapping 
experiments expanded features promising area 
related studies performed maximum entropy text classification 
study ratnaparkhi preliminary experiment 
comparison maximum entropy decision trees maximum entropy performs better classifying acq class reuters data set 
binary features counts 
generally expect representing counts binary features enhance performance 
study feature selection model building mikheev examined text classification performance corpus technical abstracts 
maximum entropy compares favorably smoothed logistic term weighting model 
features binary valued 
interestingly pairs words word phrases features improved performance 
results section provides preliminary empirical evidence maximum entropy competitive text classification algorithm 
results different data sets 
data sets protocol webkb data set craven contains web pages gathered university computer science departments 
pages divided categories student faculty staff course project department 
populous entity representing categories student faculty course project containing pages 
stemming stoplist 
resulting vocabulary words 
industry sector hierarchy available market guide 
www com consists web pages classified hierarchy industry sectors mccallum web pages partitioned classes levels deep hierarchy 
tokenizing data stem 
removing tokens occur stoplist corpus vocabulary size 
newsgroups data set contains articles evenly divided usenet discussion groups joachims categories fall confusable clusters example comp discussion groups discuss religion 
tokenizing data skip usenet headers discarding subject line tokens formed contiguous alphabetic characters stemming 
documents containing uu encoded segments discarded 
resulting vocabulary removing words occur stoplist words 
empirical results maximum entropy compared naive bayes lewis mitchell popular baseline text classification 
multinomial instantiation naive bayes mccallum nigam accounts number times word occurs 
variants multinomial naive bayes tested 
scaled naive bayes word count document scaled document constant number word occurrences 
regular naive bayes data sets available internet 
see www cs cmu edu 
data set regular naive bayes scaled naive bayes basic maximum entropy maximum entropy prior webkb industry sector newsgroups table classification error maximum entropy text classification data sets compared regular scaled naive bayes 
shown optimal vocabulary size indicated parentheses 
note maximum entropy outperforms regular naive bayes comparison mixed scaled naive bayes 
iis iterations maximum entropy prior basic maximum entropy accuracy iterations improved iterative scaling industry sector dataset full vocabulary best dataset 
basic maximum entropy initially accuracy degrades slowly indicating possibility overfitting 
problems overfitting reduced gaussian prior performance improves 
note scaled vertical axis 
word counts left unscaled 
previous nigam observed datasets scaled naive bayes outperforms regular naive bayes 
vocabulary selection naive bayes maximum entropy performed top words mutual information class variable 
technique vocabulary selection naive bayes text classification yang pederson maximum entropy feature normalized count number times word occurs document belongs specific class equation 
constraints created word class pairs training data 
constrain expected value feature zero 
experiments gaussian prior single variance chosen features 
choosing variance vocabulary size done optimizing performance test set 
practice parameters set cross validation 
maximum entropy naive bayes experiments performed trials randomly selected train test splits 
webkb data set documents held testing 
industry sector newsgroups documents held 
newsgroups industry sector basic maximum entropy suf iis iterations maximum entropy prior basic maximum entropy accuracy iterations improved iterative scaling webkb dataset word vocabulary best dataset 
trend different industry sector dataset 
accuracy continues improve gradually iterations iis 
performance essentially unchanged gaussian prior 
note scaled vertical axis 
overfitting see results discussion 
reason documents cases validation set early stopping iis iterations 
maximum entropy experiments run fixed number iterations 
experiments table shows classification error results algorithms dataset 
columns show performance variations naive bayes 
interesting aside scaled naive bayes accurate regular naive bayes data sets 
third column shows performance basic maximum entropy prior 
note cases maximum entropy performs better regular naive bayes 
cases difference dramatic example webkb dataset maximum entropy provides reduction error naive bayes 
comparison scaled naive bayes results mixed 
webkb maximum entropy gives lower error industry sector newsgroups slightly worse 
datasets maximum entropy performs worse scaled naive bayes closer analysis basic maximum entropy indicates overfitting training data 
bottom line shows vocabulary size maximum entropy scaled naive bayes regular naive bayes classification error webkb data set different vocabulary sizes 
note increase error large vocab sizes maximum entropy indicating importance feature selection 
accuracy maximum entropy classifier progresses industry sector data rounds improved iterative scaling validation set halting 
note best performance achieved second iteration iis 
th iteration iis accuracy declined 
similar trends appear newsgroups data webkb data maximum entropy performs better naive bayes variations 
results show basic maximum entropy overfitting data cases performing 
maximum entropy gaussian prior overfitting reduced performance improves 
top line shows classification accuracy maximum entropy prior 
performance degrade classification error better 
fourth column table shows classification error cases 
cases overfitting evident error decreased 
performance industry sector data set better scaled naive bayes 
performance webkb data set overfitting problems essentially unchanged 
analysis indicates areas 
shows error classifiers different vocabulary sizes webkb 
error maximum entropy classifier increases suddenly words 
shows feature selection important factor maximum entropy 
experiments feature selection performed method natural naive bayes 
section discuss feature selection techniques appropriate maximum entropy 
areas remain 
results indicate maximum entropy may sensitive poor feature selection 
feature maximum entropy combination class word need features classes vocabulary word 
example professor feature faculty course classes student class 
iterative greedy feature selection technique maximum entropy della pietra shown create compact representations result maximum entropy performance 
intend test approach text classification 
experiments gaussian prior variance feature values 
need case 
features large amount training data overfitting problem large variance prior 
features sparse training data strong prior smaller variance 
experiments adjust prior amount training data may improve results 
area ongoing lies representation features constraints 
results scaled counts features 
preliminary results unscaled counts indicate accuracy decreases 
hypothesize unscaled counts hurts long documents repeated words strong weight 
suggests feature functions form log count sub linear representation counts 
promising aspect maximum entropy naturally handles overlapping features 
example supplement word features bigram phrase non text features 
maximum entropy hurt strong independence assumptions naive bayes features 
try augment expanded feature classes 
area thorough comparison maximum entropy state art text classification algorithms domains 
ongoing includes direct comparisons maximum entropy support vector machines joachims neighbor yang ripper cohen singer summary maximum entropy technique popular natural language tasks 
overriding principle minimal assumption maximal entropy matches intuition probability distributions estimated data 
empirical analysis shows maximum entropy competitive better naive bayes text classification 
adam berger helpful insightful discussion 
supported part darpa hpkb program contract 
beeferman beeferman berger lafferty 
statistical models text segmentation 
machine learning 
berger adam berger 
convexity maximum likelihood 
www cs cmu edu 
chen rosenfeld stanley chen ronald rosenfeld 
gaussian prior smoothing maximum entropy models 
technical report carnegie mellon university 
cohen singer william cohen yoram singer 
context sensitive learning methods text categorization 
sigir proceedings nineteenth annual international acm sigir conference research development information retrieval pages 
craven craven dipasquo freitag mccallum mitchell nigam slattery 
learning extract symbolic knowledge world wide web 
proceedings fifteenth national conference artificial aaai pages 
csisz ar csisz ar 
maxent mathematics information theory 
hanson silver editors maximum entropy bayesian methods 
kluwer academic publishers 
della pietra stephen della pietra vincent della pietra john lafferty 
inducing features random fields 
ieee transactions pattern analysis machine intelligence 
dumais dumais platt heckerman sahami 
inductive learning algorithms representations text categorization 
proceedings acm cikm 
joachims thorsten joachims 
probabilistic analysis rocchio algorithm tfidf text categorization 
machine learning proceedings fourteenth international conference icml pages 
joachims thorsten joachims 
text categorization support vector machines learning relevant features 
machine learning ecml tenth european conference machine learning pages 
lewis david lewis 
naive bayes independence assumption information retrieval 
machine learning ecml tenth european conference machine learning pages 
mccallum nigam andrew mccallum kamal nigam 
comparison event models naive bayes text classification 
aaai workshop learning text categorization 
tech 
rep ws aaai press 
www cs cmu edu mccallum 
mccallum andrew mccallum ronald rosenfeld tom mitchell andrew ng 
improving text shrinkage hierarchy classes 
machine learning proceedings fifteenth international conference icml pages 
mikheev andrei mikheev 
feature maximum entropy models 
machine learning 
appear 
mitchell tom mitchell 
machine learning 
mcgraw hill new york 
nigam kamal nigam andrew mccallum sebastian thrun tom mitchell 
text classification labeled unlabeled documents em 
machine learning 
appear 
ratnaparkhi adwait ratnaparkhi jeff reynar salim roukos 
maximum entropy model prepositional phrase attachment 
proceedings arpa human language technology workshop pages 
ratnaparkhi adwait ratnaparkhi 
maximum entropy model part speech tagging 
proceedings empirical methods natural language conference 
ratnaparkhi adwait ratnaparkhi 
maximum entropy models natural language ambiguity resolution 
phd thesis university pennsylvania 
rosenfeld ronald rosenfeld 
adaptive statistical language modeling maximum entropy approach 
phd thesis carnegie mellon university 
sahami mehran sahami 
learning limited dependence bayesian classifiers 
kdd proceedings second international conference knowledge discovery data mining pages 
aaai press 
schapire singer robert schapire yoram singer 
boostexter boosting system text categorization 
machine learning 
appear 
slattery craven sean slattery mark craven 
combining statistical relational methods learning hypertext domains 
proceedings th international conference inductive logic programming ilp 
yang pederson yiming yang jan pederson 
feature selection statistical learning text categorization 
machine learning proceedings fourteenth international conference icml pages 
yang yiming yang 
evaluation statistical approaches text categorization 
journal information retrieval 
appear 
