distributional clustering words text classification douglas baker yz www cs cmu edu school computer science carnegie mellon university pittsburgh pa andrew mccallum zy www cs cmu edu mccallum just research henry street pittsburgh pa describes application distributional clustering document classification 
approach clusters words groups distribution class labels associated word 
unsupervised techniques latent semantic indexing able compress feature space aggressively maintaining high document classification accuracy 
experimental results obtained real world data sets show reduce feature dimensionality orders magnitude lose accuracy significantly better latent semantic indexing class clustering feature selection mutual information markov blanket feature selection :10.1.1.155.2293:10.1.1.13.9919:10.1.1.108.8490
show aggressive clustering results improved classification accuracy classification clustering 
popularity internet caused exponential increase amount line text number people create text 
amount documents number users rise automatic document categorization increasingly important tool helping people organize vast amount data 
statistical document classification algorithms applied categorizing classifying web pages sorting electronic mail learning interests users :10.1.1.21.7950:10.1.1.35.6633
cluster words groups specifically benefit document classification 
study devoted word clustering language modeling word occurrence little done word clustering document classification :10.1.1.13.9919
underlying clustering method apply distributional clustering information theoretic approach shown performance language modeling 
word clustering methods create new reduced size event spaces joining similar words groups 
distributional clustering joining words digital hard copy part personal classroom granted fee provided copies distributed profit commercial advantage copyright notice title publication date appear notice copying permission acm copy republish post servers redistribute lists requires prior specific permission fee 
sigir melbourne australia fl acm 
duce similar probability distributions target features occur words question 
reasoning understood intuitively follows 
different words vote similarly possible answers task hand joining words causing vote average individual votes hurt performance 
fact performance may increased clustering training data sparse averaging statistics similar words result robust estimates 
similarity distributions measured variant kullback leibler divergence 
document classification target concept class label 
measure word similarity distributions class labels associated words question 
example consider classifying documents sports categories individual sport baseball hockey tennis 
training data words puck goalie may occur hockey class 
purposes classification task need distinguish 
words strongly indicative hockey class clustered 
furthermore distributional clustering sensibly cluster words indicative class 
word team may occur equal frequency classes baseball hockey word teammates may occur equally just classes 
words merged 
section gives detailed example idea 
benefits word clustering key benefits word clustering useful semantic word clusterings higher classification accuracy smaller classification models 
second reasons shared feature selection feature clustering seen complement alternative feature selection 
feature clustering better reducing number redundant features feature selection better removing detrimental noisy features 
word clustering provide useful semantically related groups words effect automatically generated thesaurus 
interesting aspect semantic groups produced algorithm depend class labels assigned documents 
reflects fact words synonyms context 
clusters supervised machine learning paradigm task focussed 
usefulness automatically generated thesaurus difficult evaluate subject 
second word clustering result higher classifi cation accuracy described 
discussed sections 
third size classification model greatly reduced separate sets parameters words replaced single set parameters word cluster 
results include successful size reductions orders magnitude example 
argue successful small footprint text classification models increasingly important wide spread popular text classification 
example large population high volume routing tasks required companies involve text categorization hundreds thousands class labels stream documents arriving rate hundreds second word clustering avoid need machines gigabytes memory 
scale consider handheld computers automatically organize data text classification word clustering allow classification models fit restricted memory machines 
text classification spreads servers research machines home computers secretaries machines network computers reducing number features statistics maintained important 
furthermore maintain dramatically reduced dimensionality allows complex algorithms feasible original dimensions 
contributions introduces application distributional clustering document classification naive bayes classifier 
derive naive bayes explain assumptions discuss close ties cross entropy 
describe distributional clustering show distributional clustering clusters features minimize errors cross entropy 
experimental results real world text corpora including newswire stories usenet articles web pages 
results show distributional clustering reduce feature dimensionality orders magnitude lose accuracy 
performance significantly better class clustering mutual information clustering latent semantic indexing feature selection information gain feature selection markov blanket :10.1.1.155.2293:10.1.1.13.9919:10.1.1.108.8490
data sets show clustering increases classification accuracy 
hypothesize happen cases discuss possible improvements 
clustering words class distributions section introduces probabilistic framework derives naive bayes classifier explains distributional clustering 
previous distributional clustering form kullback leibler divergence mean 
weighted average simple average hard clustering soft greedy agglomerative method divisive entropy method 
probabilistic framework naive bayes approach text classification bayesian learning framework 
assume text data generated parametric model training data calculate bayes optimal estimates model parameters 
equipped estimates classify new test documents bayes rule turn generative model calculate probability class generated test document question 
classification simple matter selecting probable class 
training data consists set documents fd document labeled class set classes fc cmg 
assume data generated mixture model parameterized correspondence mixture model components classes 
data generation procedure document understood selecting class class priors having corresponding mixture generate document parameters distribution jc 
probability generating document independent class sum total probability mixture components jcj jc expand notion document generated individual mixture component 
approach document generation language modeling 
notions naive bayes documents events words document attributes event multi variate bernoulli model consider words events multinomial model 
multinomial naive bayes shown perform multi variate bernoulli real world corpora 
say document comprised ordered sequence word events write ik word position document expand expression probability document class jc saying probability sequence equal product probabilities events sequence remembering event may depend events preceded jc jd jd ik jc iq assume document length jd distributed independently class 
naive bayes assumption assume probability word event document independent word context furthermore independent position document 
note saying uni gram language model 
word event drawn multinomial distribution set words vocabulary write th word ik express naive bayes assumption writing ik jc iq jc assumption correspondence mixture model components classes naive bayes assumption mixture model composed disjoint sets parameters class parameter set class composed probabilities word jc jc 
parameters model class prior probabilities written 
calculate estimates written parameters training data 
jc estimates consist straightforward counting events supplemented smoothing laplacean prior primes estimate count 
define count number times word occurs document define jd document class label estimate probability word class jc jdj jd jv jv jdj ws jd class prior parameters estimated maximum likelihood estimate fraction documents class corpus jdj jd jdj estimates parameters calculated training documents classification performed test documents calculating probability class evidence test document selecting class highest probability 
formulate applying bayes rule substituting jc equations 
jd jc jd wd ik jc jcj cr jd wd ik mixture model word independence assumptions violated practice real world data empirical evidence naive bayes performs spite violations :10.1.1.35.6633
friedman domingos pazzani discuss violation word independence assumption little damage classification accuracy :10.1.1.144.7475
measuring word similarity distributional clustering address question cluster words context generative model naive bayes 
word clustering algorithms define similarity measure words collapse similar word single events longer distinguish constituent words 
typically parameters cluster weighted average parameters constituent words 
consider example random variable classes distribution particular word write distribution cjw 
words ws clustered new distribution weighted average individual distributions class steering tire cluster newsgroups data set class probability distributions words tire steering combination cjw ws ws cjw ws ws distributional clustering differs machine learning approaches similarity metrics nearest neighbor measures similarity target variable trying estimate task hand input attributes 
specifically examines probability distribution target variable induced different events clustered measures similarity events similarity induced target variable distributions 
context document classification target variable task hand class label 
distributional clustering measures similarity words ws similarity class variable distributions induce cjw 
example class distributions data newsgroups corpus shown 
consider line word tire 
horizontal axis ticks order irrelevant list class labels 
vertical axis indicates probability class word tire shape line shows probability distribution classes tire 
graph indicates word occurs classes rec autos rec motorcycles mildly classes 
remembering classification task graph interpreted picture word tire votes classes occurs test document 
line shows essence tire contributes classification algorithm 
notice line word steering 
shape distribution quite similar tire 
third line labeled cluster shows class distribution cluster containing words words similar distributions distribution cluster similar 
word tire voted distribution cluster tire class distribution voting differently final classification scores far 
table shows cluster cluster cluster motorcycle motorcycle baseball automobile hockey honda bike season rear players wheel yamaha scored steering harley tire riders suspension roster throttle mechanic rust leagues table lists highest probability words clusters created distributional clustering newsgroups dataset 
words fall cluster clusters 
example expresses core intuition distributional clustering document classification class distributions cjw express individual words contribute classification cluster words preserve shape distributions 
turn question exactly measure difference probability distributions 
kullback leibler divergence information theoretic measure just 
kl divergence class distributions induced ws written cjw jjp defined jcj jw log jw context information theory kl divergence intuitively understood measure inefficiency occurs messages sent distribution cjw encoded code optimal different distribution 
kl divergence odd properties 
symmetric infinite event non zero probability distribution zero probability second distribution 
distributional clustering related measure problems 
average kl divergence distribution mean distribution called kl divergence mean 
earlier weighted average simple average 
delta cjw jjp cjw ws ws delta jjp cjw ws metric understood expected amount inefficiency incurred compressing distributions optimally code code optimal mean 
explanation clear metric fit clustering distance metric 
describes perfectly effect clustering events generated individual statistics clustered generate combined statistics 
sort vocabulary mutual information class variable 
initialize clusters singletons top words 
loop words put clusters merge clusters similar equation resulting gamma clusters 
create new cluster consisting word sorted list restoring number clusters table algorithm distributional clustering minimizes error naive bayes score classification naive bayes intimately related information theory 
easily shown assuming uniform class prior choosing probable class naive bayes identical choosing class minimal cross entropy test document 
naive bayes classification formula equation assume uniform class prior dropping series transformations change class gets highest score drop denominator constant classes transform product word position document equivalent expression product words vocabulary take log entire expression divide document length jd results gamma jv jd delta log gamma jc delta precisely expression cross entropy distribution words document jd distribution words class jc random variable words 
cross entropy representative naive bayes score class express error naive bayes score incurred clustering words 
difference cross entropies words joined cross entropy words joined 
simple algebraic manipulation error expression results exactly equation weighted sum kl divergences mean 
conclude words clustered similarity metric increase error naive bayes scores minimized 
clustering algorithm address question similarity metric form clusters 
create clusters deterministic word membership simple greedy agglomerative approach works practice scaling extremely efficiently large vocabulary sizes 
comparing similarity possible pairs words daunting jv operation consider pairs smaller subset size final number clusters desired 
stages algorithm clusters 
clusters initialized words highest mutual information class variable 
similar clusters joined word added singleton cluster bring total number clusters back table contains outline algorithm 
contrast probabilistic soft clustering previous distributional clustering formally rigorous allows clustering greedy :10.1.1.136.3516
avoid costly em style update procedure find stable configuration cluster centroids cluster membership probabilities 
related distributional clustering address problem sparse data building statistical language models natural language processing previously applied document classification :10.1.1.14.2093
larger data sets prevalent sparseness fewer class labels 
chimerge uses form distributional clustering discretize numeric attributes subsequent classification 
agglomerative hard clustering algorithm uses statistic similarity metric 
tried experiments kl divergence average yields better performance 
chi extension chimerge feature selector numeric attributes :10.1.1.35.2557
liu setiono observe values attribute clustered value irrelevant classification task removed 
class clustering uses agglomerative hard clustering algorithm clustering criterion designed maximize average mutual information clusters class variable :10.1.1.13.9919
criterion implicitly measures similarity distributions cjw similarity distributions cj cj ws features 
find average mutual information clustering criterion text classification multinomial naive bayes model considers information class label indicated presence absence word document classifier considers words document 
clustering criterion class distributions words appear better suited multinomial naive bayes classifier 
argue kl divergence choice criteria 
clustering reduces dimensionality feature space seen form feature selection remove features 
previous study feature selection mutual information class label best text common time space efficient methods 
mutual information words classes capture dependencies words 
koller sahami markov blanket feature selection algorithm aims address exactly :10.1.1.155.2293
technique principles distributional clustering examines cjw tries preserve proper distribution 
latent semantic indexing unsupervised dimensionality reduction technique information retrieval explicitly accounts dependencies words :10.1.1.108.8490
brief applies principle component analysis pca documents represented word vectors 
dumais applies text classification representing class centroid vector sum feature vectors documents class 
new document labelled class centroid feature vector closest measured cosine similarity vectors 
linear squares fit llsf method classification algorithm pca equivalent dumais lsi classification llsf uses dot product compute similarity cosine sensitive length vectors compared 
experimental results section provides empirical evidence distributional clustering able aggressively reduce number features maintaining high classification accuracy 
equal feature dimensionality achieves significantly higher accuracy feature clustering feature selection algorithms supervised latent semantic indexing class clustering feature selection mutual information class variable feature selection markov blanket method :10.1.1.155.2293:10.1.1.13.9919
newsgroups data set collected ken lang contains articles evenly divided usenet discussion groups :10.1.1.21.7950
topic classes quite confusable computers discuss religion 
tokenizing data skipped usenet headers stoplist stem hurt accuracy 
resulting vocabulary removing words occur words 
modapte test train split reuters data set www research att com lewis contains training documents testing documents gathered reuters newswire 
overlapping topic categories exists training testing document 
number training documents class varies nearly 
largest classes contain documents classes fewer training documents 
removed words occurrences 
resulting vocabulary words 
gathered entirety yahoo 
science hierarchy july 
web pages divided disjoint classes chopping hierarchy levels deep 
removing stopwords words occur vocabulary contains words 
top shows classification accuracy results newsgroups data set methods considered 
horizontal axis indicates number features classification model vertical axis percentage test documents correctly classified 
results averages trials randomized test train splits case supervised lsi uses training training slow articles 
comparison show performance curves distributional clustering training training 
notice features reduction orders magni number features distributional clustering distributional clustering train clustering latent semantic indexing information gain feature selection mutual information feature clustering number features distributional clustering information gain feature selection markov blanket feature selection top classification accuracy newsgroups data set varying numbers features 
highest curves distributional clustering 
extremely tight bars data point show standard error 
bottom classification accuracy subset newsgroups data set 
temporary technical problems prevent curve markov blanket feature selector continuing features 
tude distributional clustering achieves accuracy lower full vocabulary 
comparison supervised lsi reaches accuracy 
feature selection mutual information class clustering lower respectively 
furthermore note distributional clustering provides small statistically significant increase best performance possible clustering 
highest accuracy clustering full vocabulary 
clustering results features higher best clusters 
increased performance indicates distributional clustering providing slightly accurate estimates jc supervised lsi resulted higher accuracy raw feature set data 
markov blanket feature selector missing graph due memory cpu requirements algorithm able run full data set 
bottom graph shows results corpus consisting talk politics classes newsgroups data set 
reduced data set documents removing words occurring fewer documents word vocabulary 
performance distributional clustering data set striking 
consistently better techniques features maintains accuracy near techniques fall 
examination features show clusters indicative classes 
markov blanket feature selector performed slightly better feature selection mutual information performed worse 
believe distributional clustering performs better feature selection merging preserves information discarding 
features infrequent useful occur get removed feature selector feature merging keeps 
clustering lsi advantage combines information discarding 
top graph shows lsi performing feature selector 
initial dimensionality reduction lsi unsupervised distributional clustering supervised 
supervised techniques take advantage class labels order concentrate efforts specific task hand 
believe difference explains accuracy increase distributional clustering lsi top graph 
linear discriminant analysis supervised technique similar lsi feel may text classification experimented technique 
lsi takes advantage word occurrences thought traditional lsi classification method may put lsi best light 
traditional method classifies test documents measuring cosine similarity class centroid 
class centroid average training documents class naive bayes loses document boundaries word occurrence statistics 
tested hypothesis replacing centroid distance component nearest neighbor classifier measures distances individual training documents 
change increase lsi performance features beat distributional clustering naive bayes performance 
note nearest neighbor computationally expensive centroid methods 
techniques comparison class clustering performed worst 
discussed previous section technique match classification multinomial naive bayes model 
newsgroups data set wall clock training times algorithms distributional clustering minutes lsi minutes markov blanket feature selection hours mutual information feature selection seconds 
shows classification accuracy results reuters 
number features small distributional clustering outperforms methods 
data set document labelled multiple classes 
prediction test document considered correct classes 
shows classification accuracy results yahoo 
data set 
data sets feature selection improves performance significantly 
features naive bayes gets accuracy naive bayes obtains just features highest mutual information class label 
case losing information beneficial data noisy information hurts helps 
distributional clustering substitute feature selection clustering words provide benefit raw feature space see words line gets better performance begins clustering number features distributional clustering information gain feature selection mutual information feature clustering classification accuracy reuters data set 
computational constraints prevented getting results lsi markov blanket feature selection time submission 
number features distributional clustering words information gain feature selection distributional clustering words mutual information feature clustering classification accuracy yahoo 
data set averaged runs 
error bars data point show standard error 
selected features words line 
result indicates distributional clustering somewhat able overcome noise clustering suggests place feature selection combinations 
principled approaches combinations feature selection feature merging topic 
discussion shown distributional clustering effective technique reducing number features needed text classification 
able reduce feature space orders magnitude losing percent classification accuracy 
result important text classification widespread application diverse size classification models increasing concern 
furthermore reduced dimensionality allow application complex methods 
distributional clustering better feature selection preserving information contained redundant features 
allows size model reduced aggressively maintaining performance 
susceptible detrimental features 
earlier distributional clustering shows distributional clustering addresses sparse data problem improving previously detrimental features 
observed small increase classification accuracy happened data set evenly distributed data 
surprised distributional clustering address sparse data problem experiments clusters words estimate affects performance 
bad estimate cjw clustering criterion strongly biased preserving distribution overcome lack data 
hypothesize previous distributional clustering saw sparse data improvements data sparse target variable values larger number correct jw values pattern match order fill bad jw estimate 
currently investigating ways augment distributional clustering address deficiency 
plan look techniques sensibly combining feature clustering feature selection take advantage strengths overcome need specifying advance number clusters create features remove 
previously mentioned lsi unsupervised dimensionality reduction technique singular value decomposition term document matrix 
underlying technique lsi find orthonormal basis term document space axes lie dimensions maximum variance 
linear discriminant analysis related technique attempts find basis distance means members class maximized variance class minimized 
plan investigate text classification 
acknowledgments grateful mehran sahami markov blanket feature selection code susan dumais lsi code friendly responsive help making run system 
thorsten joachims provided useful advice formatting reuters data 
hasegawa graciously answered questions regarding linear discriminant analysis 
yahoo permission data 
brown desouza mercer della pietra lai :10.1.1.13.9919
class gram models natural language 
computational linguistics 
thomas cover peter hart 
nearest neighbor pattern classification 
ieee transactions information theory 
thomas cover joy thomas 
elements information theory 
john wiley 
mark craven daniel dipasquo dayne freitag andrew mccallum tom mitchell kamal nigam sean slattery :10.1.1.35.6633
learning extract symbolic knowledge world wide web 
proceedings fifteenth national conference artificial intelligence aaai 
ido dagan fernando pereira lillian lee 
similarity estimation word cooccurrence probabilities 
proceedings nd annual meeting association computational linguistics 
deerwester dumais landauer furnas harshman :10.1.1.108.8490
indexing latent semantic analysis 
journal american society information science 
domingos pazzani 
independence conditions optimality simple bayesian classifier 
machine learning 
susan dumais 
lsi information filtering trec experiments 
technical report national institute standards technology 
jerome friedman 
bias variance loss curse dimensionality 
data mining knowledge discovery 
thorsten joachims :10.1.1.21.7950
probabilistic analysis rocchio algorithm tfidf text categorization 
international conference machine learning icml 

applied multivariate data analysis volume ii categorical multivariate methods 
springer verlag 
kerber 
chimerge discretization numeric attributes 
proceedings tenth national conference artificial intelligence aaai 
koller sahami :10.1.1.155.2293
optimal feature selection 
proceedings thirteenth international conference machine learning icml 
ken lang 
newsweeder learning filter netnews 
international conference machine learning icml pages 
lillian lee 
similarity approaches natural language processing 
phd thesis harvard university 
technical report tr 
david lewis marc ringuette 
comparison learning algorithms text categorization 
third annual symposium document analysis information retrieval pages 
david lewis kimberly knowles 
threading electronic mail preliminary study 
information processing management 
liu setiono :10.1.1.35.2557
chi feature selection discretization numeric attributes 
proceedings th ieee int conference tools artificial intelligence 
andrew mccallum kamal nigam 
comparison event models naive bayes text classification 
aaai workshop learning text categorization 
www cs cmu edu mccallum 
fernando pereira naftali tishby lillian lee 
distributional clustering english words 
proceedings st annual meeting association computational linguistics pages 

www com 
yiming yang 
noise reduction statistical approach text categorization 
proceedings th annual international acm sigir conference research development information retrieval sigir pages 
yiming yang jan pederson 
feature selection statistical learning text categorization 
icml pages 
