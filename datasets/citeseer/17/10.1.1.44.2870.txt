leave support vector machines jason weston department computer science royal holloway university london egham hill egham surrey tw oex uk 
new learning algorithm pattern recognition inspired upper bound leave error jaakkola haussler proved support vector machines svms vapnik new approach directly minimizes expression bound attempt minimize leave error 
gives convex optimization problem constructs sparse linear classifier feature space kernel technique 
algorithm possesses properties svms 
main novelty algorithm apart choice kernel parameterless selection number training errors inherent algorithm chosen extra free parameter svms 
experiments method benchmark datasets uci repository show results similar svms tuned best choice parameter 
support vector machines svms motivated minimizing vc dimension proven successful classification learning vapnik scholkopf vapnik algorithm turned favourable formulate decision functions terms symmetric positive definite square integrable function delta delta referred kernel 
class decision functions known kernel classifiers jaakkola haussler sign ff ff training data labels sigma simplicity ignore classifiers extra threshold term 
utilizing particular type decision rule training point corresponds basis function upper bound leave error svms proven jaakkola haussler bound motivates new algorithm find decision rule form equation minimizes bound 
structured follows section review svm algorithm 
section describe leave bound leave support vector machine loom algorithm motivated bound 
section reveal relationship svms looms section results comparison looms svms artificial benchmark datasets uci repository 
section summarize discuss directions 
support vector machines support vector machines vapnik aim minimize vc dimension finding hyperplane minimal norm separates training data mapped feature space nonlinear map phi construct hyperplane general case allows training error minimizes delta subject delta phi gamma uses decision rule sign delta phi tractability algorithm depends dimensionality remove dependency maximizing dual form ff gamma ff ff ff utilizing ff phi phi delta phi decision rule form equation 
alternatively primal dual formulation svm algorithm osuna girosi usual formulation describe direct correlation leave svms 
svm primal reformulation minimize ff ff subject ff gamma ff uses decision rule form equation 
leave support vector machines support vector machines obtain sparse solutions yield direct assessment generalization leave error bounded expected ratio number non zero coefficients number training examples vapnik jaakkola haussler measure generalization error derived class classifiers includes svms applied non sparse solutions 
bound follows theorem training set examples labels sigma svm trained maximizing equation leave error estimate classifier bounded gammay ff delta step function 
bound slightly tighter classical svm leave bound 
easy see considers training points ff leave errors bound 
vapnik bound assumes support vectors training points ff errors contribute errors equation ff practice means bound tighter sparse solutions 
leave bound theorem holds support vector machines motivation svms minimize vc bounds structural risk minimization vapnik term ff ff equation attempts minimize vc dimension 
wish construct classifiers motivated theorem directly attempt achieve low value expression need consider different learning technique 
theorem motivates algorithm directly minimize expression bound 
introduces slack variables standard approach cortes vapnik vapnik give optimization problem minimize ffi subject ff gamma ff chooses fixed constant margin ensure non zero solutions 
optimization problem tractable smallest value ffi obtain convex objective function ffi 
gives linear programming problem kernel classifiers uses decision rule equation 
note theorem longer valid learning algorithm 
study resulting method call leave support vector machine loom 
relationship svms section describe relationship looms svms areas method regularization sparsity induced decision function margin loss employed training 
regularization new technique appears free regularization parameter 
compared svms control amount regularization free parameter svms case obtains hard margin classifier training errors 
case noisy linearly inseparable datasets noise outliers class overlap accept training error constructing called soft margin 
find best choice training error margin tradeoff choose appropriate value looms soft margin automatically constructed 
occurs algorithm attempt minimize number training errors minimizes number training points classified incorrectly refer linear inseparability feature space 
svms loom machines essentially linear classifiers 
removed linear combination forms decision rule 
classify training point correctly removed linear combination classified correctly placed back rule 
seen ff sign training points pushed correct side decision boundary component linear combination 
sparsity support vector machines solutions new algorithm sparse coefficients ff non zero see section computer simulations confirming 
coefficient training point contribute leave error constraint algorithm assign non zero value coefficient training point order correctly classify 
training point classified correctly training points label close feature space training point contribution classification 
margin loss noting ff gamma ff equation see new algorithm written equivalent linear program minimize subject gamma ff ff setting optimization problem easy see training point linearly penalized failing obtain margin ae ff 
svms training point linearly penalized failing obtain margin ae see equation 
margin svms treated equivalently training pattern 
looms larger contribution training point decision rule larger value ff larger margin 
loom algorithm contrast svms controls margin training point adaptively 
method viewed way point phi outlier values points phi class small points class large ff equation large order classify phi correctly 
svms margin points phi attempt classify phi correctly 
looms margin automatically increased ff points attempt correctly classify phi 
adaptive margin provides robustness 
clear looms points phi representatives clusters centres feature space large values points class non zero ff experiments section describe experiments comparing new technique svms 
describe artificial data visualize techniques results benchmark datasets 
artificial data describe toy dimensional examples illustrate new technique works 
shows artificially constructed training problems left right various solutions top bottom page 
fixed kernel radial basis function rbf exp gamma yj solution problems leave machines looms free parameters svms controls soft margin free parameter solution top page training problems left right solution looms solutions svms various choices soft margin parameter 
problem left classes crosses dots linearly separable apart single outlier dot 
automatic soft margin control looms constructs classifier incorrectly classifies far right dot assuming outlier 
thick lines represent separating hyperplane dotted lines represent size margin 
support vectors training points ff emphasized rings 
note large margin loom classification 
support vector solutions second picture top downwards parameters middle bottom 
constructing hard margin overfits zero training error whilst decreasing svm solution tends decision rule similar looms 
note non smoothness decision rule examining margin dotted line 
outlier correctly classified 
second training problem right classes occupy opposite sides horizontally picture slightly overlap 
case data separable highly nonlinear decision rule reflected hard margin solution svm parameter bottom right 
reasonable choice rule middle right picture training problems left right pictures solved leave svms top left top right soft margin regularization parameter svms various lower pictures 
svms problems solved middle row bottom row 
svms correct choice free parameter 
parameterless decision constructed loom top right provides similar decision svm 
note smoothness loom solution comparison svm similar 
gives insight soft margin chosen looms 
simple toy training set shown 
picture left small cluster crosses top left picture single cross bottom right picture 
class dots distributed evenly space 
looms construct decision rule treats cross bottom right picture outlier 
second picture right problem near single cross add training points clusters crosses 
loom solution decision rule clusters single cross left hand picture close training points left decision rule classified correctly constraints 
training point close feature space points class considered outlier 
demonstration soft margin selection leave svm algorithm 
cluster crosses classified correctly sixth bottom right considered outlier left picture 
crosses placed near point previously considered outlier right picture algorithm decides training point outlier constructs classifier clusters 
benchmark datasets conducted computer simulations artificial real world datasets uci delve statlog benchmark repositories experimental setup ratsch authors article provide website obtain data briefly setup follows performance classifier measured average error partitions datasets training testing sets 
free parameter learning algorithm chosen median value best model chosen cross validation training datasets 
table compares percentage test error looms adaboost ab regularized adaboost abr svms known excellent classifiers competitiveness looms svms abr soft margin control parameter remarkable considering looms free parameter 
indicates soft margin automatically selected looms close optimal 
adaboost loses svm gmd de data benchmarks htm 
datasets pre processed zero mean standard deviation exact splits training testing sets author experiments obtained 
results ab abr svms obtained algorithms essentially algorithm designed deal noise free data 
ab abr svm loom banana cancer diabetes heart thyroid titanic table comparison percentage test error adaboost ab regularized adaboost abr support vector machines svms leave machines looms datasets 
show behaviour algorithm give plots 
top graph shows fraction training points non zero coefficients svs plotted log rbf width thyroid dataset 
see sparsity decision rule cf 
equation sparseness depends chosen value bottom graph shows number training test errors train err test err value slacks value bound theorem bound 
see training test error bound closely match natural algorithm minimized leave error 
minimum plots roughly log gamma indicating perform model selection known expressions 
note reasonable range different rbf widths test error roughly indicating automatic soft margin control overcomes overfitting problems 
values give best generalization error give sparse classifiers 
discussion article motivated bound leave error kernel classifiers new learning algorithm solving pattern recognition problems 
robustness approach despite having regularization parameter understood terms bound classify points correctly basis function terms margin see section empirical study 
point constructs kernel matrix ij regularization technique employed set diagonal matrix zero suggests control regularization control ridge regression techniques 
acknowledgments vladimir vapnik ralf herbrich alex gammerman help 
providing financial support gr 
rbf width rbf width slacks test err bound train err fraction training patterns support vectors top various error rates bottom plotted rbf kernel width leave machines thyroid dataset 
cortes vapnik corinna cortes vladimir vapnik 
support vector networks 
machine learning 
jaakkola haussler tommi jaakkola david haussler 
probabilistic kernel regression models 
proceedings conference ai statistics 
morgan kaufmann 
osuna girosi osuna girosi 
reducing run time complexity svms 
proceedings th int conf 
pattern recognition brisbane australia 
ratsch gunnar ratsch onoda klaus robert muller 
soft margins adaboost 
technical report royal holloway university london 
tr 
scholkopf bernhard scholkopf 
support vector learning 
phd thesis technische universita berlin berlin germany 
vapnik vladimir vapnik 
nature statistical learning theory 
springer verlag new york 
vapnik vladimir vapnik 
statistical learning theory 
john wiley sons new york 
