exploration strategies model learning multi agent systems david carmel shaul markovitch computer science department technion haifa israel 
cs technion ac il received 
accepted final form 

agent interacts agents multi agent systems benefit significantly adapting 
performing active learning agent action affects interaction process ways effect expected reward current knowledge held agent effect acquired knowledge rewards expected received 
agent tradeoff wish exploit current knowledge wish explore alternatives improve knowledge better decisions 
goal develop exploration strategies model learning agent handle encounters agents common environment 
show incorporate exploration methods usually reinforcement learning model learning 
demonstrate risk involved exploration exploratory action taken agent yield better model agent carries risk putting agent worse position 
lookahead exploration strategy evaluates actions expected utility expected contribution acquired knowledge risk carry 
holding model agent maintains mixed opponent model belief distribution set models reflects uncertainty opponent strategy 
action evaluated long run contribution expected utility knowledge regarding opponent strategy 
risky actions detected considering expected outcome alternative models opponent behavior 
efficient algorithm returns optimal exploration plan mixed model provide proof correctness analysis complexity 
report experimental results iterated prisoner dilemma domain comparing capabilities different exploration strategies 
experiments demonstrate superiority lookahead exploration exploration methods 
key words model learning exploration multi agent systems 
consider multi agent system mas agents repeatedly interact agents order achieve private goals 
agents may different preferences different negotiation skills different ibm haifa research lab haifa israel 
carmel markovitch levels knowledge 
system agent designed efficiently interact agents may potential partners alternatively competing opponents 
designing effective interaction strategy agent requires prior knowledge agent involved system 
agents completely autonomous necessarily familiar agents strategies 
way deal difficulty endow agent ability adapt agents 
reinforcement learning rl popular technique adaptive agents mas learning interaction strategies agents sen littman wei sen sandholm crites 
rl idea tendency produce action reinforced produces favorable results weakened produces unfavorable results 
rl spends little computation resources example requires large number examples 
previous model approach learning interaction strategies carmel markovitch carmel markovitch reduces number examples needed adaptation investing computational resources deeper analysis interaction experience 
learning agent explicit model agent strategy generate expectations behavior 
stage game agent updates opponent model consistent current sample opponent behavior 
follows optimal response current model 
models rl agents order reduce number experiences needed adaptation 
dyna sutton simultaneously uses experiences adjust agent policy build world model 
opponent model approach computing best response opponent dyna uses world model empirically adapt strategy simulation 
dyna requires order magnitude fewer real experiences model free rl arrive optimal policy consumes times computational resources kaelbling 
model approach gives priority actions highest expected utility 
policy consider effect agent behavior learning process ignores contribution agent activity exploration opponent strategy 
essence action affects interaction process ways effect expected reward current knowledge held agent 
exploration tex exploration strategies model learning effect acquired knowledge rewards expected received due better planning 
agent tradeoff wish exploit current knowledge wish explore alternatives improve knowledge better decisions 
tradeoff known decision theory exploration versus exploitation dilemma 
agents engage exploration exclusion exploitation pay cost experimentation gaining benefits accumulated knowledge 
conversely agents engage exploitation exclusion exploration find trapped sub optimal behavior 
example dilemma multi armed bandit problem berry kaelbling gittins 
machine independently pays amount money time lever pulled fixed unknown distribution 
problem develop strategy gains maximum payoff time choosing lever pull previous experience lever pulling associated payoffs 
problem exploitation corresponds choosing estimated best arm current knowledge 
exploration corresponds choosing arm aim making estimations accurate making better decisions 
exploration vs exploitation problem received significant attention rl research 
methods developed incorporating exploration strategies rl agents sutton thrun kaelbling singh 
undirected methods incorporating randomness decision procedure assigning action positive probability selected performed 
directed methods interaction history evaluate expected contribution action exploration 
goal develop exploration strategies modelbased agents 
show incorporate rl exploration methods model learning 
problem existing exploration methods take consideration risk involved exploration 
exploratory action taken agent tests unfamiliar aspects agent behavior yield better model agent 
action carries risk putting agent worse position 
strategies suggested guide exploration considering effects plan reduction agent uncertainty model world 
dayan claim agent uncertainty drive exploration behavior 
exploration tex carmel markovitch describe method uses certainty equivalence approximation uses mean values state variables variables expressions determine utility agent actions 
optimal experiments design fedorov concerned design experiments expected minimize variances parameterized model maximize confidence model 
applying method sequential decision making estimate addition new training example expected change computed variance 
cohn applies method selecting examples train artificial neural network 
presents probabilistic algorithm applies similar statistical selection procedure decide exploration needed selecting plan 
lookahead exploration strategy drives exploration player uncertainty opponent model considers effect exploration agent position 
evaluates actions expected utility expected contribution acquired knowledge risk carry 
holding model agent maintains mixed opponent model belief distribution set models reflects uncertainty opponent strategy 
action evaluated long run contribution expected utility knowledge regarding opponent strategy expressed posterior probabilities set models 
risky actions detected considering expected outcome alternative models opponent behavior 
rest organized follows section describes basic model interaction studied 
encounter agents described game sequence encounters repeated game 
game agent finite set alternative actions moves choose game outcome determined joint moves performed agents 
study repeated games bounded rational agents restricted computational abilities 
agents strategies restricted regular strategies strategies represented deterministic finite automata rubinstein model bounded rationality repeated games rubinstein 
briefly describe earlier carmel markovitch carmel markovitch model learning repeated games 
section addresses necessity exploration model agents adjusts exploration methods originally developed exploration tex exploration strategies model learning rl model learning section describe exploration strategy deals exploration criteria usually decision theory 
section show experimentally necessity exploration model learning show superiority lookahead exploration uninformed exploration methods 
section 
basic framework formalize notion interacting agents consider framework encounter agents represented player game sequence encounters repeated game 
player game tuple hr finite sets alternative moves players called pure strategies theta 
utility functions define utility joint move players 
example prisoner dilemma pd player game player actions cooperate defect fc dg 
utility functions described payoff matrix shown 
ii 
payoff matrix prisoner dilemma game 
lower left numbers utilities player upper right numbers utilities player ii 
sequence encounters agents described repeated game repetition indefinite number times 
stage game players choose moves theta simultaneously 
history finite sequence joint moves chosen agents current stage game omega gamma gamma ff null history 
set finite histories strategy player function takes history returns action 
set possible strategies player pair strategies defines infinite sequence joint moves path playing game earlier version section appeared carmel markovitch 
exploration tex carmel markovitch jj gamma delta discounted sum function utility function commonly analysis repeated games ds fl fl discount factor payoffs player thought estimation player probability game allowed continue current move 
easy show ds converges fl 
definition strategy opt called best response player respect strategy utility iff opt 
iterated prisoner dilemma ipd example repeated game pd game attracts significant attention game theory literature 
tit tat tft simple known strategy ipd shown successful ipd tournaments axelrod 
begins cooperation imitates opponent action 
best response tft respect ds depends discount parameter fl axelrod opt tft fl fl fl alternate cases best response strategy 
example fl tft best response cooperative strategies 
basic factors affecting behavior agents mas knowledge possess 
assume player aware player set actions common knowledge players preferences private 
framework history game common knowledge player predicts course game differently 
prediction player player strategy player belief opponent strategy called opponent model 
player acquire model opponent strategy 
source information available player history game 
possible source information observed games opponent agents 
note history length provides exploration tex exploration strategies model learning sample tg examples opponent behavior prefix history game 
learning algorithm infers opponent model sample behavior utility function define strategy model learning agent opt definition yields model player mb agent adapts strategy game 
mb agent begins game arbitrary opponent model finds best response opt plays best response null history 
stage game mb agent infers updated opponent model applying learning algorithm current sample opponent behavior 
finds best response current model opt plays best response 
illustrates general architecture line model learning agent repeated games 
best response inference 
general architecture model learning agent repeated games 
efficiency adaptive strategy mainly depends main processes involved finding best response model 
learning process opponent model 
generally computing best response model complicated bounded rational agents agents limited computational resources 
extensive line research game theory problem playing repeated games computationally bounded opponents papadimitriou fortnow whang freund 
see kalai survey bounded rationality repeated games 
typical approach assume exploration tex carmel markovitch opponent strategy member natural class computationally bounded strategies 
natural measure complexity strategy amount computational resources needed implementation 
adopt common restriction opponent uses regular strategies strategies represented deterministic finite automata dfa rubinstein 
complexity regular strategy measured number states minimal automaton implementing strategy 
dfa moore machine defined tuple sigma ffi sigma non empty finite set states sigma machine input alphabet initial state sigma output alphabet 
ffi theta sigma transition function 
sigma set strings sigma ffi extended range theta sigma usual way ffi ffi soe ffi ffi oe 
sigma output function 
ffi output string sigma jm denotes number states strategy player opponent represented dfa sigma sigma history gamma gamma move selected gamma 
example shows dfa implements strategy tft ipd game 
player actions sigma appear inside machine states 
opponent actions sigma change dfa states appear corresponding transitions 
initial state marked arrow 
tft 
dfa implements strategy tft ipd game 
player actions sigma appear inside machine states 
opponent actions sigma change dfa states appear corresponding transitions 
initial state marked arrow 
previous carmel markovitch carmel markovitch model strategy regular opponents repeated games 
strategy includes efficient best response procedure regular opponent model 
procedure papadimitriou tsitsiklis finding optimal policy markov decision process dynamic programming 
assumption opponent strategy modeled dfa occam razor assumption smaller models better prediction power mb agent exploration tex exploration strategies model learning infer smallest dfa consistent sample opponent behavior past order predict opponent actions 
angluin describes algorithm constructs minimal model consulting teacher answer queries 
approach suitable repeated game autonomous egocentric agent 
developed learning algorithm carmel markovitch carmel markovitch named unsupervised constructs regular model guiding principles new model consistent sample 
smaller model better prediction power 
new model similar previous model possible 
algorithm maintains observation table representing learned model 
new counterexample arrives tries extend model minimal fashion 
algorithm constructs new observation table covers data including new counterexample 
table construction requires teacher answering membership queries 
absence teacher agent consults previous model stability principle 
arranges table constructs new model consistent new table new counterexample 

exploration strategies model learning model strategy described previous section gives priority actions highest expected utility current opponent model 
behavior consider effect player action learning process 
quite possible sub optimal action yield long run benefit exploring unknown aspects opponent behavior 
subsections modify model learner take exploration exploitation consideration 

exploration versus exploitation actions taken agent opponent responses examples model learner 
exploratory value example evaluated predicting expected value exploration tex carmel markovitch revised model result processing new example 
consistent example strengthen belief current model 
counterexample lead construction new better model learner 
supervised learning teacher role provide counterexamples guiding learning process 
absence assisting teacher agent search counterexamples 
model learner described section may encounter counterexamples chance interaction process long possesses model equivalent opponent strategy 
approach converge sub optimal behavior 
deterministic nature model strategy decreases likelihood observing counterexamples behavior repeatedly prevents exploration unknown aspects opponent strategy 
left part shows example opponent strategy ipd 
opponent defect long player cooperate forever cooperation player 
best response strategy play yields utility fl gammafl player uses model shown right part best response corresponding model yielding utility gammafl suboptimal fl model player holding model repeatedly defect preventing exploring action observing counterexamples 
wrong model corrected player stuck sub optimal strategy 

example opponent model local minimum 
left opponent strategy 
right opponent model 
numbers parentheses show outcomes pd game players joint actions 
model dictates playing actual best response play 
example demonstrates better play sub optimally order explore opponent behavior 
action low utility current model tried expected strengthen belief current model expectation realized provide counterexample better model 
action high expected exploratory value 
action hand high expected 
tex exploration strategies model learning ity tried times low expected exploratory value 

uninformed exploration strategies years variety exploration strategies proposed reinforcement learning agents thrun sutton moore atkeson kaelbling 
subsection show adopt methods model learning 

undirected strategies undirected strategies incorporating randomness decision procedure learning agent 
key idea associate positive probability state action pair 
agent randomly selects action distribution action neglected forever 
semi uniform exploration singh simple undirected strategy gives priority optimal action current model gives positive probability alternatives 
alternative actions action randomly chosen probability distribution pr ae gamma gamma ff optimal ff 
exploration parameter ff determines weight exploration 
ff increases agent performs sub optimal actions increases chance receiving counterexamples 
distribution semi uniform exploration associates equal probability non optimal action utility consideration 
utility exploration strategy choices distribution determined expected utility actions 
better action expected selected 
boltzmann exploration sutton utility strategy reduces tendency exploration time 
assumption current model improves learning progresses 
boltzmann exploration assigns positive probability possible action expected utility parameter called temperature 
assume agent choose actions expected utilities current knowledge 
boltzmann distribution assigns positive probability possible action pr exploration tex carmel markovitch actions high utility associated higher probability 
temperature decreases stage game learning progresses distribution tight high utility actions exploration tendency agent reduces 
common temperature function delta ff positive constant ff exploration parameter alpha increases rate decay temperature exploration tendency decreases 
singh 
show conditions exploration parameter guarantee convergence semi uniform exploration boltzmann exploration 
model framework expected utility action computed summing instant expected reward executing discounted sum rewards expected playing best response current model 
regular opponent model state inferred learning algorithm history 
opt ds best response strategy model state utility function ds expected utility action defined fl ds element sum stands immediate expected utility playing player opponent 
ds expected discounted sum rewards player followed continuing playing optimally performing ffi new state opponent model action best response opponent model new state 
computed efficiently game automata converges finite cycle discounted sum rewards game path opponent model best response computed procedure 
hm game path automata 
path induces infinite sequence states opponent automaton converges cycle gamma gamma easily simulation game played automata 
player action changes model state symbol ff denote exploration parameter exploration strategies 
exploration tex exploration strategies model learning expected discounted sum rewards player ds hm gamma fl fl gamma fl gamma fl gammak element discounted sum rewards attained path leading cycle 
second element geometric sum infinite run cycle 
equation define probability assigned boltzmann exploration action current model pr 
directed strategies undirected exploration strategies exploration policy agent determined advance depend actual interactions agent involved 
directed strategies attempt explore opponent behavior efficiently statistics learning experience agent 
methods compute exploration bonus possible action incorporate bonus decision procedure agent 
bonus estimates expected contribution action exploration 
statistics proposed determining exploration bonus 
sato action counts higher exploration actions chosen frequently 
sutton suggests recency exploration exploration bonus time passed action taken 
kaelbling suggests interval estimation exploration bonus upper bound confidence interval computed expected utility possible actions 
moore atkeson propose prioritized sweeping exploration strategy states unfamiliar connected fictitious state high utility 
agent encouraged investigate unfamiliar areas environment 
directed strategies incorporated directly model agents traversing history game accumulating statistics 
example history compute age parameter state action pair 
recency exploration exploration tex carmel markovitch implemented computing exploration bonus proportional age 
exploration bonus computed action current model opponent strategy history 
value action computed linear combination expected utility action exploration bonus gamma ff ff undirected methods ff exploration parameter determines ratio exploration exploitation 
thrun deals question combining exploration exploitation 
shows dynamic combination changing tradeoff parameter ff learning advantage static combination 
summarize stage game exploring mb agent updates opponent model 
find best response model opt 
undirected exploration method computes pr action randomly selects action distribution 
directed method computes exploration bonus action history statistics selects action maximal combined utility 

lookahead exploration exploration methods described previous section developed handle problem stuck local minima 
exploring agent willing perform sub optimal actions order acquire better model opponent yielding better utility long run 
quite possible exploratory action yield better model lead agent poor state optimal play yields low utility 
knowledge expensive step dark lead sought treasure deep chasm 
falling chasm improved model world help 
example left part describes strategy ipd called grim 
grim player cooperates long opponent cooperates defection single defection opponent reacts repeatedly defecting 
assume exploring exploration tex exploration strategies model learning agent holds model described right part 
defection lower utility cooperation model non exploring agent cooperate yielding utility gammafl exploring agent eventually try acquire perfect model grim 
exploring action takes agent defection sink opportunity get yielding utility gamma fl gammafl lower utility cooperation fl exploration yielded better knowledge cost acquiring knowledge high 
grim sub model 
left grim strategy ipd 
right current model held agent 
exploratory action grim followed falling defection sink opportunity get 
example clearly demonstrates exploratory behavior requires better mechanism predicting risk involved sub optimal actions 
problems exploration mechanisms described section utility function assumes stationary opponent model expected course game 
assumption rational learning agent continuously modifies opponent model 
order develop risk sensitive exploration strategy need mechanism allow agent take consideration expected revision belief computing expected utility 
mechanism requires method representing agent uncertainty regarding opponent strategy 
rest section describe risk sensitive exploration methodology 
describe mixed strategies representing uncertain opponent models best response procedure strategies 
show learning mechanism acquiring uncertain models 

mixed strategies model agent described section maintains regular model opponent behavior 
regular model incapable representing uncertainty learning agent regarding model prediction 
represent uncertainty apply stronger 
tex carmel markovitch nism modeling opponent strategy game theoretic concept mixed strategy 
definition mixed strategy repeated game pair fs finite set pure strategies called set support sos probability distribution sos called belief distribution 
regular mixed strategy hm qi mixed strategy set support includes automata states 
agent uses mixed strategy randomly drawing strategy set belief distribution performing action 
mixed strategies opponent models different ways 
assume opponent applies mixed strategy randomly chooses strategies set support stage game 
gilboa shows play regular opponents solving best response problem product automaton opponents dfa 
similar method find best response regular mixed strategy 
construct product automaton automata belonging set support output function probabilistic distribution 
second find best response automaton probabilistic product automaton 
problem finding best response probabilistic action automaton paa equivalent best response problem deterministic automaton freund 
interpretation assumes agent modify opponent model game path computing utility suitable belief revision framework 
second interpretation considers opponent strategy models belonging set support belief distribution reflect subjective beliefs player alternative opponent models 
interpretation suitable learning framework studied viewed form bayesian adaptive control 
choosing specific action agent forces opponent respond 
opponent response enables revision belief distribution set opponent models 
opponent response beliefs models expect perform reduced zero 
beliefs models expect output increased 
current mixed strategy pr js probability performing action exploration tex exploration strategies model learning opponent current stage game 
posterior belief computed directly bayes law pr useful propositions implicit bayesian belief revision process confidence agent opponent models reflected prior belief distribution 
agent confident models belief distribution tight models sos posterior distribution resemble prior distribution response 
clear limit 
agent absolute prior confidence associates probability models learn new evidence 
equation prior probabilities zero corresponding post prior probabilities zero 
surprising opponent response bigger impact post prior belief 
terms equation surprising response associated low probability pr 
smaller probability post prior belief diverges prior belief 
define utility expected game path learning strategy mixed model considering modified beliefs path ds pr fl ds ben shows interpretation best response problem regular mixed strategy np complete 
shows problem polynomial size product automaton fixing size set support 
best response procedure described ben impractical product automaton extremely large small set support 
subsection describe alternative approach suitable computational bounded agent 

best response strategy regular mixed model subsection develop algorithm returns response automaton regular mixed model 
level exploration tex carmel markovitch approximation determined advance stage game computational resources available agent 
strategy returned algorithm enables agent efficiently balance exploration exploitation game path expected played 
opponent strategy approximation parameter ffl ffl best response strategy respect guarantees player utility far ffl maximal possible utility kalai 
definition strategy opt ffl called ffl best response player respect strategy utility function iff opt ffl gamma ffl 
max maximal value stage game max umax gammafl maximal utility repeated game algorithm receives regular mixed model hm qi depth parameter determined approximation parameter ffl ln ffl umax ln fl set support includes automata ffi states belief distribution algorithm returns pair ffl best response automaton expected utility game mixed model 
algorithm classifies states mixed strategy certain states models predict output uncertain states models disagree opponent current action 
uncertain states algorithm splits models sos disjoint sets expected output fm jf opponent action reduces number models corresponding set support beliefs models consistent action reduced zero 
pair player action opponent action construct posterior mixed strategy hm 
posterior belief computed equation 
player action determines new state posterior mixed model ffi algorithm calls recursively reduced depth limit 
recursion stops search reaches depth limit set support includes model problem reduced best response problem single automaton 
certain states reveal information player models predict output 
set support ignore models belief zero 
exploration tex exploration strategies model learning belief distribution modified 
states models current states modified player action best response recursively reduced depth limit 
recursion terminates depth algorithm returns constant policy state automaton outputs player actions possible history 
denote automaton returned recursive call pair expected payoff game played action maximizes expected utility state mixed model arg max pr flu ffl best response automaton begins initial state plays opponent action note automaton described provides plan steps player exploring exploiting mixed model 
plan optimizes player behavior considering expected utility expected revealed information alternative game paths 
plan contrast infinite plan returned best response procedure single automaton direct agent behavior opponent play predicted 
describes pseudo code algorithm 
demonstrates ffl best response strategy avoid falling defection sink playing ipd grim 
top part shows models held exploring agent mutual cooperations ipd game 
tft predicts opponent defect defection assumes possible withdrawal cooperation 
grim strategy 
bottom part shows computation ffl best response 
player actions marked solid lines opponent actions marked dashed lines 
save space duplicated subtrees drawn 
action exploration benefit models predict cooperation opponent utility cooperation fl fl gammafl utility defection fl gammafl prefered fl 
note exploration strategy returned algorithm 
algorithm searches deeper returned strategy postpone defection stages 
theorem proves algorithm ffl br returns ffl best response strategy mixed strategy respect ds exploration tex carmel markovitch procedure gamma hm qi delta jm return ha return hm qi uncertain jf fm jf pr ffi ha hm gamma max flu arg max flu dfa begins plays certain models predict action ffi ha gamma hm gamma delta max flu arg max flu dfa begins plays return 
algorithm returns ffl best response automaton regular mixed model 
theorem ffl positive number 
ffl ffl ln ffl ln fl hm qi regular mixed strategy 
algorithm hm qi returns ffl best response automaton hm qi respect ds computation time polynomial ffl number models sos size maximal automaton belonging set support 
proof 
proof induction depth proof trivial ffl max strategy ffl best response 
induction step number models algorithm returns best response single dfa ffl best response 
assume correctness gamma 
strategy exploration tex exploration strategies model learning 
top models belong mixed model mutual cooperations ipd game 
bottom search tree spanned ffl br 
player actions marked solid lines 
opponent actions marked dashed lines 
save space duplicated subtrees drawn 
action preferred fl 
returned algorithm arbitrary strategy 
denote expected utility returned search depth expected utility denote corresponding approximation parameter ffl depth ffl note relation ffl follows fl ffl gamma ffl current state certain models predict opponent perform utility returned algorithm max fl gamma induction max fl gamma gamma ffl gamma max fl gamma gamma ffl gamma ffl similar computation show claim uncertain states 
computing complexity algorithm look tree traverses 
inner node tree update belief distributions computation depends size sos 
leaf nodes computation polynomial size maximal automaton max note maximal number computations bounded number leaves search tree jr jjr poly ffl poly polynomial function 
total complexity poly ffl max 
exploration tex carmel markovitch 
learning mixed model describing opponent model mixed strategy requires learning procedure acquiring alternative models set support sos computational procedure determining belief distribution sos 
subsection describe lookahead learning procedure expands tree possible paths game 

lookahead learning mixed models ideally set support include models consistent history set strategies infinite 
restricting set support concentrate subset models consistent history differ predicting expected opponent action stage game 
learning algorithm model strategy constructing models 
concatenating possible pairs actions history applying learning algorithm new expanded histories acquire models consistent history predict opponent action stage game 
player action stage affect learning algorithm opponent action stage affected previous player actions 
acquire jr different models set support 
definition history learning algorithm set support sos defined set models returned algorithm applied history concatenated joint action sos fm jm ffi acquiring different models set support agent determine belief distribution set 
models consistent history serve opponent model 
models reasonable 
agent choose way computing subjective beliefs 
restriction beliefs non negative sum 
reasonable heuristic belief assignment implemented follows occam razor principle belief probability model decrease size smaller model larger associated belief 
beliefs direct relation coverage models smaller number edges exploration tex exploration strategies model learning model tried agent history smaller agent belief model 
cover jm shows example mixed strategy acquired history mutual defections ipd game 
state model predicts consecutive defections opponent 
states model predicts consecutive cooperations opponent defections 
note belief smaller belief large reasonable predict cooperation consecutive defections 


mixed strategy acquired mutual defection ipd game 
predicts defection opponent 
predicts cooperation 
beliefs computed relation model size model coverage game history 
modeling opponent strategy mixed strategy enables agent quantify uncertainty opponent behavior better decisions uncertainty 
strategies acquired set support inferred predicting different responses opponent history 
testing influence agent behavior opponent behavior search stages deeper testing effects agent actions opponent expected responses 
search stages forward shall look possible expanded histories length ffi concatenating possible pairs actions history applying learning algorithm expanded histories acquire models consistent history predict differently opponent responses player sequences actions 
note player action stage affect learning algorithm exploration tex carmel markovitch opponent action stage affected previous player actions 
acquire jr gamma jr different models set support 
definition history learning algorithm set support sos defined set models returned algorithm applied history concatenated joint actions sos phi jm gamma ffi delta psi summarize exploring opponent strategy mixed model agent searches stages forward collecting different opponent models set support 
infers belief distribution set 
finds ffl best response mixed model performs sequence actions dictated strategy 
doing agent rationally balances exploration exploitation reduces risk involved performing sub optimal actions 
may carry additional computational cost investment may profitable risk involved exploration high 
subsection analyzes complexity algorithm 

example illustrate process learning mixed model devising show algorithm behaves facing grim 
playing grim strategy great challenge exploration method possibility regret performing exploration action 
model strategies invoke exploration incorporating randomness decision procedure fail grim exploration action eventually chosen 
assume current history consists mutual cooperations grim 
key detecting grim observe opponent cooperate player defects cooperates 
opponent action response player previous action lookahead depth needed 
lookahead strategy produce extensions current history 
extensions lead inferring grim 
extensions yield alternative models included set support 
models larger grim 
updating belief distribution models size coverage members set support low associated belief exploration tex exploration strategies model learning leaving models 
set models associated beliefs shown 
procedure detect playing exploratory action improve knowledge opponent response opponent decide models 
detect low expected utility playing due significant belief grim prefer play tft grim 
left models acquired lookahead exploration strategy mutual cooperations grim playing ipd game 
possible models beliefs lower threshold 
algorithm decide cooperate search depth greater fl greater 
ability algorithm avoid falling defection sink grim depends history mutual cooperations long eliminate alternative models leading significant belief grim 
shorter history may sufficient punishment defection increases 

complexity analysis interaction lookahead exploration strategy extends history possible paths length applies learning algorithm extensions accumulates resulting models set support 
computes belief distribution sos calls procedure depth ffl complexity learning process depends lookahead horizon complexity learning algorithm cost exploration tex carmel markovitch history length jhj branching factor lookahead tree jr jjr leaf expanded tree apply learning algorithm extended history 
size resulting sos bounded number leaves jr gamma jr models sos compute belief equation complexity determined jhj 
cost learning process bounded cost jhj previous section showed cost ffl max max size largest automaton sos 
max jhj get bound cost lookahead exploration cost jhj ffl jhj obvious keep computation feasible ffl small 
small depth values dominant cost call learning algorithm computes models leaves lookahead tree 
complexity usually depends length history 
example complexity algorithm experiments described section jhj carmel markovitch 
way reduce cost windowing decrease jhj 
way exploration process incremental 
learning set support interaction maintain set interactions eliminating inconsistent models 
need call leaves associated path contradicts existing models 
complexity uninformed exploration methods described section lower ignore effect actions learning process 
example complexity boltzmann exploration bounded cost jr 
higher cost exploration outweighed benefit obtaining informative examples hopefully leads lower number examples required adaptation 

experimentation exploration strategies repeated games conducted set experiments test capabilities different exploration methods repeated games 
exploration tex exploration strategies model learning 
experimentation methodology experiments described section compare various exploration strategies test characteristics 
experiment consists repeated ipd games length tested agent randomly generated deterministic opponents 
method randomly generating strategies simulates mas agent benefit adapting different opponents 
opponent automata generated choosing random transition function random output function 
randomly generated machines minimized unreachable states dfa minimization algorithm hopcroft ullman 
agent performance measured dependent variables relative utility actual opponent strategy hm known experimenter expected utility model computed infinite discounted sum rewards game opponent automaton best response model exp ds opt hm computation exp done efficiently equation 
absolute utilities relative utility ratio expected utility current model expected utility best possible model actual opponent dfa exp exp hm average cumulative reward cumulative reward game divided number stage games gamma model size number model states 
experimented agents tft stationary strategy tit tat 
strategy learning watkins dayan known rl algorithm works estimating values state action pairs 
exploration tex carmel markovitch repeated games entire history needed representing game state 
framework state visited generalization 
possible alternative fixed window previous moves representing state 
wide window agent sparse table disable convergence learning process practical time 
narrow window cause perceptual aliasing different states appear identical represented state 
learning tried repeated games tft sandholm crites 
agent succeeded learning optimal strategy tft window width needed iterations convergence 
similar results obtained 
experiments agent window width boltzmann exploration temperature function ff learning parameter fi mb strategy non exploring model strategy described section 
mb agent begins random dfa model opponent strategy modifies model fails predict opponent actual play 
computes best response strategy current model acts 
boltzmann exploration undirected mb agent incorporated boltzmann exploration temperature function ff recency exploration directed mb agent recency exploration sutton 
action counts number stages passed time taken ae current opponent model 
computes exploration bonus ae measures utility action equation 
combined exploration combined strategy uses combination directed undirected strategies uses recency exploration strategy decreases exploration parameter time ff lookahead exploration ffl br algorithm depth limit combined lookahead learning algorithm 
addition independent variables exploration parameter ff discount parameter fl opponent automaton size specified default values parameters fl 
exploration tex exploration strategies model learning 
effect exploration parameter agent performance experiment tested effect exploration parameter performance mb agent boltzmann exploration 
exploration parameter ff values 
comparison tested fixed tft strategy learning strategy 
table average cumulative reward relative utility model size attained agents stages pd game 
results averaged trials randomly generated opponents 
standard deviation parentheses 
av 
cumulative reward relative utility model size agent opponent tft agent mb agent ff mb agent ff mb agent ff average cumulative reward relative utility model size attained agents stages pd game shown table adaptive players achieved significantly better results non adaptive tft player 
tft gained points average payoff pd game randomly generated opponents achieved similar results 
second row shows results 
agent fails learn reasonable strategy resources 
increasing width window changing learning parameters help 
agent managed achieve results comparable achieved mb agent stages length game increased 
agent achieve results similar mb agent requires significantly resources 
results show benefit exploration 
exploring model agents far successful non exploring agent ff benefit increased increasing exploration tendency larger ff 
non exploring agent achieves points exploring agent ff achieves points 
close optimal performance measured playing best response strategies opponents 
exploration tex carmel markovitch results indicate non exploring agent infers smaller models inferred exploring agent 
reason non exploring agent trapped local minima sub models smaller utile 
avg 
cumulative reward stage game mb learning boltzmann expl 
alpha alpha alpha agent tft relative utility stage game mb learning boltzmann expl 
alpha alpha alpha 
left average cumulative reward attained different playing strategies randomly generated automata size 
right average relative utility models acquired model agents game 
shows cumulative reward relative utility models attained stages game 
graphs highlight interesting phenomenon 
early stages game exploring agents pay suboptimal decisions induced curiosity 
cost exploration increases exploration parameter ff 
better models generated exploring agents pay stages game exploring strategies outperform non exploring strategy 

effect discount parameter agent performance previous results address question exploration needed interaction 
answer depends greed learning agent 
weight agent gives payoffs resources spend exploration 
weight expressed discount parameter fl 
repeated experiment various values fl mb agent boltzmann exploration 
fl tried values exploration parameter ff recorded agent achieved best performance 
shows best ff fl 
expected fl increases better invest exploration larger ff 
exploration tex exploration strategies model learning optimal alpha gamma optimal expl 
parameter vs ds 
parameter 
best exploration parameter boltzmann strategy various discount parameters 
effect opponent size agent performance avg 
cumulative reward stage game boltzmann exploration diff 
dfa size 
model learning randomly generated automata various sizes 
shows effect increasing complexity opponent strategy learning process 
curves show average results attained mb agent boltzmann exploration ff randomly generated automata sizes 
results show increasing size automata slows rate adaptation 
learning effective models complex automata demands examples learning process converges slower 
exploration tex carmel markovitch 
effect exploration strategy agent performance subsection compare main exploration methods described undirected boltzmann directed recency combined lookahead exploration 
avg 
cumulative reward stage game diff 
exploration strategies dfa size look ahead combined directed 
average cumulative reward different exploration strategies playing ipd randomly generated automata size 
results opponent automata size shown 
experiment performed automata size directed undirected methods showed comparable performance 
size automata increased directed strategies outperformed undirected 
exploration behavior undirected strategy sensitive length history content 
modify exploration behavior playing different opponents 
directed strategy hand utilizes content history modify exploration behavior 
reason outperforms undirected method increasing complexity opponent 
directed method reduce exploration tendency time 
combined strategy enjoys advantages methods outperforms 
lookahead exploration strategy outperforms significantly exploration methods 
advantage carries cost higher computational costs 
due higher costs performed ipd games length 

studies exploration methods model learning multi agent systems 
exploration essential learning agent exploration tex exploration strategies model learning adapting agents exploring better alternatives avoiding stuck sub optimal behavior 
exploration strategies undirected directed developed reinforcement learning paradigm 
section describes ways incorporate methods model learning 
issues considered dealing exploration 
balance exploration exploitation 
second risk involved exploration 
undirected directed exploration methods offer way rational agent consider factors interacting 
lookahead exploration strategy deals issues 
opponent strategy represented mixed model distribution set strategies 
action evaluated long run contribution expected utility knowledge regarding opponent strategy 
risky actions detected considering expected outcome alternative models opponent behavior 
assuming starting certain stage game set support contains target opponent strategy 
algorithm converge probability distribution giving target strategy probability 
history game provides sample opponent behavior history longer alternative models proposed look ahead mechanism larger leading decrease associated belief increase belief target strategy 
main goal algorithm obtain high expected utility 
obviously acquiring model opponent improve strategy leading high utility 
process acquiring model may costly 
quite possible algorithm purposely avoids actions leading better model opponent actions judged risky current knowledge 
model learning interaction strategies repeated games received lot attention game theory literature especially context expected outcome repeated game played learning players jordan jordan fudenberg levine 
simple form model learning game theory fictitious play luce raiffa 
game agents myopic consider current stage game 
furthermore exploration done players 
agents update beliefs opponent choices interaction choose optimal response beliefs 
agent models agents distribution set pure strategies 
exploration tex carmel markovitch shown player zero sum games fictitious play converges nash equilibrium kalai lehrer describe repeated game bayesian players consider game 
starts subjective beliefs individual strategies opponents 
uses beliefs compute optimal strategy 
addition players update subjective beliefs bayesian updating rule game 
show constraints game bayesian players converge nash equilibrium 
deal exploration mentions rational choice model optimal determination experiment difficult 
kalai lehrer deal general case assumptions players strategies 
best response modeling problems intractable general case 
framework assuming regular opponents able offer algorithms solving problems 
gilboa samet deal bounded regular players 
describe model learning strategy repeated games learns best response regular strategy 
procedure enumerates set automata chooses current opponent model automaton sequence consistent current history 
exploration achieved designing sequence actions distinguish current model consistent automaton enumeration 
risk involved exploration bypassed assuming opponents strategies limited strongly connected automata sinks opportunities regret 
automata learning algorithm guaranteed converge best response limit 
learning procedure exhaustive search space automata impractical computational bounded agents 
fortnow whang show learning algorithm adversary regular strategy learning process converge best response exponential number stages 
way dealing complexity problem limiting space strategies available opponent freund ron 
mor 
follow paradigm show limited class regular strategies best response automaton learned efficiently 
methods exploration achieved random walk incorporating randomness decision procedure agent 
learning methods embark long say strategies nash equilibrium best response 
exploration tex exploration strategies model learning exploration sequences course game 
high cost exploration sequences diminishes infinite games mean utility function measures asymptotic performance 
methods may fail discounted sum utility function takes account immediate rewards 
main role opponent model predict behavior 
choosing proper class strategies modeling essential success model strategy 
model class restricted probably fail prediction 
hand general class best response problem learning problem intractable 
ways model behavior 
general background information opponent type important discriminating different possible model classes 
concentrates deterministic finite automata modeling agents strategies 
question modelbased framework extended powerful agents remains open research 
interesting class agents class adaptive agents 
framework described assumes stationary opponent 
extending adaptive opponents difficult deserves investigation 
angluin 
learning regular sets queries counterexamples 
information computation 
robert axelrod 
evolution cooperation 
basic books new york 
ben 
complexity computing best response automaton repeated games mixed strategies 
games economic behavior 
berry bert 
bandit problems sequential allocation experiments 
chapman hall 
david carmel shaul markovitch 
learning models intelligent agents 
proceedings thirteenth national conference artificial intelligence aaai pages portland oregon august 
david carmel shaul markovitch 
exploration adaptation multi agent systems model approach 
proceedings fifteenth international joint conference artificial intelligence ijcai pages nagoya japan august 
david carmel shaul markovitch 
model learning interaction strategies multi agent systems 
journal experimental theoretical artificial intelligence june 
cohn 
neural network exploration optimal experimental design 
cowan tesauro alspector editors advances neural information processing systems pages 
morgan 
peter dayan terrence sejnowski 
exploration dual control 
machine learning 
fedorov 
theory optimal experiments 
new york academic press 
exploration tex carmel markovitch fortnow whang 
optimality domination repeated games bounded players 
proceedings th annual acm symposium theory computing pages 
freund kearns ron rubinfeld schapire linda sellie 
efficient learning typical finite automata random walks 
proceedings th annual acm symposium theory computing pages 
yoav freund michael kearns mansour dana ron robert schapire 
efficient algorithms learning play repeated games computationally bounded adversaries 
proceedings annual symposium foundations computer science pages 
fudenberg levine 
steady state learning nash equilibrium 
econometrica 
gilboa samet 
bounded versus unbounded rationality tyranny weak 
games economic behavior 
gilboa 
complexity computing best response automata repeated games 
journal economic theory 
gittins 
multi armed bandit allocation indices 
new york wiley sons 
hopcroft ullman 
automata theory languages computation 
addison wesley mass 
jordan 
bayesian learning normal form games 
games economic behavior 
jordan 
exponential convergence bayesian learning normal form games 
games economic behavior 
kaelbling littman moore 
learning survey 
journal artificial intelligence research 
leslie kaelbling 
learning embedded systems 
mit press cambridge mass 
ehud kalai ehud lehrer 
rational learning leads nash equilibrium 
econometrica september 
ehud kalai 
bounded rationality strategic complexity repeated games 
neyman editors game theory applications pages 
academic press san diego 
grigoris 
probabilistic exploration planning learning 
proceedings th conference uncertainty artificial intelligence uai pages july 
michael littman 
markov games framework multi agent reinforcement learning 
proceedings eleventh international conference machine learning pages july 
luce raiffa 
games decisions critical survey 
john wiley sons 
moore atkeson 
prioritized sweeping reinforcement learning data time 
machine learning 
mor claudia goldman jeffrey rosenschein 
learn opponent strategy polynomial time 
wei sen editors adaptation learning multi agent systems lecture notes ai 
springer verlag 

optimization rational learning games 
econometrica 
christos papadimitriou tsitsiklis 
complexity markov decision processes 
mathematics operations research 
christos papadimitriou 
players bounded number states 
games economic behavior 
dana ron 
exactly learning automata small cover time 
proceedings seventh annual acm conference computational learning theory pages 
exploration tex exploration strategies model learning rubinstein 
finite automata play repeated prisoner dilemma 
journal economic theory 
sandholm crites 
multiagent reinforcement learning iterated prisoner dilemma 
biosystems journal 
sato abe hiroshi takeda 
learning control finite markov chains explicit trade estimation control 
ieee transactions systems man cybernetics volume september 
sen hale 
learning coordinate sharing information 
proceeding twelfth national conference artificial intelligence aaai pages seattle washington 
singh jaakkola littman 
convergence results single step policy reinforcement learning algorithms 
machine learning journal appear 
richard sutton 
integrated architectures learning planning reacting approximating dynamic programming 
proceedings th international conference machine learning pages san mateo ca 
morgan kaufman 
sebastian thrun 
role exploration learning control 
david white donald editors handbook intelligent control 
press 
watkins dayan 
technical notes learning 
machine learning 
wei sen adaptation learning multi agent systems lecture notes ai 
springer verlag 
address correspondence dr shaul markovitch computer science department technion haifa israel exploration tex 
