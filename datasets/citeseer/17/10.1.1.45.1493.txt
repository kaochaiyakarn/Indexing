boosted mixture experts ensemble learning scheme ran nathan intrator department computer science faculty exact sciences tel aviv university ramat aviv tel aviv israel 
email ning math tau ac il october appear neural computation 
new supervised learning procedure ensemble machines outputs predictors trained different distributions combined dynamic classifier combination model 
procedure may viewed version mixture experts jacobs applied classification variant boosting algorithm schapire 
variant mixture experts appropriate general classification regression problems initializing partition data set different experts manner 
viewed variant boosting algorithm main gain dynamic combination model outputs networks 
results demonstrated synthetic example digit recognition task nist database compared classical ensemble approaches 
keywords boosting mixture experts ensemble machines query committee neural networks handwritten character recognition 
approach great potential improving performance machine learning mixture experts 
improved classification regression performance achieved ensemble networks single net classification regression tasks established hansen salamon wolpert breiman perrone cooper tresp taniguchi raviv intrator 
earlier focused voting schemes majority plurality studies averaging outputs usually superior 
advanced methods combining output different classifiers suggested ho logistic regression perceptron applied output classifiers achieve better corresponding author manuscript results simple averaging furthermore static combination experts replaced dynamic model dcs logistic regression functions chosen input classifier outputs 
generally combining outputs different classifiers approaches selection choosing locally best classifier averaging reducing variance combining outputs fully correlated 
dcs methods combine approaches dynamic weighted average 
stacking framework combining estimators uses non symmetric combination wolpert breiman 
principle levels learners manner basically extension choosing learner cross validation 
avoid training combination level overfit outputs lower level learners input pattern combination learner extracted copies learners trained data excluding pattern 
algorithm applicable multiple learners single learner 
popular form stacking uses levels linear combination model possibly constrained coefficients non negative sum 
methods dynamic linear combination models confidence measure ensemble members regarding pattern 
different measures confidence predictor determining relative contribution expert tresp taniguchi intrator algorithms train individual classifiers independently goal 
specifically different parts training set train individual classifiers drawn distribution 
holds different types classifiers cross validation meir krogh vedelsby different noisy bootstrap copies raviv intrator 
different approach training classifiers different parts training set partitioned manner distributions differ 
approach combines algorithms boosting mixture experts 
remainder proceeds follows 
sections describe boosting adaptive mixture experts algorithms 
algorithms compared section various ways combine suggested section 
discussion basic advanced versions new algorithm section 
empirical evaluation algorithm demonstration problem character recognition task nist database reported section 
theory boosting boosting algorithm improve performance learning machines schapire 
theoretic basis relies proof equivalence strong weak pac learning models 
standard pac probably approximately correct model distribution patterns arbitrary small ffi ffl learner able produce hypothesis underlying concept error rate ffl probability gamma ffi 
weak pac model just requires ffl slightly better random guess class model 
schapire proved equivalence models proposing technique converting weak learning algorithm distribution strong learning algorithm 
termed provably correct technique boosting 
basis technique creating different distributions different sub hypotheses trained 
schapire proved weak error rate ff respective distributions combined resulting ensemble hypothesis error rate ff gamma ff smaller ff 
schapire suggested hierarchical combinations classifiers arbitrarily low error rate achieved 
procedure creating appropriate distributions classifier trained original distribution 
training set second classifier patterns misclassified classifier patterns correctly classified change internal distribution groups 
third classifier designed break ties training set contains patterns classifiers disagree 
real world machine learning tasks necessarily match weak pac model assured performance worst case scenario necessarily higher practically achieved performance simple classifiers 
boosting proved just theoretical technique practical tool enhancing performance 
drucker demonstrated advantage combination independently trained classifiers parallel machine handwritten recognition task drucker 
boosting achieved extremely low error rate problem bottou 
various improvements original boosting algorithm freund suggested simpler structure combining sub hypotheses having tree majority gates sub hypotheses majority gate freund 
adaboost freund schapire advanced algorithm pattern assigned different probability appear training set new learner 
version prefers flat structure combining classifiers hierarchical 
idea mentioned adaboost framework usage weighted combination individual classifiers 
applications adaboost reported breiman schwenk bengio 
breiman regards boosting example algorithm performing adaptive resampling training set suggests algorithms 
applied algorithms decision trees carts various data 
schwenk bengio applied adaboost mlps auto encoder classifiers networks character recognition tasks 
mixture experts learning procedure adaptive mixture local experts jacobs learning procedure achieves improved performance certain problems assigning different subtasks different learners 
basic idea concurrently train expert classifiers regression estimators gating function 
gating function assigns probability experts current input 
training stage value states probability pattern appearing expert training set 
test stage defines relative contribution expert ensemble 
training attempts achieve goals expert find optimal gating function 
gating function train expert achieve maximal performance distribution assigned gating function 
decomposition learning task motivates em version algorithm simultaneous training 
emphasis framework making experts local key improving performance ensembles networks trained similar distributions 
basic level locality achieved targeting expert maximal performance distribution having compensate errors experts 
localization achieved giving higher learning rates better performing expert pattern 
idea extended tree structure termed hierarchical mixture experts hme experts may built lower level experts gating functions jordan jacobs 
em algorithm training hme jordan jacobs 
waterhouse robinson describe gradually grow recursive learning machines waterhouse robinson 
mixture experts procedure achieves superior generalization fast learning learning task corresponds different subtasks distinct portions input space 
mixture experts algorithm differs ensemble algorithms relation combination model basic learners algorithm follows 
ensemble learning algorithms stacking train basic predictors existing predictors try tune combination model 
mixture experts algorithm trains combination model simultaneously basic learners current model determines data sets provided learner training 
comparison algorithms boosting mixture experts developed different types problems different advantages weaknesses 
attempt combine principles address limitations overcome combining elements method 
mixture experts suitable patterns naturally divided simpler homogeneous subsets learning task subsets difficult original 
real world problems may exhibit property furthermore partition exists required gating function may complex initial stage localizing experts hen egg nature 
boosting distributions selected encourage classifier expert patterns previous classifiers err disagree difficult patterns maintaining reasonably performance easier patterns 
main advantages mixture experts localization different experts usage dynamic model combining outputs 
boosting classifier trained patterns localization criterion distributions classifiers level difficulty patterns measured classification performance 
limitation criterion applied unlabeled data disabling dynamic model similar criterion 
combining boosting hme algorithms approaches combining features boosting mixture experts improved boosting adding dynamic model combining outputs classifiers 
feature unique mixture experts 
initialized mixture experts main boosting feature introduce mixture experts framework ability initialize split training set different experts 
precisely patterns output may maximal influence ensemble classification multi level approach mixture experts classifier second third boosting classifier solve problems difficult patterns may easily partitioned subgroups second third boosting classifiers usually handle difficult problem original 
approach incorporates classifier selection classifier combination 
waterhouse cook attempted combine boosting mixture experts approaches waterhouse cook 
report dynamic model combining boost trained networks achieved improved performance vs simple addition 
report mixture experts best bootstrapped boosted networks bootstrapping simple ensemble superior starting random weights 
boosted mixture experts bme attempts design new algorithm applies principles boosting mixture experts high performance classification regression problems 
proposed algorithm may considered boost wise initialized mixture experts variant boosting uses dynamic model combining output classifiers 
main boosting feature want include scheme ability initialize split training set different experts 
split difficulty criterion 
boosting difficulty criterion errors classifier disagreement classifiers 
prefer confidence measure errors difficulty criterion 
advantages size difficult set flexible flexible error oriented criterion error confidence focuses patterns classified correctly avoids focusing 
enables confidence oriented methods 
approach constructing training set third classifier boosting 
method includes important component boosting lacks dynamic model combining outputs classifiers 
requires method assigning unlabeled patterns best fitting classifier weighted combination 
follow mixture experts scheme gating function partitioning data experts training gating function combining outputs 
training separate gating function confidence measure available unlabeled patterns 
basic algorithm algorithm designed arbitrary number experts ensemble constructed gradually adding new expert repartitioning data 
experts neural nets classifier confidence measure appropriate 
confidence measure key achieving improved performance flexibility choosing extends range applications algorithm 
basically algorithm trains learners different possibly overlapping portions data 
confidence measure scalar function basic learner output vector gating function determines probability patterns assigned data set learner training sets may change learners evolve output vectors change 
addition confidence gating may influenced basic reliability learner delta 
reliability may calculated finding optimal weighted average output confidence classifier value changes learners evolve 
output gating function dynamic combination model coefficient assigned predictor pattern 
confidence measure may specifics predictor 
mlp performing classification continuous valued output may function output vector 
confidence increase highest output higher decrease outputs higher 
confidence measures reported machine learning literature may 
tresp taniguchi various confidence measures different predictors combination model tresp taniguchi 
confidence measure variance predictor measured local sensitivity changes weights 
approach mention assuming different predictors trained different data sets american vs european digit data hidden input indicates set pattern belongs 
estimating value may extract confidence information 
tresp taniguchi suggest unified approach methods extreme cases 
intrator base level ensembles similar estimators different experts intrator 
variance base level ensemble indicates confidence 
monotone function applied confidence measure determine soft hard partition data 
confidence measure multi class classification task difference highest outputs 
network estimate confidence margin natural confidence measure provided mlp 
order encourage localization better apply power higher basic confidence measure 
continuous valued output rankings frg confidence th expert gamma algorithm constructing boosted mixture experts consists procedures ffl procedure training single classifier training set variant bp ffl procedure adding classifier existing ensemble assigning training set initial training 
took predefined portion training set experts consisting patterns confident 
ffl refining procedure repartition data current confidence level expert pattern 
done deterministically assign pattern confident expert stochastically probability assigning pattern certain expert proportional confidence stochastic version 
algorithm describes different components fit constructive algorithm creating boosted mixture experts 
boosted mixture experts bme algorithm 
train expert training set 

assign patterns current experts confident initial training set new expert train 

refining stage ffl partition data confidence expert pattern 
ffl train expert training set 

experts required return step 
experts trained may ensemble 
classifier combination model gating function localization experts 
exact choice gating function confidence measure applied function defines specific variant algorithm 
gives algorithm flexibility enables improvement hand crafting confidence measure matching specific problem didn find extra tuning necessary 
flexible nature algorithm appropriate pattern recognition problems 
choice function may depend specific features problem basic learners 
number parameters effective number parameters bme ensemble greater ensemble averages similar classifiers trained data set parallel machine 
parallel machine classifiers effective parameters effective parameters 
bme effectively parameters difference data sets confidence measure constant simple function output vector adds parameters 
upper bound kn parameters actual number closer lower bound 
multi level ensembles model selection averaging emphasized different advantages basic combination schemes classifier selection averaging 
argue applying levels ensembles selection averaging advantages ensemble approach may exploited better way compromise 
studies state certain number classifiers performance ensemble steady 
training set partitioned different experts effect overfit may cause decline performance number experts increases training set size expert small 
suggest ways combining ensemble averaging expert selection improve performance 
approach training sets multi level ensemble output ensemble simple average outputs various extracted previously described 
gain due overcoming effect patterns boundaries regions covered different experts may yield poor performance 
different sets different partitions help overcome 
ability gain multi level approach relies lower level ensemble selection ensemble 
ensembles averaging learners trained similar data just larger ensemble 
extent decision trees may considered ensembles simpler tree predictors 
ensembles combining output trees trained bootstrapped copies data bagging effectively enhance performance breiman 
ensemble methods encourage diverse training sets may gain method data partitions vary 
dynamic combination models ensemble ensemble 
approach appropriate bme algorithm 
approach follows ideas query committee framework seung freund 
approach disagreement ensemble marks interesting patterns located information gaps 
committees may basic experts average expert output disagreement committee members measure expert confidence 
agreement different members committee higher patterns similar committee training set 
follows principle perrone cooper 
suggest order achieve ensemble minimum variance coefficient member inversely proportional variance vs ground truth 
assume different training sets members committee different variances vary different regions input space 
follows internal variance committee estimate error rate intrator results synthetic example demonstrate capabilities algorithm synthetic class dimensional problem provide intuition way works class mixture gaussians 
patterns class drawn probability leftmost gaussian gamma probability lower central gaussian gamma 
patterns second class similarly drawn gaussians centered 
input distribution synthetic task performed tests points drawn equal probability classes 
simple perceptron basic learner 
single learner achieved error rate induced small gaussians 
ensemble composed independent learners combined weighted average achieved similar performance 
multi layer perceptron hidden units error 
bme ensemble absolute value perceptron output confidence score gating function combining confidence function constant coefficient basic learners hard partition training 
bme ensemble achieved error rate task 
learner performs horizontal separation main gaussians classified correctly high confidence patterns small gaussians get low confidence score 
second learner performs vertical separation tends overestimate confidence 
learner assigned higher reliability coefficient output second learner influence confident 
initialization second learner step algorithm drawing subset consisting patterns confidence lower 
subset included patterns belonging small clusters 
small amount patterns main clusters learner took account patterns decision boundary diagonal line upper left lower right 
difficult subset included data points vertical edge main cluster data points horizontally far centers gaussians 
refining stage step algorithm drawing basic reliability coefficients learner recalculated refining cycle data split deterministic manner data point assigned learner product confidence score reliability coefficient higher 
refining stage effect learner able produce better estimation classification main gaussians 
example refining stage contribute 
performed slightly different variant problem bme ensemble error rate refining refining cycles dropped learner initially performed compromise separations perform separation performance improved 
digit recognition results bme algorithm empirically evaluated digits nist database 
preprocessing operations similar described bottou applied digits digits size normalized fit pixel box grey scale centered image 
performed principal component analysis components input classifiers 
examples digits nist database representation principal components 
basic learner basic classifier feed forward neural network trained back propagation algorithm momentum 
network input layer units single hidden layer consisted units 
dimensional output vector extract output digit confidence level 
parallel machine order evaluate unique contribution new algorithm compared standard ensemble parallel machine 
ensemble consisted learners trained independently different starting conditions 
combination model extract ensemble output averaging output vectors different classifiers decision highest output 
increasing number networks enhanced ensemble performance 
bme ensemble tested performance ensembles trained bme algorithm 
initial training set new learners added ensemble constructed choosing training set learners patterns confident took bn set current size ensemble arbitrary constants 
confidence score pattern specific classifier gamma highest output classifier pattern second highest output probabilities normalized sum pattern 
gating function refining step training get probability assigning pattern training set specific classifier confidence score global reliability coefficient 
gating function combination model weight classifier weighted average output vectors 
multi level ensemble performed test multi level ensemble simple average applied output independently trained bme ensembles classifiers 
ensemble combines advantages ensemble choosing appropriate classifier pattern averaging ensemble 
table presents performance ensemble methods wide range ensemble sizes 
results collected different partitions data digit training set digit test set 
number parameters basic mlp inputs outputs hidden units 
naive counting gives free parameters 
effective number order magnitude 
naive number parameters parallel machine bme ensemble nets kn multi level ensemble kn effectively parameters parallel machine bme multi level ensemble kn tried check reported effect due increased number parameters bme ensemble 
bme number parameters may similar parallel machine similar single classifier times larger hidden layer intermediate case 
success rate parallel machine success rate number parallel machine boosted mixture multi level ensemble nets experts nets mean sd mean sd mean sd table performance various ensembles digit recognition task larger net bme 
average large nets success rate multi level ensemble success 
results indicate performance ensemble machine trained bme algorithm combined appropriately significantly better standard ensemble parallel machine 
improvement rate similar achieved boosting drucker 
encouraging improvement rate kept high number classifiers error reduction classifiers 
improved performance large ensemble achieved despite fact classifiers scheme trained small portion data set 
improvement due bme algorithm ensemble performance may larger greater training sets multiplying samples invariant transformations bottou 
results demonstrate potential combining basic schemes ensemble machines multi level approach 
ensemble weighted average classifiers tended select locally best classifier average classifiers 
averaging outputs ensembles yielded improvement results 
results fully contrasted ensembles similar size ensembles classifiers vs classifiers slight advantage 
furthermore studies claim adding classifiers certain number expected improve performance constant incremental improvement encouraging 
study analyzed advanced frameworks ensembles learning machines boosting mixture experts 
discussed advantages weaknesses algorithm reviewed ways principles algorithms may combined achieve improved performance including variants algorithm incorporating elements 
suggested flexible procedure constructing ensemble machine principles algorithms 
essential components training classifiers subsets data significantly different distribution ensemble dynamic classifier selection common training test stages usage confidence measure classifiers gating function mixture experts terminology determines contribution ensemble output 
principles lead outperforming conventional ensemble machines 
flexibility procedure due usage confidence measure may adjusted specifically classification regression problem 
boost wise algorithms appropriate regression problems 
suggest purpose confidence measure committee simple learners basic learner algorithm 
disagreement committee pattern confidence measure 
distinction groups ensemble machines classifier selectors classifier 
state mechanisms provide different advantages ensembles local experts may reduce bias averaging tends reduce variance 
claim multi level approach combining selection averaging capable improving performance ensembles may better compromise selection averaging 
digit recognition task nist database demonstrate advantages bme multi level ensemble achieve significant reduction error rate standard ensembles 
acknowledgments nist drucker handwritten digits database 
bottou cortes denker drucker guyon jackel lecun sackinger simard vapnik 

comparision classifier methods case study handwritten digit recognition 
proceedings int 
conf 
pattern recognition volume pages 
breiman 

bagging predictors 
machine learning 
breiman 

bias variance arcing classifiers 
technical report tr department statistics university california berkeley 
breiman 

stacked regressions 
machine learning 
drucker cortes jackel lecun vapnik 

boosting ensemble methods 
neural computation 
drucker schapire simard 

improving performance neural networks boosting algorithm 
hanson cowan giles editors advances neural information processing systems volume pages 
morgan kaufmann 
freund 

boosting weak learning algorithm majority 
rd annual workshop computational learning theory pages 
freund schapire 

decision theoretic generalization line learning application boosting 
nd european conference computational learning theory 
freund seung shamir tishby 

information prediction query 
hanson cowan giles editors advances neural information processing systems volume pages san mateo ca 
morgan kaufmann 
hansen salamon 

neural network ensembles 
ieee transactions pattern analysis machine intelligence 
ho hull srihari 

decision combination multiple classifier systems 
ieee transactions pattern analysis machine intelligence 
jacobs jordan nowlan hinton 

adaptive mixtures local experts 
neural computation 
jordan jacobs 

hierarchies adaptive experts 
moody hanson lippmann editors advances neural information processing systems volume pages 
morgan kaufmann san mateo ca 
jordan jacobs 

hierarchical mixtures experts em algorithm 
neural computation 
krogh vedelsby 

neural network ensembles cross validation active learning 
tesauro touretzky leen editors advances neural information processing systems volume pages 
mit press 
meir 

bias variance combination square estimators 
tesauro touretzky leen editors advances neural information processing systems volume pages 
mit press 
perrone cooper 

networks disagree ensemble method neural networks 
editor neural networks speech image processing 
chapman hall 
raviv intrator 

bootstrapping noise effective regularization technique 
connection science 
schapire 

strength weak learnability 
machine learning 
schwenk bengio 

adaptive boosting neural networks character recognition 
technical report tr department informatique universite montreal 
seung opper sompolinsky 

query 
proceedings th annual acm workshop computational learning theory pages 
intrator 

classifying seismic signals integrating ensembles neural networks 
amari xu chan king leung editors proceedings iconip hong kong 
progress neural information processing volume pages 
springer 
tresp taniguchi 

combining estimators non constant weighting function 
tesauro touretzky leen editors advances neural information processing systems volume 
mit press 
waterhouse cook 

ensemble methods phoneme classification 
advances neural information processing systems volume 
waterhouse robinson 

constructive algorithms hierarchical mixtures experts 
touretzky mozer hasselmo editors advances neural information processing systems volume 
mit press 
wolpert 

stacked generalization 
neural networks 
