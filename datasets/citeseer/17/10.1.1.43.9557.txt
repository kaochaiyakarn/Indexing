statistical models text segmentation doug beeferman adam berger john lafferty school computer science carnegie mellon university pittsburgh pa usa 
introduces new statistical approach automatically partitioning text coherent segments 
approach technique incrementally builds exponential model extract features correlated presence boundaries labeled training text 
models classes features topicality features adaptive language models novel way detect broad changes topic cue word features detect occurrences specific words may domain specific tend near segment boundaries 
assessment approach quantitative qualitative grounds demonstrates effectiveness different domains wall street journal news articles television broadcast news story transcripts 
quantitative results domains new probabilistically motivated error metric combines precision recall natural flexible way 
metric quantitative assessment relative contributions different feature types comparison decision trees previously proposed text segmentation algorithms 

task address face elementary construct system stream text identifies locations topic changes 
motivated observations seemingly simple problem prove quite difficult automate tool partitioning undifferentiated text speech video coherent regions great benefit number settings 
task ill defined exactly meant topic break 
adopt empirical definition 
disposal collection online data including corpus wall street journal articles separate corpus broadcast news transcripts containing words annotated boundaries regions articles news reports respectively 
input task constructing text segmenter may cast problem machine learning learn place breaks unannotated text observing set labeled examples 
equated topic boundaries document boundaries general documents comprised multiple topics 
clues system learns identify article news story boundary related change topic document approach applied task topic segmentation 
focuses exclusively design algorithms evaluation procedures document segmentation task address closely related task sub topic segmentation 
beeferman berger lafferty general purpose tool partitioning text multimedia coherent regions number immediate practical uses 
fact research inspired problem information retrieval large unpartitioned collection expository text year worth newspaper articles strung user query return collection coherent segments matching query 
lacking tool detecting topic breaks ir application may able locate positions database strong matches user query unable determine surrounding data provide user 
manifest quite unfortunate ways 
example video demand application described christel responding query news event may provide user news clip related event followed preceded part unrelated story commercial 
take feature approach problem detecting segment boundaries 
field machine learning offers number methods decision trees neural networks integrate set features decision procedure 
statistical techniques exponential models selecting combining features predictive model 
rest focus technique application segmentation problem 
section review previous approaches text segmentation problem 
section describe statistical framework model building 
reviewing language modeling basics section describe section candidate features available feature selection algorithm 
section shows examples algorithm action 
algorithm computationally expensive introduce section methods speeding learning process 
section introduce new probabilistically motivated metric evaluating segmenter 
section report series experiments compare different approaches segmenting problem 

previous 
approaches lexical cohesion proposed approaches text segmentation problem rely measure difference word usage sides potential boundary large difference word usage positive indicator boundary small difference negative indicator 
texttiling algorithm introduced hearst simple domain independent technique assigns score topic boundary candidate inter sentence gap cosine similarity measure chunks words appearing left right candidate 
topic boundaries placed locations valleys measure adjusted coincide known paragraph boundaries 
texttiling straightforward implement require extensive training labeled data 
texttiling designed slightly different problem addressed study 
designed identify subtopics single text find breaks consecutive documents statistical models text segmentation comparison texttiling system propose difficult 
furthermore texttiling segments paragraph level doesn assume presence explicit paragraph boundaries 
applications video retrieval may speech recognition transcripts closed captions lack structural markup 
texttiling widely implemented examine behavior task section 
approach introduced reynar graphically motivated segmentation technique called 
technique depends exclusively word repetition find tight regions topic similarity 
focusing strict lexical repetition kozima uses semantic network track cohesiveness document lexical cohesion profile 
system computes lexical cohesiveness words activating node word observing activity value word number iterations spreading activation nodes 
network trained automatically language specific knowledge source dictionary definitions 
kozima generalizes lexical cohesiveness apply window text plots cohesiveness successive text windows document identifying valleys measure segment boundaries 

combining features decision trees litman algorithm identifying topic boundaries uses decision trees combine multiple linguistic features extracted corpora spoken text 
include prosodic features pause duration lexical features presence certain cue phrases near boundary candidates deeper semantic questions noun phrases opposite sides boundary candidate corefer 
litman approach chooses space candidate features similar cue word features employ 
cue phrases drawn empirically selected list words approach allow words fixed vocabulary participate candidate features 

tdt pilot study topic detection tracking tdt pilot study carried darpa sponsored research program assess advance state art technologies managing large amounts information form newswire tv radio broadcasts 
study organized specific tasks segmenting sources stories detecting occurrence new events tracking labeled events data stream 
participants pilot study included research groups carnegie mellon university massachusetts dragon systems 
tdt study led development new complementary approaches segmentation problem approaches quantitatively beeferman berger lafferty evaluated metric described section 
yamron developed approach segmentation treats story instance underlying topic models unbroken text stream unlabeled sequence topics hidden markov model 
approach finding story boundaries equivalent finding topic transitions stories generated unigram language models depend hidden class segment 
ponte developed approach information retrieval methods local context analysis technique uses occurrence data map query text semantically related words phrases 
comparison techniques appears 

feature approach exponential models approach segmentation problem statistical framework feature selection random fields exponential models :10.1.1.43.7345
idea construct model assigns probability sentence probability exists boundary sentence 
probability distribution chosen incrementally building log linear model weighs different features surrounding context 
simplicity assume features binary questions 
illustrate show approach way restricted text consider task partitioning stream multimedia data containing audio text video 
setting features include questions ffl phrase coming appear utterance decoded speech 
ffl scene change video stream frames 
ffl match current image image near segment boundary 
ffl blank video frames nearby 
ffl sharp change audio stream utterance 
idea features natural common machine learning segmentation adopts approach 
approach differs collect incorporate information provided features described 

feature selection split task constructing text segmenter subtasks build model nog random variable corresponding presence absence segment boundary context specify decision procedure values generated applying model input corpus produces list hypothesized locations segment boundaries corpus 
statistical models text segmentation take tasks section defer discussion decision procedure section 
context mean word position text corpus surrounding words side 
context position represented word sequence gammak gammak gamma gamma course large context appear corpus 
experiments order words context surely unique 
corpus annotated segment boundaries think segments assignment label nog position case segment boundary words gamma 
annotation defines empirical distribution labels contexts appear corpus number times context labeled boundary number times 
context unique 
words corpus empirical distribution contexts number times context appears corpus 
practical purposes thought simply choice domain determines distribution labeling true segment boundaries domain determines distribution 
modeling problem construct distribution closely approximates empirical distribution drawn 
training sample learn thought number examples drawn joint distribution 
degree approximates words quality model judged terms kullback leibler divergence nog log hold fixed search model express divergence gamma log constant beeferman berger lafferty term righthand side gamma times log likelihood model respect empirical sample 
minimizing divergence fact maximizing likelihood model assigns data 
challenge build distribution accounts training sample overfitting learning salient features examples 
consider distributions linear exponential family ae deltaf oe delta linear combination binary features real valued feature parameters delta delta delta delta fn normalization constants deltaf deltaf insure family conditional probability distributions 
experiments limit class feature functions depend context combine deltaf deltaf model form additive logistic regression 
shown maximum likelihood solution arg min arg max exists unique 
number iterative scaling algorithms finding incrementally adjust parameters convergence criterion applies 
iterative step parameter updated delta delta log model parameters constant 
formula clear algorithm choosing model features expected values respect model expected values respect data 
employ improved iterative scaling algorithm uses slightly different update procedure achieve faster convergence 
statistical models text segmentation explains construct model set features fn features 
procedure follow greedy algorithm akin growing decision tree 
set candidate features initial exponential model ff denote model modified single feature weight ff ff ffg ffg ffg gain candidate feature relative defined sup ff gamma ff gain largest possible improvement model terms reduction divergence result adding feature adjusting weight 
calculating gain candidate feature candidate yielding largest gain added model model parameters adjusted iterative scaling 
repeating procedure yields exponential model containing informative features 
algorithm feature selection exponential models input collection candidate features training samples fx desired model size output selected features fn maximum likelihood parameters 
set uniform 

candidate feature compute gain gamma 

arg max gamma feature yielding largest gain 

compute arg max obtain weights improved iterative scaling 

set 
exit 

set go step 
constructing model features requires algorithm iterative scaling computations rankings candidate features 
beeferman berger lafferty feature selection necessarily require 
reasonable shortcut select top ranked features step 
take matter efficient construction section provide empirical results illustrate time quality tradeoffs feature selection 

example flipping coins simple example feature selection may help explain subtleties arise segmentation applications sections 
suppose flip biased coin probability heads entropy bits 
depending outcome random variable answer questions manner 
th question probability gamma delta answer coin came heads coin came tails 
probability gamma gamma delta answer question chosen uniformly random 
sample events exhibited 
learn posterior distribution delta delta delta coin bitstrings carried feature selection synthetic events form 
shows induced features 
table annotates feature gain resulted adding feature cross entropy model feature added initial final values parameter fi surprisingly feature chosen bit construction informative bit predicting outcome coin 
events generated happened bit disagreed coin times events coin tosses tail 
feature constrains conditional probability heads 
iterations iterative scaling training model probability 
feature cross entropy model respect true distribution bits cross entropy respect actual events turns bits 
second feature chosen queries nd bit 
weight initially feature log gamma 
consequence effect feature statistical models text segmentation feature gain entropy initial fi final fi bit bit bit bit bit bit 
statistics features induced toy coin flipping example 
gain reduction entropy feature added 
rightmost column lists value parameter fi features added iterative scaling carried 
lower probability heads nd bit set 
idea feature added roughly third time distribution uniform heads tails 
gamma delta bit correlated coin reducing probability heads set improves model 
third feature queries th bit similarly lowers probability heads 
point probability heads pushed far due overlap events nd th bits set 
compensate model chooses query second bit reestablish proper distribution heads 
similar effects appear feature selection results segmentation problem sections 
details feature selection examples action refer papers berger della pietra :10.1.1.43.7345
explains feature selection algorithm generalizes decision trees 
decision trees recursively partition training data features exponential model overlapping scheme prone overfitting 
important distinction drawing inferences text sparse data problem typically severe 
experiments segmentation bear results obtained handful features issues stopping pruning smoothing 
issues certainly relevant approach exponential models primary considerations obtaining useful models 

language models language model conditional distribution gamma identity th word sequence identities previous words 
central approach segmenting different language models short range long range model 
monitoring relative behavior models goes beeferman berger lafferty long way helping model sniff natural breaks text devote section brief review language modeling 

short range model language trigram model approximates language second order markov process making assumption gamma gamma gamma 
typically computes parameters trigram model modified maximum likelihood approach described katz 
purposes study constructed different trigram models 
parameters especially suited financial newswire text tuned approximately words archived wall street journal henceforth wsj articles 
second model trained collection broadcast news transcripts bn containing words 
details text corpora employed study appear 
case corpus served define known word set contained top frequently occurring words corpus 
purposes language modeling words outside considered single unknown word 
assumption english approximated second order markov process highly dubious 
words prior gamma certainly bear identity higher order models impractical number parameters gram model finding resources compute store parameters daunting task 
usually lexical myopia trigram model hindrance see exploit segmentation task 

long range model language fundamental characteristics language viewed stochastic process highly nonstationary written document course spoken conversation topic evolves affecting local statistics word occurrences 
model adapt context offer stationary model trigram model 
example adaptive model period time seeing word boost probabilities set words pitcher error triple outg 
illustrate point provide excerpt bn corpus 
statistical models text segmentation underlined words mark long range language model reasonably expected outperform assign higher probabilities short range model doctors skilled doing procedure recommended patients ask doctors track record 
people high risk stroke include age family history high blood pressure diabetes smokers 
urge evaluated family physicians done simple procedure simply having test symptoms blockage 
means injecting long range awareness language model retaining cache seen grams combined typically linear interpolation static model 
approach maximum entropy methods introduces parameters trigger pairs mutually informative words occurrences certain words context boost probabilities words trigger 
method described beeferman starts trigram model prior default distribution model set features account long range lexical properties language 
features trigger pairs automatically discovered analyzing corpus text mutual information heuristic described rosenfeld 
contains sample trigger pairs bn long range model 
word subset bn corpus served create long range component bn model word subset wsj corpus mined create wsj long range model 
incorporate triggers trigram language model build family conditional exponential models general form exp deltaf tri gamma gamma gamman gamman gamma history words preceding text normalization constant deltaf tri gamma gamma models built feature indicator function testing occurrence trigger pair 
trigger pair corresponds real valued parameter probability boosted factor approximately words occurrence training algorithm estimating parameters improved iterative scaling algorithm train exponential segmentation models described section 
beeferman berger lafferty residues charleston microscopic defense defense tax tax ankara vladimir steve steve education education insurance insurance sauce flower petals 
sample word pairs bn domain 
roughly speaking seeing word empirical probability witnessing corresponding word words 
experiments described 
concrete example vladimir vladimir appeared past words current word 
consulting see bn corpus presence vladimir roughly speaking boost probability factor words 
model calculating exp step process 
start probability tri assigned trigram model 
multiply probability boosting factor corresponding active trigger pair appeared 
divide normalizing term 
manner viewing model imagine assigning probability word history model consults cache containing words appeared left half trigger pair 
general cache consists content words promote probability mate correspondingly demote probability words 
say pair self trigger non self trigger 
section investigate contribution trigger pair type performance segmentation model 
statistical models text segmentation 
features segmenting 
topicality features long range language model uses words previous sentences bias regarding identity word 
accurate model previous sentences document current word 
case trigger model described section cache filled relevant words 
hand document just begun longrange model wrongly conditioning decision information different presumably unrelated document 
soap commercial instance doesn provide helpful context long range model assigning probabilities words news segment commercial 
fact long range model misled irrelevant context 
document myopia trigram model gives advantage long range model 
sufficiently far document long range model adapting growing context outperform trigram model 
monitoring long short range models inclined boundary long range model suddenly shows dip performance lower assigned probability observed words compared short range model 
conversely long range model consistently assigning higher probabilities observed words boundary 
motivates measure topicality define log exp tri gamma gamma exponential model outperforms trigram model 
observing behavior function position word segment discovers average slowly increases zero zero 
gives striking graphical illustration phenomenon 
plots average value function relative position segment words position zero indicating segment 
plot shows segment boundary crossed horizontal axis labeled predictions adaptive model undergo dramatic sudden degradation steadily accurate relevant content words new segment encountered added cache 
observed behavior consistent intuition cache longrange model unhelpful early document new content words bear little common content words previous article 
gradually cache fills words drawn current article long range model gains steam increases 
shows behavior pronounced averaged trials feature selection results indicate topicality predictor boundaries individual events 
beeferman berger lafferty 
average ratio logarithm adaptive language model static trigram model function relative position segment 
data collected reuters stories tdt corpus 
plot position labeled axis corresponds boundary position labeled gamma corresponds words segment 
appears behavior simple ratio highly correlated presence boundaries 
working assumption experiments reported sentence boundaries provided system concerns topicality score assigned entire sentences normalized sentence length geometric mean language model ratios 
determine set candidate features fix collection intervals advance allow candidate topicality features ask ratio lies interval 

cue word features certain domains selected words act cues indicating presence nearby boundary 
bn domain example observed word joins evidence segment boundary occurred 
cue words exist bn domain wsj domains cue words different different domains 
motivates inclusion cue word features 
word language model vocabulary pose questions candidate features ffl word appear sentences 
ffl word appear words 
statistical models text segmentation ffl word appear previous sentences 
ffl word appear previous words 
ffl word appear previous sentences sentences 
ffl word preceding sentence 
posing questions need restrict single definition 
ensure ask right question choose ask 
question parameterized vocabulary word distance ranges experiments 
having concluded discussion approach schematic view steps involved building segmenter approach 
exponential segmentation model boundary context candidate topicality features data trigram language model exponential language model feature induction candidate training corpus test corpus cue word features 
data flow training segmentation model 
sentences large corpus text serve train short range long range statistical language model 
feature selection process models training corpus construct set best features combined exponential model segmentation 

feature selection action section provides peek construction different domains 
inspecting sequence features selected selection algorithm reveals feature selection general applies segmenting task particular 
segmenter built wsj corpus 
second built cnn portion topic detection tracking tdt corpus 
beeferman berger lafferty 
wsj features wsj experiments total candidate features available selection program 
shows selected features 
word topicality score feature shown value feature iterative scaling complete final model 
oe figures indicate features active range sentences 
symbol oe represents feature word appear sentence true contributes factor exponential model 
symbol oe asks topicality statistic interval sentences 
similarly figures represent features active range words 
example represents question word appear words 
assigned weight 
symbol oe said oe said stands feature asks word said appear previous sentences sentences 
contributes factor answer 
features deal sense 
selected feature instance strong hint article may just begun articles wsj corpus concern companies typically full name acme incorporated instance appears article subsequently abbreviated form acme 
appearance word incorporated strong indication new article may begun 
second feature uses topicality statistic 
trigger model performs poorly relative trigram model sentence feature boosts probability segment boundary location factor roughly speaking 
fifth feature concerns presence word hindsight explain feature noting wsj data style introduce person article writing example coyote president acme incorporated article shortened form name coyote cited lack 
presence sentence discounts probability article boundary factor roughly 
sixth feature boosts probability segment previous sentence contained word closed artifact wsj domain articles statement performance stock market day story interest 
similarly article contains invitation visit related story sentence see boosts probability segment boundary large factor 
personal pronoun typically requires antecedent presence words sign current position near article boundary feature discounting factor 
statistical models text segmentation current position oe incorporated oe gamma oe oe says oe oe closed oe said oe federal oe said oe said oe point oe oe named see oe oe may oe 
features induced wsj corpus order selection factors underneath 
length bars indicate active range feature words sentences relative current word 
beeferman berger lafferty 
broadcast news features cnn experiments larger vocabulary roughly candidate features available program 
reveals features chosen algorithm 
word appears features 
occurs data tokenized speech processing cnn network identification information news segments richard tell 
feature asks letter appears previous words probability segment boundary boosted factor 
personal pronoun appears second feature word appears sentences probability segment boundary discounted 
language model topicality statistic appears time sixth feature 
word appearing seventh fifteenth features arises large number news stories relating simpson 
nineteenth feature asks term appears previous words answer raises probability segment boundary factor 
feature sense light sign conventions news reporters anchors follow wolf reporting live white house 
remaining features equally straightforward explanations 

efficient learning shortcoming feature selection approach requires patience constructing models section took hours run mhz sun ultrasparc ii workstation 
describe efficiency measures speeding process model construction 

inducing features batches feature selection summarized algorithm grows model single feature iteration 
natural consider modified algorithm adds features time 
step algorithm select features fg gb maximal gain problem method top ranked features may highly correlated 
way saying top ranked features individually offer significant gain current model gains necessarily additive 
illustrate shows features iteration algorithm word sample bn text 
clearly list exhibits considerable redundancy model boundaries bn data needn include feature list 
call set features non additive gain overlapping 
model containing overlapping features aesthetically unpleasant undesirable statistical models text segmentation current position oe oe friday oe joins qoe oe gamma oe oe oe oe oe oe oe oe oe oe oe clinton oe oe agency 
features induced cnn portion tdt corpus order selection factors underneath 
beeferman berger lafferty feature gain appears words past appears words past appears sentences past appears sentences past appears sentences past appears sentences past appears sentences past appears sentences past appears sentences appears sentences appears sentences past appears sentences past live appears sentences past unknown words appear words sides appears sentences live appears words past appears sentences unknown words appear words sides live appears sentences past appears sentences past live appears sentences past 
highest gain features selected iteration algorithm word sample bn text 
exists high degree redundancy features 
example essentially synonymous encodes slightly different manner fact words harbinger imminent boundary 
practice contains useless features need evaluated order predictions model 
worst case scenario adding features time lead model better maximum likelihood sense adding feature time 
goal induce multiple non overlapping features iteration 
approach take outlined algorithm select batch multiple features sift group non overlapping features batch add smaller group model 
remains unspecified algorithm specifics sifting procedure select set non overlapping features batch features 
view feature binary random variable takes value zero position training corpus 
denote ith position training corpus 
standard statistical test independence lack thereof random variables correlation coefficient statistical models text segmentation algorithm efficient feature selection exponential models input collection candidate features training samples fx batch size desired model size output selected features fn maximum likelihood parameters 
set uniform 

candidate feature compute gain gamma 
select features fg yielding largest gain 

sift set non overlapping features fg 
set 
gamma 
compute arg max obtain weights gamma improved iterative scaling 

set gamma 
exit 

set go step 
ae ar ar ar ar variance bit vector ff xn representing evaluation feature positions training corpus ar covariance bit vectors corresponding features implementation algorithm visits members selected batch features fg gb order discarding ae ffi value ffi set trial error 
lists set features survived sifting process applied features summarizes computation time minutes required algorithm various settings stopping model contained features 
error metric introduced section measure quality model smaller better 
beeferman berger lafferty note leads better performing model paradoxical situation arises adding features batches training algorithm adding exactly features generate model features 

targeted events sampling corpus training data experiments report section marginal probability topic break roughly 
equivalently average document length sentences 
selecting events uniformly random training corpus expect positive examples topic breaks comprise training examples 
instance word sample containing training events positive examples 
statistical technique importance sampling typically performing monte carlo estimation integral suggests optimization bias event sampling include positive examples feature selection algorithm better chance learn features suggest presence topic boundary 
complexity algorithm linear total number events hope reduce training time model compromising resulting model extracting fewer events number positive examples 
shows performance models trained fixed number total events varying proportions negative events 

probabilistic error metric precision recall statistics popular means assessing quality classification algorithms 
segmentation task recall measures fraction actual boundaries automatic segmenter correctly identifies precision measures fraction boundaries identified automatic segmenter actual boundaries 
section point shortcomings precision recall metrics propose new approach gauging quality automatic boundary detector 
conceivable application segmenting tool consistently comes close sentence say preferable places boundaries gain appears words past unknown words appear words sides appears sentence past 
features listed previous sifting procedure eliminated apparently uncorrelated features 
statistical models text segmentation batch size time grow model 
increasing size batches leads faster feature selection algorithm negligible change error rate 
denotes probabilistic error metric introduced section 
numbers reflect training time minutes mhz sun ultrasparc ii word corpus model contained features 
error rate negative event discard rate pi pi pi pi pi pi pi 
performance feature segmentation model words heldout cnn broadcast news data function negative event sampling rate 
point represents performance model trained fixed number total events sentences different ratio negative positive events 

algorithm places boundary sentence away actual boundary time receives lower precision recall scores algorithm hypothesizes boundary position 
natural expect segmenter close count 
suggestion redefine correct mean hypothesized constant sized window units away boundary approach forgiving right nose segmenter close segmenter 
beeferman berger lafferty okay false alarm okay segmentation segmentation hypothesized words 
failure modes segmentation decision procedure 
lower vertical lines represent true segment breaks upper vertical lines represent hypothesized breaks 
fixed width window slid corpus yields outcomes step acceptable hypothesized break true break absent window false negative true break hypothesized break false alarm case hypothesized break true break 
precision recall complementary nature applications 
hypothesizing boundaries raises recall expense precision algorithm designer tweak parameters trade way matches demands application 
compromise precision recall measure weighted combination difficult interpret meaningful performance measure 

occurrence agreement probability error metric introduced formalizes notion segmenter better better able identify sentences belong document 
defined task segmenter identifies boundaries successive sentences corpus text 
natural way reason developing segmentation algorithm optimize likelihood sentences correctly labeled related unrelated 
consider error metric pd simply probability sentences drawn randomly corpus correctly identified belonging document different documents 
formally segmentations ref hyp corpus sentences long pd ref hyp ijn ffi ref phi ffi hyp ffi ref indicator function evaluates corpus indices specified parameters belong document zero 
similarly ffi hyp indices hypothesized belong document zero 
phi operator xnor function operands 
function distance probability distribution set possible distances sentences chosen randomly corpus general depend certain parameters average spacing documents 
statistical models text segmentation plausible distributions uniform length text metric represents probability sentences drawn corpus correctly identified document 
practice yields forgiving metric large corpora randomly drawn pairs sentences far apart naive segmenter identify belonging different documents 
reasonable choice focuses probability mass small distances exponential distribution mean gamma fixed mean document length domain 
error rate distance parameter random 
performance degenerate algorithms segmentation cnn broadcast news data shown functions inter probe distance parameter error metric 
random algorithm places segment boundaries uniformly random generating number documents equal number segments test corpus 
algorithm places boundary sentence 
algorithm places boundaries 
algorithm places boundary mth sentence average segment length 
offered doddington probability mass fixed distance computation metric visualized probes fixed distance apart sweeping corpus 
turns empirically shown analytically strong assumptions window chosen half average segment length words major degenerate algorithms hypothesizing boundaries uniformly randomly nearly low score 
justification error metric quantitative analysis 
beeferman berger lafferty measure probability real number zero 
algorithm scores respect text exactly predicts location boundaries text 
metric captures notion nearness principled way gently penalizing algorithms hypothesize boundaries aren quite right scaling algorithm degradation 
furthermore possible obtain high score cheating degenerate model algorithms 
refer section sample results trivial algorithms score 
number ref hyp probability randomly chosen pair words distance words apart inconsistently classified segmentations pair lies segment pair spans segment boundary 
probability decomposed conditional probabilities called false alarm probabilities error ref hyp ref hyp different ref segments different ref segments ref false alarm ref hyp ref segment ref segment ref false alarm probabilities give detailed look error allowing assessment terms precision recall 

experimental results section presents results applying feature selection algorithm discussed earlier sections segment cnn broadcast news data wall street journal text 
see detailed description data training testing models 
results compared obtained decision tree methods evaluate relative contributions cue word topicality features 
order give reader intuitive feel performance algorithms qualitative results displaying graphs segmentations test data 

quantitative results section divided segmentation task modeling problem constructing model decision problem model assign segment boundaries stream data 
decision procedure employ straightforward hypothesize segment boundary position ff higher scoring position occurs positions ff ffl fixed constants 
minimum separation ffl set sentences cnn data sentences wsj data 
error probability evaluated fixing half average segment length 
model threshold ff determined heldout data requiring probability hypothesized boundary falling window words equal probability boundary falling window 
words threshold set statistical models text segmentation number segments hypothesized approximately equal number segments appearing set 
threshold chosen minimize error rate course application may require trading recall precision vice versa may motivate different choice thresholds 
sets experiments reported broadcast news data 
set experiments carried context tdt pilot study cnn portion corpus specifically prepared study second cnn data broadcast news corpus 
main differences corpora purposes average document length tdt broadcast news data nearly words smaller broadcast news corpus long documents excluded tdt corpus constructed 

comparison trees order compare exponential models common statistical learning algorithm grew decision tree data set candidate feature set identical available exponential model 
adopt cart approach inducing decision trees entropy impurity function evaluate questions 
impurity tree evaluated oe delta sum leaves tree entropy impurity function oe taken oe delta gammap log gamma log change impurity resulting asking question node calculated summing impurities resulting children deltai gamma gamma usual decision tree terminology topicality features ordered variables cue word features categorical variables 
class case various efficient algorithms optimal question selection categorical variables known employ basic decision tree methodology 
entropy loss function splitting criterion decision tree nodes grown words cnn transcripts 
tree smoothed pruned manner 
empirical distribution node estimate smoothed distribution gamma parent parent parent node interpolation weight 
resulting model naturally thought comprised series beeferman berger lafferty hidden markov models path root leaf shared parameters 
parameters trained em algorithm heldout data 
experience smoothing procedure compares favorably cart pruning algorithms reduce size tree improve robustness distributions leaves 
comparisons decision trees exponential models sets experiments segmentation model probability false alarm probability exponential model decision tree interpolated exp dtree models hidden markov model random boundaries boundaries boundaries evenly spaced 
quantitative results segmentation broadcast news portion tdt corpus 
tdt models trained words cnn transcripts furnished segment boundaries tested words tdt corpus 
total features induced 
performance decision tree grown exactly candidate feature set 
tree nodes smoothed em algorithm indicated text 
simple linear interpolation weight decision tree model exponential model resulted error rate window size words equal half average segment size 
decision thresholds exponential decision tree models chosen probability hypothesized boundary falling window words roughly equal probability boundary falling window 
thresholds chosen minimize error rate default segmentation models described text 

tdt experiments quantitative results tdt models collected 
results part extensive study carried research groups course tdt project 
exponential model evaluated result inducing features training corpus words cnn transcriptions evaluated cnn portion tdt corpus 
batch selection event discarding speed training 
error probability resulting model test corpus statistical models text segmentation words probability false alarm probability row 
performance segmentation model function number features induced 
error rate number features induced pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi 
performance segmentation model words heldout cnn broadcast news data function number features induced 
evaluated word tdt test set decision tree model resulted error probability rate false alarm rate row 
explore possibility decision tree exponential models learned different aspects training set interpolated models fixed interpolation weight threshold set heldout data resulting mixture model segmented test data error probability drop performance exponential model row 
rate drops exponential model decision tree mixture result indicates methods learned part complementary aspects segmented training data 
list performance dragon hmm approach run identical test data set row 
particular data set approach exponential models performed better hmm accuracies respectively exponential model performed worse portion tdt corpus comprised reuters newswire articles accuracies 
beeferman berger lafferty segmentation model probability false alarm probability exponential model decision tree interpolated exp dtree models cue word trigger features cue word trigger features cue word features topicality features texttiling 
quantitative results segmentation cnn broadcast news 
models trained words cnn transcripts furnished segment boundaries tested words previously unseen text 
feature exponential model combining topicality features error rate evaluated window size words equal half average segment size 
investigate relative contributions cue word topicality features additional exponential models trained 
allowed topicality features derived adaptive language model self triggers 
second allowed topicality features derived adaptive language model non self triggers 
third cue word features fourth topicality features self non self triggers 
error rate texttiling algorithm comparison caveat approach designed sub topic document segmentation 
parameters algorithm optimized test set give lowest possible error rate 

additional broadcast news experiments additional set models built broadcast news cnn data order evaluate relative contributions different types features 
began exponential model constructed automatically induced cue word topicality features similar constructed tdt experiments 
result longer documents test set correspondingly larger window size words exponential model higher error rate rate false alarm rate row 
performance decision tree grown exactly candidate feature set rate false alarm rate row 
simple linear interpolation weight decision tree model exponential model resulted error rate row 
investigate relative contributions cue word topicality features additional exponential models trained 
allowed topicality features derived adaptive language model self triggers 
second allowed topicality features statistical models text segmentation derived adaptive language model non self triggers 
third cue word features fourth topicality features self non self triggers 
error rates models respectively rows 
see cue word features powerful domain concert topicality features accurate model feature types achieve 
effect topicality features essentially self triggers non self triggers 
comparison texttiling approach blocks version texttiling run parameters optimized test set data row 
paragraph boundaries absent broadcast news data inter sentence gap data potential boundary candidate 
boundaries assigned locations depth scores exceeding threshold optimized test set 
emphasize direct comparison texttiling intended imply approaches designed applicable problems 
cue word phrases suited article story segmentation carrying wsj broadcast news domains texttiling may better suited detecting subtle sub topic shifts expository texts 
segmentation model probability false alarm probability exponential model decision tree interpolated exp dtree models cue word features topicality features texttiling 
quantitative results segmentation wall street journal text 
models trained words wsj text furnished segment boundaries tested words unseen text 
feature exponential model combining cue word topicality features error rate evaluated window size words equal half average segment size 

wall street journal experiments similar set experiments carried wall street journal domain 
experiments models trained roughly words labeled wsj data tested words unseen text 
feature exponential model combining cue word beeferman berger lafferty topicality features error rate rate false alarm rate evaluated window size words equal half average segment size row 
performance decision tree grown exactly candidate feature set error rate rate false alarm rate row 
simple linear interpolation weight decision tree model exponential model resulted small reduction error rate row 
investigate relative contributions cue word topicality features additional exponential models trained 
allowed cue word features second topicality features language models self non self triggers 
error rates models respectively rows 
error rate texttiling algorithm applied domain rate false alarm rate row 
parameters algorithm optimized test set give lowest possible error rate 

qualitative results graphical examples segmentation algorithm heldout data 
shows performance wsj segmenter typical collection data blocks approximately contiguous words 
figures segmentation shown horizontal line vertical line position words article boundary occurred 
decision automatic segmenter shown vertical line horizontal line appropriate position 
fluctuating curve probability assigned exponential model constructed feature selection 
notice domain segments quite short adding special difficulties segmentation problem 
examination errors shows false positives explained inconsistent labeling conventions 
example wsj articles collections brief summaries unrelated news items 
cases topicality features signal change topic occurred boundary hypothesized 
shows specific example 
shows typical performance cnn segmenter blocks roughly words 
examples indicate significant problem broadcast news models presence false negatives little signal probability distribution suggesting sufficiently rich candidate feature set available selection scheme 
hasten add results obtained smoothing pruning kind features induced candidate set 
machine learning methods feature selection exponential models quite robust overfitting features act concert assign probability events linear constraints splitting event space assigning probability relative counts 
expect significantly better results obtained cross validation statistical models text segmentation reagan said won charged role pipeline plan 
administration law enforcement officials attorney general involvement private mideast pipeline proposal may political legal problems 
church homes burned ground unk camp near cape town airport residents accused police aiding unk 
washington unk won professional unk super bowl defeating denver unk san diego 
died james unk junior science adviser white house ex chairman massachusetts institute technology friday cambridge massachusetts 

examples strong false positives wsj test data 
axis labeled relative position test corpus number words 
lower vertical lines indicate segment boundaries truth upper vertical lines indicate boundaries placed algorithm 
fluctuating curve probability segment boundary exponential model 
wsj articles fact collection brief summaries unrelated news items 
segments word positions composites excerpt region shown text 
segmentation algorithm wrongly hypothesizes boundaries region 
stopping techniques allowing richer set features incrementally building compound features phrases 

evaluated new statistical approach segmenting unpartitioned text coherent fragments 
approach uses feature selection collect set informative features model predict boundaries occur text 
rely exclusively simple lexical features including topicality measure number cue word features automatically selected large space candidate features 
proposed new probabilistically motivated error metric assessment segmentation algorithms 
qualitative assessment evaluation algorithm new metric demonstrates effectiveness different domains financial newswire text broadcast news transcripts 
beeferman berger lafferty alex hauptmann michael witbrock discussions segmentation problem context informedia project 
participants topic detection tracking pilot study including james allan jaime carbonell bruce croft george doddington larry gillick jay ponte rich schwartz charles wayne jon yamron yiming yang provided invaluable feedback 
grateful george doddington proposed modification original error metric adopted 
due stanley chen provided useful comments preliminary version anonymous reviewers number useful suggestions identified weaknesses original 
research reported supported part nsf iri darpa award daah ibm cooperative fellowship atr interpreting telecommunications research laboratories 
notes 
preliminary version conference empirical methods natural language processing emnlp providence ri 

beeferman berger lafferty 

model lexical attraction repulsion 
proceedings th annual meeting association computational linguistics 

berger della pietra della pietra 

maximum entropy approach natural language processing 
computational linguistics 

breiman friedman olshen stone 

classification regression trees 
wadsworth belmont 

christel kanade mauldin reddy stevens wactlar 

informedia digital video library 
communications acm 

della pietra della pietra lafferty 

inducing features random fields 
ieee transactions pattern analysis machine intelligence 

topic detection tracking workshop 
unpublished workshop notes 

gelman carlin stern rubin 

bayesian data analysis 
chapman hall london 

hastie tibshirani 

generalized additive models 
hall 

hearst 

multi paragraph segmentation expository text 
proceedings nd annual meeting association computational linguistics 

hearst 

texttiling segmenting text multi paragraph subtopic passages 
computational linguistics 

hirschberg litman 

disambiguation cue phrases 
computational linguistics 

jelinek merialdo roukos strauss 

dynamic language model speech recognition 
proceedings darpa speech natural language workshop pages 

katz 

estimation probabilities sparse data langauge model component speech recognizer 
ieee transactions acoustics speech signal processing assp 

kozima 

text segmentation similarity words 
proceedings st annual meeting association computational linguistics 
statistical models text segmentation 
kuhn de mori 

cache natural language model speech recognition 
ieee transactions pattern analysis machine intelligence 

lafferty 

right tree estimating decision trees em algorithm 
ibm research report 

lau rosenfeld roukos 

adaptive language modeling maximum entropy principle 
proceedings arpa human language technology workshop pages 
morgan kaufman publishers 

litman passonneau 

combining multiple knowledge sources discourse segmentation 
proceedings rd annual meeting association computational linguistics 

litman 

discourse segmentation human automated means 
computational linguistics 

ponte croft 

text segmentation topic 
proceedings european conference research advanced technology digital libraries 

reynar 

automatic method finding topic boundaries 
proceedings nd annual meeting association computational linguistics 

rosenfeld 

maximum entropy approach adaptive statistical language modeling 
computer speech language 

xu croft 

query expansion local global document analysis 
proceedings nineteenth annual acm sigir conference research development information retrieval 

yamron 

topic detection tracking segmentation task 
broadcast news transcription understanding workshop 

yamron carp gillick lowe van 

hidden markov model approach text segmentation event tracking 
proceedings international conference acoustics signal processing icassp 
beeferman berger lafferty 
typical segmentations wsj test data 
axis labeled relative position test corpus number words 
lower vertical lines indicate segment boundaries truth upper vertical lines indicate boundaries placed algorithm 
fluctuating curve probability segment boundary exponential model constructed automatically inducing topicality cue word features 
error probability model window size words 
statistical models text segmentation 
typical segmentations cnn test data 
exponential model segment data result automatically inducing topicality cue word features 
error probability model window size words 
