probabilistic incremental program evolution stochastic search program space sa jurgen schmidhuber idsia corso lugano switzerland mail idsia ch tel fax van someren widmer editors machine learning ecml pages lecture notes artificial intelligence springer verlag berlin heidelberg 

probabilistic incremental program evolution pipe novel technique automatic program synthesis 
combine probability vector coding program instructions schmidhuber incremental learning pbil baluja caruana tree coding programs variants genetic programming gp cramer koza 
pipe uses stochastic selection method successively generating better better programs adaptive probabilistic prototype tree 
crossover operator 
compare pipe koza gp variant function regression problem bit parity problem 
probabilistic incremental program evolution pipe synthesizes programs compute solutions problem 
pipe inspired learning probabilistic programming languages schmidhuber population incremental learning pbil baluja caruana 
pipe evolves tree coded programs koza variant genetic programming koza simply referred gp 
earlier genetic programming see cramer dickmanns 
pipe applied problem gp applied 
pipe learning algorithm pbil different gp 
pipe crossover operator 
uses probabilistic prototype tree combine experiences different programs generate better better programs 
outline 
section describes basic data structures procedures pipe 
section introduces new learning algorithm 
section compares performance pipe gp function regression bit parity 
section concludes 
basic data structures procedures overview 
pipe generates programs underlying probabilistic prototype tree 
program instructions 
programs contain instructions function set ff functions terminal set ft terminals 
instance solve dimensional function approximation task gamma sin cos exp fx rg denotes protected division ir rlog denotes protected logarithm ir rlog log abs rlog input variable represents generic random constant see ephemeral random constant koza 
program representation 
programs encoded ary trees maximal number function arguments 
argument calculated subtree 
trees parsed depth left right 
sample program trees function approximation task shown 
sin exp exp rlog fig 

sample program trees function approximation 
left sin exp 
right exp rlog 
probabilistic prototype tree 
probabilistic prototype tree ppt generally complete ary tree 
node contains random constant variable probability vector denotes node depth root node defines node horizontal position tree nodes equal depth read left right 
probability vectors components 
component denotes probability choosing instruction maintain 
program generation 
generate program prog ppt instruction selected probability accessed node ppt 
instruction denoted nodes accessed depth way starting root node traversing ppt left right 
selected subtree created argument instance called replaces prog 
exceeds threshold tr randomly generated 
illustrates relation prototype tree possible program tree 
denote result applying prog data prog 
tree shaping 
reduce memory requirements incrementally grow prune prototype tree 
growing 
initially ppt contains root node 
nodes created demand selected subtree argument missing 
shows prototype tree extraction programs 
rlog exp sin cos exp fig 

left example node instruction probability vector random constant middle probabilistic prototype tree ppt details node right possible extracted program prog 
time creation instruction dashed part prog exist 
instantiated vd probability shown exceeds random constant threshold tr sin nd program st program prototype tree fig 

left prototype tree 
right generated programs 
highlighted parts prototype tree created construction second program 
pruning 
prune subtrees ppt attached nodes contain probability vector component threshold tp case functions prune subtrees required function arguments see 
pruning tends discard old probability distributions irrelevant 
learning overview 
pipe attempts find better better programs program quality measured scalar real valued fitness value 
pipe guides search promising search space areas incrementally building previous solutions 
generates successive program populations underlying probabilistic prototype tree ppt stores tree knowledge gained evaluating programs 
cos sin exp rlog sin cos rlog exp prototype tree fig 

dashed parts prototype tree pruned probabilities adjacent nodes exceed threshold value tp contain high probabilities terminal left single function argument right 
ppt initialization 
ppt node requires initial random constant initial probability instruction pick uniformly random interval 
initialize instruction probabilities constant probability selecting instruction gamma selecting instruction initialized follows gammap learning framework 
combine forms learning learning elitist learning el 
pipe main learning algorithm 
el purpose best program far attractor 
execute 

repeat probability el el el user defined constant 
generation learning 
pipe learns successive generations comprising distinct phases creation program population population evaluation learning population mutation prototype tree prototype tree pruning 
creation program population 
population programs prog ps ps population size generated prototype tree ppt described section 
ppt grown demand 
population evaluation 
program prog current population evaluated assigned non negative fitness value prog 
prog prog program prog said embody better solution program prog programs equal fitness prefer shorter ones occam razor measured number nodes 
define index best program current generation preserve best program far prog el elitist 
learning population 
prototype tree probabilities modified probability prog creating prog increases 
call procedure adapt ppt prog 
experiments indicate beneficial increase prog regardless prog length 
compute prog look ppt nodes generate prog prog nd generate prog prog prog denotes instruction program prog node position calculate target probability target prog target prog gamma prog delta lr delta prog el prog lr constant learning rate user defined constant 
fraction prog el prog implements fitness dependent learning fdl 
learn programs higher quality lower fitness programs lower quality higher fitness 
constant determines degree fdl influence 
fit prog el 
fit prog el pipe small population sizes generations containing low quality individuals affect ppt 
learning program generation possible 
target single node probabilities prog increased iteratively parallel repeat prog target prog prog lr delta lr delta gamma prog lr constant influencing number iterations 
lr turned compromise precision speed 
random constant prog copied appropriate node ppt prog 
mutation prototype tree 
mutation pipe major exploration mechanism 
mutation ppt probabilities guided current best solution prog want explore area prog probabilities stored nodes accessed generate program prog mutated probability pmp defined pmp pm delta pm free parameter setting mutation probability denotes number nodes program prog justification square root empirical larger programs improve faster higher mutation rate 
selected probability vector components mutated follows delta gamma mutation rate free parameter 
mutated vectors renormalized 
see assignment small probabilities close subject stronger mutations high probabilities 
mutations tend little effect generation 
prototype tree pruning 
generation prune prototype tree described section 
elitist learning 
elitist learning el adapt ppt elitist program prog el calling adapt ppt prog el prune ppt 
mutate probabilities ppt create evaluate population making el computationally cheap 
focuses search previously discovered promising parts search space 
el particularly useful small population sizes 
works efficiently case noise free problems 
termination criterion 
pipe run fixed number program evaluations pe time constraint solution fitness better quality constraint 
experimental comparison gp section compare pipe method koza genetic programming variant gp problems 
investigate continuous function regression problem 
non trivial function prevent algorithm simply guessing 
compare algorithms bit parity problem discrete task allows distinct fitness values 
limited number fitness values permits test pipe built occam razor 
algorithms problems set gamma sin cos exp fx rg see section 
gp denotes set constants ephemeral random constant see koza details 
function regression function approximated delta gammax delta cos delta sin delta sin delta cos gamma see 
training testing data set tr te samples equidistant points interval 
tr learning te test generalization 
fitness value program prog prog tr jf gamma prog generalization performance gen prog te jf gamma prog set pe algorithms tried parameter settings pipe gp 
parameters pipe el ps lr pm tr tp 
parameters gp population size crossover rate maximal tree depth initial depth half half population initialization selection see koza 
results 
independent test runs conducted algorithm 
obtain idea generalization performance relates function approximation quality consider 
note generalization performance gen prog obtained constant function 
fig 

delta gammax delta cos delta sin delta sin delta cos gamma gen prog gen prog te fig 

test data set te approximations gen prog 
pipe gp generalization performances summarized 
generalization performances plotted numbers programs gen prog performance test data set generalization pipe performance test data set generalization gp fig 

cumulative histograms pipe left gp right generalization performance function regression problem 
box indicates pipe left gp right generalization performance corresponding 
top pipe runs led better results gp runs 
hand worst pipe runs led worse results gp runs 
pipe best solutions better gp variance higher 
bit parity problem problem boolean values represented integers true false 
bit parity function boolean arguments returns number non zero arguments odd 
fitness program number patterns classifies incorrectly 
best worst fitness classifying patterns correctly 
fit boolean nature problem real valued output program mapped negative 
set pe algorithms 
coarse parameter search parameter settings 
pipe el ps lr pm tr tp 
gp population size crossover rate maximal tree depth initial depth half half population initialization selection see koza 
best gp parameters turned function approximation task tried combinations 
pipe robust respect parameter settings 
results 
independent test runs conducted algorithm 
shortest pipe program embodying perfect solution program evaluations 
nodes computes rlog rlog cos cos rlog table summarizes results 
bit parity program evaluations nodes algorithm solved min med max min med max pipe gp table 
summary bit parity results 
task pipe performed better gp 
solved problem reliably faster median fewer program evaluations 
pipe complex solutions containing fewer nodes 
introduced pipe novel method automatic program synthesis 
successive generations programs generated probabilistic prototype tree ppt 
ppt guides search updated search results 
pipe performs better gp bit parity problem 
best solutions function regression problem better gp best solutions 
gp results lower variance 
baluja caruana baluja caruana 

removing genetics standard genetic algorithm 
prieditis russell editors machine learning proceedings twelfth international conference pages 
morgan kaufmann publishers san francisco ca 
cramer cramer 

representation adaptive generation simple sequential programs 
grefenstette editor proceedings international conference genetic algorithms applications hillsdale nj 
lawrence erlbaum associates 
dickmanns dickmanns schmidhuber 

der eine implementierung prolog 
institut fur informatik lehrstuhl prof technische universitat munchen 
koza koza 

genetic programming programming computers means natural selection 
mit press 
schmidhuber schmidhuber 

general method incremental multi agent learning unrestricted environments 
yao editor evolutionary computation theory applications 
scientific publ 
singapore 
press 
article processed macro package llncs style 
