information geometry maximum likelihood criteria william byrne byrne jhu edu center language speech processing department electrical computer engineering johns hopkins university barton hall charles st baltimore md presents brief comparison information geometries describe em algorithm maximum likelihood estimation incomplete data 
alternating minimization framework geometry developed csisz ar followed em algorithm amari 
comparison algorithms discussion variation likelihood criterion 
em algorithm usually formulated improve marginal likelihood criterion described section 
closely related algorithms exist intended maximize different likelihood criteria 
best criterion example leads viterbi training algorithm hidden markov modeling 
criterion information geometric description results minor modification marginal likelihood formulation 
techniques discussed rigorous detail level intended allow comparison works cited bibliography consulted complete correct presentations methods discussed 
likelihood criteria incomplete data problems estimation task choose distribution family best describe training set fy consists observations random process assuming observations independent maximum likelihood estimation problem find distribution maximizes marginal likelihood criterion variable sufficient statistic model termed incomplete data maximum likelihood problem 
formulation problem joint distribution random processes available single observation maximum likelihood training objective max available necessary choose likelihood criterion assign likelihood function possible values assume 
marginal likelihood leads training objective max em algorithm 
restrictions placed values associated observation em algorithm proceeds step eq log jy new model step arg max yields 
simplicity assumed needed maxima attained 
conference information sciences systems princeton new jersey alternating minimization known em algorithm formulated terms alternating minimization algorithm csisz ar 
formulation follows 
empirical distribution defined observed random variable places mass observed data training set jt ffi distribution define family distributions terms linear constraint fp note xjy alternating minimization algorithm proceeds projecting back forth families information divergence arg min kq arg min kq equation referred projection 
denoted thought divergence distance simple case take finite number values easily log sum inequality see example 
case xjy jjq easily shown jjq gammah gamma jt log follows decreasing increases log 
termed family desired distributions approaching family terms divergence improves likelihood training data 
inserting equation equation yields arg max log comparing result equations shown projection lead operations em algorithm 
repeated application equations yields sequence models fq decreases 
shown graphically 
alternating minimization em algorithm information geometric em algorithm developed amari geometry :10.1.1.37.8662
em algorithm discussed conditions equivalence em algorithm :10.1.1.37.8662
summary comparison alternating minimization algorithm 
set model distributions taken subset family exponential distributions parameterized dimensional parameter fs deltag distributions parameterized expectation parameters suppose parameters models restricted function parameter 
function allowed vary freely possible values ae fq deltag observed data introduced problem definition observed data conference information sciences systems princeton new jersey submanifold 
convenience training data single observation observed data submanifold defined fj varies range xg coordinates define submanifold different set models em algorithm defined terms straight line joins distributions parameters gamma straight line joins distributions parameters gamma model parameters projection 
distribution specified straight line orthogonal projection fixed new model parameters specified finding straight line orthogonal shown repeated application yields sequence models approaches local minimum jjq graphically amari projection projection em algorithm lead directly improvement marginal likelihood :10.1.1.37.8662
theorem em em algorithms equivalent eq jy linear implies improvement marginal likelihood criterion assured condition met :10.1.1.37.8662
relative merits discussed :10.1.1.37.8662
significant modeling assumptions manifolds flat distributions belong straight line joining belong similarly distributions belong straight line joining belong manifolds properties termed flat flat respectively 
summary comparison alternating minimization procedure described section properties ffl desired distributions defined likelihood criterion observed training data ffl projection directly yields marginal likelihood training data easily problems log sum inequality ffl directly increases marginal likelihood ffl leads operations em algorithm em algorithm summarized section properties ffl observed data submanifold defined observed training data minimal sufficient statistics model family ffl model observed data submanifolds flat ffl equivalence em algorithm improvement marginal likelihood criterion depends model conference information sciences systems princeton new jersey variations likelihood criterion previous section discussed training algorithms marginal likelihood criterion 
widely likelihood criterion missing data problems easily described max assumes single best value associated called best criterion 
case step equation replaced log arg max step unchanged 
new model arg max log leads max max hmm algorithm known viterbi style training segmental kmeans algorithm 
names appropriate 
viterbi algorithm find needed value step implement equation resembles means clustering 
alternating minimization description best criterion shown viterbi training algorithm formulation alternating minimization framework 
set desired distributions defined equation modified adding size constraint support xjy fp jfx xjy xjy gj modified set desired distributions necessary find projection model set 
satisfies arg min kq arg maxq jjq gammah gamma jt max case marginal likelihood yields likelihood training data criterion defines training algorithm 
finding equation yields max max 
special case adding general constraint dn fp jfx xjy xjy gj ng leads best training algorithms extends formulation commonly likelihood criteria training hmms 
boltzmann machine learning best criterion boltzmann machines binary valued stochastic neural networks may visible hidden units 
information geometric discussions boltzmann machine learning 
network state denoted 
vector visible units hidden units 
network behavior free running network achieves stationary distribution bz gammaz parameterized matrix network connectivities network hidden units maximum likelihood estimation parameters completely specified training data values defined pairs network units 
empirical observations desired network behavior algorithm chosen implement maximum likelihood solution 
example gradient methods conference information sciences systems princeton new jersey description iterative proportional fitting solution 
network hidden units behavior units determined training data 
necessary estimate values specified training data 
training marginal likelihood criterion unspecified components estimated clamping thought stochastic approximation step 
produces estimates eb zjy observing behavior boltzmann machine 
shown bz estimates algorithm chosen implement step 
possible describe boltzmann machine learning algorithm best criterion 
hmm training criterion changed 
values determined training data projection bz arg hmm training dynamic programming algorithms general available perform computation 
difficult find exactly best value hidden vector visible vector approximate value running boltzmann machine deterministically hopfield network 
training vector approach find vector hidden units local maximum bz 
taken vectors form temporary training set fz bz approximated distribution compute estimate jt resulting training procedure similar usual boltzmann machine learning clamping replaced deterministic dynamics hopfield network 
discussion boltzmann machine training procedure may may practical value 
show ease likelihood criterion developed model architecture extended different model architecture models share information geometric description 
best criterion mentioned context information geometries provide additional point comparison geometries 
marginal likelihood best criterion appear straightforward formulation em algorithm 
difficulty formulating observed data submanifold incorporates likelihood criterion flat subset likelihood criteria formulated information settings 
scope penalized likelihood criteria intended prevent overtraining avoid local maxima em algorithm interesting geometric descriptions 
dempster laird rubin 
maximum likelihood incomplete data 
journal royal statistical society 
csisz ar 
information geometry alternating minimization procedures 
statistics decisions supplementary issue number pages 
csisz ar 

information theory statistics class notes dept electrical engineering university maryland spring 
conference information sciences systems princeton new jersey csisz ar 
divergence geometry probability distributions minimization problems 
annals probability 
csisz ar korner 
information theory coding theorems discrete memoryless systems 
academic press orlando 
byrne 
alternating minimization boltzmann machine learning 
transactions neural networks 
:10.1.1.37.8662
amari 
information geometry em em algorithms neural networks 
neural networks 

amari 
information geometry em em algorithms neural networks 
technical report dept mathematical engineering information physics university tokyo ku tokyo japan 

amari 
differential geometrical methods statistics 
springer verlag new york 

amari 
information geometry boltzmann machines 
ieee transactions neural networks pages march 

juang rabiner 
segmental means algorithm estimating parameters hidden markov models 
ieee transactions acoustics speech signal processing september 
byrne 
encoding representing phonemic sequences nonlinear networks 
phd thesis university maryland college park 
rabiner 
relations modeling approaches information sources 
ieee transactions information theory march 
ackley sejnowski 
learning algorithm boltzmann machines 
cognitive science 
anderson titterington 
boltzmann machines 
kelly editor probability statistics optimization 
wiley 
hinton sejnowski ackley 
boltzmann machines constraint satisfaction networks learn 
technical report cmu cs carnegie mellon university pittsburgh pa may 
hopfield 
neural networks physical systems emergent collective computational abilities 
proceedings national 
sciences usa 
byrne 
generalization maximum likelihood small data sets 
proceedings ieee sp workshop neural networks signal processing 
ueda nakano 
deterministic annealing variant em algorithm 
advances neural information processings systems 
miller snyder 
role likelihood entropy incomplete data problems applications estimating intensities toeplitz constrained covariances 
proceedings july 
conference information sciences systems princeton new jersey 
