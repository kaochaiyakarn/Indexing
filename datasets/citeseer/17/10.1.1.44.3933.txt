implementation interior point methods large scale linear programming andersen xu technical report hec geneva section management studies university geneva bd carl vogt ch geneva switzerland january past years interior point methods ipm linear programming gained extraordinary interest alternative sparse simplex methods 
initiated fruitful competition types algorithms lead efficient implementations sides 
significant difference interior point simplex methods reflected theoretical background practical implementation 
give overview important characteristics advanced implementations interior point methods 
infeasible primal dual algorithm widely considered efficient general purpose ipm 
discussion includes various algorithmic enhancements basic algorithm 
shortcoming traditional infeasible primal dual algorithm detect possible primal dual infeasibility linear program 
discuss problem solved homogeneous self dual model 
ipms practical efficiency highly dependent linear algebra 
discuss subject great detail 
cover related topics preprocessing obtaining optimal basic solution interior point solution 
research second author supported fonds national de la recherche scientifique suisse 
research third author supported hungarian research 
appear chapter interior point methods mathematical programming terlaky ed kluwer academic publishers 
department management university dk denmark mail eda ou dk 
hec geneva section management studies university geneva bd carl vogt ch geneva switzerland mail unige ch leave systems research institute polish academy sciences warsaw poland 
department operations research decision support systems computer automation research institute hungarian academy sciences budapest hungary mail hu institute systems science academia sinica beijing china 
early late time dantzig famous simplex method researchers including von neumann hoffman frisch proposed interior point algorithms traverse interior feasible region attempt avoid combinatorial complexities vertex algorithms 
expensive computational steps require possibility numerical instability calculations discouraging experimental results led consensus view algorithms competitive simplex method practice 
fact difficult find serious discussion approach simplex method karmarkar novel interior point method claimed able solve large scale linear programs times faster simplex method 
karmarkar announcement led explosion interest interior point methods ipms researchers practitioners 
soon karmarkar publication gill showed formal relationship new interior point method classical logarithmic barrier method 
barrier method usually attributed frisch formally studied mccormick context nonlinear optimization 
research concentrated common theoretical foundations linear nonlinear programming 
fundamental theme creation continuously parametrized families approximate solutions asymptotically converge exact solution 
basic iteration path algorithm consists moving point certain neighborhood path called target preserves property lying neighborhood path near exact solution 
past years efficient implementations interior point methods developed 
lustig shanno particularly important contribution area code ob 
implementations simplex method improved lot years extensive numerical tests cf 
indicated conclusively efficient robust implementation interior point method solve large scale lp problems substantially faster state art simplex code 
efficient interior point method today infeasible primal dual algorithm 
chapter discuss techniques efficient robust implementation primal dual method 
chapter focuses implementation techniques closely related theoretical issues addressed 
relevant issues interior point method implementations illustrated computational results 
small set test problems public domain collections lps case chosen illustrate typical behavior implementational techniques 
reader see excessive numerical results demonstrate technique works practice consult appropriate 
presentation starts section description infeasible primal dual method 
issues theory implementation method understood 
remain open detecting infeasibility problem choice centered starting point 
solution problems mathematically elegant implementable practice comes homogeneous self dual linear feasibility model 
address model section 
practical success ipm implementation depends efficiency reliability linear algebra kernel 
focus issues section 
major single iteration ipm consists solving set linear equations called newton equation system 
system reduces ipms problem equivalent orthogonal projection vector null space scaled linear operator 
diagonal scaling matrix depends variant method changes considerably subsequent ipm iterations 
general purpose ipm codes direct approach solve newton equation system 
alternative iterative methods due difficulties choosing preconditioner 
competitive direct approaches solving newton equations augmented system approach normal equations approach 
requires factorization symmetric indefinite matrix works smaller positive definite matrix 
section discuss approaches detail analyse advantages point difficulties arise implementation 
unified framework covers previously techniques 
briefly discuss hardware dependencies implementations 
issues related efficient implementation ipms addressed section 
discuss important role preprocessing linear program recall related problems impact presence free variables dense columns lp problem 
mentioned direct approach solve system newton equations ipm iteration 
iteration matrix factorization computed requires nontrivial amount 
contrast step usually significantly cheaper 
obvious idea known different applications newton method reuse factorization iterations equivalently repeat guess better iterate 
call approach higher order method 
higher order method incorporated dual affine scaling method system 
efficient high order method proposed mehrotra second order strategy incorporated primal dual type implementations 
shown mehrotra improvement orders higher limited 
proposed new way exploit high order information primal dual algorithm showed considerable improvements solving large scale problems 
shall address higher order methods section 
important issue terminate ipm 
contrary simplex algorithm ipm generates exact optimal solution generates infinite sequence converging optimal solution 
necessary able terminate ipm finite number iterations report exact optimal solution 
problem solved ye finite termination scheme see 
closely related problem generate optimal basic solution optimal interior point solution 
general lp problem multiple optimal solutions ipm produce optimal solution basic solution 
megiddo shown exact primal dual optimal solution known optimal basic solution produced strongly polynomial time simplified simplex algorithm 
section discuss method combines ye finite termination scheme megiddo method produce optimal basic solution 
interior point methods reliable optimization tools 
reason inertia operations research community keeps simplex method applications undoubtedly benefit new interior point technology 
particularly important applications require solution large linear programs tens hundreds constraints variables 
chapter brief guide interior point software available nowadays 
shall list section commercial experimental research lp codes interior point methods 
exist efficient programs public domain form source code competitive terms speed best commercial products 
past years brought enormous development theory implementations ipms issues remain open 
shall address section giving section 
primal dual algorithm computationally attractive ipm infeasible primal dual algorithm 
implemented commercial software packages 
start presenting algorithm 
algorithm generates iterates positive interior respect inequality constraints necessarily satisfy equality constraints 
name infeasible interior point primal dual method 
sake brevity call primal dual algorithm 
theoretical results method due megiddo proposed apply logarithmic barrier method primal dual problems time 
independently kojima mizuno developed theoretical background method gave complexity results 
implementations showed great promise encouraged research field 
implementations continuously improved led development highly efficient lp codes 
today computational practice implementation follows 
practical implementations primal dual algorithm differ lot theoretical algorithms polynomial complexity give importance worst case analysis 
gap theory practice closed kojima megiddo mizuno show primal dual algorithm safe guards theoretical properties 
fundamentals consider primal linear programming problem minimize subject ax thetan dual maximize gamma subject gamma lp problem said feasible constraints consistent called unbounded sequence feasible points objective value goes infinity 
lp problem said solution feasible bounded 
abuse mathematics derive primal dual algorithm ffl replace nonnegativity constraints variables logarithmic barrier penalty terms ffl move equality constraints objective lagrange transformation obtain unconstrained optimization problem write order optimality conditions ffl apply newton method solve order optimality conditions solve system nonlinear equations 
exercise 
replacing nonnegativity constraints logarithmic penalty terms gives logarithmic barrier function gamma ln gamma ln write order optimality conditions ax gamma swe diagonal matrices elements respectively vector ones barrier parameter gamma observe equations linear force primal dual feasibility solution 
equations nonlinear depend barrier parameter 
complementarity conditions feasibility constraints provides optimality solutions 
seen identical karush kuhn tucker kkt system lp problem complementarity conditions perturbed 
called perturbed kkt conditions 
nonnegative solution called analytic center 
clearly depends value barrier parameter 
set solutions defines trajectory centers primal dual problem respectively called central path 
quantity measures error complementarity called complementarity gap 
note feasible point value reduces usual duality gap 
center example vanishes optimal solution 
iteration primal dual algorithm step newton method applied order optimality conditions updated usually decreased 
algorithm terminates infeasibility complementarity gap reduced predetermined tolerances 
newton direction obtained solving system linear equations gammai deltax deltay deltas deltaz deltaw gamma gamma swe gamma ax gamma gamma gamma gamma denote violations primal dual constraints respectively 
call linear system newton equations system 
note primal dual method require feasibility solutions nonzero optimization process 
feasibility attained process optimality approached 
easy verify step length newton direction feasibility reached immediately 
seldom case smaller stepsize usually chosen damped newton iteration taken preserve positivity case stepsize ff applied infeasibilities reduced factor gamma ff 
take closer look newton equation system 
elimination deltaz gamma gamma gamma deltax deltas gamma deltax deltaw gamma gamma swe gamma deltas gamma gamma swe gamma deltax reduces gammad gamma deltax deltay gamma gamma gamma gamma gamma gamma gamma gamma swe gamma gamma solution reduced newton equations system computationally involved step interior point method 
shall discuss detail section 
system solved deltax deltay compute deltas deltaz deltaw 
maximum stepsizes primal space ff dual space ff computed nonnegativity variables preserved 
stepsizes slightly reduced factor ff prevent hitting boundary 
new iterate computed follows ff ff deltax ff ff deltas ff ff deltay ff ff deltaz ff ff deltaw making step barrier parameter updated process repeated 
theory computational practice previous section outlined primal dual algorithm 
shall address practical issues implementation 
theory known barrier parameter reduced slightly iteration possible take long steps newton direction 
implies fast convergence newton method iterates close central path 
practice efficient reduce barrier parameter slightly iteration stay close central path 
recall want find solution barrier parameter zero 
hand efficient move far away central path close boundary case algorithm get stuck small step newton direction 
convergence painfully slow 
starting point difficulty arising implementing primal dual method choice initial solution 
note problem solved elegant way homogeneous model cf 
section 
point centered close primal dual feasibility possible 
surprisingly points relatively close optimal solution centered lead bad performance numerical difficulties 
mehrotra proposed solve certain quadratic programming problem obtain initial solution 
variant idea 
starting solution optimal solution qp problem minimize subject ax predetermined weight parameter 
solution explicit formula computed cost comparable single interior point iteration 
supposed minimize norm primal solution promotes points better sense lp objective 
solution may negative components negative components pushed positive values sufficiently bounded away zero elements smaller ffi replaced ffi say ffi 
independently initial dual solution chosen similarly satisfy dual constraint 
elements smaller ffi replaced ffi 
stepsize simplest way ensure iterates remain close central path decrease barrier parameter slowly subsequent ipm iterations 
gave rise called short step methods known nice theoretical properties known demonstrate hopelessly slow convergence practice 
long step methods barrier parameter reduced faster theory suggests 
preserve convergence properties strategy theory requires newton steps computed primal dual iteration new point close neighborhood central path 
practice ignored newton step barrier parameter reduced 
negative consequence iterates kept close central path 
computational practice shows remain relatively large vicinity central path algorithm converges fast 
barrier parameter chosen fraction average complementarity product current point cf 
equation new fl average fl fl 
choice fl corresponds pure step choice fl expected reduce complementarity gap iterate 
iterates feasible complementarity gap guaranteed reduced factor gamma ff gamma fl 
choice fl generally choice point called target iterate hopefully taken crucial issue efficiency primal dual method 
shall discuss detail section 
observe current implementations different stepsizes primal dual spaces 
implies infeasibility reduced faster stepsize 
implementations variant strategy 
maximum possible stepsizes computed formulae ff max ff ff deltax deltas ff max ff ff deltaz deltaw stepsizes slightly reduced factor ff ensure new point strictly positive 
codes smaller ff iterations aggressive 
cases aggressive choice ff best 
general algorithm guaranteed globally convergent choice ff 
kojima megiddo mizuno proved global convergence variant primal dual method allows aggressive choice ff iterations 
ensure global convergence stepsizes chosen converge faster zero complementarity gap iterates allowed move far away central path 
lp problems default starting point described previously additional safe guards constraining stepsize 
stopping criteria interior point algorithms terminate order optimality conditions satisfied predetermined tolerance 
case primal dual method translates conditions imposed relative primal dual feasibility relative duality gap gamma gammap jjx gamma jjujj gammap jja gamma gamma gammap jc gamma gamma jb gamma wj gammap number digits accurate solution 
digits exact solution typically required literature 
observe conditions depend strongly scaling problem 
particular denominators left hand sides usually decrease scaling problem 
practice rare condition satisfied time conditions hold 
explanation phenomena comes analysis order optimality conditions 
observe equations impose primal dual feasibility linear 
easier satisfy newton method equations nonlinear additionally change subsequent interior point iterations 
consequently important condition really checked 
complexity point theory far computational practice estimates worst case complexity 
theoretical bound log ffl iterations obtain ffl exact solution lp extremely pessimistic practice number iterations log 
rare current implementation primal dual method uses iterations reach gamma optimality 
self dual embedding important elements primal dual algorithm solved satisfactorily practical point view 
element choice initial solution 
heuristic previous section works practice scaling dependent guarantee method producing centered point 
second element lack reliable technique detect infeasibility unboundedness lp problem 
infeasibility unboundedness problems usually manifests rapid growth primal dual objective function immediately leads numerical problems 
really critical point implementation primal dual algorithm 
algorithm section removes drawbacks 
skew symmetric self dual artificial lp model considered ye 
somewhat jansen skew symmetric self dual model primal dual pair symmetric form 
xu considered homogeneous self dual linear feasibility model fact studied goldman tucker 
xu developed large step path lp algorithm model implemented 
main advantage algorithm solves lp problem regularity assumption concerning existence optimal feasible interior feasible solutions 
problem infeasible unbounded algorithm correctly detects infeasibility primal dual problems 
algorithm may start positive primal dual pair feasible infeasible near central ray positive orthant 
algorithm takes large steps achieves nl iteration complexity 
compared primal dual method previous section algorithm disadvantage requires additional solve factorization newton equation matrix iteration 
model model 
sake simplicity section simplified primal lp formulation primal variables nonnegative upper bound minimize subject ax thetan dual maximize subject introducing homogeneous variable coupling primal dual problem gives homogeneous self dual linear feasibility model ax gammab gammaa gammac free linear feasibility system homogeneous zero trivial solution 
zero solution course interest lp theory tells strictly complementary solution exists linear program 
model lp problem zero objective function zero right hand side 
furthermore self dual 
denote slack vector second inequality constraint slack scalar third inequality constraint 
skew symmetric self dual property complementary pairs 
strictly complementary solution model satisfies xz diag 
strictly complementary solution model 
prove ffl optimal strictly complementary solution 
ffl implies gamma gammab strictly zero 
infeasible gammab infeasible gammab infeasible 
path algorithm due third constraint model feasible interior point 
definition central path similar sense restricted interior feasible region 
subsection define central path connects initial positive pair strictly complementary solution model 
algorithm developed infeasible central path strictly complementary solution 
feasibility residuals average complementarity residual defined gamma ax gamma gamma gamma respectively 
barrier problem parameter defines central path minimize gamma ln ln gamma ln ln subject gammab gamma gammaa gammaz gammac gamma gamma initial residuals 
shown xu essential introduce feasibility residual terms right hand sides 
central path feasibility complementarity residuals reduced rate eventually converge zero 
rate reduction feasibility complementarity residuals guarantee limit point strictly complementary solution model 
skew symmetric property order optimality conditions barrier problem gammab gamma gammaa gammaz gammac gamma gamma xz 
worth compare system analogous order optimality conditions primal dual algorithm previous section 
note example conditions define central path model interior point 
important highly degenerate problems solved 
reason helpful add feasibility residuals 
analogously primal dual algorithm search direction infeasible path algorithm generated applying newton method 
iteration algorithm solves linear equation system direction deltay deltax delta deltaz delta gammab gammaa gammai gammac gamma deltay deltax delta deltaz delta gamma fl gamma gamma fl gamma fl fl gamma fl gamma residuals current point fl chosen reduction rate barrier path parameter 
setting fl yields affine direction setting fl yields pure centering direction 
newton direction computed stepsize chosen method primal dual algorithm new point positive 
algorithm continues stopping criteria satisfied 
ffl lp problem infeasible near infeasible gamma gamma ffl optimal approximate solution obtained jc gamma yj jb yj gamma jjr jj jjxjj gamma jjr jj gamma step length chosen updated solution certain neighborhood central path worst case polynomial complexity result established 
xu restricted iterates stay intersection norm neighborhood large norm neighborhood central path 
case implementation achieves nl iteration complexity worst case 
clearly dimension newton equation system solved homogeneous algorithm slightly larger corresponding system solved primal dual method 
fact dimension increased exactly 
primal dual method implemented factorization primal dual method computed iteration 
factorization solve compute solution newton equation system see details 
solving newton equations section noted solution newton equations system computationally involved task primal dual method 
system reduces practice set equations gammad gamma deltax deltay noted ipms solve identical system linear equations 
difference value diagonal matrix right hand side 
reason comparison different variants interior point methods simplified comparison number iterations newton steps 
linear system solved direct iterative methods 
iterative methods conjugate gradient algorithms competitive general case due difficulties choosing computationally cheap preconditioner 
success iterative methods special lp problems obtained see consequently state art implementations general purpose ipms direct approach solve newton equations 
specific say variant symmetric triangular ll decomposition lower triangular matrix block diagonal matrix blocks dimension 
complete discussion mention alternative direct approach qr decomposition approach uses orthogonal transformations guarantees high accuracy solutions practice prohibitively expensive 
summing practicable approach solve newton equations general purpose ipm codes ll decomposition 
exist numerous variants implementations 
differ essentially restrictions imposed choice pivot order perspective viewed unifying framework shall section 
able described major alternative approaches 
reduces normal equations ad deltay ad pivoting diagonal elements gammad gamma 
approach solves augmented system directly necessarily pivoting gammad part 
shall address technical aspects implementation dependency computer hardware 
due rapid changes computing technology detailed discussion effect computer hardware goes scope book 
shall display important points different computer architectures influence efficiency 
shall discuss issues accuracy control ipm implementations 
normal equations approach advantage normal equations approach works positive definite matrix ad assume lp constraint matrix full row rank positive definite definition 
cholesky decomposition matrix exists numerical pivoting necessary maintain stability 
sparsity pattern decomposition independent value constant ipm iterations 
consequently sparsity preserving pivot order chosen care involves considerable computational effort extensively solution process 
argument justify application normal equations approach professional ipm implementations 
success implementation cholesky factorization depends quality analysis phase reordering sparsity 
goal find permutation matrix cholesky factor pad sparsest possible 
practice heuristics solve problem finding optimal permutation way np complete problem unacceptably expensive 
heuristics minimum degree minimum local fill orderings particularly useful context ipm implementations 
local rely pivot choice limited small subset attractive pivot candidates 
briefly discuss heuristics 
minimum degree ordering assume kth step gaussian elimination ith column schur complement contains nonzero entries diagonal element pivot 
kth step elimination requires gamma floating point operations flops executed 
exploit fact decomposed matrix ad positive definite pivot choice limited diagonal elements 
fact choice limited diagonal elements preserve symmetry 
function evaluates computational effort gives overestimate fill result elimination ith diagonal element pivot markowitz merit function applied symmetric matrix 
best pivot step sense number flops required perform elimination step minimizes interpreting process terms elimination graph see equivalent choice node graph minimum degree gave name heuristic 
minimum degree ordering algorithm implemented efficiently terms speed storage requirements 
details reader referred excellent summary 
minimum local fill ordering observe general function considerably overestimates expected number fill ins iteration gaussian elimination take account fact positions predicted fill nonzero entries exist 
possible pivot candidate expensive terms produce fill elimination step mainly update existing nonzero entries schur complement 
minimum local fill ordering chooses pivot 
generally minimum local fill algorithm produces sparser factorization higher initial cost obtain ordering analysis exactly predicts fill chooses pivot producing minimum number expensive 
efficient technique determine pivot order proposed 
method selects set attractive pivot candidates step smaller set chooses pivot generates minimal predicted fill 
computational experience shows considerable improvement speed loss quality ordering 
numerical examples give reader rough idea advantages competitive ordering schemes shall compare performance subset medium scale linear problems netlib collection 
table collects results comparison 
abbreviations mdo denote minimum degree ordering minimum local fill ordering respectively 
columns table contain problem names times seconds analysis phase orderings considered 
analysis time includes setup ordering building representation aa ordering time time building nonzero patterns cholesky factors 
algorithms ordering time dominating factor 
columns contain number nonzeros cholesky factors produced orderings number flops needed compute factorization including flops required computation aa columns contain average time seconds execute factorization sun sparc workstation 
results table indicate mdo usually faster exception usually produces denser cholesky factors 
going details note problems nonzeros aa concentrated tight band near diagonal grow offer advantage mdo 
contrast table comparison minimum degree mdo minimum local fill orderings name analysis time nonzeros flops factorization time mdo mdo mdo mdo fv bau bnl cycle dfl grow pilot pilot pilot sparsity pattern mdo left right problem cycle problems hard structures cycle dfl may efficient 
shows sparsity patterns cholesky factors obtained minimum degree minimum local fill orderings problem cycle largest difference heuristics observed 
careful giving final 
additional difficulty comes fact numerical factorization depends hardware particular ratio performance integer floating point operations machine 
shall address problem detail section 
conclude minimum degree ordering performs sufficiently default option ipm implementation 
cases difficult problems solved sequence problems sparsity patterns solved involved analysis minimum local fill ordering may pay 
disadvantages normal equations approach normal equations approach shows uniformly performance applied solution majority linear programs 
unfortunately suffers drawbacks 
normal equations behave badly primal linear program contains free variables 
transform problem standard form free variable replaced difference nonnegative variables gamma gamma presence logarithmic terms objective function causes fast growth split brothers 
difference may kept relatively close optimal value gamma tend infinity 
results serious loss accuracy 
remedy ipm implementations prevent excessive growth gamma serious drawback normal equations approach suffers dramatically presence dense columns reason dense column nonzero elements creates dense window size theta ad matrix subject symmetric row column permutation 
assume thetan gammak thetak matrices built sparse dense columns respectively 
techniques proposed treat part separately 
simplest due birge choice factorizations aa matrices 
factorization easily accommodates dense columns dense rows 
approach clearly fails contains dense columns dense rows 
possibility column splitting technique 
cuts long column shorter pieces introducing additional linking constraints 
unfortunately works satisfactorily small number dense columns 
popular way treating dense columns normal equations approach employs schur complement mechanism 
explicit decomposition matrix ad presumably sparse part significantly denser symmetric rank update 
cholesky decomposition computed sparse part dense rank update handled sherman morrison woodbury formula 
method guaranteed correctly sparse part may rank deficient clearly full row rank assumption guarantee full row rank 
happens cholesky decomposition exist sherman morrison woodbury update defined 
practical implementation small diagonal regularization term added decomposition exists 
method usually works satisfactorily small number dense columns 
andersen proposed remedy rank deficiency arising schur complement mechanism 
approach employs old technique due stewart 
technique corrects unacceptably small pivots cholesky factorization adding regularizing diagonal term 
consequently computing decomposition computes decomposition matrix oe regularizing term matrix built unit columns nonzeros appearing rows corresponding corrected pivots 
stable decomposition obtained ll stable working basis sherman morrison woodbury update compute gamma ll gamma gamma stewart technique attractive course small rank deficiency andersen observed rank deficiency exceed number columns handled separately 
method consists correcting small pivots factorization computing stable cholesky decomposition ee factorization employed schur complement mechanism compute ad gamma gamma ee gamma summing possible overcome important drawback normal equations approach handle dense columns 
remains question heuristic choose columns treated separately 
trivial selection rule number nonzero elements column identify hard columns shall discuss issue section 
recall schur complement mechanism efficient number dense columns constraint matrix excessive 
motivated researchers pay special attention augmented system form newton equations allows freedom pivot choice 
augmented system approach augmented system approach old understood technique solve squares problem 
consists application bunch parlett factorization symmetric indefinite matrix gammad gamma ll indefinite block diagonal matrix theta theta blocks 
contrast normal equations approach analysis factorization phases separated factorization computed dynamically 
means choice pivot concerned sparsity stability triangular factor 
obvious due careful choice stable pivots factorization stable normal equations 
hand due greater freedom choice pivot order augmented system factorization may produce significantly sparser factor normal equations 
special case pivots chosen part regardless stability properties concern fill produce 
advantageous stability properties augmented system approach motivated researchers incorporate ipm codes 
soon advantages approach ease handling free lp variables dense columns ability easy extension handling quadratic programming problems recognized 
success augmented system factorization depends highly efficiency pivot selection rule 
additionally save expensive analysis phase pivot order reused subsequent ipm iterations occasionally updated numerical properties newton equation matrix changed considerably 
mehrotra implementation example bunch parlett factorization generalized markowitz count type theta pivots 
hand shown theta pivot scheme valid computing symmetric factorization augmented matrix valid pivot order computed certain theory valid arbitrary matrices occurring interior point iterations 
ordering numerically unstable 
popular way pivot selection rule detecting dense columns pivoting diagonal positions gamma augmented matrix falling outside 
difficulty arises choice threshold density group columns sparse dense parts 
fixed threshold value approach works case dense columns easily identifiable number nonzeros exceeds significantly average number entries sparse columns 
complicated sparsity structure appears sophisticated heuristic needed 
give detailed analysis issue shall 
considers partition lp constraint matrix supposed sparse additionally assumed create sparse adjacency structure presumably small set difficult columns dense columns columns referring free variables set difficult rows 
efficient heuristic find partition 
partition determined gammad gamma gammad gamma deltax deltax deltay deltay analysis system shows immediately block inexpensively pivoted delayed possible 
elimination gamma causes limited fill reduces matrix gammad gamma elimination gamma block delayed attractive pivot candidates blocks exploited 
normal equations approach distinction pivots gamma gamma blocks 
worth note close relationship approach schur complement mechanism applied handle block difficult columns observe normal equations deltay deltay table comparison normal equations ne augmented system factorizations name densest analysis time nonzeros flops fact 
time column ne ne ne ne aircraft fit fit pilot stair fv bau replaced system gammai deltay deltay deltav difficult columns handled symmetric rank update easy part cf 
easy verify matrix involved system exactly sparsity pattern subject symmetric row column permutations 
normal equations versus augmented system table compares efficiency normal equations ne augmented system approaches 
cluster test problems groups 
group contains problems dense columns aircraft fit fit 
second group collect problems dense columns nonzero pattern normal equations pilot stair 
group contains problems advantageous structure augmented system 
columns table contain name problem number nonzeros densest column 
columns show setup time seconds competing approaches 
note setup time includes generation pivot order sparsity pattern analysis time numerical factorization 
columns contain number nonzeros factorization case ne corresponds sum nonzeros cholesky factor nonzeros 
columns contain number flops thousands required factorization approaches compared 
columns show average times seconds compute factorization algorithm 
results obtained sun sparc workstation 
results table obtained problems dense columns show advantage augmented system trivial implementation normal equations dense columns handled separately 
mbyte workstation unable store lower triangular part theta totally dense matrix resulted normal sparsity patterns ne left right pivot rule problem stair equations approach applied problem fit 
contrast augmented system produced sparse factorization case 
second group problems performance augmented system better 
third group problems lower setup cost normal equations augmented system approach disadvantageous 
gives bit insight sparsity patterns generated problem stair 
displays factored augmented matrices competitive approaches 
previous examples find methods important computational practice 
advantageous implemented analyzer able determine 
numerical factorization section shall demonstrate issues implementation numerical factorization step 
normal equations approach notational convenience methods applied similar way general symmetric decomposition augmented system 
ad consider cholesky factorization ll lower triangular matrix diagonal matrix 
note solution sparse symmetric system linear equations important problem scientific computing 
developed area theory computational practice 
basic formulae computing column denoted pivot jj jj jj gamma gamma jk jj gamma gamma kk jk approaches developed compute factorization 
exploit sparsity efficient way different techniques storage management computations 
george liu demonstrate calculations organized rows columns 
row cholesky factorization rows cholesky factor computed 
approach called bordering method 
enhancements 
alternative approach column cholesky factorization columns computed 
commonly form efficient implementations example yale sparse matrix package waterloo 
method called left looking factorization computing column information left part factor columns prior computations 
implementation uses dynamic linked lists identify left columns updating pivot column double precision array accumulate column modifications resolve nonzero matching different columns 
third approach submatrix cholesky factorization referred right looking factorization 
approach column computed immediately generates contributions subsequent columns columns right matrix 
matching nonzeros transformations approach trivial problem solutions efficient implementation 
interest approach increased past years ability better exploit high performance architectures memory hierarchy 
shall important techniques increase efficiency numerical factorization step interior point methods 
techniques come parallel vector computations common trick matrix vector operations dense mode reduce overhead sparse computations 
dense window straightforward improvement factorization exploitation dense window 
practice triangular factors completely dense steps cholesky factorization see figures 
partition columns handled dense matrix columns factored 
way overhead doing sparse computations avoided 
advantageous include dense columns dense window see 
supernodes dense window technique generalized observation 
due way cholesky decomposition works blocks columns tend sparsity pattern diagonal 
block columns called supernode treated dense submatrix 
supernode terminology comes elimination graph representation cholesky decomposition 
exist different types supernodes figures 
type supernode type supernode types supernodes exploited similar manner numerical factorization step 
analogously dense window technique supernodes increases portion flops dense matrix vector transformations save indirect addressing memory 
operations take advantage presence supernodes column member supernode operation building simplified operations members supernode done dense mode ii column member supernode depends set columns belong supernode temporary array accumulate contribution supernode contribution added advisable impose lower bound size supernodes extra step ii pay case small supernodes 
suggestion upper bound number nonzeros supernode better exploit cache memory computer architectures 
effect methods highly hardware dependent results literature efficiency decomposition shared memory multiprocessors discussed peyton exploitation cache memory high performance workstations studied rothberg gupta framework right looking factorization case left looking factorization investigated 
block cholesky factorization possibility dense computations partitioning smaller presumably dense blocks 
try divide block diagonal submatrices 
technique effective cases typical cholesky factor contains blocks largest usually dense window located bottom matrix 
consider matrix ad additional simplifying assumption blocks cholesky factor define supernodes 
cholesky factorization matrix computed steps 
factorize 
update gamma 
update gamma 
factorize advantage steps performed dense mode resulting efficient implementation high performance computers 
loop unrolling dense computations specialized exploit loop unrolling technique 
typical inner loop factorization adds multiple column 
target column source column multiplier 
assume kept single register steps performed computer execute transformation cb written follows 
read memory 
read memory 
compute cb 
store result memory 
consequently memory steps associated arithmetical operation step 
factorization multiple column modifications performed single column opens possibility unroll loop column transformations 
target column source columns scalar multipliers respectively 
step loop unrolling technique consists transformation execution transformation needs memory arithmetical operations multiplications 
memory saved compared execution elementary flops exploit loop unrolling 
technique brings considerable time savings computer architectures savings may vary significantly different computers 
numerical examples give reader idea efficiency techniques discussed dense window supernodes loop unrolling show computational results application small set test problems widely computer architecture sun sparc workstation 
table compare times seconds computing decomposition standard left looking factorization ll dense window technique dw factorization loop unrolling sn factorizations step loop unrolling sn sn sn respectively 
cover possibly wide set lp problems chosen test examples different characteristics 
problems aircraft fit extremely sparse factorization table comparison different techniques factorization name ll dw sn sn sn sn aircraft fit bau fv dfl augmented system cf 
table 
problems bau fv usual sparse problems dfl examples dense ones 
case sparse problems techniques little influence factorization times 
usual sparse problems dense window method unequivocally superior standard left looking method savings resulting supernodes loop unrolling evident 
step loop unrolling gives better execution time effect step loop unrolling negligible 
dense problems superiority simple method dense window computation times monotonically decrease degree loop unrolling 
hardware dependent aspects noted previously modern computer architectures different characteristics influence choice algorithm applied 
important ffl cache memory ffl pipelining ffl vectorization ffl superscalar capabilities 
simple choice best algorithm usually impossible extensive computational tests 
reader find discussion issues numerical examples 
collect general suggestions 
choice ordering methods minimum degree minimum local fill 
hint ratio cost integer logical floating point operations 
executed fast compared little chance savings numerical factorization compensate excessive effort analysis phase 
case faster minimum degree ordering appropriate 
cases standard low cost workstations minimum local fill ordering may attractive alternative 
numerical factorization phase commonly methods right looking left looking algorithms 
right looking factorization exploits cache memory better supernodes enter cache factorization case left looking factorization supernode enters cache memory times 
right looking factorization requires additional indirect addressing 
presumed criteria choosing numerical factorization algorithm investigation cache memory size time bringing information 
handling rank deficiency instability standard assumption theory interior point methods lp constraint matrix full row rank 
practice property may satisfied 
possible determine maximum set independent rows gaussian elimination 
computational cost operation relatively low cases 
hand ipms starting point similar benefit additional cholesky factorization ad conditioned detect linearly dependent rows 
pivot factorization falls predetermined tolerance ii ffl row dropped lp model 
approach general reliable specialized gaussian elimination application need additional computational effort exploits factorization anyway computed 
practice shows approach solves problem dependent rows 
manage satisfy full row rank property optimization proces solution process matrix ad may numerically rank deficient 
case due presence primal degeneracy 
consequently ipm implementations able deal rank deficient matrices ad feasible bounded lps negative influence ill conditioning ad accuracy solution normal equations surprisingly small 
stewart gives nice explanation common experience derived analysis properties right hand side vector 
stewart result apply case lp problem infeasible 
practice case usually manifests serious loss accuracy solving newton equations 
mentioned numerical difficulties usually appear close optimal solution especially presence primal degeneracy 
exist ways overcome 
techniques mathematically elegant additionally treated precious know revealed ipm specialists 
suggestions instability problems overcome section detailed presentation technique control accuracy applied public domain ipm code available trough netlib 
possibility handle nearly rank deficient factorizations remove presumably dependent rows virtually setting nonzeros zero ii 
note easier inverse diagonal matrix stored case suffices set gamma ii 
way add small regularizing term ffli matrix ad factorization 
helps complete factorization step needs special safeguards solution steps 
approach public domain lp code consists safeguard techniques 
uses dynamically adjusted diagonal regularizing matrix elements ii vary nearly zero value added acceptably stable pivots ii quite large regularizations ii added unstable pivots ii consequently decomposition ad stable factorization different regularized matrix ad computed 
rare contains large regularizing terms refer rows ad nearly linearly dependent 
important issue take account computations solves direction 
observe form decomposed matrix obtained perturbed augmented system gammad gamma deltax deltay hr note newton equations system corresponding quadratic programming problem closely related dual lp problem maximize gamma gamma gamma subject gamma point current iterate right hand side vectors hr derived order optimality conditions barrier problem associated gamma gamma gamma gamma gamma swe gamma gamma hr delta gamma note identical take particular point regularization technique interpreted quadratic penalty changes dual variables presumably unstable pivots computed 
computational experience shows prevents propagation round errors 
apart quadratic regularization technique mentioned lp code uses extensively iterative refinement process improve accuracy newton direction 
iterative refinement technique applied augmented system formulation newton equations system direction computed reduced normal equations form see 
presolve previous section concerned efficiency solving newton equation system advanced numerical linear algebra 
way improve efficiency solving newton equation system reduce size system sparser 
aim achieved analyzing lp problem remove redundancies 
practice largescale lp problems contain redundancies 
reasons 
model tend chose formulation easy understand modify model 
leads superfluous variables redundant constraints 
unfortunately impossible remove redundancies large scale lp problem manually 
presolve analysis aims improving problem formulation 
precisely goals defined follows ffl reduce size lp problem possible ffl reformulate model suitable form solver 
presolve phase old idea see role acknowledged simplex type optimizers 
simplex method lp works sparse submatrices bases ipm needs inversion considerably denser aa matrix 
consequently potential savings resulting initial problem reduction may larger ipm implementations 
reason presolve analysis enjoyed great attention 
additional important motivation large lp problems solved routinely nowadays amount redundancy increasing size problem 
reduction methods general finding redundancies lp problem computationally expensive 
presolve procedures arsenal simple inspection techniques detect obvious forms redundancies 
techniques applied repeatedly problem reduced 
briefly common reduction procedures 
details presolve phase 
reduction techniques 
empty rows columns removed 

fixed variable substituted problem 

row singleton defines simple variable bound appropriate bound modification row removed 

lower upper limits constraint determined fj ij ij fj ij ij clearly satisfy ij observe due nonnegativity limits nonpositive nonnegative respectively 
inequalities tight original inequality type lp constraint constraint redundant 
contradicts lp constraint problem infeasible 
special cases equal row greater equal row equality type row equals limits lp constraint forcing 
means way satisfy constraint fix variables appear appropriate bounds 

constraint limits generate implied variable bounds 
note lp variables transformed standard form 
technique original form lp constraint form slack variable added transform standard equality row 
assume example nonredundant equal type constraint ij ik ik ij ik ik gamma ij new implied bounds variables row gamma ik ik gamma ik ik bounds tighter original ones variable bounds improved 
note technique particularly useful imposes finite bounds free variables 
free variables case split represented difference nonnegative variables 

variable free column singleton jk ij gamma case variable substituted problem variable disappears kth constraint eliminated 
technique applied eliminate singleton implied free variable variable implied bounds generated technique point tight original bounds 

nonnegative unbounded variables referring singleton columns generate bounds dual variables variable refers singleton column entry ij dual constraint inequality ij inequality solved depending sign ij produces lower upper bound bounds dual variables generate lower upper limits dual constraints technique similar point 
limits determine variables reduced costs 
reduced cost proved strictly positive strictly negative corresponding variable fixed appropriate bound eliminated problem 

dual constraint limits obtained technique point generate new implied bounds dual variables 
technique similar point applied 
implied bounds tighter original ones replace old bounds open possibility eliminate variables technique point 
detecting redundancy improving sparsity presolve techniques described previous section involve considerable amount arithmetical operations 
techniques discussed section mainly sparsity pattern analysis 
list 

removing duplicate constraints 
constraints said duplicate identical scalar multiplier 
duplicate constraints removed problem 

removing linearly dependent constraints 
presence linearly dependent rows may lead serious numerical problems interior point methods implies rank deficiency newton equation system 
subramanian andersen report cases computational savings removing linearly dependent constraints significant 

removing duplicate columns 
columns said duplicate identical scalar multiplier 
example duplicate columns non negative split brothers replace free variable 
discussing disadvantages normal equations approach section mentioned negative consequences presence split free variables 
possible generate finite implied bound free variable avoid need splitting 
possible general duplicate variables replaced aggregate variable linear combination duplicates 

improving sparsity look nonsingular matrix thetam matrix ma sparse possible 
primal feasibility constraints case replaced equivalent formulation max suitable direct application interior point solver 
exact solution sparsity problem np complete problem efficient heuristics usually produce satisfactory nonzero reductions algorithm example looks row sparsity pattern subset sparsity pattern rows uses pivot nonzero elements rows 
types reductions common feature previous presolve techniques increase number lp problem 
may cases advantageous allow limited fill results elimination certain variables constraints 
list elimination techniques 

free implied free variables eliminated necessarily case correspond singleton columns cf 
section point case correspond denser columns 
noted elimination technique carefully may introduce large amount fill particular create dense columns 
requires additional sparsity structure analysis implemented properly 

rows corresponding equality type constraints pivot variables 
operation clearly opposite splitting dense columns causes concatenation shorter columns longer may advantageous length new column excessive 
application presolve techniques described results impressive reductions initial lp formulation 
hopefully reduced problem obtained presolve analysis solved faster 
solution solution recover complete primal dual solutions original problem 
phase called analysis discussed extensively 
table advantages presolve analysis 
name rows cols bau aa cre ken nug osa pds pilot wood sum table cholesky factors presolve 
name bau aa cre ken nug osa pds pilot wood numerical examples table computational results reproduced 
columns rows cols show number rows columns nonzero elements respectively 
columns show numbers presolve 
lp matrix statistics presolve elimination linearly dependent rows 
results collected table clearly advocate involved presolve analysis show exist rare practice irreducible problems 
advantages presolve analysis clearer compares sparsity cholesky factors obtained original reduced lp formulations 
table reports number nonzeros cholesky factor problems listed table 
numbers original problem formulation reduced final reduced form linearly dependent rows eliminated 
higher order extensions computationally expensive step implementation ipm solution newton equation system 
system solved iteration discussed section requires computing symmetric factorization matrices followed solve employing factorization 
theory practice factorization phase computationally expensive solve phase 
allow solves iteration solves help reduce total number interior point iterations number factorizations 
main idea high order methods shall discuss 
common feature reuse factorization newton equations system solves objective compute better search direction 
exist approaches type apply different schemes compute search direction 
shall review briefly 
approach proposed karmarkar constructed parameterized representation feasible trajectory motivated differential equations 
mehrotra method builds higher order taylor approximation infeasible primal dual central trajectory pushes iterate optimum approximation 
second order variant method proved successful 
approach due uses independent directions solves auxiliary linear program dimensional subspace find search direction 
method uses subspaces spanned directions generated higher order derivatives feasible central path earlier computed points predictor step 
followed centering steps take iterate sufficiently close central path 
hung ye studied theoretically higher order predictor corrector techniques incorporated homogeneous self dual algorithm 
approach defines sequence targets vast neighborhood central path 
targets usually easier reach analytic centers 
correctors supposed take iterates points 
consequently iterates remain relatively centered large discrepancy complementarity products avoided larger steps taken primal dual spaces 
part section shall concentrate approaches proved attractive computations second order predictor corrector technique multiple centrality correction technique 
predictor corrector technique mehrotra predictor corrector strategy components adaptive choice barrier parameter computation high order approximation central path 
step predictor corrector strategy compute affine scaling predictor direction 
affine scaling direction solves newton equation system denoted delta easy show step size ff taken affine scaling direction infeasibility reduced factor gamma ff 
current point feasible complementarity gap reduced factor 
large step affine scaling direction desirable progress optimization achieved 
hand feasible stepsize affine scaling direction small current point probably close boundary 
case barrier parameter reduced 
mehrotra suggested predicted reduction complementarity gap affine scaling direction estimate new barrier parameter 
affine scaling direction computed maximum stepsizes direction primal ff pa dual ff da spaces determined preserving nonnegativity 
predicted complementarity gap ff pa deltax ff da deltaz ff pa deltas ff da deltaw computed barrier parameter chosen heuristic high order component predictor corrector direction computed 
note ideally want compute direction iterate perfectly centered 
deltax deltaz equivalent relation variables associated upper bounds 
system rewritten deltax deltaz gammax gamma deltax deltaz observe computations newton direction equation second order term deltax deltaz neglected 
setting second order term equal zero mehrotra proposes estimate deltax deltaz affine scaling direction deltax deltaz predictor corrector direction obtained solving newton equations system linearized complementarity conditions barrier parameter chosen 
note presentation predictor corrector technique follows computational practice 
abuses mathematics sense stepsizes ff ff taken account building higher order taylor approximation central trajectory 
reader interested see detailed rigorous presentation approach consult 
observe single iteration second order predictor corrector primal dual method needs solves large sparse linear system different right hand sides 
benefit method obtain estimate barrier parameter high order approximation central path 
computational practice shows additional computational cost predictor corrector strategy offset reduction number iterations factorizations 
predictor corrector mechanism applied repeatedly leading methods order higher 
computational results show number iterations decrease sufficiently justify additional computations 
consequently second order predictor corrector technique couple years computational state art 
disappointing results higher order predictor corrector technique explained difficulty building accurate higher order approximation central trajectory 
hand large scale linear programs exist factorizations extremely expensive 
problems need save number factorizations important 
method section responds need 
modified centering directions observe step deltax deltay deltas deltaz deltaw aims drawing complementarity products value 
ensure progress optimization barrier parameter smaller average complementarity product average 
perfectly centered points usually reached 
theory requires subsequent iterates neighborhood central path computational practice may stay quite far away negative consequences ability large steps fast convergence 
approach proposed applies multiple centrality corrections combines choice reasonable centered targets supposed easier reach perfectly centered usually unreachable analytic centers 
idea targets analytic centers comes jansen roos terlaky vial 
define sequence traceable weighted analytic centers targets goes arbitrary interior point point close central path 
algorithm follows targets continuously slowly improves centrality subsequent iterates 
targets defined space complementarity products 
method translates approach computational practice combining choice attractive targets multiple correctors 
abuses theory sense limit improvement centrality measured discrepancy largest smallest complementarity product 
briefly approach 
assume primal dual solutions iteration primal dual algorithm strictly positive 
assume predictor direction delta point determined maximum stepsizes primal ff dual ff spaces computed preserve nonnegativity primal dual variables respectively 
look corrector direction delta larger stepsizes primal dual spaces allowed composite direction delta delta delta enlarge stepsizes ff ff ff min ff ffi ff ff min ff ffi ff respectively corrector term delta compensate negative components primal dual variables ff delta delta ff delta delta delta try reach goal adding corrector term delta drives exterior trial point iterate lying vicinity central path 
aware little chance reach analytic center step reach space complementarity products 
compute complementarity products trial point concentrate effort correcting outliers 
project point componentwise hypercube fi min fi max get target corrector direction delta solves linear system similar right hand side gamma nonzero elements subset positions gamma refer complementarity products belong fi min fi max 
corrector term delta computed new stepsizes ff ff determined composite direction delta delta delta primal dual algorithm move iterate 
correcting process repeated desirable number times 
case direction delta new predictor delta compute new trial point 
advantage approach computing single corrector term needs exactly effort dominated solution system right hand side 
questions arise choice optimal number corrections problem criteria correcting brings improvement 
answered 
naturally expensive factorizations compared correctors tried 
computational experience proved applied solution nontrivial problems method gives significant cpu time savings second order predictor corrector technique mehrotra 
optimal basis identification practical applications linear programming sequence closely related problems solved 
example case branch bound algorithm integer programming column generation cutting planes methods 
obviously closely related problems solved previous optimal solution solve new problem faster 
context simplex algorithm aim achieved starting previous optimal basic solution 
context interior point method warm start procedure exist obvious problem solved satisfactorily cf 
section 
hope comes particular ipm application approximate analytic centers looked general case interior point inefficient 
consequently approach adopted nowadays solve problem sequence closely related problems ipm cross simplex method 
case advantages methods exploited 
section shall address problem recovering optimal basis optimal primal dual interior point solution 
note exist lp applications optimal interior point solution preferable see christiansen greenberg 
primal dual algorithm discussed previous sections produces optimal basic solution optimal solution unique rare practice 
fact case multiple primal multiple dual optimal solutions primal dual method generate optimal solution analytic center optimal face see ye 
algorithm needed generates optimal basis optimal interior point solution 
notation section problem simplified standard form primal variables upper bounds minimize subject ax dual maximize subject known optimal solution satisfy complementarity slackness conditions 
known exists strictly complementary solution satisfies see goldman tucker 
strictly complementary solution define fj 
shown invariant respect strictly complementary solutions 
unique 
pair ng set determines optimal partition 
furthermore notation xp vector set jp means number elements set denotes matrix built columns corresponding variables belong jth column denote partition variables basic non basic variables 
optimal basis non singular gamma xn gammat gamma basic solution said primal dual degenerate component zero 
pivoting algorithm best algorithm generate optimal basis proposed megiddo 
constructs optimal basis iterations starting complementary solution strongly polynomial 
megiddo proved stronger result shown optimal basis constructed primal dual optimal solution strongly polynomial time exists strongly polynomial algorithm lp 
shall discuss megiddo algorithm implementation 
convenience assume set artificial variables added problem 
mg denote set artificial variables naturally optimal solution 
furthermore assume strictly complementary solution known 
assume know optimal partition know optimal primal solution xp 
know optimal dual solution nv 
fact algorithm works complementary solution conditions xp nv assumption relaxed xp nv 
megiddo algorithm consists primal dual phase 
start description primal phase 
partition variables problem basic non basic parts 
gamma gamma xn non singular 
solution xn called super basic solution non basic variables lower bound zero non basic variables identical zero called super basic 
idea primal phase move super basic variables zero pivot basis simplex iterations 
resulting basis primal optimal feasible complementary respect dual optimal solution 
move pivot step reduces number super basic variables number super basic variables exceed jp algorithm terminates jp iterations 
state algorithm algorithm choose basis primal ratio test move variable zero possible pivot basis 
update primal optimal basis 
observed step possible choose basis 
possible choice algorithm simplified version primal simplex algorithm pricing step incoming variables predetermined 
dual phase megiddo algorithm similar primal phase case super basic dual solution known 
means reduced costs corresponding basic variables zero 
similarly primal phase reduced costs moved zero corresponding primal variable pivoted basis 
dual algorithm stated follows algorithm choose basis gamma dual ratio test move variable zero possible take basis 
update dual optimal basis 
initial basis primal feasible remains feasible steps algorithm pivots primal degenerate 
algorithm terminates final basis primal dual feasible optimal 
furthermore number iterations dual phase exceed summing algorithms generate optimal basis iterations 
practice number iterations dependent level primal dual degeneracy 
implementational issues pivoting algorithm megiddo algorithm previous section assumes exact optimal solution known 
assumption met practice primal dual algorithm generates sequence converging optimal solution 
furthermore due finite precision computations solution returned primal dual algorithm exactly feasible complementary 
bixby lustig solve problem big version megiddo algorithm cross procedure drives complementarity feasibility zero 
algorithm adds worst case simplex pivots obtain optimal basis 
approach works unfortunately complicates implementation cross procedure 
andersen ye propose alternative solution problem 
iterate generated primal dual algorithm iteration guess optimal partition generated iteration define perturbed problem minimize subject ax assume variables reordered vector strictly complementary solution 
converges optimal primal solution converges converges similarly converges problems eventually share optimal bases 
advocates application megiddo algorithm perturbed problem 
note optimal complementary solution problem known 
important practical issue choice indicator optimal partition trivial fj unfortunately indicator invariant respect column scaling 
attractive 
indicator fj delta delta delta delta primal dual affine scaling search direction indicator scaling invariant 
uses variable changes guess optimal partition 
indicator justified theory reliable practice 
question choice right iteration terminate interior point algorithm start cross 
optimal basis generation expected produce correct optimal basis interior point solution optimal guess practical criteria switch fast quadratic convergence primal dual algorithm sets 
discussion linear algebra issues related implementing pivoting algorithm computational results refer reader papers 
interior point software years karmarkar publication interior point methods understood area theory practice 
current implementations sophisticated optimization tools capable solve large linear programs 
interior point methods proved significantly efficient best available simplex implementations lp problems 
efficient lp codes interior point methods developed years 
codes primal dual algorithm differ implementational details 
exist commercial vendors cplex cplex barrier www cplex com dash xpress mp www dash com ibm osl www research ibm com osl numerous research codes public domain executable source code form 
reader may find surprising research codes compare favorably best commercial products 
public domain research codes draw particular attention 
vanderbei loqo implementation predictor corrector primal dual method linear quadratic programming 
code written available executable form callable library 
loqo available www sor princeton edu 
note loqo free academic purposes 
zhang written matlab fortran implements primal dual method 
advantage ease comprehension resulting matlab programming language 
available pc math umbc edu 
implementation higher order primal dual method cf 
section 
code public domain form fortran source files info unige ch software 
implementation higher order primal dual method 
code available form fortran source files academic purposes ftp ftp hu pub software 
reader interested information lp codes commercial research ones consult lp faq lp frequently asked questions 
world wide web address lp faq ffl www com subscribers linear programming faq ffl ftp mit edu pub usenet sci answers linear programming faq give reader idea efficiency available commercial research lp codes run public domain test problems 
table gives sizes number rows columns nonzero elements respectively 
problems pilot dfl pds come netlib problems mod world nl belong collection maintained university iowa 
table reports statistics solution iterations cpu time seconds reach digit optimality ibm power pc workstation model mhz mb ram 
case digits optimality reached give parenthesis number exact digits suboptimal solution 
solvers compared cplex version sm simplex method cplex version barrier version loqo version version version 
cplex represents current state art commercial lp optimizer 
remaining solvers earlier mentioned research codes 
table test problem statistics 
name pilot dfl pds mod world nl table commercial vs public domain solvers 
problem cplex sm cplex bar loqo time time time time time time pilot dfl pds mod world nl analyzing results collected table warn reader computational results dependent different factors 
example choice test problems choice computer choice algorithmic parameters influence relative performance codes 
results reported table obtained compared codes run default options 
analysis results collected table indicates insignificant difference efficiency commercial public domain research codes 
available free charge 
different lp codes available nowadays reader may interested preparing implementation ipm 
warn trivial task 
lot different issues dealt system design choice programming language general implementing primal dual algorithm fortran time consuming job 
ipm implemented fast matlab environment 
matlab sparse matrix capability means relatively large lp problems solved efficiently 
performance ultimate goal code implemented fortran 
programming language matter taste 
commercial codes language 
programming language chosen step choose system design 
advisable build code structured modules 
instance cholesky factorization implemented separate module 
recommendation build optimizer called stand procedure 
regarding form input data standard mps format surely accepted efficient binary formats advantageous 
refer reader book discussion mps format 
reason able read mps format majority test problems available 
collection called netlib suite available anonymous ftp netlib att com cd netlib lp 
source larger difficult problems lp test collection gathered university iowa 
available anonymous ftp col biz uiowa edu cd pub lp 
done 
reading previous section reader impression area interior point methods linear programming deeply explored 
current ipm implementations extremely powerful robust significantly faster simplex codes 
natural question arises relevant problems remain open methods implementation 
point view important questions ffl implementation analysis correct way ffl warm start 
implementing lp algorithms consider methods produce shadow prices ranges 
practitioners simplex analysis assume knowledge optimal basis aware potential mathematical errors misleading economical consequences 
especially case lp problem degenerate case practice 
interior point analysis case give accurate answers see jansen 
interior point analysis potentially computationally expensive simplex method 
general warm start procedures ipms slow competitive simplex 
mentioned section promising results date obtained particular case ipm find approximate analytic center polytope optimize lp 
best approach currently solve difficult problems ipm identify optimal basis employ simplex method reoptimization required 
apart practical problems mentioned implementational improvements expected 
concluded current ipm implementations efficiently aware exist lp problems sparse produce surprisingly dense symmetric factorizations dfl pds problems netlib collection 
possible right way solve problems apply iterative approaches newton equations system 
increasing accessibility parallel computers near ipm methods exploit architecture important 
algorithm able solve lp problems larger currently possible 
important consequences areas integer programming improved cutting plane methods area stochastic optimization 
previous sections addressed important issues efficient implementation interior point methods 
discussion concentrated important algorithmic issues role centering equivalently central path way treating infeasibility standard primal dual algorithm model solves problem detecting infeasibility efficiently 
furthermore discussed detail computationally expensive part ipm methods solution newton equations system 
progress ipm methods lp past decade impressive 
complete theory interior point methods developed 
theory efficient implementations ipms constructed 
fact due algorithmic development improvements computer hardware larger lp problems solved routinely today decade ago 
methods going improve dramatically decade predict significant improvements current implementations 
hope believe developments useful practitioners 
adler karmarkar resende veiga 
data structures programming techniques implementation karmarkar algorithm 
orsa comput 
andersen 
finding linearly dependent rows large scale linear programming 
optimization methods software 
andersen andersen 
linear programming 
preprint dept math 
computer sci university 
appear math 
programming 
andersen ye 
combining interior point pivoting algorithms linear programming 
technical report department management sciences university iowa 
available ftp col biz uiowa edu pub papers cross ps appear management science 
andersen 
modified schur complement method handling dense columns interior point methods linear programming 
technical report dept math 
computer sci university 
submitted acm transaction mathematical software 
demmel duff 
solving sparse linear systems sparse backward error 
siam mat 
anal 
appl 
duff de 
augmented system approach sparse squares problems 
numer 
math 
birge freund vanderbei 
prior reduced fill solving equations interior point algorithms 
oper 
res 
lett 
bixby 
progress linear programming 
orsa comput 
bixby 
recovering optimal basis interior point solution 
oper 
res 
lett 

methods sparse linear squares problems 
bunch rose editors sparse matrix computation pages 
academic press 
mitra williams 
analysis mathematical programming problems prior applying simplex algorithm 
math 
programming 
bunch parlett 
direct methods solving symmetric systems linear equations 
siam numer 
anal 
carpenter lustig mulvey shanno 
separable quadratic programming primal dual interior point method sequential procedure 
orsa comput 
chang mccormick 
hierachical algorithm making sparse matrices sparse 
math 
programming 
choi monma shanno 
development primal dual interior point method 
orsa comput 
christiansen 
computation collapse state limit analysis lp primal affine scaling algorithm 
comput 
appl 
math 
boggs rogers 
optimizing dimensional subspaces interior point method linear programming 
linear algebra appl 
duff reid 
direct methods sparse matrices 
oxford university press new york 
duff gould reid scott turner 
factorization sparse symmetric indefinite matrices 
ima numer 
anal 
schultz sherman 
yale sparse matrix package symmetric code 
internat 
numer 
methods engrg 
el tapia zhang 
study indicators identifying zero variables interior point methods 
siam rev 
mccormick 
nonlinear programming sequential unconstrained minimization techniques 
john wiley sons new york 
forrest goldfarb 
steepest edge simplex algorithms linear programming 
math 
programming 
forrest tomlin 
implementing simplex method optimization subroutine library 
ibm systems 
mehrotra 
solving symmetric indefinite systems interior point method linear programming 
math 
programming 
frisch 
logarithmic potential method convex programming 
technical report university institute economics oslo norway 
gay 
electronic mail distribution linear programming test problems 
coal newsletter 
george 
liu 
computing solution large sparse positive definite systems 
prentice hall englewood cliffs nj 
george 
liu 
evolution minimum degree ordering algorithm 
siam rev 
gill murray saunders tomlin wright 
projected newton barrier methods linear programming equivalence karmarkar projective method 
math 
programming 
goffin vial 
cutting planes column generation techniques projective algorithm 
optim 
theory appl 
goldman tucker 
polyhedral convex cones 
kuhn tucker editors linear inequalities related systems pages princeton new jersey 
princeton university press 
goldman tucker 
theory linear programming 
kuhn tucker editors linear inequalities related systems pages princeton new jersey 
princeton university press 

splitting dense columns constraint matrix interior point methods large scale linear programming 
optimization 

multiple centrality corrections primal dual method linear programming 
technical report hec geneva section management studies university geneva november 
revised may appear computational optimization applications 

presolve analysis linear programs prior applying interior point method 
technical report hec geneva section management studies university geneva 
revised dec appear orsa comp 

version fast lp solver primal dual interior point method 
european oper 
res 
greenberg 
optimal partition linear programming solution analysis 
oper 
res 
lett 
ye 
convergence behaviour interior point algorithms 
math 
programming 
hoffman 
computational experience solving linear programs 
journal society industrial applied mathematics 

hung ye 
asymptotical nl iteration path linear programming algorithm uses wide neighborhoods 
technical report department mathematics university iowa march 
appear siam optimization 
jansen roos terlaky 
interior point approach parametric analysis linear programming 
interior point methods 
lor university department operations research budapest 
hungary 
jansen roos terlaky vial 
primal dual target algorithms linear programming 
technical report faculty technical mathematics informatics technical university delft delft netherlands 
jansen terlaky roos 
theory linear programming skew symmetric self dual problems central path 
optimization 
karmarkar 
polynomial time algorithm linear programming 
combinatorica 
karmarkar lagarias wang 
power series variants karmarkar type algorithms 
tech 

kojima megiddo mizuno 
primal dual infeasible interior point algorithm linear programming 
math 
programming 
kojima mizuno 
primal dual interior point algorithm linear 
megiddo editor progress mathematical programming interior point algorithms related methods pages 
springer verlag berlin 

liu 
generalized envelope method sparse factorization rows 
acm trans 
math 
software 
lustig shanno 
computational experience primal dual interior point method linear programming 
linear algebra appl 
lustig shanno 
interaction algorithms architectures interior point methods 
pardalos editor advances optimization parallel computing pages 
elsevier sciences publishers 
lustig shanno 
implementing mehrotra interior point method linear programming 
siam optim 
lustig shanno 
interior point methods linear programming computational state art 
orsa comput 
markowitz 
elimination form inverse application linear programming 
management sci 
cs 

role augmented system interior point methods 
technical report tr brunel university department mathematics statistics london 
monma shanno 
implementation primal dual method linear programming 
orsa comput 
megiddo 
pathways optimal set linear programming 
megiddo editor progress mathematical programming interior point algorithms related methods pages 
springer verlag 
megiddo 
finding primal dual optimal bases 
orsa comput 
mehrotra 
handling free variables interior methods 
technical report department industrial engineering managment sciences northwestern university evanston usa march 
mehrotra 
high order methods performance 
technical report department industrial engineering managment sciences northwestern university evanston usa 
mehrotra 
implementation primal dual interior point method 
siam optim 
du goffin vial 
short note comparative behaviour kelley cutting plane method analytic center cutting plane method 
technical report hec geneva section management studies university geneva january 
cs 

fast cholesky factorization interior point methods linear programming 
technical report computer automation institute hungarian academy sciences budapest 
appear computers mathematics applications 
cs 

inexact minimum local fill ordering algorithm 
working wp computer automation institute hungarian academy sciences budapest 
cs 

augmented system variant ipms stage stochastic linear programming computation 
working wp computer automation institute hungarian academy sciences budapest 

computer solution linear programs 
oxford university press new york 
von neumann 
maximization problem 
technical report institute advanced study princeton nj usa 
ng peyton 
cholesky factorization algorithm shared memory multiprocessors 
siam sci 
statist 
comput 
portugal terlaky 
investigation interior point algorithms linear transportation problems 
technical report der technische wiskunde en informatica technische universiteit delft 
resende veiga 
efficient implementation network interior point method 
technical report bell murray hill nj usa february 
rothberg gupta 
efficient sparse matrix factorization high performance workstations exploiting memory hierarchy 
acm trans 
math 
software 
zhao 
subspace methods solving linear programming problems 
technical report institut fur angewandte mathematik und statistic universitat germany january 
stewart 
modifying pivot elements gaussian elimination 
math 
comp 
stewart 
scaled projections 
linear algebra appl 
subramanian jr 
fleet delta air lines interfaces 

mathematical optimization system 
european oper 
res 

computing sparse lu factorizations large scale linear programming bases 
orsa comput 
walker 
direct solution sparse network equations optimally ordered triangular factorization 
proceedings ieee volume pages 

tucker 
dual systems homogeneous linear relations 
linear inequalities related systems pages 
princeton university press princeton nj 
turner 
computing projections karmarkar algorithm 
linear algebra appl 
vanderbei 
splitting dense columns sparse linear systems 
linear algebra appl 
vanderbei carpenter 
symmetric indefinite systems interior point methods 
math 
programming 
xu 
nl iteration large step infeasible path algorithm linear programming 
technical report college business administration university iowa iowa city ia august 
xu 
implementation homogeneous self dual linear programming algorithm 
technical report 
manuscript 
xu 
hung ye 
simplified homogeneous self dual linear programming algorithm implementation 
technical report department management sciences university iowa 
xu ye 
generalized homogeneous self dual algorithm linear programming 
oper 
res 
lett 
yannakakis 
computing minimum fill np complete 
siam algebraic discrete methods pages 
ye 
finite convergence interior point algorithms linear programming 
math 
programming 
ye todd mizuno 
nl iteration homogeneous self dual linear programming algorithm 
math 
oper 
res 
