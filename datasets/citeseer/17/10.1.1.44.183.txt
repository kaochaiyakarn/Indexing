simulating normalizing constants importance sampling bridge sampling path sampling andrew gelman department statistics columbia university new york ny xiao li meng department statistics university chicago chicago il november computing ratios normalizing constants probability models fundamental computational problem statistical scientific studies 
monte carlo simulation effective technique especially complex high dimensional models 
aims bring attention general statistical audiences effective methods originating theoretical physics time explore methods statistical perspective establishing theoretical connections illustrating uses statistical problems 
show acceptance ratio method thermodynamic integration natural generalizations importance sampling familiar statistical audiences 
generalizes importance sampling single bridge density case bridge sampling sense meng wong 
thermodynamic integration known numerical analysis literature ogata method high dimensional integration corresponds infinitely continuously connected bridges path 
path sampling formulation offers flexibility potential efficiency thermodynamic integration search optimal paths turns close connections jeffreys prior density rao hellinger distances densities 
provide informative theoretical example empirical examples involving dimensional integrations illustrate potential implementation path sampling 
discuss open problems 
keywords acceptance ratio method hellinger distance jeffreys prior density markov chain monte carlo numerical integration rao distance thermodynamic integration 
need computing normalizing constants powerful markov chain monte carlo mcmc methods simulate complex probability model general high dimensional variable knowing normalizing constant 
evaluate unnormalized density function directly calculate normalizing constant counting measure lebesgue measure mixture 
distributions 
easily computed intractable arise statistical models spatial models bayesian hierarchical models david peter mccullagh neal michael stein wing wong helpful conversations 
gelman research supported part nsf dms sbr young investigator award dms fellowship universiteit leuven 
meng research supported part nsf dms dms dms part nsa mda 
models incomplete data 
addition normalizing constants needed statistical scientific studies quantity interest deliberately formulated normalizing constant density draws 
example likelihood analysis missing data commonly occurs observations denoted complete computation complete data likelihood parameters jy complete complete straightforward 
suggests method simulating observed data likelihood jy obs obs cases difficult calculate jy obs directly example section 
complete jy obs complete obs jy complete jy obs treat likelihood interest jy obs normalizing constant complete jy obs complete data likelihood jy complete serving unnormalized density 
formulation complete plays role general notation 
instance genetic linkage analysis key step computation likelihood locations disease genes relative set markers observed data obs pedigree 
problem turns difficult large pedigree loci missing observations allele types inherited parents members pedigree 
example simulating complete complete jy obs oe feasible far trivial example sequential imputation method see irwin cox kong kong liu wong 
draws complete jy obs oe estimate jy obs normalizing constant essentially known effective method dealing problem see thompson 
application bridge sampling discuss section linkage analysis large pedigrees jensen kong 
related general problem unnormalized joint density evaluate marginal density 
marginal densities interest physical models example evaluating distribution energy gibbs model specified temperature statistics marginal likelihoods marginal posterior densities example parameter interest vector nuisance parameters example section 
computation bayes factor requires calculation probabilities marginal probability data individual model oe doe problem sort 
problem received attention literature see example gelfand day chib raftery lewis raftery 

particular 
provide comparative study variety methods laplace approximation bridge sampling computing bayes factors 
main bridge sampling typically provides order magnitude improvement 
path sampling part study potentials dramatic improvement demonstrate current 
physics chemistry studied problem computing normalizing constants known free energy estimation 
problem starts unnormalized density system density jt ff exp gamma ff kt ff energy function state boltzmann constant temperature ff vector system characteristics volume 
free energy system defined ff log ff ff normalizing constant system density 
simulation jt ff jt ff ff typically carried mcmc methods 
detailed discussions related topics see hoover frankel smit 
statistically oriented review neal comprehensive overview simulation techniques 
applications genetics physics real interest single normalizing constant ratios equivalently differences logarithms differences free energy differences 
true applications computing observed data likelihood ratios purpose monitoring convergence monte carlo em algorithms meng schilling 
appears need deal single normalizing constant bring convenient completely known density space point done 

loss generality consider class densities space denote numerical index continuous parameter 

convention triplet fp zg defined proper index index 
generic notation log ratio log 
examples interested particular log ratio wish evaluate arbitrary multiplicative constant continuous range 
common approaches approximating analytically intractable normalizing constants analytic approximation numerical integration evans swartz monte carlo simulation 
monte carlo simulation widely statistics mainly general applicability familiarity statisticians 
arguably general method available dealing complex high dimensional problems 
current routine methods statistics simulation rely scheme importance sampling draws approximate density 
see section 
theoretical evidence provided meng wong empirical evidence provided 
meng schilling context bridge sampling show substantial reductions order magnitude monte carlo errors achieved little minor increase computational effort draws 
key idea bridge densities effectively shorten distances target densities distances responsible large monte carlo errors standard importance sampling methods 
purpose fourfold 
describe method path sampling estimating section method general formulation flexible paths aiming reduction monte carlo errors thermodynamic integration method simulating free energy differences 
second show importance sampling bridge sampling path sampling represent natural methodological evolution bridge densities infinite number section show thermodynamic integration natural generalization acceptance ratio method known method free energy estimation corresponds bridge sampling single bridge 
third investigate problem optimal paths turns closely related jeffreys prior distribution rao hellinger distances distributions illustrate theoretical results simple informative example section 
fourth provide applications section illustrate implementation potential path sampling statistical problems 
general framework path sampling basic identities path sampling stated assume densities indexed continuous vector parameter 
may come naturally parametric family statistical applications 
general unnormalized densities support necessarily family 
construct continuous path link issue optimizing choice path discussed 
example suggested statistical physics neal construct geometric path scalar parameter geometric path gamma harmonic path analogy harmonic mean 
show section geometric path general suboptimal purpose estimating ratio normalizing constants 
derive basic identity path sampling assume scalar quantity loss generality assume interested computing ratio 
logarithms differentiating sides second equation respect yields standard formula ripley assuming legitimacy interchange integration differentiation log log denotes expectation respect sampling distribution 
identity consequence fact expected score function zero 
label log analogy potential statistical physics 
special case gamma kt gamma energy function equation 
integrating yields log consider random variable bayesian analysis uniform distribution interpret right side expectation joint distribution 
generally introduce prior density rewrite expectation respect joint density 
identity immediately suggests unbiased estimator 
necessarily independent draws 
joint distribution 
addition estimate log intermediate values just sample points 
simulation error depends choice samples drawn 
key advantage summand log scale generally stable ratio scale 
particularly important computing log likelihood ratio weighted sum log ratios normalizing constants meng schilling 
extensions multivariate straightforward fact suggested term path sampling 
suppose dimensional parameter vector interested ratio vectors select continuous path dimensional parameter space links defining log dt applying argument going obtain dt log dt dt easily construct corresponding path sampling estimator 
sampled uniformly draw 
path consistent unbiased estimator long sample average converges population average requirement met mcmc methods 
choice path obviously affects monte carlo error shall illustrate 
searching optimal estimators non uniform density unnecessary density absorbed path function 
fact univariate case prior density path function solving 
thermodynamic integration ogata method identity calculating new idea 
example thermodynamic integration method uses computing free energy difference molecular dynamic systems 
simple example notation calculate free energy difference systems temperature ff gamma ff ff ff ff ff ff dff ff denotes expectation respect system density jt ff ff scalar quantity volume 
equation application conjunction log ff gammah ff kt 
similarly calculate free energy difference systems different temperatures ff identity allows different ff different simultaneously 
see frankel smit neal section discussions thermodynamic integration named identities originally derived differential equations describing thermodynamic relationships 
applying ogata see ogata proposed innovative method high dimensional integrations 
simplicity suppose interested integrating positive function 
dimensional cube includes origin 
large 
apply construct family densities indexed scale parameter oe joe oe 
oe 
oe oe delta delta delta oe 
oe 

delta delta delta 
treating oe 
oe 
oe 
unnormalized density obtain log gamma log oe doe log oe 
doe oe respect density 
exactly integration want gamma allows estimate draws 
oe ng joe oe joe oe oe 
simulations accomplished metropolis algorithm metropolis illustrated ogata 
view simulate oe uniform distribution densities may provide better monte carlo errors discuss section 
ogata original proposals include deterministic choices oe equal spaced numerical integration techniques trapezoidal rule carry dimensional integration cases needs multiple draws oe discuss various implementation issues sections 
appears ogata independently discovered thermodynamic integration method 
subsequent ogata wrote learned estimation method log zn oe called free energy derivative suitable scalar parameter oe commonly field statistical physics late see binder example 
hand ogata motivated high dimensional integrations bayesian computations ogata mention method evans swartz review article methods approximating integrals special emphasis bayesian integration problems mention thermodynamic integration popular mcmc methods physics acceptance ratio method see section 
note lack citations criticize author emphasize great need communications researchers especially different fields 
initially worked problem started scratch gelman meng aware thermodynamic integration ogata method 
lack communication particularly unfortunate case missed effective methods high dimensional integrations view routine successful physics 
main purpose bring attention statisticians powerful methods time explore flexible statistical formulations aiming potential improvements general applicability 
particular formulation section allows arbitrary construction path distribution spaces explore section 
path sampling estimates numerical integration alternative estimating numerically evaluate integral essentially amounts replacing inverses spacings 
example ogata trapezoidal rule univariate 
specifically order unique values simulation draws delta delta delta excluding duplicates occur updated metropolis algorithm 
newly labeled compute average values 
simulation draws suppose want estimate log density ratio log 
indexes delta delta delta gamma applying trapezoidal rule estimate gammaa gamma ja gamma gamma gamma gamma obtained interpolation extrapolation necessary 
similarly apply simpson rule 
estimating particularly useful evaluated fixed grid known 
happens example draws jointly metropolis hastings hastings algorithm ability evaluate view unnormalized density joint space 
case proportional unknown normalizing constant want estimate 
cases applicable see section discussion issue 
case interpreted unnormalized marginal density similar method applied estimate corresponding cumulative distribution function cdf 
estimate unnormalized cdf ja gamma gamma gamma delta delta defined 
estimate cdf multivariate just unique way performing numerical integrations apply different choices path consider combining example weighted averages estimators different paths see section 
simple method averaging component time turns effective example section 
simplicity describe method dimensional evaluated rectangular grid values estimate functions grid log gamma log log gamma log fixed point grid 
function estimated function path sampling estimate averaging similarly estimated path sampling estimates combined identity log gamma log gamma averaging values yields log gamma constant course order reversed expression giving alternative estimate find example section order integration practical difference 
section provides theoretical investigation choices paths 
methodological evolution direct importance sampling methods different importance sampling schemes commonly computing normalizing constants 
approach uses draws trial density completely known analytic approximation target density 

importance sampling estimator identity 

corresponding monte carlo estimator 

draws 
example dempster weeks method check analytic approximation logistic regression likelihood 
usual importance sampling method effective fairly approximation complex models encountered free energy estimations finding acceptable complete known importance sampling density question 
fact various variance reduction techniques control variates importance sampling provide usable answers complex problems advanced methods popular 
second kind importance sampling method uses draws densities known unnormalized forms applied directly 
typically case iterative simulation metropolis algorithm produce draws 
knowing 
unknown quantity interest 
situation address 
case various methods special cases identity studied detail meng wong ff ff denotes expectation respect ff 
arbitrary function satisfying fi fi fi fi omega omega ff fi fi fi fi omega support assume omega omega 
example ff gamma leads commonly identity ott geyer thompson green 

assuming omega ae omega ff gamma leads generalization harmonic rule newton raftery 
leads called reciprocal importance sampling method see gelfand dey 
acceptance ratio method bridge sampling trivial verify key identity underlying powerful acceptance ratio method bennett motivated considering metropolis algorithm allows moves recast derivation general metropolis hastings algorithm order reveal explicit relationship ff function corresponding proposal jumping distribution deltaj delta metropolis hastings algorithm 
start considering metropolis hastings algorithm joint space target density 
clearly jt marginally ratio interested 
consider moves stays changes switching density argument 
detailed balance requirement metropolis hastings algorithm transition kernel deltaj delta min ae oe 
min ae 

oe follows integrating summing sides respect ff 
min ae 

oe view 
derivation bennett corresponds choosing proposing switch 
probability proposal accepted transition kernel see right side ratio marginal acceptance probabilities name acceptance ratio bennett 
general ff corresponds bennett weight function meng wong suggested name bridge sampling term explain section 
note implement switching algorithm empirical proportions estimate necessary fact typically desirable generally estimate accurately averaging acceptance probabilities acceptance rates case rao blackwellization gelfand smith 
draws 
draws 
choice ff sample version ff 
ff 

ff 
ff consistent estimator long sample averages converge corresponding population means variance obviously varies ff draws 
question optimal choice ff difficult answer general due correlations draws 
case answer easily obtained independent draws assumption typically violated practice permits useful theoretical explorations fact optimal estimator obtained assumption performs general see bennett meng schilling 
specifically independence assumption optimal ff sense minimizing asymptotic variance log ff bennett equivalently asymptotic relative variance ff meng wong ff opt 


rq 

omega omega assumed asymptotically bounded away 
corresponding asymptotic minimal error ns omega omega gamma gamma optimal ff opt directly usable depends unknown ratio meng wong construct iterative estimator opt opt opt mi mi mi calculated iteration 
show iterate provides consistent estimator unique limit opt achieves asymptotic minimal error 
study non iterative choices ff ff ff gamma empirical results meng schilling shows estimators substantially factor reduce relative mean squared errors compared estimators amount draws density 
note bennett suggested graphical method obtaining opt geyer proposed interesting profile likelihood derivation opt connecting bridge path sampling importance sampling fundamental identity underlying bridge sampling motivated easily importance sampling familiar statistical readers 
see define ff 


omega omega 
arbitrary unnormalized density having support omega omega condition satisfied 
subscript indicate intend density sense overlapped 
substituting ff yields 



corresponding estimator draws draws applying directly estimate apply estimate take ratio cancel gain efficiency arises sensible choice bridge density non overlap serves bridge name bridge sampling 
terms see best bridge density weighted harmonic mean opt gamma gamma gamma interestingly asymptotic minimal error determined normalizing constant harmonic bridge 
see meng wong discussion relationship bridge sampling umbrella sampling method developed computational physics termed ratio importance sampling chen shao statistical literature 
see neal nice graphical representation 
idea creating bridge obviously pushed 
possible densities 

far separated optimal bridge density opt estimator variable practice exist completely separated 
cases useful construct finite series gamma intermediate densities draws 
simplicity derivations label corresponding unnormalized densities including points 
pair consecutive functions gamma label intermediate unnormalized density computed sampled 
unnormalized densities apply identity telescoping fashion gamma ffi gamma gamma ffi gamma gamma gamma gamma generalization bridge sampling gamma spans 
meng wong multiple bridge identities estimating ratio normalizing constants simultaneously see geyer profile likelihood approach estimating ratios simultaneously 
intermediate systems distributions implement importance sampling known idea computational physics neal section 
tempted study limiting case infinite number bridges 
easily done considering indexes corresponding parameter indexing parametric family fq 
setup logarithms sides yields log gamma gamma gamma functions defined log easy verify notation section regularity condition support depend 
taylor expansion right side log lim gamma gamma exactly basic identity underlying path sampling 
derivation may helpful studying trade implementation efficiency versus monte carlo efficiency adopting multi bridge sampling path sampling issue practical interest 
theoretical investigation path sampling optimal prior density dimension arbitrariness prior density allows search optimal estimators sense achieving minimal monte carlo variances 
due difficulty establishing general results arbitrary sampling schemes shall assume independent draws theoretical explorations guidelines real implementations sections 

independent draws joint distribution monte carlo variance var gamma gamma assume 
seek marginal prior density minimizes equivalent minimizing term 
cauchy schwartz inequality right side depend equality holds follows optimal prior density corresponding optimal variance var opt gamma interestingly independent case optimal density exactly jeffreys prior density restricted 
general viewed generalized local jeffreys prior density unnormalized density expectation respect normalized density proper 
view variance stabilizing transformation equation path function prior density current setting term second moment stabilizing transformation appropriate generally zero 
second moment stabilizing property seen noticing optimal prior second moment term free 
sense optimal procedure balance second sampling moment different locations intent minimize average 
variance appropriate estimator estimated 
derivations asymptotic variances quite involved independence assumption due presence linear nonlinear functions order statistics discuss 
optimal path dimensions generalization result multivariate immediate 
path optimal density path generalized local jeffreys prior density path 
answer question path optimal sense yielding minimal monte carlo variance possible paths 
answer question problem calculus variations 
specifically variance independent sampling var dt gamma ij dt gamma ij 
path function minimizes term right side solution euler lagrange equations atkinson mitchell boundary condition ik ij denotes second derivative respect ij symbol kind ij ik jk gamma ij similar calculus variations problems encountered statistical literature finding rao distance densities rao atkinson mitchell mitchell 
rao distance minimal variance naturally related accuracy path sampling estimator depends crucially distance unnormalized densities 
rao distance provides appropriate measure 
rao distance constructed considering variance score function projected particular path difference rao distance current calculation dealing unnormalized densities 
example differs atkinson mitchell log log defining functions inside ij fact section show rao distance distribution space directly related optimal path distribution space 
explored literature rao distance solving typically difficult 
atkinson mitchell suggested alternative ways expressing solutions hamilton equations hamilton jacobi equations courant hilbert provided differential geometry argument finding rao distance normal densities 
despite efforts general problem remains difficult 
section provides theoretical example normal distribution analytic nontrivial solution 
optimal path distribution space previous section family distributions 
different problem find optimal path space integrable nonnegative functions connects unnormalized density functions 

seek optimize nonnegative functions scalar parameter having range subject boundary conditions cq 
cq arbitrary positive constant 
loss generality assume uniform distribution absorption transformation discussed section 
practice necessary define family functions 

part common parametric family 
distributions parametric form efficient path may possible leaving parametric form moving general distribution space 
possible general constructions include geometric path suggested physics scaling path proposed ogata example possible paths general distribution space form scaling path 


path estimate need adjust known bias log 
theoretical example section geometric path scaling path lead identical monte carlo error compares favorable optimal bridge sampling improved substantially path sampling framework 
great practical interest find general simple paths properties 
finding optimal path distribution space turns easier mathematical problem optimization problem described previous section 
start writing path density expressing log log log minimize left side separately minimize terms right side appropriate boundary conditions 
term right side simple result consequence cauchy schwartz inequality provides answer 
lemma 
positive function log gamma log log equality holding equals opt gamma exp surely respect lebesgue measure 
result implies yields different optimal distributional space opt dominates discussing theoretical optimality implementation feasibility 
example geometric path suboptimal general path gamma gamma 
second term right side simply fisher information 
minimizing finding rao geodesic distance distribution space problem solved differential geometry approach reviewed see kass vos 
turns somewhat unexpectedly problem solved cauchy schwartz inequality show appendix 
result course identical result rao distance distribution space expressions convenient path sampling application 
lemma 
fisher information 


ff arctan gamma 
gamma hellinger distance ff equality holds 
cos gamma ff cos ff gamma sin gamma ff sin ff 
cos gamma ff cos ff sin gamma ff sin ff surely respect product measure formed lebesgue measure 
applying lemma see optimal distributional space 
cos gamma ff cos ff gamma sin gamma ff sin ff 
cos gamma ff cos ff sin gamma ff sin ff corresponding minimal variance var ff simple function 
intrinsic connection hellinger distance appears bridge sampling context meng wong show optimal error bounded simple functions 
bridge sampling optimal error achieved iterative solution unclear optimal error achievable asymptotically practice optimal solution assumes knowledge unknown normalizing constants ability independent draws 
adaptive method iteratively estimating may lead increase variance 
fact doubt achievable bounded note ff infinitely apart 
interesting empirically relevant problem achievable minimal error varies distance measures 
important issue unachievable theoretical optimal error choices methods contrast theoretical comparisons chan shao empirical comparisons chan shao actual computational time needed ratio importance sampling method taken account 
interests exploring theoretical results lie finding useful insights practical guidelines potential limits path sampling 
shall demonstrate section solve family normal distributions optimal path reduce monte carlo variance orders magnitude compared natural non optimal choices gain especially important densities far apart 
emphasize necessary find optimal path order gain substantial reduction variance example normal example simple paths reduce variance orders magnitude compared previous methods 
empirical implementations section confirm superiority path sampling simple choices paths 
theoretical illustration illustrate theoretically potential path sampling reducing monte carlo variances adopt example meng wong illustrating bridge sampling 
example toy nature findings fact somewhat surprised 

exp gamma 

exp gamma 
gamma true estimated zero 
purpose path sampling consider points family unnormalized normal densities exp gamma 
gamma oe oe 
order nearly fair comparisons assume importance sampling draws ii bridge sampling assume draws iii path sampling draw uniformly table comparison theoretical monte carlo errors importance bridge path sampling estimators normal densities spaced standard deviations apart method ne gamma importance sampling exp gamma ii bridge sampling geometric bridge exp gamma iii bridge sampling optimal harmonic bridge fi exp gamma iv path sampling geometric scaling path path sampling optimal path space vi path sampling optimal path oe space log note iii fi exp gammax cosh dx property fi lim fi 
draw oe oe path 
draws independent scheme 
addition importance sampling bridge sampling estimate ratio path sampling estimates log ratio convert estimates scale letting log conversion variance squared relative error gamma asymptotically 
setting table compares different estimators computations ne gamma exact path sampling estimators correct terms gamma 
table estimator importance sampling estimator estimators ii iii bridge sampling gamma gamma gamma respectively corresponding variance computations meng wong 
estimator iv path sampling estimator geometric path case leads identical estimator scaling path 
estimator optimal univariate path sampling estimator considering oe fixed letting vary 
easy verify current example generalized local jeffreys prior density defined uniform optimal path function dt 
estimator vi optimal multivariate path sampling estimator oe allowed freely dimensional space optimal path discussed shortly 
plots expressions table functions dotted line plots ff see gamma exp gammad lower bound 
discussed section doubt bound achieved reality easily improve estimator vi optimal normalizing constants densities path 
entails oe gamma expf gamma 
gamma oe place resulting monte carlo error obtained replacing values row vi table result lower values optimal distribution space 
optimal path oe space denoted oe turns quite interesting informative 
figures plot oe solid segments general expressions oe derivations appendix 
amounts half ellipsoid curve oe space gamma oe oe displayed 
optimal path increases variances normal densities middle sense want middle densities large overlaps point densities 
variance intermediate densities allowed arbitrarily large introduce sampling variability value oe 
optimal path result trade 
seen clearly displays normalized normal densities corresponding oe densities shown solid lines 
practical implementation examples issues implementing path sampling implement path sampling need draws joint distribution written 
marginal distribution draws freedom specify known 
emphasize depending nature completely unrelated want compute proportional identical 
distinguish kinds implementations path sampling 
kind subject preceding discussion specify particular form chosen typically reasons convenience perceived optimality 
simplest method obtaining simulation draws direct sampling draw drawing known choosing systematically grid ogata example section drawing 
step sampling easy freedom specify 
drawing usually difficult problems apply path sampling easy way directly sample 
preferred method form iterative simulation metropolis algorithm gibbs sampler 
implement path sampling iterative simulation proceed directly nested loops simulated chosen run iterative simulation algorithm approximate convergence 
result large number draws estimator applicable 
nested simulation approach may attractive parallel computing environment 
nested loop method flavor elaborate metropolis coupled markov chain method geyer 
second kind implementation specify multiplicative constant 
means write joint density constant draw density combining simulations single loop iterative simulation 
natural approach alternately update gibbs metropolis type algorithm 
essentially special case simulated tempering see parisi geyer thompson viewed temperature variable similar algorithms proposed statistical physics literature berg berg see neal geyer thompson discussion 
draws apply conjunction multivariate estimate function discussed section 
interestingly directly estimate relative values having adjustment 
special case kind implementation drawing joint density unnormalized density means set illustrate implementation example section 
single loop method new problem arise marginal density 
difficulty vary orders magnitude region interest problem unnormalized marginal density regions relatively high marginal mass interest 
simply sampling joint distribution proportional setting leave draws regions low marginal density little ability compute regions path sampling method 
example estimate require draws interval 
fortunately single loop sampling ability choose reduce variance estimators 
general easily compute optimal generalized jeffreys prior density aim simpler goal uniform goal proportional avoids problem regions far fewer draws 
non iterative approaches available creating approximation including laplace method method coding conditional distributions besag various numerical methods evans swartz 
approximation making draws final estimate inaccuracy approximation bias estimates path sampling 
cases reasonable approximation immediately available update function iteratively 
start initial guess say 
run simulation metropolis hastings algorithm 
occasionally say iterations estimate function path sampling density estimate directly simulated values 
case update equal current estimate continue iteration 
limit suitable mixing conditions converges uniformity 
iterative scheme similar iterative method proposed geyer thompson adjusting needed implementing simulated tempering 
emphasize estimator require computed convergence required path sampling estimators valid 
reminiscent iterative sequence optimal single bridge estimator iterate provides valid estimate iteration needed purpose optimality 
nice feature iterative procedure empirical distribution simulated values converges known distribution uniform 
monitor convergence simulations comparing known distribution far easier usual task monitoring convergence unknown target distribution 
general construct checks convergence iterative simulation byproduct path sampling 
example compare estimate empirical distribution simulated values see section distributions measured criterion practical concern comparison central posterior intervals indicates lack convergence simulation error implementation sampler path sampling estimate 
similar procedures available arbitrary chosen known 
procedures check convergence parameter model parameter take role role analysis merely changing derivative recomputing path sampling estimate 
example censored data spatial statistics problem high dimensional integration commonly arises missing censored data 
case data likelihood function easy compute 
hand likelihood censored data calculated directly 
fix ideas assume 
vector real numbers censored data max 
likelihood censored data yj gamma delta delta delta gamma 
integrating censored components 
treating yj normalizing constant jy complete data likelihood unnormalized density setting 
particular example consider stationary model spatial statistics described stein 
example observed location dimensional space 
vector data modeled joint normal distribution component mean variance correlation components gammax stein presents set simulated data theta grid evenly spread square components equal 
goal compute likelihood parameter vector log 
censored data likelihood trivial compute joint normal density spatial dependence observations dimensional integral calculated analytically 
stein importance sampling compute relative values marginal likelihood yj theta grid space 
point grid stein decomposition truncated multivariate normal distribution construct approximation jy known normalizing constant 
sampled draws point estimated yj importance sampling 
crucial step stein method pseudo random numbers draws feasible stein sampled approximate densities inverse cdf approach 
introduces desirable positive dependence importance sampling estimates different points grid stein noted greatly reduces monte carlo error resulting ratio estimator 
replicate stein results path sampling nested loops computationally straightforward simplicity gibbs sampler case 
value log theta grid gibbs sampler simulate conditional distribution values jy 
monitored convergence parallel runs gibbs sampler method gelman rubin simulations reached approximate convergence iterations 
discard half simulation draws value 
conjunction estimate function log yj yj theta grid log maximum likelihood estimate 
gives contour plot estimated negative loglikelihood ratio integrate log applying 
shows corresponding plot integrate 
effects different paths quite visible case gives smoother answer identical plot stein 
note path sampling method involve constructing approximate densities pseudo random numbers 
example regression models election forecasting statistical model substantive background consider regression model oe xfi oe gamma vector weights parameter restricted range 
gelman model forecasting presidential elections units representing states election years democratic party share vote state year matrix predictors proportional number voters state year 
predictors regression chosen existing regression models political science 
values represent extreme models considered implicitly explicitly political science 
setting corresponds equal residual variances states generally assumed forecasting regression models elections political science research convenience reason 
setting variance inversely proportional number voters theoretical appeal generalization binomial model implicit game theoretic models voting 
major trends research political science unify empirical theoretical analyses value issue needs resolved reasons applied obtaining efficient forecasts regression estimates theoretical understanding variability voters aggregate 
see gelman king discussion issues political science economics models 
point statistical practice suggests different ways data assess information data parameter 
classical approaches include significance tests accept reject null hypothesis alternative vice versa obtaining approximately unbiased point estimate considering nonlinear estimation problem nuisance parameters 
bayesian approaches include choosing bayes factor assess relative evidence favor possibilities including continuous parameter model range computing posterior distribution 
approaches involve nearly identical computations distribution likelihood ratio candidate models 
context simulation inference approach natural involve computation ratio marginal densities discussed section equivalent ratio normalizing constants 
computation done methods described extent likelihoods models far apart generally case number data points increases advisable consider path sampling 
natural choice path allow range power law family 
application prefer consider continuous parameter start wish consider possibilities models fall extremes truth existing approaches 
theoretical argument allowing vary appropriate value depend set explanatory variables model example game theoretic descriptions accurate depending information assumed known 
allowing uncertain continuous range classical estimation approach serious problems notably likelihood extremely flat parameters variance model example poorly identified data sets moderate size point estimate accurate summary 
possibility point estimate outside just due high variability strong evidence estimate constrained boundary 
example possible point estimate supported data 
problems get serious presence nuisance parameters example fi contains random effects components 
reasons prefer bayesian approach summarizing information posterior distribution non bayesian terminology marginal likelihood shall uniform prior distribution 
discussed section determining marginal posterior density mathematically equivalent computing normalizing constant parameterized 
constraining important examine behavior likelihood near boundary see evidence 
consider methods computing posterior density marginal likelihood normalizing constant function usual approach bayesian simulation consider parameter model summarize posterior distribution empirical distribution simulation draws done gelman ii path sampling 
fact shall simulation draws obtain path sampling estimates 
simulation draws posterior distribution model parameters implement path sampling compare estimated marginal posterior distribution path sampling direct estimates simulation draws 
path sampling non hierarchical model check performance path sampling consider simple non hierarchical model assigns uniform prior distribution fi log oe 
discussed gelman unnormalized marginal posterior density model written analytically posterior draws vector parameters obtained directly drawing discrete approximation numerically calculated marginal posterior distribution drawing fi oe normal inverse posterior distribution conditional drawn 
election example done independent draws draws fi oe draw 
general notation fi oe dimensional fi components 
compute path sampling estimate jy determine function differentiation easy yields gamma oe log gamma xfi simulation draws nested loop form compute path sampling estimate jy 
shows results comparing exact density smooth line path sampling estimate slightly jagged line histogram simulation draws 
path sampling estimate course worse exact density compares favorably histogram estimate 
comparison afforded shows corresponding cdf 
jagged line empirical cdf draws smooth line represents path sampling estimate exact cdf differences barely visible 
obviously case simply exact formula informative encouraging able confirm path sampling capable producing accurate approximation dimensional integration indexed entire curve draws total 
path sampling hierarchical model move realistic complicated model fitted gelman marginal density computed analytically 
model additional components fi added dealing dimensional integration hierarchical regression model additional variance components 
expanded model marginal posterior density computed exactly posterior simulations obtained gibbs sampler metropolis algorithm alternating metropolis jumps gibbs draws remaining parameters 
approximately starting points algorithm obtained approximation posterior distribution 
gibbs sampler draws performed linear regression operations simulations normal random variables metropolis steps univariate normal jumping kernel scale set times estimated standard deviation initial approximation motivated gelman roberts gilks 
election example sequences length sufficient approximate convergence simulations monitored methods gelman rubin 
compute path sampling estimate jy need function form added hierarchical part model involve parameter 
case simulation draws single loop form meaning learn jy range obtained simulation 
shows estimated marginal posterior density path sampling histogram simulation draws shows corresponding estimated cdf 
path sampling estimates far smoother 
evidence quite confident path sampling estimates close truth especially cdf 
models smoothness path sampling estimates intrinsic property estimation despite appearance smoothing creating estimates 
summary research attempts bring attention statistical researchers useful methods computing normalizing constants complex high dimensional probability models generally computing high dimensional integrations complicated 
bridge path sampling rooted popular methods theoretical physics acceptance ratio method thermodynamic integration 
due extremely challenging important computational problems physics chemistry far resolved minimum energy configurations protein molecules huge literature theoretical computational physics chemistry creative methods high dimensional integration optimization 
excellent necessarily statistician friendly review number powerful methods see long review article path integrals theory condensed helium 
particularly methods discussed section potentially useful implementing path sampling general phrase path sampling describe sampling methods simulating path integrals called thermal density matrix 
great success theoretical physics dealing complex integrations ogata successful applications bayesian computations believe path sampling generally useful statistical computations dealing complex integrations 
method capable producing remarkably accurate results quite ward class methods useful high dimensional complex integrations 
states review chapter free energy estimation thermodynamic integration ti undoubtedly method widely compute absolute free energies free energy differences 
reason may time consuming sophisticated methods described straightforward accurate run special problems high densities large system sizes 
hope simple trivial empirical illustrations theoretical example helped convey messages statistical researchers quote clear thermodynamic integration path sampling dominate methods 
furthermore hope derivation path sampling relates importance sampling bridge sampling help general statistical readers understand method intuitively able apply confidence 
statistical context path sampling gives alternative method estimating marginal distributions check convergence monte carlo simulations 
general formulation investigation reveals research needed order fully explore potential path sampling 
example construction efficient simple general paths great importance routine application path sampling 
theoretical results example general sub optimality geometric path normal example optimal path curves space oe show best paths obvious 
question achievable optimal error theoretical interest practical relevance construct easily implementable iterative procedure just bridge sampling compute optimal estimate 
questions inherently statistical statisticians able contribute substantially study efficient implementation path sampling especially view theoretical relations optimal paths jeffreys prior rao hellinger distances 
path sampling markov chain monte carlo methods statistical tool originated computational physics potential benefit powerful method efficient applicable broadening range statistical models routinely 
appendix elementary proof lemma proof differentiable positive function 
theorem verify gamma log cauchy schwartz inequality term right side bounded bound achieved surely respect product measure 

positive function determined 
solving boundary condition yields 
gamma 
ji 
freedom choosing allows ensure function proper density requirement leads differential equation gamma gamma solving boundary condition yields cos ff cos gamma ff rest proof follows simple algebraic manipulation 
qed 
ii 
derivation optimal path oe space normal example derive optimal path normal family example section general case endpoints oe oe loss generality assume start noting normal family variance formula var oe oe dt gamma corresponding euler lagrange equations optimal path simplified gamma oe oe oe gamma oe constant determined boundary conditions oe oe differential equation solved differential geometric argument developed atkinson mitchell directly follows 
substitute obtain oe oe gamma oe oe oe express oe gamma oe yields oe dv dt dv doe doe dt oe oe combining gives doe oe oe oe implies oe oe oe gamma oe constant determined 
leads doe oe gamma oe cosh gamma oe constant determined 
follows oe sech gamma combining yields oe dt tanh gamma constant determined boundary conditions 
solution ae tanh oe gamma oe oe sech oe gamma oe gamma oe oe oe gamma oe gamma oe gamma oe gamma oe tanh gamma gamma log gamma gamma implies path oe space form gamma oe reduces oe oe case corresponds special case case solution oe oe oe gammat solution induces optimal prior densities optimal path oe gamma oe gamma gammac oe oe oe gamma oe oe gamma oe min oe oe oe oe max oe max ae max oe oe density oe asymptote oe max oe oe max 
example oe max 
optimal error associated optimal path easily obtained fact path integrand inside free due second transformation see section 
optimal error general var opt oe gamma oe gamma log oe oe oe 
example simplified var opt log oe oe 
atkinson mitchell 

rao distance measure 

bennett 

efficient estimation free energy differences monte carlo data 
journal computational physics 
berg 

new approach spin glass simulations 
physical review letters 
berg 

algorithms order phase transitions 
physics letters 
besag 

spatial interaction statistical analysis lattice systems discussion 
journal royal statistical society 
binder 

theory technical aspects monte carlo simulations 
monte carlo methods statistical physics 
topics current physics vol 
binder ed 
berlin springer verlag 
gelman 

bayesian regression parametric models heteroscedasticity 
advances econometrics 


rao distance 
encyclopedia statistical science supplement volume ed 
kotz johnson read 


path integrals theory condensed helium 
reviews modern physics 
chen shao 

monte carlo methods estimating ratios normalizing constants 
research report department mathematics national university singapore 
chen shao 

estimating ratios normalizing constants densities different dimensions 
statistica sinica appear 
chib 

marginal likelihood gibbs output 
journal american statistical association 
hoover eds 

molecular dynamics simulation statistical mechanical systems 
amsterdam north holland 
courant hilbert 

methods mathematical physics vol 
ii 
new york wiley 
dempster weeks 

combining historical randomized controls assessing trends proportions 
journal american statistical association 
kass raftery wasserman 

computing bayes factors combining simulation asymptotic approximations 
journal american statistical association appear 
evans swartz 

methods approximating integrals statistics special emphasis bayesian integration problems 
statistical science 


free energy computation order phase transition 
molecular dynamics simulation statistical mechanical systems ed 
hoover 
amsterdam north holland 
frankel smit 

understanding molecular simulation 
academic press boston 
gelfand dey 

bayesian model choice asymptotic exact calculations 
journal royal statistical society 
gelfand smith 

sampling approaches calculating marginal densities 
journal american statistical association 
gelman king 

estimating probability events occurred vote matter 
journal american statistical association appear 
gelman meng 

path sampling computing normalizing constants identities theory 
technical report department statistics university chicago 
gelman roberts gilks 

efficient metropolis jumping rules 
bayesian statistics ed 
bernardo 
oxford university press 
gelman rubin 

inference iterative simulation multiple sequences discussion 
statistical science 
geman geman 

stochastic relaxation gibbs distributions bayesian restoration images 
ieee transactions pattern analysis machine intelligence 
geyer 

markov chain monte carlo maximum likelihood 
computing science statistics proceedings rd symposium interface ed 

fairfax station interface foundation 
geyer 

estimating normalizing constants reweighting mixtures markov chain monte carlo 
technical report school statistics university minnesota 
geyer thompson 

constrained monte carlo maximum likelihood dependent data discussion 
journal royal statistical society 
geyer thompson 

annealing markov chain monte carlo applications ancestral inference 
journal american statistical association 
green 

discussion constrained monte carlo maximum likelihood dependent data geyer thompson 
journal royal statistical society 
hastings 

monte carlo sampling methods markov chains applications 
biometrika 
irwin cox kong 

sequential imputation linkage analysis 
proc 
natl 
acad 
sci 
usa 
jensen kong 

blocking gibbs sampling linkage analysis large pedigrees loops 
submitted american journal human genetics 
kass vos 
geometry asymptotic inference 
new york wiley 
kong liu wong 

sequential imputations bayesian missing data problems 
journal american statistical association 
lewis raftery 
estimating factors posterior simulation laplace estimator 
journal american statistical association 
parisi 

simulated tempering new monte carlo scheme 
letters 
meng schilling 

fitting full information factor models empirical investigation bridge sampling 
journal american statistical association 
meng wong 

simulating ratios normalizing constants simple identity theoretical exploration 
statistica sinica 
metropolis rosenbluth rosenbluth teller teller 

equation state calculations fast computing machines 
journal chemical physics 
mitchell 

predictive distances 
test 
neal 

probabilistic inference markov chain monte carlo methods 
technical report crg tr department computer science university toronto 
newton raftery 

approximate bayesian inference weighted likelihood bootstrap discussion 
journal royal statistical society 
ogata 

monte carlo method high dimensional integration 
numerische mathematik 
ogata 

monte carlo method objective bayesian procedure 
annals institute statistical mathematics 
ogata 

evaluation bayesian visualization models computational methods 
research memorandum institute statistical mathematics tokyo 
ogata 

likelihood analysis spatial point patterns 
journal royal statistical society 
ott 

maximum likelihood estimation counting methods mixed models human pedigrees 
am 
hum 
genet 

raftery 

hypothesis testing model selection posterior simulation 
practical markov chain monte carlo ed 
gilks richardson spiegelhalter 
london chapman hall 
rao 

information accuracy attainable estimation statistical parameters 
bull 
calcutta math 
soc 

rao 

distance populations 

ripley 

statistical inference spatial processes 
cambridge university press 
stein 

prediction inference truncated spatial data 
journal computational graphical statistics 
thompson 

likelihood linkage fisher 
annals statistics 


sampling distributions monte carlo estimation umbrella sampling 
journal computational physics 
ii iii iv vi lower bound relative monte carlo errors various simulation estimates log comparing densities importance sampling ii bridge sampling geometric bridge iii bridge sampling optimal bridge iv path sampling geometric path optimal path sampling space vi optimal path sampling oe space 
dotted line best possible error 
mu optimal path oe space parameterization parameterization oe optimal path normalized densities optimal path 
integrating log integrating log estimated negative loglikelihood spatial statistics example replication stein path sampling theta grid draws point 
plots show estimates different paths 
theta theta cdf estimates density function cdf heteroscedasticity parameter election forecasting example non hierarchical model 
dashed line exact density dotted line estimated density path sampling histogram iid simulation draws 
dashed line exact cdf dotted line exactly top dashed line estimated cdf path sampling solid line empirical cdf simulation draws 
theta theta cdf estimates density function cdf heteroscedasticity parameter election forecasting example hierarchical model 
dotted line estimated density path sampling histogram simulation draws metropolis algorithm 
dotted line estimated cdf path sampling solid line empirical cdf simulation draws 
