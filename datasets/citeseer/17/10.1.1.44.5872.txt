refining initial points means clustering bradley computer sciences department university wisconsin madison wi usa cs wisc edu usama fayyad microsoft research redmond wa usa fayyad microsoft com research microsoft com fayyad practical approaches clustering iterative procedure means em converges numerous local minima 
known iterative techniques especially sensitive initial starting conditions 
procedure computing refined starting condition initial efficient technique estimating modes distribution 
refined initial starting condition allows iterative algorithm converge better local minimum 
procedure applicable wide class clustering algorithms discrete continuous data 
demonstrate application method popular means clustering algorithm show refined initial starting points lead improved solutions 
refinement run time considerably lower time required cluster full database 
method scalable coupled scalable clustering algorithm address large scale clustering problems data mining 

background clustering important area application variety fields including data mining statistical data analysis kr br compression vector quantization 
clustering formulated various ways machine learning pattern recognition dh optimization bms si statistics literature kr br 
fundamental clustering problem grouping clustering data items similar 
general approach clustering view density estimation problem br 
assume addition observed variables data item hidden unobserved variable indicating cluster membership data item 
data assumed arrive mixture model mixing labels cluster identifiers hidden 
general mixture model having clusters assigns probability data point follows pr pr called mixture weights 
methods assume number clusters known input 
clustering optimization problem finding parameters associated mixture model parameters components maximize likelihood data model 
probability distribution specified cluster take form 
em algorithm dlr cs known technique estimating parameters general case 
means clustering popular method historically known forgy method macqueen algorithm 
really special case em assumes cluster modeled spherical gaussian distribution data item assigned single cluster mixture weights equal 
note means dh defined numeric continuous valued data requires ability compute mean 
discrete version means exists referred harsh em nh 
kmeans algorithm finds locally optimal solutions minimizing sum distance squared data point nearest cluster center distortion bms si equivalent maximizing likelihood assumptions listed 
various approaches solving problem determining locally optimal values parameters data iterative refinement approaches include em means effective 
basic algorithm works follows initialize model parameters current model decide memberships data items clusters assuming current model correct re estimate parameters current model assuming data memberships obtained correct producing new model current model new model sufficiently close terminate go 
focus initialization step 
initial condition step algorithms define deterministic mapping initial point solution 
means em algorithms converge finitely point set parameter values locally maximal likelihood data model 
deterministic mapping means locally optimal solution sensitive initial point choice 
little prior initialization methods clustering 
dh question plagues hill climbing procedures choice starting point 
unfortunately simple universally solution problem 
repetition different random selections dh appears defacto method 
presentations address issue initialization assume user provided randomly chosen starting points dh kr 
recursive method initializing means running clustering problems mentioned dh 
variant method consists mean entire data randomly perturbing times 
method appear better random initialization case em discrete data mh 
bms values initial means coordinate axes determined selecting densest bins coordinate 
methods initialize em include means solutions hierarchical agglomerative clustering hac dh mh marginal noise 
em discrete data initialized hac marginal noise showed improvement random initialization mh 
remainder focus means algorithm method refine initial point clustering algorithms 
focus means justified standard technique clustering wide array applications way initialize expensive em clustering algorithm cs mh regardless clustering algorithm means employed internally initialization refinement method purpose illustrate refinement procedure evaluate variety clustering algorithms 

refining initial conditions address problem initializing general clustering algorithm limit presentation results means 
method initialization exists mh compare defacto standard method initialization randomly choosing initial starting point 
method applied starting point provided 
solution clustering problem parameterization cluster model 
parameterization performed determining modes maxima joint probability density data placing cluster centroid mode 
clustering approach estimate density attempt find maxima bumps estimated density function 
density estimation high dimensions difficult bump hunting 
propose method inspired procedure refines initial point point closer modes 
challenge perform refinement efficiently 
basic heuristic severely subsampling data naturally bias sample representatives near modes 
general guard possibility points tails appearing subsample 
overcome problem estimate fairly unstable due elements tails appearing sample 
shows data drawn mixture gaussians clusters means 
left full data set right small subsample shown providing information modes joint probability density function 
points right may thought guess possible location mode underlying distribution 
estimates fairly varied certainly exhibit expected behavior 
worthy note separation clusters achieved 
observation indicates solutions obtained clustering small subsample may 
gaussian bumps full sample versus small subsample 
result clustering different samples drawn distribution initialized starting point produced solution indicated 
provide refined initial estimates true means centroids data 
method produces noisy estimates due single small subsamples especially skewed distributions high dimensions 
behavior fairly common clustering small subsamples 
fact surprisingly frequent low dimensions data gaussians illustrate importance problem having initial points 
initial cluster center attracting data may remain empty left starting point empty clusters usually produces better solutions right 
clustering clusters order overcome problem noisy estimates employ procedure 
multiple subsamples say drawn clustered independently producing estimates true cluster locations 
avoid noise associated solutions employ smoothing procedure 
best perform smoothing needs solve problem grouping points solutions having clusters groups optimal fashion 
shows fact data separated gaussians low best case scenario behavior random sampling approach 
note idealized conditions noise algorithm correct number clusters real wold data ideal difficult achieve behavior expected worse 
solutions obtained 
true cluster means depicted 
show points obtained subsample second third fourth 
problem determining grouped grouped 
refinement algorithm refinement algorithm initially chooses small random sub samples data subsamples clustered means proviso empty clusters termination initial centers re assigned sub sample re clustered 
sets cm clustering solutions sub samples form set cm 
cm clustered means initialized cm producing solution fm refined initial point chosen fm having minimal distortion set cm 
clustering cm smoothing cm avoid solutions corrupted outliers included subsample refinement algorithm takes input sp initial starting point data number small subsamples taken data algorithm refine sp data 
cm 
small random subsample data cm sp cm cm cm 
fms 
fm kmeans cm cm fms fms fm 
fm cm fm distortion argmin fm 
return fm true solution solutions trial solutions trial solutions trial solutions trial 
multiple solutions multiple samples 
cluster multiple subsamples subsample multiple sample solutions cluster solutions multiple starts select best solution 
starting point refinement procedure define functions called refinement algorithm kmeans distortion 
kmeans simply call classic means algorithm initial starting point dataset number clusters returning set dimensional vectors estimates centroids clusters 
takes arguments kmeans performs iterative procedure classic means slight modification 
classic kmeans converged clusters checked membership 
clusters membership happens clustering small subsamples corresponding initial estimates empty cluster centroids set data elements farthest assigned cluster center classic means called new initial 
heuristic re assignment motivated termination means empty clusters reassigning empty clusters points farthest respective centers decreases distortion step 
example clusters having zero membership depicted left 
distortion takes set estimates means data set computes sum squared distances data point nearest mean 
scalar measures degree fit set clusters dataset 
kmeans algorithm terminates solution locally optimal distortion function si 
refinement process illustrated diagram 
computational complexity scalability large databases refinement algorithm primarily intended large databases 
working small datasets data sets irvine repository applying classic means algorithm different starting points feasible option 
database size increases especially dimensionality efficient accurate initialization critical 
clustering session data set dimensions tens thousands millions records take hours days 
bfr method scaling clustering large databases specifically targeted databases fitting ram 
show accurate clustering achieved improved results classic means applied appropriately sized random subsample database bfr 
scalable clustering methods obviously benefit better initialization 
method works small samples data initialization fast 
example sample sizes full dataset size trials samples run time complexity time needed clustering full database 
large databases initial sample negligible size 
data set clustering algorithm requires iter iterations cluster time complexity iter 
small subsample typically requires significantly fewer iteration cluster 
empirically safe expect iter iter 
specified budget time user allocates refinement process simply determine number subsamples refinement process 
large small proportion refinement time essentially negligible large desirable property refinement algorithm easily scales large databases 
memory requirement hold small subsample ram 
secondary clustering stage solutions obtained subsamples need held ram 
note assume possible obtain random sample large scale database 
sounds simple reality challenging task 
guarantee records database ordered property random sampling expensive scanning entire database scheme reservoir sampling 
note database environment thinks data table view may exist physical table 
result query may involve joins groupings sorts 
cases database operations impose special ordering result set randomness resulting database view assumed general 
example illustrates sensitivity means solutions initial conditions 
elements sampled gaussians dimensions 
note gaussians case happen centered diagonal 
reason choice dimensionality data goes higher dimensional projection higher dimensional data form making data set easy visualization approach 
simply project data dimensions clusters reveal 
rare property gaussians aligned diagonal projection may result overlaps separability dimensions lost 
left shows random starting point corresponding means solution 
right shows initial random points result refinement procedure random initial point 
note case refined point close true solution 
running means refined point converges true solution 
important point example illustrative purposes 
interesting cases highdimensional data sets data items 
computational results indicate refinement method scales higher dimensions 

results synthetic data data set description synthetic data created dimension 
value data sampled gaussians elements mean vectors true means sampled uniform distribution 
elements diagonal covariance matrices sampled uniform distribution 
number data points sampled chosen times number parameters estimated means 
gaussians evenly weighted 
experimental methodology goal experiment evaluate close means estimated classic means true gaussian means generating synthetic data 
compare initializations 
refinement random starting point chosen uniformly range data 

refinement starting point refined method 
size random subsamples full dataset size number subsamples taken 

refinement single random subsample size 
classic means computed solution full dataset initial points described estimated means matched true gaussian means optimal way prior computing distance estimated means true gaussian means 
true gaussian means means estimated classic means full dataset 
permutation determined left mean solution large red circles random initial point blue squares 
right refined initial point red circles random initial point blue squares 
quantity minimized score solution computed classic means full dataset simply quantity divided average distance true gaussian means estimated means full dataset initial starting point 
experimental results summarizes results averaged random initial points determined uniformly range data 
note means solution computed refined consistently nearer true gaussian means generating dataset means solution computed random initial point refined initial point 
left summarize ratios average distance true gaussian means relative average distance classic solution computed refined initial point 
worthy note results facts 
dimensions refinement method refined better random starting point unrefined point refined subsample refined 

dimension independent trials refinement method better random starting point 

refiner solutions times closer true gaussian means solutions random initial point times closer solution computed refined initial point 
run slightly worse 
explains large variance number dimensions 
exclude data point variance drops range dimensions 
fact minimum ratio occurs datasets small dimensionality maximum ratio occurs datasets large dimensionality indicates utility refinement algorithm datasets 

results real world data computational results classes publicly available real world datasets 
primarily interested large databases hundreds dimensions tens thousands millions records 
data sets method exhibits greatest value 
reason simple clustering session large database time consuming affair 
refined starting condition insure time investment pays 
illustrate large publicly available data set available reuters news service 
data described section 
wanted demonstrate refinement procedure data sets uci machine learning repository 
part data sets easy low dimensional small number records 
small number records feasible perform multiple restarts efficiently 
sample size small sub sampling initialization effective 
data sets interest 
report general experience detailed experience data sets illustrate method advocate useful applied smaller data sets 
emphasize refinement procedure best suited large scale data 
refinement algorithm operates small sub samples database run times needed determine initial starting point speeds convergence full data set orders magnitude total time needed clustering large scale situation 
note cluster labeling associated real world databases correspond distortion measure minimized kmeans 
datasets uci ml repository ratios distances truth dimension refined unrefined refined dimension refined start unrefined start refined 
comparing performance dimensionality increases evaluated method irvine data sets 
results image segmentation data set discuss results data sets 
image segmentation data set data set consists data elements dimensions 
instances drawn randomly database outdoor images sky foliage cement window path grass 
images represented instances 
experimental methodology random initial starting points computed sampling uniformly range data 
compare solutions achieved classic means algorithm starting random initial starting points initial points refined method 
classic means converged quality solution determined 
case synthetic data measure distance true solution truth known 
average class purity cluster measure quality 
measure dependent classification distortion data clusters 
quality scoring methods information gain estimates amount information gained clustering database measured reduction class impurity clusters 
database known classes number data elements class total number data points database 
total entropy database entropy total log convergence classic means algorithm initial starting point weighted entropy computed clustering follows form cluster class matrix th element number elements class belonging cluster notice clustering completely recover assigned classes cluster class matrix permuted identity nonzero structure 
cs size th cluster class entropy th cluster cs cs log weighted entropy entire clustering cs tropy information gain total entropy weighted entropy 
detailed description data see irvine ml data repository www ics uci edu mlearn mlrepository html distortion means estimated classic means algorithm distortion value consider simply sum distance squared data items mean assigned cluster 
smaller value distortion measure indicates model parameters means better fit database means assumptions true 
results image segmentation database average information gain random initial points classic mean refining initial point standard deviation 
average information gain mean initialized refined starting point 
amount information gained average solutions computed refined point time solution computed random initial point 
furthermore average solutions computed refined initial points reduced distortion solutions computed random initial points 
real world datasets evaluated refinement procedure data sets fisher iris star galaxy bright data sets low dimensional sizes small majority results interest 
clustering data sets random initial points refined initial points led approximately equal gain entropy equal distortion measures cases 
observe random starting point leads bad solution refinement takes solution 
admittedly rare cases refinement provide expected improvement 
reuters information retrieval data set demonstrate method real difficult clustering task 
reuters information retrieval data set reuters text classification database derived original reuters data set publicly available part reuters corpus available part reuters corpus reuters carnegie group david lewis data consists documents 
document news article topic earnings commodities acquisitions grain copper 
categories belong higher level categories hierarchy categories 
reuters database consists word counts documents 
hundreds thousands words purposes experiments selected frequently see www research att com lewis reuters readme txt details data set 
occurring words instance dimensions indicating integer number times corresponding word occurs document 
document ir reuters database classified categories 
clustering purposes reflect top level categories 
task find best clustering 
reuters results data set clustering entire database requires large amount time chose evaluate results randomly chosen starting conditions 
results shown chart 
chart shows significant decrease total distortion measure 
average distortion solution obtained starting refined initial point corresponding distortion obtained clustering corresponding randomly chosen initial starting point 
document belongs category categories measure quality achieved clustering measuring gain information categories cluster gives pure clusters informative 
done manner measure entropy image segmentation dataset section 
quality clusters measured average category purity cluster 
case average information gain clusters obtained refined starting point times higher information gain obtained refining initial points 
information gain refined clustering standard deviation 
unrefined initial points resulted average information gain standard deviation equal 

concluding remarks fast efficient algorithm refining initial starting point general class clustering algorithms 
refinement algorithm operates small subsamples database requiring small proportion total memory needed store full database making approach appealing large scale clustering problems 
procedure motivated observation subsampling provide guidance regarding location modes joint probability density function assumed generated data 
initializing general clustering algorithm near modes true clusters follows clustering algorithm iterate fewer times prior convergence 
important clustering methods discussed require full data scan iteration may costly procedure largescale setting 
computational results synthetic gaussian data indicate solutions computed means algorithm refined initial points superior random initial starting points point refined single random subsample 
results small real world image segmentation data set indicate means solution refined points provide twice information solutions computed random initial point 
furthermore average distortion decreased 
computational results reuters database newswire stories dimensions indicate drop distortion 
information gain improved factor times data set 
believe method ability obtain substantial refinement randomly chosen starting points due large part ability avoid empty clusters problem plagues traditional means 
refinement reset empty clusters far points reiterate means algorithm starting point obtained refinement method lead subsequent clustering algorithm bad solution 
intuition confirmed empirical results 
refinement method far context means algorithm 
note method easily generalized algorithms discrete data means defined 
generalized method initializing em algorithm empirical results 
key insight algorithm cluster data cluster subsamples 
algorithm produce model 
model essentially described parameters 
parameters continuous space 
stage clusters clusters step algorithm refine section remains means algorithm step 
reason means goal stage find average sd results reuters data starting points percentage total distortion refined solution relative unrefined solution 
centroid models case harsh membership assignment means desirable 
cory reina help implementation debugging running results large datasets sue dumais mehran sahami making reuters data set available chris meek valuable comments earlier draft 
br banfield raftery model gaussian non gaussian clustering biometrics vol 
pp 

bishop 
neural networks pattern recognition 
oxford university press 
bms bradley mangasarian street 

clustering concave minimization advances neural information processing systems mozer jordan petsche eds 
pp mit press 
bfr bradley fayyad reina scaling clustering algorithms large databases appear proc 
th international conf 
knowledge discovery data mining kdd 
aaai press aug 
cs cheeseman stutz bayesian classification autoclass theory results pp 

mit press 
dlr dempster laird rubin maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
dh duda hart pattern classification scene analysis 
new york john wiley sons 
fhs fayyad haussler stolorz 
mining science data 
communications acm 
fayyad piatetsky shapiro smyth uthurusamy eds 
advances knowledge discovery data mining 
mit press 
fayyad reina bradley refining initialization expectation maximization clustering algorithms appear proc 
th international conf 
knowledge discovery data mining kdd 
aaai press aug 
fisher 
knowledge acquisition incremental conceptual clustering 
machine learning 
forgy cluster analysis multivariate data efficiency vs interpretability classifications biometrics 

fukunaga statistical pattern recognition san diego ca academic press 
jones note sampling tape file 
communications acm vol 
kr kaufman rousseeuw 
finding groups data new york john wiley sons 
macqueen methods classification analysis multivariate observations 
proceedings fifth berkeley symposium mathematical statistics probability 
volume statistics le cam neyman eds 
university california press 
mh meila heckerman 
experimental comparison clustering methods microsoft research technical report msr tr redmond wa 
nh neal hinton view em algorithm justifies incremental sparse variants jordan ed learning graphical models kluwer 
rasmussen clustering algorithms information retrieval data structures algorithms frakes baeza yates eds pp 
new jersey prentice hall 
scott multivariate density estimation new york wiley 
si ismail means type algorithms generalized convergence theorem characterization local optimality 
ieee trans 
pattern analysis machine intelligence vol 
pami 
silverman density estimation statistics data analysis london chapman hall 
thiesson meek chickering heckerman 
learning mixtures bayesian networks microsoft research technical report tr redmond wa 
zhang ramakrishnan livny 
birch new data clustering algorithm applications data mining knowledge discovery vol 

