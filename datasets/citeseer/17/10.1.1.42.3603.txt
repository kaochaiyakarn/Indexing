machine learning pedro domingos department computer science engineering university washington box seattle wa cs washington edu tel fax machine learning focus ill defined problems highly flexible methods ideally suited kdd applications 
ideas machine learning contributes kdd importance empirical validation impossibility learning priori assumptions utility limited search limited representation methods 
machine learning provides methods incorporating knowledge learning process changing combining representations curse dimensionality learning comprehensible models 
kdd challenges machine learning include scaling algorithms large databases cost information learning automating data pre processing enabling rapid development applications 
kdd opens new directions machine learning research brings new urgency 
directions include interfacing human user database system learning non attribute vector data learning partial models learning continuously open ended stream data 
machine learning methods kdd machine learning characterized focus complex representations ill defined problems search methods 
representations studied include described section particularly decision trees sets propositional order rules sets instances clusters concept hierarchies probabilistic networks 
ill defined problems studied include generalizing set tuples absence known model structure clustering combining logic theories domain learning de raedt learning delayed feedback large decision spaces sutton barto 
search methods learning include greedy search gradient descent expectation maximization genetic algorithms forms lookahead pruned breadth search 
types search frequently artificial intelligence best search simulated annealing tend see machine learning reasons discussed 
flexibility machine learning methods suited applications little known priori domain relevant knowledge hard elicit 
flexibility means able successfully learn data gathered purposely designed experimental procedure obtained process goal necessarily knowledge discovery 
flip side theoretical analysis machine learning methods di cult strong guarantees regarding correctness results consequently seldom available 
compensated fact machine learning full power computer experimentally validate methods results 
approach indispensable kdd 
standard elements machine learning empirical toolbox include holdout sets cross validation verify generalization comparison systems large collections benchmark problems blake keogh merz lesion studies elucidate contribution specific system components experiments carefully designed synthetic datasets test specific hypotheses approach kibler langley 
di culties notwithstanding significant body theory developed machine learning see kearns vazirani produced highly successful practical algorithms boosting freund schapire winnow littlestone support vector machines scholkopf burges smola :10.1.1.133.1040
theory form bounds generalization error learner empirical error measure ective size hypothesis space explores cardinality finite spaces dimension infinite ones vapnik 
wealth theoretical results produced possible insisting absolute guarantees error aiming probabilistic ones error greater probability 
machine learning readiness perform generalization absence strong guiding assumptions led face squarely problem needed generalize successfully 
lessons learned form important part kdd practitioner baggage 
generalization impossible absence assumptions biases purely empirical learning chimera mitchell scha er wolpert 
induction seen knowledge lever higher leverage deduction applied force 
converse lesson general purpose learning method method utility contingent assumptions application requires individual attention 
universal laws discovery simple hypotheses accurate known occam razor viewed suspicion scha er webb domingos 
having notion bias explicit machine learning gone study changes bias gordon desjardins combinations di erent biases michalski required practical success 
awareness importance knowledge led development methods explicitly incorporating learning process 
knowledge appear form propositional order logic theory pazzani kibler saitta neri ourston mooney towell shavlik variety weaker forms clearwater provost donoho rendell pazzani mani 
machine learning concerned simultaneously statistical soundness computational ciency 
led explore issues tend arise considered isolation concern kdd applications 
issue deciding kdd algorithm fall lazy eager computational spectrum 
eager extreme traditional modeling approaches fall generalization computation performed learning time 
lazy extreme exemplified nearest neighbor algorithms generalization computation occur performance time 
machine learning gone extremes identifying entire lazy eager spectrum useful design dimension proceeding explore aha 
example rise system autonomously determines best combination rules neighbors domingos 
central issue involves statistical soundness computational ciency ect massive search significance patterns discovered 
thousands millions hypotheses generated course search probability apparently meaningful discoveries simply result chance neglected 
quantifying notoriously di cult 
traditional testing assumes single hypothesis tested 
techniques exist multiple comparison simultaneous inference problems miller sax tend consequently reject valid patterns 
machine learning provides number techniques assessing hypotheses fashion controlling search best computational power falling trap noise mining quinlan cameron jones freund domingos jensen cohen 
open problem hard fast heuristics emerge far apparently impoverished search methods greedy search preferable powerful ones exhaustive search quinlan cameron jones murthy salzberg dietterich 
subfields computer science machine learning constrained finite computational resources constrained finite resource quantity data available learning 
type resource bottleneck application 
computation bottleneck underfitting result data overfitting 
kdd projects large computational resources large quantities data frequent case 
generalization previous observation 
machine learning researchers powerful representations necessarily lead better results holte domingos pazzani 
flexibility price instability 
tradeo captured notions statistical bias variance developed regression extended classification geman bienenstock doursat kong dietterich breiman kohavi wolpert friedman 
related observation computational power better induce multiple models combine searching single best 
approach followed best performing learning methods available including boosting freund schapire bagging breiman stacking wolpert error correcting output codes kong dietterich :10.1.1.133.1040
instance curse dimensionality duda hart 
human intuitions dimensional world fail high dimensions 
expect adding attributes data improve learning provide additional information point reverse typically case 
increasing dimension tuple space exponentially increases quantity data needed populate densely reliable learning 
problem particularly acute large kdd applications attributes number hundreds thousands 
machine learning provides best techniques available high dimensional problems decision tree induction rule induction attribute selection moore lee 
hallmark machine learning focus comprehensible results 
comprehensibility di cult define precisely ultimately subjective essential insights main goal kdd 
machine learning methods produce models comprehensible mathematical training 
example sets 
rules graphical form 
inducing models directly machine learning provides methods converting comprehensible ones neural networks decision trees craven 
research problems machine learning relevant kdd point view accurate generalization machine learning algorithms appropriate ones great variety kdd applications 
volume data available applications far capacity classical machine learning algorithms 
solution adopted simply learn small subset data selection subset typically done ad hoc fashion randomly potentially missing learnable structure 
ort underway enable machine learning algorithms learn orders magnitude data originally designed 
basic requirement algorithms mine large databases linear slightly superlinear running time function database size 
true learning algorithms necessary adapt 
done partly lossless fashion optimizing algorithms changing output typically requires lossy approach developing related algorithms may produce exactly results achieve similar levels performance 
cohen ripper domingos cws algorithm examples approach case rule induction 
learning data large fit main memory learning algorithms able ciently retrieve disk 
implies making sequential passes data opposed randomly accessing making passes possible 
sliq sprint algorithms decision tree induction exemplify approach mehta agrawal rissanen shafer agrawal mehta apriori algorithm finding association rules agrawal mannila srikant toivonen verkamo 
ability learn ciently disk increasingly seen fundamental characteristics machine learning algorithms appropriate kdd 
ideally algorithms constant ram able learn full disk scan making useful results available time start running smyth wolpert advantage additional time scan gracefully improve output 
larger quantities data data reduction inevitable 
classes approaches distinguished sampling summarization 
sampling approach divide examples multiple subsets learn combine results chan stolfo breiman 
start small subset examples iterate adaptively selecting new examples include obtain maximum possible improvement new addition catlett catlett russell 
summarization approaches attempt produce summaries data fit main memory containing information necessary learn ciently accessible form 
summaries may form su cient statistics moore lee graefe fayyad chaudhuri may result applying compression techniques data davies moore 
sampling summarization approaches complementary bradley fayyad reina 
problems large quantities data may available may necessary learn desired concepts required level accuracy oates jensen 
large quantities available may su cient capture relevant structure 
useful methods heuristic nature estimate early data needed 
examples research direction fitting power laws learning curves frey fisher statistical tests slope curves provost jensen oates 
complementary approach attempt estimate bayes rate error rate infinite capacity learner necessarily asymptote dasarathy cortes tumer ghosh learning level reached 
numbers examples attributes classes kdd applications ectively constitute previously unexplored region machine learning space 
empirical nature validity assembled machine learning knowledge new circumstances open question 
important determine elements knowledge need revised 
type adaptation machine learning algorithms needed useful kdd involves aspects kdd problems currently capture 
example cost information 
machine learning algorithms assume errors cost seldom case practice 
related problem imbalanced classes large majority class easy obtain high accuracy useful results 
implicitly misclassifying minority tuples incurs higher cost may hard quantify 
research adapting machine learning algorithms problems growing pazzani turney provost fawcett domingos 
generally important research direction involves methods formulating problems terms amenable machine learning 
integrating cleaning preprocessing learning data stage machine learning application process typically consumes time requires human intervention 
automating stage produce order magnitude speedup process corresponding reduction cost increase number viable applications 
main focus machine learning research classification problems significant motivation belief classifiers building blocks solutions types problems 
problems kdd see section research interface classification problems particularly relevant 
perspective widespread large scale application machine learning creates need rapid development deployment learning systems 
possible computer scientists minor knowledge machine learning produce robust reliable learning component system building 
requires developing libraries standard machine learning components ways putting 
despite number early developments direction gilks thomas spiegelhalter buntine kohavi sommerfield dougherty part clear best 
deciding representations techniques black art 
designer personal preferences long trial error process determines outcome 
imprecise intuitions rules thumb exist theoretical empirical research needed conditions favor approaches current jungle techniques 
results research codified form easily non experts directly incorporated self su cient learning modules 
impact kdd machine learning kdd presents treasure new research opportunities machine learning 
respects allows renewed focus problems original concerns field received decreasing attention time arguably large measure due previous limited availability relevant real world datasets 
machine learning powerful methods find compelling applications today large databases available methods originally developed 
kdd allows machine learning extend ideas motivations new directions develop productive interfaces disciplines databases statistics human computer interfaces visualization information retrieval highperformance computing 
applying machine learning large databases kdd involves qualitative changes go simply scaling algorithms 
example traditional goal creating model represented database give way finding local patterns deviations norm 
compared model building case little theory developed far type problem 
current practical kdd approaches concerned ciency sound generalization 
central concern machine learning analyzing improving approaches potentially fertile ground new theoretical methodological developments 
database large model entirety sampling su cient statistics compression focus attention mechanism necessary 
heuristics sources information brought bear design mechanisms 
large databases gathered period time years learning mind usual assumption 
independently identically distributed data hold 
important research direction account examples independent past data necessarily population 
majority date machine learning focused learning examples represented attribute vectors attribute single number symbol single table contains vectors 
data kdd applications type 
example relational databases typically contain di erent relations tables performing global join reduce losing information seldom computationally feasible 
inductive logic programming de raedt handle data multiple relations simultaneously focuses learning concepts order form addressing doubly di cult problem 
world wide web composed combination text html plus image audio files 
data recorded sensors processes telescopes earth sensing satellites medical business records spatial temporal structure 
customer behavior mining applications central concern companies people hierarchically aggregated occupation characteristics products category simply converting data types attribute vectors learning common today risks missing significant patterns 
case traditional techniques handling types data exist typically quite limited power compared machine learning algorithms available attribute vector case scope extending ideas techniques machine learning direction 
machine learning system appropriate kdd applications able function continuously learning open ended stream data constantly adjusting behavior remaining reliable requiring minimum human supervision 
see increasing number applications type opposed shot standalone applications common today 
early indicators trend commerce sites potentially respond new user di erently learn preferences systems automated trading stock market 
trend apparent increasing preoccupation corporations instantly continuously adapt changing market conditions leveraging purpose distributed data gathering capabilities 
relevant research machine learning widmer kubat learners type address interesting new issues 
smoothly incorporating new relevant data sources come online coping changes decoupling unavailable 
maintaining clear distinction types change learner evolving model simply result accumulating data consequently progressing learning curve result changes environment modeled 
kdd applications learning seldom isolated process 
typically embedded larger system 
addressing multiple problems raises opportunity machine learning expand focus reach 
need ciently integrate learning algorithms underlying database system creates new interface machine learning database research finding query classes executed ciently providing information useful learning simultaneously finding learning approaches ciently executable queries 
relevant questions types sampling ciently supported 
best single sequential scan entire database 
outcome iterative process may query types learning algorithms di erent known today 
interface machine learning databases involves learning purposes meta data available database systems 
example definitions fields constraints values may valuable source background knowledge learning process 
full potential kdd requires integrated data warehouse 
assembling complex time consuming process machine learning partially automate 
example main problems identifying correspondences fields di erent related databases knoblock levy data sources results web searches perkowitz etzioni 
problem formulated learning terms target schema 
examples data schema induce general rules constitutes column 
table source schema 
goal classify columns results potentially constraining 
data cleaning key aspect building data warehouse ers research opportunities machine learning 
large databases invariably contain large quantities noise missing fields 
significantly noise multiple types occurrence varies systematically part database data comes multiple sources 
similarly causes missing information multiple vary systematically database 
research enabling machine learning algorithms deal noise missing data main drivers jump laboratory widespread real world application 
example independent noise missing data typically assumed 
modeling systematic sources error missing information finding ways minimizing impact logical step 
need produce learning results contribute larger scientific business goal leads research problem finding ways integrate goals deeply learning process increasing communication bandwidth learning process clients simply providing say class predictions new examples 
importance kdd interaction human user expert gives new urgency traditional machine learning concerns comprehensibility incorporation background knowledge 
today multiple kdd application domains provide wealth driving problems testing grounds new developments direction 
major application domains molecular biology earth sensing finance marketing fraud detection unique concerns characteristics developing machine learning algorithms specifically occupy increasing number researchers 
machine learning research date dealt circumscribed problem finding classification model single small relatively clean dataset attribute vector form attributes previously chosen facilitate learning goal accurate classification simple defined 
kdd machine learning breaking constraints 
machine learning valuable contributions kdd reciprocated ect 
doubt mutually beneficial interaction continue develop 
author grateful david aha tom dietterich doug fisher rob holte david jensen michalski foster provost ross quinlan saitta valuable comments suggestions regarding article 
word count reading 
aha 
ed 

lazy learning 
boston ma kluwer 
collection research lazy learning 

cohen 

empirical methods artificial intelligence 
mit press 
useful primer experiments machine learning subfields ai 

dasarathy 
ed 

nearest neighbor nn norms nn pattern classification techniques 
los alamitos ca ieee computer society press 
source main papers decades pattern recognition research learning concepts represented sets instances 

de raedt 
ed 

advances inductive logic programming 
amsterdam netherlands ios press 
collection articles learning examples background knowledge concepts expressed subset order logic 

dietterich 

machine learning research current directions 
ai magazine 
overview developments main subareas machine learning including scaling algorithms large databases 
useful complement article 

duda hart 

pattern classification scene analysis 
new york ny wiley 
classic textbook statistical pattern recognition 

jordan 
ed 

learning graphical models 
boston ma kluwer 
tutorials research learning probabilistic representations 

kearns vazirani 

computational learning theory 
cambridge ma mit press 
accessible theory machine learning 

kibler langley 

machine learning experimental science 
proceedings third european working session learning 
london uk pitman 
reprinted shavlik dietterich 
eds 
readings machine learning san mateo ca morgan kaufmann 
useful methodology machine learning 

langley 

elements machine learning 
san mateo ca morgan kaufmann 
systematic introductory presentation field 

michalski bratko kubat 
eds 

machine learning data mining methods applications 
new york ny wiley 
collects variety research interface machine learning kdd 

michalski carbonell mitchell 
eds 

machine learning artificial intelligence approach vols 

palo alto ca tioga 
series books containing early research 

michalski tecuci 
eds 

machine learning multistrategy approach 
san mateo ca morgan kaufmann 
continuation previous series focus combining multiple machine learning biases background knowledge 

michie spiegelhalter taylor 
eds 

machine learning neural statistical classification 
new york ny ellis horwood 
describes large scale experimental comparison algorithms 
contains introductions algorithms discussion strengths weaknesses 
print available online www leeds ac uk charles statlog 

mitchell 

machine learning 
new york ny mcgraw hill 
standard introductory machine learning textbook 

provost kolluri 

survey methods scaling inductive algorithms 
data mining knowledge discovery 
excellent overview scaling research 
place start re looking way scale algorithm 

quinlan 

programs machine learning 
san mateo ca morgan kaufmann 
describes widely machine learning system 

scholkopf burges smola 

advances kernel methods support vector machines 
cambridge ma mit press 
expanded papers workshop support vector machines 

shavlik dietterich 
eds 

readings machine learning 
san mateo ca morgan kaufmann 
collection classic machine learning papers 

sutton barto 

reinforcement learning 
cambridge ma mit press 
active research areas machine learning focus learning delayed feedback 

vapnik 

nature statistical learning theory 
new york ny springer 
vapnik chervonenkis dimension theory structural risk minimization application development support vector machines 

weiss kulikowski 

computer systems learn classification prediction methods statistics neural nets machine learning expert systems 
san mateo ca morgan kaufmann 
older textbook compares machine learning algorithms alternative techniques 
machine learning journal published kluwer single important repository research field 
machine learning articles appear artificial intelligence journal online journal artificial intelligence research www cs washington edu research jair home html neural computation journal ieee transactions pattern analysis machine intelligence 
main conference field international conference machine learning proceedings published morgan kaufmann 
machine learning research reported european conference machine learning international joint conference artificial intelligence national conference artificial intelligence aaai european conference artificial intelligence annual conference neural information processing systems international workshop multistrategy learning international workshop artificial intelligence statistics 
research theory machine learning appears international conference computational learning theory european conference computational learning theory 
useful online machine learning resources include uci repository machine learning databases www ics uci edu mlearn mlrepository html list home pages machine learning researchers maintained david aha www aic nrl navy mil aha people html online bibliographies subareas machine learning maintained peter turney www iit nrc ca bibliographies machine learning list maintained michael pazzani mailto ml request ics uci edu ai statistics list maintained doug fisher mailto uwaterloo ca subscribe ai stats 
publicly available machine learning software includes mlc weka libraries respectively www sgi com technology mlc www cs waikato ac nz ml weka 
agrawal mannila srikant toivonen verkamo 

fast discovery association rules 
fayyad piatetsky shapiro smyth uthurusamy eds advances knowledge discovery data mining pp 

menlo park ca aaai press 
aha 
ed 

special issue lazy learning 
artificial intelligence review 
blake keogh merz 

uci repository machine learning databases machinereadable data repository 
irvine ca department information computer science university california irvine 
www ics uci edu mlearn mlrepository html bradley fayyad reina 

scaling clustering algorithms large databases 
proceedings fourth international conference knowledge discovery data mining pp 

new york ny aaai press 
breiman 

bagging predictors 
machine learning 
breiman 

bias variance arcing classifiers tech 
rep 
berkeley ca statistics department university california berkeley 
breiman 

pasting bites prediction large data sets line tech 
rep 
berkeley ca statistics department university california berkeley 
buntine 

operations learning graphical models 
journal artificial intelligence research 
catlett 

machine learning large databases 
unpublished doctoral dissertation basser department computer science university sydney sydney australia 
chan stolfo 

learning arbiter combiner trees partitioned data scaling machine learning 
proceedings international conference knowledge discovery data mining pp 

montreal canada aaai press 
clearwater provost 

rl tool knowledge induction 
proceedings second ieee international conference tools artificial intelligence pp 

san jose ca ieee computer society press 
cohen 

fast ective rule induction 
proceedings twelfth international conference machine learning pp 

tahoe city ca morgan kaufmann 
cortes 

prediction generalization ability learning machines 
unpublished doctoral dissertation department computer science university rochester rochester ny 
craven 

extracting comprehensible models trained neural networks 
unpublished doctoral dissertation department computer sciences university wisconsin madison madison wi 
dasarathy 
ed 

nearest neighbor nn norms nn pattern classification techniques 
los alamitos ca ieee computer society press 
davies moore 

bayesian networks lossless compression data mining 
proceedings fifth international conference knowledge discovery data mining 
san diego ca acm press 
de raedt 
ed 

advances inductive logic programming 
amsterdam netherlands ios press 
dietterich 

overfitting machine learning 
computing surveys 
domingos 

linear time rule induction 
proceedings second international conference knowledge discovery data mining 
portland aaai press 
domingos 

unifying instance rule induction 
machine learning 
domingos 

occam sharp blunt 
proceedings fourth international conference knowledge discovery data mining pp 

new york ny aaai press 
domingos 

process oriented estimation generalization error 
proceedings sixteenth international joint conference artificial intelligence 
stockholm sweden morgan kaufmann 
domingos 

metacost general method making classifiers cost sensitive 
proceedings fifth international conference knowledge discovery data mining 
san diego ca acm press 
domingos pazzani 

optimality simple bayesian classifier zero loss 
machine learning 
donoho rendell 

constructive induction fragmentary knowledge 
proceedings thirteenth international conference machine learning pp 

bari italy morgan kaufmann 
duda hart 

pattern classification scene analysis 
new york ny wiley 
freund 

self bounding learning algorithms 
proceedings eleventh annual conference computational learning theory 
madison wi morgan kaufmann 
freund schapire 

experiments new boosting algorithm 
proceedings thirteenth international conference machine learning pp 

bari italy morgan kaufmann 
frey fisher 

modeling decision tree performance power law 
proceedings uncertainty seventh international workshop artificial intelligence statistics pp 

fort lauderdale fl morgan kaufmann 
friedman 

bias variance loss curse dimensionality 
data mining knowledge discovery 
geman bienenstock doursat 

neural networks bias variance dilemma 
neural computation 
gilks thomas spiegelhalter 

language program complex bayesian modelling 
statistician 
gordon desjardins 
eds 

special issue evaluation selection biases machine learning 
machine learning 
graefe fayyad chaudhuri 

cient gathering su cient statistics classification large sql databases 
proceedings fourth international conference knowledge discovery data mining pp 

new york ny aaai press 
holte 

simple classification rules perform commonly datasets 
machine learning 
jensen cohen 

multiple comparisons induction algorithms 
machine learning 
appear kearns vazirani 

computational learning theory 
cambridge ma mit press 
kibler langley 

machine learning experimental science 
proceedings third european working session learning 
london uk pitman 
sax 

multiple comparisons 
beverly hills ca sage 
knoblock levy 
eds 

proceedings aaai workshop ai information integration 
madison wi aaai press 
kohavi sommerfield dougherty 

data mining mlc machine learning library 
international journal artificial intelligence tools 
kohavi wolpert 

bias plus variance decomposition zero loss functions 
proceedings thirteenth international conference machine learning pp 

bari italy morgan kaufmann 
kong dietterich 

error correcting output coding corrects bias variance 
proceedings twelfth international conference machine learning pp 

tahoe city ca morgan kaufmann 
littlestone 

learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 
mehta agrawal rissanen 

sliq fast scalable classifier data mining 
proceedings fifth international conference extending database technology pp 

avignon france springer 
michalski 
eds 

proceedings third international workshop multistrategy learning 
ferry va aaai press 
miller jr 

simultaneous statistical inference nd ed 
new york ny springer 
mitchell 

need biases learning generalizations tech 
rep 
new brunswick nj computer science department rutgers university 
moore lee 

cient algorithms minimizing cross validation error 
proceedings eleventh international conference machine learning pp 

new brunswick nj morgan kaufmann 
moore lee 

cached su cient statistics cient machine learning large datasets 
journal artificial intelligence research 
murthy salzberg 

lookahead pathology decision tree induction 
proceedings fourteenth international joint conference artificial intelligence pp 

montreal canada morgan kaufmann 
catlett russell 

decision theoretic subsampling induction large databases 
proceedings tenth international conference machine learning pp 

amherst ma morgan kaufmann 
oates jensen 

ects training set size decision tree complexity 
proceedings fourteenth international conference machine learning pp 

madison wi morgan kaufmann 
ourston mooney 

theory refinement combining analytical empirical methods 
artificial intelligence 
pazzani kibler 

utility knowledge inductive learning 
machine learning 
pazzani mani 

concise colorful learning intelligible rules 
proceedings third international conference knowledge discovery data mining pp 

newport beach ca aaai press 
pazzani merz murphy ali hume brunk 

reducing misclassification costs 
proceedings eleventh international conference machine learning pp 

new brunswick nj morgan kaufmann 
perkowitz etzioni 

category translation learning understand information internet 
proceedings fourteenth international joint conference artificial intelligence pp 

montreal canada morgan kaufmann 
provost fawcett 

analysis visualization classifier performance comparison imprecise class cost distributions 
pp 

newport beach ca aaai press 
provost jensen oates 

optimal progressive sampling 
proceedings fifth international conference knowledge discovery data mining 
san diego ca acm press 
quinlan cameron jones 

oversearching layered search empirical learning 
proceedings fourteenth international joint conference artificial intelligence pp 

montreal canada morgan kaufmann 
saitta neri 

multistrategy learning theory revision 
machine learning 
scha er 

overfitting avoidance bias 
machine learning 
scha er 

conservation law generalization performance 
proceedings eleventh international conference machine learning pp 

new brunswick nj morgan kaufmann 
scholkopf burges smola 

advances kernel methods support vector machines 
cambridge ma mit press 
shafer agrawal mehta 

sprint scalable parallel classifier data mining 
proceedings second international conference large databases pp 

bombay india morgan kaufmann 
smyth wolpert 

anytime exploratory data analysis massive data sets 
proceedings third international conference knowledge discovery data mining pp 

newport beach ca aaai press 
sutton barto 

reinforcement learning 
cambridge ma mit press 
towell shavlik 

knowledge artificial neural networks 
artificial intelligence 
tumer ghosh 

classifier combining analytical results implications 
proceedings aaai workshop integrating multiple learned models improving scaling machine learning algorithms pp 

portland aaai press 
turney 

cost sensitive classification empirical evaluation hybrid genetic decision tree algorithm 
journal artificial intelligence research 
vapnik 

nature statistical learning theory 
new york ny springer 
webb 

experimental evidence utility occam razor 
journal artificial intelligence research 
widmer kubat 
eds 

special issue context sensitivity concept drift 
machine learning 
wolpert 

stacked generalization 
neural networks 
wolpert 

lack priori distinctions learning algorithms 
neural computation 

