speaker adaptation constrained estimation gaussian mixtures digalakis neumeyer sri international ravenswood ave menlo park ca sa trend automatic speech recognition systems continuous mixture density hidden markov models hmms 
despite recognition performance systems achieve average large vocabulary applications large variability performance speakers 
performance degrades dramatically user radically different training population 
popular technique improve performance robustness speech recognition system adapting speech models speaker generally channel task 
continuous mixture density hmms number component densities typically large may feasible acquire sufficient amount adaptation data robust maximum likelihood estimates 
solve problem propose constrained estimation technique gaussian mixture densities 
algorithm evaluated large vocabulary wall street journal corpus native nonnative speakers american english 
nonnative speakers recognition error rate approximately halved small amount adaptation data approaches speaker independent accuracy achieved native speakers 
native speakers recognition performance adaptation improves accuracy speaker dependent systems times training data 
recognition error rates ranging achieved word open vocabulary recognition task wall street journal wsj corpus hidden markov models hmms continuous mixture observation densities 
recognition performance far satisfactory usable recognition applications 
recognition accuracy sensitive speaker variability degrade move lab field 
speaker channel task dependent solutions require excessive collection training data decrease system utility portability 
popular technique improve performance robustness speech recognition system adapting speech model speaker channel task 
consider adaptation speaker techniques modified levels 
novel adaptation techniques state art continuous mixture density hmms 
shown hmms continuous density probability distributions achieve better recognition performance distributions 
refer group gaussians form gaussian mixture distribution collection groups hmm systems arbitrary degree sharing hmms 
degree sharing significantly affects recognition performance 
hmm systems sharing typically smaller number gaussians larger total number gaussians systems fewer 
increase number gaussians usually compensated decrease number mixture weights systems sharing smaller number parameters 
suited adaptation tied mixture hmms single systems hmm states sharing gaussians mixture distributions 
families adaptation schemes proposed past 
transforms speaker feature space match space training population 
transformation applied directly features speech models 
second main family adaptation algorithms follows bayesian approach information encapsulated prior distributions 
transformation approach advantage simplicity 
addition number free parameters small transformation techniques adapt user small amount adaptation speech quick adaptation 
bayesian approach usually nice asymptotic degree sharing refer average number distinct hmm states share gaussians output distributions 
properties speaker adaptive performance converge speaker dependent performance amount adaptation speech increases 
adaptation rate usually slow 
hmms small degree sharing large total number gaussians impractical expect adaptation data obtain robust maximum likelihood ml estimates gaussians 
deal problem adapting large number gaussians small amounts adaptation speech new algorithm constrained estimation 
algorithm viewed estimating transformation speaker independent models maximizing likelihood adaptation data 
contrast previous adaptation schemes feature transformations algorithm desirable property text independent 
require new speaker record sentences text recorded previously speakers require time warping new speaker utterances uttered speakers 
bayesian adaptation techniques limited amount speaker specific data combined speaker independent models optimal manner 
maximum posteriori map reestimation continuous gaussian mixture hmms equivalent linearly combining speaker dependent sufficient statistics priors 
typically gaussians speaker independent models generated adaptation data adapted speaker 
behavior may problematic continuous hmms large number gaussians small percentage gaussians seen adaptation data 
contrast adaptation scheme adapt gaussian requiring training examples specific gaussian exist adaptation data 
constrained reestimation method algorithm able extrapolate adapt gaussians data generated gaussians neighboring 
organized follows 
section presents algorithm constrained estimation gaussian mixtures expectation maximization em algorithm 
give solution static case single random vector modeled gaussian mixture density dynamic case vector process modeled hmms gaussian mixtures output distributions 
section discuss application main algorithm speaker adaptation problem 
section describes experiments presents results wsj corpus 
discussion results appear section 
constrained estimation gaussian mixtures speaker adaptation paradigm fits approach hmms shared gaussian codebooks employ transformation models best correspond available adaptation data 
transformation efficiently achieved assuming gaussians speaker adapted system obtained transformation corresponding gaussians 
transformation unique shared different 
choose apply transformation distribution level transforming data directly em algorithm estimate transformation parameters maximizing likelihood adaptation data 
advantage em algorithm estimate transformation data 
eliminates need form time alignment new speaker data training speaker data previous techniques needed 
estimation transformation viewed constrained estimation gaussian mixtures 
estimation single gaussian mixture better illustrate constrained gaussian estimation method estimation formulae single gaussian mixture density 
section extend method mixture densities observation distributions hidden markov models 
consider gaussian mixture density form 

am model parameters number mixture components constraint 

assume parameters fixed matrices positive definite 
model equivalent assuming random vector obtained affine transformation ay unobserved vector known mixture density 

ml estimation constrained gaussian mixture model equivalent estimating regression parameters observations dependent variable knowledge distribution unobserved variable shown em algorithm obtain ml estimates parameters gaussian mixture density unconstrained case 
em algorithm estimate model parameters constrained case 
em iteration new parameter estimates obtained maximizing auxiliary function arg max omega jx previous parameter estimates denotes collection observed samples omega denotes collection corresponding unobserved mixture indices iteration em algorithm involves expectation step maximization step step 
appendix show step involves computation sufficient statistics 
ja sigma 
ja gamma gamma 
ja posterior probabilities computed bayes rule 
ja 


dimensional case case diagonal covariances diagonal scaling matrix quantities sigma oe scalars 
case step equivalent solving quadratic equation see appendix 
gamma 
gamma 
ab 

gamma 
oe offset gamma 
oe straightforward verify equation real roots 
general multidimensional case covariances scaling matrix diagonal step equivalent solving system second order equations 
iterative schemes may general case 
estimation gaussian mixture density hmms constrained estimation gaussian mixtures easily extended dynamic case time varying processes underlying discrete markovian state 
specifically consider finite state process modeled order markov chain transition probabilities ij jjs gamma 
state process generate observed process stochastic mapping js model process hidden markov model 
reestimation formulae hmms gaussian mixture output distributions form js 

js gaussian codebook index 
assume collection indexed mapping hmm state fl 
inverse image fl gamma set hmm states map set hmm states share mixture components 
static case assume parameters fixed matrices positive definite free parameters mixtures transformation parameters simplicity assumed dependent 
em algorithm estimate parameters model 
unobserved variables hmm state mixture index em algorithm case takes form known baum welch algorithm 
formulae conventional reestimation hmms gaussian mixture densities derived applying baum welch algorithm see example 
case constrain estimation gaussians reestimation formulae different training procedure baum welch algorithm summarized 

initialize transformations set 
step perform iteration forward backward algorithm speech data gaussians transformed current value transformations 
component gaussians collect sufficient statistics fl gamma ae oe sigma fl gamma ae oe gamma gamma fl gamma ae oe ae jx probability state time current hmm parameters oe posterior probability oe 
ja 
step compute new transformation parameters estimation formulae 

iteration goto 
application speaker adaptation adaptation gaussian codebooks continuous mixture density hmms large number component mixtures impractical assume adaptation data available independent reestimation component densities 
constrained estimation overcome problem components mixture group mixtures tying transformations transformed jointly 
see method applied adaptation assume speaker independent si hmm model si vector process observation densities form si js 

js adaptation system achieved jointly transforming gaussians 
specifically assume index hmm state speaker dependent vector process obtained underlying process transformation 
case speaker adapted sa observation densities form sa js 

js transformation parameters need estimated adaptation 
chose groups affine transformations model underlying relationship speaker independent speaker adapted densities reasons constraining reestimation gaussian mixtures affine transformations results mathematically tractable problem second increasing number transformations achieve approximation underlying relationship 
reestimation formulae full rank transformation experiments independent constraints diagonal covariances scaling matrices 
algorithm modified asymptotically approach speaker dependent sd training amount adaptation speech increased 
achieve setting threshold constraints individual gaussians number samples assigned larger threshold 
gaussians sufficiently large amount adaptation speech reestimated independently gaussians little adaptation data adapted groups 
addition total amount adaptation data particular prespecified threshold identity transformation gaussians 
gaussian adaptation algorithm instance baum welch algorithm hmms constrained mixture densities implemented efficiently 
specifically sufficient statistics case unconstrained mixture densities 
step iteration adaptation algorithm requires computation storage statistics equivalent step algorithm unconstrained mixture densities 
computational requirements step small compared step 
adaptation mixture weights constrained estimation algorithm described previous sections adapt component densities observation distributions 
set parameters continuous mixture hmm speech recognizer comprised mixture weights 
js 
high degree sharing mixture components different hmm states number small distributions corresponding different hmm states mainly distinguished different mixture weights 
hmms sharing increases shift focus discrimination different states mainly achieved component densities 
significance adapting mixture weights varies depending type sharing 
systems small degree sharing usually perform better adaptation gaussians may greater effect recognition performance 
may prove beneficial incorporate adaptation scheme form adaptation mixture weights 
technique chose characterized pseudo bayesian 
specifically adapting component gaussians described section additional pass adaptation data performed forward backward algorithm 
sd counts mixture weights accumulated linearly combined si forwardbackward counts fashion similar reported 
weighting factor determines relative prominence adaptation data 
algorithm viewed pseudo bayesian adaptation scheme relative contribution si prior knowledge sd adaptation data determined experimentally 
experiments evaluated adaptation algorithms large vocabulary wall street journal corpus 
experiments carried sri decipher tm speech recognition system configured feature front outputs cepstral coefficients gamma cepstral energy second order differences 
cepstral features computed fft filterbank subsequent cepstral mean normalization sentence basis performed 
hidden markov models arbitrary degree gaussian sharing different hmm states described 
fast experimentation progressive search framework initial speaker independent recognizer bigram language model outputs word lattices utterances test set 
word lattices speaker dependent speaker adapted models bigram language model 
performed series experiments native nonnative speakers american english respectively 
experiments performed word closed vocabulary task described 
adaptation native speakers compare si sd sa recognition performance native speakers performed initial study adaptation algorithms phase wsj corpus 
mixture hmm systems context independent phone sharing mixture components systems phone 
speaker independent systems trained sentences male speakers 
different cepstral features modeled independent observation streams codebook gaussians vector features gaussians scalar energy features 
total phonetic models states 
number distinct output distributions clustered fold reduction statebased clustering compact system fewer parameters better suited adaptation 
performance adaptation algorithm evaluated sentences male speakers varying amounts training adaptation sentences 
si word error rate speakers clustering distributions including deletions insertions 
clustering degraded slightly si performance word error 
phonetically tied mixture system phonetic classes total number gaussians system vector scalar feature respectively 
able train speaker dependent system speakers utterances sd error rate 
tested adaptation algorithm small amount adaptation data phonetically balanced utterances common speakers word error rate adaptation 
adaptation sentences gap si sd performance overcome 
evaluated sa system performance varying amounts adaptation data speakers 
results summarized 
adaptation sentences adaptation scheme achieves performance speaker dependent system times speaker specific training data 
sd training data adaptation data sa system achieves reduction error rate si system reduction sd system 
difficult compare adaptation schemes appeared literature 
results usually confounded differences ffl task complexity 
includes vocabulary size strict language model noise conditions ffl type recognition system baseline accuracy 
systems exhibit si performance may show small improvement due adaptation ffl fluency speakers test sample size 
see section adaptation helps nonnative speakers significantly native speakers 
order overcome problems compare algorithm previous implemented adaptation algorithm described 
algorithm suitable tied mixture systems adaptation gaussians achieved unconstrained baum welch reestimation mixture weight adaptation 
built si tied mixture system si sentence sa word error rates test set respectively 
numbers higher word error rates observed si phonetically tied mixtures adaptation algorithm 
reasons mentioned qualitative comments comparing algorithm previous 
lee gauvain obtained similar sd sa recognition performance word error rate sentences word arpa resource management rm task context independent models 
adaptation algorithm achieved lower error sd training wsj sentences 
adaptation sentences method reduced si word error rate 
case observed reduction 
differences may attributed different domains amount initial si training data quality si models 
huang lee reported adaptation results rm task 
simple gaussian reestimation scheme proposed pseudo bayesian adaptation method mixture weights similar 
different test set lee gauvain reported si word error rate sd word error rate sd training sentences 
sa results adaptation sentences respectively 
error rates general lower ones 
consequence huang lee error rate reduction adaptation sentences smaller lee gauvain comparable 
huang lee method achieves sentence sd performance adaptation sentences sentence sa error rate corresponding sd error rate 
case achieved sentence sd performance adaptation sentences sentence sa error rate lower corresponding sd error rate 
adaptation nonnative speakers speaker adaptation important technology outlier speakers si error rate high practical application testing adaptation algorithm spoke task phase wall street journal corpus focused improving recognition performance nonnative speakers american english adaptation 
phase corpus available series experiments si systems built training utterances male speakers 
reduce computing requirements tuned algorithm male speakers phase wsj development data set 
bigram language model experiments 
evaluation data set run development phase 
data set includes test sentences phonetically balanced adaptation sentences speaker 
speakers selected fluency english covering strong light accents 
tested different systems determine optimal degree gaussian sharing task 
systems context dependent phonetic models states 
context dependency modeled words preliminary experiments modeling coarticulation word boundaries improve recognition performance nonnative speakers 
numbers systems phone 
consisted mixture gaussian distributions 
si sa performance shown table 
adaptation applied sequentially gaussian distributions mixture weights 
general increase number increases computational requirements recognition larger number gaussian likelihoods need evaluated 
gaussian evaluations may sped methods clustering vector quantization 
hmms arbitrary degree mixture tying different hmm states selected agglomerative clustering procedure 
degree tying small consequently number large systems table large number transformations may estimated adaptation 
overcome problem tying transformations different agglomerative clustering scheme construction suitable 
node tree generated clustering procedure corresponds set states leaves tree corresponding single hmm states 
degree tying particular system represented cut tree 
location cut determined stopping criterion additional motivation authors nonnative speakers american english 
authors included test sets section experiments 
agglomerative clustering 
want smaller number transformations number system somewhat relax stopping criterion cluster aggressively determine second cut higher level tree 
nodes original cut fall node new cut share transformation 
third column table indicates number transformations gaussian distributions 
systems transformation 
remaining systems large numbers grouped transformations order reduce number parameters estimated 
si word error rates various systems similar ranging 
tying transformations adaptation systems reducing number transformations sa error rates reduced respectively 
sa error rate lowest systems examined average improvement due adaptation algorithm speakers 
evaluate relative contribution stages adaptation scheme evaluated sa error rate best system mixture weight adaptation disabled 
adapting gaussian codebooks constrained estimation method sa word error rate 
continuous hmms performance gain adaptation achieved adapting gaussian codebooks 
table shows results november arpa evaluation set system 
case improvement 
difference development evaluation test sets attributed large variability inherent outlier speakers relatively small test set size 
evaluate performance algorithm tested full november evaluation set including male female speakers 
si word error rate reduced adaptation algorithm 
result comparable obtained kubala official november evaluation trigram language model 
compare nonnative performance adaptation native speakers evaluated systems speakers section 
results summarized table 
see si performance detailed systems larger number gaussian distributions significantly better detailed ones 
important difference nonnative results 
plausible explanation nonnative case additional detail continuous systems needed speakers different training population 
observe natives sa error rate utterances si opposed improvement observed 
improvement decrease word error observed native speakers experiments phase wsj corpus uniform speakers 
phase wsj corpus times training data phase corpus conclude large amount si training data available adaptation nearly effective speakers drawn population matches training data outlier speakers 
si sa word error rates best systems native nonnative speakers summarized table 
si word error rate nonnative speakers times native speakers 
adapting adaptation utterances nonnative sa error rate approximately factor higher native speakers 
summary new algorithm maximum likelihood ml estimation mixture gaussians subject constraint means covariances obtained transformation needs estimated fixed set component densities 
constrained estimation method suited speaker adaptation problem continuous mixture density hmms large number component densities hard estimate unconstrained fashion small amount adaptation data 
tested algorithm large vocabulary wsj corpus native nonnative speakers american english variety recognition systems 
native speakers recognition performance adaptation similar speaker dependent systems times training data 
small amounts adaptation data utterances average length seconds decrease rate native speakers approximately larger nonnative speakers ranging 
important result speaker independent word error rates outlier speakers nonnative speakers times high native speakers 
speaker adaptation outlier nonnative speakers automatic speech recognition performance levels similar native speakers 
algorithm propose significantly increase usability continuous mixture density hmm systems 
wsj database results serve benchmark researchers want evaluate nonnative speaker adaptation techniques data 
studied relationship adaptation behavior degree mixture sharing continuous hmm systems 
large amount training continuous systems large number gaussians perform better typical native speakers speaker independent speaker adapted modes 
situation different atypical nonnative speakers 
increasing detail modeling context dependencies beneficial nonnative speakers follow typical coarticulation patterns observed native speakers 
result compact systems exhibit better adaptation performance fewer parameters adapt 
results study encouraging currently investigating methods extend adaptation algorithm unsupervised manner prompting text available adaptation 
appendix derivation expectation maximization steps apply expectation maximization em algorithm estimation gaussian mixture rewrite auxiliary function omega jx 

jx log xj 
log 
parameters consist transformation parameters second term summation depend em iteration need maximize term 
known joint log likelihood collection samples drawn independently multivariate normal distribution mean covariance sigma expressed log gamma log sigmaj gamma gamma sigma gamma gamma gamma sigma gamma sigmag sigma sample mean covariance respectively number samples 
similar expression derived term expected log likelihood 
note expectation written omega jx 

jx gamma log sigma gamma gamma sigma gamma gamma 

jx gamma log sigma sigma gamma sigma gamma gamma sigma gamma gamma sigma gamma means covariances constrained am sigma expanding summation write 
gamma 
jx log sigma 
jx sigma gamma sigma gamma 
jx xg gamma 
jx sigma gamma gamma 
jx sigma gamma define sufficient statistics 
ja 
ja sigma 
ja gamma gamma rewrite equation 
gamma log sigma gamma gamma sigma gamma gamma sigma gamma gamma 
jx sigma gamma 
gamma log sigma gamma gamma sigma gamma gamma gamma sigma gamma 
jx xx gamma gamma 
log sigma gamma sigma gamma gamma sigma gamma sigma second equation matrix identity ax matrix vector third equation definition statistic sigma equations computation sufficient statistics comprise step algorithm summarized 
derive step algorithm rewrite equation transformation parameters gamma 
log js log jaj gamma gamma gamma gamma gamma gamma gamma gamma gamma gammat gamma gamma sigma assumed transformation matrix full rank 
gradient respect transformation parameters find system equations 
ae gamma gamma gamma gamma gamma gamma gamma gamma gamma sigma oe 
gammat gamma gamma gamma 
gammat gamma gamma gamma am assumption diagonal covariance matrices diagonal transformation matrices multidimensional case equivalent set dimensional problems solved independently 
auxiliary function written case gamma 
log log gamma am gamma oe maximizing quantity respect transformation parameters easily derive equations 
acknowledgments gratefully acknowledge support arpa office naval research contracts 
government certain rights material 
opinions findings recommendations expressed material authors necessarily reflect views government funding agencies 
colleagues mike cohen hy mitch weintraub comments improved quality manuscript 
anderson multivariate statistical analysis nd edition wiley new york 
bahl jelinek mercer maximum likelihood approach continuous speech recognition ieee trans 
pattern analysis machine intelligence vol 
pami pp 
march 
baum petrie soules weiss maximization technique statistical analysis probabilistic functions finite state markov chains ann 
math 
stat vol 
pp 

bellegarda robust speaker adaptation piecewise linear acoustic mapping proceedings icassp pp 
san fransisco ca 
brown 
lee spohrer bayesian adaptation speech recognition proceedings icassp pp 
boston ma 
chollet spectral transformations canonical correlation analysis speaker adaptation asr proceedings icassp pp 
tokyo japan 
dempster laird rubin maximum likelihood estimation incomplete data journal royal statistical society vol 
pp 

digalakis generalized mixture tying continuous hidden markov model speech recognizers submitted ieee trans 
speech audio processing june 
furui unsupervised speaker adaptation method hierarchical speaker clustering proceedings icassp pp 
glasgow scotland 
huang 
lee speaker independent speaker dependent speech recognition ieee trans 
speech audio processing vol 
pp 
april 

hwang huang modeling markov states proceedings icassp pp 
san fransisco ca 
jelinek continuous speech recognition statistical methods ieee proceedings vol 
pp 
april 

juang maximum likelihood estimation mixture multivariate stochastic observations markov chains technical journal vol july august 
kubala hub spoke paradigm csr evaluation proceedings hlt workshop princeton nj march 

lee 
lin 
juang study speaker adaptation parameters continuous density hidden markov models ieee trans 
acoust speech signal proc vol 
assp pp 
april 

lee 
gauvain speaker adaptation map estimation hmm parameters proceedings icassp pp 
ii ii minneapolis minnesota 
digalakis weintraub large vocabulary dictation sri decipher tm speech recognition system progressive search techniques proceedings icassp pp 
ii ii minneapolis minnesota 
nakamura shikano comparative study spectral mapping speaker adaptation proceedings icassp pp 
albuquerque nm 
pallet benchmark tests arpa spoken language program proceedings hlt workshop princeton nj march 
paul baker design wall street journal csr corpus proceedings darpa speech natural language workshop pp 
feb 
redner walker mixture densities maximum likelihood em algorithm siam review vol 
pp 
april 
nahamoo speaker adaptation vq prototype modification ieee trans 
speech audio processing vol 
pp 
january 
schwartz chow kubala rapid speaker adaptation probabilistic spectral mapping proceedings icassp pp 
dallas tx 
number adaptation sentences word error speaker adaptive speaker dependent training sent 
speaker independent speaker independent speaker dependent training sentences varying number sentences word error rates native speakers 
speaker avg sum num 
sentences num 
words type num 
num 
transf 
si sa si sa si sa sa si sa sa table speaker independent si speaker adapted sa word error rates nonnative speakers wsj male development set various degrees tying numbers transformations 
speaker nd ne nf ni nn avg sum num 
sentences num 
words si sa table word error rates nonnative speakers november wsj evaluation set 
speaker avg sum num 
sentences num 
words type num 
si sa si sa si sa si sa table speaker independent si speaker adapted sa word error rates native speakers various degrees tying 
si sa natives non natives table speaker independent si speaker adapted sa word error rates native nonnative speakers american english 
list tables speaker independent si speaker adapted sa word error rates nonnative speakers wsj male development set various degrees tying numbers transformations 
word error rates nonnative speakers november wsj evaluation set 
speaker independent si speaker adapted sa word error rates native speakers various degrees tying 
speaker independent si speaker adapted sa word error rates native nonnative speakers american english 
list figures speaker independent speaker dependent training sentences varying number sentences word error rates native speakers 

