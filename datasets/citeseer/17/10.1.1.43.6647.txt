international computer science institute center street ffl suite ffl berkeley california ffl ffl fax growing cell structures self organizing network unsupervised supervised learning bernd fritzke tr may new self organizing neural network model having variants 
variant performs unsupervised learning data visualization clustering vector quantization 
main advantage existing approaches kohonen feature map ability model automatically find suitable network structure size 
achieved controlled growth process includes occasional removal units 
second variant model supervised learning method results combination abovementioned self organizing network radial basis function rbf approach 
model possible contrast earlier approaches perform positioning rbf units supervised training weights parallel 
current classification error determine insert new rbf units 
leads small networks generalize 
results spirals benchmark vowel classification problem better results previously published 
submitted publication international computer science institute berkeley ca fritzke icsi berkeley edu contents unsupervised growing cell structures problem definition network architecture network dynamics removal cells approximation voronoi regions efficient manipulation high dimensional topologies network visualization high dimensional input data alternative insertion criteria extension supervised learning motivation radial basis functions supervised growing cell structures simulation examples discussion notation ii self organizing neural network models proposed willshaw von der malsburg kohonen generate mappings high dimensional signal spaces topological structures 
mappings able preserve neighborhood relations input data property represent regions high signal density correspondingly large parts topological structure 
interesting applications various areas ranging speech recognition kohonen data compression schweizer combinatorial optimization walker 
fact similar mappings various places brains humans animals indicates preservation topology important principle natural signal processing systems 
noted predetermined structure size kohonen model imply limitations resulting mappings 
number variations proposed concerning networks variable topology variable number elements 
approach leads networks complicated structure 
tree network described kangas kohonen laaksonen preservation neighborhood relations done small degree due sparse connectivity network 
neural gas algorithm martinetz schulten produce compact networks preserve neighborhood relations extremely 
generates general networks dimensionality input data dimensionality reduction performed 
models allow variable number elements predefined principal structure rectangular array interpolative algorithm rodrigues almeida learning expectation method introduced xu 
proposal random structures stems ritter 
interesting approach network growing grid introduced blackmore miikkulainen 
network contribution flexible compact structure variable number elements dimensional topology arbitrarily chosen 
demonstrated new model improves kohonen feature map respect various important criteria fritzke 
acknowledge new model owes ideas kohonen approach extension completely different formalism 
outline network unsupervised learning introduce extension model supervised learning 
unsupervised growing cell structures problem definition describe network model appropriate exactly define kind problems network supposed solve 
place number dimensional input signals obeying unknown probability distribution 
denote vector space input signals stem 
objective generate mapping discrete dimensional topological cell structures different dimensionality structure mapping properties ffl similar input signals mapped topologically close elements ffl topologically close elements similar signals mapped 
ffl regions probability density input vector distribution high represented correspondingly elements points mean mapping preserve similarity relations forward backward direction 
dimensionality smaller dimensionality reduction performed 
spite possible preserve similarity relations complexity data reduced loss information 
third point means gain information unknown probability density input signals 
network architecture initial topology network dimensional simplex 
line segment triangle higher structure denoted tetrahedron 
vertices simplex cells neurons 
edges denote topological neighborhood relations 
self organization process described new cells added network superfluous cells removed 
modification network performed network consists solely dimensional simplices 
typical structures different values shown fig 

choose model minimal complexity easily combined larger structures 
note number vertices dimensional grows linear dimensional hypercube exponentially growing number vertices 
choice high dimensional networks 
voronoi tessellation generated dimensional cell structure initial triangular topology 
dimension input vector space example 
neuron projected drawing circle position vector points 
circles corresponding topologically neighboring neurons connected lines 
cell dimensional synaptic vector attached 
vector may seen position input vector space 
denote set synaptic vectors mapping oe input vector space network defined mapping input signal cell nearest position vector 
formally write oe 
oe oe called best matching unit defined kw oe gamma min kw gamma delta denotes euclidean vector norm 
partitioned number regions consisting locations having common nearest synaptic vector see fig 

known voronoi tessellation regions denoted voronoi regions 
order simplify formulas assume input space arbitrarily large finite subregion consequence finite input space voronoi regions finite 
general true voronoi regions belonging synaptic vector convex hull network dynamics principle adaptation synaptic vectors model done earlier proposed kohonen 
determine best matching unit current input signal 

increase matching best matching unit topological neighbors 
kohonen model strength adaptation decreasing cooling schedule 
topological neighborhood inside significant changes chosen large decreases 
model follows basic strategy 
important differences ffl adaptation strength constant time 
specifically constant adaptation parameters best matching unit neighboring cells respectively 
ffl best matching unit direct topological neighbors adapted 
choices eliminate need define cooling schedule model parameters 
denotes set direct topological neighbors cell furthermore define cell local counter variable basically containing number input signals cell best matching unit 
cells slightly moving signals weighted stronger previous ones 
achieved decreasing counter variables certain fraction adaptation step 
enable decay signal counters represented real valued variables 
adaptation step model formulated follows see fig 

choose input signal probability distribution 

locate best matching unit oe 

increase matching direct topological neighbors deltaw gamma deltaw gamma 
increment signal counter delta 
decrease signal counters fraction ff 
delta gammaff choose small values cells move initial random positions locations dynamic equilibrium changes directions 
moving completely adaptation parameters decreased stochastic approximation 
objective structure synaptic vectors distributed 
achieved cell probability best matching unit current input vector 
know explicitly local signal counters compute estimate relative frequency input signals received certain cell 
relative signal frequency cell deltax stands new old concise notation incremental changes 
initial situation occurrence input signal adaptation adaptation step dimensional cell structure 
best matching unit direct neighbors adapted 
columns represent signal counter values 
signal counter best matching unit incremented 
figures project network input vector space drawing cell position corresponding vector points 
useful technique input vector space dimension equal 
eventually cells similar relative signal frequencies 
high value indicates position insert new cell new cell reduce high value certain degree 
insertions basis criterion 
fixed number adaptation steps determine cell property look direct neighbor largest distance input space 
cell see fig 
satisfying kw gamma kw gamma insert new cell see fig 

new cell connected cells way structure consisting dimensional simplices synaptic vector initialized insertion leads new voronoi region input space 
time voronoi regions topological neighbors diminished 
change reflected redistribution counter variables compute changes signal counters achieved connecting common neighbors part simplex having vertices 
second original connection removed 
situation insertion 
columns represent signal counter variables 
cell received input signals far 
grey lines indicate voronoi tessellation 
new cell inserted new voronoi region exists 
signal counter variables redistributed changes voronoi regions 
insertion new cell 
start dimensional simplex random positions desired network size reached perform constant number adaptation steps 
insert new cell distribute counter variables eqn 

principal algorithm growing cell structures delta jf new gamma jf old jf old jf dimensional volume initial value new cell defined gamma nr delta redistribution counter variables seen ascribing new cell input signals got existed process 
way reduction counter variables neighbors motivated 
basic algorithm growing cell structures shown fig 

schematic example process shown fig 

main characteristic model adaptation steps followed single insertion 
note feedback relation types action ffl adaptation step increases signal counter best matching unit increases chance cell inserted near cell 
dimensional cell structure grows directed input signals stemming uniform distribution circle shaped sub area initial structure triangle neurons randomly initialized vectors 
structure distributed constant number input signals 
new cell white circle inserted connected neurons way structure triangles results 
new structure distributed cell inserted ffl insertion near cell decreases size voronoi field value signal counter reduction voronoi field probable best matching unit input signals 
simulations indicate wide range parameter settings model approaches state cell probability best matching unit input signal approximately equal 
case entropy gamma log approximately maximized local density vectors gives estimate unknown probability density input vectors 
abovementioned comparative study indicates growing cell structures estimate unknown probability distributions significantly better kohonen feature maps fritzke 
fig 
stages simulation depicted 
cell structure grows guided input vectors finds suitable structure model cloud shaped distribution 
note early phases simulation network basically final shape fewer neurons 
behavior described fractal growth observed frequently plants ferns 
important property kind change interrupt process time shaped structure 
property model especially evident viewing computer simulations certain number cells created little movement vectors occurs 
main source change insertion new cells 
signals signals signals signals signals signals development dimensional growing cell structure 
underlying probability distribution case dimensional uniform cloud shaped area 
sub picture number received input signals number adaptation steps shown 
employed simulation parameters ff 
growth process leads adapted network structure 
short connections indicate 
synaptic vectors lie outside relevant circular areas 
removal superfluous cells substructures formed 
positions synaptic vectors indicate nearly perfect modeling probability distribution short connections 
growing cell structures network cells adapted probability distribution previous example 
simulation parameters ff 
property facilitates extension model new supervised learning method demonstrated section 
removal cells cases especially consists separate regions positive probability density better modeling achieved removing superfluous cells 
cell regarded superfluous position synaptic vector region low probability density 
general unknown relate relative signal frequency cell size receptive field voronoi field get local estimate 
specifically note jf local estimate probability density near periodically removing cells values threshold model structured distributions accurately 
fig 
shows simulation results probability distribution removal neurons 
approximation voronoi regions computation voronoi tessellation difficult dimensions 
replaced voronoi region approach dimensional hypercube side length equal mean length edges emanating jf took computed card nc kw gamma estimation probability density anymore accurate sufficient reliably identify superfluous neurons 
appropriate value threshold depends strongly probability distribution 
reason probability distribution non zero large area input vector space lower density distribution concentrated locally 
uses normalized value defined computed multiplying total volume hypercubes gets sufficient independence 
threshold value appropriate cases 
check cell value performed insertion cells fulfilling condition removed 
simulation leading fig 
performed method 
note choice dimensional hypercube appropriate underlying data spans dimensional space 
hand data stems lower dimensional subspace better hypercube dimensionality 
illustrate point consider extreme example 
assume input data dimensional uncommon real problems stems dimensional sub manifold know 
take twodimensional network able visualize data see section 
cells mean edge length shrinks percent volume corresponding dimensional hypercube collapses percent previous size 
obviously reflect change receptive field cell 
case dimensional hypercubes appropriate 
evident helpful know true dimensionality data meaning smallest dimensionality dimensional sub manifold containing input data 
dimensional hypercubes estimate size voronoi regions model 
unfortunately general difficult value especially mentioned sub manifold linear arbitrarily twisted curved surface 
principal component analysis data general reveal true dimensionality gives upper bound 
long simple method determine true data dimensionality define estimate 
give general rules choosing estimate problems encountered far ffl set ffl different components input vectors known stochastically independent ffl known dependencies components set number independent variables 
ffl set smaller equal number input vectors 
rule applies unusual case total number vectors smaller dimensionality ffl perform principal component analysis data 
set number principal eigenvalues covariance matrix data 
cases necessary principal component analysis method sensitive choice case avoid choose value high 
happen data high dimensional strong dependencies components 
case happen insertions occur region structure 
due fact newly inserted cell gets attributed nearly signals neighbors change voronoi fields overestimated see 
simple remedy problem choose lower value perform simulation 
choose case value approximate volume voronoi regions mean edge length see eqn 

stressed choice critical step 
set data usually different estimates 
efficient manipulation high dimensional topologies implementation dimensional growing cell structures somewhat complicated implementation kohonen feature map usually rectangular array processing units chosen 
appropriate give hints done relatively small effort 
implementation model support structural update operations ffl insertion neuron ffl deletion neuron operations performed resulting structure consists exclusively dimensional 
general structure network represented undirected graph standard data type consisting nodes edges pairs nodes 
nodes correspond neurons edges topological neighborhood relations 
data structure sufficient principle considerable search effort needed consistent update operations 
problem removal neuron require neurons connections removed structure consistent 
simple heuristics current implementation model leda see mehlhorn publicly available library data types algorithms 
leda contains particular elaborated data type graph 
growing cell structures 
node removed 
done removing adjacent edges node 
structure removal node edges ab ce part triangle anymore 
structure inconsistent 
simple heuristics cell removal lead inconsistent structures 
remove node remove neighboring connections node 
properly shown fig 

key idea solve problem change level observation nodes connections 
purpose keep track current network consists 
technically new data type simplex created instance contains set nodes belonging certain 
furthermore node associate set node part 
update operations formulated follows ffl insertion new node inserted splitting existing edge qf node connected common neighbors updated 
containing words edge split replaced containing set nodes respectively replaced new node original edge qf removed 
new inserted sets associated participating nodes 
ffl deletion delete node necessary sufficient delete node part 
done removing sets associated nodes 
edges nodes common removed done nodes having edges 
strategy leads structures edge belonging node edge 
resulting dimensional structures consistent contain dimensional 
fig 
demonstrated problematic example fig 
handled correctly 
network visualization high dimensional input data important property kohonen feature map ability project high dimensional input data dimensional usually rectangular grid 
visualization complex data possible speech data kohonen high dimensional symbolic descriptions objects ritter kohonen 
growing cell structures 
node removed consequently triangles dimensional participates 
structure removal triangles participated 
structure consists triangles consistent 
correct removal additional structural information growing cell structures generate regular networks 
dimensional case network consists number connected triangles possibly networks removal cells performed 
construction network twodimensional obvious embed network plane visualize 
hand method projecting network input vector space allows visualization input vector dimensions 
method embed dimensional network dimensional space 
possible visualize networks arbitrarily high vector dimensions long network dimension low 
method employs simple physical model construct dimensional embedding self organization process 
assume 
generalization dimensions straightforward 
ffl cell network modelled disc elastic material 
ffl diameter disc discs centers distance touch 
distance gets smaller discs repel 
ffl neighborhood connection modeled elastic string 
connected currently touching discs pulled 
ffl discs positively electrically charged repel 
self organization process discs positioned plane overlap 
time new cell inserted position corresponding disc interpolated neighbors way vector interpolated 
may occur overlaps exist 
insertion compute disc sum forces acting move accordingly 
done asynchronous manner order avoid oscillation effects 
try build physically accurate model 
discs associated mass forces lead proportionally large motions 
forces experimentally determined values 
repelling force discs center distance projection input vector space 
shortly start simulation cells connected deletions took place 
embedding twodimensional space projection input vector space 
deletion superfluous cells led separate structures 
embedding twodimensional space 
sub structures recognized easily 
example embedding method 
probability distribution uniform separated cubes 
network dimensional 
show state simulation respectively 
embedding easily possible detect splitting network seen fig 



attracting force connected discs center distance gamma forces balanced 
usually multiplied 
cases different values appropriate 
example results obtained described method shown fig 

animal small medium big legs legs hair feathers hunt likes run fly swim table animal names binary attributes ritter kohonen attribute applies animal corresponding table entry 
larger benefit gained having embedding input data highdimensional visualize network input vector space anymore 
real applications case data consisting components 
ritter kohonen introduced illustrative example high dimensional data 
consists description animals binary property lists see table 
thirteen properties coding name animal led dimensional vectors 
vectors fed dimensional kohonen feature map consisting theta neurons 
self organization process tested input vectors represented map 
came kohonen method interesting projection positioning similar animals generally neighboring locations map 
possible partition map connected regions containing birds respectively see fig 

tested growing cell structures data constructed self organization dimensional embedding network method just described 
different stages specific simulation shown fig 

comparing results ritter kohonen main advantage model lies fact automatically finds meaningful partitions data ritter kohonen identify partitions 
general technique possible visualize cluster high dimensional data useful application areas process control pattern recognition 

owl 
lion 
eagle 
hen fox 
cat dove dog 
duck 
cow horse 
tiger goose wolf 
hawk 
zebra kohonen feature map representing animal data table 
animal cell shown best matching unit corresponding feature vector 
animals similar properties represented neighboring locations map shown manually added partition regions ritter kohonen 
alternative insertion criteria goal model described far estimate unknown probability density input signals local density vectors input vector space 
goal achieved perfectly neuron chance randomly drawn input signal mapped 
approach goal introduced local signal counter neuron inserted new neurons near existing neurons high signal counter values 
pointed underlying general principle method exploited achieve quite different goals estimation probability density 
principle insert new neurons way expected value certain error measure called resource equal neurons 
appropriate resources property insertion new neuron near existing neuron reduces expected value resource additional conditions resource characterized behavedness fulfilled expect strategy inserting new neurons near neurons high resource values lead desired result neurons similar expected resource values 
interesting example alternative resource quantization error generated neuron 
simply accumulated squared distance vector neuron input signals mapped neuron 
incrementing denotation stems idea accumulated resource values cause insertion growth play nutrition role network 
dove hen duck goose owl hawk eagle fox dog wolf cat tiger lion horse zebra cow dove hen duck goose owl hawk eagle fox dog wolf cat tiger lion horse zebra cow birds divided mammals 
birds ones birds prey different positions 
mammal cluster similar animals generally neighboring positions 
mammal cluster split cluster 
contains large animals horse zebra cow second contains animals run tiger lion wolf dog third cluster contains animals hunt avoid excessive running cat fox 
growing cell structures 
data stems ritter kohonen see table 
data ordered kohonen model ability growing cell structures form sub structures possible partition data clusters mutually similar items 
original version growing cell structures leads solution approximately percent vectors theta field theta field 
mean square error 
error minimizing variant growing cell structures positions vectors theta field 
mean square error 
minimization quantization error 
probability distribution consists theta field theta field 
percent input signal come areas 
letting networks grow size mean square error determined test signals 
signal counter best matching unit earlier change delta gamma effectively replaces eqn 

measure insertion criterion new neurons inserted anymore near neurons getting input signals near neurons input signals different vectors 
resulting network structures differ especially probability distributions nonuniform probability density see fig 
particular insertion criterion develop new method vector quantization see fritzke 
application consistency requirements structures allowing separate cells neighbors exist 
method able generate codebooks exceptionally quality 
useful example resource discussed section report results new supervised network growing cell structures 
extension supervised learning motivation self organizing networks perform unsupervised learning 
frequently generate ordered mappings input data low dimensional topological structure 
cases partition input data subsets clusters data items inside subset similar items different subsets dissimilar 
situations input corresponding output data 
problem learn underlying relation limited number examples 
sake concreteness assume data consists number pairs input desired output th pair 
supervised learning methods cases train networks generate desired output input part specific data pair 
useful se hoped finishing training network able generate reasonable output values unknown input data 
denoted generalization 
commonplace today achieve generalization number free parameters network kept small 
danger fitting denotes situation network improves training data decreasing performance test data 
typical applications areas supervised learning include pattern classification function approximation 
demonstrate self organizing model extended supervised learning procedure 
result method resembles known radial basis function network rbf eliminates serious drawbacks approach 
radial basis functions radial basis function networks moody darken consist layer units gaussian activation functions output layer linear summation units see fig 

assume data pairs input desired output 
gaussian unit associated vector indicating position gaussian input vector space standard deviation oe input datum activation unit described exp gamma gamma oe general activation function limited local area input vector space considerably different zero 
radial basis function network 
dimensional input signal directed layer units gaussian activation function 
layer weighted connections linked output layer linear summation units 
eqn 
realizes normalization proposed moody darken holds 
consequently input signal causes summa activation 
gaussian units output units exists complete layer modifiable weights 
goal set free parameters network output units produce suitable values input data 
free parameters case positions widths gaussians weights output units 
usual procedure training network consists consecutive phases unsupervised supervised gaussians positioned dimensional input vector space 
moody darken propose means clustering algorithm purpose 
gaussian standard deviation defined 
moody darken report results distance nearest gaussian 
layer modifiable weights trained produce desired values output units 
commonly delta rule called mean square rule conventional method solving linear system 
described networks reported computationally efficient compared backpropagation important drawbacks 
define number gaussians priori 
leads similar problems number hidden units dilemma multi layer perceptrons difficult estimate appropriate number units 
second problem stems fact means clustering algorithm positions gaussians locations input vector space input vectors 
cases classification problem classes shown example points find method map points square class 
alternatively consider rejection points class appropriate 
optimal 
consider simple classification problem classes data vectors lie separated clusters remaining vectors classes scattered small clusters pretty close see fig 

case means position available gaussians large clusters 
better choice cover large clusters gaussians having large standard deviation rest cover complicated region containing small clusters 
generally relatively gaussians positioned locations difficult differentiate classes 
locations known priori 
supervised growing cell structures fairly obvious way extend growing cell structures supervised radial basis function network see fig 
ffl cell vector defines center gaussian activation function 
ffl standard deviation oe gaussian defined mean length edges emanating comparable heuristic proposed moody 
ffl number linear output units defined gaussian units completely connected weighted connections 
realized associating cell output weight vector mc 
ic denotes weight connection cell output unit supervised growing cell structures network 
contrast conventional radial basis function network exist topological neighborhood relations gray arrows gaussians 
define radius gaussian interpolate position newly created gaussians existing ones 
far similar standard rbf network 
difference lies training strategy characterized points ffl having phase scheme self organization rbf layer supervised adaptation weighted connections performed parallel 
ffl classification error occurring training data determine insert new cells resp 
gaussians 
parallel training possible earlier mentioned property algorithm existing weight vectors moved changed little 
sense train weights output units right growth process 
extend described algorithm unsupervised learning accordingly 
particular learning step delta rule adaptation step 
assuming data consists pairs input vector desired output vector 
compute activation cell exp gamma gamma oe perform normalization 
advantage outliers activate gaussian identified easily 
normalize hand input signals arbitrarily far away gaussians activate considerably 
support position argue somewhat questionable generalize patterns different patterns seen training 
activation output units computed ic delta delta delta mg change weights delta rule defined deltaw ic gamma delta delta delta mg learning rate 
update resource variable current best matching unit adding squared error actual output delta delta delta desired output delta delta delta delta ki gamma ok replaces eqn 
incremented resource variable respectively eqn 
summed quantization error 
current task classification problem opposed continuous input output mapping alternatively classification error 
case resource updated delta classified correctly networks built classification error insertion criterion tend small start classifying training examples correctly 
due fact new cells inserted regions input vector space misclassifications occur 
hand learning practically halt misclassifications occur anymore raw mean square error network large cases lead poor generalization unknown patterns 
advisable weighted combination classification mean square error 
pointed merely matter fine tuning 
experience networks generate usually satisfying mappings areas training vectors available matter combination kinds error 
new cell inserted gets vector wmr weighted connections output units 
initializing vectors zero random values obtained redistribution similar resource variable new cell compare eqn 
deltaw jf new gamma jf old jf old jf dimensional volume initial output weight vector new cell defined gamma nr deltaw doing redistribution new cell output weights activate output units way similar mean neighbor 
neighboring gaussians particular behavior characteristic original perceptron learning rule introduced rosenblatt 
overlap considerably output behavior network changed 
adaptation steps new unit develop different weights contribute better error reduction area input vector space 
complete algorithm supervised growing cell structures shown fig 

simulation examples example simple classification problem described supervised version growing cell structures construct classifier data shown fig 

network chosen dimensional 
data classified classes output units 
combined growth learning process continued mse training data fell bound 
resulting network see fig 
map theta points inside square region class see fig 

observe size triangles standard deviation gaussians considerably smaller region small clusters upper right 
reason classification area difficult classification errors occur training 
leads insertions area 
resulting decision regions demonstrate final network classifies training vectors correctly 
job classifying points inside depicted region 
example spirals known benchmark connectionist community called spiral problem 
consists dimensional vectors lying interlocked spirals classes case see fig 

task construct classifier able distinguish classes 
benchmark interesting due low data dimensionality possible visualize decision regions network training 
difficult task typical feed forward networks multi layer perceptrons sigmoidal activation functions 
lang witbrock unable solve problem standard multi layer network additional connections achieve convergence 
fahlman lebiere constructive algorithm called cascade correlation solve problem 
resulting decision regions network shown fig 

note cascade correlation algorithm able learn training data decision regions show artifacts 
cases points training vectors specific class classified belonging class 
occurs especially outer parts spirals example patterns class apart class 
resulting cuts spiral interpreted poor generalization 
absence evidence natural assume intermediate points belong class 
decision regions produced network lang witbrock look similar 
baum lang proposed constructive method tested problem 
approach employs oracle tell point plane desired class 
queries oracle position hyperplanes initialize cell structure dimensional simplex random positions create linear output units 
create weighted connection ic cell output unit mg associate cell vertex simplex gaussian function 
classification error low repeat times choose pair theta training data determine best matching unit increase matching direct neighbors 
compute activation cell see eqn 

compute vector output unit activations see eqn 

perform delta rule learning step weights see eqn 
increase resource variable delta ki gamma ok determine cell maximum resource value insert new cell direct neighbor maximum distance input vector space redistribute resource values weight vectors direct neighbors eqn 
resp 
supervised growing cell structures algorithm final network decision regions supervised growing cell structures 
network decision regions data shown fig 
simulation parameters ff removal cells 
spiral problem decision regions cascade correlation reprinted permission fahlman lebiere spiral problem learning results constructive network 
network model number epochs reported backpropagation lang witbrock cross entropy bp lang witbrock cascade correlation fahlman lebiere growing cell structures table training epochs necessary spiral problem corresponding certain hidden units 
explicit test set points defined consisting points pair adjacent class training points 
training test points form spirals times higher point density training set 
best model baum lang report average errors test set 
generated dimensional growing cell structure solve spiral problem 
network corresponding decision regions shown fig 

case decision regions form separated spirals smooth borders 
fact decision regions exhibit strong similarity oracle defined baum lang 
data points training vectors class mapped class network errors mentioned test set baum lang 
outer regions spiral decision regions follow example vectors accurately 
local density cells uniform follow density training vectors higher near center spirals 
surprising near center fewer units training point needed facilitate correct classification 
learning method important practical aspect number pattern presentations necessary achieve satisfying performance 
case finite training set common measure number cycles training patterns called epochs 
list table number epochs spiral problem earlier methods approach 
seen number epochs required new method orders magnitude smaller standard backpropagation nearly order magnitude smaller cascade correlation 
example speaker independent vowel recognition explicitly investigate generalization capability model performed experiments vowel recognition problem 
data collected recorded examples eleven steady state vowels english spoken fifteen speakers speaker normalization study 
vowel data spiral data electronically available carnegie mellon university connectionist benchmark collection see fahlman 
ascii approximation international phonetic association symbol word eleven vowel sounds recorded table 
word uttered fifteen speakers female male 
speech signals low pass filtered khz digitized bits final network cells decision regions performance growing cell structures spiral benchmark 
simulation parameters ff removal cells 
vowel word vowel word heed hod hid hoard head hood hard heard hud table words recording vowels robinson slp mlp kanerva rbf gaussian square nn gcs percentage correctly classified test patterns vowel recognition problem 
khz sampling rate 
twelfth order linear predictive analysis carried sample hamming windowed segments steady part vowel 
reflection coefficients calculate log area parameters giving dimensional input space 
general speech processing explanation technique rabiner schafer 
speaker yielded frames speech eleven vowels 
gave frames fifteen speakers 
robinson data thesis robinson investigate types neural network algorithms 
frames male female speakers train networks remaining frames male female speakers testing performance 
classifiers examined single layer perceptrons multi layer networks sigmoidal gaussian quadratic activation functions modified kanerva model radial basis networks conventional method nearest neighbor classifier 
due limited computational facilities available robinson run different architectures 
run continued epochs robinson 
get comparable results trained growing cell structure networks data robinson test data evaluate generalization capabilities networks 
input vector dimension high dimensional networks somewhat higher dimension previous examples 
results robinson results shown table 
easier comparison percentage correctly classified test patterns shown graphically fig 

evident simulations approach best results considered methods 
networks trained epochs compares methods 
ratio approximately lines simulations spiral problem compares number epochs needed cross entropy backpropagation model see table 
classifier number hidden units correctly classified percent correct single layer perceptron multi layer perceptron multi layer perceptron multi layer perceptron modified kanerva model modified kanerva model radial basis function radial basis function gaussian node network gaussian node network gaussian node network gaussian node network square node network square node network square node network nearest neighbor dimensional gcs dimensional gcs dimensional gcs dimensional gcs dimensional gcs table test results vowel recognition problem 
table shows network size number correctly classified test patterns corresponding percentage 
upper box shows results reported robinson thesis robinson 
got best classification rate nearest neighbor method 
lower box shows result nets generated growing cell structures method 
higher rate correctly classified test patterns nearest neighbor method models examined robinson 
tried networks dimensionality 
parameter set equal network dimension case 
second run dimensional network continued long see training effects produced case simulation 
different choices influence outcome algorithm 
discussion part introduced new self organizing network model 
advantages existing models ffl network structure determined automatically input data 
ffl network size predefined 
growth process continued performance criterion met 
ffl parameter model constant 
necessary define decay schedule models 
ffl insertion new units influenced generated network estimates probability density input signals minimizes quantization error pursues goals 
ffl final structure depends input data data visualization clustering 
contrast models fixed structure provide information kind 
second part developed combination self organizing network radial basis function rbf approach 
provides number improvements current network models localized receptive fields models ffl number diameter position rbf units determined automatically growth process stopped soon network performance 
ffl positioning rbf units supervised training connection weights performed parallel current classification error determine insert new rbf units 
previous approaches rely clustering algorithms fail find positions rbf units respect classification accuracy 
ffl networks relatively small generalize 
ffl necessary number training epochs orders magnitude smaller approaches 
results obtained far promising necessary investigate performance network larger problems ones 
furthermore improvement find ways automatically choose parameters set user 
interesting goal model parameters properties desired classifier 
goal course distant hope proposed methods step right direction 
author likes scott fahlman permission reproduce maintaining cmu benchmark collection 
notation growing cell structures network denotes set cells network dimensionality growing cell structures network dimensional input vector space dimensionality dimensional synaptic weight vector cell set vectors cells oe mapping adaptation steps insertion adaptation parameter best matching unit adaptation parameter neighboring cells threshold cell removal probability distribution input signals ff decrease parameter resource variables set direct neighbors cell voronoi field cell true data dimensionality estimate jf dimensional volume dimensional input signal dimensional output signal dimension output vector space supervised learning pair supervised learning dimensional vector output unit activations resource variable cell contains signals quantization error classification error layer gaussian units rbf networks activation gaussian unit relative signal frequency cell estimate probability density near estimate normalized probability density near deltax short cut new old dimensional vector weights cell output units ic weighted connection gaussian unit output unit delta euclidean vector norm baum lang constructing hidden units examples queries advances neural information processing systems lippmann moody touretzky eds morgan kaufmann publ san mateo pp 

blackmore miikkulainen incremental grid growing encoding high dimensional structure dimensional feature map university texas austin tr ai austin tx 

speaker normalisation automatic speech recognition university cambridge ph thesis 
fahlman 
cmu benchmark collection neural net learning algorithms carnegie mellon university school computer science machine readable data repository pittsburgh 
fahlman lebiere cascade correlation learning architecture advances neural information processing systems touretzky ed morgan kaufmann san mateo pp 

walker study application kohonen type neural networks travelling salesman problem biological cybernetics pp 

fritzke 
kohonen feature maps growing cell structures performance comparison advances neural information processing giles hanson cowan eds morgan kaufmann publishers san mateo ca 
fritzke 
vector quantization growing splitting elastic net appear proceedings icann amsterdam 

neural network adapts structure set patterns parallel processing neural systems computers hartmann eds elsevier science publishers pp 

kangas kohonen laaksonen variants self organizing maps ieee transactions neural networks pp 

kohonen 
self organized formation topologically correct feature maps biological cybernetics pp 

kohonen 
neural phonetic typewriter ieee computer pp 

kohonen maps insightful representation phonological features speech recognition proc 
th int 
conf 
pattern recognition montreal 
lang witbrock learning tell spirals apart proceedings connectionist models summer school touretzky hinton sejnowski eds morgan kaufmann san mateo pp 

martinetz schulten neural gas network learns topologies artificial neural networks kohonen simula kangas eds north holland amsterdam pp 

mehlhorn leda library efficient data types algorithms universitat des saarlandes fachbereich informatik tr saarbrucken 
moody darken learning localized receptive fields proceedings connectionist models summer school touretzky hinton sejnowski eds morgan kaufmann san mateo pp 

rabiner schafer digital processing speech signals prentice hall englewood cliffs nj 
ritter 
learning self organizing map artificial neural networks kohonen simula kangas eds north holland amsterdam pp 

ritter kohonen self organizing semantic maps biological cybernetics pp 

robinson 
dynamic error propagation networks cambridge university phd thesis cambridge 
robinson 
personal communication 
rodrigues almeida improving learning speed topological maps patterns proc 
paris 
rosenblatt 
perceptron probabilistic model information storage organization brain psychological review pp 

schweizer fully neural approach image compression artificial neural networks kohonen simula kangas eds north holland amsterdam pp 

willshaw von der malsburg patterned neural connections set self organization proceedings royal society london pp 

xu 
adding learning expectation learning procedure self organizing maps int 
journal neural systems pp 


