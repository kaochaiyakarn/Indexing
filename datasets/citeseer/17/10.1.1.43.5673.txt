practical reinforcement learning continuous spaces william smart cs brown edu computer science department box brown university providence ri usa leslie pack kaelbling ai mit edu arti cial intelligence laboratory mit technology square cambridge ma usa dynamic control tasks candidates application reinforcement learning techniques 
tasks inherently continuous state action variables 
cause problems traditional reinforcement learning algorithms assume discrete states actions 
introduce algorithm safely approximates value function continuous state control tasks learns quickly small amount data 
give experimental results algorithm learn policies simulated task real robot operating unaltered environment 
algorithm works traditional learning setting demonstrates extremely learning bootstrapped small amount human provided data 

dynamic control tasks candidates application reinforcement learning techniques 
tasks inherently continuous state action variables 
existing reinforcement learning rl algorithms assume discrete sets states actions means directly applicable tasks 
continuous variables discretized new discrete version problem tackled rl techniques 
choose bad discretization state action space introduce hidden state problem making learning optimal policy impossible 
discretize nely lose ability generalize increase amount training data need 
especially true task state multi dimensional number discrete states exponential state dimension 
reasonable replace discrete lookup tables rl algorithms function approximators capable handling continuous variables dimensions generalizing similar states 
simply replacing lookup table shown cause learning fail benign cases boyan moore 
function approximation carried carefully system succeed learning control policy 
reinforcement learning techniques successfully applied robots researchers typically easier faster simply write control program achieve task directly 
goal reported robust reinforcement learning augmented information easy human expert supply resulting construction methodology real practical value 
describes initial steps direction 
mainly concerned value function approximation reinforcement learning real robots 
addition empirical studies value function approximation example boyan moore sutton growing body theoretical subject 
relevant thrun schwartz gordon looking reasons general function approximators fail 
variety approaches gradient descent methods example williams baird moore guaranteed convergence ability handle continuous spaces proposed limited severe training data restrictions 
number successes reinforcement learning real robots 
systems discretize state action spaces examples continuous spaces 
lin recurrent neural networks navigation tasks addition teaching methods accelerating learning similar reported 
mahadevan real valued abstracted sensor information learn navigation tasks 
growing number robot soccer learning systems example asada 
group real valued inputs learning increasing complex control policies 
section introduce hedger algorithm safely approximating value function qlearning watkins dayan online learning dynamic control policies 
discuss address problems involved rl learn policies line setting 

safely approximating value function stated value function approximation learning simply case substituting supervised learning algorithm value lookup table 
supervised learning algorithm su er prediction error trained real data set 
approximator small errors quickly accumulate render approximation useless 
learning action state resulting transition state reward causes table value function approximation updated max value state action pair updated learning rate discount factor current approximation expected maximum value state generally checking values possible action selecting largest 
key thing note new value current approximated value values actions state means error predictions incorporated update value 
case new value learned approximation algorithm slightly incorrect 
subsequently update approximation state action pair error larger 
error quickly dominate approximation 
reducing approximation error thrun schwartz gordon note main sources error function approximators represent value functions prone estimation 
major cause function approximators relate problem known statistics literature hidden extrapolation 
predictions function approximator general valid queries 
strong assumptions form function learning con dent predictions area input space covered training data 
put way function approximators interpolate training data extrapolate 
supervised learning setting perform cross validation experiments determine accuracy learned function di erent areas input space 
allow empirically determine allow queries 
iteratively estimating value function tests 
tell learned function models training data tell training data derived model correct 
solution problem construct convex hull training data answer queries lie 
problem lies eciently construct enclosing hull 
cook calls structure independent variable hull suggests best compromise eciency safety hyper elliptic hull arguing expense computing complex hulls outweighs predictive bene ts 
determining point lies hull straightforward 
matrix rows correspond training data points calculate hat matrix arbitrary point lies hull formed training points max ii ii diagonal elements section show incorporate learning algorithm 
hedger prediction algorithm hedger instance learning algorithm locally weighted regression lwr 
variation standard linear regression techniques training points close query point uence tted regression surface away 
new regression performed query point 
results globally nonlinear model retaining simple locally linear models estimated understood techniques see atkeson 
comprehensive survey lwr techniques 
training points lwr weighted function distance query point 
function typically kernel function gaussian width parameter known bandwidth 
large bandwidths mean points away uence resulting globally smoother approximated function 
small bandwidths allow high frequency variations learned model 
algorithm hedger prediction input query point lwr bandwidth output value function prediction concatenate form find set points closer thresh jkj kmin return don know default value 
calculate calculate kernel weight exp local regression weights return tted function return don know default value 
chose base hedger lwr techniques number reasons 
compelling lwr aggressive learning algorithm fast train 
reasonable predictions little training data 
important interesting line learning initially small amounts training data 
lwr retains training data able reevaluate approximation original training points 
storage requirements large prediction times longer compact learning representations 
suitable optimizations lwr choice purposes 
order lwr value function approximation combine state action vectors 
combined vector input vector lwr system 
currently simply concatenate state action vectors 
standard lwr techniques available training data query 
large data sets prohibitively expensive 
data points far query point typically little ect regression case 
allows computational optimization apply idea improve performance time 
algorithm sketches hedger predictions 
set distance threshold thresh lwr bandwidth training point distance query point negligible ect regression 
points closer threshold included search points include done relatively eciently storing points kd tree structure atkeson 
typically need points parameters regression model 
fewer points return default don know value 
constructed query point compared hull 
hull perform lwr prediction normal 
inside hull return default don know value 
results behavior similar initializing table representation initial value learning begins 
hedger training algorithm section describe hedger uses experiences world construct value function approximation 
experiences supplied tuples representing action taken state resulting reward transition state algorithm sketches hedger uses experience tuple update value function approximation 
obtain approximations current value maximum value resulting state keep track points lwr prediction line associated weights line 
line calculates new approximation standard learning update rule 
new value normal supervised training point lwr subsystem 
learned simply storing memory 
update points bringing value closer depending proximity measured kernel weighting updated values changed directly points stored memory 
added new di erent points 
updating approximation way allows keep value function smooth generalize ects updating point 
allows deal non stationarity inherent value function approximation 
algorithm hedger training input experience learning rate discount factor lwr bandwidth predict algorithm max predict set calculation exp new learn new point new line nd best action associated value state continuous state space visited state 
try nd best action region space close state 
optimization involved executing greedy policy de ned learned value function 
optimization dicult especially continuous actions involved 
currently optimization implemented hedger simple iterative algorithm methods proposed brent 
sampling di erent actions states close query state predicting values 
algorithm iterates tting quadratic surface sampled points sampling new point maximum tted function 
successive maxima closer threshold algorithm terminates 
approach similar newton method consequently shares problems 
critically dependent initial sample points problems local maxima value function 
initially sampling actions uniformly applying procedure works reasonably practice problems tried 
perform computational optimization hedger 
interested learning online small numbers training runs data points 
training data get fullest 
mind technique proposed lin experiences learner reverse order 
allows immediate rewards propagated value function ecient presenting order generated 
mean learning takes place training episode 
episode store generated experiences replaying 
supplying initial knowledge reinforcement learning systems perform extremely poorly early stages learning forced act random acquire experience world 
serious problem domains reward function largely uniform informative rewards states 
problem compounded states dicult reach random walk 
learning agent spend huge amount time exploratory actions learning rewards happens unusual state di erent reward value 
problems especially relevant robot control setting especially di erent actions similar ects 
robots real mechanical devices succession random actions discernible ect due mechanical gear trains robot inertia momentum 
solution propose provide system example training runs bootstrap value function approximation 
runs take form pre recorded experience tuples piece software control system human directly driving robot 
cases hedger begins learning passively observing experiences supplied initial policy generates 
uses experiences derive approximation value function normal 
initial training phase system switches learned control policy 
initial knowledge bootstrapped value function approximation allows agent learn ectively helps reduce time spent acting randomly 
key point note robot tries learn replicate training policy arbitrarily bad simply allows led world executing learning optimal policy 
follows initial example policies supplied robot optimal 
knew optimal policy learning pointless 
role supplied policy show agent expose interesting areas state action space 
assume uniform reward function de ne interesting area state action space generates unusual reward 
approach commonly address problem value functions largely uniform set default values overly optimistic 
default value unknown states higher actual value tend drive agent areas state space seen 
soon state experienced value lowered appealing unvisited states 
approach encourages exploration somewhat random fashion suited robot control problems 
supplying example trajectories state space manner leads phase learning process 
rst phase example policy human directly controlling system example piece control code control 
reinforcement learning system operates passively bootstrapping information value function 
value function sucient information second learning phase begins 
learned policy control system learning continues guided policy 

experimental results section give experimental results hedger learn policies control tasks 
rst known simulation powered car trying drive steep hill 
second corridor task involving real robot 
mountain car task mountain car task involves trying drive car top steep hill arriving zero velocity 
car powerful drive directly goal 
rst reverse opposite slope order build momentum get top hill 
task described continuous state variables position velocity car action variable 
consider formulations problem discrete continuous actions 
discrete case possible actions backward coast forward 
represent actions variable take values respectively 
continuous case action real valued lying 
dynamics system correspond described singh sutton 
reward top hill linear function velocity 
zero velocity yields reward maximum velocity results reward 
greedy exploration strategy set 
mountain car experiments training episode randomly selected point state space 
episode runs time steps goal reached whichever happens sooner 
experiments learning rate discount factor set 
hedger returns don know value greedy action selection random action 
periodically learning disabled evaluation runs performed 
evaluation consisted episodes starting random points state space current greedy policy 
evaluate ectiveness learning looking average number steps taken goal reached lets compare performance results reported literature 
provide performance baseline ran standard tabular learning nely discretized version problem converged 
resulted mean steps goal state 
comparisons learning trials signi cant con dence level stated 
shows results basic hedger system trained randomly sampled states actions 
performance tabular learning discretized state space discrete actions shown actions discrete actions steps goal training points tabular learning 
basic hedger learning results domain 
parison 
learning evaluated training points 
main point note gure hedger capable dealing continuous actions discrete ones 
initially performance continuous action system worse discrete action 
training progresses performances indistinguishable 
nal performance somewhat higher optimal steps 
running running system longer results slow convergence performance signi cantly di erent training points 
learning faster supplying example trajectories bootstrap value function approximation 
examples generated users controlling graphical simulation mountain car system real time 
example training runs limited steps users constrained discrete actions runs started bottom hill 
eleven examples generated di erent people 
average number steps reach goal lowest 
results incorporating example trajectories shown 
top line represents system goal discrete actions continuous actions phase training runs 
basic hedger augmented initial training examples 
ous actions bottom corresponds discrete actions 
performance continuous action case lags 
phase training runs performance signi cantly di erent 
initial performance systems better average training trajectories 
underlines point learning actual policies examples simply expose interesting parts state space 
noted considerably training data experiment previous 
total approximately data points discrete case continuous case 
numbers compare training points results 
performance amount data better previous experiment sampled trajectories state space 
combined presenting training data reverse order allows reward propagate eciently value function approximation 
discrete actions phase training runs steps goal continuous actions 
basic hedger calculations 
illustrates happens disable checks value function predictions 
discrete continuous action systems perform signi cantly worse checking removed 
nal performance systems corresponds control policy slightly better selects actions random 
demonstrates crucial role knowledge area learning coverage refusal propagate bad value estimates 
inspection actual values returned hedger revealed far large symptom hidden extrapolation described previously 
update neighboring points training lines algorithm performance continuous action task decreases shown 
performance discrete action system ected better initially 
discrete case sample actions values 
suciently far apart current distance metric altering value ect region updating enabled 
continuous actions ensuring smoothness value function approximation important ensure value function prediction 
discrete actions phase training runs steps goal continuous actions 
basic hedger local region updating training 
corridor task corridor task involves learning steer real robot corridor dead 
detect corridor walls data laser range nder determine angle corridor relative robot current heading 
calculate robot position respect centerline corridor 
quantities distance corridor state input hedger 
problem learn steering policy maps state variables rotation velocity 
robot forward speed controlled xed policy 
reward zero corridor reward 
performance metric task number time steps taken traverse section corridor time step corresponds roughly seconds 
delayed reward formulation encourages robot get corridor receives reward quickly possible 
policies spend time zig side corridor successful 
learning rate experiments set discount factor 
greedy exploration strategy set 
selecting random exploratory action added gaussian noise greedy action 
mainly reduce exploratory actions 
example trajectories generated hand coded corridor algorithm 
algorithm developed quickly little attempt ne tuning 
result took slightly steps reach goal average 
robot driven corridor direct human control provide estimate best achievable performance 
yielded steps goal average 
learning evaluated placing robot number pre speci ed starting positions recording long took reach corridor 
phase phase training runs best guided average example steps goal 
performance real robot corridor task 
shows performance robot rst second learning phases 
rst phase supplied policy control learning particularly fast 
immediately initial training training run gure performance signi cantly better supplied initial policy 
training done performance improves signi cantly di erent best achieved human control 
phase training runs system seen total approximately training points including generated example trajectories total time taken write initial control policy perform training approximately hours 
experience worst par time needed write debug policy performs similar level 

hedger algorithm safely approximating learning value functions 
described details algorithm showed ectiveness domains mountain car task real robot 
outlined method bootstrapping initial knowledge value function supplied initial policies looked limited version experience replay help value function approximation converge quickly 
algorithm works domains achieves performance competitive best published results 
key success enable safe value function predictions 
removing feature causes algorithm fail learn value function illustrating importance guarding propagating bad value estimates 
conservative predictions allows sure validity learned value function limits generalization abilities 
value function wellbehaved refuse predictions outside area supported training data 
problem domains looked 
trajectories state space jump areas coverage step 
issues related merit attention 
concatenation states actions function approximator unsatisfying 
standard euclidean distance metric compare vectors believe metric perform better 
improvements representation learning algorithm including learning dynamics model tandem value function better action selection mechanism 
allow powerful algorithms reinforcement learning literature limited data available 
exploration exploitation problem important issue currently addressed 
logical continuation learned value function model built knowledge training data coverage generate appropriate exploratory actions 
acknowledgments cindy grimm kee kim help providing example policies mountain car task 
reviewers helpful comments 
supported darpa contract dabt 
asada noda hosoda 

purposive behaviour acquisition real robot vision reinforcement learning 
machine learning 
atkeson moore schaal 

locally weighted learning 
ai review 
baird moore 

gradient descent general reinforcement learning 
advances neural information processing systems proceedings conference 
cambridge mit press 
boyan moore 

generalization reinforcement learning safely approximating value function 
advances neural information processing systems proceedings conference pp 

cambridge ma mit press 
brent 

algorithms minimization derivatives 
englewood cli nj prentice hall 
cook 

uential observations linear regression 
journal american statistical association 
gordon 

stable function approximation dynamic programming 
proceedings twelfth international conference machine learning 
san francisco morgan kaufmann 
lin 

self improving reactive agents reinforcement learning planning teaching 
machine learning 
mahadevan 

enhancing transfer reinforcement learning building stochastic models robot actions 
proceedings ninth international conference machine learning pp 

san francisco morgan kaufmann 
singh sutton 

reinforcement learning replacing eligibility traces 
machine learning 
sutton 

generalization reinforcement learning successful examples sparse coarse coding 
advances neural information processing systems proceedings conference pp 

cambridge ma mit press 
thrun schwartz 

issues function approximation reinforcement learning 
proceedings fourth connectionist models summer school 
hillsdale nj lawrence erlbaum 
watkins dayan 

learning 
machine learning 
williams 

simple statistical algorithms connectionist reinforcement learning 
machine learning 
