locally weighted learning christopher atkeson andrew moore stefan schaal college computing georgia institute technology atlantic drive atlanta ga cga cc gatech edu cc gatech edu www cc gatech edu fac chris atkeson www cc gatech edu fac stefan schaal carnegie mellon university forbes ave pittsburgh pa cs cmu edu www cs cmu edu hp html atr human information processing research laboratories seika cho soraku gun kyoto japan october surveys locally weighted learning form lazy learning memorybased learning focuses locally weighted linear regression 
survey discusses distance functions smoothing parameters weighting functions local model structures regularization estimates bias assessing predictions handling noisy data outliers improving quality predictions tuning fit parameters interference old new data implementing locally weighted learning efficiently applications locally weighted learning 
companion surveys locally weighted learning robot learning control 
keywords locally weighted regression loess lwr lazy learning memorybased learning commitment learning distance functions smoothing parameters weighting functions global tuning local tuning interference 
lazy learning methods defer processing training data query needs answered 
usually involves storing training data memory finding relevant data database answer particular query 
type learning referred memory learning 
relevance measured distance function nearby points having high relevance 
form lazy learning finds set nearest neighbors selects votes predictions stored points 
surveys form lazy learning locally weighted learning uses locally weighted training average interpolate extrapolate combine training data vapnik bottou vapnik vapnik bottou 
learning methods single global model fit training data 
query answered known processing training data training local models possible lazy learning 
local models attempt fit training data region location query query point 
examples types local models include nearest neighbor weighted average locally weighted regression 
local models combine points near query point estimate appropriate output 
nearest neighbor local models simply choose closest point output value 
weighted average local models average outputs nearby points inversely weighted distance query point 
locally weighted regression fits surface nearby points distance weighted regression 
weighted averages locally weighted regression discussed sections survey focuses locally weighted linear regression 
core survey discusses distance functions smoothing parameters weighting functions local model structures 
lessons learned research locally weighted learning practical implementations require dealing locally inadequate amounts training data regularization estimates deliberate bias methods predicting prediction quality filtering noise identifying outliers automatic tuning learning algorithm parameters specific tasks data sets efficient implementation techniques 
motivation exploring locally weighted learning techniques came suitability real time online robot learning fast incremental learning avoidance negative interference old new training data 
provide example interference clarify point 
briefly survey published applications locally weighted learning 
companion atkeson surveys locally weighted learning robot learning control 
review augmented web page atkeson 
review emphasizes statistical view learning function approximation plays central role 
order concrete review focuses narrow problem formulation training data consists input vectors specific attribute values corresponding output values 
input output values assumed continuous 
alternative approaches problem formulation include statistical nonparametric regression techniques multi layer sigmoidal neural networks radial basis functions regression trees projection pursuit regression global regression techniques 
discussion section section argues locally weighted learning applied broader context 
global learning methods improved localizing locally weighted training criteria vapnik bottou vapnik vapnik bottou 
survey emphasizes regression applications real valued outputs discussion section outlines techniques applied classification discrete outputs 
conclude short discussion research directions 
nearest neighbor input output input output weighted average input output input output locally weighted regression input output input output fits different types local models data points 
notation scalars represented italic lower case letters 
column vectors represented boldface lower case letters row vectors represented column vectors transposed 
matrices represented bold face upper case letters 
distance weighted averaging illustrate locally weighted learning distance function applied consider simple example distance weighted averaging 
turn form locally weighted regression local model constant 
prediction average training values fy estimate minimizes criterion gamma case training values fy taken different conditions fx sense emphasize data similar query dissimilar data treat training data equally 
equivalent ways weighting data directly weighting error criterion choose weighting data directly weighting data viewed replicating relevant instances discarding irrelevant instances 
case instance represented data point 
relevance measured calculating distance query point data point input vector typical distance function euclidean distance ith input vector jth component vector de gamma gamma gamma weighting function kernel function calculate weight data point distance 
typical weighting function gaussian gammad weight weighted average note estimate depends location query point query point unweighted averaging springs 
query point locally weighted averaging springs 
weighting error criterion trying find best estimate outputs local model constant distance weighting error criterion corresponds requiring local model fit nearby points concern distant points gamma best estimate minimize cost 
value 
achieved equation case weighting error criterion weighting data equivalent 
note criterion estimate depend location query point process physical interpretation 
figures show data points black dots fixed space pulling horizontal line constant model springs 
strength springs equal unweighted case position horizontal line minimizes sum stored energy springs equation 
ignore factor energy calculations simplify notation 
weighted case springs equal spring constant spring 
stored energy springs case equation minimized physical process 
note locally weighted average emphasizes points close query point produces answer height horizontal line closer height points near query point unweighted case 
distance weighted averaging literature statistics approach fitting constants locally weighted training criterion known kernel regression vast literature wand jones 
nadaraya watson proposed weighted average set nearest neighbors regression 
approach independently reinvented computer graphics shepard 
specht describes memory neural network approach probabilistic model motivates weighted averaging local model regression 
connell utgoff kibler 
aha applied weighted averaging artificial intelligence problems 
locally weighted regression locally weighted regression lwr local models fit nearby data 
described section derived weighting training criterion local model general case directly weighting data case local model linear unknown parameters 
lwr derived standard regression procedures global models 
start exploration lwr reviewing regression procedures global models 
nonlinear local models nonlinear global models general global model trained minimize unweighted training criterion fi output values corresponding input vectors fi parameter vector nonlinear model fi general loss function predicting training data example model neural net fi vector synaptic weights 
squares criterion loss function gamma leading training criterion fi gamma values parameters global model provide approximation true function 
approaches problem 
larger complex global model hope approximate data sufficiently 
second approach discuss fit simple model local patches region interest 
training criterion nonlinear local models data set tailored query point emphasizing nearby points regression 
weighting training criterion fi weighting kernel function distance data point query training criterion fi local model different set parameters fi query point linear local models local models advantageous keep simple keep training criterion simple 
leads explore local models linear unknown parameters squares training criterion 
derive squares training algorithms linear local models regression procedures linear global models 
linear global models global model linear parameters fi expressed myers fi follows assume constant appended input vectors include constant term regression 
training examples collected matrix equation xfi matrix ith row vector ith element dimensionality theta number data points dimensionality estimating parameters fi unweighted regression minimizes criterion fi gamma solving normal equations fi fi fi gamma inverting matrix numerically best way solve normal equations point view efficiency accuracy usually matrix techniques solve equation press 
weighting criterion physical interpretation fitting line plane set points unweighted regression gives distant points equal influence nearby points ultimate answer query equally spaced data 
linear local model specialized query emphasizing nearby points 
distance weighted average example weight error criterion minimized weight data directly 
approaches equivalent planar local models 
weighting criterion done way fi gamma physical interpretation equation 
thin plate splines minimize bending energy plate energy constraints pulling query point unweighted springs 
query point weighted springs 
plate locally weighted regression interpreted physical process 
lwr planar local model line figures rotate translate 
springs forced remain oriented vertically move smallest distance data points line 
shows fit line produced equally strong springs set data points black dots minimizing criterion equation 
shows happens fit springs nearer query point strengthened springs away weakened 
strengths springs fit minimizes criterion equation 
direct data weighting version directly weighting data involves steps 
computational analytical simplicity origin input data shifted subtracting query point data point making query point appended constant term regression 
distance calculated stored data points query point weight stored data point square root kernel function equation simplify notation row multiplied corresponding weight creating new variables done matrix notation creating diagonal matrix diagonal elements ii zeros multiplying times original variables 
wx wy equation solved fi new variables fi formally gives estimator form gamma relationship kernel regression locally weighted regression data distributed regular grid away boundary locally weighted regression kernel regression equivalent muller 
irregular data distributions significant difference lwr advantages kernel regression hastie loader jones 
lwr planar local model preferred kernel smoothing exactly reproduces line data distribution 
failure reproduce line function generate training data indicates bias function approximation method 
lwr methods planar local model fail reproduce quadratic function reflecting bias due planar local model 
lwr methods quadratic local model fail reproduce cubic function 
locally weighted regression literature cleveland loader fan fan gijbels review history locally weighted regression discuss current research trends 
sabin survey distance weighted nearest neighbor fit surfaces arbitrarily spaced points eubank surveys nonparametric regression 
lancaster refer nearest neighbor approaches moving squares survey fitting surfaces data 
surveys kernel lwr approaches nonparametric regression 
farmer survey nearest neighbor local model approaches modeling chaotic dynamic systems 
local models polynomials century smooth regularly sampled time series interpolate extrapolate data arranged rectangular grids 
crain bhattacharyya falconer suggested weighted regression irregularly spaced data fit local polynomial model point function evaluation desired 
available data points 
data point weighted function distance desired point regression 
authors suggested fitting polynomial surface nearby points distance weighted regression brent palmer walters whittle stone tukey franke nielson friedman cleveland proposed robust regression procedures eliminate outlying erroneous points regression process 
programs implementing refined version approach loess available directly part package cleveland cleveland loader 
katkovnik developed robust locally weighted smoothing procedure 
cleveland 
analyzes statistical properties loess algorithm cleveland devlin cleveland 
show examples 
stone devroye lancaster lancaster cheng li tsybakov provide analyses lwr approaches 
stone shows lwr optimal rate convergence minimax sense 
fan shows local linear regression smoothers best smoothers asymptotic minimax linear smoother high asymptotic efficiency suitable choice kernel bandwidth possible linear smoothers including produced kernel orthogonal series spline methods unknown regression function class functions having bounded second derivatives 
fan extends result show lwr high minimax efficiency possible estimators including nonlinear smoothers median regression 
fan fan gijbels hastie loader jones 
show lwr handles wide range data distributions avoids boundary cluster effects 
ruppert wand derive asymptotic bias variance formulas multivariate lwr cleveland loader argue asymptotic results limited practical relevance 
fan gijbels explore variable bandwidth locally weighted regression 
vapnik bottou give error bounds local learning algorithms 
locally weighted regression introduced domain machine learning robot learning atkeson atkeson atkeson explored techniques detecting irrelevant features 
atkeson schaal explore locally weighted learning point view neural networks 
dietterich 
report workshop memory learning including locally weighted learning 
distance functions locally weighted learning critically dependent distance function 
different approaches defining distance function section briefly surveys 
distance functions locally weighted learning need satisfy formal mathematical requirements distance metric 
relative importance input dimensions generating distance measurement depends inputs scaled stretched squashed 
term scaling purpose having reserved term weight contribution individual points dimensions regression 
refer scaling factors 
ways define distance functions scott ffl global distance functions 
distance function parts input space 
ffl query local distance functions form distance function parameters set query optimization process typically minimizes cross validation error related criterion 
approach referred uniform metric stanfill discussed stanfill waltz hastie tibshirani friedman 
ffl point local distance functions stored data point associated distance function values corresponding parameters 
training criterion uses different point fi gamma selected direct computation minimizing cross validation error 
frequently chosen advance queries stored data points 
approach referred variable metric stanfill 
classifiers version point local distance function different distance function class waltz aha aha 
aha goldstone explore pointbased distance functions human subjects 
distance functions asymmetric nonlinear distance particular dimension depend query point value dimension larger smaller stored point value dimension medin shoben 
distance dimension depend values compared nosofsky 
feature scaling altering distance function serve purposes 
feature scaling factors nonzero input space warped distorted lead accurate predictions 
scaling factors set zero dimensions ignored distance function local model global directions 
zeroing feature scaling factors tool combat curse dimensionality reducing locality function approximation process way 
note feature scaling factor zero mean local model ignores feature locally weighted learning 
points aligned direction get weight feature affect output local model 
example fitting global model features equivalent setting feature scaling factors zero fitting model local model 
local model feature selection separate process distance function feature scaling 
ignoring features ridge regression dimensionality reduction entire modeling process algorithms feature scaling selection discussed sections 
stanfill waltz describe variant feature selection predictor restriction scaling factor feature large difference query dimension causes training point ignored 
describe initial prediction output augmented distance function select training data similar equal outputs goal restriction 
distance functions continuous inputs functions discussed section especially appropriate ordered vs categorical symbolic nominal input values continuous ordered set discrete values 
ffl unweighted euclidean distance de gamma gamma gamma ffl diagonally weighted euclidean distance dm gamma gamma gamma de mx mq feature scaling factor jth dimension diagonal matrix jj ffl fully weighted euclidean distance dm gamma gamma de mx mq longer diagonal arbitrary 
known mahalanobis distance tou gonzalez 
ffl unweighted norm minkowski metric jx gamma ffl diagonally weighted fully weighted norm weighted norm mx mq 
diagonal distance function matrix coefficient dimension radially symmetric scaling function axis parallel ellipse shows ellipses axes symmetry aligned coordinate axes 
shows example full distance function matrix cross terms arbitrarily orient ellipse ruppert wand wand jones 
cleveland grosse cleveland 
cleveland point distance functions zero coefficients entire column zero singular model global corresponding directions 
refer conditionally parametric model 
fukunaga james tou gonzalez describe choose distance function matrix maximize ratio variance classes variance cases classification 
mohri tanaka extend approach symbolic input values 
approach uses eigenvalue eigenvector decomposition help distinguish relevant attributes irrelevant attributes filter noisy data 
approach localized hastie tibshirani 
distance functions symbolic inputs developed discussed atkeson 
weighting function gaussian contours constant distance center diagonal matrix 
weighting function gaussian contours constant distance center matrix diagonal elements 
smoothing parameters smoothing bandwidth parameter defines scale range generalization performed 
ways parameter scott cleveland loader ffl fixed bandwidth selection constant value fan marron volumes data constant size shape 
case appear implicitly distance function determinant fully weighted distance functions jmj magnitude vector diagonally weighted distance functions jmj explicitly weighting function parameters redundant explicit case provide convenient way adjust radius weighting function 
redundancy eliminated requiring determinant scaling factor matrix jmj fixing element ffl nearest neighbor bandwidth selection set distance kth nearest data point stone cleveland farmer hastie loader fan gijbels ge wang cleveland loader 
data volume increases decreases size density nearby data 
case changes scale distance function canceled corresponding changes giving scale invariant weighting pattern data 
cancel changes distance function coefficients alter shape weighting function identity kth neighbor change distance function shape changes 
ffl global bandwidth selection set globally optimization process typically minimizes cross validation error data 
ffl query local bandwidth selection set query optimization process typically minimizes cross validation error related criterion vapnik 
ffl point local bandwidth selection stored data point associated bandwidth weighted criterion uses different point fi gamma set direct computation optimization process typically minimizes cross validation error related criterion 
typically computed advance queries stored data points 
fan marron argue fixed bandwidth easy interpret limited 
cleveland loader argue favor nearest neighbor smoothing fixed bandwidth smoothing 
fixed bandwidth weighting function goes zero finite distance large variance regions low data density 
problem edges data clusters gets worse higher dimensions 
general fixed bandwidth selection larger changes variance nearest neighbor bandwidth selection 
fixed bandwidth smoother data span leading undefined estimates cleveland loader 
fan marron describe reasons variable local bandwidths adapt data distribution adapt different levels noise heteroscedasticity adapt changes smoothness curvature function 
fan gijbels argue point favor query local bandwidth selection explaining having bandwidth associated data point allow rapid asymmetric changes behavior data accommodated 
section discusses global local tuning bandwidths 
weighting functions requirements weighting function known kernel function straightforward gasser muller cleveland loader fedorov 
maximum value weighting function zero distance function decay smoothly distance increases 
discontinuities weighting functions lead discontinuities predictions training points cross discontinuity query changes 
general smoother weight function smoother estimated function 
weights go infinity query equals stored data point allow exact interpolation stored data 
finite weights lead smoothing exp exp uniform kernel shapes described text 
data 
weight functions go zero finite distance allow faster implementations points query distance ignored error 
mentioned previously kernels fixed finite radius raise possibility having points non zero area possibility handled locally weighted learning system 
necessary normalize kernel function kernel function need unimodal 
kernel function non negative negative value lead training process increasing training error order decrease training criterion 
weights square root kernel function positive negative 
non negative weights 
kernel functions discussed section shown 
simple weighting function just raises distance negative power shepard atkeson ruprecht ruprecht muller 
magnitude power determines local regression rate dropoff weights distance 
type weighting function goes infinity query point approaches stored data point forces locally weighted regression exactly match stored point 
data noisy exact interpolation desirable weighting scheme limited magnitude desired 
inverse distance wolberg approximate functions equation quadratic kernel defined value 
smoothing weight function gaussian kernel wand schaal atkeson exp gammad kernel infinite extent 
related kernel exponential kernel psychological models aha goldstone exp gamma jdj kernels infinite extent truncated smaller threshold value ignore data particular radius query 
quadratic kernel known epanechnikov kernel bartlett priestley kernel epanechnikov altman hastie loader fan gijbels fan hall gamma jdj kernel finite extent ignores data radius query building local model 
fan marron muller argue kernel function optimal mean squared error sense 
discontinuity derivative kernel attractive real applications analytical treatments 
kernel cleveland cleveland devlin diebold nason lebaron 
wang 
ge 
gamma jdj jdj kernel finite extent continuous second derivative means second derivative prediction continuous 
comparison uniform weighting kernel boxcar weighting kernel stone friedman tsybakov muller jdj triangular kernel locally weighted median regression gamma jdj jdj variant triangular kernel franke nielson ruprecht muller ruprecht jdj jdj general new kernel functions created raising kernel functions power :10.1.1.34.9472
example kernel square quadratic kernel 
power non integral 
triangular kernels form family 
ruprecht muller generalize distance function point set metric 
view clear evidence choice weighting function critical scott cleveland loader examples show differences fedorov 
cleveland loader criticize uniform kernel similar reasons signal processing spectrum estimation 
optimal kernels discussed gasser muller gasser 
scott fedorov 

finite extent kernel function useful literature noted substantial empirical difference cases 
local model structures far discussed kinds local models constant linear local models 
limits model structure local model 
models linear unknown parameters local polynomials train faster general models 
major component lookup cost training cost important benefit 
cleveland devlin atkeson farmer cleveland loader 
wang 
applied local quadratic cubic models analyzed ruppert wand 
higher order polynomials reduce bias increase variance estimates 
locally constant models handle flat regions handle areas high curvature peaks valleys 
cleveland loader approach blending polynomial models non integral model order indicates weighted blend integral model orders 
cross validation optimize local model order query 
regularization insufficient data prediction bias uniquely interpolate extrapolate training data express preference learning bias 
function approximation preference typically expressed smoothness criterion optimize 
case locally weighted learning smoothness constraint explicit 
fit parameters affect smoothness predicted outputs 
smoothing bandwidth important control knob ridge regression parameter described section 
order local model serve smoothing parameter 
shape distance weighting functions play secondary role smoothing estimates general number derivatives respect exist determine order smoothness predicted outputs 
important link smoothness control overfitting 
seifert gasser explore variety approaches handling insufficient data local regression 
ridge regression potential problem data points distributed way regression matrix equation nearly singular 
nearby points non zero weights directions different equations solve unknown parameters fi 
ridge regression draper smith prevent problems due singular data matrix 
equation equation solved fi fi fi diagonal matrix small positive diagonal elements delta delta delta delta delta delta 
delta delta delta fi apriori estimate expectation local model parameters fi taken vector zeros 
equivalent adding extra rows having single non zero element ith column 
equation delta delta delta delta delta delta 
delta delta delta fi fi fi 
fi adding additional rows viewed adding fake data absence sufficient real data biases parameter estimates fi draper smith 
view ridge regression parameters bayesian assumptions apriori distributions estimated parameters 
described section tuning optimizing ridge regression parameters cross validation identify irrelevant dimensions 
techniques help combat overfitting 
dimensionality reduction principal components analysis pca globally eliminate directions data wettschereck 
rarely case dimensional input points scattered dimensional nonlinear manifold 
absolutely data particular direction 
closely related technique singular value decomposition svd typically locally weighted regression perform dimensionality reduction 
cleveland grosse compute inverse singular value decomposition set small singular values zero calculated inverse 
corresponds eliminating directions local model 
principal components analysis done locally weighted data stored data point response query 
directions eliminated hard fashion explicitly setting corresponding parameters zero soft fashion performing ridge regression coordinate system defined pca svd 
bregler omohundro interesting locally weighted learning approach identifying low dimensional submanifolds data lying 
space dimensions dot locally embedded dimensional curve 
bregler omohundro method uses locally weighted principal component analysis performs singular value decomposition matrix equation identify local manifolds 
useful analysis tool identifying local dependencies variables dataset 
important consequences developing local distance function principal component matrix reveals directions input space data support 
approaches consider input space space spanned 
important consider outputs performing distance function smoothing parameter optimization 
outputs provide opportunities dimensionality reduction flat direction predicted local model 
alternative perspective consider conditional probability yjx 
local principal components analysis joint density space eliminate input directions contribute predicting outputs 
potential problem dimensionality reduction general new dimensions aligned previous dimensions necessarily meaningful 
focus reducing prediction error ignoring comprehensibility local models 
assessing predictions important aspect locally weighted learning possible estimate prediction error derive confidence bounds predictions 
bottou vapnik vapnik bottou analyze confidence intervals locally weighted classifiers 
start analysis locally weighted regression pointing lwr estimator linear output data equations gamma wy vector written useful calculating bias variance locally weighted learning 
estimating variance calculate variance prediction assume training data came sampling process measures output values additive random noise ffl ffl independent zero mean variance oe 
assumption oe oe oe constant linear model correctly models structure data linear regression generates unbiased estimate regression parameters 
additionally error normally distributed ffl oe regression estimate best linear unbiased estimate maximum likelihood sense 
stated explicitly avoid distributional assumption form noise 
model additive noise dropping assumption linear model correctly models structure data expectation variance estimate equation var gamma oe way derive confidence intervals predictions locally weighted learning assume locally constant variance oe prediction point equation 
equation modified reflect additive noise sampling new point oe prediction error estimator oe 
var new oe oe expression prediction intervals independent output values training data reflects data distributed input space 
variance reflects difference prediction mean prediction difference prediction true value requires knowledge predictor bias 
local model structure correct bias zero 
conveniently derive estimate oe define additional quantities terms weighted variables 
locally weighted linear regression centered point produces local model parameters fi 
produces errors residuals training points 
weighted residual defined equation fi gamma training criteria weighted sum squared errors reasonable estimator local value noise variance oe lwr lwr lwr modified measure data points lwr analogy unweighted regression myers reduce bias estimate oe account number parameters locally weighted regression oe lwr gamma lwr lwr measure local number free parameters local model lwr gamma described variance estimator uses local information 
alternative way obtain variance estimate uses global information information lwr fit assumes single global value additive noise cleveland cleveland grosse cleveland 
estimating bias assessing bias requires making assumptions underlying form true function data distribution 
case locally weighted learning weak assumption need know local behavior function local distribution data 
assume real function described locally quadratic model gamma gamma gamma query point true gradient query point true hessian matrix second derivatives query point 
expected value estimate equation find bias bias gamma true gamma equation solved know true function 
example locally quadratic function plug quadratic function equation equation get bias gamma gamma gamma gamma locally weighted regression process generates guarantees linear local model exactly matches linear trend data gamma 
bias depends quadratic term katkovnik cleveland loader bias gamma gamma assuming ridge regression parameters set zero 
formula raises temptation estimate cancel bias estimating second derivative matrix clear better simply quadratic local model linear local model 
quadratic local model eliminate local bias due quadratic term remove need distance metric compensate different curvature different directions 
course quadratic local model bias due cubic terms taylor series true function elimination require estimation cubic terms cubic local model 
principled termination cycle 
assessment cross validation assess locally weighted learning doing testing experience memory predicted rest experiences 
simple measure ith prediction error difference predicted output input observed value non parametric learners overfitting data measure may deceptively small 
example nearest neighbor learner error measure zero closest neighbor 
sophisticated measure ith prediction error leave cross validation error experience removed memory prediction 
cv output predicted input memory ith point removed gamma gamma ith leave cross validation error cv cv gamma 
lazy learning formalism takes place prediction time expensive predict value data point removed included 
contrasts majority learning methods explicit training stage cases easy pick earlier experience temporarily pretend see 
ignoring training data point typically requires retraining scratch modified training set fairly expensive nonlinear parametric model neural network 
addition dependence nonlinear parametric training initial parameter values complicates analysis 
handle effect correctly training runs different starting values undertaken different data set 
avoided locally weighted learning local models linear unknown parameters tuning fit parameters reintroduce problem 
tuning fit parameters background process operates slower time scale adding new data answering queries 
cross validation performed locally just fitting locally linear model query point cleveland loader 
consider locally weighted average squared cross validation error mse cv training point myers mse cv cv estimate requires locally weighted regression performed training point non zero weight 
imagine storing cv training point value updated new data learned 
approximate cv cv generate mse cv cv cv lwr cv weighted cross validation error point removed locally weighted regression centered weighted cross validation residual cv related weighted residual myers cv gamma gamma obtain final equation mse cv mse cv lwr gamma gamma equation local version press statistic myers 
allows perform leave cross validation recalculating regression parameters excluded point 
computationally efficient 
optimal fit parameters example section try find optimal fit parameters distance metric weighting function smoothing bandwidth simple example 
restrictive assumption data uniformly spaced rectangular grid 
approach question exploring kernel shapes dimension 
allow weights unknown numerically optimize minimize mean squared error 
assume underlying function quadratic second derivative equation additive independent identically distributed zero mean noise equation variance oe sampled data regularly spaced kernel shape minimizes mean squared error dimension 
large single dot predicted value deviation zero correct value reveals bias 
vertical bars show standard deviation prediction square root variance greatly reduced standard deviation sigma original data 
set large dots optimized minimize mean squared error prediction reveal optimal kernel shape criterion 
line points quadratic kernel appropriate bandwidth match optimized kernel values 
small dots value quadratic portion underlying function comparison 
distance delta data point delta 
equation solved query 
mean squared error sum bias equation squared variance equation 
quantity minimized adjusting weights resulting kernel shape shown 
kernel shape matches quadratic kernel gamma jdj described section 
numerical experimentation dimension revealed optimal scaling factor dimensional distance function approximately ch contour plot 
contour plot optimal kernel 
constant takes account issues data spacing delta standard deviation additive noise delta oe width resulting kernel directly related optimal smoothing bandwidth 
dimensions explore optimization distance metric 
optimizing values kernel data points current computational resources assume form kernel function quadratic kernel 
choose particular value hessian equation optimize scaling matrix multidimensional distance function minimize mean squared error 
optimal approximately satisfies equation ch dimensional case 
shows hessian matrix orient quadratic component arbitrary orientation 
distance function matrix needs full matrix order allow optimal kernel match orientation quadratic component 
numerical experiment chosen gamma gamma optimal scaling matrix numerical search gamma equation approximately satisfied gamma multiple identity matrix 
gamma gamma input output locally weighted regression approximating dimensional dataset shown black dots 
outlier 
input output locally weighted regression supplemented outlier removal 
noisy training data outliers averaging performed locally weighted regression process naturally filters noise weighting function infinite zero distance 
tuning process optimize noise filtering adjusting fit parameters smoothing parameters weighting function parameters ridge regression parameters choice local model structure 
useful explicitly identify outliers training points erroneous noise larger neighboring points 
example effect outlier effect outlier rejection shown 
robust regression see example hampel cross validation allow extensions locally weighted learners identify reduce effects outliers 
outliers identified removed globally identified ignored query query basis 
query outlier detection allows training points ignored queries queries 
areas explored detecting discontinuities nonstationarity training data 
global weighting stored points finding outliers possible attach weights stored points training process lookup points suspected unreliable aha kibler cost salzberg 
weights multiply weight weighting function 
totally unreliable points assigned weight zero leading ignored 
reliability weights cross validation stored point correctly predicts classifies neighbors 
approach utilize stored points shown reduce cross validation error aha 
important issues weighting decision decision reevaluated 
global methods typically assign weight point training case decision usually reevaluated asynchronous database maintenance process decisions reevaluated time process cycles entire database 
local weighting stored points finding outliers local outlier detection methods label points outliers queries global methods 
points outliers queries outliers 
generate weights training data query time cross validation nearby points 
press statistic myers modified serve local outlier detector locally weighted regression 
need standardized individual press residual called residual press oe gamma gamma measure zero mean unit variance assumes locally normal distribution error 
data point deviates zero certain threshold point called outlier 
conservative threshold discarding points lying outside area normal distribution 
applications cutting data outside area normal distribution 
robust regression approaches data outliers viewed having additive noise long tailed symmetric distributions 
robust regression useful global local detection outliers cleveland 
weighting function additionally points residuals gamma med je med median absolute value residuals weights 
process repeated times refine estimates tuning learning algorithms locally weighted learning usually needs tuned particular problem 
tuning means adjusting parameters learning algorithm 
locally weighted fit criteria fi gamma includes fit parameters bandwidth smoothing parameter distance metric weighting kernel function 
additional fit parameters ridge regression parameters outlier thresholds 
ways tune fit parameters 
ffl global tuning fit parameters set globally optimization process typically minimizes cross validation error data constant size shape volumes data answer queries 
ffl query local tuning fit parameters set query local information 
ffl point local tuning weighted training criteria uses different fit parameters point bandwidth distance metric weighting function possibly weight fi gamma typical implementations approach fit parameters computed advance queries stored data points 
approaches computing fit parameter values ffl plug approach fit parameters set direct computation 
ffl optimization approaches fit parameters set optimization process marron minimizes training set error minimizes test validation set error minimizes cross validation error cv minimizes generalized cross validation error gcv myers maximizes akaike information criterion aic adjusts fit parameters optimized isolation 
combination fit parameters generates particular fit quality 
fit parameter changed typically optimal values parameters change response 
locally constant model smoothing parameter distance function reflect flatness neighborhood different directions 
local model hyperplane smoothing parameter distance function reflect second derivative neighborhood 
local model quadratic third spatial derivative data dealt 
practical purposes useful clear understanding accurate non linear fit parameters fit 
intuition approximate values usually result barely distinguishable performance optimal parameters practical states true kernel regression 
section considers optimizing single set parameters possible queries global tuning 
section considers optimizing multiple sets parameters specific queries local tuning 
global tuning global cross validation particularly robust method tuning parameters special assumptions 
independent noise distribution data distribution underlying function cross validation value unbiased estimate set parameters perform new data drawn distribution old data 
robustness lead global cross validation applications attempt achieve high autonomy making assumptions general memory learning system described moore 
performs large amounts cross validation search optimize feature subsets diagonal elements distance metric smoothing parameter order regression 
continuous search continuous fit parameters continuous search possible 
inevitably local hill climbing large risk getting stuck local optima 
sum squared cross validation errors minimized nonlinear parameter estimation procedure nl sol dennis 
discussed section locally weighted learning approach computing cross validation error single point computationally expensive answering query 
quite different parametric approaches neural network new model network trained cross validation training set particular point removed 
addition local model linear unknown parameters analytically take derivative cross validation error respect parameters estimated greatly speeds search process 
optimized distance metric find input variables important function represented 
distance scaling factors go zero indicate directions irrelevant consistent local model structure global model suffice directions 
interpret ridge regression parameters 
ridge regression parameters irrelevant terms local model large fit parameter optimization process 
effect force corresponding estimated parameters fi apriori values fi corresponds dimensionality reduction 
relatively unexplored area stochastic gradient descent approaches optimizing fit parameters 
cross validation errors associated contributions derivative small random sample cross validation errors associated derivative contributions 
describes approach optimizing fit parameters partitioning training data subsets calculating cross validation errors subset data subset averaging results 
discrete search discrete search algorithms fit parameters active area research 
maron moore describe racing techniques find fit parameter values 
techniques compare wide range different types models simultaneously handle models discrete parameters 
bad models quickly dropped race focuses computational effort distinguishing models 
typically continuous fit parameters discretized maron moore 
techniques selecting features distance metric local model developed statistics draper smith miller including subsets forward regression backwards regression stepwise regression 
explored stepwise regression procedures determine terms local model useful similar results gradient search described 
feature selection hard problem features examined independently 
value feature depends features selected 
goal find set feature weights individual feature weights feature 
maron moore number algorithms doing described compared including methods monte carlo sampling 
aha gives algorithm constructs new features addition selecting features 
friedman gives techniques query dependent feature construction 
continuous vs discrete search discrete search explore settings discrete fit parameters select training algorithm features function approximation methods locally weighted regression neural networks rule systems 
continuous fit parameter optimization choices 
case 
blending output different approaches blending parameter ff continuous search choose model order algorithm features approximation method 
example ff optimized blend methods equation fff gamma ff cleveland loader approach automatically choose local model structure order polynomial model blending polynomial models non integral model order indicates weighted blend integral model orders 
cross validation optimize local model order query 
local tuning local fit parameter optimization referred adaptive variable statistics literature adaptive bandwidth variable bandwidth smoothers 
reasons consider local tuning dramatically increases number degrees freedom training process leading increased variance predictions increased risk overfitting data cleveland loader ffl adaptation data density distribution adaptation addition adaptation provided locally weighted regression procedure bottou vapnik 
normalized rms torque error lwr nn lwr nn nd training performance various methods joint arm dynamics 
ffl adaptation variations noise level training data 
variations known heteroscedasticity 
ffl adaptation variations behavior underlying function 
function may locally planar regions high curvature 
plug estimators derived local locally weighted training set error cross validation validation test set error drive optimization local model 
interference negative interference old new training data important motivations exploring locally weighted learning 
illustrate differences global parametric representations locally weighted learning approach sigmoidal feedforward neural network approach compared locally weighted learning approach problem 
architecture sigmoidal feedforward neural network taken goldberg pearlmutter section modeled arm inverse dynamics 
ability methods predict torques simulated joint arm random points compared atkeson 
plots normalized rms prediction error 
points sampled uniformly ranges comparable miller 
looked joint arm inverse dynamics modeling 
initially method trained training set random samples predictions torques separate test set random samples joint arm dynamics function assessed 
solid bar marked lwr location shows test set error locally weighted regression quadratic local model 
light bar marked nn location shows best test set error neural network 
methods generalize problem bars low error 
method trained attempts particular desired movement 
method successfully learned desired movement 
second round training performance random test set measured bars locations 
sigmoidal feedforward neural network lost memory full dynamics light bar location large error represented dynamics particular movements learned second training set 
interference new previously learned data prevented increasing number hidden units single layer network 
locally weighted learning method show interference effect solid bar location 
interference caused failure neural network model structure match arm inverse dynamics structure perfectly 
noise data concept drift causes eliminated possible sources interference 
argued sigmoidal neural network forgot original training data include data second training data set learning specific movement 
exactly point past data retained combat interference method lazy learning method 
case argue take advantage opportunity locally weight training procedure get better performance vapnik bottou vapnik vapnik bottou 
implementing locally weighted learning concerns locally weighted learning systems including locally weighted learning systems answer queries fast speed unacceptably degrade size database grows 
section explores concerns 
discuss fast ways find relevant data trees software special purpose hardware massively parallel computers current performance lwr implementation 
goal minimize need compromises forgetting discarding data keep database size limit instance averaging averages similar data maintaining elaborate data structure intermediate results accelerate query processing 
discuss lwr acceleration approaches limited low dimensional problems binning fan marron wand 
discussions fast implementations include seifert 
seifert gasser 
retrieving relevant data choice method storing experiences depends fraction experiences locally weighted regression computational technology available 
experiences locally weighted regression simply maintaining list array experiences sufficient 
nearby experiences included locally weighted regression efficient method finding nearest neighbors required 
nearest neighbor lookup accelerated serial proces sor tree data structure 
parallel processors special purpose processors typically parallel exhaustive search 
trees naively implemented search dimensional nearest neighbor database size requires distance computations 
nearest neighbor search implemented efficiently means tree bentley friedman bentley friedman bentley murphy selkow paliwal broder samet sproull 
tree binary data structure recursively splits dimensional space smaller subregions 
search nearest neighbor proceeds initially searching tree branches nearest query point 
frequently distance constraints mean need explore branches 
shows tree segmenting dimensional space 
shaded regions correspond areas tree searched 
generally nearest neighbor search leaf nodes need inspected 
query point marked distance nearest neighbor indicated circle 
black nodes inspected path leaf node 
access time asymptotically logarithmic size memory overhead costs mean nearly data points accessed supposed logarithmic search example dimensions fewer approximately uniformly distributed data points 
fact uniformly distributed data points tree size logarithmic performance noticeable increases exponentially dimensionality 
things alleviate problem 
data points distributed uniformly 
fact randomly distributed training data better 
second approximate algorithms find nearby experiences guaranteeing nearest operate logarithmic time 
empirically approximations greatly reduce prediction accuracy omohundro moore 
bump trees omohundro promising efficient approximation 
cleveland 
farmer grosse moore cleveland grosse loader wess 
deng moore lowe van der 
trees memory learning locally weighted regression 
special purpose devices special purpose hardware finding nearest neighbors long history taylor 
machines calculated manhattan euclidean distance stored points comparisons pick winning point 
current version technology wafer scale memory reasoning devices proposed kitano 
devices allocate processor data point handle approximately data points inch wafer 
designers exploited properties memory learning ways 
resolution computed distance critical analog adders multipliers weighting distance calculations digital circuits saving space silicon processors 
second device robust faulty processors faulty processor causes loss single data point 
authors advocate simply ignoring processor failures possible map faulty processors skip loading data 
massively parallel implementations nearest neighbor systems implemented massively parallel connection machines waltz 
massively parallel computer cm cm hillis exhaustive search faster trees due limited number experiences allocated processor 
connection machine processors simulate parallel computer processors 
experiences stored local memory associated processor 
experience compared desired experience processor processors running parallel hardwired global bus find closest match constant time independent number stored experiences 
search time depends linearly number dimensions distance metric distance metric changed easily depend current query point 
critical feature massively parallel computer system associative memories addition multiple processors higuchi 
processors transputers processor kx bits associative memory increases effective number processors 
architecture suited memory learning distance metric involves exact matches symbolic fields operation associative memory chips support 
associative memories implement euclidean distance basic operation 
implementations memory translation parsing kitano higuchi sumita kitano 
current generic parallel computer order standard microprocessors tightly connected communication network 
examples design cm snap system kitano 
details communication network critical locally weighted learning time critical processing consists broadcasting query processors determining answer best easily done prespecified communication pattern 
form communication difficult implement 
machine thousands processors exhaustive search obvious nearest neighbor algorithm 
processors probably maintain sort search data structure tree local trees may small efficient search performance 
kitano 
describe implementation memory reasoning snap system 
type parallel computer excellent locally weighted learning regression calculation dominates lookup time large fraction points regression 
implementing locally weighted regression locally weighted learning minimizes computational cost training new data points simply stored memory 
price trivial training costs expensive lookup procedure 
locally weighted regression uses relatively complex regression procedure form local model expensive nearest neighbor weighted average memory learning procedures 
query new local model formed 
rate local models formed evaluated limits rate queries answered 
implemented locally weighted regression procedure mhz intel microprocessor 
peak computation rate processor mflops 
achieved effective computation rates mflops learning problem input dimensions output dimensions linear local model 
leads lookup time approximately milliseconds database points exhaustive search 
time includes distance weight calculation stored points forming regression matrix solving normal equations 
applications locally weighted learning presence loess software statistics package lead locally weighted regression standard tool areas including modeling biological motor control feeding cycles smokers lead induced categories tonal alignment spoken english growth sexual maturation disease cleveland cleveland 
atkeson 
survey applying locally weighted learning robot control 
explored locally weighted regression robot control modeling time series compared lwr neural networks methods 
connolly compared different approximation schemes neural nets kohonen maps radial basis functions local fits simulated robot inverse kinematics added noise showed local polynomial fits accurate methods 
van der 
learned robot kinematics local linear models leaves tree data structure 
tadepalli ok apply local linear regression reinforcement learning 
baird klopf apply nearest neighbor techniques weighted averaging reinforcement learning thrun thrun sullivan apply similar techniques robot learning 
connell utgoff interpolated value function locally weighted averaging balance inverted pendulum pole moving cart 
peng performed cart pole task locally weighted regression interpolate value function 
aha salzberg explored nearest neighbor locally weighted learning approaches tracking task robot pursued caught ball 
mccallum explored lazy learning techniques situations states completely measured 
farmer apply locally weighted regression modeling prediction chaotic dynamic systems 
huang uses nearest neighbor weighted averaging techniques cache simulation results accelerate movement planner 
lawrence 
compare neural networks local regression methods benchmark problems 
local regression outperformed neural networks half benchmarks 
factors affecting performance included data differing density input space noise level dimensionality nature function underlying data 
researchers applied locally weighted averaging regression free form deformation morphing image interpolation computer graphics wolberg ruprecht muller ruprecht 
jr grosse describe locally weighted regression scientific visualization auralization data 
ge 
apply locally weighted regression predict cell density process 
nearest neighbor weighting weighting function 
principal components cross validation select features globally 
locally weighted regression outperformed methods including global nonlinear regression 
hammond lwr model 

wang 
apply locally weighted regression analytical chemistry 
global principal components reduce dimensionality inputs cross validation set number components 
explore weighted euclidean distance metrics including weighting depending range data principal component coordinates weighting depending dimension predicting output distance metric includes output value 
quadratic local model weighting function 
cross validation select number points include local regression 
important point optimal experiment design quite different locally weighted regression compared global linear regression 

apply memory learning water demand forecasting 
select features akaike information criterion aic locally weighted averaging neighborhood 
default temporally local regression scheme points neighborhood 
error rates set feature weights perform outlier removal 
applies locally weighted regression analysis modeling coding prediction speech signals 
uses singular value decomposition reduce dimensionality regression fixed value determined criteria 
uses closest points form local model 
distance nearest point estimate confidence prediction 
clustering process inputs outputs handle noise mapping problems 
tree speed nearest neighbor search 
process lead significant improvement linear predictor 
johnson apply locally weighted regression interpolating air quality measurements 
cross validation optimize smoothing parameter globally find defined minimum smoothing parameter 
describe lwr model automobile emissions 
walden prescott lwr remove trends time series involving climate data 
estimated variance noise level time series climate data having removed mean lwr 
locally weighted regression applied economics econometrics wallace lebaron 
rose lwr model exchange rates conclude significant nonlinearity exists data 
diebold nason lwr predict exchange rates success nonparametric regression techniques 

raz 
lwr smooth biological evoked potential data explore approaches choosing smoothing parameter 
bottou vapnik apply locally weighted classification optical character recognition ocr 
rust apply lwr marketing data 
range applications locally weighted techniques statistics cleveland cleveland loader 
idea local fitting extended likelihood regression models tibshirani hastie hastie tibshirani applied locally weighted techniques distributional settings logistic regression developed general fitting algorithms 
applied locally weighted regression estimation distribution density functions 
cleveland 
applied locally weighted regression density estimation spectrum estimation predicting binary variables 
fan applied locally weighted regression spectral density estimation 
discussion local learning approach 
explore idea local learning useful consider global learner global distributed representation typically characterized 
incrementally learning single new training point affects parameters 

prediction answer query depends parameters 
characteristics distributed representations 
additional criterion 
fewer parameters data 
serve definition global representation model predictor true particular method 
possible local methods attribute attributes low resolution tabular representation non overlapping cells 
part design space explored learning algorithms huge numbers parameters distributed representations 
different views constitutes local learning local representations local selection locally weighted learning 
lead confusion convoluted terminology 
local representation new data point affects small subset parameters answering query involves small subset parameters 
view local learning stems distinction local distributed representations neuroscience thorpe 
examples local representations lookup tables exemplar prototype classifiers 
necessarily case number parameters representation order number data points considerable amount local averaging occur 
local selection methods store training data distance function determine stored points relevant query 
function local selection select single output nearest neighbor distance voting scheme nearest neighbor 
examples types approaches common include stanfill waltz aha 
locally weighted learning stores training data explicitly local selection approaches fits parameters training data query known 
critical feature locally weighted learning criterion locally weighted respect query location fit type parametric model data vapnik bottou vapnik vapnik bottou 
paradoxical situation seemingly global model structures polynomials multilayer sigmoidal neural nets called local models locally weighted training criterion 
data involved training local model long distant data matters nearby data 
explores locally weighted training procedures involves deferring processing training data query leading terms lazy learning commitment learning 
global approaches representations rules decision trees parametric models polynomials sigmoidal neural nets radial basis functions projection pursuit networks 
approaches transformed locally weighted approaches locally weighted training criterion vapnik bottou vapnik vapnik bottou scope locally weighted learning quite broad 
discuss locally weighted classification example 
locally weighted classification classification ways incorporate distance weighting 
nearest neighbor approaches number occurrences class closest points query counted class occurrences votes predicted 
distance weighting weight votes nearby data points receive votes distant points 
second way incorporate distance weighting classifier training incorporate cost criterion minimized training vapnik bottou vapnik vapnik bottou truei cost minimized truei cost predicting class training point true class truei weighting kernel function 
simple version approach select nearest points just train classifier data 
case uniform boxcar kernel 
form classifier constrained way 
locally weighted learning specifies form training criterion form performance algorithm 
third way incorporate distance weighting treat classification regression problem decision functions class decision function largest value query point determines class query 
training decision functions distance weighted gamma ij decision function class ij target decision function training point hastie tibshirani describe approach global approaches finding discriminants localized locally weighting algorithm directly criterion 
described fitting simple linear models distance weighted fit criterion 
imagine distance weighted criterion train linear decision functions linear discriminants create local classifiers 
possible train general models logistic regression perform classification locally weighted fashion 
requirements locally weighted learning locally weighted learning requirements ffl distance function locally weighted learning systems require measure relevance 
major assumption locally weighted learning relevance measured measure distance 
nearby training points relevant 
possible measures relevance general notions similarity 
distance function needs input objects produce number 
distance function need satisfy formal requirements distance metric 
ffl separable criterion locally weighted learning systems compute weight training point 
apply weight training criterion general function predictions training points separable way 
additive separability forms separability 
ffl data needs data compute statistics true statistical learning approaches 

run robot learning experiments performance improvements started occur order points training set typically collect points experiment 
amount training data needed highly problem dependent 
ffl labelled data training point needs associated output classification label regression function approximation number 
ffl representations requirements system nearest neighbor techniques locally weighted regression requires object produces fixed length vector values symbolic numeric list specified features 
general representations handled locally weighted learning approaches 
example general training criterion fl fi inputs outputs query complex objects entire semantic networks distance functions graph matching algorithms graph difference measuring algorithms graph transformation fi adjustable parameters elliot scott 
objects text computer files inputs japanese outputs english distance functions number characters output file difference program unix diff local model machine translation program adjustable parameters fi 
typical parameters expert system strengths rules changing fi affects rules selected application 
input space distance generalized take account output space distance output values training data predicted output fi pred useful function approximated distinct outputs similar inputs 
extensively explored current research possible locally weighted learning systems stored objects provide separate information query distance function local model hammond callan nguyen 
case training criterion fl fi example measures volatility stock market measure distance data points query price histories factors form local respect volatility predictive models prices fi lebaron 
example nationality input distance function requiring distance calculation symbolic values numeric features age height weight blood pressure build locally respect nationality distance weighted regression predict heart attack risk 
research directions view interesting areas research include ffl hybrid tuning algorithms developed independent continuous discrete fit parameter optimization techniques 
clear hybrid approach better approach 
parameters initially treated discrete continuous optimization performed optimal values approached example 
approach racing algorithms allow continuous tuning race racing fixed sets parameters 
ffl new forms local tuning far research focused locally tuning bandwidth smoothing parameters 
needs done locally tuning distance metrics ridge regression parameters outlier thresholds overfitting 
ffl multiscale local tuning dimensional fit parameters bandwidth model order locally optimized small neighborhoods 
multidimensional fit parameters distance scale parameters distance matrix set ridge regression parameters need larger neighborhoods different kinds regularization tuned locally 
different tuning processes interact 
ffl stochastic gradient approaches continuous tuning continuous optimization estimates gradient small numbers random queries exhaustive query sets promising approach efficient tuning algorithms moore schneider 
ffl properties massive cross validation discussed crossvalidation locally weighted learning particularly suited 
better understanding cross validation take place danger overfitting guarded extra level cross validation desirable 
ffl probabilistic approaches explore analogies locally weighted learning probabilistic models including bayesian models ting cameron jones 
ffl forgetting far forgetting played important role implementations robot learning run memory 
expect forgetting play important role expect necessary implement principled approach storage control 
ffl computational techniques enormous dataset sizes new data management algorithms may needed 
include principled ways forget coalesce old data compactly represent high dimensional data clouds ways samples datasets entire datasets case multi gigabyte datasets hardware software techniques managing data secondary storage 
ffl lazy learning review focussed pure form lazy learning data stored queries 
approach extreme circumstances tuning algorithms fit parameters store optimized fit parameters queries 
substantial amounts data compression achieved building set local models fixed locations techniques described 
addition computational speedup presence large datasets may statistical advantages compressing data merely storing fritzke schaal atkeson 
summary surveyed locally weighted learning 
local weighting weighting data error criterion turn global function approximation powerful alternative approaches 
means local weighting unnecessary bias global function fitting reduced higher flexibility obtained desirable properties smoothness statistical analyzability retained 
concentrated linear regression behaves local weighting surveyed ways tools conventional regression analysis global regression locally weighted regression 
major question concerned notion locality choice distance metric close metric points decisions automatically data 
field local learning large interest statistics community provided entry points literature 
locally weighted learning rapidly increasing popularity machine learning community outlook promising interesting statistical computational application oriented development 
acknowledgments support atkeson schaal provided atr human information processing research laboratories 
support atkeson provided air force office scientific research national science foundation presidential young investigator award 
support schaal provided german scholarship foundation alexander von humboldt foundation 
support moore provided science engineering research council nsf research initiation award iri research gift 
aaai 
ninth national conference artificial intelligence 
aaai press mit press cambridge ma 
aha 

incremental instance learning independent graded concept descriptions 
sixth international machine learning workshop pages 
morgan kaufmann san mateo ca 
aha 

study instance algorithms supervised learning tasks mathematical empirical psychological observations 
phd dissertation university california irvine department information computer science 
aha 

incremental constructive induction instance approach 
eighth international machine learning workshop 
morgan kaufmann san mateo ca 
aha goldstone 

learning attribute relevance context instance learning algorithms 
th annual conference cognitive science society pages 
aha goldstone 

concept learning flexible weighting 
th annual conference cognitive science society pages bloomington il 
lawrence erlbaum associates mahwah nj 
aha kibler 

noise tolerant instance learning algorithms 
eleventh international joint conference artificial intelligence pages 
morgan kaufmann san mateo ca 
aha 

learning relative attribute weights instance concept descriptions 
th annual conference cognitive science society pages 
lawrence erlbaum associates mahwah nj 
aha salzberg 

learning catch applying nearest neighbor algorithms dyna mic control tasks 
proceedings fourth international workshop artificial intelligence statistics pages ft lauderdale fl 
altman 

kernel nearest neighbor nonparametric regression 
american statistician 
atkeson 

local models control movement 
touretzky editor advances neural information processing systems pages 
morgan kaufman san mateo ca 
atkeson 

memory approaches approximating continuous functions 
casdagli eubank pages 
proceedings workshop nonlinear modeling forecasting september santa fe new mexico 
atkeson 

local learning 
www cc gatech edu fac chris atkeson local learning 
atkeson moore schaal 

locally weighted learning control 
artificial intelligence review 
press 
atkeson 

associative content addressable memories control robots 
proceedings th ieee conference decision control volume pages austin texas 
ieee cat 
ch 
atkeson 

associative content addressable memories control robots 
proceedings ieee international conference robotics automation scottsdale arizona 
atkeson schaal 

memory neural networks robot learning 
neurocomputing 
baird klopf 

reinforcement learning high dimensional continuous actions 
technical report wl tr wright laboratory wright patterson air force base ohio 
kirk af mil baird papers index html 


representation approximation surfaces 
rice editor mathematical software iii pages 
academic press new york ny 


practical approach pattern classification 
plenum press new york ny 


nonparametric estimation regression functions 
journal royal statistical society series 
bentley 

multidimensional binary search trees associative searching 
communications acm 
bentley friedman 

data structures range searching 
acm comput 
surv 
bentley weide yao 

optimal expected time algorithms closest point problems 
acm transactions mathematical software 


optimal kernel weights power criterion 
journal american statistical association 
bottou vapnik 

local learning algorithms 
neural computation 
bregler omohundro 

surface learning applications lipreading 
cowan 
pages 
gasser herrmann 

locally adaptive bandwidth choice kernel regression estimators 
journal american statistical association 
broder 

strategies efficient incremental nearest neighbor search 
pattern recognition 
callan fawcett rissland 

adaptive approach case search 
ijcai pages 
casdagli eubank editors 
nonlinear modeling forecasting 
proceedings volume xii santa fe institute studies sciences complexity 
addison wesley new york ny 
proceedings workshop nonlinear modeling forecasting september santa fe new mexico 
cheng 

strong consistency nearest neighbor regression function estimators 
journal multivariate analysis 
cleveland 

robust locally weighted regression smoothing scatterplots 
journal american statistical association 
cleveland 

nonparametric regression conditionally parametric fits 
technical report bell laboratories statistics department murray hill nj 
netlib att com netlib att stat doc 
cleveland 

visualizing data 
hobart press summit nj 
books hobart com 
cleveland devlin 

locally weighted regression approach regression analysis local fitting 
journal american statistical association 
cleveland devlin grosse 

regression local fitting methods properties computational algorithms 
journal econometrics 
cleveland grosse 

computational methods local regression 
statistics computing 
ftp cm bell labs com cm cs doc ps gz 
cleveland grosse 

local regression models 
chambers hastie editors statistical models pages 
wadsworth pacific grove ca 
netlib att com netlib ps cleveland loader 

computational methods local regression 
technical report bell laboratories statistics department murray hill nj 
netlib att com netlib att stat doc 
cleveland loader 

local fitting semiparametric nonparametric regression comments fan marron 
technical report bell laboratories statistics department murray hill nj 
netlib att com netlib att stat doc ps earlier version ps 
cleveland loader 

smoothing local regression principles methods 
technical report bell laboratories statistics department murray hill nj 
netlib att com netlib att stat doc 
cleveland mallows 

ats methods nonparametric regression non gaussian data 
journal american statistical association 
connell utgoff 

learning control dynamic physical system 
sixth national conference artificial intelligence pages seattle wa 
morgan kaufmann san mateo ca 
cost salzberg 

weighted nearest neighbor algorithm learning symbolic features 
machine learning 
jr grosse 

seeing hearing dynamic loess surfaces 
interface proceedings pages 
springer verlag 
ftp cm bell labs com cm cs doc ps gz long 
ps gz 
cowan tesauro alspector editors 
advances neural information processing systems 
morgan kaufman san mateo ca 
crain bhattacharyya 

treatment dimensional data digital computer 



estimation non param del la par en es 
revue statistique appliqu 
deng moore 

multiresolution instance learning 
fourteenth international joint conference artificial intelligence 
morgan kaufmann san mateo ca 
dennis gay 

adaptive nonlinear squares algorithm 
acm transactions mathematical software 
devroye 

convergence nonparametric regression function estimates 
annals statistics 
diebold nason 

nonparametric exchange rate prediction 
journal international economics 
dietterich wettschereck atkeson moore 

memory methods regression classification 
cowan 
pages 
draper smith 

applied regression analysis 
john wiley new york ny nd edition 
elliot scott 

instance generalization learning procedures applied solving integration problems 
proceedings eighth conference society study artificial intelligence pages leeds england 
springer verlag 
epanechnikov 

nonparametric estimation multivariate probability density 
theory probability applications 
eubank 

spline smoothing nonparametric regression 
marcel dekker new york ny 
falconer 

general purpose algorithm contouring scattered data points 
technical report nac national physical laboratory united tw lw 
fan 

design adaptive nonparametric regression 
journal american statistical association 
fan 

local linear regression smoothers minimax efficiencies 
annals statistics 
fan 

local modeling 
ees update written encyclopedia statistics science www stat unc edu faculty fan papers html 
fan gijbels 

variable bandwidth local linear regression smoothers 
annals statistics 
fan gijbels 

censored regression local linear approximations applications 
journal american statistical association 
fan gijbels 

adaptive order polynomial fitting bandwidth bias reduction 
fan gijbels 

data driven bandwidth selection local polynomial fitting variable bandwidth spatial adaptation 
journal royal statistical society series 
fan gijbels 

local polynomial modeling applications 
chapman hall london 
fan hall 

curve estimation minimizing mean absolute deviation implications 
annals statistics 
fan 

automatic local smoothing spectral density estimation 
ftp stat unc edu pub fan spec ps 
fan marron 

comment hastie loader 
statistical science 
fan marron 

fast implementations nonparametric curve estimators 
journal computational graphical statistics 
fan marron 

discussion cleveland loader 
farmer 

predicting chaotic time series 
physical review letters 
farmer 

exploiting chaos predict reduce noise 
lee editor evolution learning cognition pages 
world scientific press nj 
available technical report la ur los alamos national laboratory los alamos new mexico 
farmer 

predicting chaotic dynamics 
kelso schlesinger editors dynamic patterns complex systems pages 
world scientific nj 


multivariate interpolation scattered data moving squares methods 
mason cox editors algorithms approximation pages 
clarendon press oxford 
fedorov muller 

moving local regression weight function 
nonparametric statistics 
franke nielson 

smooth interpolation large sets scattered data 
international journal numerical methods engineering 
friedman 

variable span smoother 
technical report lcs stanford university statistics department stanford ca 
friedman 

flexible metric nearest neighbor classification 
stanford edu reports friedman 
friedman bentley finkel 

algorithm finding best matches logarithmic expected time 
acm transactions mathematical software 
fritzke 

incremental learning local linear mappings 
proceedings international conference artificial neural networks icann pages paris france 
fukunaga 

statistical pattern recognition 
academic press new york ny second edition 
gasser muller 

kernel estimation regression functions 
gasser rosenblatt editors smoothing techniques curve estimation number lecture notes mathematics pages 
springer verlag heidelberg 
gasser muller 

estimating regression functions derivatives kernel method 
journal statistics 
gasser muller 

kernels nonparametric regression 
journal royal statistical society series 
ge 

noninvasive spectroscopy monitoring cell density process 
analytical chemistry 
goldberg pearlmutter 

neural network learn dynamics cmu direct drive arm ii 
technical report cmu cs carnegie mellon university pittsburgh pa connolly 

comparison neural network scattered data approximations inverse manipulator kinematics example 
neural computation 


image registration local approximation methods 
image vision computing 
grosse 

loess multivariate smoothing moving squares 
chui schumaker ward editors approximation theory vi pages 
academic press boston ma 
hammond 

nir analysis 
murray editors making light advances near infrared spectroscopy pages 
vch new york ny 
developed th international conference near infrared spectroscopy aberdeen scotland august 
hampel rousseeuw 

robust statistics approach influence functions 
john wiley new york ny 


applied nonparametric regression 
cambridge university press new york ny 
hastie loader 

local regression automatic kernel carpentry 
statistical science 
hastie tibshirani 

generalized additive regression 
chapman hall london 
hastie tibshirani 

discriminant adaptive nearest neighbor classification 
ftp stanford edu pub hastie dann ps higuchi kitano takahashi 

parallel associative processor knowledge processing 
aaai pages 
hillis 

connection machine 
mit press cambridge ma 
huang 

planning dynamic motions search tree 
ms thesis university toronto graduate department computer science 
www dgp utoronto ca people psh home html 
ijcai 
twelfth international joint conference artificial intelligence 
morgan kaufmann san mateo ca 
ijcai 
thirteenth international joint conference artificial intelligence 
morgan kaufmann san mateo ca 
meyer 

alfa automated load forecasting assistant 
proceedings ieee power engineering society summer meeting san francisco ca 
james 

classification algorithms 
john wiley sons new york ny 
jones davies park 

versions kernel type regression estimators 
journal american statistical association 


employing linear regression regression tree leaves 
neumann editor ecai th european conference artificial intelligence pages vienna austria 
john wiley sons 
katkovnik 

linear nonlinear methods nonparametric regression analysis 
soviet automatic control 


adaptive systems pattern recognition 
ieee transactions electronic computers ec 
kibler aha albert 

instance prediction real valued attributes 
computational intelligence 
kitano 

challenges massive parallelism 
ijcai pages 
kitano 

comprehensive practical model memory machine translation 
ijcai pages 
kitano higuchi 

high performance memory translation massively parallel associative memory processor 
aaai pages 
kitano higuchi 

massively parallel memory parsing 
ijcai pages 
kitano moldovan cha 

high performance natural language processing semantic network array processor 
ijcai pages 


new nonparametric estimation method local nonlinear 
interface 
lancaster 

moving weighted squares methods 
editor polynomial spline approximation pages 
reidel publishing boston ma 
lancaster 

surfaces generated moving squares methods 
mathematics computation 
lancaster 

curve surface fitting 
academic press new york ny 
lawrence tsoi black 

function approximation neural networks local methods bias variance smoothness 
australian conference neural networks canberra australia canberra australia 
available www neci nj nec com homepages lawrence www elec uq edu au lawrence 
lebaron 

forecast improvements volatility index 
unpublished 
lebaron 

nonlinear forecasts stock index 
casdagli eubank pages 
proceedings workshop nonlinear modeling forecasting september santa fe new mexico 
brent 

automatic contouring 
th australian computer conference pages 


optimization non parametric regression 
proceedings computational statistics pages prague 
physica verlag wien 


estimation non param par polynomial mobile 
revue de statistique appliqu ee 


smooth estimators distribution density functions 
computational statistics data analysis 
li 

consistency cross validated nearest neighbor estimates nonparametric regression 
annals statistics 
loader 

computing nonparametric function estimates 
technical report bell laboratories statistics department murray hill nj 
available anonymous ftp netlib att com netlib att stat doc ps 
whittle 

technique automatic contouring field survey data 
australian computer journal 
lowe 

similarity metric learning variable kernel classifier 
neural computation 
maron moore 

racing algorithm model selection memory learners 
artificial intelligence review 
press 
marron 

automatic smoothing parameter selection survey 
empirical economics 
mccallum 

instance utile distinctions reinforcement learning hidden state 
prieditis russell pages 
pollard smith 

computer programs automatic contouring 
technical report kansas geological survey computer contributions university kansas lawrence ka 


drawing contours arbitrary data points 
computer journal 
medin shoben 

context structure conceptual combination 
cognitive psychology 
wallace 

nonparametric estimation dynamic hedonic price models construction residential housing price indices 
american real estate urban economics association journal 
rose 

nonlinear nonparametric nonessential exchange rate estimation 
american economic review may 
miller 

subset selection regression 
chapman hall london 
miller kraft 

application general learning algorithm control robotic manipulators 
international journal robotics research 
mohri tanaka 

optimal weighting criterion case indexing numeric symbolic attributes 
aha editor aaai workshop program case reasoning working notes pages 
aaai press seattle wa 
moore 

acquisition dynamic control knowledge robotic manipulator 
seventh international machine learning workshop 
morgan kaufmann san mateo ca 
moore 

efficient memory learning robot control 
phd 
thesis technical report computer laboratory university cambridge 
moore hill johnson 

empirical investigation brute force choose features smoothers function approximators 
hanson judd petsche editors computational learning theory natural learning systems volume 
mit press cambridge ma 
moore schneider 

memory stochastic optimization 
appear proceedings nips available technical report cmu ri tr ftp ftp cs cmu edu afs cs cmu edu project reinforcement papers ps 


user guide 
technical report anl argonne national laboratory argonne illinois 
muller 

weighted local regression kernel methods nonparametric curve fitting 
journal american statistical association 
muller 

comment hastie loader 
statistical science 
murphy selkow 

efficiency trees finding nearest neighbors discrete space 
information processing letters 
myers 

classical modern regression applications 
pws kent boston ma 
nadaraya 

estimating regression 
theory probability applications 


locally weighted regression diffuse near infrared spectroscopy 
applied spectroscopy 
kowalski 

locally weighted regression scatter correction near infrared reflectance data 
analytical chemistry 
nguyen lee 

compaq providing consumer power artificial intelligence 
proceedings fifth annual conference innovative applications artificial intelligence pages washington dc 
aaai press 
nosofsky clark shin 

rules exemplars categorization identification recognition 
journal experimental psychology learning memory cognition 
omohundro 

efficient algorithms neural network behaviour 
journal complex systems 
omohundro 

efficient function constraint classification learning 
lippmann moody touretzky editors advances neural information processing systems 
morgan kaufmann 
palmer 

automatic mapping 
th australian computer conference pages 
boyd 

automatic contouring irregularly spaced data 
geophysics 
peng 

efficient memory dynamic programming 
prieditis russell pages 
press teukolsky vetterling flannery 

numerical recipes cambridge university press new york ny 
prieditis russell editors 
twelfth international conference machine learning 
morgan kaufmann san mateo ca 
kasif salzberg aha 

better understanding memorybased reasoning systems 
cohen hirsh editors eleventh international conference machine learning pages 
morgan kaufmann san mateo ca 
updated submitted artificial intelligence journal technical note kasif salzberg waltz aha 

framework memory reasoning 


efficient cross validation algorithm window width selection nonparametric kernel regression 
communications statistics simulation computation 
paliwal 

generalized optimization tree fast nearest neighbour search 
international conference acoustics speech signal processing 
raz 

selecting smoothing parameter estimation smoothly changing evoked potential signals 
biometrics 


multivariate interpolation large sets scattered data 
acm transactions mathematical software 
ruppert wand 

multivariate locally weighted squares regression 
annals statistics 
ruprecht muller 

image warping scattered data interpolation methods 
technical report universitat dortmund fachbereich informatik dortmund germany 
available anonymous ftp ftp ls informatik uni dortmund de pub reports ls rr ps ruprecht muller 

free form deformation scattered data interpolation methods 
farin hagen editors geometric modelling computing suppl 
pages 
springer verlag 
available anonymous ftp ftp ls informatik uni dortmund de pub reports iif rr ps ruprecht muller 

deformed cross dissolves image interpolation scientific visualization 
journal visualization computer animation 
available anonymous ftp ftp ls informatik uni dortmund de pub reports ls rr ps ruprecht muller 

framework generalized scattered data interpolation 
technical report universitat dortmund fachbereich informatik dortmund germany 
available anonymous ftp ftp ls informatik uni dortmund de pub reports ls rr ps ruprecht nagel muller 

spatial free form deformation scattered data interpolation methods 
technical report fachbereich informatik der universitat dortmund dortmund germany 
accepted publication computers graphics available anonymous ftp ftp ls informatik uni dortmund de pub reports ls rr ps rust 

distribution free methods approximating nonlinear marketing relationships 
journal marketing research xix 
sabin 

contouring review methods scattered data 
editor mathematical methods computer graphics design pages 
academic press new york ny 
saitta editor 
thirteenth international conference machine learning 
morgan kaufmann san mateo ca 
samet 

design analysis spatial data structures 
addison wesley reading ma 
schaal atkeson 

assessing quality learned local models 
cowan 
pages 
schaal atkeson 

isolation cooperation alternative view system experts 
nips proceedings press 
scott 

multivariate density estimation 
wiley new york ny 


linear regression analysis 
john wiley new york ny 
seifert engel gasser 

fast algorithms nonparametric curve estimation 
journal computational graphical statistics 
seifert gasser 

variance properties local polynomials 
www unizh ch manuscripts html 
shepard 

dimensional function irregularly spaced data 
rd acm national conference pages 


detecting changes time variance long term temperature record application robust locally weighted regression 
journal climate 
specht 

general regression neural network 
ieee transactions neural networks 
sproull 

refinements nearest neighbor searching trees 
algorithmica 
stanfill 

memory reasoning applied english pronunciation 
sixth national conference artificial intelligence pages 
stanfill waltz 

memory reasoning 
communications acm 


die 
kybernetik 


learning matrices applications 
ieee transactions electronic computers ec 
stone 

nearest neighbor estimators nonlinear regression function 
computer science statistics th annual symposium interface pages 
stone 

consistent nonparametric regression 
annals statistics 
stone 

optimal rates convergence nonparametric estimators 
annals statistics 
stone 

optimal global rates convergence nonparametric regression 
annals statistics 
sumita oi iida higuchi takahashi kitano 

example machine translation massively parallel processors 
ijcai pages 
tadepalli ok 

scaling average reward reinforcement learning approximating domain models value function 
saitta 
www cs orst edu research publications html 
maruyama nakamura abe maeda 

water demand forecasting memory learning 
water science technology 
taylor 

pattern recognition means automatic analogue apparatus 
proceedings institution electrical engineers 
taylor 

parallel analogue reading machine 
control 
thorpe 

localized versus distributed representations 
arbib editor handbook brain theory neural networks pages 
mit press cambridge ma 
thrun 

learning th thing easier learning 
advances neural information processing systems nips 
www cs cmu edu afs cs cmu edu web people thrun publications html 
thrun sullivan 

discovering structure multiple learning tasks tc algorithm 
saitta 
www cs cmu edu afs cs cmu edu web people thrun publications html 
tibshirani hastie 

local likelihood estimation 
journal american statistical association 
ting cameron jones 

exploring framework instance learning naive bayesian classifiers 
proceedings seventh australian joint conference artificial intelligence australia 
world scientific 
tou gonzalez 

pattern recognition principles 
addison wesley reading ma 


nonlinear prediction speech signals 
casdagli eubank pages 
proceedings workshop nonlinear modeling forecasting september santa fe new mexico 
tsybakov 

robust reconstruction functions local approximation method 
problems information transmission 
tukey 

exploratory data analysis 
addison wesley reading ma 
raz 

estimation trial trial variation evoked potential signals smoothing trials 
psychophysiology 
wand 

fast computation auxiliary quantities local polynomial regression 
wustl edu html 
van der groen van het 

locally linear nested network robot manipulation 
proceedings ieee international conference neural networks pages 
ftp ftp uva nl pub computer systems aut sys reports ps gz 
vapnik 

principles risk minimization learning theory 
moody hanson lippmann editors advances neural information processing systems pages 
morgan kaufman san mateo ca 
vapnik bottou 

local algorithms pattern recognition dependencies estimation 
neural computation 
walden prescott 

identification trends annual maximum sea levels robust locally weighted regression 
coastal shelf science 
walters 

contouring machine user guide 
american association petroleum bulletin 
waltz 

applications connection machine 
computer 
wand jones 

comparison smoothing parameterizations bivariate kernel density estimation 
journal american statistical association 
wand jones 

kernel smoothing 
chapman hall london 
wand 

gaussian kernels curve estimation window width selection 
canadian journal statistics 
wang kowalski 

new approach distance measurement locally weighted regression 
analytical chemistry 
watson 

smooth regression analysis 
indian journal statistics series 


applied linear regression 
john wiley sons 
wess althoff 

trees improve retrieval step case reasoning 
wess althoff richter editors topics casebased reasoning pages 
springer verlag new york ny 
proceedings european workshop ewcbr 
wettschereck 

study distance machine learning algorithms 
phd dissertation oregon state university department computer science 
johnson 

estimation missing values lead air quality data sets 
johnson editors quality assurance air pollution measurements 
air pollution control association pittsburgh pa tr transactions international specialty conference 
wolberg 

digital image warping 
ieee computer society press los alamitos ca 
kitano 

robustness memory reasoning implemented wafer scale integration 
ieice transactions information systems 


algorithmic logical models automatic synthesis robot action 
phd dissertation university ljubljana ljubljana slovenia yugoslavia 


new methods machine learning construction integrated associative memory knowledge bases 
editors proceedings th mediterranean electrotechnical conference volume ii pages ljubljana slovenia yugoslavia 
ieee catalog number ch 


geometric learning nonlinear modeling control forecasting 
proceedings ieee international symposium intelligent control pages glasgow scotland 
ieee catalog number ch 


comparing predictions neural networks memory learning 
proceedings icann international conference artificial neural networks pages paris france 
