predictive prefetching improve world wide web latency venkata padmanabhan university california berkeley cs berkeley edu jeffrey mogul digital equipment western research laboratory mogul pa dec com long term success world wide web depends fast response time 
people web access information remote sites wait long results 
latency retrieving web document depends factors network bandwidth propagation time speed server client computers 
proposals reducing latency difficult push point insignificant 
motivates investigate scheme reducing latency perceived users predicting prefetching files requested soon user browsing currently displayed page 
scheme server gets see requests clients predictions individual clients initiate prefetching 
evaluate scheme trace driven simulations prefetching high bandwidth low bandwidth links 
results indicate prefetching quite beneficial cases resulting significant reduction average access time cost increase network traffic similar fraction 
expect prefetching particularly profitable non shared dialup links high bandwidth high latency satellite links 
people world wide web www gives quick easy access tremendous variety information remote locations 
users wait results tend avoid complain web pages take long time retrieve 
users care web latency 
perceived latency comes sources 
web servers take long time process request especially overloaded slow disks 
web clients add delay quickly parse retrieved data display user 
retrieval time web documents depends network latency 
web useful precisely provides remote access transmission data distance takes time 
delay depends bandwidth retrieve mb file mbps link seconds 
network latency comes propagation delay 
delays client server slowness transmission time principle reduced buying faster computers higher bandwidth links 
components propagation delay basically determined physical distance traversed reduced point 
hypertext transport protocol version currently web simple far optimal far latency concerned 
researchers analyzed inefficiencies network proposed modifications reduce retrieval latency significantly 
difficult push retrieval latency point insignificant 
motivates investigation ways hiding retrieval latency user reducing 
describe scheme clients collaboration servers prefetch web pages user access soon viewing currently displayed page 
user request prefetched pages local site cache 
retrieval latency called retrieval time masked user cases yielding lower access time 
maintain distinction retrieval latency time access time rest 
distributed prefetching scheme distinct roles clients servers 
servers get see accesses clients predictions files accessed near 
clients initiate prefetching advice servers 
clearly effectiveness prefetching critically depends predictions 
prediction algorithm patterned proposed griffioen appleton context file systems noteworthy differences 
results trace driven simulations indicate prefetching helps significantly decrease average access time cost increase network traffic 
latency retrieving web data involves relatively large component independent amount data transferred 
includes network round trip times overheads hosts 
situations effective prefetching reduce latency simply increase available bandwidth 
rest organized follows 
section briefly discuss basics needed understand rest 
section briefly describe modifications proposed 
section scheme predictive prefetching 
methodology simulation experiments described section results 
section discuss issues pertaining prefetching 
section 
protocol elements protocol layered reliable bidirectional byte stream normally tcp 
interaction consists request sent client server followed response sent back server client 
requests responses expressed simple ascii format 
existing implementations conform original version protocol 
version presently draft form 
request includes elements method get put post uniform resource locator url set hypertext request headers clients specifies things kinds documents willing accept authentication information optional data field certain methods put 
server parses request takes action specified method 
sends response client including status code indicate request succeeded set object headers meta information object returned server optionally including response data field containing file requested output generated server side script 
limitations look way interaction clients servers appears network particular emphasis affects latency 
mainly look servers clients today 
syn syn ack dat ack dat fin ack fin ack syn syn ack dat dat client server server reads disk server reads disk rtt rtt rtt rtt rtt client opens tcp connection client sends request html client parses html client opens tcp connection client sends request image image begins arrive ack shows packet exchanges client server 
time runs page 
dat ack denote data packets respectively data packets carry 
syn fin denote packets tcp signal start respectively connection 
left client timeline horizontal dotted lines show mandatory round trip times rtts network imposed combination tcp protocols 
depicts packet exchange client server typical interaction retrieval html document uncached inline image 
note obvious inefficiencies protocol 
transfer html image file involves setting tearing new tcp connection 
second request response protocol client server operates go manner new request sent reply previous arrived 
result considerable delays 
persistent connection briefly discuss persistent connection proposed padmanabhan mogul term 
uses single long lived tcp connection multiple transactions 
connection stays open inline images single document multiple html retrievals 
helps solve problem mentioned 
protocol defines persistent connection mechanism solve problem 
avoid second problem proposes new methods primitives allow pipelining requests responses client server 
request fetch specified html file inline images reside server 
request fetch files specified list client passes server 
possible simulate asynchronous series pipelined gets 
modifications result considerably reduced retrieval latency cases half original latency 
predictive prefetching clear section retrieval typical web page involves network round trips 
reduces cost considerably reports image rich web pages suffer multi second retrieval latencies 
light decided investigate techniques reduce retrieval time improve response time perceived user 
users usually browse web hyperlinks web page 
hyperlinks page refer pages stored server 
typically pause page loaded user reads displayed material 
time client prefetch files accessed soon avoiding retrieval latency files requested 
retrieval latency reduced just overlapped time user spends reading decreasing access time 
proposal server computes likelihood particular web page accessed conveys information client 
client program decides prefetch page 
partitioning server client natural 
server opportunity observe pattern accesses clients information intelligent predictions 
hand client best position decide prefetch files cached cost terms cpu time memory network bandwidth needed prefetch data 
aside note server prefetch files disk memory independent client requests 
believe benefit limited dominance network latency disk latency especially wide area context 
study investigated prefetching server clients network 
architecture system prefetching describe architecture system prefetching depicted 
server side types user level processes 
set daemon processes httpd support persistent connections features described 
httpd process gets spawned service requests client 
persistent connections supported process client client request 
process prediction daemon prefetching related predictions 
server new client request client 
furthermore communicates httpd directly clients 
design ncsa server invokes processes threads service client requests 
receiving request client httpd passes identity client names files requested 
concerned file accesses looks client requests get method variants 
internet daemon daemon daemon engine client server prediction prefetch engine browser depicts architecture system prefetching 
server side set httpd processes active client communicate prediction engine predictions 
client side prefetch engine initiates prefetching advice server factors 
bidirectional arrows denote local communication entities server client 
uses prediction algorithm described section determine files candidates prefetching likelihood accessed soon conveys information client 
information piggy backed reply sent httpd client special field 
client side consists browser mosaic prefetch engine 
prefetch engine uses prediction information sent server reply decide prefetch files 
decision variety factors contents local cache contain file current system load browser current mode operation image loading turned 
prefetch engine decided prefetch file sends request server 
request indicates prefetching data fetching data user explicitly requested 
information server variety ways 
decide prefetching related computation request prefetch request 
multiple requests scheduled way request assigned lower priority explicit fetch requests 
prediction algorithm prediction algorithm described griffioen appleton 
noteworthy differences 
scheme designed operating system prefetch files disk file system cache model distributed user level processes server client hosts managing prefetching network client cache 
scheme require kernel modifications 
second scheme described try maintain distinction accesses different processes clients context file system 
independent accesses different processes occur close time incorrectly considered related 
explain popular html home html image gif image gif depicts small portion hypothetical dependency graph 
past observations home html accessed chance image gif accessed soon chance image gif accessed soon 
furthermore image gif accessed chance image gif follow soon 
scheme avoids problem false correlations 
prediction algorithm constructs dependency graph depicts pattern accesses different files stored server 
graph node file accessed 
arc node point time accessed accesses lookahead window size 
weight arc ratio number accesses window number accesses 
weight probability requested immediately weights arcs emanating particular node need add 
depicts portion hypothetical dependency graph 
dependency graph dynamically updated server receives new requests 
done prediction daemon receives information requests httpd process running server machine 
maintains ring buffer size equal window size client currently connected server assuming persistent connections 
receives new request httpd processes inserts id file accessed corresponding ring buffer 
entries ring buffer considered related corresponding arcs dependency graph updated 
logically separates accesses different clients avoids problem false correlations 
cases clients located proxy cache able distinguish accesses different clients 
way getting problem mechanisms proposed pass session state identification clients servers proxy 
bases predictions dependency graph 
accessed sense prefetch arc large weight implies chance accessed soon 
general declare candidate prefetching arc weight higher prefetch threshold possible set threshold differently client vary dynamically 
issues implemented prediction daemon necessary changes httpd communicate information accesses unix pipe 
case request modified httpd conveys fact aware files corresponding sent client need considered candidates prefetched time 
implemented client server communications interface client side support prefetching 
issue lookahead window managed multiple accesses file window 
example consider window size sequence accesses abb delta delta delta ac delta delta delta ad delta delta delta abb delta delta delta denotes gaps larger window size 
counted multiple occurrences window weight arc 
reflect dependency accesses correctly fact follow window time 
caching clients eliminate multiple accesses happen instance data pointed url case updated frequently 
ignored multiple accesses file window computing weights arcs 
dependencies accesses different files may vary time 
instance certain pages web site popular days sense prefetch client accesses home page site 
popularity pages prefetching beneficial 
weights arcs dependency graph adjusted accordingly form aging 
furthermore nodes dependency graph files accessed long time pruned limit size graph 
implementation ignored issues 
note items inherently non result filling form 
items intermediate example live camera shot worth prefetching second actual minutes early 
ideally prediction algorithm prefetching scheme take account 
experimental methodology evaluate usefulness prefetching scheme simulations 
access logs digital equipment main web server www digital com drive simulations 
regular httpd server ncsa accesses 
run simulator uses access log entries prime dependency graph simulating prefetching 
uses entries simulate working real system prefetching predictions updates dependency graph 
simulates mb lru cache client 
simulations clients prefetch files server advises file client cache 
parameters varied experiments 
prefetch threshold varied steps 
value corresponds prefetching weight arc dependency graph exceed 
lookahead window size takes values 
window size corresponds minimal step lookahead 

maximum number urls advise client prefetch time standing amount prediction information 
assigned integer values set infinity corresponds limit 
performance metrics computed simulation run 
average access time file computed assuming zero retrieval time hit client cache retrieval time models described sections 
fractional increase network traffic computed ratio increase total number data bytes transferred server client prefetching total prefetching 
network model need way estimating time retrieving files network order evaluate benefits prefetching 
purpose construct simple model network 
data pipe client server modeled linear regression 
retrieving file size bytes assumed incur startup cost byte cost yielding total time startup cost includes round trip times setting new connection time sending request byte cost reflects share network bandwidth available communication client server 
set data points parameters linear regression computed xy gamma gamma gammab simplicity network model clients 
order obtain data points constructing model instrumented web browser record retrieval time files 
ran browser host connected ethernet segment uc berkeley random retrievals various sizes digital equipment main web server palo alto california far berkeley 
shows data points line corresponding linear regression model parameters seconds theta gamma seconds byte equivalent bandwidth kbps 
network model attempt model progress transport tcp connections detail slow start congestion control 
see simple model fits data quite 
extrapolate model case client host connected kbps modem line ethernet 
larger latency modem link increased transmission time requests link estimate startup cost seconds 
assuming kbps modem line bottleneck link byte cost theta gamma seconds byte 
retrieval model file retrievals demand fetches response explicit user requests prefetches share bandwidth client server data pipe 
demand fetches priority prefetches going prefetches suspended new user request issued resumed fetches linear model time fetch seconds file size bytes scatter plot time taken client uc berkeley fetch files web pages inline images different sizes digital web server line corresponding linear regression model 
completed 
practice client separate tcp connections demand fetches prefetches 
required client throttle prefetch connection shrinking tcp receiver window 
algorithms employed tcp allow server send min congestion window receiver window bytes data connection fully throttled 
consider different models file retrieval data pipe client server 
overlap model assumes file retrievals happen sequentially prefetches suspended middle resumed 
second overlap model allows client issue new retrieval requests earlier ones completed 
fixed startup latency file retrieval largely arises due network round trip delays overlapped going transfers 
models effect multiple parallel connections netscape navigator pipelined requests described 
simplicity ignore interaction data transfers different clients 
introduce inaccuracies believe plausible assumption reason 
measurements network connectivity uc berkeley digital done presence competing traffic clients general internet traffic 
consequently model developed reflects share total network bandwidth available data pipe uc berkeley digital 
fair distribution network resources reasonable assume client server data path guaranteed share network bandwidth 
results section discuss results obtained simulation experiments 
discussion focuses results obtained network model connectivity uc berkeley digital 
results prefetching slower modem speed link 
mentioned upper bound number urls server advise client prefetch set 
discussion justify choice 
shows variation average file access time prefetch threshold lookahead window size 
shown overlap overlap network models 
increasing average access time seconds prefetch threshold overlap overlap overlap overlap overlap overlap average file access time versus prefetch threshold upper set curves overlap model described section lower set overlap model 
set lookahead window size varied 
fractional increase network traffic prefetch threshold fractional increase network traffic versus prefetch threshold different values lookahead window size curves shown overlap model overlap model essentially 
prefetch threshold results aggressive prefetching consequently larger average file access time 
access time maximum prefetch threshold larger resulting prefetching 
increasing lookahead window size decreases average access time 
larger window better able capture dependencies accesses different files including accessed consecutively 
benefit reduced access time due prefetching comes cost increase amount data transferred server clients quantify terms fractional increase network traffic 
shown increase prefetch threshold decreases quantity increase lookahead window size increases 
assuming overlap network model overlap results increase estimated file access times 
relative improvement access times quite similar models 
furthermore amount network traffic affected choice model versus 
remainder section focus overlap model closer reality overlap model 
clear balance needs struck improved access time increase traffic 
inverse relationship quantities clear 
note general larger lookahead window size results smaller access time increase traffic 
instance window size results better performance values shown 
performance improvement derived increasing window size limited setting experiments 
discussed section retrieving file web server involves significant startup cost largely independent network bandwidth 
just increasing bandwidth reduce access time point 
illustrates graphically uc berkeley digital network model 
horizontal dotted lines show simulated mean access times non prefetching systems available bandwidths simulating prefetching system 
solid curve average access time seconds fractional increase traffic average file access time versus fractional increase network traffic 
curves shown lookahead window sizes 
corresponds prefetching lookahead window size 
clear prefetching result lower access times compared just increasing available bandwidth 
instance shows prefetching reduce average access time seconds increase network traffic 
contrast doubling bandwidth reduces access time second absence prefetching 
investigate benefit prefetching bandwidth low consider case kbps modem link bottleneck path client server 
analogue case 
bandwidth low contribution data transmission time total average access time seconds fractional increase traffic change average access time seconds fractional increase traffic change average file access time prefetching uc berkeley digital network model kbps modem link 
solid curve corresponds case prefetching lookahead window size 
horizontal dotted lines correspond non prefetching systems available bandwidths simulating prefetching system 
file retrieval time significant 
explains increase bandwidth reduces average file access time drastically 
prefetching quite beneficial prefetching threshold set point requires increase network traffic resulting access time lower average access time seconds prefetch threshold infinity average file access time versus prefetch threshold different values fractional increase network traffic prefetch threshold infinity fractional increase network traffic versus prefetch threshold different values non prefetching system obtain increase available bandwidth 
discussion far set parameter determines amount prefetching related information server piggyback replies clients constant value 
provide justification choice 
figures respectively show average file access time increase network traffic various values ideal case set infinity implying limit amount prediction information server convey clients 
figures see set traffic access time curves close equal infinity especially prefetch threshold larger 
indicates setting relatively small value consequently having server send small amount prefetching information clients sufficient performance 
access time seconds file size bytes prefetching scatter plot access time versus file size prefetching 
access time seconds file size bytes prefetching scatter plot access time versus file size prefetch threshold set lookahead window size set 
consider effect prefetching variability file access times 
user perspective desirable reduce variability decreasing average access time 
figures show scatter plots file access time versus file size prefetching prefetching respectively 
plots show distinctive line corresponding linear model estimating file retrieval times 
absence prefetching data points lie line due queuing delays 
points zero access times corresponding cache hits 
prefetching done numerous points lie line corresponding linear model prefetching masks part retrieval time 
prefetching system yields points access time zero 
trends evident distribution access times shown table 
quantify variability file access times terms standard deviation errors 
collection data points simple linear regression parameters constructed described section 
sum squared errors sse defined gamma gamma xy 
standard deviation errors computed sse gamma measure deviation datapoints line corresponding linear regression 
table shows regression parameters standard deviation errors corresponding figures 
distribution access times linear regression parameters zero small large sec sec byte sec prefetching theta gamma prefetching theta gamma table columns left show distribution file access times terms categories zero cache hit non zero smaller implied linear network model larger implied linear model 
columns right show parameters linear regression fixed cost byte cost standard deviation errors 
note linear regression computed purpose quantifying variation files access times 
model access times 
standard deviation errors prefetching seconds slightly higher prefetching seconds 
prefetching reduce average file access time significantly increasing variability 
discussion results indicate predictive prefetching web data lead significant reduction perceived latency cost increase network traffic 
discuss issues related prefetching 
explained section prefetching related predictions servers observe pattern accesses clients 
purpose server needs maintain dependency graph reflects patterns 
client access server consults data structure predictions 
necessary minimize additional load imposed server construction dependency graph scheduled peak hours late night 
reasonable expect client access patterns remain stable duration day believe scheduling adversely impact effectiveness prefetching 
clients access web proxy caches prefetching happen ways web servers proxy cache proxy cache clients 
case proxy cache prefetching related predictions conveys clients 
advantage proxy caches relative web servers observe client access patterns servers 
consider situations presence proxy cache advantageous point view prefetching 
case client connected directly proxy non shared link modem isdn line 
situation optimal idle time link filled prefetch traffic 
mechanism needed rapidly throttle prefetch traffic needed avoid affecting flow traffic link 
second case client receives data high bandwidth high latency link satellite downlink bandwidth mbps latency hundreds milliseconds 
reverse connection may slow telephone line 
scenario availability spare bandwidth downlink large startup latency fetching data demand prefetching attractive proposition 
placing proxy cache near satellite ground station throughputs close downlink bandwidth achieved cache client increasing load part network 
prefetching scheme world wide web aimed reducing latency perceived users 
scheme servers tell clients files requested user clients decide prefetch files local considerations contents local cache 
simulation results show substantial reduction latency perceived client quantified terms average time access file achieved cost similar increase network traffic 
retrieval time file includes substantial startup latency prefetching effective reducing access time just increasing bandwidth 
conclude prefetching worthwhile especially increasing bandwidth demands significantly degrade service users increase cost service 
scenarios prefetching especially useful involve clients connected proxy cache non shared modem isdn line high bandwidth high latency satellite downlink 
support prefetching protocol enhanced allow servers piggyback prefetching hints replies clients 
help scheduling server prefetches distinguished demand fetches instance give lower priority 
done facilities digital western research lab network systems lab cambridge research lab 
thomas kroeger carlos provided useful comments draft 
berners lee fielding frystyk 
hypertext transfer protocol rfc may 
fielding gettys mogul frystyk berners lee 
hypertext transfer protocol internet draft draft ietf spec txt ietf june 
working draft 
james griffioen randy appleton 
reducing file system latency predictive approach proceedings summer usenix technical conference cambridge ma june 
raj jain 
art computer systems performance analysis john wiley sons 
david kristol 
proposed state info mechanism internet draft draft kristol txt ietf september 
working draft 
jeffrey mogul 
case persistent connection proceedings acm sigcomm conference boston ma august 
netscape communications corporations www netscape com 
venkata padmanabhan jeffrey mogul 
improving latency proceedings second international world wide web conference chicago il pages october 
updated version appeared computer networks isdn systems nos december pp 

venkata padmanabhan 
improving world wide web latency technical report ucb csd computer science division university california berkeley ca may postel 
transmission control protocol rfc network information center sri international september 
simon 
analysis performance problems url sunsite unc edu release prob html july 
