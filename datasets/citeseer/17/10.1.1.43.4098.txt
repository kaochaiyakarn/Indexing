learning probabilistic models decision theoretic navigation mobile robots daniel cs cmu edu nourbakhsh cs cmu edu robotics institute carnegie mellon university forbes ave pittsburgh pa usa decision theoretic reasoning planning algorithms increasingly mobile robot navigation due significant uncertainty accompanying robots perception action 
algorithms require detailed probabilistic models environment robot desirable automate process compiling models means autonomous learning algorithms 
compares experimentally learning methods combination heuristic decision theoretic planning algorithms purpose learning probabilistic model environment mobile robot model navigation 
learning methods novel presents approach probabilistic model learning merging states clustering trajectories observation action pairs 
strengths weaknesses combination learning planning method explored sample environment mobile robot navigation 

mobile robots operating office environments decisions significant uncertainty observations effect actions 
furthermore set locations environments indistinguishable introduces perceptual aliasing hidden state 
decision theoretic planning boutilier preferred approach robot control situations implemented research prototypes mobile robots nourbakhsh koenig simmons 
decision theoretic planner usually provided stochastic model environment reflects uncertainty robot sensing actions 
hand crafting model typically time consuming error prone require blueprints cad models buildings robot operate considerable human effort 
desirable develop methods learning stochastic models observation action traces obtained autonomous exploration environment part robot 
reports experimental results simulated exploration control office mobile robot uses learning methods acquire stochastic models environment operates applies planners learned models order navigate robot home position 
learning methods novel explores state merging approach model learning clustering percept action trajectories contrast traditional methods nonlinear maximization observation likelihood 
principal goal reported experiments determine experimentally combination learning planning methods works best mobile robot navigation office spaces explore limitations approaches 
section provides overview decision theoretic methods representing domains reasoning planning means partially observable markov decision processes pomdps 
section presents algorithms learning pomdps data 
section describes experimental environment experimental results reported section 
section discusses results directions 

decision theoretic planning decision theoretic planning extension classical ai planning paradigm handle problems effect actions uncertain observations noisy incomplete 
frameworks markov decision processes mdps partially observable mdps pomdps 
representation reasoning pomdps pomdp described tuple set states initial probability distribution states set actions transition function maps theta discrete probability distributions states observable observations set perceived 
function maps theta discrete probability distributions pomdps observations depend state 
function describes immediate reward received pomdp states structure transition emission reward functions control objectives agent controlling pomdp 
scenario interested mobile robot navigation state location world observation discrete percept produced perceptual apparatus mobile robot 
objective robot reach particular goal state stay reward state example reward states zero 
agent maximizes reward receives effectively achieving goal 
pomdp controlled executing available actions time step 
result pomdp transfers new unobservable state emits new observation perceived agent controlling pomdp 
agent infer state pomdp basis sequence observations knowledge transition emission probabilities pomdp 
agent reason uncertainty completely sure exact state pomdp maintain belief state bel represented probability distribution states cassandra boutilier 
planning pomdps controlling pomdp amounts choosing correct actions maximize expected cumulative reward received agent course operation 
course infinite duration sum infinite way avoid apply exponential discounting factor fl reward 
goal controller maximize quantity fl reward received th step operation delta denotes expectation 
algorithms exist finding optimal control policy pomdps kaelbling 
problem proven pspace hard considering approximate solutions planning mdp approaches cassandra nourbakhsh koenig simmons 
planning heuristic strategy proposed nourbakhsh performs full belief updating belief state transition emission probabilities simplifying assumptions choosing action 
assumes state complete certainty ignoring possibility states 
constructs deterministic fsa transition emission probabilities original pomdp uses general search algorithm iterative deepening find path fsa 
furthermore planner produces list percepts ought seen plan executed fsa true representation world 
cases percepts seen agent differ expected ones indication plan longer valid agent lost 
case planner called find new plan latest estimate state 
set heuristic strategies solves underlying mdp solution conjunction estimated belief state cassandra koenig simmons 
solution underlying mdp optimal policy maps state action maximizes expected cumulative reward 
order find action auxiliary function computed meaning expected cumulative reward action performed state optimal policy followed 
function known state optimal action state highest value argmax 
function means learning iterative monte carlo reinforcement learning method sutton barto gamma fl max estimate function iteration learning rate coefficient decrease gradually time state sampled transition probabilities state action mdp solved values ways approximate optimal policy pomdp case cassandra koenig simmons 
method mls chooses optimal action state ignoring possibility robot states 
voting method chooses action highest probability mass belief vector 
qmdp method similar voting method adding state beliefs sum winning action added sums actions weighted proportionally actual values 

learning pomdps learning pomdp observations having available advance furthermore complicates problem determining optimal policies 
fundamental questions arise correspondence world model states related problem goal determination 
general agent know states existed original process generated data decision final number states model 
difficulty determining goal criteria planning learned pomdp 
planning mdp strategies need goal state terminate iterative deepening search planning construct proper reward function mdp algorithms 
pomdp learned clear states correspond true goal states real world general won oneto correspondence learned true states 
circumstance changes significantly goal criterion planning 
possibility transfer goal state domain perceptual domain trying reach goal location agent tries observe percept corresponded location training sensory motor trace 
planning states fsa idealized representation learned pomdp labeled percept observed state 
solution exists mdp planners 
assigning reward goal state zero states reward state equal probability goal percept observed state 
policy maximizes reward effect maximize probability goal percept seen system goal location time goal percept seen policy maximize probability reaching goal state 
learning pomdps performed means general algorithms learning probabilistic networks data proposed russell 
pomdp represented chain time slices state node observation node time slices transition emission matrices 
objective learning find values entries transition emission conditional probability tables cpt pomdp maximize loglikelihood ln djw training data set generated pomdp parameters case data set consists assignments observable nodes learning pomdps iterative adjustments probabilities traditional approach learning pomdps fix structure pomdp initialize probability matrices structure iteratively adjust maximize likelihood training data generated model 
algorithms considered experiments steepest gradient ascent algorithm due russell 
baum welch learning rule expectationmaximization em algorithm 
gradient ascent likelihood russell 
proposed particularly simple learning rule performs gradient ascent space cpt entries deltaw ijk jw ijk jw ij ik jd ijk ijk entry cpt state node designates probability node th state ij parents th configuration russell 
summations index runs data cases baum welch chrisman koenig simmons adapted baum welch algorithm learning hidden markov models hmms problem improving entries cpts pomdps data 
learning rule ijk fl ijk estimate transition probability ik ju ij node th state ij parents th configuration iteration ik ij jd fl ij jd learning pomdps state merging baum welch steepest gradient ascent learn hmms pomdps general converge true models generated training data 
algorithms iterative start random assignment parameters iteratively update order maximize likelihood data 
fail converge transition matrices close deterministic 
possible reason probabilities transition matrices random initial values highly learning algorithms overcome local maxima likelihood drive parameters boundaries parameter space correspond close deterministic transition functions 
furthermore failing converge functions negative impact heuristic methods planning simplify pomdp fsa perform best comparison optimal pomdp planning algorithms loss precision due simplification minimal 
attempt overcome shortcomings baum welch steepest gradient ascent novel algorithms learning pomdps explored state merging 
best model merging completely different method learning hmms suggested stolcke omohundro originally purposes speech recognition stolcke omohundro 
algorithm builds initial model state observation action pair 
call states initial model time states distinguish true states pomdp generated observation data 
model fits training data perfectly highest possible log likelihood generalize novel cases practice overfits data 
objective algorithm find came true pomdp state reduce initial overfit model correct 
done consecutive merging pairs correspond hidden state underlying pomdp generated data 
criterion merging resulting decrease data likelihood 
step algorithm candidate mergers computed decreases likelihood accepted search continues greedy fashion likelihood model respect data unacceptably low predetermined number states reached 
algorithm adapted purpose learning pomdps chrisman adapted baum welch purpose 
experimental comparisons baum welch synthetic worlds reported nourbakhsh demonstrate cases best model merging algorithm outperforms baum welch significantly 
computational complexity number observations far slowest algorithms considered experiments 
state merging trajectory clustering major shortcoming algorithm state merging proceeds greedily suboptimal mergers pairs time states 
better approach rank possible mergers carry promising ones 
ultimate objective state merging group time states data points groups time states single group correspond state true pomdp generated data 
observation suggests idea clustering time states form similarity 
simplest approach merge time states emitted symbol bound fail perceptual aliasing 
better measure similarity length matching action observation sequences prior time states 
exactly similarity measure mccallum instancebased learning algorithm mccallum 
intuition measure trajectories leading time state represent embedding space true hidden state space close points embedding space matching trajectories correspond close hidden states 
common approach system identification exploits fact immediate observation reliable indication true state system sequence trajectory observations usually disambiguates state 
furthermore mccallum algorithm constrained design matching trajectories leading current time state novel algorithm available data cur rent moment match sequences leading current time state 
analogous measure length matching sequence time states 
lengths sequences leading current time state added 
denote henceforth length matching trajectories prior states length states introduce variable value directly emitted symbols states match don 
experiments sums possible combinations measures algorithm executes steps 
compute similarities possible pairs time states place similarity matrix 

perform clustering time states similarity matrix 
time states cluster assumed correspond true state pomdp 

label time states number respective cluster previous step 
point pomdp fully observable mdp 
states possibly mislabeled 

compute transition emission matrices straightforward 
noted popular clustering algorithms means applied second step algorithm require averaging data points case underlying metric space addition multiplication defined 
clustering algorithms similarity matrix 
algorithm widely pattern recognition finding minimum spanning trees mst graph adjacency structure defined similarity matrix duda hart 
mst edges corresponding similar pairings severed 
results cliques define clusters time states correspond state true pomdp generated observations 
step algorithm state merging trajectory clustering assign consecutive numbers remaining cliques label hidden states observation sequence respective clique numbers estimate transition emission probabilities pomdp fully observable 
approach give better results considers possible mergers carrying 
furthermore method guaranteed recover completely fully observable pomdp necessarily true sga em 
sga em get stuck local maximum log likelihood choose bad merger due limited number mergers considers step 
expected outperform em worlds mildly unobservable environments moderate perceptual aliasing 
just algorithm quite expensive computationally implemented directly 
finding similarity matrix sequence actions observations worst case running time matches considered length matching subsequences backwards forwards long elements 
practice matches terminate time steps 
complexity finding mst similarity matrix elements prim algorithm log kruskal algorithm employed combination fast sorting routine 
clique labeling stage takes computations order matrix multiplications determine adjacency members clique 

experimental environment experiments reported commercial simulator nomad robots available nomadic technologies mountain view california 
nomad robot equipped infrared sensors give proximity readings range gamma inches 
readings relatively low noise high repeatability limited range robot experiences significant perceptual aliasing 
experimental world shown fig illustrates problem 
size open space white surrounded obstacles black inches robot starts exploration coordinates inches origin coordinate system lower left corner open space 
actions allowed move forward inches turn left degrees turn right degrees 
robot complete move inches collision wall backs original position 
robot locations orientations plus small ran dom drift supplied simulator result inches difference locations course steps 
robot different states time perceive dimensional vector infrared readings 
fact world discrete states similar maze worlds commonly reinforcement learning research difference observations continuous discrete 
size state space comparable previous recovering worlds hidden state chrisman experimented space station docking problem states mccallum synthetic worlds states 
previous experiments discrete observations 
states world indistinguishable pairs states generate readings 
due limited range infrared sensors inches allow robot perceive distinguishing feature upper right corner area central location coordinates 
leftmost rightmost locations identifiable feature 
type aliasing typically arising office spaces robot long corridors 
total observation action pairs acquired exploration stage sequence robot starting leftmost location facing east 
dimensional vectors continuous observations quantized symbols kmeans clustering iterations typically necessary convergence 
observation vectors quantized observation action pairs labeled symbols emission tables learned pomdp models 
note symbols labeled states number discernible states total number due perceptual aliasing 
correct number states learning algorithms learn transition emission probabilities pomdp 
action tried average times state case state action pairs experienced 
result learning algorithms produce transition matrices strictly stochastic sum probabilities certain state particular action required value stochastic matrices 
planning algorithms deal cases straightforward manner planning considers actions planning process action available state iteration learning algorithm mdp planners assigns zero value state action pair 
reasonable practical approach expected learning planning algorithms better cases certain transitions simply training data 
goal robot reach home location facing east steering angle 
robot acquired model means learning methods described placed available starting locations controlled planning methods 
reached goal action steps discounted reward equal raised number steps took reach goal methodology cassandra 
conversely failed reach goal reward zero 
particular planner learning method rewards averaged twelve starting states states goal state actions necessary combination learning method planner guaranteed reward 
average cumulative discounted award random action selection computed comparison baseline combination learner planner claimed built useful pomdp model achieves significantly better cumulative discounted reward corresponding random choice actions 
mdp planners solved underlying mdp learned pomdp learning sweeping state turn sampling successive states transition probabilities mdp 
learning rate discounting factor fl total sweeps 

experimental results experimental results shown table learning algorithms modifications similarity measure clustering trajectories 
similarity measure sum components depending states matched emitted exactly symbol maximum length matching action observation pairs prior states maximum length matching action observation pairs states 
noted necessarily 
components similarity measure particular modification algorithm shown name modification table example means similarity measure entry table corresponds combination learning planning method form sigma average cumulative discounted reward achieved runs sample standard deviation reward statistic measuring number standard deviations average award achieved random number selection runs learning methods listed table 
tests computed table assumption gaussian distributions respective samples 
computed values regarded caution distribution rewards random choice actions roughly gaussian distribution rewards pair learner planner typically 
usual behavior robot series test runs go goal directly achieve maximum reward model acquired bang wall get reward 
due fact mdp policies plans fixed belief distribution converges fixed value robot held location pomdp policies practice fixed wrong robot escape location 
consequently rewards distributed extremes violates gaussian requirement tests 
cautionary note mind results table interpreted indicate combinations learning planning methods achieve performance significantly better random action selection sga ap bw ap mdp planners planners 
version employs sum matching components performs best confirms expectations 
versions backward matching distance direct match perform badly mentioned 
sake comparison line table lists performance planners fully observable model world represented pomdp states emits unique observation 
evident planners find optimal plan world suggests combination learning planning method achieves suboptimal results due acquiring wrong model part learner failure model correctly 
seen learning methods achieve performance statistically significantly better random action selection far recovering reliably pomdp model simple test environment 

algorithms autonomous learning pomdp models data tested conjunction approximate decision theoretic planners simplified robot navigation task 
methods novel radically different approach existing methods typically employ nonlinear optimization log likelihood 
algorithm clusters states similarities trajectories lead states subsequently compiles pomdps formed clusters 
similarity measures state clustering explored sum matching sequences matched states proved best similarity measures achieved best performance tested algorithms 
algorithm fails recover correct pomdp quite new improved similarity measures explored 
major challenge scale algorithm truly continuous state spaces current simplified world practice discrete experimental worlds reinforcement learning research 
leads associated problem finding small number actions sequences guaranteed reach goal state 
essential keep number actions small number affects adversely computational complexity algorithms 
current algorithmic implementations algorithms improved number observations experiments practical limit time takes seconds implemented matlab mex files pentium ii computer running mhz 
computational complexity allow experiments longer observation sequences absolutely necessary larger worlds 
currently working new version employs fast matching algorithm field dna analysis expect achieve complexity log allow experiments larger worlds 
table 
results learning methods planners 
shown average reward trials associated standard deviation statistical test difference achieved reward random action selection 
abbreviations ap planning mls state mdp voting voting mdp qmdp mdp proportional values 
see text definitions variables method ap mls voting qmdp random sga sigma sigma gamma sigma gamma sigma gamma sigma bw sigma sigma gamma sigma gamma sigma gamma sigma sigma gamma sigma gamma sigma gamma sigma gamma sigma sigma sigma sigma sigma sigma sigma gamma sigma gamma sigma gamma sigma gamma sigma sigma sigma gamma sigma gamma sigma gamma sigma sigma sigma sigma sigma sigma sigma sigma gamma sigma gamma sigma gamma sigma sigma gamma sigma gamma sigma gamma sigma gamma sigma sigma sigma sigma sigma sigma true sigma sigma sigma sigma sigma 
experimental world learning planning 
open space size inches results total possible location orientation pairs 
boutilier dean hanks 

decisiontheoretic planning structural assumptions computational leverage 
journal artificial intelligence research 
cassandra kaelbling kurien 

acting uncertainty discrete bayesian models mobile robot navigation 
proceedings ieee rsj international conference intelligent robots systems 
chrisman 

reinforcement learning perceptual aliasing perceptual distinctions approach 
proceedings tenth national conference artificial intelligence pp 

san jose ca mit press 
duda hart 

pattern recognition scene analysis 
john wiley sons 
kaelbling littman cassandra 

planning acting partially observable stochastic domains technical report cs brown university providence ri 
koenig simmons 

xavier robot navigation architecture partially observable markov decision process models 
kortenkamp bonasso murphy eds artificial intelligence mobile robots 
cambridge mit press 
mccallum 

instance state identification reinforcement learning 
advances neural information processing systems pp 

mit press 


learning stationary temporal probabilistic networks 
proceedings conference autonomous learning discovery 
pittsburgh pa carnegie mellon university 
nourbakhsh 

learning discrete bayesian models autonomous agent navigation 
proceedings ieee symposium computational intelligence robotics automation 
pp 
monterey ca ieee 
nourbakhsh 

office navigating robot 
kortenkamp bonasso murphy eds artificial intelligence mobile robots 
cambridge mit press 
russell binder koller 

adaptive probabilistic report csd university california berkeley 
stolcke omohundro 

hidden markov model induction bayesian model merging 
advances neural information processing systems pp 

morgan kaufmann san mateo ca 
sutton barto 

reinforcement learning 
cambridge mit press 
