review dimension reduction techniques miguel carreira technical report cs dept computer science university sheffield carreira dcs shef ac uk january problem dimension reduction introduced way overcome curse dimensionality dealing vector data high dimensional spaces modelling tool data 
defined search low dimensional manifold embeds high dimensional data 
classification dimension reduction problems proposed 
survey techniques dimension reduction including principal component analysis projection pursuit projection pursuit regression principal curves methods topologically continuous maps kohonen maps generalised topographic mapping 
neural network implementations techniques reviewed projection pursuit learning network bcm neuron objective function 
appendices complement mathematical treatment main text 
contents problem dimension reduction motivation 
definition problem 
dimension reduction possible 
curse dimensionality empty space phenomenon 
intrinsic dimension sample 
views problem dimension reduction 
classes dimension reduction problems 
methods dimension reduction 
overview report 
principal component analysis projection pursuit 
interesting projection 
projection index 
definition 
classification 
desirable properties 
outliers 
indices polynomial moments 
projection pursuit density estimation 
examples 
extension higher dimensions 
multidimensional projections 
relation pca 
partially funded spanish ministry education 
relation computer tomography 
exploratory projection pursuit epp 
localised epp 
projection pursuit regression ppr 
penalty terms ppr 
projection pursuit density approximation 
projection pursuit density estimation ppde 
generalised additive models 
backfitting 
multivariate adaptive regression splines mars 
principal curves principal surfaces construction algorithm 
extension dimensions principal surfaces 
topologically continuous maps kohonen self organising maps 
neighbourhood function 
kohonen learning 
summary 
disadvantages 
generative modelling density networks 
bayesian neural networks 
density networks 
generative topographic mapping gtm 
neural network implementation statistical models architectures layer perceptron 
principal component analysis networks 
projection pursuit learning network 
performance ppl compared bpl 
parametric ppr 
cascade correlation learning network 
bcm neuron objective function 
bcm neuron 
extension nonlinear neuron 
extension network feedforward inhibition 

glossary notation properties covariance matrix transformation covariance matrix transformations sample 
centring scaling sphering pca 
probability density function projected distribution density estimation density smoothing parametric nonparametric density estimation 
smoothing parameter 
nonparametric density estimation techniques univariate case 
histogram estimation 
naive estimator 
kernel density estimation 
nearest neighbours estimation 
generalised th nearest neighbour estimation 
variable adaptive kernel estimator 
orthogonal series estimation 
maximum penalised likelihood estimator 
general weight function estimator 
nonparametric density estimation techniques multivariate case 
nearest neighbour estimation 
generalised nearest neighbour estimation 
kernel estimation 
approximation sampling properties 
approximation properties kernel estimators 
method 
regression smoothing multivariate regression problem 
parametric nonparametric regression 
nonparametric regression techniques 
kernel regression smoothing nadaraya watson estimator 
nearest neighbour regression smoothing 
spline regression smoothing 

orthogonal series regression 
projection pursuit regression 
partitioning methods regression trees 
generalised linear models manifolds geometry high dimensional spaces 
consequences limit high dimensions 
multidimensional scaling way mds 
selection map dimension 
problems mds 
list figures dimension reduction problem 
example coordinate representation dimensional manifold 
curve surface 
bidimensional normal point cloud principal components 
dimensional projections dimensional data set 
principal curves generalised nonlinear symmetric regression 
jump latent space data space 
layer perceptron output unit 
examples neural networks principal component analysis 
autoencoder implemented layer nonlinear perceptron 
layer perceptron output units 
kernel function parametric ppr 
cascade correlation learning network 
bcm neuron associated synaptic loss functions 
bcm neurons inhibiting connections 
plot typical kernel functions 
drawback kernel smoothing 
generalised linear network 
examples manifolds 
coordinate system manifold 
examples manifolds boundary 
dependence geometric quantities dimension 
map resulting morse code similarities 
horseshoe phenomenon 
list tables comparison gtm kohonen som 
vs 
transformation covariance matrix transformations sample 
typical kernel functions efficiencies 
statistics smoothers 
density estimation vs regression smoothing 
volumes unit hypersphere hypercube 
rothkopf data similarities morse code symbols 
problem dimension reduction motivation consider application system processes data speech signals images patterns general form collection real valued vectors 
suppose system effective dimension individual vector number components high high depends particular application 
problem dimension reduction appears data fact higher dimension tolerated 
example take typical cases ffl face recognition classification system theta greyscale images row concatenation transformed mn dimensional real vectors 
practice images dimensional vectors say multilayer perceptron classification system number weights exceedingly large 
need reduce dimension 
crude solution simply scale images manageable size 
elaborate approaches exist neural network reduce vector dimension performs principal component analysis training data original dimension theta components second perform classification 
ffl statistical analysis multivariate population 
typically variables analyst interested finding clusters structure population interpreting variables 
aim quite convenient visualise data reasonably possible dimensions example data species ch 
ch 
ch 

individual measurements taken including head width front angle 
case reduction dimensional space projection pursuit techniques easily show clustered structure data 
ffl straightforward example simply estimation function variables finite sample 
due curse dimensionality see section greatly reduce size sample reducing number variables dimension data 
number occasions useful necessary reduce dimension data manageable size keeping original information possible feed reduced dimension data system 
summarises situation showing dimension reduction preprocessing stage system 
dimension reduction high dimensional data processing system intractable low dimensional data dimension reduction problem 
processing system effective vector data certain dimension data higher dimension reduced fed system 
phenomenon appearance high dimensional complex governed simple variables called hidden causes latent variables 
dimension reduction powerful tool modelling phenomena improve understanding new variables interpretation 
example ffl genome sequences modelling 
protein sequence different ones residue lengths varying tens tens thousands 
proteins spatial structure different sequences grouped families 
representation techniques see review exist allow visualise dimensional data sets colours rotation glyphs devices lack appeal simple plot known grand tour 
chernoff faces allow dimensions difficult interpret produce spatial view data 
tb ta sin cos st example coordinate representation dimensional manifold curve segment spiral fx sin cos st model protein families give insight properties particular families help identify new members family discover new families 
probabilistic approaches investigation structure protein families include hidden markov models density networks 
ffl speech modelling 
conjectured speech recognition undoubtedly exceedingly complex process accomplished variables 
definition problem formally problem dimension reduction stated follows 
suppose sample ft dimensional real vectors drawn unknown probability distribution 
fundamental assumption justifies dimension reduction sample lies approximately manifold nonlinear general smaller dimension data space 
goal dimension reduction find representation manifold coordinate system allow project data vectors obtain low dimensional compact representation data 
example principal component analysis employed linear manifold vector subspace obtained representation vector basis formed principal components data 
dimensional nonlinear manifold curve spiral radius step parameterised terms dimensionless parameter fx sin cos st notice domain parameter bounded interval 
arc length spiral points xa xb xb xa dx dt dx dt dx dt dt gamma manifold terms arc length gamma see section example 
parameterisation terms angle shows election coordinate system manifold means unique formal definition manifold appendix dimension reduction possible 
original representation data redundant reasons ffl variables variation smaller measurement noise irrelevant 
unfortunately don ready assertion 
different coordinate systems keep linear relationship 
familiar example cartesian spherical cylindrical systems ffl variables correlated linear combinations functional dependence new set variables 
situations possible strip redundant information producing economic representation data 
curse dimensionality empty space phenomenon curse dimensionality term coined bellman refers fact absence simplifying assumptions sample size needed estimate function variables degree accuracy get reasonably low variance estimate grows exponentially number variables 
example density smoothers base estimates local average neighbouring observations see appendix order find neighbours high dimensional spaces multivariate smoothers reach farther locality lost 
way avoid curse dimensionality reduce input dimension function estimated basis local objective functions depending small number variables unsupervised methods 
related fact responsible curse dimensionality empty space phenomenon scott thompson high dimensional spaces inherently sparse 
example probability point distributed uniformly unit dimensional sphere falls distance centre 
difficult problem multivariate density estimation regions relatively low density contain considerable part distribution regions apparently high density completely devoid observations sample moderate size 
example dimensional standard normal mass points contained sphere radius standard deviation gamma interval dimensional hyper sphere contains mass take radius standard deviations contain 
contrarily intuition high dimensional distributions tails important dimensional ones 
problem caused curse dimensionality linear correlations data situation high dimensions optimal mean integrated squared error estimating data density large see appendix sample size arbitrarily large 
appendix gives insight geometry high dimensional spaces 
intrinsic dimension sample consider certain phenomenon governed independent variables 
practice phenomenon appear having degrees freedom due influence variety factors noise imperfection measurement system addition irrelevant variables provided influence strong completely mask original structure able filter recover original variables equivalent set 
define intrinsic dimension phenomenon number independent variables explain satisfactorily phenomenon 
purely geometrical point view intrinsic dimension dimension manifold embeds sample unknown distribution dimensional space satisfying certain smoothness constraints 
incidentally know set theory card gamma delta card means map continuously example diagonal cantor construction principle allow find nonlinear continuous mapping preserving information 
course due finite precision practical application 
determination intrinsic dimension distribution sample central problem dimension reduction knowing eliminate possibility underfitting 
dimension reduction methods known author take intrinsic dimension parameter user trial error process necessary obtain satisfactory value practical applications domain information may give insight intrinsic dimension 
course problem ill posed data sample possible manifold dimension pass negligible error parameters 
example fig 
assume knowledge factors simplification problem possible 
write components xd binary expansion interleave expansions obtain binary expansion number dimensional manifold dotted curve forced interpolate set points naturally lie dimensional manifold shown dotted rectangle 
necessary introduce priori knowledge degree smoothness manifold terms regularisation term certain objective function 
curve surface 
views problem dimension reduction basic nature curse dimensionality surprising different fields affected 
look dimension reduction problem number perspectives ffl basically projection mapping dimensional space dimensional associated change coordinates 
ffl statistics related multivariate density estimation regression smoothing techniques 
ffl pattern recognition standpoint equivalent feature extraction feature vector reduced dimension 
ffl information theory related problem data compression coding 
ffl visualisation techniques performing kind dimension reduction multidimensional scaling sammon mapping ffl complexity reduction complexity time memory algorithm depends dimension input data consequence curse dimensionality reducing algorithm efficient expense moving complexity dimension reduction procedure course 
ffl latent variable models latent variable approach assume small number hidden causes acting combination gives rise apparent complexity data 
space low dimensional representation coordinate system mentioned 
classes dimension reduction problems attempt rough classification dimension reduction problems ffl hard dimension reduction problems data dimension ranging hundreds hundreds thousands components usually drastic reduction possibly orders magnitude sought 
components repeated measures certain magnitude different points space different instants time 
class find pattern recognition classification problems involving images face recognition character recognition speech auditory models 
principal component analysis widespread techniques practical cases 
ffl soft dimension reduction problems data high dimensional tens components reduction drastic 
typically components observed measured values different variables straightforward interpretation 
statistical analyses fields social sciences psychology fall class 
typical methods include usual multivariate analysis methods principal component analysis factor analysis discriminant analysis multidimensional scaling ffl visualisation problems data doesn normally high dimension absolute terms need reduce order plot 
lots applications disciplines fall category 
number methods practice including projection pursuit principal component analysis multidimensional scaling self organising maps variants interactive programs allow manual intervention xgobi 
allow time variable find categories static dimension reduction timedependent dimension reduction 
possibly useful vector time series video sequences continuous speech 
categorisation done attending discrete continuous nature data vectors 
example discrete data epg vector sequence binary values indicate presence absence tongue contact coarticulation studies example discrete data genome sequences mentioned earlier 
images example continuous data pixel value scaled interval 
methods dimension reduction 
overview report see manifold sought linear criterion maximise directional variance way dimension reduction problem exact analytical solution corresponds principal component analysis 
reasonably results practice probably widespread known dimension reduction techniques 
cases problem harder number techniques approach different perspectives low dimensional projections data projection pursuit generalised additive models regression principal curves self organisation kohonen maps topologically continuous mappings generative topographic mapping 
rest report organised follows section briefly presents principal component analysis 
section deals projection pursuit general statistical technique dimension reduction pca methods particular cases particular emphasis done concept projection index 
projection pursuit regression nonparametric regression approach projection pursuit introduced section generalised additive models particular case projection pursuit regression section 
section deals principal curves principal surfaces appear naturally generalisation regression nonlinear symmetrical case 
section introduces concept self organisation topological continuity mappings illustrates known kohonen maps generative topographic mapping gtm method mackay density networks 
section gives connectionist implementations techniques previously reviewed improvements 
report concluded short discussion material 
appendices complement mathematical part main text 
see discussion ad hoc dimension reduction methods 
principal component analysis principal component analysis pca possibly dimension reduction technique widely practice due conceptual simplicity fact relatively efficient algorithms polynomial complexity exist computation 
signal processing known karhunen lo eve transform 
consider sample fx mean covariance matrix sigma phi gamma gamma psi spectral decomposition sigma uu orthogonal diagonal 
principal component transformation gamma yields system sample mean diagonal covariance matrix containing eigenvalues sigma variables uncorrelated 
discard variables small variance project subspace spanned principal components obtain approximation best linear ls sense original sample 
shows example 
pc pc bidimensional normal point cloud principal components 
geometrically hyperplane spanned principal components regression hyperplane minimises orthogonal distances data 
sense pca symmetric regression approach opposed standard linear regression points component response variable rest predictors see sections 
key property principal component analysis attains best linear map gamma 
senses ffl squared sum errors reconstructed data 
ffl maximum mutual information assuming data vectors distributed normally original vectors projections ln eigenvalues covariance matrix 
principal components starting points algorithms projection pursuit regression principal curves kohonen maps generalised topographic mapping reviewed report 
exist neural network architectures capable extract principal components see section 
data clustered convenient apply pca locally 
example piecewise pca leen partition defined form vector quantisation data set pca applied locally region 
approach fast comparable results 
number numerical techniques exist finding eigenvalues eigenvectors square symmetric semidefinite positive matrix covariance matrix singular value see comprehensive treatment 
see section comparison transformations covariance matrix 
decomposition cholesky decomposition see 
covariance matrix order theta large explicitly computed neural network techniques section require memory space needed data vectors principal components 
unfortunately techniques usually gradient descent method slower traditional methods 
disadvantages pca ffl able find linear subspace deal properly data lying nonlinear manifolds 
ffl know principal components keep thumb rules applied practice 
example eliminate components eigenvalues smaller fraction mean eigenvalue keep necessary explain certain fraction total variance 
projection pursuit especially initial stages analysis data set exploratory wishes gain insight structure data 
projection pursuit unsupervised technique picks interesting low dimensional linear orthogonal projections high dimensional point cloud optimising certain objective function called projection index 
typically take profit human ability discover patterns low dimensional projections visual representations projected data density histograms smoothed density estimates scatterplots contour plots inspected ascertain structure data clustering 
projections smoothing operations structure obscured enhanced structure seen projection shadow actual structure original space 
interest pursue sharpest projections reveal information contained high dimensional data distribution 
remarkable feature projection pursuit applied regression density estimation projection pursuit regression multivariate methods able bypass curse dimensionality extent 
power projection pursuit algorithm find important structure suffer sample size small dimension large 
methods classical multivariate analysis special cases projection pursuit example principal component analysis 
disadvantages projection pursuit ffl works linear projections poorly suited deal highly nonlinear structure 
ffl projection pursuit methods tend computationally intensive 
scaled variable loadings components projection vectors define corresponding solution indicate relative strength variable contributes observed effect 
additionally applying rotation similar procedures projections produce picture easier interpretation variable loadings 
interesting projection 
consider projection interesting contains structure 
structure data ffl linear correlations variables readily detected linear regression 
ffl nonlinear clustering multimodality density function presents peaks skewness kurtosis sharp peaking discontinuities general concentration nonlinear manifolds 
results ffl fixed variance normal distribution information senses fisher information negative entropy 
ffl high dimensional clouds low dimensional projections approximately normal diaconis freedman 
consider normal distribution structured interesting density 
example shows projections data set consisting clusters 
projection plane spanned informative clusters confuse projection nearly coincides direction principal component proves projection index pca maximum variance see section indicator structure 
projection plane spanned clearly shows clusters 
term projection pursuit introduced friedman tukey projection index 
reviews projection pursuit huber jones sibson 
rotation procedure subspace projection selects new basis maximises variance giving large loadings variables possible 
projection mainly explained variables easier interpret 
projection basis fe projection basis fe dimensional projections dimensional data set 
projection index definition projection index real functional space distributions gamma 
normally fa distribution projection matrix dimensional random variable distribution correspond dimensional random variable theta see section 
abusing notation write fa 
projection pursuit attempts find projection directions distribution produce local optima optimisation problem independent length projection vectors obtain uncorrelated directions constrained unit length mutually orthogonal column vectors orthonormal 
optimisation problem optimise subject ffi ij consider dimensional projections treatment easily extendable dimensional projections consider problem maximisation index indicated 
classification random variable values huber distinguish classes indices ffl class location scale equivariance sx sq ffl class ii location invariance scale equivariance ii sx ii ffl class iii affine invariance iii sx iii huber introduces versions projection pursuit ffl version dimensional probability distributions random variable values ffl practical version dimensional samples point clouds xd thetan matrix representation sample vectors fx xng keep report short stick version context show refer sample 
equalities hold jq gamma ii ii ii iii class index desirable properties general interesting projections showing different insight correspond local optima projection index 
way discovering repeatedly invoke optimisation procedure time removing consideration solutions previously structure removal 
accordingly projection index ffl continuous derivative allow gradient methods 
ffl rapidly computable derivative optimisation procedure requires evaluating times 
ffl invariant nonsingular affine transformations data space discover structure captured correlation class iii 
ffl satisfy max central limit theorem normal interesting normal 
implies delta delta delta xn xn copies normal 
known class iii indices satisfying form iii monotone increasing class ii satisfying subadditivity superadditivity outliers projection pursuit procedure tend identify outliers presence sample gives appearance 
obscure clusters interesting structure sought 
sample covariance matrix strongly influenced extreme outliers 
consequently methods relying data sphering robust outliers 
effect outliers partially tackled robust sphering simple multivariate trimming method set convenient threshold distance 
repeat delete observations lie farther mean 
recompute mean 
certain small fraction data deleted 
indices polynomial moments approximating density truncated series orthogonal polynomials legendre hermite see section allows easy computation moments 
indices polynomial moments see eqs 
examples ffl computationally efficient 
ffl don recomputed step numerical procedure derived projection direction sufficient statistics original data set 
ffl heavily emphasise departure normality tails distribution outliers result perform poorly 
methods try address problem ways epp section nonlinear transformation gamma normal distribution function introduced 
bcm neuron objective function section sigmoidal function applied projections 
second order polynomials give measures mean variance distribution information multimodality 
higher order polynomials required measure deviation normality projection pursuit density estimation silverman suggests maximisation projection index carried 
finding estimate density projected points 

maximising projection index operating estimate 
step enormously facilitated appropriate differentiable density estimate particular example orthogonal expansions section 
results obtained corresponding optimum smoothing parameter opt behaviour index sensitive choice see appendix 
examples consider random variable density expectation fxg covariance matrix sigma phi gamma gamma psi ffl average deltag case max kak ffl variance ii var deltag phi gamma psi case max kak max umax largest eigenvalue normalised principal eigenvector sigma respectively 
words index finds principal component 
ffl standardised absolute cumulants km defined eq 
iii jk ffl fisher information iii ae ln oe gamma delta dx depends parameter 
varfxg minimised normal pdf oe oe 
ffl negative shannon entropy iii gammah fln ln dx fixed variance gammah minimised normal pdf oe ln eoe 
jones sibson propose ways evaluate entropy index synaptic plasticity models second order statistics lead extraction principal components 
visualisation program xgobi implements indices 
variant projection pursuit call projection pursuit exploratory data analysis 
implementing sample entropy ln dx numerical integration ln univariate nonparametric density estimate projected points slow compute 
approximating expansion terms cumulants projections ln dx ffl original univariate friedman tukey index oe ff gamma jx gamma oe ff ff trimmed standard deviation standard deviation ff data deleted parameter 
criterion large points clustered neighbourhood size huber noted index proportional kernel estimate uniform kernel gamma friedman tukey index essentially optimisation difficult discontinuous kernel discontinuous measure scale 
jones little difference practice ln integral ln minimised normal minimised epanechnikov kernel see table 
previous class iii indices satisfy property 
indices computed orthogonal series estimator hermite polynomials ffl gamma oe dx hall 
ffl gamma oe oe dx cook 
oe normal density eq 

indices ffl marriott propose dimensional indices designed display clusters minimise polar nearest neighbour index fmin gamma gamma gamma polar angles projected sample points ordered increasingly 
maximise mean radial distance equivalent minimising variance radial distance sphered data 
ffl proposes dimensional index radial symmetry 
general unanimity merits projection indices moment indices give poor results rest albeit faster compute 
extension higher dimensions indexes discussed admit simple extension dimensions depending nature ffl density estimation estimating dimensional density integrating 
ffl moments bivariate moments rr gamma oe oe oe oe dxdy bivariate hermite polynomials 
extension high dimensions analytically complicated indexes 
multidimensional projections projections dimension ffl computations get harder optimisation kd variables 
ffl optimising yields dimensional subspace cases preferable get ordered set directions 
attained recursive approach find interesting direction remove structure associated iterate 
leads projection pursuit density estimation see section 
usual starting projections optimisation procedure principal components just random starts 
repeated runs different starting projections required explore local optima provide locally interesting views data 
optima small domains attraction occur inevitably missed relation pca point view projection pursuit pca yields set directions ordered decreasing projected variance projection index equation variance projected data maximised subject constraint projection directions orthonormal 
pca particular case projection pursuit 
covariance matrix data set rank dimension data space number cases happens sample size smaller projection pursuit restricted largest principal components subspace contains variation contain structure 
principal components having small variation ignored reducing space dimension usually dominated noise contain little structure 
relation computer tomography huber points duality projection pursuit computer tomography ffl projection pursuit reduction projections high dimensional information random sample density seek reduced set projections density best approximate additive multiplicatively 
ffl computer tomography reconstruction projections finite usually fairly dense equispaced set projections density seek density projections best agree 
exploratory projection pursuit epp exploratory projection pursuit epp friedman projection pursuit procedure provides projection index optimisation strategy serves illustrate projection pursuit works practice 
consider dimensional epp 
centred sphered random variable projection 
obtain new random variable phi gamma gamma phi standard normal cdf eq 

uniform gamma standard normal 
nonuniformity mean gamma 
take projection index measure nonuniformity gamma pr gamma dr gamma dr gamma pr pdf approximated legendre polynomial expansion 
projected normal minimises problem max kak 
rapidly computed recursive relations legendre polynomials derivatives eq 

dimensional epp analogous kak kbk uncorrelated linear combinations unit variance 
sphering ensures 
avoid suboptimal maxima maximisation procedure visualised highfrequency ripple superimposed main variational structure hybrid optimisation strategy 
simple coarse stepping optimiser rapidly gets close substantive maximum domain attraction avoiding 
maximises coordinate axes sphered data principal components original data see details 

gradient method steepest ascent quasi newton quickly converge solution 
methods epp gives large weight fluctuations tails normal density oe small sensitive outliers scaling 
algorithms exist try avoid optima epp section 
localised epp localised epp intrator nonparametric classification method high dimensional spaces 
recursive partitioning method applied high dimensional space low dimensional features extracted epp node tree cart method exploratory splitting rule potentially biased training data 
implementation localised epp backpropagation network leads modified unsupervised delta rule 
localised epp computationally practical ffl sensitive curse dimensionality due feature extraction step 
ffl biased training data due cart method 
projection pursuit regression ppr projection pursuit regression ppr friedman stuetzle nonparametric regression approach multivariate regression problem see section projection pursuit 
works additive composition constructing approximation desired response function means sum low dimensional smooth functions called ridge functions depend low dimensional projections data constant hyperplanes 
ppr algorithm determines fa follows starting points set projection directions principal components random vectors 
residuals initially set 
repeat 
assuming fa gamma determined compute current residuals gamma gamma gamma gamma gamma gamma gamma 
fit nonparametric smooth curve residuals fr gamma function kak 
projection pursuit step minimise sum squared residuals norm relative arg min kak gamma gamma arg min kak ij 
insert term 
improvement small 
notice ffl representation exact unique polynomial 
ffl functions represented sum form finite equation readily implemented multilayer perceptron allows neural network implementation ppr see section details 
consider component vector function section 
friedman stuetzle friedman see section smoothers 
penalty terms ppr expressing smoothness constraint ridge functions smoothness measure merge steps ppr algorithm arg min kak ij 
shows estimation nonparametric ridge functions coupled estimation projection directions overfitting occurs estimating ridge functions search optimal projections yield results 
addressed ways ffl choosing ridge functions small family functions sigmoids variable bias 
need estimate nonparametric ridge function complexity architecture increased 
approach widely neural networks 
ffl concurrently sequentially estimating fixed number ridge functions projection directions provided ridge functions taken limited set functions 
presents small additional computational burden widely neural networks 
ffl partially decoupling estimation ridge functions estimation projections 
intrator proposes combination minimise ij ffl partial decoupling search achieved penalty term projection index 
ffl concurrent minimisation expressed general way penalty term 
neural network implementation ideas mlp number hidden units smaller input units penalty added hidden layer learning rule takes form ij gammaj ij ij contribution cost complexity terms error network example see equation bcm network inhibiting connections 
projection pursuit density approximation just arbitrary function probability density additive decompositions awkward approximating sums densities integrate general 
multiplicative decompositions better cf 
eq 
integrable take density mean covariance construction algorithm ppr stepwise types synthetic successive modifications starting simple density build structure analytic successive modifications strip away structure 
quality approximation estimate measured various criteria relative entropy hellinger distance relative entropy jjg huber proposes algorithms ffl synthetic start gaussian distribution mean covariance matrix step find new projection direction maximising jj current estimate indicates dimensional marginalisation corresponding density 
replace current estimate ff ffl analytic start step replace current estimate oe oe gaussian distribution mean covariance matrix continue jjg gaussian density 
projection pursuit density estimation ppde projection pursuit density estimation ppde friedman stuetzle schroeder appropriate variation densities concentrated linear manifold high dimensional space 
data sample operates follows 
sphere data 

take starting estimate standard normal dimensions 

apply synthetic 
generalised additive models generalised additive model gam hastie tibshirani dimensional density ff gams particular case ppr neural network implementation see section fg eq 
projection directions fixed axis directions determine zero mean ridge functions fg general interaction input variables function modelled easily interpretable functions plotted see applications 
add cross terms form kl achieve greater flexibility combinatorial explosion quickly sets 
backfitting ridge functions fg estimated backfitting algorithm start initial estimates fg obtained parametrically 
repeat 
compute current residuals gamma 
fit nonparametric smooth curve residuals fr function stopping criterion satisfied 
keeps iterating variables smoothing residuals explained gamma predictors remaining 
gam fitted local scoring algorithm natural generalisation iterative squares procedure linear case applies backfitting 
hastie tibshirani running line smoother 
multivariate adaptive regression splines mars multivariate adaptive regression splines mars friedman extension gams allow interactions variables ff lk kl case th basis function lk kl product dimensional spline functions kl depending variable number factors labels knots dimensional splines determined data 
basis functions added incrementally learning sequential forward selection 
principal curves principal surfaces principal curves hastie stuetzle smooth curves pass middle dimensional data set providing nonlinear summary 
estimated nonparametric way shape suggested data 
principal curves considered generalisation regression see fig ffl linear regression minimises sum squared deviations response variable ax vertical direction graph 
changing roles produces different regression line dotted line 
ffl principal component regression line symmetrical variables minimising orthogonal deviation line 
ffl nonlinear regression variety methods see appendix produce curve attempts minimise vertical deviations sum squared deviations response variable subject form smoothness constraint 
ffl principal curves natural generalisation nonlinear symmetric regression attempt minimise sum squared deviations variables orthogonal shortest distance curve points subject smoothness constraints 
linear nonsymmetric regression line 
linear symmetric principal component line 
nonlinear nonsymmetric regression curve 
nonlinear symmetric principal curve 
principal curves generalised nonlinear symmetric regression 
linear regression line minimises sum squared deviations response variable independent dashed line 
principal component line minimises sum squared deviations variables 
smooth regression curve minimises sum squared deviations response variable subject smoothness constraints 
principal curve minimises sum squared deviations variables subject smoothness constraints 
hastie stuetzle 
say curve self consistent respect dataset average data points project point curve coincides point curve 
formally smooth curve parameterised arc length data point seek nearest point curve euclidean distance orthogonal projection curve 
fxj self consistent distribution say principal curves pass middle data smooth way self consistent dataset 
definition poses questions far unanswered general case definiteness exceptional case nearest points take largest ffl kinds distributions principal curves exist 
ffl different principal curves exist distribution 
ffl properties 
questions answered particular cases ffl ellipsoidal distributions principal components principal curves 
ffl spherically symmetric distributions line mean principal curve 
ffl spherically symmetric distributions circle centre mean radius principal curve 
properties principal curves known ffl model form ffl smooth ffflg principal curve general 
means principal curve biased functional model bias small decrease variance errors gets small relative radius curvature ffl straight line self consistent principal component eigenvector covariance matrix 
words linear principal curves principal components 
principal curves depend critically scaling features projection techniques 
current algorithms depend degree smoothing 
construction algorithm 
iteration index start smooth prior summary data typically principal component 

projection step project distribution candidate curve 

averaging step fxj conditional expectation terms arc length 
points curve self consistent principal curve finish 
take smooth local average dimensional points definition local distance arc length fixed point parameterisation curve projections points previous curve goto 
observe ffl construction algorithm converges principal component conditional expectations replaced squares straight lines 
principal curves local minima distance function sum squared distances 
ffl probability distributions operations projection average conditional expectation reduce expected distance points curve discrete data sets unknown 
ffl construction algorithm proven converge general 
extension dimensions principal surfaces principal curves naturally extended dimensions called principal surfaces 
curse dimensionality smoothing dimensions hard data abundant 
investigation required see principal surfaces interest high dimensional problems 
discrete data sets averaging step estimated means scatterplot smoother 
topologically continuous maps term includes related techniques mainly visualisation high dimensional data known kohonen self organising maps 
objective techniques learn unsupervised manner mapping space fixed dimension called lattice latent space high dimensional data space embeds data distribution 
common element concept topological topographic map basically means continuous mapping mapping assigns nearby images codomain nearby points domain 
term topologically continuous maps appropriate undesirable connotations mathematical theory topological spaces similar term topographic maps satisfactory 
self organising maps somewhat vague probably better election unfortunately linked kohonen maps literature 
kohonen self organising maps ft sample data space kohonen self organising maps soms considered form dimension reduction sense learn unsupervised way mapping lattice data space 
mapping preserves dimensional topology lattice adapting manifold spanned sample 
visualise learning process plane sheet twists dimensions resemble possible distribution data vectors 
neighbourhood function som vector quantisation set codebook vectors data space initially distributed random associated node lattice vector quantisation topology exists 
assume defined distances typically euclidean dd dl data space lattice respectively 
topology lattice determined neighbourhood function ij symmetric function values behaves inverse distance lattice node ii node node ij smaller farther apart node node lattice 
neighbourhood node composed nodes ij negligibly small 
practice usually ij exp gammad oe oe quantify range neighbourhood 
kohonen learning competitive learning rule applied iteratively data vectors convergence achieved 
data vector vector closest data space arg min lattice dd learning occurs follows iteration index ff learning rate new old ff gamma old gamma ae old aet vector drawn distance ae ff data vector update affects vectors associated nodes lie neighbourhood winner intensity decreases iteration index ff range decrease convergence considerations 
intuitively sees vectors dense regions common sparse uncommon replicating distribution data vectors 
summary kohonen learning creates dimensional arrangement typical case idea valid dimensional topological arrangements 
take random set data vectors principal components 
online learning 
batch version exists 
ffl number density vectors data space approximately proportional data probability density 
ffl mapping dimensional arrangement data space topologically continuous 
disadvantages proven successful practical applications particularly visualisation 
due heuristic nature number shortcomings ffl cost function optimise defined 
ffl schedules selecting ff exist guarantee convergence general 
ffl general proofs convergence exist 
ffl probability distribution function generative model data obtained 
dimensional manifold data space defined indirectly location vectors intermediate points interpolate 
generative modelling density networks generative modelling observables problem assigned probability distribution bayesian machinery applied 
density networks mackay form bayesian learning attempts model data terms latent variables 
introduce bayesian neural networks density networks conclude gtm particular model density networks 
bayesian neural networks assume data want model parameters define likelihood djw probability data parameters 
learning classified ffl traditional frequentist mlp distribution parameters assumed interested single value maximum likelihood estimator arg max fln regularisation term quadratic regulariser hyper parameter ff ff new data predicted jw 
ffl bayesian density networks probability distribution model parameters obtained prior distribution expresses initial belief values data arrived 
data update prior posterior distribution bayes rule wjd djw new data predicted jd jw wjd dw traditional learning viewed maximum posteriori probability map estimate bayesian learning cf 
eq arg max wjd arg max ln fl arg max fln ln prior exp 
quadratic regulariser prior proportional gaussian density variance ff 
bayesian approach presents advantage frequentist finding full distribution parameters 
earned expense introducing prior selection criticised arbitrary 
data space dimension manifold induced prior latent space dimension jump latent space data space 
density networks want model certain distribution data space sample ft drawn independently terms small number latent variables 
likelihood log likelihood mackay calls evidence log evidence djw jw ln djw ln jw define functions convenient way problem considered ffl prior distribution dimensional latent space ffl smooth mapping latent space dimensional manifold data space parameters example mlp weights biases gamma 
ae gamma 
jump dimensions key dimension reduction 
ffl error functions gn ln jx ln jy 
example softmax classifier classification problem classes ijx ff functions parameterised linear logistic model single sigmoidal neuron binary classification problem jx gammaw deltax particular case softmax classifier 
function euclidean squared distance kt gamma yk gtm model eq 

bayes rule compute posterior latent space xjt jx jw dimensional mapping singular 
normalisation constant jw jx dx applying bayes rule find posterior parameter space ft jw ft ft jw jw 
learning parameters maximum likelihood take place gradient descent log likelihood eq 
rw fln jw jw gn dx xjt dx gradient log likelihood expectation traditional backpropagation gradient posterior xjt eq 

computational point view integral analytically difficult priors 
log likelihood derivatives approximated monte carlo sampling ln gn dx ln gn xr rw gn xr gn xr fx random samples costly process sampling depends exponentially dimension integral summary density networks provide framework generative modelling adapted specific problems conveniently selecting ffl dimension latent space ffl prior latent space simple possible allow easy integration error function 
ffl smooth mapping possible differentiable allow standard optimisation techniques 
ffl error function tjx 
ffl optimisation algorithm maximising posterior parameter space 
relationship mlps mlps outputs conditioned values input variables density model input variables 
conversely density modelling generative modelling density observable quantities constructed 
target outputs specified inputs 
density networks considered generative half bottleneck output 
recognition mapping input bottleneck responsible feature extraction plays role probabilistic model see fig 

generative topographic mapping gtm generative topographic mapping gtm put forward bishop svens en williams principled view kohonen soms density network constrained gaussian mixture model trained em algorithm 
biological motivation intended 
gtm dimension latent space assumed small usually various features general framework density networks selected follows dx sample fx pdf 
ffl error functions gn tjx spherical gaussian fi gamma centred variance fi gamma convenience notation separate parameters mapping parameters variance parameter fi tjx fi fi exp gamma fi ky gamma tk behaves noise model extends manifold data vector generated point probability tjx fi 
ffl prior latent space ffi gamma fx stand regular grid latent space 
prior choice equivalent monte carlo approximation arbitrary 
eq 
fi tjx fi constrained mixture gaussians gaussians lie dimensional manifold data space fi ln jx fi just eq 

linear gaussian gaussian gtm reduce particular case factor analysis variances equal fi gamma 
ffl mapping selection generalised linear model woe theta matrix weights oe theta vector basis functions see appendix 
turns step optimisation matrix equation see 
typically gaussians explicitly set parameters basis functions centres drawn grid latent space 
variances chosen accordingly known distance centres 
actual shape basis functions probably unimportant long localised 
generalised linear networks universal approximators limits dimension latent space number basis functions required attain accuracy grows exponentially serious disadvantage cases 
number sample points bigger order obtain smooth mapping suggested take 
parameter model increasing favour overfitting 
ffl model parameters fi determined maximum likelihood provides objective function fi ln jw fi ln jw fi regularisation term prior parameters added control mapping mixture distribution suggests em algorithm maximise log likelihood reason position points nodes regular grid facilitate visualisation posterior computer screen 
additional column biases added consequently additional fixed component oe oe 
step computation responsibilities see eq 

step partial derivatives parameters fi obtains matrix equation phi old phi new phi old solvable new standard matrix inversion techniques 
reestimation formula fi fi nd fi ky gamma phi oe ij theta matrix constants oe ij oe 
theta matrix constants 
theta matrix posterior probabilities responsibilities fi jt fi jx fi jx fi diagonal theta matrix elements ii fi fi 
em algorithm increases log likelihood monotonically convergence gtm guaranteed 
convergence usually achieved tens iterations 
initial weights take principal components sample data ft convergence value fi small approximation 
online version step obtained decomposing objective function data points robbins monro procedure kj kj ff kj fi fi ff fi learning rate ff appropriately decreasing function iteration step convergence extremum assured 
posterior probabilities visualisation fixed fi point data space eq 
gives posterior distribution considered inverse mapping jt fi jx fi jx fi responsibility visualisation techniques provide just single responsible point gtm provides full posterior distribution jt result bayesian approach 
considerable amount data simplify summarise distribution mean misleading multimodal gives actual examples situation 
magnification factors local magnification factor relation volume elements manifold latent space computed analytically gtm dv dv det psi psi jk oe dv dx local magnification factor may contain information clustering properties data small near center cluster high cluster boundaries 
useful highlight boundaries 
centroid theta matrix ik responsibility matrix rt gm 
centroids defined tn quadratic regulariser added fi equation phi old phi gamma ff fi new phi old constant regularisation hyper parameter ff 
notice equation incorrect 
som gtm internal representation manifold nodes fig dimensional array held neighbourhood function point grid fx dimensional latent space keeps topology smooth mapping definition manifold data space indirectly locations vectors mapping objective function log likelihood self organisation difficult quantify smooth mapping preserves topology convergence guaranteed 
batch em algorithm online robbins monro smoothness manifold depends ff depends basis functions parameters prior distribution generative model density function fi additional parameters select ff arbitrarily ff annealed robbins monro schedules online version speed training comparable magnification factors approximated difference vectors exactly computable table comparison gtm kohonen som 
extensions gtm basic generative model gtm extended ffl deal missing values unobserved components data vectors missing random objective function obtained integrating unobserved values 
ffl mixtures gtm models form lth model independent set parameters discrete distribution provides mixing coefficients model 
mixture trainable maximum likelihood em algorithm 
gtm effective visualisation purposes advantage kohonen soms having solid theoretical basis see table comparison gtm 
stands doesn suitable hard dimension reduction problems dimension latent space large number basis functions exceedingly big 
different choice mapping able cope curse dimensionality overcome difficulty 
dimension reduction technique insight dimension latent space 
gtm choice latent space driven ffl prior knowledge problem 
example problem multiphase flows oil pipelines bishop apply gtm known advance points data space generated random process degrees freedom dimension data space 
ffl constraints fact dimension latent space exceed visualisation 
neural network implementation statistical models section see algorithms previous sections implemented natural way neural networks clever election architecture learning algorithm 
implementation new projection indices intrator bcm neuron objective function 
architectures layer perceptron xd fi fi hidden output input layer perceptron output unit 
consider layer nonlinear perceptron fig 

input units hidden units output unit 
activation function unit general different 
output general function activation hidden layer weights second layer fi fi 
particular cases network implement techniques previously mentioned ffl projection pursuit regression cf 
eq 
fi activation functions determined data training represent projection directions 
incidentally shows continuous functions uniformly approximated sigmoidal mlp input 
approximation capabilities mlps pp similar 
architecture admits generalisations output variables depending output share common basis functions separate share common projection directions ffl generalised additive models fi ffl kernel estimation section radial basis functions gammad kx gamma radial basis function spherically symmetric multivariate gaussian density centred scale parameter variations regularisation wavelets 
principal component analysis networks exist neural network architectures capable extract pcs see fig 
classified see details ffl called bottlenecks networks linear layer perceptrons inputs hidden units outputs trained replicate input output layer minimising squared sum errors typically trained backpropagation 
bourlard kamp baldi hornik showed network finds basis subspace spanned pcs necessarily coincident see applications 
ffl networks oja rule kind decorrelating device kung apex ak network sanger generalised hebbian algorithm 
input units hidden output units output units linear apex network examples neural networks able perform principal component analysis training set left linear trained backpropagation right apex network hebbian weights anti hebbian decorrelating weights cases number hidden units determines principal components kept 
linear extended including nonlinear activation functions layers see fig 

surprisingly approach little success possibly due difficulty train network 
linear sigmoidal recognition mapping generative mapping autoencoder implemented layer nonlinear perceptron 
projection pursuit learning network hwang propose layer perceptron depicted fig 
projection pursuit learning ppl algorithm solve multivariate nonparametric regression problem section 
outputs necessary true principal directions obtained projections data basis method numerical methods pca networks 
computational effort second step smaller number dimensions decreased substantially compared original 
applications basis necessarily principal components practical purposes 
fi fi qj yq input hidden output jd xd layer perceptron output units 
perceptron expressed fi lk gamma fi lk gamma wk kwk bias th hidden unit 
argument ak gamma sk translated scaled dimensional projection 
backprojection learning bpl usually sigmoids weights network estimated simultaneously gradient descent 
hwang propose projection pursuit learning algorithm ppr see section trains weights sequentially considered neurons added hidden layer time ffl fy fy fi lk fy il fg phi psi ka fg unknown smooth functions 
ffl estimate projection directions fa projection strengths ffi lk unknown smooth functions fg ls minimisation loss function fa ffi lk fg phi gamma psi gamma fy gamma fi lk fw response weightings specify relative contributions output typically var fy variances known frequently estimated data 
expressing function residual functions gamma fy gamma fi lm gm am phi gamma fi lk psi ffl learn unit unit layer layer cyclically patterns algorithm unit assign initial guesses ffi lk repeat repeat times estimate iteratively new value estimate values estimate ffi lk minimised certain stopping criterion 
training algorithm presents clear difference conventional training neural networks weights updated time 
hwang stopping criterion jl new gammal old old 
estimation different sets parameters follows ffi lk quadratic fi lk fixed fi im am linear ls minimisation fi lk yields fi lk phi psi ff ffi lk data smoother 
construct unconstrained nonsmooth estimate satisfying min fl ki gamma gamma fi lk fl ki delta fl ki fi lk fi lk 
find smooth curve best represents scatterplot ki ki ki hwang propose section smoother hermite functions section 
ffi lk gauss newton nonlinear ls method quadratic ffl forward growing procedure hidden neurons added time 
parameters hidden neuron estimated parameters previous neurons fine tuned backfitting see section fast easy output units linear 
ffl backward pruning procedure implemented overfitting neurons removed time fitting models decreasing size gamma gamma neurons obtained 
performance ppl compared bpl approximation theorems form noise free square integrable function approximated arbitrary degree accuracy exist bpl ppl see section said number neurons required practice 
hwang report empirical results simulation ffl ppl hermite polynomials outperforms bpl learning accuracy mse estimation test data set equal number hidden neurons learning parsimony number hidden neurons required attain fixed mse gauss newton bpl particularly learning speed cpu time employed training fixed mse 
ffl ppl hermite polynomials normally outperform 
parametric ppr backfitting nonparametric ppr presents problems ffl difficult choose smoothing parameter 
ffl converges slowly large bias noise free data 
zhao atkeson propose parametric ppr direct training achieve improved training speed accuracy lk direction projections equispaced knots range projections data points fixed directions dimensional weight functions usually depending js gamma tj :10.1.1.17.6536
ideal similar cubic spline smoother asymptotic form large gamma nf gamma juj sin juj local density local bandwidth kernel function eq 
depicted fig 

directions parallel hyperplanes evenly distributed xd kernel function parametric ppr 
jp linear parameters lk jd nonlinear parameters dimensional directions consider layer perceptron fig 
activation function fixed location parameters nodes grouped directions trainable mlp achieves dimension reduction zhao atkeson propose efficient training algorithm direct search parameters initial values repeat times 
optimise simultaneously lk nonlinear ls method stopping convergence 

ls fit lk fixed 
initial estimates directions obtained second derivatives target functions data points known difficult 
backfitting simple smoother provide approximate initial directions 
grouping hidden units number hyperplanes jp groups reduces number parameters net fraction keeps constant asymptotically number cells dimensional space hidden layer partitions space cells produced intersecting hyperplanes 
means accuracy layer perceptron fewer neurons training easier generalisation ability better 
dc number cells number hidden units grows linearly dimension fixed accuracy curse dimensionality reduced 
cascade correlation learning network shows cascade correlation learning network 
supervised learning architecture dynamically grows layers hidden neurons fixed nonlinear activations sigmoids network topology size length efficiently determined 
training algorithm follows new candidate unit 
train input hidden hidden layer units criterion maximum correlation candidate unit value residual output errors gamma deltaw maxr phi gamma gamma psi gamma gamma averages training patterns 
lms output input hidden cascade correlation learning network 
inputs new hidden unit input units previous hidden units increasing dimension input units fixed dimension high order nonlinearity employed residual error approximation input connections input connections nonparametric trainable activation functions activation function fixed variable trainable nonparametric suitable classification regression classification retraining hidden layer backfitting table vs 

re train output hidden output layer units new previous hidden units mse criterion speedy quickprop 
hwang give comparison ffl new hidden unit pool candidate units receives connections input layer previous hidden units provide high order linearity approximating residual error 
ffl input dimension candidate hidden unit increases nonlinear activation remains fixed making training difficult weight search high dimensional space 
ffl maximum correlation criterion pushes hidden units saturated extreme values active region making decision boundary classification interpolated surface regression zigzag 
suitable regression classification 
ffl fast retraining output layer mse speedy quickprop need errors hidden units hidden layer retrained 
table compares 
bcm neuron objective function bcm neuron shows bcm neuron bienenstock cooper munro 
input neuron output 
define threshold phi psi phi psi functions oe gamma oe gamma oe suggested biologically plausible synaptic modification function explain visual cortical plasticity 
threshold dynamic depends projections nonlinear way move position distribution concentrated sides see fig 

distribution part mass sides plausible projection index seeks 
xd wd oe lw bcm neuron associated functions oe synaptic lw loss 
loss function attached bcm neuron learning weights seeks minimise average loss risk 
risk smooth function minimisation accomplished gradient descent 
different loss functions yield different learning procedures 
consider family loss functions lw oe gamma phi psi gamma lw risk rw flw phi psi gamma phi psi learning rule dw dt gamma rw gamma phi psi gamma phi psi delta foe dw dt gamma wrw foe xg time decaying learning rate 
extension nonlinear neuron turning linear bcm neuron nonlinear achieve ffl insensitivity outliers means sigmoidal activation function cf 
eq 
oe oe outliers points away mean lie flat part sigmoid 
ffl ability shift distribution part satisfies mode means bias oe 
biological viewpoint bias identified spontaneous activity 
lw oe oe gamma gamma foe oe oe xg extension network feedforward inhibition adding inhibition connections bcm neurons see fig 
inhibited activity neuron gamma threshold phi psi risk phi psi gamma phi psi total intrator uses different definition functions papers oe gamma oe gamma inhibiting connections hidden input bcm neurons inhibiting connections 
compare 
risk learning rule linear neurons gamma wk gamma gamma oe gamma oe nonlinear neurons activation oe gamma gamma oe oe gamma oe oe gamma oe oe gamma oe oe intrator bcm feedforward inhibition network speech data facial images 
ffl uses low order polynomial moments efficiently computable sensible outliers due addition sigmoidal activation function 
ffl natural extension multidimensional projection pursuit feedforward inhibition network 
ffl gradient optimisation nn linear dimension number projections sought 
defined problem dimension reduction search economic coordinate representation submanifold high dimensional euclidean space problem far solved satisfactory general way 
overview known techniques dimension reduction straightforward implementations neural networks 
major issues remain open ffl overcome curse dimensionality demands huge sample sizes obtain reasonable results 
techniques reviewed suffer extent 
ffl determine intrinsic dimension distribution sample 
central problem dimension reduction knowing eliminate possibility underfitting 
conclude mentioning number techniques related dimension reduction included due lack time 
include helmholtz machine variations self organising maps growing neural gas bayesian approaches population codes curvilinear component analysis 
glossary ffl backfitting algorithm iterative method fit additive models fitting term residuals rest see section 
version gauss seidel methods numerical linear algebra 
ffl bcm bienenstock cooper munro model neuron selectivity 
ffl bpl backpropagation learning 
ffl cart classification regression trees 
ffl cascade correlation learning network 
ffl cdf cumulative distribution function 
ffl epp exploratory projection pursuit 
ffl equivariant procedure correspondingly transforms answer transformation input oe oe 
invariant procedure transform answer transformation input oe oe 
ffl gam generalised additive model 
ffl hmm hidden markov models 
ffl iid independent identically distributed 
ffl ls squares 
ffl map maximum posteriori probability 
ffl mars multivariate adaptive regression splines 
ffl mds multidimensional scaling 
ffl mlp multilayer perceptron 
ffl plot sample fx point gets part density mass put needle value observation get impression distribution sample 
noisy estimate density 
equivalent sum discrete dirac deltas located observations 
ffl pc principal component 
ffl pca principal component analysis 
ffl pdf probability distribution function density 
ffl pmf probability mass function 
ffl pp projection pursuit 
ffl projection pursuit density approximation 
ffl ppde projection pursuit density estimation 
ffl ppl projection pursuit learning 
ffl projection pursuit learning network 
ffl ppr projection pursuit regression 
ffl rbf radial basis function 
notation ffl kronecker delta ffi ij ffl indicator function ffl sigmoid function oe gammax ffl norm vector kvk jv just write deltak mean deltak kvk max jv ffl sphere centred origin radius fx kxk rg hollow sphere replace 
ffl hypercube centred origin side fx kxk rg gammar hollow hypercube replace 
ffl half space fx xd 
ffl coordinate unit vectors fe ij ffi ij ffl vector ones whichever dimension ffl identity matrix whichever dimension ffl gamma gamma delta thetan theta verifies 
ffl square orthogonal matrix gamma ffl projection pi gamma 
base im pi direction ker pi represented square matrix pi thetad symmetrical pi pi idempotent pi pi 
properties hold phi dim dim gamma pi rank represented coordinates matrix ad thetaj gamma 

projection orthogonal column vectors basis orthonormal 
piz pi matrix projection unit vector pi vv ffl matrix representation sample vectors fx xn xd thetan xn 
columns coordinate vectors rows univariate samples 
ffl translation sample vectors fx xn vector vector form matrix form xd thetan xn ffl mean covariance matrix sigma thetad oe ij sample vectors fx xn vector form sigma phi gamma fxg gamma fxg psi gamma xx matrix form xd thetan xn sigma gamma gamma delta xjx ffl square root positive semidefinite symmetric square matrix aa diag diag gamma delta spectral decomposition uu note unique orthogonal valid 
ffl order statistics sample fx fx delta delta delta handy notation deal ordered data 
ffl average operator fxg fx sample discrete random variable pmf xf dx continuous random variable pdf ffl moment order univariate density fu dt phi tu psi particular fxg mean var fxg oe gamma variance 
ffl cumulant order univariate density km dt ln phi itu psi gamma particular mean gamma variance gamma skewness gamma gamma gamma kurtosis 
random variable zero mean unit variance gamma standard normal 
ffl pdf multivariate gaussian normal distribution dimensions sigma mean covariance matrix sigma thetad oe det sigma gamma gamma sigma gamma gamma det sigma gamma gamma sigma gamma gamma pdf standard multivariate oe gamma kxk pdf dimensional normal distribution oe mean variance oe oe oe gamma gamma oe pdf standard normal oe gamma cdf standard normal phi gamma oe dt gamma gamma dt ffl norm function kfk gammar dx delta just write deltak mean deltak ffl space space random dimensional variables fx 
convergence xn means fxn gamma xg 
space functions gamma 
kfk 
space square integrable functions 
ffl criteria measure goodness approximation function density estimate density random variable sum squares norm kf gamma gk relative entropy kullback leibler distance jjg ae ln oe distance metric jjg general 
hellinger distance gamma dx mean squared error pointwise error criterion sample fx mse phi gamma psi bias fg bias fg gamma 
shows bias variance decomposition 
mean integrated squared error global error criterion sample fx mise phi kg gamma fk psi gamma dx oe mse dx ffl entropy random variable ae ln oe continuous random variable entropy called differential entropy 
conditional entropy random variable random variable jx gamma fln jx mutual information random variables gamma jy properties covariance matrix sigma thetad symmetric semidefinite positive sigma admits spectral decomposition sigma uu ud orthogonal diag normalised eigenvector sigma associated eigenvalue transformation covariance matrix transformations sample spectral decomposition covariance matrix matrix notation introduced section easy construct table gamma diag diagonal orthogonal primes denote new entity transformation symbol indicate subsequent transformation complex obviously related 
transformation sigma translation sigma rotation qx qx qu rotated xq xq sigma gamma axis scaling dx dx uniform axis scaling ax ax sigma scaled affine ax ax sigma scaled centring gamma sigma pca gamma fe sphering sigma gamma gamma fe table transformation covariance matrix transformations sample 
centring scaling sphering pca assume xd thetan sample vectors mean fxg covariance matrix sigma oe ij spectral decomposition sigma sigma uu orthogonal diagonal 
ad thetaj set projection directions 
ffl centring procedure translate sample mean origin gamma fxg gamma fx 
centring inherited set projections fxg phi psi fxg 
ffl scaling achieves unit variance axis dividing componentwise standard deviation oe diag gamma oe gamma delta oe ii 
ffl sphering affine transformation converts covariance matrix centred sample unit variance matrix destroying second order information sample sigma gamma gamma fxg fx sigma sigma gamma gamma sphering inherited orthogonal set projections fxg cov fxg cov phi psi phi psi phi xx psi ffl pca affine transformation converts covariance matrix centred sample diagonal matrix decorrelating variables preserving variance information gamma fxg fx sigma pca sphering translation rotation invariant applying translation rotation data performing pca sphering produces results performing original data 
sigma gamma gamma fxg orthogonal produce result 
case canonical transformation sigma gamma gamma fxg case gives gamma gamma fxg 
probability density function projected distribution pdf pi projection base direction dim orthogonal projection distribution subspace new pdf pi dy integral point projection subspace extended points project point fa fb gammaj basis respectively previous integral pi delta delta delta gammaj dd gammaj particular case marginal densities delta delta delta xd dx cloud points projected clouds obtained projecting stepwise subspace density estimation density smoothing density estimation set unlabelled data fx drawn unknown distribution want model 
learning model unsupervised nature cf 
regression smoothing appendix 
distribution characterised pdf predicts observations cluster occur frequently clearly shows skewness multimodality 
problem density estimation stated estimate unknown density sample fx realisations iid random variables density 
parametric nonparametric density estimation approaches estimate ffl parametric assume follows certain model equip finite set parameters estimate 
presents problem selecting model 
deal 
ffl nonparametric assumptions shape dictated data fx main nonparametric density estimation approaches shortly described 
section consider univariate data fx ae section generalise results multivariate case 
method local smoothness pdf properties resulting estimate continuous differentiable pdf problems 
estimators consistent mse nh 
smoothing parameter estimators tuned smoothing parameter ffl small spurious fine structure data appears estimate noisy 
limit case obtains sum dirac deltas 
ffl large structure obscured estimate smooth 
limit case obtains constant function 
common problem density estimation approaches optimum selection smoothing parameter 
methods exist cross validation bootstrap automatically select data 
nonparametric density estimation techniques univariate case histogram estimation divide real line bins gamma jh bin width origin estimator count data fall bin nh bin nh bins varying size bin width bin containing histogram kernel estimator uniform kernel 
local smoothness discontinuous bin boundaries zero derivative 
problems choice origin affect estimate 
estimators independent origin choice 
appendix mainly books silverman scott 
possible estimate cdf obtain pdf df dx empirical cdf fn gamma 
shown unbiased estimator smallest variance unbiased estimators cdf fact cdf continuous difficult observe skewness multimodality particularly multivariate data preferable directly try estimate pdf 
naive estimator naive estimator approximation formula lim gamma holds definition cdf 
small nh interpreted histogram point centre bin independent origin choice 
naive estimator kernel estimator uniform kernel 
local smoothness discontinuous sigma zero derivative 
kernel density estimation kernel estimators motivated points view including numerical analysis finite difference approximations derivative cdf pdf signal processing convolution pdf window function 
define kernel function nonnegative unit integral typically pdf dx typically gammax kernel estimator bandwidth window width gamma sum bumps shape width placed observations 
table lists popular kernels shows graphical aspect 
local smoothness pdf inherited problems applied data long tailed distributions spurious noise appears tails estimate fixed 
smoothing estimate avoid masks essential detail main body distribution see fig 

nearest neighbours estimation previous methods count number observations box width nearest neighbours estimation amount smoothing adapts local density box wide contain samples 
considerably smaller sample size typically arrange distances descending order delta delta delta dn 
estimate nd bigger tails main body reduces problem tails 
gamma gamma gammak tails die away rate gamma integrable pdf 
local smoothness inherited continuous discontinuous derivative 
problems heavy tails 
kernel estimator implemented fast fourier transform 
strictly necessary dirichlet kernel table kernel zhao atkeson fig :10.1.1.17.6536

kernel eff exact epanechnikov gamma gamma quartic gamma gamma triangular gamma juj gamma gaussian gammau uniform rectangular gamma gamma gamma cosine cos gamma delta gamma dirichlet sin sin gamma er sin sin gamma table typical kernel functions efficiencies 
uniform gaussian epanechnikov cosine triangular plot typical kernel functions 
drawback kernel smoothing 
left picture shows original distribution dotted line center picture kernel smoothing optimum applied spurious noise appears tail right kernel smoothing larger applied estimate producing large bias peak 
generalised th nearest neighbour estimation kernel estimator evaluated nd gamma uniform kernel gives ordinary nearest neighbours estimation 
local smoothness differentiable 
pdf precise integration tail properties depend particular kernel 
variable adaptive kernel estimator define ik distance th nearest point set comprising gamma data points 
variable kernel estimator hd ik gamma hd ik controls degree smoothing sensitivity local detail 
data points regions data sparse flatter kernels associated 
words local bandwidths reduce bias peaks valleys avoid near boundary interval restricted 
local smoothness pdf inherited orthogonal series estimation assume weight function restricted interval family orthonormal functions respect scalar product hf gi dx ffi jk sum hf fw converges pointwise 
sample coefficients give natural unbiased estimate 
converges ffi gamma omega ffi gamma ff order smooth estimate apply low pass filter typically truncating series expansion terms smoothing parameter smooth noisy 
case estimation parametric fc generally define weights rate converge smoothing parameter 
local smoothness pdf depend particular series system weights 
general nonnegative 
usual families orthogonal functions ffl fourier series ijx respect considered dirichlet kernel see table 
ffl legendre polynomials rodrigues formula dx gamma gamma respect satisfying recurrence relation gamma xp gamma gamma gamma gamma gamma gamma gamma normalised legendre polynomials ffl hermite polynomials rodrigues formula gamma dx gammax gamma respect gammax satisfying recurrence relation xh gamma gamma gamma gamma jh gamma normalised hermite polynomials 

series estimators give compact representation estimates class densities low number coefficients particularly useful low dimensional projections 
maximum penalised likelihood estimator likelihood curve density underlying set fx iid observations xn finite maximum class densities shown close desired ffi gamma 
define penalised log likelihood ff log gamma ffr ffl quantifies goodness fit 
ffl quantifies roughness dx 
ffl ff smoothing parameter 
maximum penalised likelihood estimator ff arg max ff set functions pdf 
local smoothness imposed search space pdf definition problems difficult calculation implicit definition 
similar done regularised gtm algorithm section 
general weight function estimator generalisation previous estimators useful theoretical point view 
consider weight function dy 
general weight function estimator previous estimators obtained particular election weight function ffl histogram fall bin width bin containing ffl kernel gamma gammax delta ffl orthogonal series 
local smoothness inherited pdf 
nonparametric density estimation techniques multivariate case methods section don higher dimensions different reasons 
example case multivariate histogram ffl due discontinuous nature difficult visualise dimensions 
ffl specify origin bin orientation bin length direction 
dimensions bins hypercubes volume marginal bin widths ffl possible draw meaningful contour diagrams bivariate histogram 
ffl small total number bins large compared sample size due curse dimensionality 
density estimation methods suffer degree curse dimensionality empty space phenomenon see section 
means ffl difficult estimate density enormous samples 
ffl density give false impression behaviour sample data sets 
methods outlined easily extended dimensions 
nearest neighbour estimation dk volume dimensional sphere radius see section 
generalised nearest neighbour estimation nd gamma kernel estimation multivariate kernel usually radially symmetric unimodal pdf 
estimator nh gamma dispersion data directions better data complicate kernel introducing vector smoothing parameters approximation sampling properties general weight function estimator section expanding unknown density taylor series find approximated expressions mean squared error mse mean integrated squared error mise defined appendix estimator 
table lists statistics smoothers order particular full expression 
decomposition shows bias variance trade small bias need small bandwidth penalising small variance nh large penalising 
words estimate need small bandwidth observations 
optimal bandwidth minimises mse mise 
table shows effect curse dimensionality multivariate kernels order dimension fixed component need sample size grows exponentially dimension 
difficult kernel smoothers small dimension 
smoother bias variance optimal bandwidth opt mse opt mise opt histogram nh gamma gamma kernel nh gamma gamma multivariate kernel nh gamma gamma nearest neighbours gamma table statistics smoothers order delta sufficiently large shown actual value statistic 
nasty effect curse dimensionality occurs data rank deficient due linear correlations common situation high dimensions proven case optimal bandwidth goes zero corresponding optimal mise goes infinity large sample size 
approximation properties kernel estimators proven symmetric kernels minimum mise kernel estimator reached opt fi ff fl gamma ff dt fi dt fl dt minimum mise opt fl gamma ff fi opt mise opt depend unknown density nonnegative kernels fi minimised epanechnikov kernel table suggests defining efficiency kernel eff remarkable efficiency kernels close table shows difference kernels basis mise considerations may taken account choosing kernel degree differentiability computational effort required 
fact finds parallel radial basis functions neural networks actual shape rbfs relatively unimportant 
depending way sample size pointwise global convergence probability uniform integration sense proven kernels satisfying general conditions 
method 
silverman proposes rule thumb choosing density estimation method 
choice kernel estimator 
practical drawback unable deal satisfactorily tails distribution main part 

second choice variable kernel estimator greater accuracy tails required 
regression smoothing contrarily case density estimation regression smoothing data paired distribution want model relationship 
learning supervised 
table compares density estimation regression smoothing 
smoothing data objective learning density unlabelled fx ae model distribution unsupervised regression paired ae theta model relationship supervised table density estimation vs regression smoothing 
multivariate regression problem consider multivariate regression problem ffl pairs vectors generated unknown models ffl multivariate response vectors 
independent explanatory variables predictors carriers 
gamma 
unknown smooth nonparametric functions 
ffl multivariate random variables zero mean independent assumed iid 
ffl construct estimator function data best approximate mean regression curve fy jx xg yg dy joint density marginal density predict new new independent vector 
parametric nonparametric regression density estimation types regression ffl parametric multiple regression functional form regression curve fixed parameters space optimum searched example linear regression 
approach valid model chosen curve correct difficult verify 
ffl nonparametric model free methods general assumptions regression surface 
nonparametric methods approaches local averaging smoothing kernel nearest neighbours splines estimate regression surface point average responses observations predictors neighbourhood formally xn weight function depending smoothing parameter sample fx explanatory variables 
desirable asymptotic properties requires samples high dimensional space 
successive refinement polynomial regression recursive partitioning hierarchy models increasing complexity degrees freedom formulated 
step model subsequent level hierarchy best fits data selected 
nonparametric regression techniques techniques analogous appendix appendix mainly 
kernel regression smoothing nadaraya watson estimator estimating joint density multiplicative kernel smoother obtain weight function xn gamma gammax delta properties ffl depends sample fx marginal density neighbourhood fixed 
ffl observations receive weight areas corresponding sparse 
ffl set 
ffl controls smoothness regression curve defined interpolation 
fyg 
nearest neighbour regression smoothing varying neighbourhoods variable ki nearest observations number neighbours regulates smoothing larger smoother estimate fyg step function matching jumping middle adjacent spline regression smoothing assume delta delta delta xn proved variational problem smoothing parameter min gamma unique solution xn cubic spline set cubic polynomials adjacent points satisfying boundary conditions gamma gamma gamma smoothing grows pure interpolation squares linear fit 
shape smoothing spline converges kernel gamma juj sin juj unfortunately theoretical difficulties extend analytically variational problem dimensions prevents spline regression smoothing 
friedman fast nonparametric variable bandwidth smoother provides piecewise interpolation 
ffl requires storage huge regression tables estimated values fg ridge function ffl derivative estimated order finite differences estimates fg constant unstable 
orthogonal series regression orthogonal series introduced section 
point view regression orthogonal polynomials advantages ffl provide smooth interpolation piecewise 
ffl fast accurate derivatives computed recursive relations polynomials derivatives 
ffl contrarily section huge regression tables needed 
projection pursuit regression see section 
partitioning methods regression trees partitioning methods cart id operate follows ffl partition input space regions data training typically hyperplanes parallel coordinate axes 
binary partitions common region represented leaf binary tree 
ffl fit different mapping region simplest case fit constant 
ffl optionally prune tree control complexity model 
generalised linear models multivariate regression problem see appendix data generalised linear model learns mapping form woe oe nonlinear linear oe gamma 
gamma 
gamma 
oe gamma 
woe linear mapping implemented linear single layer neural network biases see fig 
called generalised linear network 
xl input nonlinear oe yd basis functions output oe linear oe oe biases wk generalised linear network 
learning follows ffl supervised learning oe independent variables fx typically oe vector localised radial basis functions gaussians oe gamma kx gamma oe parameters oe determined 
ffl supervised learning full data criterion 
sum squared errors norm criterion min kt gamma woe assuming oe fixed reaches unique minimum value solution matrix equation phi phi phi phi phi ik theta phi oe ij oe theta kj theta 
computation phi phi pseudoinverse matrix phi phi performed time various standard numerical algorithms see usually faster gradient descent backpropagation 
generalised linear networks mlp universal approximators provided basis functions chosen appropriately 
number basis functions required obtained accuracy grows exponentially dimension space predictor variables 
phi phi singular positive definite cholesky decomposition somewhat slower singular value decomposition 
manifolds appendix briefly formalises concept dimensional manifold main idea keep mind manifold just subset dimension geometrical point view 
dimensional manifold subset degrees freedom described coordinates 
example restrict case vector subspaces vector subspace dimension described system linearly independent vectors basis projection vector subspace basis gives real numbers coordinates coordinate system basis 
needless say election coordinate system unique 
define formally previous ideas 
introduce naming convention ffl call manifold dimensional manifold ffl consider mapping differentiable iff continuous continuous derivatives orders 
ffl diffeomorphism differentiable mapping gamma 
open sets ae differentiable inverse gamma definition 
ae manifold iff condition holds exist open sets ae diffeomorphism gamma 
theta fy delta delta delta yn example ffl point manifold 
ffl dimensional vector subspace manifold 
ffl hollow sphere gamma manifold 
ffl open subset manifold 
shows examples manifolds 
manifold manifold examples manifolds 
manifolds expressed functional formula hollow unit sphere 
theorem helps find dimension cases 
theorem 
open subset gamma 
differentiable 
jacobian rank gamma gamma manifold example gamma gamma 
defined kxk gamma gamma 
theorem introduces concept coordinate system manifold 
theorem 
ae manifold iff condition holds mainly book 
exist open sets ae ae differentiable mapping gamma 


jacobian rank 
continuous inverse gamma gamma 
called coordinate system define coordinate neighbourhood illustrates point 
dimensional coordinate system manifold coordinate system manifold introduce manifolds boundaries 
definition 
ae manifold boundary iff condition condition hold exist open subsets diffeomorphism gamma 
theta fy delta delta delta yn th component equal 
definition separates manifold disjoint sets ffl boundary fx gamma manifold 
ffl rest gamma fx manifold 
gives examples manifolds boundaries 
manifold boundary manifold boundary examples manifolds boundary 
highlighted points belong boundary 
geometry high dimensional spaces geometry high dimensional spaces provides surprises 
fact say surprises usual intuitive low dimensional cases dimensions compared general asymptotic case higher dimensions 
consider euclidean space ffl volume hypersphere radius dimension dependent constant gamma gamma gamma function 
ffl volume hypercube side dimension dependent constant volumes depend exponentially linear size object constants different 
interesting consequence distortion space section shows 
consequences limit high dimensions ffl sphere inscribed hypercube ratio volume hypersphere volume hypercube gamma gamma gamma gamma gamma 
increasing dimension volume hypercube concentrates corners centre important 
table show volumes ratio dimensions 
ffl thin shell consider volume concentric shells respective radii gamma ffl ffl small 
ratio gamma gammaffl gamma gamma ffl gamma gamma gamma gamma 
virtually content hypersphere concentrated close surface gamma dimensional manifold see appendix 
data distributed uniformly hypersphere hypercube data fall near boundary edges volume 
example illustrates important aspect curse dimensionality introduced section 
illustrates point ffl 
ffl tail probability multivariate normal preceding examples clear spherical neighbourhoods data distributed uniformly hypercube high dimensions empty 
case standard dimensional normal distribution eq 
equiprobable contours hyperspheres 
probability point contour density ffl times value mode equivalently inside hypersphere radius gamma ln ffl pr theta kxk gamma ln ffl pr theta gamma ln ffl xd distributed standard normal univariate standard normal kxk distributed distribution degrees freedom 
equation gives probability random point fall tails fall medium high density region 
shows probability ffl radius standard deviations dimensions notice probability mass multivariate normal begins rapid migration extreme tails 
high dimensions entire sample tails 
appendix greatly chapter scott book 
ffl diagonals hyperspace consider hypercube gamma diagonal vectors centre corner denoted vectors form sigma sigma sigma angle diagonal vector coordinate axis cos sigma gamma gamma gamma gamma 
diagonals nearly orthogonal coordinate axes large pairwise scatter diagrams essentially project multivariate data dimensional coordinate planes 
data cluster lying near diagonal hyperspace mapped origin paired scatterplot cluster coordinate axis visible plot 
choice coordinate systems critical data analysis dimensional intuition valuable continuing higher dimensions 
delta delta delta delta delta delta delta delta delta delta delta delta table volumes unit hypersphere hypercube 
pr multivariate normal point tails relative thin shell dimension dependence geometric quantities dimension course natural numbers meaningful 
see main text explanation 
multidimensional scaling multidimensional scaling mds set mathematical techniques enable researcher uncover hidden structure data 
applications psychology sociology anthropology economy educational research suppose set objects auditory stimuli measure similarity objects known 
measure called proximity number indicates similar dissimilar objects perceived 
obtained different ways asking people judge psychological closeness stimulus objects 
mds draw spatial representation map object represented point distances points resemble faithfully possible original similarity information larger dissimilarity objects farther apart spatial representation 
geometrical configuration points reflects hidden structure data easier understand 
consider example 
confusions auditory morse code signals collected rothkopf 
signal consists sequence dots dashes 
subjects know morse code listened pair signals produced fixed rapid rate machine separated quiet period seconds required state signals heard different 
number table percentage roughly observers responded row signal followed column signal 
matrix roughly symmetric diagonal entries large diagonal entries small expected contains proximities data 
delta delta delta delta delta delta delta delta delta table rothkopf data similarities morse code symbols 
left shows result applying mds proximities table shepard dimensional map 
circles represent points labelled corresponding morse code 
case mds clearly shows data governed sort length parameter number components signal individual numbers dots dashes 
way mds formally assume input data roughly symmetric matrix theta containing proximities delta ffi ij 
dimensional map output data set points fx referred unimportant coordinate system distances ij typically euclidean close possible function corresponding proximities ffi ij 
mds called metric linear called nonmetric 
plotted pairs ffi ij ij scatter shepard diagram plots distance dimensional space versus proximities 
proximities dissimilarities dissimilar objects large proximity value rising pattern falling 
example morse code case proximities similarities corresponding falling scatter diagram shown fig 
right 
computational procedure follows define objective function stress minimised stress delta ffi ij gamma ij scale factor scale factor typically ij find function produces minimum stress stress delta min stress delta appendix draws largely kruskal book 
way mds matrices delta ffi ij correspond different measures proximities different times different subjects 


number components dashes dots interpoint distance percent judgments left map resulting morse code similarities shepard interpretation 
right scatter diagram corresponding map 
determined optimal map stress delta min stress delta solution map freely translated rotated appear way changing value stress actual coordinate system map meaningless 
degeneracy happen objects natural clustering dissimilarities objects different clusters larger dissimilarities cluster 
case points single cluster converge single location stress converge scatter diagram staircase 
objects associated value linear regression performed generated map help interpret data selection map dimension obviously larger dimension map smaller stress keep small possible map visualised necessary dimensions avoid false interpretations 
assure adequate degree statistical stability dimension arbitrarily large sample size 
convenient rule thumb number significant pairs objects twice number parameters estimated gamma il large small dimension map give misleading view data 
example points apparently clustered map lie far apart 
simple way embed information original data map draw line pair objects proximity exceeds threshold value presence long crossing lines indicate discrepancy closeness data closeness space 
clusters valid consonant lines points cluster connected poorly connected outside cluster 
lines connect points nonlinear shape suggests curvilinear dimension give reasonable description data 
called horseshoe phenomenon 
horseshoe phenomenon 
problems mds ffl difficult select appropriate dimension map try 
ffl mds better job representing large distances global structure small ones local structure 
ffl contrarily principal component analysis mds obtain gamma dimensional map dimensional dropping coordinate general linearly projecting direction 
asimov grand tour tool viewing multidimensional data siam sci 
stat 
comput pp 

baldi hornik neural networks principal component analysis learning examples local minima neural networks pp 

bellman adaptive control processes guided tour princeton university press princeton 
bienenstock cooper munro theory development neuron selectivity orientation specificity binocular interaction visual cortex neurosci pp 

bishop neural networks pattern recognition oxford university press new york oxford 
bishop svens en williams em optimization latent variable density models advances neural information processing systems touretzky mozer hasselmo eds vol 
mit press cambridge ma 
gtm principled alternative self organising map tech 
rep ncrg neural computing research group aston university apr 
accepted oral presentation nips submitted neural computation 
magnification factors gtm algorithm tech 
rep neural computing research group aston university 
bourlard kamp autoassociation multilayer perceptrons singular value decomposition biological cybernetics pp 

breiman friedman olshen stone classification regression trees wadsworth belmont calif 
carreira compression neural networks feature extraction application human recognition ear images master thesis facultad de inform atica technical university madrid sept 
cheng titterington neural networks review statistical perspective statistical science pp 
comments pp 

chernoff faces represent points dimensional space graphically amer 
stat 
assoc pp 

cook buja cabrera projection pursuit indexes orthonormal function expansions journal computational graphical statistics pp 

cottrell munro zipser image compression backpropagation demonstration extensional programming advances cognitive science sharkey ed vol 
norwood nj 
cover thomas elements information theory wiley series telecommunications john wiley sons new york london sydney 
cowan tesauro alspector eds advances neural information processing systems vol 
morgan kaufmann san mateo 
cybenko approximation superpositions sigmoidal function math 
control signals sys pp 

dayan hinton varieties helmholtz machine neural networks pp 

dayan hinton neal zemel helmholtz machine neural computation pp 

dayan zemel competition multiple cause models neural computation pp 

curvilinear component analysis self organizing neural network nonlinear mapping data sets ieee trans 
neural networks 
appear 
dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society pp 

diaconis freedman asymptotics graphical projection pursuit annals statistics pp 

diaconis shahshahani nonlinear functions linear combinations siam sci 
stat 
comput pp 


kung principal component neural networks 
theory applications wiley series adaptive learning systems signal processing communications control john wiley sons new york london sydney 
principal component analysis sage university series quantitative applications social sciences sage publications beverly hills 
marriott criteria projection pursuit statistics computing pp 

everitt latent variable models monographs statistics applied probability chapman hall london new york 
fahlman lebiere cascade correlation learning architecture advances neural information processing systems touretzky ed vol 
morgan kaufmann san mateo pp 

fleming cottrell categorization faces unsupervised feature extraction proc 
int 
conf 
neural networks vol 
ii pp 

ak adaptive network optimal linear feature extraction proc 
int 
conf 
neural networks vol 
pp 

friedman variable span smoother tech 
rep stanford university 
exploratory projection pursuit amer 
stat 
assoc pp 

multivariate adaptive regression splines annals statistics pp 
comments pp 

friedman stuetzle projection pursuit regression amer 
stat 
assoc pp 

friedman stuetzle schroeder projection pursuit density estimation amer 
stat 
assoc pp 

friedman tukey projection pursuit algorithm exploratory data analysis ieee trans 
computers pp 

fritzke growing cell structures self organizing network unsupervised supervised learning neural networks pp 

competitive learning methods draft institute neural computation bochum june 
gray vector quantization ieee assp magazine pp 

hall polynomial projection indices exploratory projection pursuit annals statistics pp 

gibbon epg data reduction methods implications studies lingual coarticulation phonetics pp 

jones knight calder new developments state art report clinical linguistics phonetics pp 

smoothing techniques implementations springer series statistics springer verlag berlin 
hastie stuetzle principal curves amer 
stat 
assoc pp 

hastie tibshirani generalized additive models statistical science pp 
comments 
generalized additive models applications amer 
stat 
assoc pp 

generalized additive models monographs statistics applied probability chapman hall london new york 
hornik approximation capabilities multilayer feedforward networks neural networks pp 

hornik stinchcombe white multilayer feedforward networks universal approximators neural networks pp 

universal approximation unknown mapping derivatives multilayer feedforward networks neural networks pp 

huber projection pursuit annals statistics pp 
comments pp 


hwang 
lay martin regression modeling back propagation projection pursuit learning ieee trans 
neural networks pp 


hwang 

lay 
jou cascade correlation learning projection pursuit learning perspective ieee trans 
neural networks pp 

intrator localized exploratory projection pursuit proceedings rd conference interface computer science statistics seattle 
feature extraction unsupervised neural network neural computation pp 

intrator combining exploratory projection pursuit projection pursuit regression application neural networks neural computation pp 

intrator cooper objective function formulation bcm theory visual cortical plasticity statistical connections stability conditions neural networks pp 

intrator reisfeld yeshurun face recognition hybrid supervised unsupervised neural network tech 
rep dept computer science tel aviv university june 
jackson user guide principal components wiley series probability mathematical statistics john wiley sons new york london sydney 
jolliffe principal component analysis springer series statistics springer verlag berlin 
jones conjecture huber concerning convergence projection pursuit regression annals statistics pp 

jones projection pursuit algorithm exploratory data analysis phd thesis university bath 
jones sibson projection pursuit journal royal statistical society pp 
comments pp 

kaiser criterion analytic rotation factor analysis psychometrika pp 

leen fast non linear dimension reduction cowan pp 

cook kernel projection pursuit indices xgobi tech 
rep berlin error 
kohonen self organizing map proc 
ieee pp 

krogh brown mian sj haussler hidden markov models computational biology molecular biology pp 

kruskal wish multidimensional scaling sage university series quantitative applications social sciences sage publications beverly hills 
kung adaptive principal component extraction apex applications ieee trans 
signal processing pp 

discriminant functions taxonomy biometrics pp 

mackay bayesian neural networks density networks nuclear instruments methods physics research pp 

mardia kent bibby multivariate analysis probability mathematical statistics series academic press new york 
neal bayesian learning neural networks springer series statistics springer verlag berlin 
oja principal components minor components linear neural networks neural networks pp 

effective dimensional projection pursuit algorithm communications statistics simulation computation pp 

projection pursuit exploratory data analysis computational statistics data analysis pp 

tools dimensional exploratory projection pursuit journal computational graphical statistics pp 

press teukolsky vetterling flannery numerical recipes art scientific computing cambridge university press cambridge second ed 
quinlan induction decision trees machine learning pp 

smoothing spline functions numerische mathematik pp 

ripley neural networks related methods classification journal royal statistical society pp 
comments pp 

robbins monro stochastic approximation method annals mathematical statistics pp 

rothkopf measure stimulus similarity errors paired associate learning tasks experimental psychology pp 

sammon jr nonlinear mapping data structure analysis ieee trans 
computers pp 

sanger optimal unsupervised learning single layer linear feedforward neural network neural networks pp 

scott multivariate density estimation 
theory practice visualization wiley series probability mathematical statistics john wiley sons new york london sydney 
scott thompson probability density estimation higher dimensions computer science statistics proceedings fifteenth symposium interface gentle ed amsterdam new york oxford north holland elsevier science publishers pp 

shepard analysis proximities technique study information processing man human factors pp 

silverman aspects spline smoothing approach non parametric regression curve fitting journal royal statistical society pp 

density estimation statistics data analysis monographs statistics applied probability chapman hall london new york 
calculus manifolds modern approach classical theorems advanced calculus addison wesley 
cook buja user manual xgobi dynamic graphics program data analysis implemented window system release bellcore nov 
hyperparameter selection self organizing maps neural computation 
appear 
topology selection self organizing maps network computation neural systems pp 

wegman data analysis parallel coordinates amer 
stat 
assoc pp 

wilkinson algebraic eigenvalue problem oxford university press new york oxford 
zemel hinton developing population codes minimizing description length cowan pp 

zhao atkeson implementing projection pursuit learning ieee trans :10.1.1.17.6536
neural networks pp 

