sequential minimal optimization fast algorithm training support vector machines john platt microsoft research microsoft com technical report msr tr april john platt proposes new algorithm training support vector machines sequential minimal optimization smo 
training support vector machine requires solution large quadratic programming qp optimization problem 
smo breaks large qp problem series smallest possible qp problems 
small qp problems solved analytically avoids time consuming numerical qp optimization inner loop 
amount memory required smo linear training set size allows smo handle large training sets 
matrix computation avoided smo scales linear quadratic training set size various test problems standard chunking svm algorithm scales linear cubic training set size 
smo computation time dominated svm evaluation smo fastest linear svms sparse data sets 
realworld sparse data sets smo times faster chunking algorithm 

years surge interest support vector machines svms :10.1.1.117.3731:10.1.1.117.3731:10.1.1.117.3731
svms empirically shown give generalization performance wide variety problems handwritten character recognition face detection pedestrian detection text categorization 
svms limited small group researchers 
possible reason training algorithms svms slow especially large problems 
explanation svm training algorithms complex subtle difficult average engineer implement 
describes new svm learning algorithm conceptually simple easy implement generally faster better scaling properties difficult svm problems standard svm training algorithm 
new svm learning algorithm called sequential minimal optimization smo 
previous svm learning algorithms numerical quadratic programming qp inner loop smo uses analytic qp step 
provides overview svms review current svm training algorithms 
smo algorithm detail including solution analytic qp step heuristics choosing variables optimize inner loop description set threshold svm optimizations special cases pseudo code algorithm relationship smo algorithms 
smo tested real world data sets artificial data sets 
presents results timing smo versus standard chunking algorithm data sets presents timings 
appendix describes derivation analytic optimization 
overview support vector machines negative examples space possible inputs vladimir vapnik invented support vector machines 
simplest linear form svm hyperplane separates set positive examples set negative examples maximum margin see 
linear case margin defined distance hyperplane nearest positive negative examples 
formula output linear svm normal vector hyperplane input vector 
separating hyperplane plane 
nearest points lie planes 
margin 
maximizing margin expressed optimization problem min subject wb linear support vector machine positive examples maximize distances nearest points xi ith training example yi correct output svm ith training example :10.1.1.117.3731:10.1.1.117.3731:10.1.1.117.3731:10.1.1.117.3731
value yi positive examples class negative examples 
lagrangian optimization problem converted dual form qp problem objective function solely dependent set lagrange multipliers min min yy number training examples subject inequality constraints linear equality constraint yi 
relationship lagrange multiplier training example 
lagrange multipliers determined normal vector threshold derived lagrange multipliers 
computed equation training data amount computation required evaluate linear svm constant number non zero support vectors 
course data sets linearly separable 
may hyperplane splits positive examples negative examples 
formulation non separable case correspond infinite solution 
cortes vapnik suggested modification original optimization statement allows penalizes failure example reach correct margin 
modification min yi xi subject wb slack variables permit margin failure parameter trades wide margin small number margin failures :10.1.1.117.3731
new optimization problem transformed dual form simply changes constraint box constraint variables appear dual formulation 

svms generalized non linear classifiers 
output non linear svm explicitly computed lagrange multipliers kernel function measures similarity distance input vector stored training vector examples include gaussians polynomials neural network non linearities :10.1.1.117.3731:10.1.1.117.3731:10.1.1.117.3731:10.1.1.117.3731
linear equation linear svm recovered 
lagrange multipliers computed quadratic program 
non linearities alter quadratic form dual objective function quadratic min min 
qp problem equation qp problem smo algorithm solve 
order qp problem positive definite kernel function obey mercer conditions :10.1.1.117.3731:10.1.1.117.3731:10.1.1.117.3731:10.1.1.117.3731
karush kuhn tucker kkt conditions necessary sufficient conditions optimal point positive definite qp problem 
kkt conditions qp problem particularly simple 
qp problem solved yu yu yu 
ui output svm ith training example 
notice kkt conditions evaluated example time useful construction smo algorithm 
previous methods training support vector machines due immense size qp problem arises svms easily solved standard qp techniques 
quadratic form involves matrix number elements equal square number training examples 
matrix fit megabytes training examples 
vapnik describes method solve svm qp known chunking chunking algorithm uses fact value quadratic form remove rows columns matrix corresponds zero lagrange multipliers 
large qp problem broken series smaller qp problems ultimate goal identify non zero lagrange multipliers discard zero lagrange multipliers 
step chunking solves qp problem consists examples non zero lagrange multiplier step worst examples violate kkt conditions value see :10.1.1.117.3731:10.1.1.117.3731
fewer examples violate kkt conditions step violating examples added 
qp sub problem initialized results previous sub problem 
step entire set non zero lagrange multipliers identified step solves large qp problem 
chunking seriously reduces size matrix number training examples squared approximately number non zero lagrange multipliers squared 
chunking handle large scale training problems reduced matrix fit memory 
chunking osuna smo 
alternative methods training svms chunking osuna algorithm smo 
method steps illustrated 
horizontal thin line step represents training set thick boxes represent lagrange multipliers optimized step 
chunking fixed number examples added step zero lagrange multipliers discarded step 
number examples trained step tends grow 
osuna algorithm fixed number examples optimized step number examples added discarded problem step 
smo examples analytically optimized step step fast 
osuna proved theorem suggests new set qp algorithms svms 
theorem proves large qp problem broken series smaller qp sub problems 
long example violates kkt conditions added examples previous sub problem step reduce objective function maintain feasible point obeys constraints 
sequence qp sub problems add guaranteed converge 
notice chunking algorithm obeys conditions theorem converge 
osuna suggests keeping constant size matrix qp sub problem implies adding deleting number examples step see 
constant size matrix allow training arbitrarily sized data sets 
algorithm osuna suggests adding example subtracting example step 
clearly inefficient entire numerical qp optimization step cause training example obey kkt conditions 
practice researchers add subtract multiple examples unpublished heuristics 
event numerical qp solver required methods 
numerical qp notoriously tricky get right numerical precision issues need addressed 

sequential minimal optimization sequential minimal optimization smo simple algorithm quickly solve svm qp problem extra matrix storage numerical qp optimization steps 
smo decomposes qp problem qp sub problems osuna theorem ensure convergence 
previous methods smo chooses solve smallest possible optimization problem step 
standard svm qp problem smallest possible optimization problem involves lagrange multipliers lagrange multipliers obey linear equality constraint 
step smo chooses lagrange multipliers jointly optimize finds optimal values multipliers updates svm reflect new optimal values see 
advantage smo lies fact solving lagrange multipliers done analytically 
numerical qp optimization avoided entirely 
inner loop algorithm expressed short amount code invoking entire qp library routine 
optimization sub problems solved course algorithm sub problem fast qp problem solved quickly 
addition smo requires extra matrix storage 
large svm training problems fit inside memory ordinary personal computer workstation 
matrix algorithms smo susceptible numerical precision problems 
components smo analytic method solving lagrange multipliers heuristic choosing multipliers optimize 

lagrange multipliers fulfill constraints full problem 
inequality constraints cause lagrange multipliers lie box 
linear equality constraint causes lie diagonal line 
step smo find optimum objective function diagonal line segment 
solving lagrange multipliers order solve lagrange multipliers smo computes constraints multipliers solves constrained minimum 
convenience quantities refer multiplier subscript quantities refer second multiplier subscript 
multipliers constraints easily displayed dimensions see 
bound constraints cause lagrange multipliers lie box linear equality constraint causes lagrange multipliers lie diagonal line 
constrained minimum objective function lie diagonal line segment shown 
constraint explains minimum number lagrange multipliers optimized smo optimized multiplier fulfill linear equality constraint step 
ends diagonal line segment expressed quite simply 
loss generality algorithm computes second lagrange multiplier computes ends diagonal line segment terms 
target equal target bounds apply max min 
target equals target bounds apply max min 
second derivative objective function diagonal line expressed kx kx kx 
normal circumstances objective function positive definite minimum direction linear equality constraint greater zero 
case smo computes minimum direction constraint new ei ui yi error ith training example 
step constrained minimum clipping unconstrained minimum ends line segment new clipped new new new new value computed new clipped new new clipped 
unusual circumstances positive 
negative occur kernel obey mercer condition cause objective function indefinite 
zero occur correct kernel training example input vector event smo positive case objective function evaluated line segment lf lf lk lk hf hf 
smo move lagrange multipliers point lowest value objective function 
objective function ends small roundoff error kernel obeys mercer conditions joint minimization progress 
scenario described 
heuristics choosing multipliers optimize long smo optimizes alters lagrange multipliers step lagrange multipliers violated kkt conditions step step decrease objective function osuna theorem 
convergence guaranteed 
order speed convergence smo uses heuristics choose lagrange multipliers jointly optimize 
separate choice heuristics lagrange multiplier second 
choice heuristic provides outer loop smo algorithm 
outer loop iterates entire training set determining example violates kkt conditions 
example violates kkt conditions eligible optimization 
pass entire training set outer loop iterates examples lagrange multipliers non bound examples 
example checked kkt conditions violating examples eligible optimization 
outer loop repeated passes non bound examples non bound examples obey kkt conditions 
outer loop goes back iterates entire training set 
outer loop keeps alternating single passes entire training set multiple passes non bound subset entire training set obeys kkt conditions algorithm terminates 
choice heuristic concentrates cpu time examples violate kkt conditions non bound subset 
smo algorithm progresses examples bounds stay bounds examples bounds move examples optimized 
smo algorithm iterate subset subset self consistent smo scan entire data set search bound examples kkt violated due optimizing non bound subset 
notice kkt conditions checked fulfillment 
typically set recognition systems typically need kkt conditions fulfilled high accuracy acceptable examples positive margin outputs 
smo algorithm svm algorithms converge quickly required produce high accuracy output 
lagrange multiplier chosen smo chooses second lagrange multiplier maximize size step taken joint optimization 
evaluating kernel function time consuming smo approximates step size absolute value numerator equation smo keeps cached error value non bound example training set chooses error approximately maximize step size 
positive smo chooses example minimum error 
negative smo chooses example maximum error 
unusual circumstances smo positive progress second choice heuristic described 
example positive progress second training examples share identical input vectors causes objective function semi definite 
case smo uses hierarchy second choice heuristics finds pair lagrange multipliers positive progress 
positive progress determined making non zero step size joint optimization lagrange multipliers hierarchy second choice heuristics consists 
heuristic positive progress smo starts iterating non bound examples searching second example positive progress 
non bound examples positive progress smo starts iterating entire training set example positive progress 
iteration non bound examples iteration entire training set started random locations order bias smo examples training set 
extremely degenerate circumstances examples adequate second example 
happens example skipped smo continues chosen example 
computing threshold threshold re computed step kkt conditions fulfilled optimized examples 
threshold valid new bounds forces output svm input new new clipped threshold valid new bounds forces output svm input new new clipped valid equal 
new lagrange multipliers bound equal interval thresholds consistent kkt conditions 
smo chooses threshold halfway 
optimization linear svms compute linear svm single weight vector needs stored training examples correspond non zero lagrange multipliers 
joint optimization succeeds stored weight vector needs updated reflect new lagrange multiplier values 
weight vector update easy due linearity svm rnew new new clipped 
code details pseudo code describes entire smo algorithm target desired output vector point training point matrix procedure return alph lagrange multiplier target svm output point check error cache compute equations return kernel point point kernel point point kernel point point eta eta alph eta objective function objective function eps eps alph alph eps alph eps return alph alph update threshold reflect change lagrange multipliers update weight vector reflect change svm linear update error cache new lagrange multipliers store alpha array store alpha array return procedure target alph lagrange multiplier svm output point check error cache tol alph tol alph number non zero non alpha result second choice heuristic section return loop non zero non alpha starting random point identity current alpha return loop possible starting random point loop variable return return main routine loop training examples loop examples alpha relationship previous algorithms smo algorithm related previous svm optimization algorithms 
smo algorithm considered special case osuna algorithm size optimization lagrange multipliers replaced step new multipliers chosen heuristics 
smo algorithm closely related family optimization algorithms called bregman methods row action methods 
methods solve convex programming problems linear constraints 
iterative methods step projects current primal point constraint 
unmodified bregman method solve qp problem directly threshold svm creates linear equality constraint dual problem 
constraint projected step linear equality constraint violated 
technical terms primal problem minimizing norm weight vector combined space possible weight vectors thresholds produces bregman projection unique minimum 
interesting consider svm threshold held fixed zero solved 
fixed threshold svm linear equality constraint 
lagrange multiplier need updated time row action method 
unfortunately traditional bregman method applicable svms due slack variables equation 
presence slack variables causes bregman non unique combined space weight vectors slack variables fortunately smo modified solve fixed threshold svms 
smo update individual lagrange multipliers minimum corresponding dimension 
update rule new ye kx update equation forces output svm similar bregman methods hildreth qp method 
new computed clipped interval previous methods 
choice lagrange multiplier optimize choice heuristic described section 
fixed threshold smo linear svm similar concept perceptron relaxation rule output perceptron adjusted error output exactly lies margin 
fixed threshold smo algorithm reduce proportion training input weight vector order maximize margin 
relaxation rule constantly increases amount training input weight vector maximum margin 
fixed threshold smo gaussian kernels related resource allocating network ran algorithm 
ran detects certain kinds errors allocate kernel exactly fix error 
smo perform similarly 
smo svm adjust heights kernels maximize margin feature space ran simply lms adjust heights weights kernels 
benchmarking smo smo algorithm tested standard chunking svm learning algorithm series benchmarks 
algorithms written microsoft visual compiler 
algorithms run unloaded mhz pentium ii processor running windows nt 
algorithms written exploit sparseness input vector 
specifically kernel functions rely dot products inner loop 
input sparse vector input stored sparse array dot product merely iterate non zero inputs accumulating non zero inputs multiplied corresponding weights 
input sparse binary vector position input stored dot product sum weights corresponding position input 
chunking algorithm uses projected conjugate gradient algorithm qp solver suggested burges :10.1.1.117.3731:10.1.1.117.3731:10.1.1.117.3731:10.1.1.117.3731
order ensure chunking algorithm fair benchmark burges compared speed chunking code mhz pentium ii running solaris speed benchmark chunking code sparse dot product code turned 
speeds comparable indicates benchmark chunking code reasonable benchmark 
ensuring chunking code smo code attain accuracy takes care 
smo code chunking code identify example violating kkt condition output away correct value half space 
threshold chosen insignificant error classification tasks 
projected conjugate gradient code stopping threshold describes minimum relative improvement objective function step :10.1.1.117.3731:10.1.1.117.3731:10.1.1.117.3731
projected conjugate gradient takes step relative improvement smaller minimum conjugate gradient code terminates chunking step taken 
burges recommends constant minimum :10.1.1.117.3731:10.1.1.117.3731:10.1.1.117.3731
experiments stopping projected conjugate gradient accuracy left kkt violations larger especially large scale problems 
benchmark chunking algorithm heuristic set conjugate gradient stopping threshold 
threshold starts chunking step output computed examples lagrange multipliers bound 
outputs computed order compute value threshold see :10.1.1.117.3731:10.1.1.117.3731
example suggests proposed threshold 
largest proposed threshold smallest proposed threshold kkt conditions possibly fulfilled starting chunk conjugate gradient threshold decreased factor 
heuristic optimize speed conjugate gradient high precision difficult problems 
tests described threshold stayed smallest threshold occurred chunking largest web page classification problem 
smo algorithm tested income prediction task web page classification task different artificial data sets 
times listed tables cpu seconds 
income prediction data set test smo speed uci adult data set available ftp ftp ics uci edu pub machine learning databases adult 
svm attributes census form household 
task svm predict household income greater 
attributes categorical continuous 
ease experimentation continuous attributes discretized yielded total binary attributes true 
examples adult training set 
different svms trained problem linear svm radial basis function svm gaussian kernels variance 
variance chosen minimize error rate validation set 
limiting value chosen linear svm rbf svm 
limiting value chosen minimize error validation set 
timing performance smo algorithm versus chunking algorithm linear svm adult data set shown table training set size smo time chunking time number non bound number bound support vectors support vectors training set size varied random subsets full training set 
subsets nested 
entries chunking time column matrices large fit megabytes timed due memory thrashing 
number number bound support vectors determined smo chunking results vary small amount due tolerance inaccuracies kkt conditions 
fitting line log log plot training time versus training set size empirical scaling smo chunking derived 
smo training time scales chunking scales smo improves empirical scaling problem order 
timing performance smo chunking gaussian svm shown training set size smo time chunking time number non bound number bound support vectors support vectors smo algorithm slower non linear svms linear svms time dominated evaluation svm 
smo training time scales chunking scales smo scaling roughly order faster chunking 
income prediction test indicates real world sparse problems support vectors bound smo faster chunking 
classifying web pages second test smo text categorization classifying web page belongs category 
training set consisted web pages sparse binary keyword attributes extracted web page 
different svms tried problem linear svm non linear gaussian svm variance 
value linear svm chosen value non linear svm chosen 
parameters chosen maximize performance validation set 
timings smo versus chunking linear svm shown table training set size smo time chunking time number non bound number bound support vectors support vectors linear svm data set smo training time scales chunking scales experiment situation smo superior chunking computation time 
timings smo versus chunking non linear svm shown table training set size smo time chunking time number non bound number bound support vectors support vectors non linear svm data set smo training time scales chunking scales case scaling smo somewhat better chunking smo factor times faster chunking 
non linear test shows smo faster chunking number non bound support vectors large input data set sparse 
artificial data sets smo tested artificially generated data sets explore performance smo extreme scenarios 
artificial data set perfectly linearly separable data set 
input data random binary dimensional vectors fraction inputs 
dimensional weight vector generated randomly 
dot product weight input point greater positive label assigned input point 
dot product negative label assigned 
dot product lies point discarded 
linear svm fit data set 
linearly separable data set simplest possible problem linear svm 
surprisingly scaling training set size excellent smo chunking 
running times shown table training set size smo time chunking time number non bound number bound support vectors support vectors smo running time scales slightly better scaling chunking easy sparse problem chunking smo generally comparable 
algorithms trained set 
chunk size chunking set 
acceleration smo algorithm chunking algorithm due sparse dot product code measured easy data set 
data set tested sparse dot product code 
case non sparse experiment input point stored dimensional vector floats 
result sparse non sparse experiment shown table training set size smo time smo time chunking time chunking time sparse non sparse sparse non sparse smo sparse data structure speeds code factor shows evaluation time svm totally dominates smo computation time 
sparse dot product code speeds chunking factor approximately shows evaluation numerical qp steps dominates chunking computation 
linearly separable case absolutely lagrange multipliers bound worst case smo 
poor performance non sparse smo versus non sparse chunking experiment considered worst case 
sparse versus non sparse experiment shows part superiority smo chunking comes exploitation sparse dot product code 
fortunately real world problems sparse input 
addition real word data sets described section section quantized fuzzy membership encoded problems sparse 
optical character recognition handwritten character recognition wavelet transform coefficients natural images tend naturally expressed sparse data 
second artificial data set stands stark contrast easy data set 
second set generated random dimensional binary input points random output labels 
svms fitting pure noise 
value set problem fundamentally unsolvable 
results smo chunking applied linear svm shown training set size smo time chunking time number non bound number bound support vectors support vectors scaling smo chunking higher second data set 
reflects difficulty problem 
smo computation time scales chunking computation time scales second data set shows smo excels support vectors bound 
determine increase speed caused sparse dot product code smo chunking tested sparse dot product code training set size smo time smo time chunking time chunking time sparse non sparse sparse non sparse linear svm case sparse dot product code sped smo factor chunking sped minimally 
experiment smo faster chunking data 
second data set tested gaussian svms variance 
value set 
results gaussian svms tables training set size smo time chunking time number non bound number bound support vectors support vectors training set size smo time smo time chunking time chunking time sparse non sparse sparse non sparse gaussian svm fit pure noise smo computation time scales chunking computation time scales pure noise case yields worst scaling far smo superior chunking order scaling 
total run time smo superior chunking applied non sparse data 
sparsity input data yields speed approximately factor smo non linear case indicates dot product speed dominating smo computation time non linear svms smo improved training algorithm svms 
svm training algorithms smo breaks large qp problem series smaller qp problems 
algorithms smo utilizes smallest possible qp problems solved quickly analytically generally improving scaling computation time significantly 
smo tested real world problems artificial problems 
tests deduced smo user easy access quadratic programming package wish tune qp package 
smo svms lagrange multipliers bound 
smo performs linear svms smo computation time dominated svm evaluation evaluation linear svm expressed single dot product sum linear kernels 
smo performs svms sparse inputs non linear svms kernel computation time reduced directly speeding smo 
chunking spends majority time qp code exploit linearity svm sparseness input data 
smo perform large problems scaling training set size better chunking test problems tried far 
various test sets training time smo empirically scales training time chunking scales scaling smo order better chunking 
real world test sets smo factor times faster linear svms factor times faster non linear svms 
ease better scaling training set size smo strong candidate standard svm training algorithm 
benchmarking experiments qp techniques best osuna heuristics needed final drawn 
lisa assistance preparation text 
chris burges running data set projected conjugate gradient code 
leonid pointing similarity smo bregman methods 

bengio lecun henderson globally trained handwritten word recognizer spatial representation convolutional neural networks hidden markov models advances neural information processing systems cowan tesauro alspector eds 

boser guyon vapnik training algorithm optimal margin classifiers fifth annual workshop computational learning theory acm 

bregman relaxation method finding common point convex sets application solution problems convex programming ussr computational mathematics mathematical physics 

burges tutorial support vector machines pattern recognition submitted data mining knowledge discovery svm research com html 

censor row action methods huge sparse systems applications siam review 

censor lent iterative row action method interval convex programming optimization theory applications 

cortes vapnik support vector networks machine learning 

duda hart pattern classification scene analysis john wiley sons 

joachims text categorization support vector machines ls viii technical report university dortmund ftp ftp ai informatik de pub reports report ps 

hildreth quadratic programming procedure naval research logistics quarterly 

gill murray wright practical optimization academic press 

lecun jackel bottou cortes denker drucker guyon muller sackinger simard vapnik learning algorithms classification comparison handwritten digit recognition neural networks statistical mechanics perspective oh kwon cho 
ed world scientific 

mallat wavelet tour signal processing academic press 

oren sinha osuna poggio pedestrian detection wavelet templates proc 
computer vision pattern recognition 

osuna freund girosi training support vector machines application face detection proc 
computer vision pattern recognition 

osuna freund girosi improved training algorithm support vector machines proc 
ieee 

osuna personal communication 

platt resource allocating network function interpolation neural computation 

vapnik estimation dependences empirical data springer verlag 

vapnik nature statistical learning theory springer verlag 
appendix derivation example minimization step smo optimize lagrange multipliers 
loss generality multipliers 
objective function equation written sk constant ij ij starred variables indicate values previous iteration 
constant terms depend 
step find minimum line defined linear equality constraint 
linear equality constraint expressed 
objective function linear equality constraint expressed terms sk constant 
extremum objective function sk sk 
second derivative positive usual case minimum expressed 
expanding equations yields 
algebra yields equation 

