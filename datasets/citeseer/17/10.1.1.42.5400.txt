incorporating invariances support vector learning machines bernhard scholkopf chris burges vladimir vapnik max planck institut fur kybernetik tubingen germany mail bs mpg de bell laboratories holmdel nj usa 
developed support vector learning machines achieve high generalization ability minimizing bound expected test error far existed way adding knowledge invariances classification problem hand 
method incorporating prior knowledge transformation invariances applying transformations support vectors training examples critical determining classification boundary 
incorporating invariances von der malsburg von seelen sendhoff eds artificial neural networks icann 
springer lecture notes computer science vol 
berlin applications learning procedures prior knowledge properties function learned available review see abu mostafa 
instance certain transformations input known leave function values unchanged 
different ways exploiting knowledge knowledge directly incorporated algorithm generate artificial training examples virtual examples transforming training examples accordingly 
case additional term error function force learning machine construct function desired invariances simard alternatively invariance achieved appropriate distance measure pattern space simard le cun denker 
akin changing representation data mapping suitable space approach pursued instance rubinstein vetter poggio 
second case hoped sufficient time learning machine extract invariances artificially enlarged training data 
contains illustrations different approaches 
simard 
compare techniques find considered problem learning function plateaus function values locally invariant training artificially enlarged data set significantly slower due correlations artificial data increase training set size 
moving real world applications factor important 
size training set multiplied number desired invariances generating corresponding number artificial examples training pattern resulting training set get large ones drucker schapire simard 
hand method generating virtual examples advantage readily implemented kinds learning machines symmetries 
lie groups symmetry transformations dealing discrete symmetries bilateral symmetries vetter poggio bulthoff derivative methods ones simard 
applicable 
desirable intermediate method advantages virtual examples approach computational cost 
try convince reader method realized build type learning machine described section 
fig 

different ways incorporating invariances decision function 
dotted line marks true boundary disks circle training examples 
assume prior information tells classification function depends norm input vector origin center picture 
solid lines crossing example indicate type information conveyed different methods incorporating prior information 
left incorporating regularizer learn tangent values cf 
simard middle generating virtual examples localized region training example right changing representation data mapping example norm 
feasible method yields information 
necessary nonlinear transformation desired invariances localized nature resort techniques 
reader may note right hand side picture particular examples close boundary allow exploit prior knowledge effectively method get approximation true boundary examples closest allow estimation true boundary 
similar step approach shall pursued 
support vector learning machines support vector algorithm vapnik boser guyon vapnik cortes vapnik uses structural risk minimization principle vapnik construct decision rules generalize 
doing extract small subset training data 
space permit explain algorithm detail merely outline main ideas 
method structural risk minimization fact test error rate bounded sum training error rate term depends called vc vapnik chervonenkis dimension learning machine 
minimizing sum quantities high generalization performance achieved 
linear hyperplane decision functions sgn delta vc dimension controlled controlling norm weight vector vapnik 
training data sigma separating hyperplane generalizes minimizing cortes vapnik kwk fl delta subject delta delta gamma fl constant determines trade training error vc dimension 
solution problem shown expansion nonzero belong precisely meeting constraint lie closest decision boundary called support vectors 
solving quadratic programming problem defined 
method generalized nonlinear decision surfaces mapping input nonlinearly high dimensional space finding separating hyperplane space boser guyon vapnik 
achieved implicitely different types symmetric functions ordinary scalar product deltay 
way gets generalization sgn delta virtual support vectors choice determines type classifier constructed possible choices include polynomial classifiers delta neural networks tanh delta delta gamma theta radial basis function classifiers exp gamma gamma yk oe delta 
prior knowledge machines exhibit high generalization ability extract identical sets support vectors cf 
eq 

possible train performance comparison bottou 
note support vector machine excellent accuracy remarkable high performance classifiers include knowledge geometry problem 
fig 

example support vector classifier radial basis function kernel exp gamma yk 
coordinate axes range 
circles disks classes training examples middle line decision surface outer lines precisely meet constraint 
note support vectors algorithm marked extra circles centers clusters examples critical classification task 
machines solely support vector set extracted machine test performance worse training full data base scholkopf burges vapnik 
finding starting point investigated question sufficient generate virtual examples support vectors 
hope add information generate virtual examples patterns close boundary 
experiments proceeded follows 
train support vector machine extract support vector set 

generate artificial examples applying desired invariance transformations support vectors 
refer examples virtual support vectors 

train support vector machine generated examples 
set experiments conducted postal service data base handwritten digits containing training examples test examples 
data base extensively literature lenet convolutional network achieving test error rate le cun 
experiments fl cf 
smoothing data gaussian kernel width 
get class classifier digit recognition combine binary classifiers described type cf 
vapnik 
case virtual support vectors generated set different support vectors classifiers 
alternatively carry procedure separately binary classifiers dealing smaller training sets training second machine 
table shows incorporating translational invariance improves performance significantly error rate 
noted test set difficult human error rate discussion see simard le cun denker 
table 
comparison support vector sets performance training original data base training generated virtual support vectors 
training runs polynomial classifier degree 
virtual support vectors generated simply shifting images pixel principal directions 
adding unchanged support vectors leads training set second classifier times size classifier support vector set union support vector sets binary classifiers note due overlap smaller sum support set sizes 
classifier trained size average 
svs 
different svs test error full training set sv set virtual sv set types invariances improvements albeit smaller ones generating virtual support vectors rotation line thickness transformation drucker schapire simard constructed polynomial classifiers error rate cases 
larger database information invariances decision function contained differences patterns class 
show possible improve classification accuracies technique applied method mnist database handwritten digits 
database standard performance comparisons department error rate record held boosted lenet bottou 
virtual support vectors generated pixel translations improved degree polynomial classifier error rate 
case applied technique separately support vector sets binary classifiers union order avoid getting overly large support vector sets retraining 
improvements possibly achieved combining different types invariances choosing kind transformations applied individual support vectors provide new information decision boundary 

intriguing extension scheme techniques image correspondence vetter poggio extract transformations training examples 
transformations generate virtual support vectors 
discussion shown support vector learning machines invariances readily incorporated generating virtual examples support vectors training set 
method yields significant gain classification accuracy moderate cost time requires training runs constructs classification rules utilizing support vectors slowing classification speed cf 
eq 
case points amounted factor 
support vector machines known allow short training times bottou point usually critical 
certainly training virtual examples generated data base significantly slower 
compensate second point reduced set method burges increase speed 
ideas influenced discussions girosi niyogi poggio sung stay massachusetts institute technology 
abu mostafa hints 
neural computation boser guyon vapnik training algorithm optimal margin classifiers 
fifth annual workshop computational learning theory pittsburgh acm 
bottou cortes denker drucker guyon jackel le cun muller sackinger simard vapnik comparison classifier methods case study handwritten digit recognition 
proceedings th international conference pattern recognition neural networks jerusalem burges simplified support vector decision rules 
th international conference machine learning cortes vapnik support vector networks 
machine learning drucker schapire simard boosting performance neural networks 
international journal pattern recognition artificial intelligence le cun boser denker henderson howard hubbard jackel backpropagation applied handwritten zip code recognition 
neural computation scholkopf burges vapnik extracting support data task 
fayyad uthurusamy 
eds proceedings international conference knowledge discovery data mining aaai press menlo park ca rubinstein canonical coordinates method pattern deformation theoretical computational considerations 
ieee transactions pattern analysis machine intelligence simard le cun denker efficient pattern recognition new transformation distance 
hanson cowan giles 
eds advances neural information processing systems morgan kaufmann san mateo ca simard le cun denker tangent prop formalism specifying selected invariances adaptive network 
moody hanson lippmann advances neural information processing systems morgan kaufmann san mateo ca vapnik estimation dependences empirical data 
russian nauka moscow english translation springer verlag new york vapnik nature statistical learning theory 
springer verlag new york vetter poggio image synthesis single example image 
proceedings european conference computer vision cambridge uk press vetter poggio bulthoff importance symmetry virtual views dimensional object recognition 
current biology 
