clustering high dimensional space hypergraph models hong sam han george karypis vipin kumar mobasher department computer science engineering army hpc research center university minnesota eecs bldg union st se minneapolis mn usa karypis kumar cs umn edu clustering data large dimension space great interest data mining applications 
traditional algorithms means autoclass fail produce meaningful clusters data sets known dimensionality reduction techniques principal component analysis latent semantic indexing 
propose method clustering data high dimensional space hypergraph model 
hypergraph model maps relationship original data high dimensional space hypergraph 
hyperedge represents relationship affinity subsets data weight hyperedge reflects strength affinity 
hypergraph partitioning algorithm find partitioning vertices corresponding data items partition highly related weight hyperedges cut partitioning minimized 
results experiments different data sets stock data period protein coding data web document data 
applicable compared results autoclass means clustering algorithm original data reduced dimensionality data obtained principal component analysis latent semantic indexing scheme 
experiments demonstrate approach applicable effective wide range domains 
specifically approach performed better traditional schemes high dimensional data sets terms quality clusters runtime 
approach able filter noise data clusters effectively compromising quality clusters 
keywords clustering data mining association rules hypergraph partitioning dimensionality reduction principal component analysis latent semantic indexing 
supported nsf asc army research office contract da daah army high performance computing research center cooperative agreement number daah contract number daah content necessarily reflect position policy government official endorsement inferred 
additional support provided ibm partnership award ibm sur equipment 
access computing facilities provided minnesota supercomputer institute 
see www cs umn edu han related papers 
clustering data mining sad chy discovery process groups set data intracluster similarity maximized intercluster similarity minimized chy 
discovered clusters explain characteristics data distribution 
example business applications clustering characterize different customer groups allow businesses offer customized solutions predict customer buying patterns profiles cluster belong 
set data items variables traditional clustering techniques nh cs sd fis dj lee group data measure similarity distance data points 
clustering algorithms able effectively cluster data dimensionality space number variables relatively small 
schemes fail produce meaningful clusters number variables large 
clustering data large dimension space great interest data mining applications 
example market basket analysis typical store sells thousands different items thousands different customers 
cluster items sold knowledge perform effective shelf space organization target sales promotions 
clustering items sales transactions requires handling thousands variables corresponding customer transactions 
finding clusters customers sales transactions presents similar problem 
case items sold store correspond variables clustering problem 
way handling problem reduce dimensionality data preserving relationships data 
traditional clustering algorithms applied transformed data space 
principal component analysis pca jac multidimensional scaling mds jd kohonen self organizing feature maps sofm koh commonly techniques dimensionality reduction 
addition latent semantic indexing lsi method frequently information retrieval domain employs dimensionality reduction technique similar pca 
inherent problem dimensionality reduction presence noise data may result degradation clustering results 
partly due fact projecting smaller number dimensions noise data may appear closer clean data lower dimensional space 
domains possible practical remove noise preprocessing step 
furthermore discuss section dimensionality reduction techniques suffer shortcomings impractical real world application domains 
propose method clustering data high dimensional space hypergraph model 
hypergraph model data item represented vertex related data items connected weighted hyperedges 
hyperedge represents relationship affinity subsets data weight hyperedge reflects strength affinity 
hypergraph partitioning algorithm find partitioning vertices corresponding data items partition highly related weight hyperedges cut partitioning minimized 
relationship data points association rules correlations set items bms distance metric defined pairs items 
current version clustering algorithm frequent item sets apriori algorithm capture relationship hypergraph partitioning algorithm hmetis find partitions highly related items hypergraph 
frequent item sets derived apriori part association rules discovery meet specified minimum support criteria 
association rule algorithms originally designed transaction data sets transaction contain subset possible item set variables data set clustered binary data naturally represented set transactions follows 
set data items clustered item set binary variable transaction contains items value variable 
non binary discrete variables handled creating binary variable possible value variable 
continuous variables handled discretized techniques similar sa 
association rules particularly representing affinity set items item small fraction transactions presence item transaction significant absence item 
true data sets experiments 
test applicability robustness scheme evaluated wide variety data sets 
results different data sets stock data period protein coding data web document data 
applicable compared results autoclass cs means clustering algorithm original data reduced dimensionality data obtained pca lsi 
experiments demonstrate approach applicable effective wide range domains 
specifically approach performed better traditional schemes high dimensional data sets terms quality clusters runtime 
approach able filter noise data clusters effectively compromising quality clusters 
rest organized follows 
section contains review related 
section presents clustering method hypergraph models 
section presents experimental results 
section contains directions 
related clustering methods studied areas including statistics dj lee cs machine learning sd fis data mining nh cs 
approaches probability distance similarity measure 
schemes effective dimensionality data relatively small 
scheme tend break dimensionality data large reasons 
trivial define distance measure large dimensional space 
secondly distance schemes generally require calculation mean document clusters neighbors 
dimensionality high calculated mean values differ significantly cluster 
clustering mean values produce clusters 
similarly probabilistic methods bayesian approach autoclass cs perform size feature space larger size sample set common large dimensional data sets 
furthermore underlying expectation maximization em algorithm tsm autoclass computational complexity kd number clusters number attributes number items clustered average number iterations em algorithm 
large dimensional data sets autoclass run time unacceptably high 
known widely technique dimensionality reduction principal component analysis pca jac 
consider data set data items variables 
pca computes covariance matrix size mm calculate leading eigenvectors covariance matrix 
leading eigenvectors matrix principal features data 
original data mapped new principal directions 
projected data lower dimensions clustered traditional clustering algorithms means jd hierarchical clustering jd autoclass 
pca provides guidelines determine right number dimension data proportion variance explained characteristic roots covariance matrix 
noted jac different methods provide widely different guideline data dif find right number dimension 
choice small lose important features data 
hand choice large capture important features dimensionality large traditional clustering algorithms effectively 
disadvantage pca memory requirement computational requirement higher depending number eigenvalues 
requirements unacceptably high large latent semantic indexing lsi dimensionality reduction technique extensively information retrieval domain similar nature pca 
finding singular value decomposition covariance matrix finds singular value decomposition original data 
lsi require calculation covariance matrix smaller memory cpu requirements jac 
kohonen self organizing feature map sofm koh scheme neural networks projects high dimensional input data feature map smaller dimension proximity relationships input data preserved 
neuron competitive layer approach competes best stimulus response terms similarity input data winning neuron neighbor neurons update weight vectors direction weight vectors similar input data 
network trained data point projected neurons best match weight vectors neurons 
sofm provide measure transformation furthermore data sets large dimensions convergence network training slow 
multidimensional scaling mds jd transforms original data smaller dimensional space trying preserve rank ordering distances data points 
mds provide guidelines find right number dimension capture variations original data 
furthermore techniques high computational complexity number items clustered 
hypergraphs data mining studied 
example shown problem finding maximal elements lattice patterns closely related hypergraph transversal problem 
clustering grouping association rules proposed lsw ka 
lsw focus finding clusters association rules right hand side finding item clusters 
ka scheme proposed cluster database attributes binary associations 
approach association graph constructed attributes database vertex set con binary associations items edges graph 
minimum spanning tree mst constructed edges minimum spanning tree proposed interesting associations 
successive removal edges minimum spanning tree produce clusters attributes ka jd 
mst algorithm focuses finding set edges connect vertices minimizing sum weights resulting minimum spanning tree information density edges connecting related vertices 
information essential finding clusters 
mst scheme may produce quality clusters 
graph partitioning algorithms implicitly take account density interconnectivity vertices better candidates finding clusters 
hypergraph clustering algorithm clustering related items consists steps 
step weighted hypergraph constructed represent relations different items second step hypergraph partitioning algorithm find partitions items partition highly related 
current implementation frequent item sets association rule algorithm hyperedges 
brief overview association rules model information transaction database hypergraph describe hypergraph modeling clustering algorithm 
association rules association rules capture relationship items transaction ams 
set transactions transaction subset item set subset define support count respect 
number transactions contain example consider set transactions supermarket shown table 
items set transactions beer coke diaper 
support count diaper mi lk diaper mi lk beer 
association rule expression form support rule defined confidence defined 
example consider rule tid items bread coke milk beer bread beer coke diaper milk beer bread diaper milk coke diaper milk table transactions supermarket 
presence diaper milk transaction tends indicate presence beer transaction 
support rule diaper ilk beer 
confidence rule diaper ilk beer diaper ilk 
rules high confidence close important denote strong correlation items rule 
rules high support important supported non trivial fraction transactions database 
task discovering association rule find rules greater minimum support threshold greater minimum confidence threshold 
association rule discovery composed steps 
step discover frequent item sets candidate sets support greater minimum support threshold specified 
second step generate association rules frequent item sets 
number algorithms developed discovering frequent item sets ais hs 
apriori algorithm efficient algorithms available 
algorithm experimentally shown linearly scalable respect size database implemented parallel computers large memory processing power effectively 
hypergraph modeling hypergraph ber consists set vertices set hyperedges 
hypergraph extension graph sense hyperedge connect vertices 
model set vertices corresponds set data items clustered hyperedge corresponds set related items 
key problem modeling data items hypergraph determination related items grouped hyperedges determining weights hyperedge 
frequent item sets computed association rule algorithm apriori excellent candidates find support support support support support support support bc ac ab illustration power confidence capturing relationships items 
related items 
note algorithms find frequent item sets support greater specified threshold 
value threshold may determined domain specific manner 
frequent item sets capture relationship items size greater equal 
note distance relationships capture relationship pairs data points frequent items sets capture relationship larger sets data points 
added modeling power nicely captured hypergraph model 
assignment weights resulting hyperedges tricky 
obvious possibility support frequent item set weight corresponding hyperedge 
possibility weight function confidence underlying association rules 
size hyperedges support confidence provide similar information 
fact items equal number transactions support item set fag item set fbg direct correspondence support confidence rules items greater support fa bg confidence rules fag fbg fag fbg 
support carries meaning hyperedges size greater general support large hyperedge smaller support smaller hyperedges 
furthermore larger item sets confidence underlying association rules capture correlation data items captured support 
example consider items relationship represented 
support pairs fa bg fb cg fa cg support fa cg 
way relationship stronger pairs ab ac bc 
conditional probabilities captured confidence corresponding association rules 
example illustrates relationships item sets size greater captured pairwise relationships subsets irrespective relationship modeled support confidence individual rules 
hypergraph expressive model graph problem 
natural possibility define weight function support confidence rules group items frequent item set 
options include correlation distance similarity measure 
current implementation model frequent item set represented hyperedge weight equal average confidence association rules called essential rules items edge singleton right hand side 
call essential rules capture information unique frequent item set 
rule subset items rule included rules subset frequent item set 
furthermore rules item right hand size covered subset frequent item set 
example fa cg frequent item set hypergraph contains hyperedge connects consider rule fag fb cg 
interpreted implication rule information captured fag fbg fag fcg 
consider essential rules confidences noted arrows item set fa cg fa bg fcg fa cg fbg fb cg fag 
assign weight hyperedge connecting refer hypergraph association rule hypergraph 
finding clusters items note frequent items sets represent relationship items transaction 
relationships fine grain 
example consider frequent item sets database stock transactions inst intel micron tech intel micron tech item sets indicate different days stocks texas instrument intel micron technology moved days stocks intel national semiconductor moved appears texas instrument intel micron technology national semiconductor related 
frequent item set stocks may small support may captured association rule computation algorithm 
hypergraph representation cluster relatively large groups related items partitioning highly connected partitions 
way achieving hypergraph partitioning algorithm partitions hypergraph parts weight hyperedges cut partitioning minimized 
note minimizing hyperedge cut essentially minimize relations violated splitting items groups 
parts bisected recursively partition highly connected 
hmetis multi level hypergraph partitioning algorithm shown produce high quality bi sections wide range problems arising scientific vlsi applications 
hmetis minimizes weighted hyperedge cut tends create partitions connectivity vertices partition high resulting clusters 
hypergraph partitioned parts eliminate bad clusters cluster fitness criterion 
set vertices representing hyperedge set vertices representing partition 
fitness function measures goodness partition defined follow ness weight weight fitness function measures ratio weights edges partition weights edges involving vertex partition 
high fitness value suggests partition weights edges connecting vertices partition 
partitions fitness measure greater threshold value considered clusters retained clusters approach 
experiments set fitness threshold 
note fitness criterion easily incorporated partitioning algorithm partition bisected fitness partition threshold value 
partitions considered partitions 
partitions partition examined filter vertices highly connected rest vertices partition 
connectivity function vertex defined follow connectivity measures percentage edges vertex associated 
high connectivity value suggests vertex edges connecting proportion vertices partition 
vertices connectivity measure greater give threshold value considered belong partition remaining vertices dropped partition 
experiments set connectivity threshold 
computational complexity problem finding association rules meet minimum support criterion shown linearly scalable respect number transactions ams 
highly efficient algorithms apriori able quickly find association rules large databases provided support high 
example experiments apriori shown data set transactions containing subset items minutes 
note computational requirement increases dramatically minimum support decreases 
scheme need find association rules low support 
need find right amount information association rules cluster items 
minimum support low capture minor relationships data points noise 
furthermore hypergraph partitioning algorithm find partitions hypergraph reasonably sparse 
keeping support threshold high limit computational complexity apriori algorithm 
hypergraph partitioning studied problem context vlsi circuit partitioning highly efficient algorithms hmetis developed 
algorithm find bisection circuits containing nodes minutes workstation 
particular complexity hmetis way partitioning log number vertices number edges 
number vertices association rule hypergraph number data items clustered 
number hyperedges number frequent item sets support greater specified minimum support 
note number frequent item sets hyperedges increase number transactions variables increases 
clustering method linearly scalable respect number variables dimension data 
experimental results tested ability item clustering algorithm find groups related items data sets application areas 
results experiments described subsections 
compared results autoclass means algorithm raw data compared result means reduced dimensionality data produced pca lsi 
chose autoclass comparison autoclass find right number clusters automatically known producing quality clusters 
distance clustering chose means algorithm comparison known distance clustering algorithms 
experiments pca lsi schemes data normalized appropriately 
pca lsi means performed values parameters algorithms number dimensions number clusters 
report best results 
experiments locally implemented version apriori algorithm find association rules construct association rule hypergraph 
stock data data set consists daily price movement stocks belong index 
known financial community stocks belonging industry group tend trade similarly 
example group stocks particular industry tend move depending market belief health industry group 
reason data set verify ability clustering algorithm correctly cluster various stocks industry group 
data set consists binary table size 
row table corresponds indicator stocks stocks column corresponds trading day jan oct 
entry location corresponds indicator stock means closing price stock th day significantly higher point day 
similarly entry location corresponds indicator stock means closing price stock th day significantly lower point day 
clustered stock indicators hypergraph method 
minimum support threshold means stocks frequent item set moved days 
lead hypergraph consisting vertices hyperedges 
note number vertices hypergraph considerably smaller number distinct items data set 
stocks move frequently corresponding items sufficient support 
hypergraph partitioned partitions 
partitions satisfy fitness function 
item clusters industry groups associated clusters shown table 
looking clusters see item clustering algorithm successful grouping stocks belong industry group 
clusters shown table clusters contain items clusters contains stocks primarily industry group 
example algorithm able find technology bank financial oil related stock clusters 
remaining clusters contain companies belong different industry groups 
interesting see clustering algorithm partitioned technology companies groups consists networking semiconductor companies cluster 
note item cluster contains stocks rail oil related move opposite direction 
contains rail related stocks move oil related stocks move 
single industry group cluster corresponds strongly related groups stocks oil prices go oil related stocks go rail related stocks go profit margins improve 
means autoclass find clusters related stocks data set 
clusters stocks methods quite poor 
example regardless number clusters set means algorithm produced couple large clusters containing mixed stocks small clusters 
smaller clusters clusters contained primarily technology related financial stocks remaining small clusters mixed contained stocks industry groups 
results demonstrate traditional clustering methods data set 
assess effectiveness dimensionality reduction techniques problem applied pca stock data 
experimented number dimensions reduced data sets ranging 
kmeans algorithm reduced dimension data set numbers clusters ranging 
results improved dramatically means algorithm clusters reduced data dimension 
clusters pca applied raw data normalization resulting clusters quite poor 
results reported case pca normalized data 
discovered item clusters industry group applied bay network com sys cisco dsc comm hp intel lsi logic micron tech technology natl oracle sgi sun texas inst apple comp adv micro device andrew computer assoc circ city stores compaq dec emc technology gen instrument motorola microsoft scientific atl fannie mae fed home loan morgan stanley financial baker hughes inds hld louisiana land oil phillips schlumberger gold echo bay mines mining gold mining placer dome aluminum cyprus amax min inland steel metal reynolds metals stone container steel applied bay network com sys cisco compaq hp intel lsi logic micron tech technology natl oracle motorola sun texas inst dsc comm dec emc computer assoc gen instrument microsoft scientific atl technology sgi merrill lynch cascade champion intl georgia pacific intl james river louisiana pacific stone container temple inland union camp cp bell atlantic nynex regional bell gold echo bay mines mining gold mining placer dome bank boston chase new bank great west fin aluminum cyprus amax min inland steel phelps dodge metal reynolds metals steel america union ferris caterpillar cnf trans ford motor motor machinery foster wheeler general motors rand circ city stores dayton hudson fed dept str gap nordstrom retail department stores sears tjx wal mart stores apple comp adv micro device andrew boston cp enter shared med sys technology electronics tektronix united health cp surgical burl santa hld payne rail oil amr columbia hca computer science delta air lines green tree fin discover home depot air financial retail companies sw airlines morton intl pep boys general motors rand louisiana merrill lynch auto technology novell tektronix cnf trans gap enter companies sw airlines mci cp sears home building retail technology shared med sys tele comm united health cp surgical table clustering stock data clusters contained stocks primarily industry group 
clusters contained stocks 
remaining clusters small contained stocks contained stocks industry groups 
clusters available www cs umn edu han sigmod html 
clusters significantly better means original data stocks clustered belong clusters 
contrast clusters hypergraph clustered stocks belong clusters 
argue fair comparison pca means clustered items hypergraph method clustered items 
knowing stock labels easy way distinguish clusters clusters pca means algorithm 
scatter value cluster sum squared error cluster correspond quality clusters confirmed examining labels stocks 
contrast hypergraph scheme filtered stocks belong clusters 
clearly filtered stocks genuinely belonged clusters note pca means clustered stocks clusters number stocks filtered relatively small 
consider number stocks clusters schemes hypergraph shows able capture stocks pca means noise clusters 
result reported items belong clusters items 
lower support criteria relax filtering criteria able increase number items belonging clusters fraction items clusters goes 
reduced threshold support total number vertices graph items clustered total clusters 
clusters clusters contained items 
comparing clusters pca means note ratio items belonging clusters hypergraph scheme higher pca means 
demonstrate ability hypergraph clustering scheme filter noise introduced dummy stocks numbering original data 
movements dummy stocks selected randomly stock moved average days 
ensured dummy stocks survived pruning support criterion 
noise come hypergraph model dummy stocks significant relationships dummy genuine stocks 
furthermore dummy stocks survived final clusters highly connected items clusters 
result quality clustering unaffected dummy stocks 
hand pca means results dummy stocks survived genuine clusters 
positive side clusters previously pca means affected dummy stocks clusters contained dummy stocks number stocks clustered 
dummy stocks ended mixed clusters clusters dummy stocks 
scatter values clusters distinguish clusters mixed clusters clusters containing dummy stocks 
protein coding database data set problem domain molecular biology 
molecular biologists study genes cells organisms determine biological function proteins genes code 
faced new protein biologists perform laborious painstaking experimental process determine function protein 
rapidly determine function previously unknown genes biologists generate short segments protein coding sequences called expressed sequence tags ests match est sequences known proteins similarity matching algorithms agm pl 
result matching table showing similarities ests known proteins 
est clusters table ests cluster related functionally biologists match new est new proteins est clusters find est clusters match new est closely 
point biologists focus experimentally verifying functions new est represented matching est clusters 
finding clusters related ests important problem 
related area hhs reports clustering sequence level building blocks proteins finding transitive closure pairwise probabilistic similarity judgments 
assess utility hypergraph clustering domain performed experiments data provided authors nrs scc 
data set consists binary table size 
row table corresponds est column corresponds protein 
entry location means significant match est protein clustered ests hypergraph method 
finding frequent item sets support essentially created frequent item sets est supported proteins 
led hypergraph vertices hyperedges 
hmetis find partitions satisfied fitness criteria 
total number ests clustered 
est clusters biologist determine related 
analysis showed est clusters algorithm correspond ests related 
fact clusters corresponds single protein family 
clusters contain ests 
clusters bad contained random ests analysts determine quality clusters 
remaining clusters subclusters corresponding distinct protein families clusters subclusters clusters subclusters clusters subclusters clusters subclusters 
furthermore examined connectivity sub hypergraphs correspond clusters able see subdivision created single protein clusters 
particularly important verified association rule hypergraph highly effective modeling relations various ests able verify quality clusters looking connectivity 
clustering algorithm took minutes find various clusters includes time required find association rules form hypergraph 
autoclass try find clusters ests 
autoclass clusters clusters 
remaining clusters extremely poor included large number ests 
autoclass clusters size clusters size greater bad mixed clusters 
believe case data set autoclass unable successfully cluster ests due high dimensionality sparsity data set 
limitations autoclass handle data sets high dimensionality manifested amount time required 
took hours find clusters ests 
tried pca data set ran memory calculating singular value decomposition svd covariance matrix size matlab sgi challenge machine gbytes main memory 
avoid large memory requirement pca lsi approach svd original matrix computed covariance matrix 
varied number dimension varied number clusters means algorithm 
exploring set parameters noticed quality clusters produced means algorithms sensitive number clusters 
particular reduced dimensionality quality clusters produced means deteriorated number clusters greater 
chose number clusters algorithm produced relatively large bad clusters chose number clusters larger algorithm produced lot small clusters 
provide detailed report analysis experiment dimensionality data reduced means algorithm find clusters 
clusters lsi means total clusters clusters contain ests 
clusters bad contain random ests theme ests 
problem having huge cluster ests common experiments lsi means different size reduced dimension different number clusters 
remaining clusters ests little clustering information protein families clusters analysts arrive conclusive decision 
web document data data set consisted web documents words frequently appear 
data set find types clusters words tend appear documents documents words common 
word clusters find similar documents web potentially serve description label classifying documents wp 
ability find clusters documents useful filtering categorizing large collection documents 
clustering related words collected documents network excellence manufacturing online site 
documents stemming algorithm fra find distinct stems appear 
total distinct word stems 
data set represented table size row corresponds words column corresponds document 
entry location frequency occurrence word document original apriori algorithm works binary data converted original table binary ta web org index html cluster cluster cluster cluster cluster access act data action internet approach busi engineer mov comput check includes please electron manag site goal feder network establish web follow services health ww power govern law step support laws page systems nation public wide regulations table word clusters hypergraph scheme note words stemmed 
ble entry location means word occurs document apriori algorithm find frequent item sets words minimum support threshold words frequent item sets appear documents 
resulting association rule hypergraph vertices hyperedges 
note distinct word stems nodes hypergraph 
rest words survive association rules containing words support preset threshold 
pruning infrequent word stems important words appear characterize documents 
minimum support threshold provides simple effective method prune non essential information hypergraph model 
hypergraph partitioned partitions passed fitness criteria 
word clusters shown table remaining www cs umn edu han sigmod html 
looking table see word clusters algorithm contain highly related words 
example cluster includes word stems internet site web clearly correspond web related information 
similarly words cluster cluster cluster cluster domains computing electronic government software networking legal domains respectively 
autoclass find word clusters 
autoclass handle non binary data conveniently original data contains actual frequency words document binary data scheme 
autoclass word clusters include large clusters containing words 
smallest better clusters shown table remaining clusters www cs umn edu han sigmod html 
looking table see word clusters autoclass quite poor 
fact see obvious relation words cluster 
cluster cluster cluster cluster cluster copyright adopt concern court cornell effort congress affirm design held agent doc amend employ engineer hr html documents appeals appear equal home hyper juli list meet news house object mov apr organize ii iii offices please nov pac pages programm patents people portions mail register sites publications sections major name resist bas bear tac select server page basic ww timeout topics servers section bookmarks changes trademark user send com uspto word version visit thomas bills web track trade center central action table word clusters autoclass cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster copyright death notices posit handbook adopt earth nuclear heart favor harm organize iron interview base investigate awards race ill policies metal involves structures third share richard increases letter mica applicants classifi protect central vii sense labor resources assign charge class sex names barriers paint people tell services polish committees steps publish train stock promotion busi tobacco screen speak zinc treatment table word clusters lsi means contrast word clusters algorithm considerably better easily identify type documents describe 
interesting note cluster algorithm web related words largely dispersed word clusters autoclass 
poor performance autoclass particularly surprising dimensionality data set relatively small items space variables 
assess effectiveness dimensionality reduction techniques tried pca lsi approach reduce dimensionality data 
note pca lsi methods applied normalized data original frequency matrix 
means algorithm resulting data set 
resulting word clusters improve significantly 
specifically reduced dimensionality lsi scheme clusters means algorithm clusters size clusters size ranging seventeen clusters size ranging clusters size ranging clusters size greater 
clusters size words show trend clusters size greater words kind trend 
remaining clusters better chance capturing related words 
table shows better word clusters clusters 
remaining clusters www cs umn edu han sigmod html 
note quality appear better autoclass original data 
surprise original dimension data small result original data 
clustering related documents problem document clustering extensive comparison hypergraph method autoclass distance hierarchical clustering algorithm reported 
results show hypergraph method consistently gave better clusters autoclass hierarchical clustering different data sets prepared web documents 
discuss results experiment documents spread different document categories 
results clustering documents hypergraph method autoclass shown taken respectively 
reader see scheme provides better clusters autoclass 
performed dimensionality reduction data pca lsi clusters resulting data means algorithm 
dimensionality reduction techniques worked quite data set provided clusters quality quite similar hypergraph method 
class distribution clusters hypergraph method 
class distribution clusters autoclass 
shows class distribution clusters experiment dimension reduced lsi number clusters means algorithm 
reader see quality clusters quite similar 
particular schemes clusters consist primarily documents category 
directions method clustering data high dimensional space hypergraph model 
experiments indicate hypergraph clustering holds great promise clustering data large dimensional spaces 
traditional clustering schemes autoclass means directly large dimensionality data sets tend produce extremely poor results 
methods perform better dimensionality data reduced methods pca lsi 
shown experiments hypergraph scheme produces clusters better produced autoclass class distribution clusters lsi means 
means algorithm reduced dimensionality data sets 
major advantages scheme traditional clustering schemes require dimensionality reduction uses hypergraph model represent relations data items 
model allows effectively represent important relations items sparse data structure computationally efficient partitioning algorithms find clusters related items 
note sparsity hypergraph controlled appropriate support threshold 
additional advantage scheme ability control quality clusters requirements users domains 
different levels minimum support apriori algorithm amount relationship captured hypergraph model adjusted 
higher support gives better quality clusters containing smaller number data items lower support results clustering larger number items poorer quality clusters 
furthermore hypergraph model allows correctly determine quality clusters looking internal connectivity nodes cluster 
fitness criterion conjunction connectivity threshold discussed section provide additional control quality cluster 
computationally scheme linearly scalable respect number dimensions data measured terms number binary variables items provided support threshold generating association rules sufficiently high 
clustering schemes dimensionality reduction schemes approach suffers fact right parameters necessary find clusters 
appropriate support level finding frequent item sets largely depend application domain 
limitation approach scheme naturally handle continuous variables need discretized 
discretization lead distortion relations items especially cases higher value indicates stronger relation 
type domains continuous variables higher values imply greater importance developed new algorithm called min apriori operates directly continuous variables discretizing 
fact clustering documents section results constructed min apriori 
current clustering algorithm relies hypergraph partitioning algorithm hmetis find way partitioning 
discussed section hmetis produces high quality partitions limitations 
particularly number partitions specified users hmetis know recursive bisection 
working incorporate fitness criteria partitioning algorithm partitioning algorithm determines right number partitions automatically 
way clustering bottom partitioning followed cluster refinement 
approach hmetis find partitions top fashion start growing partitions bottom repeatedly grouping highly connected vertices 
bottom partitions refined way partitioning refinement algorithm implemented hmetis 
acknowledgments elizabeth john providing protein coding data verifying results 
jerome moore providing web document data 
agm stephen altschul warren gish webb miller eugene myers david lipman 
basic local alignment search tool 
journal molecular biology 
ais agrawal imielinski swami 
mining association rules sets items large databases 
proc 
acm sigmod int 
conf 
management data washington 
ams agrawal mannila srikant toivonen verkamo 
fast discovery association rules 
fayyad piatetsky shapiro smith uthurusamy editors advances knowledge discovery data mining pages 
aaai mit press 
agrawal srikant 
fast algorithms mining association rules 
proc 
th vldb conference pages santiago chile 
berry dumais brien 
linear algebra intelligent information retrieval 
siam review 
ber berge 
graphs hypergraphs 
american elsevier 
bms brin motwani 
market baskets generalizing association rules correlations 
proc 
acm sigmod int 
conf 
management data tucson arizona 
chy chen han yu 
data mining overview database perspective 
ieee transactions knowledge data eng december 
cs cheeseman stutz 
baysian classification autoclass theory results 
fayyad piatetsky shapiro smith uthurusamy editors advances knowledge discovery data mining pages 
aaai mit press 
dj dubes jain 
clustering methodologies exploratory data analysis 
editor advances computers 
academic press new york 
eiter gottlob 
identifying minimal transversals related problems 
siam journal computing 
fis fisher 
optimization simplification hierarchical clusterings 
proc 
int conference knowledge discovery data mining pages montreal quebec 
fra frakes 
stemming algorithms 
frakes baeza editors information retrieval data structures algorithms 
prentice hall 
gunopulos khardon mannila toivonen 
data mining hypergraph transversals machine learning 
proc 
symposium principles database systems tucson arizona 
han boley gini gross hastings karypis kumar mobasher moore 
webace web agent document categorization 
technical report tr department computer science university minnesota 
hhs harris hunter states 
mega classification discovering motifs massive 
proceedings tenth international conference artificial intelligence aaai 
han karypis kumar 
min apriori algorithm finding association rules data continuous attributes 
technical report tr department computer science university minnesota minneapolis 
han karypis kumar 
scalable parallel data mining association rules 
proc 
acm sigmod int 
conf 
management data tucson arizona 
han karypis kumar mobasher 
clustering association rule hypergraphs position 
proc 
workshop research issues data mining knowledge discovery pages tucson arizona 
han karypis kumar mobasher 
clustering association rule hypergraphs 
technical report tr department computer science university minnesota minneapolis 
hs swami 
set oriented mining association rules relational databases 
proc 
th int conf 
data eng pages taipei taiwan 
jac jackson 
user guide principal components 
john wiley sons 
jd jain dubes 
algorithms clustering data 
prentice hall 
ka 
analysing binary associations 
proc 
second int conference knowledge discovery data mining pages portland 
karypis aggarwal kumar shekhar 
multilevel hypergraph partitioning application vlsi domain 
proceedings acm ieee design automation conference 
koh kohonen 
self organization associated memory 
springer verlag 
lee lee 
clustering analysis applications 
editor advances information systems science 
plenum press new york 
lsw lent swami widom 
clustering association rules 
proc 
th int conf 
data eng birmingham 
moore han boley gini gross hastings karypis kumar mobasher 
web page categorization feature selection association rule principal component clustering 
th workshop information technologies systems dec 
nh ng han 
efficient effective clustering method spatial data mining 
proc 
th vldb conference pages santiago chile 
nrs newman chi 
arabidopsis expressed sequence tags generation analysis dissemination 
plant genome iii international conference status plant genome research san diego ca 
pl william pearson david lipman 
improved tools biological sequence comparison 
proceedings national academy sciences 
sa srikant agrawal 
mining quantitative association rules large relational tables 
proc 
acm sigmod int 
conf 
management data montreal quebec 
sad stonebraker agrawal dayal neuhold reuter 
dbms research crossroads vienna update 
proc 
th vldb conference pages dublin ireland 
scc chi riedl dalton newman 
implementation testing automated est processing analysis system 
lawrence hunter bruce shriver editors proceedings th annual hawaii international conference system sciences volume pages 
ieee computer society press 
sd shavlik dietterich 
readings machine learning 
morgan kaufman 
toivonen klemettinen ronkainen mannila 
pruning grouping discovered association rules 
ecml workshop statistics machine learning knowledge discovery databases heraklion greece 
tsm titterington smith makov 
statistical analysis finite mixture distributions 
john wiley sons 
wp marilyn bill punch 
finding salient features personal web page categories 
th www conference santa clara ca 

