analytic qp sparseness speed training support vector machines john platt microsoft research microsoft way redmond wa microsoft com training support vector machine svm requires solution large quadratic programming qp problem 
proposes algorithm training svms sequential minimal optimization smo 
smo breaks large qp problem series smallest possible qp problems analytically solvable 
smo require numerical qp library 
smo computation time dominated evaluation kernel kernel optimizations substantially smo 
mnist database smo times fast pcg chunking uci adult database linear svms smo times faster pcg chunking algorithm 
years surge interest support vector machines svms 
svms empirically shown give generalization performance wide variety problems 
svms limited small group researchers 
possible reason training algorithms svms slow especially large problems 
explanation svm training algorithms complex subtle difficult implement 
describes new svm learning algorithm easy implement faster better scaling properties standard svm training algorithm 
new svm learning algorithm called sequential minimal optimization smo 
appear advances neural information processing systems kearns solla cohn eds mit press overview support vector machines general non linear svm expressed ff gamma output svm kernel function measures similarity stored training example input gamma desired output classifier threshold ff weights blend different kernels 
linear svms kernel function linear equation expressed delta gamma ff training svm consists finding ff training expressed minimization dual quadratic form min ff psi ff min ff ff ff gamma ff subject box constraints ff linear equality constraint ff ff lagrange multipliers primal quadratic programming qp problem correspondence ff training example equations form qp problem smo algorithm solve 
smo algorithm terminate karush kuhn tucker kkt optimality conditions qp problem fulfilled 
kkt conditions particularly simple ff ff ff output svm ith training example 
previous methods training support vector machines due immense size qp problem arises svms easily solved standard qp techniques 
quadratic form involves hessian matrix dimension equal number training examples 
matrix fit megabytes training examples 
vapnik describes method solve svm qp known chunking 
chunking relies fact removing training examples ff change solution 
chunking breaks large qp problem series smaller qp sub problems object identify training examples non zero ff qp sub problem updates subset ff associated sub problem leaving rest ff unchanged 
qp sub problem consists non zero ff previous sub problem combined worst examples violate kkt conditions 
step entire set non zero ff identified step solves entire qp problem 
chunking reduces dimension matrix number training examples approximately number non zero ff standard qp techniques chunking handle large scale training problems reduced matrix fit memory 
kaufman described qp algorithm require storage entire hessian 
decomposition technique similar chunking decomposition breaks large qp problem smaller qp sub problems 
osuna suggest keeping fixed size matrix sub problem deleting examples adding violate kkt conditions 
fixed size matrix allows svms trained large training sets 
joachims suggests adding subtracting examples heuristics rapid convergence 
smo decomposition required numerical qp library costly slow 
sequential minimal optimization sequential minimal optimization quickly solves svm qp problem numerical qp optimization steps 
smo decomposes qp problem qp sub problems similar decomposition method 
previous methods smo chooses solve smallest possible optimization problem step 
standard svm smallest possible optimization problem involves elements ff ff obey linear equality constraint 
step smo chooses ff jointly optimize finds optimal values ff updates svm reflect new values 
advantage smo lies fact solving ff done analytically 
numerical qp optimization avoided entirely 
inner loop algorithm expressed short amount code invoking entire qp library routine 
avoiding numerical qp computation time shifted qp kernel evaluation 
kernel evaluation time dramatically reduced certain common situations linear svm input data sparse zero 
result kernel evaluations cached memory 
components smo analytic method solving ff heuristic choosing multipliers optimize 
pseudo code smo algorithm relationship optimization machine learning algorithms 
solving lagrange multipliers solve lagrange multipliers ff ff smo computes constraints multipliers solves constrained minimum 
convenience quantities refer multiplier subscript quantities refer second multiplier subscript 
multipliers constraints easily displayed dimensions see 
constrained minimum objective function lie diagonal line segment 
ends diagonal line segment expressed quite simply terms ff bounds apply ff max ff sff gamma min ff sff gamma gamma normal circumstances objective function positive definite minimum direction linear equality constraint 
case smo computes minimum direction linear equality constraint ff new ff gamma gamma lagrange multipliers ff ff fulfill constraints full problem 
inequality constraints cause lagrange multipliers lie box 
linear equality constraint causes lie diagonal line 
gamma error ith training example 
step constrained minimum clipping ff new interval 
value ff computed new clipped ff ff new ff ff gamma ff new clipped linear non linear svms threshold re computed step kkt conditions fulfilled optimized examples 
heuristics choosing multipliers optimize order speed convergence smo uses heuristics choose lagrange multipliers jointly optimize 
separate choice heuristics ff ff choice ff provides outer loop smo algorithm 
example violate kkt conditions outer loop eligible optimization 
outer loop alternates single passes entire training set multiple passes non bound ff ff cg 
multiple passes terminate non bound examples obey kkt conditions ffl 
entire smo algorithm terminates entire training set obeys kkt conditions ffl 
typically ffl gamma choice heuristic concentrates cpu time examples violate kkt conditions non bound subset 
smo algorithm progresses ff bounds stay bounds ff bounds move examples optimized 
optimization smo uses shrinking heuristic proposed 
pass entire training set shrinking finds examples fulfill kkt conditions worst example failed kkt conditions 
passes training set ignore fulfilled conditions final pass training ensures example fulfills kkt condition 
ff chosen smo chooses ff maximize size step taken joint optimization 
smo approximates step size absolute value numerator equation je gammae smo keeps cached error value non bound example training set chooses error approximately maximize step size 
positive smo chooses example minimum error negative smo chooses example maximum error experiment kernel sparse kernel training number inputs caching set support sparse size vectors inputs linear mix linear mix linear mix linear mix gaussian gaussian gaussian gaussian gaussian gaussian gaussian gaussian mnist 
table parameters various experiments kernel optimizations computation time smo dominated kernel evaluations smo accelerated optimizing kernel evaluations 
utilizing sparse inputs generally applicable kernel optimization 
commonly kernels equations dramatically sped exploiting sparseness input 
example gaussian kernel expressed exponential linear combination sparse dot products 
sparsely storing training set achieves substantial reduction memory consumption 
compute linear svm single weight vector needs stored training examples correspond non zero ff qp sub problem succeeds stored weight vector updated reflect new ff values 
benchmarking smo smo algorithm tested standard chunking algorithm decomposition method series benchmarks 
smo chunking written microsoft visual compiler 
joachims package svm light version default working set size test decomposition method 
cpu time algorithms measured unloaded mhz pentium ii processor running windows nt 
chunking algorithm uses projected conjugate gradient algorithm qp solver suggested burges 
algorithms sparse dot product code kernel caching appropriate 
smo chunking share folded linear svm code 
smo algorithm tested real world data sets 
results experiments shown tables 
tests artificial data sets 
test set uci adult data set 
svm attributes census form household asked predict household income greater 
attributes categorical continuous 
continuous attributes discretized yielding total binary attributes 
second test set text categorization classifying web page belongs experiment smo svm light chunking smo svm light chunking time time time scaling scaling scaling sec sec sec exponent exponent exponent mnist table timings algorithms various data sets 
category 
web page represented sparse binary keywords attributes 
third test set mnist database handwritten digits research labs 
classifier mnist class trained 
inputs dimensional non binary vectors stored sparse vectors 
fifth order polynomial kernel match accuracy results 
adult set web set trained linear svms gaussian svms variance 
adult web data sets parameter chosen optimize accuracy validation set 
experiments adult web sets performed sparse inputs kernel caching order determine effect kernel optimizations computation time 
kernel cache cache size smo svm light megabytes 
chunking algorithm uses kernel caching matrix values previous qp step re 
linear experiments smo kernel caching svm light 
table scaling algorithm measured function training set size varied random nested subsets full training set 
line fitted log training time versus log set size 
slope line empirical scaling exponent 
seen table standard pcg chunking slower smo data sets shown dense inputs 
decomposition smo advantage standard pcg chunking ignoring examples lagrange multipliers advantage reflected scaling exponents pcg chunking versus smo svm light pcg chunking altered similar property 
notice pcg chunking uses sparse dot product code linear svm folding code smo 
optimizations speed pcg chunking due overhead numerically solving large qp sub problems 
smo svm light similar decompose large qp problem small qp sub problems 
smo decomposes smaller sub problems uses analytical solu tions dimensional sub problems svm light uses numerical qp solve dimensional sub problems 
difference timings methods partly due numerical qp overhead due difference heuristics kernel optimizations 
example smo faster svm light order magnitude linear problems due linear svm folding 
svm light potentially linear svm folding 
experiments smo uses simple kernel cache hessian rows svm light uses complex kernel cache modifies heuristics utilize kernel effectively 
smo benefit kernel cache largest problem sizes svm light speeds factor utilizing sparseness compute kernels yields large advantage smo due lack heavy numerical qp overhead 
sparse data sets shown smo speed factor pcg chunking obtained maximum speed times 
mnist experiments performed kernel cache mnist data set takes memory benchmark machine 
due sparse inputs smo factor faster pcg chunking lagrange multipliers machine memory svm light fast faster smo mnist due kernel caching 
summary smo simple method training support vector machines require numerical qp library 
cpu time dominated kernel evaluation smo dramatically kernel optimizations linear svm folding sparse dot products 
smo times faster standard pcg chunking algorithm depending data set 
chris burges running data sets projected conjugate gradient code various helpful suggestions 
burges 
tutorial support vector machines pattern recognition 
data mining knowledge discovery 
joachims 
making large scale svm learning practical 
scholkopf burges smola editors advances kernel methods support vector learning pages 
mit press 
kaufman 
solving quadratic programming problem arising support vector classification 
scholkopf burges smola editors advances kernel methods support vector learning pages 
mit press 
lecun 
mnist handwritten digit database 
available web www research att com yann ocr mnist 
merz murphy 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
irvine ca university california department information computer science 
osuna freund girosi 
improved training algorithm support vector machines 
proc 
ieee neural networks signal processing 
platt 
fast training svms sequential minimal optimization 
scholkopf burges smola editors advances kernel methods support vector learning pages 
mit press 
platt 
sequential minimal optimization fast algorithm training support vector machines 
technical report msr tr microsoft research 
available www research microsoft com smo html 
vapnik 
estimation dependences empirical data 
springer verlag 
