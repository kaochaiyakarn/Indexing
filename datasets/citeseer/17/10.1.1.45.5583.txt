mean field inference general probabilistic setting hofmann tresp corporate technology department information communications siemens ag munchen germany mail michael hofmann volker tresp siemens de systematic model independent formulation mean field theory mft inference method probabilistic models 
model independent means assume particular type dependency variables domain general probabilistic setting 
bayesian network example may arbitrary tables specify conditional dependencies run mft bayesian network 
furthermore general mean field equations derived shed light essence mft 
mft interpreted local iteration scheme relaxes consistent state solution mean field equations 
iterating mean field equations means propagating information network 
general multiple solutions mean field equations 
show improved approximations obtained forming weighted mixture multiple mean field solutions 
simple approximate expressions mixture weights 
benefits account multiple solutions demonstrated mft inference small bayesian network representing medical domain 
turns solution mean field equations interpreted disease scenario 
benefits probabilistic setting applied fields uncertainty plays prominent role image processing neural networks artificial intelligence increasingly apparent 
unfortunately probabilistic solutions require involved computation progress closely related development methods efficient handling probability distributions 
goal extend concept mean field theory mft systematic approach approximating probability distributions 
mft widely physics particular statistical mechanics number applications areas 
mft generic way context graphical models general framework dealing uncertainty dependency models 
mft context graphical models pioneered jordan saul 
develop approach new directions 
contrast previous develop systematic approach mft particular model general probabilistic setting mean field equations rigorous formalism new general form 
applied example arbitrary graphical models include boltzmann machines special case 
main advantage mean field equations provide local inference rules 
global operations needed mft propagating information large systems interacting modules 
second contribution address problem multiple solutions mean field equations 
problem originally discussed simultaneously 
show case multiple solutions weighted mixture solutions leads reasonable estimates expected values 
approximate plausible jordan sigmoid belief nets network binary variables particular kind dependencies 
boltzmann machines completely connected networks binary variables twoway interactions 
assume particular kind variables particular type dependencies 
consequence may run mean field inference bayesian network 
moment implemented interface hugin net file format 
mixing parameters derived 
general formalism far applied special case bayesian networks 
case mixing parameters obtained consistent framework means local computations 
benefits account multiple solutions mean field equations demonstrated mft inference small illustration network representing medical domain 
comment relevance mft human reasoning 
consistent propagation information large networks interacting modules general demanding task requires global operations 
mft hand suggests local simple prescription communication autonomous processors 
mean field theory probabilistic setting cross entropy measure distance set variables fx xn finite number discrete states assumed 
denotes probability distribution domain omega delta delta delta omega hn assume distribution strictly positive 
resp 
probability event resp 
real number 
interesting domains computationally intractable 
reason introduce distribution defined domain variables incorporates simplifying constraints 
goal determine obeying constraints close possible distribution 
measure distance cross entropy kullback leibler distance qkp log log ae note distance symmetric justification pkq log log ae distance measure expectation respect true distribution measures distance zero 
reason measure calculated easily expectation respect complex approximate distribution leads local concept mft 
mean field assumption mft concept theoretical physics describe systems interacting particles 
different facets mft fields different relativistic nuclear physics statistical physics neural networks 
consequence exist number ways derive mean field equations 
discussion define mean field approximation distribution closest distance measure qkp 
furthermore really heart mean field approximation assume variables distribution independent variables case write xn sight ansatz simple obviously ignoring interaction variables take advantage approach approximate propagation information evidence see 
best knowledge jordan ones define mft general way ansatz qkp measure distance 
general mean field equations minimization qkp done iterative way 
suppose current estimates marginals 
goal obtain improved approximation minimizing qkp respect assuming fixed marginals denote complement fx ig minimizing qkp respect take account normalization constraint 
done lagrange parameter solve equations qkp gamma gamma respect probabilities split qkp relations jx 
inserting relations obtain qkp omega log ff gamma omega log ff hlog gamma omega log jx ff terms line depend line 
differentiating term hlog find hlog log log differentiating second term omega log jx ff line constraint eq 
obtain exp gamma exp omega log gamma jx deltaff lagrange parameter normalizing constant exp gamma calculated easily exp gamma exp omega log gamma jx deltaff sum involves jh terms binary variables terms 
result unique solution eq 

corresponds global minimum qkp respect current estimates means updating decreases qkp 
subsequently choose variable solve mean field equations variable 
iterating repeatedly variables stepwise descend qkp 
cross entropy qkp positive iteration ends local minimum qkp 
equations may viewed mean field equations general form model assumptions 
special case assume boltzmann distribution system spins sigma defined hamiltonian gamma jx symmetric interaction matrix diagonal elements ii 
system conditional distribution jx reads jx exp gamma fix delta delta ith row interaction matrix fi inverse temperature 
mean field equations obtain exp fix delta omega ff binary variables mean values hx completely determine marginals 
case sigma hx 
fact shown easily eq 
leads hx tanh fij delta omega ff known mean field equation system interacting spins expected values hx usually denoted locality mean field theory appealing point mft local operations needed iteration mean field equation 
markov boundary variable mean field equation may simplified exp hlog jm iterating mean field equations means recursively estimating marginals current marginals neighboring variables system relaxes consistent state 
updating need conditional distribution jm stored locally node current estimates marginals stored corresponding neighboring nodes node information needed renewed estimation equation available node neighboring nodes node markov boundary node 
mixing mean field solutions iteration mean field equations converges typically local minima qkp 
physical model systems local solutions particular interest explain phase transitions phenomenon spontaneous symmetry breaking 
mean field dynamics hopfield network converges local minimum free energy landscape restores stored patterns 
want approximation global distribution particular interested expected values respect care solutions mean field equations 
pursue idea selecting particular mean field solution advantageous form weighted average mixture mean field solutions 
mixture weights derived principled way shown optimal certain assumption 
additional benefit markov boundary variable minimal set variables ae independent rest jm rest jm 
physical example markov boundary set variables ij 
relax assumption independent units mixture distribution approximate larger class distributions components mixture 
enumerate different mean field solutions hidden variable xja denotes different mean field solutions different assigning mixture weights solution form mixture distribution xja goal determine constraint qkp minimized 
easy exercise perform optimization lagrange parameter analogous previous derivation 
lines obtain hlog xja hlog xja gamma solve eq 
implicitly enters expression eq 

eq 
solved straightforward way 
aim simple expression additional approximation 
left hand side may expressed hlog xja log xja xja xja log omega log xja ff xja neglected terms xja argument logarithm 
may xja xja sufficiently small overlap different mean field solutions 
means small overlap approximation obtain mixture weights exp gamma log xja ae xja exp gamma gamma xja kp delta means different mean field solutions xja contribute global distribution distance xja kp 
plausible result guessed 
note nice result relies approximation assumption different minima qkp close 
mean field theory bayesian networks far assumptions results mean field equations mixture weights general 
focus particular parameterization probability distribution bayesian networks 
bayesian network expansion form jx gamma pi typical bayesian network variable small set parents pi fx gamma equality valid general follows repeated application bayes formula 
pi ae fx gamma eq 
second equality corresponds assertion conditional independencies 
usually structure bayesian network depicted acyclic graph arcs point parent pi corresponding children see fig 
text example 
tables pi associated nodes parameters bayesian network 
updating node eq 
need know markov boundary conditional distribution jm 
bayesian network markov boundary node parents children parents children 
index set children node conditional distribution jm jm pi pi easily derived 
result obtain exp pi pi right hand side instantiation fixed expected values evaluated remaining variables 
compared eq 
result greatly mean field updating rule 
evaluation expectation perform sum state space markov boundary calculate different expectations expensive evaluate involve table pi tables pi furthermore note table pi exactly evaluate expectation hlog pi just performing corresponding sum 
may run mean field inference bayesian network approximations 
nodes large number parents pi evaluation expectation hlog pi expensive 
practise large tables simple structure assuming noisy gate 
rarely degrees freedom large table needed 
course try exploit structure large table calculate expectation hlog pi efficiently 
saul additional approximation evaluate corresponding terms case sigmoid belief network 
remains shown case bayesian network mixture weights calculated efficient way means local computations 
expansion obtain xja kp log ja pi ae pi ja term sum right hand side requires local information conditional distribution pi distribution pi ja ja pi ja 
pi ja properties stored locally node ja pi describes neighboring nodes node bayesian networks instance find simple computational scheme 
cases computationally expensive perform expectation updating rule compute distance xja kp obtain mixture weights 
mean field inference formulated section directly refers parameters bayesian network tables pi see update equation distance 
intermediate redundant representation bayesian network junction tree 
illustration mean field inference quite bit theory far 
time show things practice 
particular want demonstrate benefits mixing multiple mean field solutions 
simple bayesian network illustration purposes depicted fig 

goal network support medical diagnosis 
simple example just want discern measles fever 
suppose patient complains weakly throat 
enter piece knowledge corresponding nodes 
goal obtain probabilities remaining nodes particular disease nodes 
reason discussed mean field ansatz remaining nodes iterate mean field equations remaining nodes 
illustration network find different solutions mean field equations 
solutions corresponding mixture distribution exact probabilities compared table 
roughly speaking solution measles scenario solution scenario scenario cause throat 
mean field method supplies beliefs unknown nodes obtain additional information character exact distribution joint distribution approximately composition modes 
modes may easily calculate approximate joint probabilities set nodes see example table 
modes disease scenarios mainly differ belief node red eyes 
obtain unique diagnosis natural question patient red eyes 
suppose eyes red 
propagating evidence iterating mean field equations jet unknown nodes find solution left measles scenario 
final belief measles 
discussion article discussed mft way method approximate probability distribution 
furthermore extended conventional mean field approach idea mixing different mean field solutions 
illustrated toy experiment approach approximate propagation evidence inference 
evidence entered model mean field approximation calculated 
results clearly demonstrated reasonable probabilistic approximations achieved take account multiple solutions mean field equations 
doing may obtain easy interpretable information joint distribution variables 
compare solutions solutions spins spins ferro magnet curie temperature 
red tongue measles red eyes fever low high weak throat strong bayesian network example illustration mean field inference 
network modeling children diseases measles 
arcs pointing diseases symptoms red eyes fever throat red tongue cause effect 
note variables just binary 
plausible values conditional probabilities tables network estimated consulting text book children diseases 
mf solution measles scenario second mf solution scenario marginals mf mixture distribution marginals exact distribution measles red eyes red tongue low fever high fever table marginal probabilities mft compared exact results 
columns show single mean field solution results poor approximation exact marginals 
furthermore note solutions nearly overlap seen rows measles 
measles table joint probability table mean field mixture distribution compared exact results 
plain mft assumption independent variables easily explain joint tables 
example shows mixture distribution may give reasonable approximations joint tables 
procedure finding solutions mean field equations mixing optimize parameters xja approximating distribution simultaneously different solutions xja mean field equations different determined independently prior determining mixture weights 
possible derive refined simultaneous optimization parameters xja 
resulting equations simple 
simplicity locality 
justifies step step procedure introduced small overlap approximation 
inference graphical models mft exploits structure graphical model non tree graphs discussed previously neighboring nodes communicate 
locality appealing point mft 
necessity compile original graph tree cover model done junction tree algorithm means moralization triangulation 
particular case bayesian networks mean field inference exhibits simplifications 
additional advantage cases existence mean field solutions sheds light structure exact distribution 
example exact distribution interpreted composed scenarios 
mft represents interesting complement inference methods 
words human reasoning appropriate 
book probabilistic reasoning intelligent systems pearl argues viable model human reasoning able perform task consistent propagation information self activated propagation mechanism array simple autonomous processors communication locally links provided network 
impact new piece evidence viewed perturbation propagates network message passing neighbouring variables minimal external supervision 
mean field inference exactly meets demands 
consequence mean field inference permits significant amount parallelism ascribed human way information processing 
furthermore arguing terms scenarios closer human way reasoning global probabilistic calculations 
mean field inference reflects way arguing 
pearl 
probabilistic reasoning intelligent systems 
morgan kaufmann san francisco edition 
cooper 
computational complexity probabilistic inference bayesian belief networks 
artificial intelligence 
fisher newman 
theory critical phenomena 
oxford university press new york edition 
parisi 
statistical field theory 
addisonwesley reading ma 
peterson anderson 
mean field theory learning algorithm neural networks 
complex systems 
peterson hartman 
explorations mean field theory learning algorithm 
neural networks 
hofmann buhmann 
pairwise data clustering deterministic annealing 
ieee trans 
pattern analysis machine intelligence 

robust topological codes keeping control internal redundancy 
phys 
rev letters 
lauritzen spiegelhalter 
local computations probabilities graphical structures expert systems 
roy 
statist 
soc 

jensen lauritzen olsen 
bayesian updating causal probabilistic networks local computations 
computational statistics 

graphical models applied multivariate statistics 
wiley new york 
saul jordan 
exploiting substructures networks 
touretzky moser hasselmo editors advances neural information processing systems proceedings conference pages cambridge ma 
mit press 
saul jaakkola jordan 
mean field theory belief networks 
journal artificial intelligence research 
hofmann tresp 
mean field theory local method approximate propagation information 
technical report www informatik 
de mf abstr html 
bishop lawrence jaakkola jordan 
approximating posterior distributions belief networks mixtures 
jordan kearns solla editors advances neural information processing systems proceedings conference pages 
mit press cambridge ma 
lawrence bishop jordan 
mixture representations inference learning boltzmann machines 
cooper moral editors uncertainty artificial intelligence proceedings fourteenth conference pages 
morgan kaufmann san francisco ca 
jaakkola jordan 
improving mean field approximation mixture distributions 
jordan editor learning graphical models pages 

cover thomas 
elements information theory 
wiley new york 

advances nuclear physics vol 
relativistic nuclear body problem 
plenum press new york 
von 
relativistic mean field calculations sigma 
phys 
rev 
fischer hertz 
spin glasses 
cambridge university press 
van hemmen kuhn 
nonlinear neural networks 
phys 
rev letters 
van hemmen kuhn 
collective phenomena neural networks 
domany van hemmen schulten editors models neural networks pages 
springer new york 
hertz krogh palmer 
theory neural computation 
addison wesley 
jensen 
bayesian networks 
ucl press london 
