multi instance neural networks jan ramon luc de raedt department computer science katholieke universiteit leuven celestijnenlaan leuven belgium jan ramon cs kuleuven ac institut ur informatik albert universit freiburg am geb freiburg br germany informatik uni freiburg de neural networks provide powerful known framework representing functions learning popular attributevalue learning various extensions studied handle richer representations domain time series 
concerned extending neural networks multi instance learning 
multi instance learning example corresponds set tuples single relation 
furthermore examples classi ed positive tuple attribute value pair satis es certain conditions 
tuples satisfy requirements example classi ed negative 
study extend standard neural networks backpropagation multi instance learning 
clear multi instance setting expressive attribute value setting expressive relational learning inductive logic programming 
multi instance setting attribute value learning set examples subset domain th attribute 
unknown target function maps example class training examples classes known 
absence noise 
task learning algorithm learn function maps elements approximates closely possible 
function represented neural network input nodes output node 
classes output nodes assume loss generality 
multi instance setting set examples emi dk unknown function maps instance class simplicity suppose classes pos assign value negative 
class example en fen nan de ned en max nj 
class example en de ned terms classes nj instances 
data set contains classi cations examples 
situation introduces asymmetry examples 
instances belonging negative examples negative concerns positive examples instance positive know instance 
clear learn suces learn multi instance neural network max function di erentiable suitable neural network 
approximate dmaxm fx xn ln elementary calculus shown fx xn maxfx xn gj ln dmaxm xn mx mx 
rst de ne neural network represent normal attribute value representation 
network process examples en instances 
possible value construct network see gure consisting times network dmaxm node 
subnetworks identical weight updated subnetwork 
weights inputs dmaxm node equal ignore 
resulting network able represent function approximations thereof multi instance examples 
backpropagation show introduced representation multi instance neural nets adapted learn 
illustrate approach simple backpropagation 
backpropagation stochastic gradient descent method 
formulas network constructed notation pg 
ji weight associated th input th node 
ji ji changed 
dmaxm dmaxm input input input fig 

network process example consisting instances activation function nodes 
suppose sigmoid function 
derivative function dx output th node net ji net output node quantity denoted additional superscript mean quantity th subnetwork 
stochastic gradient descent method move vector weights ji direction gradient error function 
ji ji chain rule gives ji ji ji ji ed ji partial derivative ji assuming ji xed ed ji partial derivative ji ji ji 
ji ji ji ji means change weights computed independently subnetworks added 
chain rule gives ed ji ed net net ji ed net ji give ji net ji filling gives 
ji net ji written 
ji ji nodes target dmaxm calculate ed target dmaxm target dmaxm target dmaxm target dmaxm dmaxm know dmaxm mo mo filling gives target dmaxm mo mo 
chain rule gives ed net ed net ed net net ed filling gives net target dmaxm mo mo written target dmaxm mo mo 
formula needed change weights oi calculated 
hidden nodes chain rule de nitions symbols follows ed net downstream ed net net net downstream net net downstream net net downstream lj net downstream lj net net downstream lj follows downstream lj provides relation calculated recursively values output nodes known 
allows compute weights changed 
setting suciently high ln smaller needed accuracy output working get network di erent weights output network guaranteed converge local optimum 
suciently large local optimum correspond local optimum ef network experiments performed preliminary experiments musk dataset available uci 
dataset example describes molecule 
example poses instances attributes 
poses property molecule said musk 
describe dataset results obtained learning algorithms 
versions dataset musk molecules musk molecules 
contains examples instances 
summarize results obtained papers compare results multi instance neural network approach 
method musk musk iterated discrim apr gfs elim kde apr gfs elim count apr gfs positive apr positive apr simple backpropagation pruned nearest neighbor euclidean distance neural network standard poses nearest neighbor tangent distance neural network dynamic multi instance neural network backprop tangent distance dynamic technique require computation molecular surface done feature vectors included data set 
expand conclude dataset multi instance neural networks improvement normal neural networks instance example perform best methods uci data 
smoothed max node allows neural networks multi instance context 
main idea approach worked simple backpropagation 
showed preliminary promising experimental results 
instance example possible directions 
experiments done validate approach 
method neural network settings doing clustering 
interesting investigate possibilities method domains richer representation relational domains converted multi instance representation 

de raedt 
attribute value learning versus inductive logic programming missing links extended 
page editor proceedings th international conference inductive logic programming volume lecture notes arti cial intelligence pages 
springer verlag 

dietterich jain lathrop lozano perez 
comparison dynamic tangent distance drug activity prediction 
advances neural information processing systems pages san mateo ca 
morgan kaufmann 

dietterich lathrop lozano erez 
solving multiple instance problem axis parallel rectangles 
arti cial intelligence 

mitchell 
machine learning 
mcgraw hill 
