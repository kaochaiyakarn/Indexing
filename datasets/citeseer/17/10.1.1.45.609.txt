integer sorting algorithms coarse grained parallel machines sanjay ranka school cis department cise syracuse university university florida top cis syr edu ranka cise ufl edu integer sorting subclass sorting problem elements integer values largest element polynomially bounded number elements sorted 
useful applications size maximum value element sorted bounded 
new distributed radix sort algorithm integer sorting 
structure algorithm similar radix sort typically requires number communication phases 
experimental results algorithm distributed memory multiprocessors intel paragon thinking machine cm 
results compared known practical parallel sorting algorithms radix sort sample sort 
experimental results show distributed radix sort competitive algorithms 
sorting widely studied problem parallel machines 
integer sorting subclass sorting problem elements integer values value maximum element polynomially bounded number elements sorted 
requirements integer sorting number elements values drawn ff ff positive constant 
new integer sorting algorithm call distributed radix sort 
compare algorithm known sorting algorithms sample sort radix sort 
comparative study algorithms fine coarse grained machines 
author supported part afmc arpa contracts wm subcontract syracuse university 
content information necessarily reflect position policy government official endorsement inferred 
sample sort single permutation step radix sort requires multiple communication steps 
radix sort incurs higher communication cost compared sample sorting 
computational cost radix sort better comparable sample sort 
proposed algorithm distributed radix sort structure radix sort requires permutation steps cases 
competitive algorithm sample sort especially large number processors load imbalances generated sample sort higher 
implemented algorithms distributed memory multiprocessors intel paragon thinking machine cm 
results machines show algorithm efficient competitive algorithms 
bit integers algorithm outperforms algorithms 
bit integers outperforms radix sort comparable better sample integer sort inferior sample integer sort bit integers 
rest organized follows 
describe machine model section 
sections describe analyze radix sort sample sort distributed radix sort respectively 
experimental results section 
drawn section 
coarse grained parallel machine coarse grained machines consist set processors tens connected interconnection network 
memory physically distributed processors 
interaction processors message passing shared address space 
cut routed networks primary thrust modeling communication cost algorithms 
analysis done interconnection networks hypercubes dimensional meshes 
analysis permutation networks hypercubes cases 
cover nearly commercially available machines 
algorithms analyzed types interconnection networks architecture independent efficiently implemented interconnection networks 
parallelization applications requires distributing data structures processors 
processor needs access non local data required local computation 
generates aggregate collective communication structures 
algorithms described literature primitives part standard textbooks 
follows refers number processors 
model cost sending message node size message 
brief description primitives follows 
broadcasting collective communication operation requires node send message size processors 

global combine prefix scans assume processor contains vector 
number processors 
global combine operation computes element wise sum local vectors processor 
resultant vector stored processors 
global vector prefix sum performs elementwise prefix scan local vectors processor 

transportation primitive operation performs personalized communication possibly high variance message size 
maximum outgoing incoming traffic processor 
running time operation shown equal communication operations maximum message size 

order maintaining data movement consider data movement problem initially processor integers elements data gamma gamma max max gamma max max gamma objective redistribute data processor contains elements 
suppose processor set elements stored array 
view gamma elements globally sorted processor array indices 
element processor appears earlier sorted order element processor order maintaining data movement problem global order preserved distribution data 
detailed description analysis primitives reader referred 
radix sort radix sort count sort relies binary representation elements sorted 
parallel radix sort structure sequential radix sort 
performs iterations number bits binary representation elements number bits examined time 
assume processor initially elements total number elements number processors 
iteration algorithm rank element computed current block 
elements permuted new destinations 
new destination element determined rank 
core step iteration algorithm compute ranks elements 
done parallel finding rank element count array processor 
rank element entry count array sufficient find rank elements locally 
step elements redistributed processors transportation primitive 
destination processor element rank processor address structure communication balanced processor send receive elements irregular individual messages may different sizes 
final step iteration locally reorders elements ranks 
total time performing radix sort hypercube mesh effi log effi gamma gamma respectively sample sort sample sort partitions list intervals elements interval smaller elements interval 
intervals independently sorted sort list 
details omitted space limitations 
ffi time perform unit computation data available local memory sample size selected elements range intervals determined sorting sample choosing gamma elements called splitters 
represent interval boundaries 
element values interval boundaries 
variations sample sort studied 
differ method sampling method sorting sample size sample chosen number splitters 
sample integer sort similar algorithm proposed 
uses regular sampling method derive splitters 
processor sorts local list sequential sorting algorithm 
experiments performed local radix sort radix outperformed quick sort bit integers large data set intel paragon cm 
chosen radix sort perform local sorting step 
tradeoff determining sample size large sample size leads better load balance results higher time requirements finding splitters 
sample points selected regular sampling selects elements relative indices gamma sample points globally sorted bitonic sort 
processor processor picks number splitter sublist performs broadcast unit message size 
processor divides list sublists gamma splitters 
done performing binary search local list size gamma splitters 
forming sublists elements redistributed transportation primitive 
communication phase irregular messages may different sizes unbalanced processors may receive sublists sorted obtain final sorted list 
sequential radix sort carry step 
bucket expansion fi measure degree load balance achieved defined maximum number elements assigned processor divided average number elements assigned processor 
assuming data set duplicates upper bound maxi sake simplicity assume divisible divisible sublists sorted merge sorting list 
asymptotically inferior experimental results suggest approach achieve better results 
easily generalized presence duplicate elements concatenating element index 
may mum number elements assigned processor gamma ps 
comparing different algorithms require number elements assigned processor sorting equal 
achieve data movement primitive final step algorithm 
total time requirement sample sort hypercube mesh interconnection networks ffi gamma log log log fi ffi gamma log fi respectively 
excludes time data movement primitive 
experiments time taken primitive compared total running time algorithm 
distributed radix sort structure new proposed algorithm distributed radix sort similar structure radix sort 
requires iterations number bits binary representation number bits examined time permuting elements processors 
iteration steps 
distributed radix sort assumes count array radix sort distributed processors 
advantages radix sort 
value radix distributed larger value radix sort 
size prefix calculation required distributed radix sort smaller radix sort 
radix sort iteration distributed radix sort sends elements destination processor intermediate phase 
processor participates source intermediate destination processor 
processing steps complicated radix sort described 
assume count array range distributed radix sort partitioned pk buckets number processors positive constant 
source processor array size kp created record number hits increase communication time 
typically log kp 
bucket 
bucket corresponds interval size kp step count number local hits buckets element belongs bucket current kp 
vector referred local bucket hits 
step global vector sum combine performed local bucket hits results identical vector processor 
vector bucket hits gives total number hits bucket 
rank element bucket obtained performing local sequential prefix sum scan 
kp entries give contention bucket 
bucket classified sparse kp hits dense 
intermediate processor sparse bucket processor sparse buckets definition kp hits 
intermediate processor buckets initially receive elements sparse buckets 
assigning sparse buckets intermediate processors stretch dense buckets intermediate processors processor receive elements sparse dense buckets 
dense bucket stretched consecutive numbered processors 
done follows dense bucket hits look intermediate processor assigned elements say intermediate processor bucket assign bucket processor entirely increment assign gamma elements bucket processor set decrement gamma examine intermediate processor potential assignment 
process repeated elements bucket assigned 
dense bucket split intermediate processors 
source processors may local data corresponding dense bucket 
allocate appropriate portions dense bucket source processors performing global vector prefix sum scan local 
step processor determine exact portions corresponding intermediate processors kp buckets needs communicate data items 
note processors may assigned zero elements 
happen case assigned elements sparse buckets 
communication source intermediate processors step shown balanced transportation primitive intermediate processor receive exactly elements 
step processes data elements intermediate processors 
finding rank elements belonging sparse bucket bucket done table size kp calculate count number elements entry table 
easily compute global rank rank element bucket locally available 
rank determine destination processor element 
element rank assigned processor dense buckets processing complicated 
distinguish types dense buckets full bucket partial bucket 
buckets contained entirely processor called full buckets stretch processors boundaries called partial buckets 
buckets processor buckets partial 
exist called preceding partial bucket succeeding partial bucket respectively 
processing full dense buckets processing sparse buckets 
difference number elements received sparse buckets smaller 
partial dense buckets step global segmented sum scan segmented sum vector size kp required calculate global rank elements 
segment corresponds stretched bucket 
processor succeeding preceding partial bucket ignore preceding partial buckets processor executing segmented scan 
results augmented having processor ignored preceding partial buckets send contents processor corresponding dense bucket 
step requires simple communication vector size kp step assigns elements destination processors follows 
element rank assigned processor step transfers data intermediate destination processors 
shown balanced personalized communication intermediate processor exactly destination processor receive exactly step reorders elements received ranks 
total time requirement distributed time required single iteration times number iterations total computation time single iteration algorithm kp kp log 
total communication cost single iteration algorithm hypercube mesh interconnection networks table 
choose number buckets processor satisfy kp 
great deal flexibility choosing value close reduces time prefix scan combine array size kp increases time scan segmented sum array size kp close opposite 
network time requirement hypercube log kp log kp log mesh gamma kp kp gamma table time requirement communication cost single iteration distributed radix sort different interconnection networks distributed radix sort allows choose larger radix iteration compared radix sort 
count array distributed processors parallel prefixes required size pk compared radix sort 
extra stage communication processing required 
experimental results implemented algorithms intel paragon thinking machine cm 
conducted extensive performance evaluation algorithms variety parameters 
section briefly summarize experimental results due space limitations 
detailed description discussion experimental results reader referred 
algorithm assumed elements processor processors 
sets element values generated parameters assuming bits representation elements 
alpha elements uniformly distributed range 

alpha elements uniformly distributed gamma arbitrary processor number 
data sets represents uniform skewed distributions target algorithms 
algorithms compared total running time alpha alpha data sets 
algorithms marginally affected data skew 
results rest section alpha data set 
data point ran algorithm times median running times reported 
performance radix sort algorithms depends radix sample sort depends size sample 
studied effect parameters performance 
optimal radix radix sort depends cost prefix sum combine cost transportation primitive cache size 
large value results iterations needed sort elements time spent prefix sum combine time spent counting computing ranks due cache misses smaller radix results opposite 
empirically determined iterations required sort bit integers 
performance sample sort depends bucket expansion fi 
ran sample sort different sample sizes choosing gives performance sample sort cases 
sorting local data local radix sort radix 
resulted slightly better performance quick sort bit integers 
performance distributed radix sort depends number buckets processor radix 
small value reduces time prefix scan combine array size kp increases time scan segmented sum size kp small value results larger number iterations needed sort elements smaller value bucket size 
empirically determined value 
radix distributed radix sort depends number processors 
determined radix machine size empirically value processors respectively 
distributed radix sort needs iterations sort bit integers 
radix switch radix sort second iteration sort bits required 
shows communication computation times processors algorithms bit integers 
radix sort spends equal time communication computation 
sample sort hand large computation cost compared communication cost 
time taken order maintaining data movement sample sort small compared time 
computational requirements distributed radix sort smaller sample sort communication requirements significantly lower radix sort 
time requirements radix sort larger algorithms cases 
performance distributed radix sort better comparable sample sort 
outperforms sample sort larger data sets higher number processors 
ran algorithms bit bit integers cm 
bit integers distributed radix sort outperforms algorithms data set sizes 
distributed radix sort requires iteration 
bit integers sample sort outperforms algorithms 
elements processor time seconds node cm sample sort distributed radix sort radix sort computation time total communication time 
elements processor node cm sample sort distributed radix sort radix sort computation time order maintaining data movement communication time communication computation times algorithms node node cm cm machine relatively low unit computation unit communication ratio 
executed algorithms bit integers intel paragon processors higher unit computation communication ratio cm 
distributed radix sort outperforms radix sort cases 
performance comparable better sample sort especially larger number processors 
bit integers distributed radix sort outperforms algorithms data set sizes processors 
developed new integer sorting algorithm called distributed radix sort 
experimental cm intel paragon suggest superior performance radix sort practical integer sizes practice 
quick sort perform local sorting sample sort gives better performance local radix sort bit elements 
performance algorithm comparable superior sample integer sort small sized integers bit bit especially large data sets number processors 
akl 
parallel sorting algorithms 
academic press toronto 
ranka 
integer sorting algorithms coarse grained parallel machines 
technical report department cise university florida 
ranka shankar 
transportation primitives 
proceedings fifth symposium frontiers massively parallel computation february 
blelloch comparison sorting algorithms connection machine cm 
proc 
symposium parallel algorithms architectures pages july 
cormen leiserson rivest 
algorithms 
mcgraw hill book 
culler fast parallel sorting logp theory practice 
proc 
workshop portability performance parallel processing wiley england 
leonardo dagum 
parallel integer sorting medium fine scale parallelism 
international journal high speed computing 
fox solving problems concurrent processors vol 

prentice hall englewood cliffs nj 
kumar parallel computing design analysis algorithms 
benjamin cummings publishing 
leighton 
tight bounds complexity parallel sorting 
ieee transactions computers april 
hui li kenneth sevcik 
parallel sorting 
technical report csri february 
li versatility parallel sorting regular sampling 
parallel computing october 
rajasekaran reif 
optimal time randomized parallel sorting algorithms 
siam journal computing june 
won sahni 
balanced bin sort hypercube multicomputers 
journal supercomputing 
