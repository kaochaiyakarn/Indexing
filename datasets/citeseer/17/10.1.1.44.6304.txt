journal artificial intelligence research submitted published system induction oblique decision trees sreerama murthy murthy cs jhu edu simon kasif kasif cs jhu edu steven salzberg salzberg cs jhu edu department computer science johns hopkins university baltimore md usa article describes new system induction oblique decision trees 
system oc combines deterministic hill climbing forms randomization find oblique split form hyperplane node decision tree 
oblique decision tree methods tuned especially domains attributes numeric adapted symbolic mixed symbolic numeric attributes 
extensive empirical studies real artificial data analyze oc ability construct oblique trees smaller accurate axis parallel counterparts 
examine benefits randomization construction oblique decision trees 

current data collection technology provides unique challenge opportunity automated machine learning techniques 
advent major scientific projects human genome project hubble space telescope human brain mapping initiative generating enormous amounts data daily basis 
streams data require automated methods analyze filter classify presenting digested form domain scientist 
decision trees particularly useful tool context perform classification sequence simple easy understand tests semantics intuitively clear domain experts 
decision trees classification tasks moret landgrebe 
breiman book classification regression trees cart quinlan id quinlan provided foundations large body research central techniques experimental machine learning 
variants decision tree dt algorithms introduced decade 
concentrated decision trees node checks value single attribute breiman friedman olshen stone quinlan 
quinlan initially proposed decision trees classification domains symbolic valued attributes extended numeric domains 
attributes numeric tests form attributes example constant 
class decision trees may called axis parallel tests node equivalent axis parallel hyperplanes attribute space 
example decision tree shows tree partitioning creates attribute space 
fl ai access foundation morgan kaufmann publishers 
rights reserved 
murthy kasif salzberg left side shows simple axis parallel tree uses attributes 
right side shows partitioning tree creates attribute space 
researchers studied decision trees test node uses boolean combinations attributes pagallo pagallo haussler sahami linear combinations attributes see section 
different methods measuring goodness decision tree nodes techniques pruning tree reduce overfitting increase accuracy explored discussed sections 
examine decision trees test linear combination attributes internal node 
precisely example take form class label real valued attributes 
test node form real valued coefficients 
tests equivalent hyperplanes oblique orientation axes call class decision trees oblique decision trees 
trees form called multivariate brodley utgoff 
prefer term oblique multivariate includes non linear combinations variables curved surfaces 
trees contain linear tests 
clear simply general form axis parallel trees setting coefficients test eq 
familiar univariate test 
note oblique decision trees produce polygonal polyhedral partitionings attribute space axis parallel trees produce partitionings form hyper rectangles parallel feature axes 
intuitively clear underlying concept defined polygonal space partitioning preferable oblique decision trees classification 
example exist domains oblique hyperplanes best model classification 
domains axis parallel methods ap 
constraint xd real valued necessarily restrict oblique decision trees numeric domains 
researchers studied problem converting symbolic unordered domains numeric ordered domains vice versa breiman utgoff brodley van de 
keep discussion simple assume attributes numeric values 
induction oblique decision trees left side shows simple domain oblique hyperplanes define classes 
right side shows approximation sort axis parallel decision tree create model domain 
proximate correct model staircase structure oblique tree building method capture tree smaller accurate 
gives illustration 
breiman suggested method inducing oblique decision trees 
little research trees relatively utgoff brodley heath kasif salzberg murthy kasif salzberg beigel brodley utgoff 
comparison existing approaches detail section 
purpose study review strengths weaknesses existing methods design system combines strengths overcomes weaknesses evaluate system empirically analytically 
main contributions study follows ffl developed new randomized algorithm inducing oblique decision trees examples 
algorithm extends original breiman randomization helps significantly learning concepts 
ffl algorithm fully implemented oblique decision tree induction system available internet 
code retrieved online appendix anonymous ftp ftp ftp cs jhu edu pub oc oc tar 
ffl randomized hill climbing algorithm oc efficient existing randomized oblique decision tree methods described 
fact current implementation oc guarantees worst case running time log times greater worst case time inducing axis parallel trees dn log vs dn 
ffl ability generate oblique trees produces small trees compared axis parallel methods 
underlying problem requires oblique split oblique 
note oblique tree may fewer leaf nodes axis parallel tree mean smaller oblique tree may cases larger terms information content increased complexity tests node 
murthy kasif salzberg trees accurate axis parallel trees 
allowing tree building system oblique axis parallel splits broadens range domains system useful 
remaining sections follow outline remainder section briefly outlines general paradigm decision tree induction discusses complexity issues involved inducing oblique decision trees 
section briefly reviews existing techniques oblique dt induction outlines limitations approach introduces oc system 
section describes oc system detail 
section describes experiments compare performance oc axis parallel oblique decision tree induction methods range real world datasets demonstrate empirically oc significantly benefits randomization methods 
section conclude discussion open problems directions research 
top induction decision trees algorithms inducing decision trees follow approach described quinlan top induction decision trees 
called greedy divide conquer method 
basic outline follows 
set examples called training set examples belong class halt 

consider tests divide subsets 
score test splits examples 

choose greedily test scores highest 

divide examples subsets run procedure recursively subset 
quinlan original model considered attributes symbolic values model test node splits attribute values 
test attribute values child nodes corresponding value 
algorithm considers possible tests chooses optimizes pre defined goodness measure 
split symbolic values subsets values gives choices split examples 
explain oblique decision tree methods consider tests due complexity considerations 
complexity induction oblique decision trees reason relatively papers problem inducing oblique decision trees increased computational complexity problem compared axis parallel case 
important issues addressed 
context top decision tree algorithms address complexity finding optimal separating hyperplanes decision surfaces node decision tree 
optimal hyperplane minimize impurity measure impurity measured total number examples mis classified 
second issue lower bound complexity finding optimal smallest size trees 
induction oblique decision trees points dimensions delta distinct axis parallel splits delta gamma delta distinct dimensional oblique splits 
shows distinct oblique axis parallel splits specific points 
consider issue complexity selecting optimal oblique hyperplane single node tree 
domain training instances described real valued attributes delta gamma delta distinct dimensional oblique splits hyperplanes divide training instances uniquely nonoverlapping subsets 
upper bound derives observation subset size points define dimensional hyperplane hyperplane rotated slightly directions divide set points possible ways 
illustrates upper limits points dimensions 
axis parallel splits delta distinct possibilities axis parallel methods quinlan cart breiman exhaustively search best split node 
problem searching best oblique split difficult searching best axis parallel split 
fact problem np hard 
precisely heath proved problem np hard set labelled examples find hyperplane minimizes number misclassified examples hyperplane 
result implies method finding optimal oblique split exponential cost assuming np 
intuitively problem impractical enumerate delta gamma delta distinct hyperplanes choose best done axis parallel decision trees 
non exhaustive deterministic algorithm searching hyperplanes prone getting stuck local minima 

terms split hyperplane interchangeably refer test node decision tree 
usage standard moret refers fact test splits data partitions 
second usage refers geometric form test 
murthy kasif salzberg hand possible define impurity measures problem finding optimal hyperplanes solved polynomial time 
example minimizes sum distances mis classified examples optimal solution linear programming methods distance measured dimension 
classifiers usually judged points classify correctly regardless close decision boundary point may lie 
standard measures computing impurity base calculation discrete number examples category side hyperplane 
section discusses commonly impurity measures 
address second issue complexity building small tree 
easy show problem inducing smallest axis parallel decision tree np hard 
observation follows directly rivest 
note generate smallest axis parallel tree consistent training set polynomial time number attributes constant 
done dynamic programming branch bound techniques see moret pointers 
tree uses oblique splits clear fixed number attributes generate optimal smallest decision tree polynomial time 
suggests complexity constructing oblique trees greater axis parallel trees 
easy see problem constructing optimal smallest oblique decision tree np hard 
follows blum rivest 
result implies dimensions attributes problem producing node oblique decision tree consistent training set np complete 
specifically show decision problem np complete training set examples boolean attributes exist node neural network consistent 
easy show question np complete training set exist leaf node oblique decision tree consistent 
result complexity considerations took pragmatic approach trying generate small trees looking smallest tree 
greedy approach oc virtually decision tree algorithms implicitly tries generate small trees 
addition easy construct example problems optimal split node lead best tree philosophy embodied oc find locally splits spend excessive computational effort improving quality splits 

previous oblique decision tree induction describing oc algorithm briefly discuss existing oblique dt induction methods including cart linear combinations linear machine decision trees simulated annealing decision trees 
methods induce tree classifiers linear discriminants node notably methods linear programming mangasarian setiono wolberg bennett mangasarian 
methods find optimal linear discriminants specific goodness measures size linear program grows fast number induction oblique decision trees induce split node decision tree normalize values attributes 
true current split fl search ffi maximizes goodness split gamma ffi fl ffi fl settings result highest goodness searches 
gamma ffi gamma ffi fl perturb maximize goodness keeping constant 
sl goodness sl gamma ffl exit loop 
eliminate irrelevant attributes fa backward elimination 
convert split un normalized attributes 
return better best axis parallel split split procedure cart linear combinations cart lc node decision tree 
instances number attributes 
closely related algorithms train artificial neural networks build decision tree classifiers brent cios liu herman yeung 
oblique decision tree algorithm proposed cart linear combinations breiman chapter 
algorithm referred henceforth cart lc important basis oc 
summarizes breiman notation cart lc algorithm node decision tree 
core idea algorithm finds value ffi maximizes goodness split 
idea oc explained detail section 
describing cart lc breiman point room development algorithm 
oc represents extension cart lc includes significant additions 
addresses limitations cart lc ffl cart lc fully deterministic 
built mechanism escaping local minima minima may common domains 
shows simple example cart lc gets stuck 
ffl cart lc produces single tree data set 
ffl cart lc adjustments increase impurity split 
feature probably included allow escape local minima 
ffl upper bound time spent node decision tree 
halts perturbation changes impurity ffl impurity may increase decrease algorithm spend arbitrarily long time node 
murthy kasif salzberg cart lc oc deterministic perturbation algorithm cart lc fails find correct split data starts location best axis parallel split 
oc finds correct split random jump 
oblique decision tree algorithm uses different approach cart lc linear machine decision trees system utgoff brodley brodley utgoff successor perceptron tree method utgoff utgoff brodley 
internal node tree linear machine nilsson 
training algorithm presents examples repeatedly node linear machine converges 
convergence guaranteed uses heuristics determine node stabilized 
training stable set training instances linearly separable thermal training method frean similar simulated annealing 
third system creates oblique trees simulated annealing decision trees sadt heath oc uses randomization 
sadt uses simulated annealing kirkpatrick gelatt find values coefficients hyperplane node tree 
sadt places hyperplane canonical location iteratively perturbs coefficients small random amounts 
initially temperature parameter high sadt accepts perturbation hyperplane regardless changes goodness score 
system changes improve goodness split accepted 
sadt randomization allows effectively avoid local minima compromises efficiency 
runs slower cart lc oc considering tens thousands hyperplanes single node finishes annealing 
experiments section include results showing methods perform artificial domains 
describe way combine strengths methods just mentioned avoiding problems 
algorithm oc uses deterministic hill climbing time ensuring computational efficiency 
addition uses kinds randomization avoid local minima 
limiting number random choices algorithm guaranteed spend polynomial time node tree 
addition randomization produced benefits example means algorithm induction oblique decision trees find split set examples find best axis parallel split impurity split 
repeat times choose random hyperplane 
iteration initialize best axis parallel split 
step impurity measure improve perturb coefficients sequence 
step repeat times choose random direction attempt perturb direction 
reduces impurity go step 
impurity set output split corresponding overview oc algorithm single node decision tree 
produce different trees data set 
offers possibility new family classifiers decision tree algorithms example classified majority vote trees 
heath 
shown decision tree methods call dt consistently outperform single tree methods classification accuracy main criterion 
experiments indicate oc efficiently finds small accurate decision trees different types classification problems 

oblique classifier oc section discuss details oblique decision tree induction system oc 
part description include ffl method finding coefficients hyperplane tree node ffl methods computing impurity goodness hyperplane ffl tree pruning strategy ffl methods coping missing irrelevant attributes 
section focuses complicated algorithmic details question find hyperplane splits set instances reasonably pure nonoverlapping subsets 
randomized perturbation algorithm main novel contribution oc 
summarizes basic oc algorithm node decision tree 
explained sections 
perturbation algorithm oc imposes restrictions orientation hyperplanes 
order powerful standard dt methods finds best axis parallel univariate split node looking oblique split 
oc uses oblique split improves best axis parallel split 

pointed breiman chapter sense oblique split number examples node equal number features murthy kasif salzberg search strategy space possible hyperplanes defined procedure perturbs current hyperplane new location 
exponential number distinct ways partition examples hyperplane procedure simply enumerates unreasonably costly 
main alternatives considered past simulated annealing sadt system heath deterministic heuristic search cart lc breiman 
oc combines ideas heuristic search finds local minimum non deterministic search step get local minimum 
nondeterministic step oc simulated annealing 
start explaining perturb hyperplane split training set node decision tree 
number examples number attributes dimensions example number categories 
write jd jth example training set ji value attribute category label 
defined eq 
equation current hyperplane node decision tree written 
substitute point example equation get ji sign tells point hyperplane splits training set perfectly points belonging category sign sign sign iff category category 
oc adjusts coefficients individually finding locally optimal value coefficient time 
key idea introduced breiman works follows 
treat coefficient am variable treat coefficients constants 
viewed function am particular condition equivalent am jm gamma jm def assuming jm ensure normalization 
definition point am 
plugging points equation obtain constraints value am problem find value am satisfies constraints possible 
constraints satisfied perfect split 
problem easy solve optimally simply sort values consider setting am midpoint pair different values 
illustrated 
categories indicated font size larger belong category smaller 
distinct placement coefficient am oc computes impurity resulting split location illustrated examples left example right misclassified see section different ways computing impurity 
illustrates problem simply find best dimensional split requires considering just gamma values am value obtained solving dimensional problem considered data concept 
default oc uses axis parallel splits tree nodes 
user vary threshold 
induction oblique decision trees finding optimal value single coefficient am large correspond examples category small 
perturb compute eq 
sort un non decreasing order 
best univariate split sorted result substituting am impurity impurity am impurity impurity am probability gamma perturbation algorithm single coefficient am replacement am hyperplane obtained perturbing am better lower impurity discarded 
lower impurity new location hyperplane 
identical impurities replaces probability contains pseudocode perturbation procedure 
method locally improving coefficient hyperplane need decide coefficients pick perturbation 
experimented different methods choosing coefficient adjust sequential best random 
seq repeat coefficient values modified loop perturb best repeat coefficient remains unmodified coefficient perturbed results maximum improvement impurity measure 
perturb repeat fixed number times experiments random integer perturb 
parameter denoting stagnation probability probability hyperplane perturbed location change impurity measure 
prevent impurity remaining long time decreases constant amount time oc perturbation constant number perturbations occur node 
constant set user 
reset time global impurity measure improved 
murthy kasif salzberg previous experiments murthy indicated order perturbation coefficients affect classification accuracy parameters especially randomization parameters see 
orders uniformly better sequential seq perturbation experiments reported section 
randomization perturbation algorithm halts split reaches local minimum impurity measure 
oc search space local minimum occurs perturbation single coefficient current hyperplane decrease impurity measure 
course local minimum may global minimum 
implemented ways attempting escape local minima perturbing hyperplane random vector re starting perturbation algorithm different random initial hyperplane 
technique perturbing hyperplane random vector works follows 
system reaches local minimum chooses random vector add coefficients current hyperplane 
computes optimal amount hyperplane perturbed random direction 
precise hyperplane improved deterministic perturbation oc repeats loop times user specified parameter set default 
ffl choose random vector 
ffl ff amount want perturb direction words ffr ffr 
ffl find optimal value ff 
ffl hyperplane obtained decreases impurity replace exit loop deterministic perturbation algorithm individual coefficients 
note treat ff variable equation examples plugged equation imposes constraint value ff 
oc coefficient perturbation method see section compute best value ff 
random jumps fail improve impurity oc halts uses split current tree node 
intuitive way understanding random jump look dual space algorithm searching 
note equation defines space axes coefficients attributes point space defines distinct hyperplane original formulation 
deterministic algorithm oc picks hyperplane adjusts coefficients time 
dual space oc chooses point perturbs moving parallel axes 
random vector represents random direction space 
finding best value ff oc finds best distance adjust hyperplane direction induction oblique decision trees note additional perturbation random direction significantly increase time complexity algorithm see appendix 
experiments single random jump local minimum proves helpful 
classification accuracy improved data sets perturbations 
see section examples 
second technique avoiding local minima variation idea performing multiple local searches 
technique multiple local searches natural extension local search widely mentioned optimization literature see roth early example 
steps perturbation algorithm deterministic initial hyperplane largely determines local minimum encountered 
perturbing single initial hyperplane lead best split data set 
cases random perturbation method fails escape local minima may helpful simply start afresh new initial hyperplane 
word restart denote run perturbation algorithms node decision tree random initial hyperplane 
restart cycles perturbs coefficients time tries perturb hyperplane random direction algorithm reaches local minimum 
perturbation reduces impurity algorithm goes back perturbing coefficients time 
restart ends deterministic local search random jump find better split 
optional parameters oc specifies restarts 
restart best hyperplane far saved 
experiments classification accuracies increased restart 
accuracy tended increase point level restarts depending domain 
multiple initial hyperplanes substantially improved quality decision trees see section examples 
carefully combining hill climbing randomization oc ensures worst case time dn log inducing decision tree 
see appendix derivation upper bound 
best axis parallel split 
clear axis parallel splits suitable data distributions oblique splits 
take account distributions oc computes best axis parallel split oblique split node picks better 
calculating best axis parallel split takes additional dn log time increase asymptotic time complexity oc 
simple variant oc system user opt switch oblique perturbations building axis parallel tree training data 
section empirically demonstrates axis parallel variant oc compares favorably existing axis parallel algorithms 

run algorithm node begins location best axis parallel hyperplane subsequent restarts random locations 

simple axis parallel split preferable oblique split oblique split slightly lower impurity 
user specify bias input parameter oc 
murthy kasif salzberg details impurity measures oc attempts divide dimensional attribute space homogeneous regions regions contain examples just category 
goal adding new nodes tree split sample space minimize impurity training set 
algorithms measure goodness impurity difference goodness values maximized impurity minimized 
different measures impurity studied breiman quinlan mingers buntine niblett fayyad irani heath 
oc system designed large class impurity measures 
stated simply impurity measure uses counts examples belonging category sides split oc 
see murthy salzberg ways mapping kinds impurity measures class impurity measures 
user plug impurity measure fits description 
oc implementation includes impurity measures 
information gain 
gini index 
rule 
max minority 
sum minority 
sum variances measures defined literature cases slight modifications defined precisely appendix experiments indicated average information gain gini index rule perform better measures axis parallel oblique trees 
rule current default impurity measure oc experiments reported section 
artificial data sets sum minority max minority perform better rest measures 
instance sum minority easily induces exact tree pol data set described section methods difficulty finding best tree 
rule 
rule proposed breiman 

value computed defined jt jt jl jt gamma jt jj jt jt number examples left right split node number examples node number examples category left right split 
goodness measure impurity measure 
oc attempts minimize reciprocal value 
remaining impurity measures implemented oc defined appendix induction oblique decision trees pruning virtually decision tree induction systems prune trees create order avoid overfitting data 
studies judicious pruning results smaller accurate classifiers decision trees types machine learning systems quinlan niblett cestnik kononenko bratko kodratoff cohen hassibi stork wolpert schaffer 
oc system implemented existing pruning method note tree pruning method fine oc 
experimental evaluations mingers cited chose breiman cost complexity cc pruning default pruning method oc 
method called error complexity weakest link pruning requires separate pruning set 
pruning set randomly chosen subset training set approximated cross validation 
oc randomly chooses default value training data pruning 
experiments reported default value 
briefly idea cc pruning create set trees decreasing size original complete tree 
trees classify pruning set accuracy estimated 
cc pruning chooses smallest tree accuracy standard errors squared best accuracy obtained 
se rule tree highest accuracy pruning set selected 
smaller tree size preferred higher accuracy 
details cost complexity pruning see breiman 
mingers 
irrelevant attributes irrelevant attributes pose significant problem machine learning methods breiman aha dietterich kira rendell salzberg cardie schlimmer langley sage brodley utgoff 
decision tree algorithms axis parallel ones confused irrelevant attributes 
oblique decision trees learn coefficients attribute dt node hope values chosen coefficient reflect relative importance corresponding attributes 
clearly process searching coefficient values efficient fewer attributes search space smaller 
reason oblique dt induction methods benefit substantially feature selection method algorithm selects subset original attribute set conjunction coefficient learning algorithm breiman brodley utgoff 
currently oc built mechanism select relevant attributes 
easy include standard methods stepwise forward selection stepwise backward selection ad hoc method select features running tree building process 
example separate experiments data hubble space telescope salzberg ford murthy white feature selection methods preprocessing step oc reduced number attributes 
resulting decision trees simpler accurate 
currently underway incorporate efficient feature selection technique oc system 
murthy kasif salzberg regarding missing values example missing value attribute oc uses mean value attribute 
course techniques handling missing values considered study 

experiments section sets experiments support claims 

oc compares favorably variety real world domains existing axis parallel oblique decision tree induction methods 

randomization form multiple local searches random jumps improves quality decision trees produced oc 
experimental method experiments described section 
sections describe experiments corresponding claims 
experimental section begins description data sets presents experimental results discussion 
experimental method fold cross validation cv experiments estimate classification accuracy 
fold cv experiment consists steps 

randomly divide data equal sized disjoint partitions 

partition build decision tree data outside partition test tree data partition 

sum number correct classifications trees divide total number instances compute classification accuracy 
report accuracy average size trees 
entry tables result fold cv experiments result tests decision trees 
fold cross validations different random partitioning data 
entry tables reports mean standard deviation classification accuracy followed mean standard deviation decision tree size measured number leaf nodes 
results high values accuracy low values tree size small standard deviations 
addition oc included experiments axis parallel version oc considers axis parallel hyperplanes 
call version described section oc ap 
experiments oc oc ap rule section measure impurity 
parameters oc took default values stated 
defaults include number restarts node 
number random jumps attempted local minimum 
order coefficient perturbation sequential 
pruning method cost complexity se rule training set exclusively pruning 
comparison oblique version cart algorithm cart lc 
implemented version cart lc description breiman 
chapter may differences version induction oblique decision trees versions system note cart lc freely available 
implementation cart lc measured impurity rule se cost complexity pruning separate test set just oc 
include feature selection methods cart lc oc implement normalization 
cart coefficient perturbation algorithm may alternate indefinitely locations hyperplane see section imposed arbitrary limit perturbations forcing perturbation algorithm halt 
included axis parallel cart comparisons 
implementations algorithms ind package buntine 
default cart styles defined package altering parameter settings 
cart style uses rule se cost complexity pruning fold cross validation 
pruning method impurity measure defaults style described quinlan 
oc vs decision tree induction methods table compares performance oc known decision tree induction methods plus oc ap different real world data sets 
section consider artificial data concept definition precisely characterized 
description data sets star galaxy discrimination 
data sets came large set astronomical images collected 
pennington humphreys 
study images train artificial neural networks running perceptron back propagation algorithms 
goal classify example star galaxy 
image characterized real valued attributes attributes measurements defined astronomers relevant task 
objects image divided bright dim data sets image intensity values dim images inherently difficult classify 
note bright objects bright relation data set 
actuality extremely faint visible powerful telescopes 
bright set contains objects dim set contains objects 
addition results reported table results appeared star galaxy data 

reported accuracy accuracy bright objects dim ones noted study single training test set partition 
heath reported accuracy bright objects sadt average tree size leaves 
study single training test set 
salzberg reported accuracies bright objects dim objects nearest neighbor nn coupled feature selection method reduces number features 
breast cancer diagnosis 
mangasarian bennett compiled data problem diagnosing breast cancer test new classification methods mangasarian bennett mangasarian 
data represents set patients breast cancer patient characterized numeric attributes plus diagnosis tumor benign malignant 
data set currently entries murthy kasif salzberg algorithm bright dim cancer iris housing diabetes oc sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma cart lc sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma oc ap sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma cart ap sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma table comparison oc decision tree induction methods different data sets 
line method gives accuracies second line gives average tree sizes 
highest accuracy domain appears boldface 
available uc irvine machine learning repository murphy aha 
heath 
reported accuracy subset data set instances average decision tree size nodes sadt 
salzberg reported accuracy nn smaller data set 
herman yeung reported accuracy piece wise linear classification somewhat smaller data set 
classifying irises 
fisher famous iris data extensively studied statistics machine learning literature 
data consists examples example described numeric attributes 
examples different types iris flower 
weiss obtained accuracies data back propagation nn respectively 
housing costs boston 
data set available part uci ml repository describes housing values boston function continuous attributes binary attribute harrison rubinfeld 
category variable median value owner occupied homes continuous discretized category value 
uses data see quinlan 
diabetes diagnosis 
data catalogs presence absence diabetes pima indian females years older function numeric valued attributes 
original source data national institute diabetes kidney diseases available uci repository 
smith 
reported accuracy data adap learning algorithm different experimental method 
induction oblique decision trees discussion table shows data sets considered oc consistently finds better trees original oblique cart method 
accuracy greater domains difference significant standard deviations dim star galaxy problem 
average tree sizes roughly equal domains dim stars galaxies oc considerably smaller trees 
differences analyzed quantified artificial data section 
decision tree induction methods oc highest accuracy domains bright stars dim stars cancer diagnosis diabetes diagnosis 
remaining domains oc second highest accuracy case 
surprisingly oblique methods oc cart lc generally find smaller trees methods 
difference quite striking domains note example oc produced tree just nodes average dim star galaxy problem produced tree nodes times larger 
course domains axis parallel tree appropriate representation axis parallel methods compare oblique methods terms tree size 
fact iris data methods similar sized trees 
randomization helps oc second set experiments examine closely effect introducing randomized steps algorithm finding oblique splits 
experiments demonstrate oc ability produce accurate tree set training data clearly enhanced kinds randomization uses 
precisely artificial data sets underlying concept known experimenters show oc performance improves substantially deterministic hill climbing augmented ways ffl multiple restarts random initial locations ffl perturbations random directions local minima ffl randomization steps 
order find clear differences algorithms needs know concept underlying data difficult learn 
simple concepts say linearly separable classes different learning algorithms produce accurate classifiers advantages randomization may detectable 
known commonly data sets uci repository easy learn simple representations holte data sets may ideal purposes 
created number artificial data sets different problems learning know correct concept definition 
allows quantify precisely parameters algorithm affect performance 
second purpose experiment compare oc search strategy existing oblique decision tree induction systems brodley utgoff sadt heath 
show quality trees induced oc better trees induced existing systems murthy kasif salzberg artificial domains 
show oc achieves balance amount effort expended search quality tree induced 
sadt information gain experiment 
change oc default measure rule observed experiments reported oc information gain produce significantly different results 
maximum number successive unproductive perturbations allowed node set sadt 
parameters default settings provided systems 
description artificial data ls ls data set instances divided categories 
instance described attributes values uniformly distributed range 
data linearly separable hyperplane name ls defined equation instances generated randomly labelled side hyperplane fell 
oblique dt induction methods intuitively prefer linear separator exists interesting compare various search techniques data set know separator exists 
task relatively simple lower dimensions chose dimensional data difficult 
pol data set shown 
instances dimensions divided categories 
underlying concept set parallel oblique lines name pol dividing instances homogeneous regions 
concept difficult learn single linear separator minimal size tree quite small 
rcb rcb stands rotated checker board data set subject experiments hard classification problems decision trees murthy salzberg 
data set shown instances belonging categories 
concept difficult learn axis parallel method obvious reasons 
quite difficult oblique methods reasons 
biggest problem correct root node shown separate class 
impurity measures sum minority fail miserably problem rule better 
problem deterministic coefficient perturbation algorithm get stuck local minima places data set 
table summarizes results experiment smaller tables data set 
smaller table compare variants oc sadt 
different results oc obtained varying number restarts number random jumps 
random jumps random jumps tried local minimum 
soon improved impurity current hyperplane algorithm moved hyperplane started running deterministic perturbation procedure 
random jumps improved impurity search halted restarts tried 
training test partitions methods cross validation run recall results induction oblique decision trees root rr rrr root ll lr rl rr pol rcb data sets linearly separable ls data accuracy size hyperplanes sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sadt sigma sigma parallel oblique lines pol data accuracy size hyperplanes sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sadt sigma sigma rotated checker board rcb data accuracy size hyperplanes sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sadt sigma sigma table effect randomization oc 
column labelled shows number restarts followed maximum number random jumps attempted oc local minimum 
results sadt included comparison variants oc 
size average tree size measured number leaf nodes 
third column shows average number hyperplanes algorithm considered building tree 
murthy kasif salzberg average fold cvs 
trees pruned algorithms data noise free furthermore emphasis search 
table includes number hyperplanes considered algorithm building complete tree 
note oc sadt number hyperplanes considered generally larger number perturbations algorithms compare newly generated hyperplanes existing hyperplanes adjusting existing 
number estimate effort algorithm expends new hyperplane evaluated impurity measure 
number hyperplanes considered identical actual number perturbations 
discussion oc results quite clear 
line table labelled gives accuracies tree sizes randomization variant similar cart lc algorithm 
increase randomization accuracy increases tree size decreases exactly result hoped decided introduce randomization method 
looking closely tables ask effect random jumps 
illustrated second line table attempted random jumps local minimum restarts 
accuracy increased domain tree size decreased dramatically roughly factor pol rcb domains 
note noise domains high accuracies expected 
increases percent accuracy possible 
looking third line sub table table see effect multiple restarts oc 
restarts random jumps escape local minima improvement noticeable ls data random jumps 
data set accuracy jumped significantly tree size dropped nodes 
pol rcb data improvements comparable obtained random jumps 
rcb data tree size dropped factor leaf nodes leaf nodes accuracy increased 
fourth line table shows effect randomized steps 
oc entries line highest accuracies smallest trees data sets clear randomization big win kinds problems 
addition note smallest tree rcb data leaf nodes oc average trees pruning just leaf nodes 
clear data set thought difficult oc came close finding optimal tree nearly run 
recall numbers table average fold cv experiments average decision trees 
ls data show difficult find simple concept higher dimensions optimal tree just single hyperplane nodes oc unable find current parameter settings 
pol data required minimum leaf nodes oc tree time seen table 
shown table 
separate experiment oc consistently finds linear separator ls data restarts random jumps 
induction oblique decision trees oc sum minority performed better pol data rule impurity measure correct tree time 
results sadt data lead interesting insights 
surprisingly linearly separable ls data require inordinate amount search 
clearly data linearly separable method linear programming 
oc sadt difficulty finding linear separator experiments oc eventually find sufficient time 
hand non linearly separable data sets produces larger trees significantly accurate produced oc sadt 
deterministic variant oc zero restarts zero random jumps outperforms problems search 
sadt produces accurate trees main weakness enormous amount search time required roughly times greater oc setting 
explanation oc advantage directed search opposed strictly random search simulated annealing 
table shows oc randomization quite effective non linearly separable data 
natural ask randomization helps oc task inducing decision trees 
researchers combinatorial optimization observed randomized search usually succeeds search space holds abundance solutions gupta smolka bhaskar 
furthermore randomization improve deterministic search local maxima search space lead poor solutions 
oc search space local maximum hyperplane improved deterministic search procedure solution complete decision tree 
significant fraction local maxima lead bad trees algorithms local maximum encounter perform poorly 
randomization allows oc consider different local maxima modest percentage maxima lead trees chance finding trees 
experiments oc far indicate space oblique hyperplanes usually contains numerous local maxima substantial percentage locally hyperplanes lead decision trees 

described oc new system constructing oblique decision trees 
shown experimentally oc produce classifiers range real world artificial domains 
shown randomization improves original algorithm proposed breiman 
significantly increasing computational cost algorithm 
randomization beneficial axis parallel tree methods 
note find optimal test respect impurity measure node tree complete tree may optimal known problem finding smallest tree np complete rivest 
axis parallel decision tree methods produce ideal decision trees 
quinlan suggested windowing algorithm way introducing randomization algorithm designed purpose quinlan 
windowing murthy kasif salzberg algorithm selects random subset training data builds tree 
believe randomization powerful tool context decision trees experiments just example exploited 
process conducting experiments quantify accurately effects different forms randomization 
clear ability produce oblique splits node broadens capabilities decision tree algorithms especially regards domains numeric attributes 
course axis parallel splits simpler sense description split uses attribute node 
oc uses oblique splits impurity impurity best axis parallel split easily penalize additional complexity oblique split 
remains open area research 
general point domain best captured tree uses oblique hyperplanes desirable system generate tree 
shown problems including experiments oc builds small decision trees capture domain 
appendix complexity analysis oc show oc runs efficiently worst case 
data set examples points attributes example oc uses dn log time 
assume analysis 
analysis assume coefficients hyperplane adjusted sequential order seq method described 
number restarts node number random jumps tried constants fixed advance running algorithm 
initializing hyperplane random position takes just time 
need consider maximum amount oc finds new location hyperplane 
need consider times move hyperplane 

attempting perturb coefficient takes dn log time 
computing points equation requires dn time sorting takes log 
gives dn log 

perturbing improve things try perturb computing new take just time term different re sorting take log step takes log log time 

likewise take log additional time assuming better hyperplane checking coefficient 
total time cycle attempt perturb additional coefficients gamma logn dn log 

summing time cycle coefficients dn log dn log dn log 

coefficients improved split attempt random jumps 
constant just consider analysis 
step induction oblique decision trees involves choosing random vector running perturbation algorithm solve ff explained section 
need compute set sort takes dn log time 
amount time dominated time adjust coefficients total time far dn log 
time oc spend node halting finding improved hyperplane 

assuming oc sum minority max minority error measure reduce impurity hyperplane times 
clear improvement means example correctly classified new hyperplane 
total amount node limited dn log dn log 
analysis extends linear cost factors information gain gini index rule categories 
apply measure example uses distances mis classified objects hyperplane 
practice number improvements node smaller assuming oc adjusts hyperplane improves impurity measure dn log worst case 
oc allows certain number adjustments hyperplane improve impurity accept change worsens impurity 
number allowed determined constant known perturbations 
value works follows 
time oc finds new hyperplane improves old resets counter zero 
move new hyperplane different location equal impurity times 
moves repeats perturbation algorithm 
impurity reduced re starts counter allows moves equally locations 
clear feature just increases worst case complexity oc constant factor note cost oc dn log upper bound total running time oc independent size tree ends creating 
upper bound applies sum minority max minority open question similar upper bound proven information gain gini index 
worst case asymptotic complexity system comparable systems construct axis parallel decision trees dn worst case complexity 
sketch intuition leads bound total impurity summed leaves partially constructed tree sum currently misclassified points tree 
observe time run perturbation algorithm node tree halt improve unit 
worst case analysis node realized perturbation algorithm run examples happens longer mis classified examples tree complete 
appendix definitions impurity measures available oc addition rule defined text oc contains built definitions additional impurity measures defined follows 
definitions murthy kasif salzberg set examples node split contains 
instances belong categories 
initially set entire training set 
hyperplane divides non overlapping subsets tl tr left right 
number instances category tl tr respectively 
impurity measures initially check see tl tr homogeneous examples belong category return minimum zero impurity 
information gain 
measure information gained particular split popularized context decision trees quinlan 
quinlan definition information gain goodness measure maximize 
oc attempts minimize impurity measure uses reciprocal standard value information gain oc implementation 
gini index 
gini criterion index proposed decision trees breiman 

gini index originally defined measures probability misclassification set instances impurity split 
implement variation gamma jt gamma jt impurity jt jt gini index left side hyperplane right 
max minority 
measures max minority sum minority sum variances defined context decision trees heath kasif salzberg 
max minority theoretical advantage tree built minimizing measure depth log experiments indicated great advantage practice seldom impurity measures produce trees substantially deeper produced max minority 
definition maxl maxr max minority max 
sum variances called sum impurities heath induction oblique decision trees sum minority 
measure similar max minority 
defined max minority measure sum minority just sum values 
measure simplest way quantifying impurity simply counts number misclassified instances 
sum minority performs domains obvious flaws 
example consider domain examples numeric attribute classes 
suppose examples sorted single attribute instances belong category followed instances category followed instances category 
possible splits distribution sum minority 
impossible sum minority distinguish split preferable splitting alternations categories clearly better 
sum variances 
definition measure jt cat gamma jt cat tl jt jt cat gamma jt cat tr jt sum variances cat category instance measure computed actual class labels easy see impurity computed varies depending numbers assigned classes 
instance consists points category points category consists points category points category sum variances values different avoid problem oc uniformly category numbers frequency occurrence category node computing sum variances 
authors richard beigel yale university suggesting idea jumping random direction 
wray buntine nasa ames research center providing ind package carla brodley providing code david heath providing sadt code assisting 
anonymous reviewers helpful suggestions 
material supported national science foundation nos 
iri iri iri 
aha 

study instance algorithms supervised learning mathematical empirical psychological evaluations 
ph thesis department information computer science university california irvine 
murthy kasif salzberg dietterich 

learning irrelevant features 
proceedings ninth national conference artificial intelligence pp 

san jose ca 


regression diagnostics identifying influential data sources collinearity 
wiley sons new york 
bennett mangasarian 

robust linear programming discrimination linearly inseparable sets 
optimization methods software 
bennett mangasarian 

multicategory discrimination linear programming 
optimization methods software 
bennett mangasarian 

serial parallel multicategory discrimination 
siam journal optimization 
blum rivest 

training node neural network np complete 
proceedings workshop computational learning theory pp 

boston ma 
morgan kaufmann 
breiman friedman olshen stone 

classification regression trees 
wadsworth international group 
brent 

fast training algorithms multilayer neural nets 
ieee transactions neural networks 
brodley utgoff 

multivariate versus univariate decision trees 
tech 
rep coins cr dept computer science university massachusetts amherst 
brodley utgoff 

multivariate decision trees 
machine learning appear 
buntine 

tree classification software 
technology third national technology transfer conference exposition 
buntine niblett 

comparison splitting rules decision tree induction 
machine learning 
cardie 

decision trees improve case learning 
proceedings tenth international conference machine learning pp 

university massachusetts amherst 
cestnik kononenko bratko 

assistant knowledge acquisition tool sophisticated users 
bratko lavrac 
eds progress machine learning 
sigma press 
cios liu 

machine learning method generation neural network architecture continuous id algorithm 
ieee transactions neural networks 
induction oblique decision trees cohen 

efficient pruning methods separate conquer rule learning systems 
proceedings th international joint conference artificial intelligence pp 

morgan kaufmann 
fayyad irani 

attribute specification problem decision tree generation 
proceedings tenth national conference artificial intelligence pp 

san jose ca 
aaai press 
frean 

small nets short paths optimising neural computation 
ph thesis centre cognitive science university edinburgh 
gupta smolka bhaskar 

randomization sequential distributed algorithms 
acm computing surveys 


linear function neurons structure training 
biological cybernetics 
harrison rubinfeld 

hedonic prices demand clean air 
journal environmental economics management 
hassibi stork 

second order derivatives network pruning optimal brain surgeon 
advances neural information processing systems pp 

morgan kaufmann san mateo ca 
heath 

geometric framework machine learning 
ph thesis johns hopkins university baltimore maryland 
heath kasif salzberg 

dt multi tree learning method 
proceedings second international workshop multistrategy learning pp 

ferry wv 
george mason university 
heath kasif salzberg 

learning oblique decision trees 
proceedings th international joint conference artificial intelligence pp 

chambery france 
morgan kaufmann 
herman yeung 

piecewise linear classification 
ieee transactions pattern analysis machine intelligence 
holte 

simple classification rules perform commonly datasets 
machine learning 
rivest 

constructing optimal binary decision trees npcomplete 
information processing letters 
kira rendell 

practical approach feature selection 
proceedings ninth international conference machine learning pp 

aberdeen scotland 
morgan kaufmann 
kirkpatrick gelatt 

optimization simulated annealing 
science 
murthy kasif salzberg kodratoff 

generalization noise 
international journal man machine studies 
langley sage 

scaling domains irrelevant features 
learning systems department siemens corporate research princeton nj 
mangasarian setiono wolberg 

pattern recognition linear programming theory application medical diagnosis 
siam workshop optimization 
mingers 

empirical comparison pruning methods decision tree induction 
machine learning 
mingers 

empirical comparison selection measures decision tree induction 
machine learning 
moret 

decision trees diagrams 
computing surveys 
murphy aha 

uci repository machine learning databases machinereadable data repository 
maintained department information computer science university california irvine 
anonymous ftp ics uci edu directory pub machine learning databases 
murthy kasif salzberg beigel 

oc randomized induction oblique decision trees 
proceedings eleventh national conference artificial intelligence pp 

washington mit press 
murthy salzberg 

structure improve decision trees 
tech 
rep jhu department computer science johns hopkins university 
niblett 

constructing decision trees noisy domains 
bratko lavrac 
eds progress machine learning 
sigma press england 
nilsson 

learning machines 
morgan kaufmann san mateo ca 
pennington humphreys 

automated star galaxy neural networks 
astronomical journal 
pagallo 

adaptive decision tree algorithms learning examples 
ph thesis university california santa cruz 
pagallo haussler 

boolean feature discovery empirical learning 
machine learning 
quinlan 

learning efficient classification procedures application chess games 
michalski carbonell mitchell 
eds machine learning artificial intelligence approach 
morgan kaufmann san mateo ca 
quinlan 

induction decision trees 
machine learning 
induction oblique decision trees quinlan 

simplifying decision trees 
international journal man machine studies 
quinlan 

programs machine learning 
morgan kaufmann publishers san mateo ca 
quinlan 

combining instance model learning 
proceedings tenth international conference machine learning pp 
university massachusetts amherst 
morgan kaufmann 
roth 

approach solving linear discrete optimization problems 
journal acm 
landgrebe 

survey decision tree classifier methodology 
ieee transactions systems man cybernetics 
sahami 

learning non linearly separable boolean functions linear threshold unit trees madaline style networks 
proceedings eleventh national conference artificial intelligence pp 

aaai press 
salzberg 

nearest hyperrectangle learning method 
machine learning 
salzberg 

combining learning search create classifiers 
tech 
rep jhu johns hopkins university baltimore md salzberg ford murthy white 

decision trees automated identification cosmic rays hubble space telescope images 
publications astronomical society pacific appear 
schaffer 

overfitting avoidance bias 
machine learning 
schlimmer 

efficiently inducing determinations complete systematic search algorithm uses optimal pruning 
proceedings tenth international conference machine learning pp 

morgan kaufmann 
smith dickson johannes 

adap learning algorithm forecast onset diabetes 
proceedings symposium computer applications medical care pp 

ieee computer society press 
utgoff 

perceptron trees case study hybrid concept representations 
connection science 
utgoff brodley 

incremental method finding multivariate splits decision trees 
proceedings seventh international conference machine learning pp 

los altos ca 
morgan kaufmann 
utgoff brodley 

linear machine decision trees 
tech 
rep university massachusetts amherst 
murthy kasif salzberg van de 

system learns flexible concepts decision trees numerical attributes 
proceedings ninth international workshop machine learning pp 

van de 

decision trees numerical attribute spaces 
proceedings th international joint conference artificial intelligence pp 

weiss 

empirical comparison pattern recognition neural nets machine learning classification methods 
proceedings th international joint conference artificial intelligence pp 

detroit mi 
morgan kaufmann 
wolpert 

overfitting avoidance bias 
tech 
rep sfi tr santa fe institute santa fe new mexico 
