acta numerica pp 
fl cambridge university press approximation theory mlp model neural networks allan pinkus department mathematics technion israel institute technology haifa israel mail pinkus tx technion ac il survey discuss various approximation theoretic problems arise multilayer feedforward perceptron mlp model neural networks 
mlp model popular practical neural network models 
mathematically simpler models 
mathematics model understood problems approximation theoretic character 
research discuss vintage 
report done various unanswered questions 
presenting practical algorithmic methods 
exploring capabilities limitations model 
sections brief overview neural networks multilayer feedforward perceptron model 
section discuss great detail question density 
model theoretical ability approximate reasonable function 
section conditions simultaneously approximating function derivatives 
section considers interpolation capability model 
section study upper lower bounds order approximation model 
material sections treats single hidden layer mlp model 
section discuss differences arise considering hidden layer 
lengthy list includes papers cited text relevant subject matter survey 
pinkus contents neural networks mlp model density derivative approximation interpolation degree approximation hidden layers 
neural networks assumed readers pure applied mathematicians conversant theory neural networks 
survey brief inadequate 
question neural network ill posed 
quick glance literature quickly realizes universally accepted definition theory neural networks 
generally agreed neural network theory collection models computation loosely biological motivations 
haykin neural network massively parallel distributed processor natural propensity storing experiential knowledge making available 
resembles brain respects 
knowledge acquired network learning process 

interneuron connection strengths known synaptic weights store knowledge 
highly formulation 
try bit heuristic 
neural network models certain common characteristics 
models set inputs process results corresponding set outputs basic underlying assumption models process mathematical function function function may complicated 
importantly expect able compute exactly unknown choose candidate parametrized set functions set examples inputs associated correct outputs assume help choose parameters 
general framework 
fact approximation theory mlp model neural networks general 
neural network models may considered particular choices classes functions parameters various rules regulations specific procedures optimizing choice parameters 
people agree neural network input output system simple processors having small amount local memory 
units connected communication channels carrying data 
neural network models sort training rule learn trained set examples 
different models neural network 
lists different recognized neural network models plethora additional candidates 
neural networks emerged emerging practical technology successfully applied real world problems 
applications pattern recognition pattern classification function approximation large set available examples training set 
bishop importance neural networks context offer powerful general framework representing non linear mappings input variables output variables form mapping governed number adjustable parameters 
nonlinearity neural network models presents advantages disadvantages 
price cost procedure determining values parameters problem nonlinear optimization tends computationally intensive complicated 
problem finding efficient algorithms vital importance true utility model crucially depends efficiency 
issue consider survey 
theory neural nets increasing popular fields computer science statistics engineering especially electrical engineering physics directly applicable areas 
major journals field numerous minor journals 
leading journals ieee transactions neural networks neural computation neural networks neurocomputing 
similarly dozens textbooks theory 
listed books haykin bishop ripley devroye lugosi bos appeared years 
ieee generally sponsored annual conferences neural networks 
proceedings run pages contains articles abstracts 
quick search mathematical reviews turned mere entries phrase neural network entered realize neural network literature including mentioned journals pinkus written mathematicians reviewed mathematical reviews 
words active research area deserves attention readership acta numerica 
initially definite lack mathematical sophistication theory 
tended collection ad hoc techniques debatable justifications 
pure mathematician author reading early literature field alien experience 
years professionals especially statisticians established organized framework theory 
reader acquire balanced enlarged view theory neural networks urged abovementioned texts 
additional excellent source information neural networks literature frequently asked questions faqs usenet newsgroup comp ai neural nets see 
survey neural networks se approximation theory multilayer feedforward perceptron mlp model neural networks 
consider certain mathematical computational statistical problems associated widely neural net model 
explicitly shall concern problems density models theoretical capability providing approximations degree approximation extent approximate function number parameters interpolation related issues 
theoretical results survey usually direct applications 
fact far removed practical considerations 
meant tell possible equally importantly 
meant explain certain things occur highlighting salient characteristics useful 
tried provide proofs results surveyed 
issue acta numerica contained detailed survey aspects numerical analysis neural networks 
years elapsed editors opted solicit survey time albeit slightly altered emphasis related neural networks 
unwarranted 
half survey devoted approximation theoretic results neural networks results superseded 
hoped said years 

mlp model conceptually attractive neural network models multilayer feedforward perceptron mlp model 
basic form model consisting finite number successive layers 
layer approximation theory mlp model neural networks consists finite number units called neurons 
unit layer connected unit subsequent previous layer 
connections generally called links synapses 
information flows layer subsequent layer term feedforward 
layer called input layer consists input 
intermediate layers called hidden layers 
resulting output obtained layer surprisingly called output layer 
rules regulations governing model 

input layer output jth unit input value 
kth unit ith layer receives output ij jth unit gamma st layer 
values ij multiplied constants called weights ijk products summed 

shift ik called threshold bias fixed mapping oe called activation function applied sum resulting value represents output kth unit ith layer oe ij gamma ik priori typically fixes reasons activation function number layers number units layer 
step choose way values weights ijk thresholds ik values generally chosen model behaves set inputs associated outputs 
called training set 
process determining weights thresholds called learning training 
multilayer feedforward perceptron model basic learning algorithm called backpropagation 
backpropagation gradient descent method 
extremely important model neural network theory 
shall detail algorithm numerous numerical difficulties involved 
classify multilayer feedforward perceptron models number layers number hidden layers number layers excluding input output layer 
evident neural network theory terminology 
unfortunately true terminology consistent logical 
example term multilayer perceptron generically applied model hidden layer 
hand word perceptron coined rosenblatt hidden layer model specific activation function heaviside function oe ae pinkus oe fires fire breakpoint threshold 
activation function model referred mcculloch pitts model 
mathematically zero hidden layer perceptron network confusingly termed single layer feedforward network follows 
assume inputs outputs output oe jk gamma choice oe jk hidden layer perceptron network generally longer problems linear separation 
simple mathematical rationale 
function form constant certain parallel hyperplanes limited 
example assume output oe increasing function 
input output oe gamma assume inputs lie straight line 
easily seen output values interpolated approximated 
example assume lie opposite sides line set 
solve oe gamma gamma delta choice 
fact difference associated output 
totally unacceptable wishes build network approximate reasonable function classify points different criteria 
heaviside activation function hidden layer sets points separated classified model linearly separable 
hidden layers necessary 
problem able arbitrarily separate generic points sets hidden layer perceptron model heaviside activation function output considered baum 
showed problem solvable uses units hidden layer 
model continuously valued discrete inputs 
baum considers consider 
prove hidden layers nonlinearity precise activation function models capability approximating interpolating arbitrarily 
approximation theory mlp model neural networks model permits generalization done number ways 
activation function may change layer layer unit unit 
replace simple linearity unit ijk ij complicated function ij architecture may altered allow different links units different layers layer 
just possible generalizations 
mathematical analysis multilayer perceptron model far understood consider basic model minor modifications 
example usual multilayer perceptron model apply activation function hidden layer case follow convention activation function threshold applied output layer 
may various reasons practical point view depending problem considered 
mathematical perspective applying activation function output layer especially activation function bounded unnecessarily restrictive 
simplification consider models output noted 
real restriction tremendously simplify notation 
modifications activation function threshold applied output layer output write output single hidden layer perceptron model units hidden layer input oe ij gamma ij weight jth unit input ith unit hidden layer threshold ith unit hidden layer weight ith unit hidden layer output 
generally write succinctly oe delta gamma delta standard inner product 
express output hidden layer perceptron model units hidden layer units second hidden layer input 
oe ik oe ik delta gamma ik gamma fl iterate hidden layer model 
write exact formula output model hidden layers 
pinkus common choices activation functions oe may literature 

heaviside function mentioned oe 
referred neural network literature threshold function 

logistic sigmoid oe gammat 
oe tanh constant just shift logistic sigmoid 

piecewise linear function form oe gamma gamma 
gaussian sigmoid oe gamma gammay dy 
arctan sigmoid oe arctan logistic sigmoid suited demands backpropagation 
function derivative easily calculated 
note functions bounded generally increasing 
term sigmoidal class activation functions satisfying lim gamma oe lim oe 
certain lack consistency terminology 
authors demand oe continuous monotonic strictly monotonic demands 
shall try explicit mean term 

density section consider density questions associated single hidden layer perceptron model 
consider set oe delta gamma ask question 
oe true compact subset exists oe max jf gamma approximation theory mlp model neural networks words density linear space oe space topology uniform convergence compact sets 
fact shall restrict permissible set weights thresholds 
set terminology shall say oe density property oe dense topology 
noted norm strong 
nonnegative finite borel measure support compact set dense 
results section extend spaces 
renaissance neural net theory started mid clearly understood density question single hidden number hidden layer perceptron model fundamental importance theory 
density theoretical ability approximate 
density imply efficient scheme approximation 
lack density means impossible approximate large class functions effectively precludes scheme thereon useful 
killed efficacy hidden layer model 
understood density imply approximate function oe oe delta gamma fixed contrary generally lower bound reasonable set functions degree approximate oe independent choice oe 
consider length section 
expected natural 
sense similar situation approximation polynomials 
polynomials dense polynomials fixed degree sparse 
note sets oe subspaces 
important property oe oe oe 
hecht nielsen consider density problem single hidden layer perceptron model 
observations kolmogorov superposition theorem see section 
researchers subsequently questioned exact relevance result model certainly true stimulated interest problem 
proceedings ieee topic neural networks papers appeared discussed density problem 
gallant white constructed specific continuous nondecreasing sigmoidal function possible obtain trigonometric fourier series 
activation function called cosine density property 
irie miyake constructed integral representation kernel form oe delta gamma oe arbitrary function 
pinkus allowed interpretation framework course restricted oe 
appeared cited papers considered density problem general classes activation functions 
carroll dickinson cybenko hornik stinchcombe white 
carroll dickinson discretized inverse radon transform approximate functions compact support norm continuous sigmoidal function activation function 
main result cybenko density property uniform norm continuous sigmoidal function 
cybenko demand monotonicity definition 
method proof uses hahn banach theorem representation riesz representation theorem continuous linear functionals space continuous functions compact set 
independently cybenko proves density property uniform norm continuous monotone sigmoidal function 
notes oe continuous monotone bounded follows oe delta ff gamma oe delta fi ff fi 
applies previously mentioned result irie miyake 
hornik stinchcombe white unaware prove result 
demand activation function monotone bounded permit activation functions 
method proof totally different somewhat circuitous 
allow sums products activation functions 
permits apply stone theorem obtain density 
prove desired result products cosine functions ability write products cosines linear combinations cosines 
subsequent papers dealt density problem related issues 
quickly review 
stinchcombe white prove oe density property oe gamma oe dt 
considers different types models activation functions non sigmoidal stone theorem employed obtain density instance oe 
jones shows ridge functions shall soon define answer question density suffices consider univariate problem 
proves constructive methods bounded necessarily monotone continuous sigmoidal activation function suffices 
stinchcombe white reduce question density univariate case consider various activation functions necessarily sigmoidal piecewise linear knot subset polynomial splines subset analytic functions 
consider density question bounding set permissible weights thresholds 
hornik proves density approximation theory mlp model neural networks continuous bounded nonconstant activation function norms 
ito series papers ito studies problem density monotone sigmoidal functions weights norm 
considers conditions obtains uniform convergence chui li constructively prove density activation function continuous sigmoidal weights thresholds integer values 
micchelli extend density result call kth degree sigmoidal functions 
prove oe continuous bounded polynomial degree lim gamma oe lim oe density holds oe polynomial 
results may light chen chen chen chen liu pag es burton 
noted variety techniques attack problem considered important difficult 
solution problem turns surprisingly simple 
lin pinkus prove necessary sufficient condition continuous activation function density property polynomial 
considered sufficient conditions activation functions imply density 
reason publication article delayed submission date incorrectly reported 
subsequent issue appeared hornik lin pinkus results slightly altered form 
pinkus somewhat different proof noted characterization continuous activation functions density property essentially schwartz see edwards pp 

problem fact related characterizing translation dilation invariant subspaces topology uniform convergence 
said main theorem prove 
theorem oe 
oe dense topology uniform convergence oe polynomial 
oe polynomial density possibly hold 
immediate 
oe polynomial degree choice oe delta gamma multivariate polynomial total degree oe space polynomials total degree span 
main content theorem converse result 
pinkus shall prove considerably stated theorem 
shall show diverse cases restrict permissible weights thresholds enlarge class eligible oe obtaining desired density 
propositions results techniques lin pinkus schwartz 
start analysis defining ridge functions 
ridge functions multivariate functions form delta delta delta delta 
rand nf fixed direction 
words multivariate functions constant parallel hyperplanes delta ridge functions considered study hyperbolic partial differential equations go name plane waves computerized tomography projection pursuit approximation theory neural networks see instance pinkus details 
set delta 
rg ridge functions relevant theory single hidden layer perceptron model factor oe delta gamma ridge function choice oe 
immediately follows lower bound extent model units single hidden layer approximate function order approximation manifold delta 
return fact section 
addition ridge functions dense topology possible oe dense choice oe 
ridge functions density property 
easily seen 
contains functions form cos delta sin delta 
functions shown dense compact subset 
dense subset ridge functions deltax set spanf delta contains polynomials dense 
fact result due see lin pinkus tells exactly sets directions sufficient necessary density 
result 
approximation theory mlp model neural networks theorem 
set ridge functions delta ag dense topology uniform convergence nontrivial homogeneous polynomial vanishes homogeneity directions allowing direction equivalent allowing directions real vary fact suffices consider directions normalized lie unit ball gamma fy kyk delta delta delta theorem says dense gamma nontrivial homogeneous polynomial zero set containing example contains open subset gamma nontrivial homogeneous polynomial vanishes follows assume gamma proposition simple consequence ridge function form problem immediately reduces discussion tractable univariate follows theta subsets theta mean subset theta fa ag proposition assume theta subsets oe theta gamma thetag dense topology uniform convergence 
assume addition gamma dense topology uniform convergence 
oe theta theta delta gamma theta thetag dense topology uniform convergence 
proof 
compact set dense exist fi fi fi fi gamma delta fi fi fi fi compact fa delta kg ff fi finite interval ff fi oe theta dense ff fi exist constants ij ij ij theta pinkus fi fi fi fi gamma ij oe ij gamma ij fi fi fi fi ff fi fi fi fi fi gamma ij oe ij delta gamma ij fi fi fi fi proposition permits focus prove density restricted class activation functions 
proposition oe assume oe polynomial 
oe dense 
proof 
known fact known problem advanced math students oe open interval polynomial thereon exists point gamma interval oe gamma earliest result 
appears accessible donoghue exist simpler proofs appears 
oe oe gamma gamma oe gamma oe follows oe gamma fi fi fi toe gamma contained oe closure oe 
argument oe gamma fi fi fi oe gamma contained oe oe gamma set oe contains monomials polynomials 
theorem implies oe dense compact ae consider elementary proof detail 
properties function oe sets theta weights thresholds respectively 
fact really needed show oe gamma fi fi fi oe gamma contained oe theta oe gamma approximation theory mlp model neural networks suffices set containing sequence values tending zero oe theta theta contains open interval oe polynomial 
restate proposition general form 
corollary set containing sequence values tending zero theta open interval 
oe oe theta oe polynomial theta 
oe theta dense 
note method proof proposition shows conditions closure linear combination shifts dilations oe space polynomials degree fact section 
state formally 
corollary oe oe gamma theta open interval oe theta polynomial theta oe contains gamma linear space algebraic polynomials degree gamma 
consider weaken smoothness demands oe 
steps 
assume theta necessary proof proposition state appropriate analogue corollary 
proposition oe assume oe polynomial 
oe dense 
proof 
oe compact support 
oe set oe oe gamma oe gamma oe dy oe oe oe oe convolution oe oe 
oe oe oe compact support integral converges easily seen riemann sums oe oe contained closure oe 
furthermore oe oe 
follows oe oe contained oe oe oe gamma gamma oe gamma gamma oe dy oe oe method proof proposition oe oe gamma oe oe pinkus oe dense oe oe oe oe 
implies oe oe gamma oe 
oe oe polynomial degree gamma oe 
known exist sequences oe oe oe converges oe uniformly compact set example take called see instance adams 
polynomials fixed degree form closed finite dimensional linear subspace 
oe oe polynomial degree gamma oe follows oe polynomial degree gamma 
contradicts assumption 
assumed oe showed obtain result oe 
consider class discontinuous oe 
prove result density holds oe bounded riemann integrable finite interval 
theorem lebesgue property riemann integrability functions equivalent demanding set discontinuities oe lebesgue measure zero see instance 
true arbitrary oe space oe dense oe polynomial smoothness conditions oe 
proposition assume oe 
bounded riemann integrable finite interval 
assume oe polynomial 
oe dense 
proof 
remains true oe oe oe gamma oe gamma oe dy 
furthermore oe oe defined proposition lim koe gamma oe oe compact see instance adams 
oe oe polynomial degree gamma oe polynomial degree gamma 
proof proposition exactly follows method proof proposition show oe oe closure oe oe 
prove 
oe assume oe support gammaff ff 
set gammaff iff approximation theory mlp model neural networks delta gamma deltay gamma gamma ff definition oe gamma oe deltay oe prove sum uniformly converges oe oe compact subset definition fi fi fi fi oe oe gamma oe gamma oe deltay fi fi fi fi delta oe gamma oe gamma oe gamma oe dy delta oe gamma gamma oe gamma oe dy delta oe gamma oe gamma oe dy oe bounded gamma gammaff ff oe uniformly continuous gammaff ff easily follows lim delta oe gamma oe gamma oe dy fi fi fi fi delta oe gamma gamma oe gamma oe dy fi fi fi fi koek gammaff ff sup delta oe gamma gamma inf delta oe gamma ff oe riemann integrable gamma gammaff ff follows lim sup delta oe gamma gamma inf delta oe gamma ff proves result 
difficult check conditions need hold locally corollary 
corollary set containing sequence values tending zero theta open interval 
assume oe pinkus oe bounded riemann integrable theta polynomial theta 
oe theta dense 
results taken mean recommend minimal set weights thresholds 
strategy wrong 
cases far considered necessary method proof allow dilations set containing sequence tending zero 
fact necessary 
example simple result proven classical methods 
proposition assume oe oe continuous nondecreasing bounded constant function 
oe dense 
proof 
assume oe 
continuous linear functionals represented borel measures finite total variation compact support 
oe dense exists nontrivial measure satisfying gamma oe gamma oe nice fourier transforms 
convolution implies oe 
entire function exponential type oe continuous 
oe vanish follows oe oe 
contradiction proves result 
oe continuous nondecreasing bounded constant function oe delta gamma oe delta zero function fixed 
apply result previous paragraph obtain desired result 
proposition tell full story 
formal study oe schwartz introduced definition class mean periodic functions 
definition 
function said mean periodic gamma dense topology uniform convergence 
translation invariant subspaces space studied various norms especially 
study functions attempt provide parallel analysis space approximation theory mlp model neural networks 
unfortunately subject understood 
luckily interested univariate case schwartz provided thorough analysis spaces see 
theory mean periodic functions unfortunately complicated proofs 
central result subspaces gamma rg spanned mean periodic functions totally characterized functions form flt contained closure fl 
values fl determine spectrum note fl spectrum fl 
fact follows result 
proposition oe assume oe polynomial 
contains sequence tending finite limit point set oe dense 
proof 
ffi nf 
oe ffit mean periodic ffit gamma rg dense finished 
assume 
oe polynomial span contains closure flt nonnegative integer fl nf 
may assume finite linear combination shifts follows flt contained closure 
closure gamma contains fl ffi claim fl ffi dense finite limit point 
known result 
prove method proof proposition 
alternatively span dense gamma fl ffi nontrivial borel measure finite total variation compact support 
gamma zt entire function vanishes set ffl ffi set contains sequence tending finite limit point 
implies identically zero turn implies zero measure 
contradiction proves density 
pinkus 
may noted method proof proposition condition replaced demand contained zero set nontrivial entire function 
mention schwartz proved result 
proposition oe 
oe oe bounded limit infinity minus infinity constant function oe mean periodic 
cases oe fg dense 

input preprocessed working directly input data converted hm fixed continuous functions set oe delta gamma rg theorem valid setting separates points implies see lin pinkus 
analogues results section depend explicit form 
derivative approximation section consider conditions neural network single hidden layer perceptron model simultaneously uniformly approximate function various partial derivatives 
fact requisite algorithms 
introduce standard multivariate notation 
denote lattice nonnegative multi integers set jmj delta delta delta delta delta delta mn jmj delta delta delta mn polynomial mean differential operator usual ordering say set approximation theory mlp model neural networks special case jmj ff jkj mg recall oe delta gamma rg say oe dense compact exists oe satisfying max jd gamma outline proof skipping various details result 
theorem set sg 
assume oe oe polynomial 
oe dense 
density question considered hornik stinchcombe white 
showed oe oe dense 
subsequently hornik generalized oe bounded constant function 
hornik uses functional analytic method proof 
suitable modifications method proof applied prove theorem 
ito hornik result oe polynomial 
method proof different 
essentially follow 
approach similar approach taken li theorem effectively 
papers concerned problem gallant white ito micchelli pag es 
papers contain generalizations density norms related questions 
proof 
polynomials dense 
may shown number ways 
proof fact li 
suffices prove approximate polynomials appropriate norm 
polynomial represented form delta choice univariate polynomials pinkus precise proof fact 
result section detail proof 
denote linear space homogeneous polynomials degree linear space polynomials degree set gamma gamma delta dim jm jm ffi easily calculated 
implies linear functional may represented delta delta jmj delta 
dim exist points dim fa claim delta span exists nontrivial linear functional annihilates delta nontrivial satisfies delta contradicts choice delta span follows delta spans exists nontrivial vanishes gammas function pq vanishes contradiction 
spanf delta kg denote linear space univariate polynomials degree follows delta may written form 
follows suffices see proof proposition prove approximate univariate polynomial finite interval ff fi oe gamma rg norm kfk ff fi max max ff fi jf oe polynomial results section oe dense 

exists oe kf gamma ff fi approximation theory mlp model neural networks polynomial degree gamma closure oe respect norm delta ff fi choosing polynomial satisfying ff gamma ff gamma follows integrating times close norm delta ff fi follows iterating inequality jf gamma gamma gamma fi fi fi fi ff gamma dt fi fi fi fi fi gamma ff max jf gamma reduced problem proving gamma closure oe respect norm delta ff fi oe follows method proof proposition gamma function oe gamma contained closure oe respect usual uniform norm delta ff fi ff fi oe polynomial exists oe gamma 
detailed analysis skip proves gamma contained closure oe respect stringent norm delta ff fi neglected numerous possible nuances parallel contained section see instance corollary propositions 

interpolation ability approximate related ability interpolate 
approximate expects able interpolate inverse need general hold 
pose problem precisely setting 
assume oe 
distinct points fx ae associated data fff ae find fw ae fc ae oe delta gamma ff 
furthermore relationship 
problem considered example antsaklis ito ito saito huang 
ito saito proven oe sigmoidal continuous nondecreasing interpolate fw ae gamma pinkus huang extend result bounded continuous nonlinear oe limit infinity minus infinity restricted way 
technique section prove result 
theorem oe assume oe polynomial 
distinct points fx ae associated data fff ae exist fw ae fc ae oe delta gamma ff furthermore oe mean periodic may choose fw ae gamma proof 
vector delta distinct set fix vary proven show existence fc satisfying oe gamma ff solving equivalent proving linear independence continuous functions oe gamma functions linearly independent exist det oe gamma solved choice fff hand linearly dependent exist nontrivial coefficients fd oe gamma rewrite form gamma oe gamma de measure de ffi approximation theory mlp model neural networks ffi measure point mass 
measure de nontrivial borel measure finite total variation compact support 
words represents nontrivial linear functional 
constructed nontrivial linear functional annihilating oe gamma implies gamma rg dense contradicts proposition 
proves theorem case 
oe mean periodic gamma rg dense 
implies foe gamma linearly independent choice distinct ft gamma delta distinct exist det oe delta gamma choosing solve 
oe polynomial interpolate depends choice points fx degree oe 
oe polynomial exact degree delta gamma gamma rg precisely space multivariate polynomials total degree 
degree approximation activation function oe set oe oe delta gamma know results section oe polynomial compact subset exist oe lim max jf gamma tells rate approximation 
tell method reasonable finding approximants 
questions especially address section 
pinkus fix additional notation 
denote unit ball fx kxk delta delta delta section approximate functions defined denote set functions defined defined continuous satisfying jkj see section 
sobolev space may defined completion respect norm kfk kd fk max kd fk equivalent norm thereon 
kgk ae jg dx ess sup jg set ff kfk 
compact dense oe dense oe polynomial 
consider lower bounds degree approximate oe 
mentioned section choice function oe factor oe delta gamma ridge function 
set delta oe oe follows norm delta normed linear space containing oe inf oe kf gamma inf rr kf gamma estimate right hand side reasonable way 
relevant lower bound 
proved lower bound 
assume 
exists cr gammam gamma generic positive constant independent things independent 
case independent 
case may 
approximation theory mlp model neural networks proves cr gammam gamma obtains result 
theorem 
sup gammam gamma somewhat precise proves result integer definition somewhat different 
addition meir show set functions lower bound holds large measure 
words simply worst case result 
proof lower bound difficult complicated 
proof upper bound elementary standard follows 
exhibit 
valid 
theorem cr gammam gamma constant independent proof 
proof theorem denote linear space homogeneous polynomials degree linear space polynomials degree set gamma gamma delta dim note gamma claim follows proof theorem proven linear space univariate polynomials degree set satisfying dim fa delta classical result ck gammam gamma follows cr gammam gamma appropriate pinkus 
true ck gammam exist linear operators sup kf gamma ck gammam metatheorem years 
proof see 
theorem strong result 
simply says ridge functions approximate approximate polynomial space contained 
unfortunately lower bound currently proven case says better sobolev spaces 
lower bound stated lower bound approximation error oe oe 
relevant 
true oe oe cr gammam gamma 
oe see example theorem 
exist oe oe cr gammam gamma 
answer 
exist activation functions lower bound attained 
hardly surprising 
simple consequence separability gamma 
oe exhibited pathological 
somewhat surprising glance fact exist activation functions lower bound attained sigmoidal strictly increasing belong 
proposition 
pinkus exist oe sigmoidal strictly increasing property exist rand satisfying fi fi fi fi gamma oe delta gamma fi fi fi fi result theorem immediately imply result 
corollary exist oe sigmoidal strictly increasing oe cr gammam gamma 
approximation theory mlp model neural networks proof proposition 
space gamma separable 
contains countable dense subset 
subset 
gamma exists dependent jh gamma gamma 
assume um gamma 
example choose set polynomials rational coefficients 
construct strictly increasing sigmoidal function oe lim gamma oe lim oe gamma exists integer real coefficients dependent jh gamma oe gamma oe oe gamma 
constructing oe oe gamma oe oe gamma 
strictly monotone sigmoidal function 
instance gammat 
define oe way 
set oe gamma choose constants dm 
oe 
oe 
easily done 
assumption 
intervals gamma gamma demand oe satisfy conditions linear oe gamma oe linearly independent gamma 
finish construction simply fill gaps domain definition oe including gamma way lim gamma oe 
construction exists reals oe gamma oe oe 
may write delta gamma gamma construction oe exist constants integers jg gamma oe gamma oe oe gamma pinkus jg delta gamma oe delta gamma oe delta oe delta fi fi fi fi gamma oe delta gamma oe delta oe delta fi fi fi fi oe delta gamma oe delta linear function linear combination oe delta gamma oe delta may rewritten terms sum 
proves proposition 

implications proposition proof corollary twofold 
firstly monotonicity smoothness impediments optimal degrees approximation 
secondly surprisingly excellent properties sufficient deter construction pathological activation functions 
fact exist real entire analytic sigmoidal strictly increasing oe optimal error estimates hold replaces proposition 
details see pinkus 
practice approximation process depends degree order approximation possibility complexity cost finding approximants 
activation functions smooth give best degree approximation 
unacceptably complex 
know possible theoretically 
interesting lower bound larger 

method approximation 
show choice coefficients weights thresholds depend continuously function approximated totally unreasonable assumption lower bound error approximation functions oe order gammam gammam gamma proven 
show oe oe polynomial oe bound attained 
devore howard micchelli introduced call continuous nonlinear width 
defined follows 
compact set normed linear space continuous map map whatsoever approximation theory mlp model neural networks delta map particular peculiar factorization 
set sup kf gamma define continuous nonlinear width inf infimum taken 
devore howard micchelli prove facts asymptotic estimate gammam context interested lower bound 
provide proof 
theorem 
devore howard micchelli fixed cd gammam constant independent proof 
bernstein width compact convex centrally symmetric set term applied codification standard methods providing lower bounds common width concepts 
lower bound valid setting show 
set sup supf kg dimensional subspace unit ball continuous map set gamma gammaf odd continuous map gammaf gamma 
assume odd continuous map boundary borsuk theorem exists gammaf 
consequence map gamma gamma gammaf gamma gammaf gamma gamma gamma gammaf kf pinkus implies inequality valid choice eligible 
particular 
remains prove bound cd gammam proof quite standard 
oe nonzero function support set oe oe gamma gamma support oe lies 
large number support oe lies totally order simple change variable argument shows koe gamman koek kd oe jkj gamman kd oek furthermore oe distinct support fixed fl fl fl fl oe fl fl fl fl gamman kck koek fl fl fl fl oe fl fl fl fl jkj gamman kck kd oek kck norm fc fl fl fl fl oe fl fl fl fl fl fl fl fl oe fl fl fl fl restricted summands support oe lies totally obtained linear subspace dimension order property fl fl fl fl oe fl fl fl fl gammam fl fl fl fl oe fl fl fl fl approximation theory mlp model neural networks constant independent 
exactly implies gammam cd gammam proves theorem 
theorem useful tells approximating oe certain continuous methods 
things noted understood 
firstly permissible methods approximation necessarily include continuous methods approximation 
secondly approximation methods developed today setting iterative necessarily continuous 
element oe form oe delta gamma constants general approximating choice depend parameters 
parameters may fixed independent function approximated 
method approximation continuously depends parameters lower bound theorem holds 
theorem oe method approximation parameters continuously dependent function approximated may course fixed independent function 
sup kf gamma fk cr gammam independent additional upper lower bound estimates appear meir 
particular cases lower bounds specific oe improve lower bound oe theorem assumption continuity approximating procedure 
state result 
proof complicated 
theorem 
meir 
oe logistic sigmoid oe gammat pinkus polynomial spline fixed degree finite number knots 
oe log gammam independent consider upper bounds 
theorem may minor modifications see bos 
note logistic sigmoid satisfies conditions theorem 
theorem assume oe 
oe theta open interval theta oe polynomial theta 
oe cr gammam constant independent proof 
conditions theorem imply corollary oe closure oe contains linear space univariate algebraic polynomials degree proof theorem see theorem dim gamma exist gamma delta linear space variate algebraic polynomials degree oe oe oe oe follows oe set 
oe oe ck gammam constant independent oe cr gammam proves theorem 

important note upper bound theorem attained continuous fact linear methods sense theorem 
thresholds chosen equal oe gamma see proposition 
weights chosen independent function approximated 
dependence function choice previously noted see approximation theory mlp model neural networks theorem fact done linear manner 
operator best approximation continuous 

functions analytic neighbourhood better order approximation estimates polynomial approximation see 
optimal order approximation oe really better obtained approximating polynomial space dimension wonder really worthwhile model case single hidden layer 
clear perspective mathematical computational justifications choosing model models 
researchers content construct neural networks algorithmically achieve order approximation 
proves general estimates concerning ridge neural network approximation 
results valid 
generalize theorem setting 
gamma usual norm kgk gamma jg dt similarly denote sobolev space gamma norm kg set oe sup khk inf oe kh gamma gk point place theorem 

assume oe property oe ck gammam independent gamma oe cr gamma gamma independent pinkus 
follows general interpolation properties spaces hold specific hold positive value proof theorem complicated 
underlying idea similar proof theorem 
uses multivariate polynomials approximate functions gamma decomposes multivariate polynomials ridge polynomials proof theorem approximates univariate ridge polynomials oe 
consequence theorem wish highlight directly covered theorem 
corollary 
oe ae oe cr gammam gamma constant independent variation result proves corollary gamma 
cases follow differences really just differentiating consequence 
note oe heaviside function 
assume oe continuous piecewise continuous satisfies lim gamma oe lim oe essentially micchelli call kth degree sigmoidal 
lim oe oe uniformly gammac converges gamma 
oe defined corollary 
oe oe 
addition oe spline degree simple knot finite number shifts approximate oe gamma norm 
applying corollary obtain 
corollary oe defined previous paragraph 
oe cr gammam gamma constant independent approximation theory mlp model neural networks note error approximation results exactly form 
oe theta theorem holds oe contains gamma different interesting approach problem determining bounding order approximation set oe initiated barron 
considered certain standard smoothness classes tried estimate worst case error approximation functions class 
approach oe try find classes functions approximated oe 
generally difficult problem worth pursuing 
barron sense specific interesting setting 
barron generalizations due 
start general result generalization due result barron see jones 
result contain factor 
mentioned previously discussed upper bounds upper bounds obtained strictly nonlinear necessarily continuous methods 
hilbert space bounded set 
denote convex hull set inff covered sets diameter theorem 
bounded subset hilbert space form satisfying kf gamma letting set approximants may reasonable approximation theoretic result 
problem identify significant subset tautological terms 
result sterile 
barron considered oe bounded measurable sigmoidal set oe sigmaoe delta gamma rg recall proved oe contains set functions defined extended pinkus shift constant fourier transform satisfying ksk ds fl fl 
quickly explain general terms result holds 
mentioned earlier continuous sigmoidal oe see comment corollary oe delta approaches oe delta norm oe heaviside function 
oe oe equally important follow essentially oe oe replace oe delta gamma term oe delta gamma sufficiently large 
suffices prove set functions fact contained oe 
set sigmaoe gamma rg gamma 
simply oe constant shift previously mentioned contained function bounded variation total variation bounded 
continuously differentiable just means gamma jh dt applying result oe implies fle deltax ksk oe fl dependent 
ksk ds fl fle deltax ksk ksk fl ds oe apply theorem obtain estimate oe 
quantity generally impossible estimate 
oe oe oe oe suffices consider oe 
approximating oe sigmaoe delta gamma kwk add additional function set oe 
kw kw kw gamma approximation theory mlp model neural networks gamma joe delta gamma gamma oe delta gamma dx constant estimate oe find net kwk easily shown need gamman elements 
oe cr gamma summarize 
theorem 
defined 
bounded measurable sigmoidal function oe oe oe cr gamma constant independent oe piecewise continuous sigmoidal function corollary oe cr gamma error bound activation function appears 
natural ask stronger result 
fact results comparable 
condition defining restated terms conditions derivatives 
known see barron essentially span leftmost inclusion quite correct see barron 
error estimate barron originally contain term form cr gamma constant 
initiated unfortunate discussion concerning results having defeated curse dimensionality 
literature contains various generalizations results expect follow 
generalizes theorems probability measure set 
discussion analogous problem uniform norm see barron 
donahue darken sontag consider different generalizations theorem provide general perspective type problem 
hornik stinchcombe white auer see chen white consider generalizations barron results function derivatives simultaneously approximated 
lower bounds error pinkus approximation barron 
lower bounds essentially apply approximating oe restricted set approximants particular activation function apply approximation oe 
related results may micchelli stinchcombe white 
algorithm approximation introduced jones obtain iterative sequence fh approximants oe oe sigmoidal 
approximants satisfy kf gamma cr gamma constant independent sequence constructed follows 
initialize process setting consider min ff min oe kf gamma ffh gamma gamma ff assume minima attained ff oe 
set ff gamma gamma ff assume oe compact 
fact mentioned jones improved barron improved jones see donahue darken sontag ff need chosen attain minima exactly convergence rate hold 
section pointing remains done finding upper bounds constructing reasonable methods approximation identifying classes functions approximated model 
worth noting results surveyed intrinsic properties activation functions 
theorem property 
corollary depends solely approximation properties oe theorem result concerning heaviside activation function 

hidden layers relatively little known concerning advantages disadvantages single hidden layer units neurons hidden layers fewer units 
mathematics approximation theory mlp model hidden layer understood 
authors see little theoretical gain considering hidden layer single hidden layer model suffices density 
authors allow possibility certain benefits gained hidden layer 
see de barnard comparison models 
approximation theory mlp model neural networks important advantage multiple single hidden layer model existence locally supported localized functions hidden layer model see lapedes farber blum li geva chui li 
activation function oe oe jg dx oe compact support 
longer true hidden layer model 
example oe heaviside function 
oe oe delta gamma gamma gamma ae delta 
hidden layer model activation function oe unit second hidden layer represent characteristic function closed convex polygonal domain 
example oe oe gamma oe gammax gamma gamma characteristic function rectangle 
boundary values function representation oe oe gamma gamma oe gamma gamma gamma oe gammat gamma oe 
oe continuous piecewise continuous sigmoidal function similar result holds functions oe delta approaches oe delta say gamma 
function oe oe delta gamma gamma gamma approximates function 
approximating localized functions advantages 
advantage multiple hidden layer model 
noted section lower bound degree single hidden layer model units hidden layer approximate function 
extent linear combination ridge functions approximate function 
lower bound shown attainable proposition corollary importantly ridge function approximation bounded away pinkus zero non dependence set approximated 
single hidden layer model intrinsic lower bound degree approximation depending number units 
case hidden layer model 
prove activation function proposition theoretical lower bound error approximation permit hidden layers 
precise prove theorem 
theorem 
pinkus exists activation function oe strictly increasing sigmoidal property 
exist constants ij ij fl vectors ij fi fi fi fi fi gamma oe ij oe ij delta ij fl fi fi fi fi fi words specific activation function continuous function unit cube uniformly approximated error hidden layer neural network units hidden layer units second hidden layer 
recall constructed activation function pathological 
proof theorem kolmogorov superposition theorem 
theorem quoted discussed neural network literature see hecht nielsen girosi poggio lin unbehauen 
fact uses kolmogorov superposition theorem construct approximations hidden layer model arbitrary sigmoidal function 
number units needed exceedingly large provide error bounds opinion reasonably efficient method approximation 
better error bounds follow localized functions see instance blum li ito especially chui li 
see frisch ord williams sprecher sprecher interested kolmogorov superposition theorem find algorithms approximation 
aim 
kolmogorov superposition theorem prove theoretical lower bound degree approximation common activation functions case single hidden layer model 
fact showing exists activation function nice properties fixed finite number units hidden layers approximation theory mlp model neural networks sufficient approximate arbitrarily continuous function 
advocate activation function 
kolmogorov superposition theorem answers negative hilbert th problem 
proven kolmogorov series papers late 
quote improved version theorem see lorentz von detailed discussion 
theorem exist constants strictly increasing continuous functions oe map continuous function variables represented form oe depending note theorem representing approximating functions 
numerous generalizations theorem 
attempts understand nature theorem led interesting concepts related complexity functions 
theorem direct applications 
proof theorem 

oe 
oe constructed proposition 
recall gamma find constants integer jh gamma oe gamma oe oe gamma 
result certainly valid restrict interval functions continuous thereon 
exist constants integer jg gamma oe gamma oe oe 
recall oe gamma oe linear polynomials 
substituting obtain fi fi fi fi gamma oe oe gamma oe oe oe oe fi fi fi fi pinkus oe oe gamma oe oe linear polynomials oe fact rewrite oe oe gamma oe oe oe oe fl oe oe fl gamma 
may rewrite fi fi fi fi gamma oe oe fl gamma oe oe fi fi fi fi ffi exist constants integers fi fi fi fi oe gamma oe gamma oe oe fi fi fi fi ffi 
fi fi fi fi oe gamma oe gamma oe oe fi fi fi fi ffi fact oe gamma oe linear polynomials rewrite fi fi fi fi oe gamma ij oe ij delta ij fi fi fi fi ffi constants ij ij vectors ij fact ij unit vectors 
approximation theory mlp model neural networks substitute 
oe uniformly continuous closed interval choose ffi sufficiently small fi fi fi fi oe oe fl oe oe gamma oe ij oe ij delta ij fl gamma oe ij oe ij delta ij fi fi fi fi renumbering renaming theorem follows 
consequence stated proof proposition fact prove theorem oe analytic strictly increasing sigmoidal see pinkus 
difference units layer units second layer 
restriction theorem unit cube convenience 
result holds compact subset established facts section 
shown exist localized functions theoretical lower bound degree approximation common activation functions contrary situation single hidden layer model 
reason conjecture hidden layer model may significantly promising single hidden layer model purely approximation theoretic point view 
problem certainly warrants study 
author indebted lee jones moshe reading various parts 
errors omissions author responsibility 
adams sobolev spaces academic press new york 
sontag uniqueness weights neural networks artificial neural networks speech vision ed chapman hall london pp 


pag es approximations functions multilayer perceptron new approach neural networks 
barron neural net approximation proc 
seventh yale workshop pinkus adaptive learning systems narendra ed yale university new haven pp 

barron universal approximation bounds superpositions sigmoidal function ieee trans 
inform 
theory 
barron approximation estimation bounds artificial neural networks machine learning 
bartlett meir linear vc dimension bounds piecewise polynomial networks neural computation 
baum capabilities multilayer perceptrons complexity 
bishop neural networks pattern recognition oxford university press oxford 
blum li approximation theory feedforward networks neural networks 
buhmann pinkus identifying linear combinations ridge functions adv 
appl 
math 

burton universal approximation mean neural networks neural networks 
approximation function derivatives neural network neural networks 
carroll dickinson construction neural nets radon transform proceedings ieee international joint conference neural networks vol 
ieee new york pp 

chen chen approximations continuous functionals neural networks application dynamic systems ieee trans 
neural networks 
chen chen universal approximation nonlinear operators neural networks arbitrary activation functions application dynamical systems ieee trans 
neural networks 
chen chen liu approximation capability multilayer feedforward networks related problems ieee trans 
neural networks 
chen white improved rates asymptotic normality nonparametric neural network estimators preprint 
choi choi constructive neural networks piecewise interpolation capabilities function approximations ieee trans 
neural networks 
chui li ridge functions neural networks hidden layer approx 
theory 
chui li realization neural networks hidden layer multivariate approximations cagd wavelets eds world scientific singapore pp 

chui li neural networks localized approximation math 
comp 

chui li limitations approximation capabilities neural networks hidden layer adv 
comput 
math 

approximation theory mlp model neural networks para que una derivable sea un rev mat 

amer 

stone theorem application neural networks ieee trans 
neural networks 
cybenko approximation superpositions sigmoidal function math 
control signals systems 
devore howard micchelli optimal nonlinear approximation math 

devore approximation feedforward neural networks ann 
numer 
math 

devroye lugosi probabilistic theory pattern recognition springer new york 
donahue darken sontag rates convex approximation non hilbert spaces const 
approx 

donoghue distributions fourier transforms academic press new york 
constructive neural network algorithm function approximation proceedings ieee international conference neural networks vol 
ieee new york pp 

edwards functional analysis theory applications holt rinehart winston new york 
aspects numerical analysis neural networks vol 
acta numerica cambridge university press pp 

bos neural networks deterministic methods analysis international thomson computer press london 
reconstructing neural net output revista mat 


damper comparison multilayer radial basis function neural networks text dependent speaker recognition proceedings ieee international conference neural networks vol 
ieee new york pp 

frisch ord williams approximate representation functions variables terms functions variable phys 
review letters 
approximate realization continuous mappings neural networks neural networks 
gallant white exists neural network avoidable mistakes proceedings ieee international conference neural networks vol 
ieee new york pp 

gallant white learning derivatives unknown mapping multilayer feedforward networks neural networks 
geva constructive method multivariate function approximation multilayer perceptrons ieee trans 
neural networks 
girosi poggio representation properties networks kolmogorov theorem irrelevant neural computation 
pinkus girosi poggio networks best approximation property biol 
cybern 

gori tsoi classes functions multilayer perceptron approximate proceedings ieee international conference neural networks vol 
ieee new york pp 

haykin neural networks macmillan new york 
hecht nielsen kolmogorov mapping neural network existence theorem proceedings ieee international conference neural networks vol 
ieee new york pp 

hecht nielsen theory backpropagation neural network proceedings ieee international joint conference neural networks vol 
ieee new york pp 

hornik approximation capabilities multilayer feedforward networks neural networks 
hornik new results neural network approximation neural networks 
hornik stinchcombe white multilayer feedforward networks universal approximators neural networks 
hornik stinchcombe white universal approximation unknown mapping derivatives multilayer feedforward networks neural networks 
hornik stinchcombe white auer degree approximation results feedforward networks approximating unknown mappings derivatives neural computation 
huang upper bounds number hidden neurons feedforward networks arbitrary bounded nonlinear activation functions ieee trans 
neural networks 
huang huang bounds number hidden neurons multilayer perceptrons ieee trans 
neural networks 
irie miyake capability layered perceptrons proceedings ieee international conference neural networks vol 
ieee new york pp 

ito representation functions superpositions step sigmoid function applications neural network theory neural networks 
ito approximation functions compact set finite sums sigmoid function scaling neural networks 
ito approximation continuous functions linear combinations shifted rotations sigmoid function scaling neural networks 
ito approximations differentiable functions derivatives compact sets neural networks math 


ito approximation capabilities layered neural networks sigmoidal units layers neural computation 
ito differentiable approximation means radon transformation applications neural networks comput 
appl 
math 

approximation theory mlp model neural networks ito nonlinearity creates linear independence adv 
comput 
math 

ito saito superposition linearly independent functions finite mappings neural networks math 


jones constructive approximations neural networks sigmoidal functions proc 
ieee 
correction addition proc 
ieee 
jones simple lemma greedy approximation hilbert space convergence rates projection pursuit regression neural network training ann 
stat 

jones weights hyperbolic kernels neural networks projection pursuit pattern classification fourier strategies extracting information high dimensional data ieee trans 
inform 
theory 
jones computational intractability training sigmoidal neural networks ieee trans 
inform 
theory 
jones local greedy approximation nonlinear regression neural network training preprint 
lectures mean periodic functions tata institute bombay 
vogt approximation neural networks continuous preprint 
sprecher computational aspects kolmogorov superposition theorem neural networks 
arbitrary nonlinearity sufficient represent functions neural networks theorem neural networks 
kolmogorov theorem relevant neural computation 
kolmogorov theorem multilayer neural networks neural networks 
approximation functions perceptron networks bounded number hidden units neural networks 
kolmogorov theorem handbook brain theory neural networks arbib ed mit press cambridge pp 

trade size weights number hidden units feedforward networks neural network world 
functionally equivalent feedforward neural networks neural computation 
estimates number hidden units variation respect half spaces neural networks 
lapedes farber neural nets neural information processing systems anderson ed american institute physics new york pp 

ya 
lin pinkus multilayer feedforward networks non polynomial activation function approximate function neural networks 
pinkus li simultaneous approximations multivariate functions derivatives neural networks hidden layer neurocomputing 
light ridge functions sigmoidal functions neural networks approximation theory vii cheney chui schumaker eds academic press new york pp 

lin unbehauen realization kolmogorov network neural computation 
ya 
lin pinkus ridge functions approx 
theory 
ya 
lin pinkus approximation multivariate functions advances computational mathematics new delhi india micchelli eds world scientific singapore pp 

lippman computing neural nets ieee magazine 
lorentz von constructive approximation advanced problems vol 
springer berlin 
best approximation ridge functions appear approx 
theory meir near optimality stochastic approximation smooth functions neural networks appear adv 
comput 
math 
meir approximation functional classes equipped uniform measure ridge functions appear approx 
theory 
pinkus lower bounds approximation mlp neural networks neurocomputing 
random approximants neural networks approx 
theory 
uniform neural networks approx 
theory 
shoham approximating functions neural networks constructive solution uniform norm neural networks 
approximation properties multilayered feedforward artificial neural network adv 
comput 
math 

approximation real functions neural networks advances computational mathematics new delhi india micchelli eds world scientific singapore pp 

neural networks optimal approximation smooth analytic functions neural computation 
neural networks functional approximation system identification neural computation 
micchelli approximation superposition sigmoidal function radial basis functions adv 
appl 
math 

micchelli choose activation function approximation theory mlp model neural networks vol 
neural information processing systems cowan tesauro alspector eds morgan kaufman san francisco pp 

micchelli dimension independent bounds degree approximation neural networks ibm research development 
micchelli degree approximation neural translation networks single hidden layer adv 
appl 
math 

choice sampling nodes optimal approximation smooth functions generalized translation networks appear proceedings international conference artificial neural networks cambridge england 
approximative versions kolmogorov superposition theorem proved constructively comput 
appl 
anal 

chebyshev approximation discrete superposition application neural networks adv 
comput 
math 

ridge approximation chebyshev fourier analysis optimal quadrature formulas tr 
mat 
inst 



anal 
approximation ridge functions neural networks siam math 
anal 

pinkus density problems multivariate approximation approximation theory proceedings international dortmund meeting muller felten eds akademie verlag berlin pp 

pinkus subspaces density problems neural networks approx 
theory 
pinkus approximating ridge functions surface fitting multiresolution methods le schumaker eds vanderbilt university press nashville pp 

sur un non publi de analyse ecole polytechnique centre de math ematiques palaiseau france 
ripley neural networks related methods classification royal statist 
soc 
ripley pattern recognition neural networks cambridge university press cambridge 
real analysis macmillan new york 
editor neural network faq parts usenet newsgroup comp ai neural nets ftp ftp sas com pub neural faq html antsaklis simple method derive bounds size train multilayer neural networks ieee trans 
neural networks 
tsoi universal approximation feedforward neural networks survey existing methods new results neural networks 
pinkus schwartz sur non de fonctions continues bull 
soc 
math 
france 
schwartz th eorie en erale des fonctions ann 
math 

siu roychowdhury kailath rational approximation techniques analysis neural networks ieee trans 
inform 
theory 
sontag feedforward nets interpolation classification comput 
system sci 

sprecher universal mapping kolmogorov superposition theorem neural networks 
sprecher numerical implementation kolmogorov superpositions ii neural networks 
stinchcombe precision approximate flatness artificial neural networks neural computation 
stinchcombe white universal approximation feedforward networks non sigmoid hidden layer activation functions proceedings ieee international joint conference neural networks vol 
ieee new york pp 

stinchcombe white approximating learning unknown mappings multilayer feedforward networks bounded weights proceedings ieee international joint conference neural networks vol 
ieee new york pp 

theory applications neural computing chemical science annual rev phys 
chem 

uniqueness weights minimal feedforward nets input output map neural networks 
takahashi generalization approximation capabilities multilayer networks neural computation 
de barnard backpropagation neural nets hidden layers ieee trans 
neural networks 
approximation continuous functions superpositions plane waves dokl 
akad 
nauk sssr soviet math 
dokl 

wang tham morris multilayer feedforward neural networks canonical form approximation nonlinearity internat 
control 
watanabe solvable models layered neural networks differential structure adv 
comput 
math 

williamson existence uniqueness results neural network approximations ieee trans 
neural networks 
wray green neural networks approximation theory finite precision computation neural networks 
xu light cheney constructive methods approximation ridge functions radial functions numerical alg 

approximation theory mlp model neural networks stinchcombe white sup norm approximation bounds networks probabilistic methods ieee trans 
inform 
theory 
