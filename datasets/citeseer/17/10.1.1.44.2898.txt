learning th thing easier learning 
sebastian thrun computer science department carnegie mellon university pittsburgh pa world wide web www cs cmu edu thrun investigates learning lifelong context 
lifelong learning addresses situations learner faces stream learning tasks 
scenarios provide opportunity transfer knowledge multiple learning tasks order generalize accurately training data 
different approaches lifelong learning described applied object recognition domain 
shown board lifelong learning approaches generalize consistently accurately training data ability transfer knowledge learning tasks 
supervised learning concerned approximating unknown function examples 
virtually current approaches supervised learning assume set input output examples denoted characterize unknown function denoted target function drawn class functions learner space hypotheses denoted order preference prior considers learning 
example space functions represented artificial neural network different weight vectors 
formulation establishes rigid framework research machine learning important aspects essential human learning 
psychological studies shown humans employ just training data generalization 
able generalize correctly single training example 
key aspects learning problem faced humans differs vast majority problems studied field neural network learning fact humans encounter stream learning problems entire lifetime 
faced new thing learn humans usually exploit enormous amount training data affiliated institut fur informatik iii universitat bonn 
germany experiences stem related learning tasks 
example learning drive car years learning experience basic motor skills typical traffic patterns logical reasoning language precede influence learning task 
transfer knowledge learning tasks play essential role generalizing accurately particularly training data scarce 
framework study transfer knowledge lifelong learning framework 
framework assumed learner faces collection learning problems entire lifetime 
scenario opens opportunity synergy 
facing th learning task learner re knowledge gathered previous gamma learning tasks boost generalization accuracy 
interested simple version lifelong learning problem learner faces family concept learning tasks 
specifically functions learned lifetime learner denoted type gamma 
sampled function ff indicator function defines particular concept pattern member concept 
learning th indicator function training set contains examples type hx may distorted noise 
addition training set learner gamma sets examples concept functions denoted gamma 
contains training examples characterize additional data desired support learning called support set training set example recognition faces 
learning recognize th person say bob learner set positive negative example face images person 
lifelong learning may exploit training persons ff rich mike dave support sets usually directly training patterns learning new concept describe different concepts different class labels 
certain features shape eyes important facial expression location face image 
invariances domain learned transferred new learning tasks new people improve generalization 
illustrate potential importance related learning tasks lifelong learning just particular approach transfer knowledge 
describes extend conventional memory neural network algorithms 
approaches compared traditional learning algorithms transfer knowledge 
goal research demonstrate independent particular learning approach complex functions learned training data learning embedded lifelong context 
memory learning approaches memory algorithms memorize training examples explicitly interpolate query time 
sketch simple known approaches memory learning propose extensions take support sets account 
nearest neighbor shepard method probably widely memory learning algorithm nearest neighbor knn 
suppose query pattern know output knn searches set training examples examples hx input patterns nearest distance metric euclidian distance 
returns mean output value nearest neighbors 
commonly method due shepard averages output values training examples weights example inverse distance query point hx jjx gamma jj delta hx jjx gamma jj gamma small constant prevents division zero 
plain memory learning uses exclusively training set learning 
obvious way incorporate support sets carry wrong class labels 
learning new representation modification memory learning proposed employs support sets learn new representation data 
specifically support sets employed learn function denoted gamma 
maps input patterns new space new space forms input space memory algorithm 
obviously key property data representations multiple examples single concept similar representation representation example counterexample concept different 
property directly transformed energy function gamma hx xk hx xk jjg gammag jj gamma hx xk jjg gammag jj adjusting minimize forces distance pairs examples concept small distance example counterexample concept large 
implementation realized neural network trained back propagation algorithm 
notice new representation obtained support sets 
assuming learned representation appropriate new learning tasks standard memory learning applied new representation learning th concept 
learning distance function alternative way exploiting support sets improve memory learning learn distance function 
approach learns function theta gamma 
accepts input patterns say outputs members concept regardless concept training examples derived pairs examples hx yi hx taken single support set gamma 
implementation artificial neural network trained back propagation 
notice training examples lack information concerning concept originally derived 
support sets train training interpreted probability patterns examples concept 
trained generalized distance function memory approach 
suppose training set query point positive example hx interpreted probability member target concept 
votes multiple positive examples hx hx combined bayes rule yielding prob fn gamma hx xk gamma gamma notice distance metric 
generalizes notion distance metric triangle inequality needs hold example target concept provide evidence member concept 
neural network approaches comparison complete briefly describe approaches rely exclusively artificial neural networks learning back propagation standard back learn indicator function training set 
approach employ support sets unable transfer knowledge learning tasks 
learning hints learning hints constructs neural network output units function 
network trained simultaneously minimize error support sets fx training set doing internal representation network determined shaped support sets fx similar internal representations required functions support sets provide additional training examples internal representation 
explanation neural network learning method described uses explanation neural network learning algorithm ebnn originally proposed context reinforcement learning 
ebnn trains artificial neural network denoted gamma 
just back propagation 
addition target values training set ebnn estimates slopes tangents target function example specifically training examples ebnn sort hx fn fit tangent prop algorithm 
input target value taken training set third term slope fn estimated learned distance function described 
suppose hx positive training example 
function gamma 
maps single input pattern approximation represented neural network neural networks differentiable gradient estimate slope setting yields desired estimate fn 
stated target value slope vector fit tangent prop algorithm training example slope provides additional information target function learned support sets ebnn approach transfers knowledge support sets new learning task 
ebnn relies assumption accurate yield helpful sensitivity information 
ebnn fits training patterns values slopes misleading slopes overridden training examples 
see detailed description ebnn 
experimental results approaches tested database color camera images different objects see fig 

object database distinct color size 
th support sets compiled images bottle hat hammer coke book 
th learning tasks involves distinguishing shoe 
images subsampled theta pixel matrix pixel color saturation brightness value shown right side 
learning task recognition objects shoe 
previous gamma learning tasks correspond recognition objects bottle hat hammer coke book 
ensure images simply additional training data counterexamples shoe seventh object 
training set contained images shoe support sets contained images objects 
object recognition domain testbed transfer knowledge lifelong learning 
finding approximation involves recognizing target object invariant rotation translation scaling size change lighting 
invariances common object recognition tasks images showing objects provide additional information boost generalization accuracy 
transfer knowledge important training data scarce 
initial experiment tested methods single image shoe 
methods able transfer knowledge provided images objects 
results intriguing 
generalization accuracies knn shepard repr 

distance sigma sigma sigma sigma back prop hints ebnn sigma sigma sigma illustrate approaches transfer knowledge printed bold font generalize significantly better 
exception hint learning technique approaches grouped categories generalize approximately testing set correctly achieve approximately generalization accuracy 
group contains standard supervised learning algorithms contains new algorithms proposed capable transferring knowledge 
differences group statistically significant differences level 
notice random guessing classifies testing examples correctly 
results suggest generalization accuracy merely depends particular choice learning algorithm memory vs neural networks 
main factor determining generalization accuracy fact knowledge transferred past learning tasks 
training examples accuracy distance function shepard method representation shepard method training examples accuracy ebnn back propagation generalization accuracy function training examples measured independent test set averaged experiments 
confidence bars displayed 
happens training data arrives 
fig 
shows generalization curves increasing numbers training examples methods 
number training examples increases prior knowledge important 
presenting training examples results knn shepard repr 

distance sigma sigma sigma sigma back prop hints ebnn avail 
sigma sigma illustrate standard methods especially back propagation generalize accurately methods exploit support sets 
differences underlying learning mechanisms dominant 
comparing lifelong learning methods corresponding standard approaches ones inferior backpropagation outperformed ebnn shepard method generalizes accurately representation learned distance function learned 
differences significant confidence level 
discussion experimental results reported provide evidence learning easier embedded lifelong learning context 
transferring knowledge related learning tasks learner experienced generalize better 
test conjecture systematic way variety learning approaches evaluated compared methods unable transfer knowledge 
consistently lifelong learning algorithms generalize significantly accurately particularly training data scarce 
notice results tune results obtained author 
approaches ebnn extensively studied context robot perception reinforcement learning robot control chess 
domains consistently generalize better training data transferring knowledge previous learning tasks 
results consistent observations human learning previously learned knowledge plays important role generalization particularly training data scarce 
extends techniques situations support sets related lifelong learning rests assumption single task learned learning tasks appropriately related 
lifelong learning algorithms particularly suited domains costs collecting training data dominating factor learning costs amortized learning tasks 
domains include example autonomous service robots learn improve entire lifetime 
include personal software assistants perform various tasks various users 
pattern recognition speech recognition time series prediction database mining potential application domains techniques 
abu mostafa 
learning hints neural networks 
journal complexity 

ahn brewer 
psychological studies explanation learning 
dejong editor investigating explanation learning 
kluwer academic publishers boston dordrecht london 
atkeson 
locally weighted regression robot learning 
proceedings ieee international conference robotics automation pages sacramento ca april 
baxter 
learning internal representations 
proceedings conference computation learning theory 
beymer poggio 
face recognition model view 
proceedings international conference computer vision 
caruana 
multitask learning knowledge source inductive bias 
utgoff editor proceedings tenth international conference machine learning pages san mateo ca 
morgan kaufmann 
edelman 
generalizing single view face recognition 
technical report cs tr department applied mathematics computer science weizmann institute science rehovot israel january 
mitchell thrun 
explanation neural network learning robot control 
hanson cowan giles editors advances neural information processing systems pages san mateo ca 
morgan kaufmann 
moore hill johnson 
empirical investigation brute force choose features smoothers function approximators 
hanson judd petsche editors computational learning theory natural learning systems volume 
mit press 
moses ullman edelman 
generalization illumination viewing position upright inverted faces 
technical report cs tr department applied mathematics computer science weizmann institute science rehovot israel 
sullivan mitchell thrun 
explanation neural network learning mobile robot perception 
ikeuchi veloso editors symbolic visual learning 
oxford university press 
rumelhart hinton williams 
learning internal representations error propagation 
rumelhart mcclelland editors parallel distributed processing 
vol 
ii 
mit press 
shepard 
dimensional interpolation function irregularly spaced data 
rd national conference acm pages 
simard lecun denker 
tangent prop formalism specifying selected invariances adaptive network 
moody hanson lippmann editors advances neural information processing systems pages san mateo ca 
morgan kaufmann 
stanfill waltz 
memory reasoning 
communications acm december 
suddarth holden 
symbolic neural systems hints developing complex systems 
international journal machine studies 
thrun 
explanation lifelong 
kluwer academic publishers boston ma 
appear 
thrun sullivan 
clustering learning tasks selective cross task transfer knowledge 
technical report cmu cs carnegie mellon university school computer science pittsburgh pa november 
