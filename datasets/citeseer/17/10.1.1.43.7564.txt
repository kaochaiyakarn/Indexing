comparing bayesian network classifiers jie cheng russell greiner department computing science university alberta edmonton alberta canada email greiner cs ualberta ca empirically evaluate algorithms learning types bayesian network bn classifiers na bayes tree augmented na bayes bn augmented na bayes general bns learned variants conditional independence ci algorithm 
experimental results show obtained classifiers learned ci algorithms competitive superior best known classifiers bayesian networks formalisms computational time learning classifiers relatively small 
results suggest way learn effective classifiers demonstrate empirically new algorithm expected 
collectively results argue bn classifiers deserve attention machine learning data mining communities 
tasks including fault diagnosis pattern recognition forecasting viewed classification requires identifying class labels instances typically described set features attributes 
learning accurate classifiers pre classified data active research topic machine learning data mining 
past decades algorithms developed learning decision tree neural network classifiers 
bayesian networks bns pearl powerful tools knowledge representation inference conditions uncertainty considered classifiers discovery na bayes simple kind bns assumes attributes independent classification node surprisingly effective langley :10.1.1.135.7718
explores role bns 
section provides framework research describing standard approaches learning simple bayesian networks motivating exploration effective extensions 
section defines classes bns na bayes tree augmented na bayes bn augmented na bayes general bns describes methods learning 
implemented learners test effectiveness 
section presents analyzes experimental results set standard learning problems 
propose new algorithm learning better bn classifiers empirical results support claim 
framework bayesian networks bayesian network directed acyclic graph dag conditional probability distribution cp table node represents domain variable arc nodes represents probabilistic dependency see pearl 
general bn compute conditional probability node values assigned nodes bn classifier gives posterior probability distribution classification node values attributes 
learning bayesian networks datasets nodes represent dataset attributes 
idea markov boundary node bn markov boundary subset nodes shields affected node outside boundary 
markov boundaries markov blanket union parents children parents children 
bn classifier complete data markov blanket classification node forms natural feature selection features outside markov blanket safely deleted bn 
produce smaller bn compromising classification accuracy 
learning bn major tasks learning bn learning graphical structure learning parameters cp table entries structure 
trivial learn parameters structure optimal corpus complete data simply empirical conditional frequencies data cooper herskovits focus learning bn structure 
ways view bn suggesting particular approach learning 
bn structure encodes joint distribution attributes 
suggests best bn best fits data leads scoring learning algorithms seek structure maximizes bayesian mdl kullback leibler kl entropy scoring function heckerman cooper herskovits 
second bn structure encodes group conditional independence relationships nodes concept separation pearl 
suggests learning bn structure identifying conditional independence relationships nodes 
statistical tests chi squared test mutual information test find conditional independence relationships attributes relationships constraints construct bn 
algorithms referred algorithms constraint algorithms spirtes glymour cheng 
heckerman 
compare general learning show scoring methods certain advantages ci methods terms modeling distribution 
friedman 
show theoretically general scoring methods may result poor classifiers classifier maximizes different function viz classification accuracy 
greiner 
reach albeit different analysis 
scoring methods efficient practice 
demonstrates ci learning algorithms suffer drawbacks learning bn classifiers 
famous chow liu algorithm learning tree bns complete data provably finds optimal term loglikelihood score tree structured network pair wise dependency calculations heuristic search 
interestingly algorithm features scoring methods ci methods algorithm find structure best score analyzing pair wise dependencies 
unfortunately unrestricted bn learning connection scoring ci methods 
simple bn classifiers na bayes na bayes bn discussed duda hart simple structure classification node parent node nodes see 
connections allowed na bayes structure 
simple na bayes structure na bayes effective classifier years 
advantages classifiers 
easy construct structure priori structure learning procedure required 
second classification process efficient 
advantages due assumption features independent 
independence assumption obviously problematic na bayes surprisingly outperformed sophisticated classifiers large number datasets especially features strongly correlated langley 
improving na bayes years lot effort focussed improving na bayesian classifiers general approaches selecting feature subset relaxing independence assumptions 
selecting feature subsets langley sage forward selection find subset attributes subset construct selective bayesian classifier na classifier variables 
kohavi john best search accuracy estimates find subset attributes 
algorithm wrap classifiers including decision tree classifiers na classifier pazzani algorithm pazzani performs feature joining feature selection improve na bayesian classifier 
relaxing independence assumption kononenko algorithm kononenko partitions attributes disjoint groups assumes independence attributes different groups 
friedman 
studied tan allows tree structures represent dependencies attributes see section 
experiments show tan outperforms na bayes maintaining computational simplicity learning 
presents methods learning sections classifiers minimum description length mdl scoring method deciding different structures 
combining approaches singh provan combine approaches performing feature selection information theoretic methods scoring bn learning method learn selective bn subset features 
motivations algorithms accurate na bayes classifiers lot done 
particularly interested questions 

mdl scoring functions learning unrestricted bayesian networks may result poor classifier 
friedman natural question non scoring methods condition independence ci test methods mutual information test chi squared test methods learn classifiers 

treat classification node ordinary node learn unrestricted bn get natural feature subset markov blanket section classification node 
take approach select features earlier approaches perform feature selection bn learning 

parameters increases risk overfitting relatively small training sets 
cope concern 
overfitting phenomenon occurs model tries fit training set closely generalizing 
overfitted model perform data outside training samples 

efficiency major reason learning tan na bayes 
expensive learn gbn ban classifiers 
investigate questions empirical study 
variants general algorithm conditional independence tests learn 
empirically compared classifiers tan na bayes datasets uci machine learning repository murphy aha 
results motivate new type classifier wrapper gbn ban empirically evaluated 
learning bayesian network classifiers section presents algorithms learning different successively general types bayesian network classifiers differ structures permitted 
recall section learning parameters structure straightforward 
methods implemented top program cheng implements bn learning algorithms case node ordering cbl algorithm cheng case node ordering cbl algorithm cheng 
node ordering specifies total order nodes insist node ancestor node appears earlier order 
treat classification node node ordering view order nodes arbitrary simply order appear dataset 
need cbl algorithm uses mutual information tests number attributes dataset linear number cases 
efficiency achieved directly extending tree construction algorithm chow liu phase bn learning algorithm drafting essentially chow liu algorithm thickening adds edges draft thinning verifies necessity edge 
sufficient number samples algorithm guaranteed learn optimal structure underlying model data dag faithful spirtes glymour 
correctness proof complexity analysis detailed information please refer cheng 
na bayes procedure learning na bayes 
classification node parent nodes 

learn parameters recall just empirical frequency estimates output na bayes bn 
tree augmented na bayes tan 
represent node set classification node data 
algorithm learning tan classifiers friedman learns tree structure mutual information tests conditioned adds link classification node feature node similar na bayes structure classification node parent nodes see :10.1.1.30.9978
note features form tree 
simple tan structure tan learning procedure 
take training set input 

call modified chow liu algorithm 
original algorithm modified replacing mutual information test conditional mutual information test 
add parent 
learn parameters output tan 
complete algorithm extends chow liu algorithm requires conditional mutual information tests 
bn augmented na bayes ban ban classifiers extend tan classifiers allowing attributes form arbitrary graph just tree friedman see :10.1.1.30.9978
ban learning algorithm just tan learner section step ban learner calls unrestricted bn learning algorithm algorithm 
letting 
represent feature set classification node data learning procedure mutual information test described follows 
simple ban structure 
take training set node ordering input 

call modified cbl algorithm modified replacing mutual information test conditional mutual information test replacing conditional mutual information test 
add parent 
learn parameters output ban 
tan learning algorithm algorithm require additional mutual information tests requires mutual information tests 
general bayesian network gbn bn classifier learners gbn learner treats classification nodes ordinary node see 
learning procedure described follows 

take training set feature set node ordering input 

call unmodified cbl algorithm 

find markov blanket classification node 

delete nodes outside markov blanket 

learn parameters output gbn 
algorithm requires mutual information tests 
simple gbn experiments methodology experiments carried datasets downloaded uci machine learning repository 
choosing datasets selected datasets large numbers cases allow measure learning classification efficiency 
preferred datasets continuous features avoid information loss discretization able compare learning accuracy algorithms fairly 
needed discretize continuous features discretization utility mlc kohavi default setting 
datasets summarized table 
cv stands fold cross validation 
table datasets experiments 
instances dataset attributes 
classes train test adult nursery mushroom chess dna car cv flare cv vote cv brief descriptions datasets 
adult dataset data extracted census bureau database 
prediction task determine person year 
discretization process ignores attributes learners omitted rest attributes experiments 
nursery ranking nursery school applications features 
mushroom classifying type mushroom edible 
missing values treated having value experiments 
chess chess game result classification board descriptions 
dna recognizing boundaries exons introns sequence dna 
car dataset car evaluation features car 
flare classifying number times occurrence certain type solar flare 
vote voting records classify democrat republican 
experiments carried follows 
learning algorithms section learn classifiers type export bns bayesian interchange format bif files 
gbn ban classifiers learned default threshold setting 
threshold determines mutual information nodes considered significant see section tan na bayes learning algorithms threshold 
test classifiers test sets modified version cozman 
added classes read test datasets perform classification bn 
classification case test set done choosing class label value class variable highest posterior probability instantiations feature nodes 
classification accuracy measured percentage correct predictions test sets loss function 
experiments performed pentium ii mhz pc mb ram running nt 
results table provides prediction accuracy standard deviation classifier 
ordered datasets training sets large small 
best results dataset emphasized boldfaced font 
list best results reported literature data sets far know 
table see gbn ban tan better performance na bayes experiments 
ban best datasets gbn tan best datasets 
data sets nursery car gbn classifier inferior na bayes 
reason cases gbn reduced na bayes missing links reduced na called selective na bayes 
reveals features datasets independent 
note selective na bayes appropriate situations 
ban tan classifiers weak dependencies features classification node successfully captured 
weak dependencies improve prediction accuracy significantly see table 
gbn classification node treated way feature nodes weak dependencies captured 
suggests treating classification node differently useful domains 
measured running time procedures 
table gives total learning time bn classifier includes time learning structure parameters 
shows bn classifiers learned efficiently longest learning time minutes 
unrestricted bn classifier learning procedures times slower tan learning procedure 
took minutes learn ban adult data 
training set cases attributes efficiency quite satisfactory 
learning ban dna data take long time minutes suggests threshold small see section 
table running time cpu seconds classifier learning procedure 
gbn ban tan adult nursery mushroom chess dna car flare vote experiments classification process efficient 
perform classifications second depending complexity classifier 
faster na bayes classification contain subset features 
extensions mentioned earlier gbn ban classifiers learned default threshold setting 
experience threshold setting appropriate domains dataset large 
dataset small high setting cause missing edges low setting cause overfitting 
situations decrease prediction accuracy gbn ban 
table see gbn ban produce outstanding results datasets large domains 
example adult data gbn gives best result far know mushroom data ban gives prediction accuracy 
dataset gbn gives accuracy features 
datasets small evidence show threshold appropriate 
example flare data gbn uses features probably harsh 
example note gbn ban learned dna data perform worse tan na classifiers 
examining obtained gbn ban structures complex suggests threshold probably small learner overfitting 
advantage tan classifier need threshold 
believed proper threshold setting unrestricted bn classifiers gbn ban outperform tan classifier dataset small 
section propose wrapper algorithm searches optimal threshold demonstrate effectiveness datasets 
table find datasets unrestricted bn classifiers gbn ban give best performance 
suggests wrapping algorithms returning winner probably lead better performance see section 
improved bn classifier analysis experimental results suggests ways improve unrestricted bn classifiers automatic threshold selection prediction accuracy 
wrapping gbn ban returning winner 
algorithm automatic threshold selection bn construction fung crawford 
propose wrapper algorithm incorporates ideas 

partition input training set internal training set internal holdout set 

call gbn learner different threshold settings select gbn performs best holdout set 

call ban learner different threshold settings select ban performs best holdout set 

select better classifiers learned step step 
table experimental results gbn selected fea total 
fea 
ban tan na bayes best reported adult nursery mushroom chess dna car flare 
vote 
keep classifier structure re learn parameters conditional probability tables training set 

output new classifier 
training set large cross validation evaluate performance classifier 
wrapper algorithm fairly efficient reuse mutual information tests 
note mutual information tests take running time bn learning process 
accuracy estimation process slow predictions second 
test performance wrapper largest datasets 
experiment training set internal training set rest internal test set 
table see wrapper significantly better gbn ban datasets 
dna dataset searching optimal threshold prediction accuracy gbn improved classifier best results reported literature 
possible way improving bn classifiers providing domain knowledge node ordering forbidden links cause effect relationships 
help bn learners find better bn structure 
cbl learn bn classifiers prior node ordering specifying arbitrary node ordering 
empirically evaluated compared bn classifiers 
unrestricted classifiers gbn ban learned variants cbl algorithm give encouraging results wrapper function 
analyzing experimental results proposed wrapper algorithm wraps gbn ban demonstrated wrapper classifier give better results gbn ban classifiers 
table experimental results wrapper gbn ban fixed threshold optimal threshold fixed threshold optimal threshold wrapper best gbn ban optimal thresholds best reported adult unchanged unchanged nursery mushroom unchanged unchanged chess dna experiments see time expenses unrestricted bn learning times slower efficient 
due phase learning mechanism cbl algorithm 
theoretical analysis friedman empirical comparison results methods data sets reported friedman singh provan believe methods ci tests mutual information tests suitable bn classifier learning standard methods 
note addition mutual information tests standard decision tree learning feature selection 
advantage learners constraint 
means incorporate domain knowledge relatively easily just adding additional constraints 
additional information lead better classification accuracy 
experiments show classification performed efficiently general bn classifiers 
results believe improved type bn classifiers ones shown real world data mining machine learning applications 
cheng bell liu 

algorithm bayesian belief network construction data 
proceedings ai stat pp florida 
cheng bell liu 

learning belief networks data information theory approach 
proceedings acm cikm 
cheng 

system 
www cs ualberta ca htm 
chow liu 

approximating discrete probability distributions dependence trees 
ieee trans 
information theory pp 
cooper herskovits 

bayesian method induction probabilistic networks data 
machine learning pp 

cozman 

system 
www cs cmu edu 
duda hart 

pattern classification scene analysis 
john wiley sons 
friedman geiger goldszmidt 

bayesian network classifiers 
machine learning pp 

fung crawford 

constructor system induction probabilistic models 
proceedings aaai 
greiner grove schuurmans 

learning bayesian nets perform 
proceedings uai 
heckerman 

tutorial learning bayesian networks 
tech report msr tr microsoft research 
heckerman meek cooper 

bayesian approach causal discovery 
technical report msr tr 
microsoft research 
kohavi john long manley pfleger 

mlc machine learning library 
proceedings sixth international conference tools artificial intelligence 
ieee computer society 
kohavi john 
wrappers feature subset selection 
artificial intelligence journal special issue relevance vol 
pp 
kononenko 

semi na bayesian classifier 
kodratoff ed proceedings sixth european working session learning pp 
springerverlag 
langley iba thompson 

analysis bayesian classifiers 
proceedings aaai pp 

langley sage 

induction selective bayesian classifiers 
proceedings uai 
murphy aha 

uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
pazzani 

searching dependencies bayesian classifiers 
proceedings ai stat 
pearl 

probabilistic reasoning intelligent systems networks plausible inference morgan kaufmann 
singh provan 

efficient learning selective bayesian network classifiers 
proceedings icml 
spirtes glymour scheines 

causation prediction search 
hss cmu edu html departments philosophy ad book book html 
