accelerating em empirical study luis ortiz leslie pack kaelbling computer science department box brown university providence ri usa cs brown edu applications require learn parameters model data 
em expectationmaximization method learning parameters probabilistic models missing hidden data 
instances method slow converge 
accelerations proposed improve method 
proposed acceleration methods theoretically dominant experimental comparisons lacking 
different proposed accelerations compare experimentally 
results experiments argue acceleration em possible acceleration superior depends properties problem 
applications artificial intelligence statistics require fitting parametric model data 
desired find maximum likelihood ml maximum posteriori probability map model data 
variables model directly observable data relatively straightforward 
variables hidden common popular model classes bayesian networks hidden variables hidden markov models estimation complicated 
problem cast directly optimization problem data set model form find setting parameters maximizes likelihood dj jd map case 
unfortunately objective function form easily optimized globally generally reduced local search methods 
material supported part national science foundation graduate fellowship darpa rome labs planning initiative 
supported part darpa rome labs planning initiative 
probably naive strategy perform simple gradient ascent likelihood 
method problems 
known relatively inefficient class local search methods 
second case model class constrains choice parameters 
example components may constrained describe probability distribution sum 
naive gradient methods specially modified respect constraints 
em algorithm described dempster generalization baum welsh algorithm learning hidden markov models rabiner 
typically monotonically convergent local optimum likelihood space probabilistic models directly satisfying possible constraints parameters 
main classes strategies finding maximum likelihood models hidden parameters accelerated gradient methods constraint handling em 
observed empirically settings algorithm appears better settings algorithms appear better 
furthermore number researchers statistics ai literatures proposed extensions accelerations combinations methods 
seek understand relative merits optimization strategies extensions 
attempts theoretical comparisons convergence rate xu jordan results clear cut depend properties individual problems applied 
undertaken empirical study simplest hidden variable models density estimation mixture gaussians 
infinite number possible situations encounter hope empirical results suggesting merits disadvantages method situations 
believe study yields insight general properties methods results guaranteed transfer 
sections describe problem common optimization methods available solve experiments ran results obtained 
please refer technical report ortiz kaelbling additional details 
density estimation mixture gaussians simplest ml estimation problems hidden variables model probability density data set mixture gaussians 
model assumes number underlying centers dimensional space 
data point independently generated choosing center probability drawing gaussian distribution mean center covariance matrix autoclass cheeseman casts problem centers bayesian perspective stating probable number centers data 
address subproblem autoclass desired number centers find model maximizes posterior probability 
problem parameter vector independence data points write logarithm likelihood parameters respect data dj sum logarithm likelihood respect individual points ln dj ln xj multivariate gaussian density parameters 
constraints symmetric positive definite matrix 
sufficient maximize order maximize dj 
unfortunately direct method performing ml estimation 
gradient methods simplest gradient method find maximum log likelihood function gradient ascent shewchuk bertsekas polak 
case starting initial values parameters iteration obtain new values follows rl rl gradient log likelihood function evaluated current values parameters step size take uphill gradient direction 
value fixed predetermined decrease iteration 
method respect constraints parameters 
satisfy constraints project part gradient relevant parameters constraint space take step size take space bertsekas binder 
projected gradient denotes th component vector strategy parameterize terms parameter unconstrained assignment satisfies constraints way doing follows 
satisfy constraint verify step size take constraint space decrease step size get step size take constraint space 
fixed predetermined step size step slow convergence 
optimize step size step trying find largest value function direction gradient step means line search gradient ascent argmax rl method appealing drawback hessian function local optimum function elongated exhibits zig behavior significantly slow convergence bertsekas 
conjugate gradient method tries eliminate behavior optimized step size gradient ascent requiring optimize conjugate directions step 
formally starting initial setting parameters rl direction iteration follows argmax line search take best step current direction rl new gradient mod start weight current direction new conjugate direction disregard line search iterations conjugate gradient method appealing property number iterations convergence close solution roughly equal number parameters 
sophisticated methods newton quasi newton variable metric try solve deficiencies fixed optimized step size gradient ascent reshaping function 
methods additional information function higher order derivatives 
higher order derivatives requires additional storage perform additional computations typically outweigh reduction number iterations 
em basics em method optimizing log likelihood functions case missing data hidden variables dempster mclachlan krishnan 
starting initial value parameters iteration uses value parameters compute distribution density hidden variables conditioned data expectation step uses distribution get new values parameters maximization step 
likelihood function monotonically increases iteration regularity conditions likelihood function improvement strict stationary point likelihood function wu 
case mixture gaussians iteration find data point center conditional probability center generated data point probability distribution assign new values probability mean covariance matrix center 
expectation step providing maximization step information allow compute expected sufficient statistics center conditional probability distribution center data current value parameters 
em uses expected sufficient statistics maximization step true sufficient statistics obtain new values parameters 
specifically starting initial setting parameters iteration em algorithm mixture gaussians step compute distribution induced ij jjx step update parameters ij ij ij formally close solution means neighborhood local optimum approximated quadratic function 
ij ij mixture gaussians model method typically converges local maximum likelihood function stationary points go singular point likelihood function grows bound redner walker 
singular point mixture model occurs data points means variance center go zero duda hart 
practice avoid singularities initializations 
cases assign small value variances go small threshold value delete components small variances priors perform map estimation restart em different initial value parameters 
view em special form general gradient method 
allows see em function optimizing better conditioned xu jordan 
allows analyze convergence theoretically 
mixture gaussians model method gives updates automatically satisfy constraints parameters case true probability sufficiently large redner walker xu jordan 
practice numerical errors take parameter space 
cases take smaller step direction em update guarantee constraint satisfaction eliminate components put small thresholds priors perform map estimation 
acceleration methods em appealing monotonicity property convergence rate significantly slow instances 
mixture gaussians em slows centers gaussian components close redner walker xu jordan 
alternative start optimization em move gradient methods close solution 
accelerate em directly information em iteration mclachlan krishnan 
direct accelerations parameterized em peters walker redner walker bauer 
starting initial setting parameters iteration get new values parameters follows em em em update respect current parameters method change values parameters em iteration take step uphill direction current position value parameters 
step size fixed gradient ascent optimized step optimized step size gradient ascent 
equivalent em 
case mixture gaussians achieve convergence method close solution redner walker improve convergence speed xu 
parameterized em gradient steepest ascent find zero function change parameters provided em finding fixpoint em update 
acceleration method conjugate gradient acceleration em thiesson 
idea change value parameters em iteration find better conjugate directions performing conjugate gradient 
method uses information provided em iteration reshape function improve convergence speed close solution 
formally starting initial setting parameters rl direction em iteration follows argmax line search take best step current direction rl new gradient em em direction mod start weight current direction em information new conjugate direction method special form generalized conjugate gradient method 
interesting aspect conjugate gradient acceleration em contrary traditional generalized conjugate gradient method require specification preconditioning matrix evaluation second order derivatives matrix vector multiplications change parameters provided em update rule approximates generalized gradient 
conjecture relationship parameterized em conjugate gradient em information similar relationship fixed optimized step size gradient ascent regular conjugate gradient finding step size problem dependent optimizing step size idea produces zig behavior moving conjugate directions better 
pointed anonymous reviewer extensions em algorithm speed convergence 
due lack space refer reader mclachlan krishnan additional information extensions variations em 
extensions em algorithm ecm expectation conditional maximization meng rubin expectation conditional maximization liu rubin algorithms 
useful regular maximization step complex closed form optimization simpler conditioned function current value parameters 
furthermore differs ecm conditionally maximize log likelihood function directly steps 
ecm typically slower convergence rate em faster actual total computing time 
convergence rate typically faster em ecm actual computing time convergence typically faster 
maximization step simple context mixture gaussians model extensions really apply case reparameterized equation 
case version ecm algorithm speed convergence respect typical alternative algorithm generalized version em 
typically slower regular em 
note version ecm algorithm help learning context mixture experts architecture jordan xu 
experiments theoretical convergence speed different methods problem dependent 
theoretically dominant 
empirical evidence em superior gradient ascent mixture gaussians case xu jordan 
conjugate gradient acceleration methods close solution 
argue running conjugate gradient idea requires precise line search far solution 
practice precise line searches increase time convergence 
methods compare em idea uses monotonic convergence properties em 
algorithmic description follows thiesson starting initial setting parameters repeat run em get close solution run acceleration stopping condition needed inexact line searches accelerations save time 
close solution 
decrease loglikelihood occur due inexact line search 
decrease log likelihood occurs acceleration return em repeat process 
interpret condition close solution true change log likelihood 
means continue run em long statistic testing equality successive iterates 
needed line search adapted version secant line search 
note methods line search parameterized em need sure step size take outside constraint space 
cases reduce step size find keeps inside parameter space 
basis empirical analysis idea needed compute gradient em update approximately 
long sufficiently large extra computation significant compute time require information 
see expression gradient xu jordan 
expected counts center ij em em em result applying em update rule em em vec em em em 
note require computation expected sufficient statistics ij ij ij obtain em update 
takes dealing independent features em update computing em direction takes md extra computing gradient takes md extra case independent features bounds md 
note constants bounds extra small 
sufficiently large time compute expected sufficient statistics dominates 
say computation gradient em iteration time em equivalent iterations 
way need compare cpu times significantly depend implementation details different methods 
need optimize methods respect iterations 
different initialization methods 
studied large dimensional data context naive bayesian network model meila heckerman 
simplicity initialization initialize uniform random sample space distribution events 
initialize center sampling uniformly random space defined hypercube minimum volume containing data points 
initialize center diagonal matrix variances equal square distance center closest bishop 
assuming complexity exponentiation 
remaining issue 
ideally iterative method reached values parameters provide model 
clear way iterative method determine 
detecting crucial hard problem general 
instance common encounter situations function want optimize areas large small changes local optimum 
turn causes methods produce burst large small improvements handled stopping rules 
different stopping rules optimization literature bertsekas 
deal stopping problem simple typical stopping rule progress method log likelihood log posterior space change log likelihood iteration obtain information need test condition easily em equivalent iteration 
experiments synthetic data methods tested converged point log likelihood space parameter space modulo symmetrically equivalent models 
methods tested regular em em algorithmic structure acceleration step differed 
tried accelerations regular conjugate gradient cg conjugate gradient em information cg em parameterized em inexact line search optimize step size pem opt parameterized em fixed step sizes pem pem 
conjugate gradient em information reparameterized 
space cg em rp 
order examine properties different methods tested data generated simple models varying degree separation gaussians 
generated data models gaussians dimensions 
models parameters 
center gaussians models 
models differed center second gaussian 
generated data set points models see 
generated initial sets parameters method theory convergence speed log likelihood space faster parameter space 
cases methods converged parameter values equivalent models need compare terms kl divergence resulting model true model 
em algorithm case generalized em algorithm exact closed form optimization maximization step 
data set data set data set data sets 
data set method num 
iters 
speed em cg cg em cg em rp pem opt pem pem table table presents average number em equivalent iterations method took converge different data sets set experiments 
table approximate confidence intervals average speed methods respect number iterations taken em speed run number iterations em number iterations acceleration run 
ran algorithm data set starting initial parameters 
table presents results 
data set results column average number em equivalent iterations method 
results second column average speed 
define speed achieved proposed acceleration method run number iterations em divided number em equivalent iterations method run 
average speed may better measure drastically influenced cases hard 
performed bootstrap version shift method sided paired sample test cohen compare method best average number iterations speed methods 
data set results bold method best empirical mean 
test reject null hypothesis difference mean significant respect method best empirical mean methods results bold 
results experiment show accelerating cg em cg em rp significantly improve convergence speed gaussians get closer 
gaussians farther apart accelerations pem pem slow convergence due false starts acceleration false closeness solution 
improvement conver scatter plot cg em vs em data set runs iters em scatter plot number em equivalent iterations cg em vs number iterations em data set 
data set data set data set 
data set method easy hard num 
iters 
speed em cg em pem opt pem pem table table presents average number em equivalent iterations method took converge approximate confidence intervals average speed different data sets second set experiments 
scatter plot pem vs em data set runs iters em scatter plot number em equivalent iterations pem vs number em iterations data set 
linear behavior typical fixed step size parameterized em data sets 
gence speed provided pem pem impressive cg em cg em rp hard instances 
slow downs produced attempted accelerations tend occur mainly easier instances models means problem severe potential improvements hard instances model means 
presents scatter plot behavior cg em data set 
note small values confidence intervals speed fixed stepsize parameterized em method behavior size parameterized em methods general consistent 
results reported tried running em reparameterized space running conjugate gradient experiments success average 
ran experiments models gaussians higher dimensions similar results 
models random models different characteristics 
models 
differ data generated model harder distinguish different clusters generated model see 
model larger 
generated data set points model points model points model 
table presents results 
results averages random initial settings parameters models respectively 
cg em superior hard case slow easy cases slow severe 
note pem number iterations histogram number iterations em data set runs histogram number iteration em data set 
pem consistently better em 
note runs missing results table model 
runs em took iterations run compared pem pem pem opt 
stopped cg em iterations 
cg em pem opt failing line search running time value log likelihood point methods converged run smaller common 
suspect saddle point flat region log likelihood space 
run took iterations run compared cg em pem opt pem pem 
inspection behavior pem opt showed method wasting time line searches failing immediately going back em immediately attempt 
run point methods converged common point log likelihood space 
note behavior runs uncommon suggested histogram number iterations em data set largest values histogram runs mentioned 
anonymous reviewer pointed stopping rule scale 
insensitive range log 
restating issue stopping rules note partial preliminary experiments suggest scaled stopping rule help reduce number iterations required em pem hard instances 
cases log likelihood func inexact line search optimum number trials failing set 
instance suggested anonymous reviewer change log likelihood iteration relative total change far smaller threshold 
method iras data results num 
iters 
speed em cg em pem table table presents average number iterations speed method took converge iras data set starting initially 
tion ill conditioned stopping relatively flat region close solution produces estimates regard maximizing log likelihood 
help preventing fitting amount data small important issue learning models data 
partial preliminary experiments suggest different threshold values different instances scaled stopping rule increases potential stopping early easier instances producing bad estimates 
stopping rules em know study done compare 
conjecture right stopping rule eliminates need type accelerations proposed eliminates basis 
proposed accelerations close solution problem ill conditioned exactly right stopping rule detect 
ran experiments infrared astronomical satellite iras data set cheeseman cheeseman stutz 
data set contains data points dimensions 
autoclass classes mixture gaussians model independent features 
high dimensionality problems data poses performed map estimation model independent features took careful steps way computed expected sufficient statistics value change log posterior 
implementation similar autoclass 
assumed priori parameters independent 
dirichlet prior parameters prior counts set uniform prior hypercube minimum volume containing data points mean parameters gaussian components 
scaled inverse prior variances degrees freedom scale 
instances update rule took close boundary constraint 
solution problem eliminate components probability variances removed invalid components restarted method value parameters remaining components initial value parameters 
ran experiments assuming mixture gaussians model initially components 
generated random initial parameters adapted version initialization procedure autoclass ran method starting initial parameters 
table presents results runs 
note em stable progress iterations 
behavior noted redner walker xu jordan 
fixed step size parameterized em tends perform best data set respect speed convergence 
conjugate gradient acceleration em slow convergence 
understand result note shape log posterior valleys suggested typical behavior em data shown 
false closeness start acceleration really close solution 
methods line searches wasted time line searches fail attempt accelerate fails 
type behavior suggests need better ways signaling closeness stopping 
preliminary analysis indicate behavior em simply result performing map estimation 
wonder cause related fact ratio number parameters number samples small just consequence high dimensionality note authors performance em away solution impressive particularly log likelihood log posterior space 
right stopping rule problem typically produces reasonable estimates relatively fast 
addition typically exhibits global convergence practice 
properties monotonicity property simplicity em powerful method choice finding ml map estimates context mixture gaussians probabilistic models 
assumption amount data sufficiently large extra computation gradient em direction significant simple albeit conservative stopping rule experimental results synthetic data suggest method conjugate gradient acceleration em choice finding ml estimates mixture model 
significantly improves convergence hard cases slow convergence easy cases slow severe relatively low number iterations required converge cases 
addition numerical instability certainly reason 
global property optimization method able converge stationary point necessarily global optimum function space initial point parameter space 
em behavior signaling closeness iteration em behavior signaling closeness iteration em behavior signaling closeness iteration plots log posterior constants divided change log posterior relative change log posterior signaling closeness typical run em iras data set 
plot log posterior opposed log posterior reduce scale axis 
relative change current change divided previous change approximation convergence rate 
complicated implement parameterized em eliminates setting step size parameter 
furthermore requires line searches searches simple inexact neighborhood close solution 
behavior conjugate gradient acceleration em best function smooth flat elongated neighborhood local optimum 
hand results experiments iras data suggest idea attempt simple acceleration method fixed step size parameterized em trying complicated accelerations 
cases surface log likelihood relatively flat smooth neighborhood local optimum 
attempt complicated methods note simple acceleration method slow 
think necessary perform similar comparison analysis context learning bayesian networks hmms verify characterization superiority accelerations easy hard instances suggested carries 
eric bauer daphne koller yoram singer 
update rules parameter estimation bayesian networks 
dan geiger prakash shenoy editors proceedings thirteenth conference uncertainty artificial intelligence pages san francisco ca 
morgan kaufmann 
dimitri bertsekas 
nonlinear programming 
athena scientific belmont massachusetts 
john binder daphne koller stuart russell kanazawa 
adaptive probabilistic networks hidden variables 
machine learning 
christopher bishop 
neural networks pattern recognition 
oxford university press 
peter cheeseman james kelly matthew self john stutz taylor don freeman 
autoclass bayesian classification system 
proceedings fifth international conference machine learning 
peter cheeseman john stutz 
bayesian classification autoclass theory results 
usama fayyad gregory piatetsky shapiro padhraic smyth uthurusamy editors advances knowledge discovery data mining menlo park 
aaai press 
paul cohen 
empirical methods artificial intelligence 
mit press 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
richard duda peter hart 
pattern classification scene analysis 
wiley interscience 
robert 
conjugate gradient acceleration em algorithm 
journal american statistical society march 
theory methods 
michael jordan lei xu 
convergence results em approach mixtures experts architectures 
technical report memo memo massachusetts institute technology artificial intelligence laboratory center biological computational learning department brain cognitive sciences november 
liu donald rubin 
algorithm simple extension em ecm faster monotone convergence 
biometrika 
geoffrey krishnan 
em algorithm extensions 
wiley series probability statistics 
wiley interscience 
isaac 
fast improvement em algorithm terms 
journal royal statistical society 
marina meila david heckerman 
experimental comparison clustering initialization methods 
proceedings fourteenth conference uncertainty artificial intelligence 
xiao li meng donald rubin 
maximum likelihood estimation ecm algorithm general framework 
biometrika 
luis ortiz leslie kaelbling 
notes methods maximum likelihood estimation learning parameters mixture gaussians model 
technical report cs brown university providence ri february 
charles peters jr homer walker 
iterative procedure obtaining maximum likelihood estimates parameters mixture normal distributions 
siam journal applied mathematics september 
polak 
computational methods optimization unified approach volume mathematics science engineering 
academic press new york 
lawrence rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee volume pages february 
richard redner homer walker 
mixture densities maximum likelihood em 
siam review april 
jonathan richard shewchuk 
conjugate gradient method idiot understand 
technical report cmu cs school computer science carnegie mellon university pittsburgh pa march 
bo thiesson 
accelerated quantification bayesian networks incomplete data 
fayyad uthurusamy editors proceedings international conference knowledge discovery data mining pages 
aaai press 
jeff wu 
convergence properties em algorithm 
annals statistics 
lei xu 
comparative analysis convergence rates em algorithm modifications gaussians mixtures 
neural processing letters 
lei xu michael jordan 
convergence properties em algorithm gaussian mixtures 
neural computation 
