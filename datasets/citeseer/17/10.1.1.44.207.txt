boosting methodology regression problems greg david madigan thomas richardson box department statistics university washington seattle wa greg madigan tsr stat washington edu classification problems dominated research boosting date 
application boosting regression problems hand received little investigation 
develop new boosting method regression problems 
cast regression problem classification problem apply interpretable form boosted na bayes classifier 
induces regression model show expressible additive model derive estimators discuss computational issues 
compare performance boosted na bayes regression model interpretable multivariate regression procedures 

wide variety classification problems boosting techniques proven effective method reducing bias variance improving misclassification rates bauer kohavi 
evidence compiles utility techniques classification problems little known effectiveness regression problems 
freund schapire provide suggestion boosting produce regression models algorithm adaboost breiman suggests boosting apply regression problems algorithm arc gv promises study near 
actual implementation experimentation boosting regression models know drucker applies ad hoc modification adaboost regression problems obtains promising results 
develop new boosting method regression problems 
progress represents earliest connect boosting methodology regression problems 
motivated concept adaboost project regression problem classification problem dataset infinite size 
variant boosted na bayes classifier offers flexibility modeling predictive strength voting methods interpretability 
spite infinite dataset obtain closed form parameter estimates iteration boosting algorithm 
consequence model formulation na bayes regression model turns estimation procedure additive regression monotone transformation response variable 
derive boosted na bayes regression model bnb show results experiments discrete approximation 

boosting classification binary classification problems observe wish formulate model accurately predicts boosting describes general voting method constructing sequence models model uses different weighting dataset estimate parameters 
observations poorly modeled receive greater weight learning final boosted model combination predictions weighted quality classification training data 
boosting algorithm classification problems empirically yielded reduction bias variance misclassification rates variety base classifiers problem settings 
adaboost adaptive boosting algorithm dominant form boosting practice experimentation far 
adaboost proceeds follows 
initialize weight observation 

weights learn model 

compute error 
update weights observations scheme increases weights observations poorly predicted 
normalize sum 
classify new observation suggest combining classifiers log log prove boosting manner places upper bound final misclassification rate training dataset note long weighted misclassification rate classifiers slightly better worse random guessing bound decreases 
boosting drives training error zero boosted models tend overfit 
adaboost produced bounds generalization error vc dimension 
adaboost performance practice better bound implies 
empirical evidence shown base classifier fairly simplistic classification trees boosted capture complex decision boundaries breiman 
substituted na bayes classifier taylor series approximation sigmoid function obtain accurate interpretable boosted na bayes classifier 
equation shows version boosted na bayes classifier form log odds favor 
evidence weight boosted evidence ht prior boosted log log log estimate probability density function weighted likelihood account observation weights th boosting iteration 
weights individual classifiers assigned boosting algorithm 
boosted weights evidence version described 
positive weight corresponding indicates state evidence favor hypothesis 
negative weight evidence 
practice non boosted na bayes classifier consistently demonstrates robustness violations assumptions tends sensitive extraneous predictor variables 
note remains na bayes classifier boosted 
boosting biased estimates weights evidence favor improved misclassification rates 
subsequent classifiers place weight observations poorly predicted 
intuitively boosting weights regions sample space modeled exemplify violations model assumptions na bayes case conditional independence features 
similar weight evidence logistic regression proposal spiegelhalter knill jones boosting na bayes classifier shrinking effect weights evidence classifier optimism 
methods offset violations na bayes assumption build decision trees fit local na bayes classifiers leaves 
zheng webb give history methods propose new method 
leaf method fits na bayes classifier observation weighting assigns weight observations leaf weight observations outside leaf 
final model mixes leaves 
boosting performs similar manner 
partitioning dataset boosting smoothly learning iteration degree fit classifier observation 

boosting regression problems spite attention boosting receives classification methodology results exist apply ideas regression problems 
boosting effectiveness extends classification problems expect boosting simplistic regression models result richer class regression models 
breiman describes boosting method called arc gv date produced performance results 
drucker considered ad hoc boosting regression algorithm 
assigned weight observation fit cart model weighted sample 
similar adaboost algorithm classification set max offers candidate loss functions constrained 
definition remains reweighting proceeds adaboost fashion 
max manner boosting iteration constructed regression tree different weightings dataset 
lastly weighted median merge predictions regression tree 
method empirical analysis showed consistent improvement prediction error non boosted regression trees 
drucker methods share little common 
order extend theoretical classification results regression problems project regression dataset classification dataset apply adaboost algorithm 
algorithm proceeds similarly 

projecting observed data project data reduced adaboost space classification dataset way 
moment assume 
methodology readily extends real line 
transition classification problem expand size dataset 
consider toy dataset observations shown table 
transform original regression dataset new classification dataset follows 
table example data sequence equally spaced values interval 
secondly create cartesian product append dataset binary variable value table shows example transformation table 
call dataset observations 
construct classifier form 
words give model ask associated larger smaller probabilistic classifier may give probabilistic prediction 
note large precision exceeds precision transform classification dataset contains information regression dataset 
index observations observations 
point methodology methodology depart 
adaboost fits regression model regression dataset turn induces classifier classifier dataset ask regression model predicts greater value vector features performance induced classifier determines reweighting observations weight model 
adaboost drucker method fail weighted misclassification exceeds iteration 
practice method really guarantee constraint hold 
binary classification problems classifier performs poorly sense getting observation wrong adaboost classifier just gets right 
drawback led investigate avoid fitting regression model induces classifier fit classifier directly table transformed data tm tm tm tm 
classification infinite datasets classifier constructed predicted value smallest value predicts 
classifiers base classification rule estimates 
obtain prediction inf 
easily stated prediction equally uncertain true smaller larger 
concretely believe definition larger 
hand beliefs divided larger smaller 
situation reasonable prediction bears similarity slicing regression duan li 
stage potentially try fit classifier date just experimented na bayes classifier 

boosted na bayes classification infinite datasets generally na bayes classification assumes features independent class label 
setting features consist class label model corresponds factorization 
conditional independence assumption necessarily sensible 
fact positively correlated knowledge small highly informative small small 
surface na assumption necessarily appear reasonable 
rely robustness violations boosting ability compensate incorrectly specified models 
note exists construction lim lim implies lim lim na bayes model holds substituting computation regression prediction model log log log inf note equation bears resemblance equation 
call function left inequality 
necessarily non increasing increases 
large values right side evidence favor 
true na bayes model continuous function case smooth density estimator intermediate value theorem exists value equality holds case simplifies log log log continuous na bayes regression model additive model hastie tibshirani transformation response 
estimation additive regression model shown traditional model relies probability estimates backfitting friedman stuetzle 
usual additive model framework transformations response variable usually take form transformation stabilizes variance 
transformation response component model 
earliest boosted na bayes classification elkan showed equivalent non linear form logistic regression 
friedman shows boosting fits additive logistic regression model modified fitting procedure 
estimation components usual na bayes model fairly straightforward 
assuming mle simply count rows divided total number observations estimation discrete simple ratio counts 
estimation continuous may rely density estimate discretization 
estimation remains mathematically tractable resolution refined 
demonstrate consider simplest part estimation problem estimating approaches infinity 
indicates greatest integer function 
lim lim lim says randomly select observation draw number uniform presence sufficient data believe close new observation drawn distribution observations comprising difficulties arise simplest component model consider 
particularly definable 
clearly generate equally spaced 
accommodate assign finitely integrable weight function observation presumably weight neighborhood constrain functions ds initially fix ds estimate different sampling scenario 
sample observation probability selecting observation equal ds draw number wish compute 
derivations estimates follow section 

parameter estimation na bayes regression propose estimators components na bayes regression model 
derivations rely sampling scenario just described section 
particularly probability selecting observation ds ds ds ds ds ds ds ds ds ds ds see estimation prior incorporating weights total weight observations place region 
case expression reduces 
conditional density follows similar techniques 
ds ds ds ds conditional density proportional sum mass observation puts observations responses similar computation yields 
ds ds ds lastly model components 
case discrete ds ds ds ds case continuous ds ds ds ds form cdf continuous predictor resulting discreteness observed introduces unfortunate complexity estimation problem 
na bayes computation may require form non parametric density estimation discretization density smoothing algorithm 
derivation intuitive derive results directly maximizing weighted likelihood observations indexed ds nw model components wish estimate pp denotes product integral friedman 
log weighted likelihood ds log lastly utilizing na bayes assumption factor subsequently maximizing estimators previously derived follow 

boosted na bayes regression algorithm establishment weight functions observation leads directly application boosting 
manipulation weights central idea boosting previously mentioned manipulation improves misclassification rates application regression error 
constrain weight functions observation iteration may uniform 
extensions method real line involve modifying weight function finitely integrable tails function decays exponentially 
suggest initially laplace distribution weight functions form exp letting fairly large respect spread data laplace distribution flat may boosting algorithm drive modification weight functions 
consider propose initializing weight function poor choice weight function ties weight function increases weight regions far difficult region classify neighborhood classifier performs predicting smaller larger easy task 
usual idea boosting easy classify regions 
little surprise experiments algorithm initialized uniform boosting increased mass weight function neighborhood predicted true region misclassification phenomenon precisely opposite choice initial weighting 
shows typical collection weight functions iterations peaked true value 
example weight functions total weighted empirical misclassification rate iteration dy dy dy dy compiles preceding results boosted na bayes regression algorithm 
reweighting step algorithm complicated na bayes classifier puts probabilistic prediction 
abandon added information available probability estimates step merely weight update simpler 
prediction update step scales weight function interval note discontinuity indicator function occurs 
update scheme adaboost implement alternate scheme algorithm stores boosting iteration 
integrals estimation na bayes model integral step boosting procedure computable closed form integrals piecewise scaled sections laplace distribution 
change affect performance currently unclear 

experimental results 
methods experimental boosted na bayes regression uses discrete approximation algorithm developed previous section 
construct finite sequence evenly spaced values experiments fit boosted na bayes model 
experimenting way gave intuition performance method adaboost modifies weights hard classify regions 
show empirically boosted na bayes regression capture interesting regression surfaces 
experimentation discrete approximation able handle response bounded 
experiments shifted scaled response unit interval 
continuous predictors non parametric density estimator estimate 
loader local density estimator handle observation weights 
simulated test function generated observations training dataset observations validation set 
real datasets randomly selected half observations training remaining half validation set 
training dataset fit boosted na bayes regression model squares plane generalized additive model cart model 
replicated experiment times measured performance validation set mean squared bias 
bias squared mean boosting iterated log approached zero additional iteration contributed error improvement 
ran boosting iteration log fairly small 
stopping criterion generally affect performance validation set 
tested functions 
plane input sequence examples number boosting iterations initialize laplace density function mean scale 
estimate components na bayes regression model 

calculate loss model ds 
set 
update weight functions observation region heavily weighted 
normalize weights ds output model log log log inf log log boosted naive bayes regression algorithm friedman friedman sin friedman friedman friedman friedman tan tuned true underlying function explains variability bank dataset george mcculloch dataset contains financial information banks greater new york area 
selected eleven variables predicting number new accounts sold fixed time period 
body fat dataset penrose dataset contains physical measurements men 
set non invasive body measurements attempt predict body fat percentage 
datasets linearly scaled response fell interval 

performance results bnb gam lm cart bnb gam lm cart mean squared bias bnb gam lm cart bnb gam lm cart mean squared bias performance comparison plane friedman friedman friedman example plane squares linear model best predictive model fit 
naturally model outperform ordinary squares plane terms generalization error desire boosted na bayes regression model perform relatively 
shows bnb perform lm gam performance satisfactory 
friedman proposed friedman test learning noisy functions additive interactions 
furthermore introduces variables purely extraneous variables 
bnb outperformed linear model cart gam preceded performance 
shows performance validation datasets 
table performance results function model mean squared bias standard deviation bnb gam lm plane cart bnb gam lm friedman cart bnb gam lm friedman cart bnb gam lm friedman cart bnb gam lm friedman cart bnb gam lm bank cart bnb gam lm body fat cart friedman proposed learning functions friedman impedance phase shift specific circuit relate resistor generator capacitor 
show performance friedman 
function friedman bnb outperformed linear model cart performed little worse gam 
simulated function experiments bnb competitive gam friedman 
table summarizes performance results including performance real datasets 
bank dataset gam performed especially poorly bnb third lm cart 
simulated examples datasets don controlled error structure 
point uncertain sensitive bnb noisy data 
briefly investigated changes sample size affect performance including second analysis friedman 
cart bayes risk consistent regression procedure naturally gains substantially performance 
gam bnb improve slightly significant amount 
lastly univariate function 
bnb appealing multivariate regression problems include example easily visualized 
shows fit bnb linear threshold saturation model 
estimation procedure smooth necessarily translate smoothness bnb fit visibly jagged 
appear fitting correctly 
boosted na bayes regression linear threshold saturation model 

brought ideas boosting na bayes learning additive modeling induced regression models classification models 
derived bnb algorithm fit boosted na bayes regression model 
discrete approximation bnb compared performance interpretable multivariate regression procedures widely 
results show stage development bnb competitive established methods believe novelty unexpected satisfactory performance warrants research 
important aspect research perspective applying boosted na bayes classifier fashion provides early link advances boosting classification problems potential application regression contexts 
changes base classifier improved implementation research properties boosting may introduce new rich class regression models 
national science foundation supported dms 
bauer kohavi 
empirical comparison voting classification algorithms bagging boosting variants machine learning vv 
breiman 
arcing classifiers annals statistics 
breiman 
prediction games arcing algorithms technical report december statistics department university california berkeley 
friedman 
product integration applications differential equations addison wesley publishing 
drucker 
improving regressors boosting techniques proceedings fourteenth international conference machine learning pp 

duan li 
slicing regression link free regression method annals statistics 
elkan 
boosting na bayes learning technical report 
cs september ucsd 
freund schapire 
decision theoretic generalization line learning application boosting journal computer system sciences 
friedman hastie tibshirani 
additive logistic regression statistical view boosting technical report 
www stat stanford edu trevor papers boost ps 
friedman 
multivariate adaptive regression splines discussion annals statistics 
friedman grosse stuetzle 
multidimensional additive spline approximation siam journal scientific statistical computing 
friedman stuetzle 
projection pursuit regression journal american statistical association 
george mcculloch 
variable selection gibbs sampling journal american statistical association 

estimation probabilities essay modern bayesian methods mit press 
hastie tibshirani 
generalized additive models chapman hall 
loader 
statistical computing graphics newsletter april 
available cm bell labs com cm ms departments sia project 
penrose nelson fisher 
generalized body composition prediction equation men simple measurement techniques medicine science sports exercise vol 

madigan richardson kane 
interpretable boosted naive bayes classification proceedings fourth international conference knowledge discovery data mining 
spiegelhalter knill jones 
statistical knowledge approaches clinical decision support systems application discussion journal royal statistical society series 
zheng webb 
lazy bayesian rules technical report tr school computing mathematics deakin university victoria australia 
