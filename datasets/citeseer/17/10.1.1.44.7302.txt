feature selection ensembles david opitz computer science department university montana mt opitz cs umt edu traditional motivation feature selection algorithms find best subset features task particular learning algorithm 
success ensembles investigate notion ensemble feature selection 
task harder traditional feature selection needs find features germane learning task learning algorithm needs find set feature subsets promote disagreement ensemble classifiers 
ensemble feature selection approach genetic algorithms 
algorithm shows improved performance popular powerful ensemble approaches adaboost bagging demonstrates utility ensemble feature selection 
feature selection algorithms attempt find remove features unhelpful destructive learning almuallim dietterich shavlik kohavi john 
previous feature selection focused finding appropriate subset relevant features constructing inference model ensemble shown combining output set models generated separately trained inductive learning algorithms greatly improve generalization accuracy breiman maclin opitz shapire :10.1.1.32.9399
argues importance presents approach task feature selection ensembles 
research shown ective ensemble consist set models highly correct ones errors different parts input space hansen salamon krogh vedelsby opitz shavlik copyright american association artificial intelligence www aaai org 
rights reserved 

varying feature subsets member ensemble help promote necessary diversity 
traditional feature selection algorithms goal finding best feature subset germane learning task selected inductive learning algorithm task ensemble feature selection additional goal finding set features subsets promote disagreement component members ensemble 
search space enormous non trivial problem 
genetic algorithm ga approach searching appropriate set feature subsets ensembles 
gas logical choice shown ective global optimization techniques holland mitchell 
approach works creating initial population classifiers classifier generated randomly selecting di erent subset features 
continually produce new candidate classifiers genetic operators crossover mutation feature subsets 
algorithm defines fitness individual combination accuracy diversity 
fit individuals population turn comprise ensemble 
neural networks classifier results datasets show simple straight forward algorithm creating initial population produces better ensembles average popular powerful ensemble approaches bagging boosting 
results show running algorithm genetic operators improves performance 
review ensembles illustrates basic framework predictor ensemble 
predictor ensemble predictor predictor case trained training instances 
example predicted output predictors combined produce output predictor predictor combine predictor outputs ensemble output input predictor predictor ensemble 
ble 
researchers breiman hansen salamon opitz shavlik demonstrated ectiveness combining schemes simply weighted average predictors type ensemble focus :10.1.1.32.9399
combining output predictors useful disagreement inputs 
obviously combining identical predictors produces gain 
hansen salamon proved ensemble average error rate example predictors ensemble independent production errors expected error example reduced zero number predictors combined goes infinity assumptions rarely hold practice 
krogh vedelsby proved ensemble error divided term measuring average generalization error individual classifier term measuring disagreement classifiers :10.1.1.37.8876
formally showed ideal ensemble consists highly correct classifiers disagree possible 
numerous authors empirically verified ensembles generalize opitz shavlik breiman freund 
result methods creating ensembles center producing predictors disagree predictions 
generally methods focus altering training process hope resulting predictors produce di erent predictions 
example neural network techniques employed include methods training different topologies di erent initial weights di erent parameters training portion training set breiman freund schapire hansen salamon 
varying feature subsets create diverse set accurate predictors focus article 
numerous techniques try generate disagreement classifiers altering training set classifier sees 
popular techniques bagging breiman boosting particularly adaboost freund schapire 
bagging bootstrap ensemble method trains predictor ensemble di erent partition training set 
generates partition randomly drawing replacement examples training set size training set 
breiman showed bagging ective unstable learning algorithms neural network small changes training set result large changes predictions 
bagging adaboost chooses training set size initially sets probability picking example predictor probabilities change follows 
sum misclassified instance probabilities currently trained classifier adaboost generates probabilities trial multiplying probabilities incorrectly classified instances factor re normalizing probabilities sum equals 
adaboost combines classifiers 
weighted voting weight log 
numerous empirical studies shown bagging boosting highly successful methods usually generalize better base predictors bauer kohavi maclin opitz quinlan include methods baselines study 
ensemble feature selection kohavi john showed cacy set features learning depends learning algorithm appropriate feature subset learning algorithm may appropriate feature subset learner 
kohavi john wrapper approach works conducting search space possible feature subsets explicitly testing accuracy learning algorithm search node feature subset 
search space feature subsets enormous quickly impractical hill climbing searches traditional wrapper search technique slow training learners neural networks search space larger considering appropriate set feature subsets ensembles 
consider global search technique gas 
notion feature selection ensembles new 
researchers investigated gas feature selection shavlik guo uhrig looked aspect traditional feature selection find ing appropriate set learning ensemble perspective 
gefs algorithm table summarizes algorithm called gefs genetic ensemble feature selection uses gas generate set classifiers accurate diverse predictions 
focus neural networks gefs easily extended learning algorithms 
gefs starts creating training initial population networks 
representation individual population simply dynamic length string integers integer indexes particular feature 
create networks strings having input nodes match string integers creating standard single hidden layer fully connected neural network 
algorithm creates new networks genetic operators crossover mutation 
gefs trains new individuals 
adds new networks population scores population member respect prediction accuracy diversity 
gefs normalizes scores defines fitness population member accuracy diversity defines tradeo accuracy diversity 
gefs prunes population fit members repeats process 
point time current ensemble consists simply averaging equal weight predictions output member current population 
population evolves ensemble 
define accuracy network training set accuracy 
may validation set training instances 
define diversity average di erence prediction component classifier ensemble 
separately normalize terms values range 
normalizing terms allows meaning domains 
clear value set automatically adjust discrete derivatives ensemble error average population error average diversity ensemble 
change decreasing increase increasing population diversity decreasing decrease increasing decreasing 
started experiments article 
amount changes current value 
table gefs algorithm 
goal find set input subsets create accurate diverse classifier ensemble 

varying inputs create train initial population classifiers 

stopping criterion reached genetic operators create new networks 
measure diversity network respect current population 
normalize accuracy scores diversity scores individual networks 
calculate fitness population member 
prune population fittest networks 
adjust 
current population composes ensemble 
create initial population randomly choosing number features include feature subset 
classifier size feature subset independently chosen uniform distribution twice number original features dataset 
randomly pick replacement features include classifier training set 
note features may picked multiple times may picked replicating inputs neural network may give network better chance utilize feature training 
replicating feature genome encoding allows feature better survive generations 
crossover operator uses dynamic length uniform crossover 
case chose feature subsets individuals current population proportional fitness 
feature parent subset independently considered randomly placed feature set children 
possible feature set larger smaller largest smallest parent feature subset 
mutation operator works traditional genetic algorithms randomly replace small percentage parent feature subset new features 
operators network trained scratch new feature subset internal structure parents saved crossover 
gefs continually considers new networks include ensemble viewed anytime learning algorithm 
learning algorithm table summary data sets 
shown number examples data set number output classes number continuous discrete input features number input output hidden units neural networks tested epochs neural network trained 
features neural network dataset cases classes continuous discrete inputs outputs hiddens epochs credit credit diabetes glass heart cleveland hepatitis house votes hypo ionosphere iris kr vs kp labor letter promoters ribosome bind satellite segmentation sick sonar soybean vehicle produce concept quickly continue search concept space reporting new best concept opitz shavlik 
important domains expert willing wait weeks months learning system produce improved concept 
gefs inspired previous approach applying gas ensembles called opitz shavlik algorithms quite di erent 
far complex vary inputs genetic operators designed explicitly hidden nodes knowledge neural networks fact problems lacking prior knowledge 
results evaluate performance gefs obtained number data sets university wisconsin machine learning repository uci data set repository murphy aha 
data sets hand selected came real world problems varied characteristics deemed useful previous researchers 
table gives characteristics data sets 
data sets chosen vary number dimensions including type features data set continuous discrete mix number output classes number examples data set 
table shows architecture training parameters neural networks 
results averaged standard fold cross validation experiments 
fold cross validation data set partitioned sets set turn test set classifier trains sets 
fold ensemble networks created total networks fold cross validation trained neural networks standard backpropagation learning 
parameter settings neural networks include learning rate momentum term weights initialized randomly 
table shows architecture training parameters neural networks experiments 
chose number hidden units number input output units 
choice criteria having hidden unit output hidden unit inputs hidden units minimum 
parameter settings ga portion gefs includes mutation rate population table test set error rates data sets single neural network classifier bagging ensemble method adaboost ensemble method ensemble gefs initial population gefs run consider networks 
bottom table contains win loss tie comparison learning algorithms datasets 
traditional gefs dataset single net bagging adaboost initial pop networks credit credit diabetes glass heart cleveland hepatitis house votes hypo ionosphere iris kr vs kp labor letter promoters ribosome bind satellite segmentation sick sonar soybean vehicle single net bagging adaboost initial pop size search length networks note generations 
mutation rate may high compared traditional gas certain aspects approach call higher mutation rate goal generating population cooperates emphasis diversity mutation values tried pilot studies 
table shows error rates algorithms datasets 
points comparison include results running bagging adaboost algorithms 
described algorithms second section 
results gefs algorithm accuracy initial population created population size accuracy trained networks considered search initial population plus genetic operators 
convenience reader bottom table contains win comparison learning algorithms datasets 
comparison bold means di erence performance algorithms statistically significant confidence level tailed sign test 
discussion results confirm earlier findings maclin opitz quinlan bagging produces better classifier single neural network adaboost method powerful technique usually produce better ensembles bagging susceptible noise quickly overfit data set 
draw main new algorithm performance 
gefs produce initial population algorithm producing population simple fast 
fact initial population competitive bagging adaboost somewhat surprising 
shows cases diversity created pre varying feature set manner lost individual predictor accuracy feature set 
second draw running gefs longer usually increases performance 
desirable allows user fully utilize available computer cycles generate improved model 
running adaboost bagging longer appreciably increase performance previous results shown performance nearly fully asymptotes networks appear ability get better time 
gefs results impressive view just step ensemble feature selection 
important contribution simply demonstration utility creating ensemble feature selection algorithms 
improvements possible need explored 
area research combining gefs adaboost approach emphasizing examples correctly classified current ensemble 
plan investigation tuning parameters maximum size feature subsets initial population results experiments due limited space 
plan investigate applying gefs inductive learning algorithms decision trees bayesian learning 
argued importance feature detection ensembles algorithm gefs genetic algorithms 
ensemble feature selection approach straightforward simple generates results quickly ability increase performance allowed run longer 
results show gefs compared favorably powerful ensemble techniques adaboost bagging 
shows utility feature selection ensembles provides important ective step direction 
acknowledgments partially supported national science foundation iri university montana 
almuallim dietterich 
learning boolean concepts presence irrelevant features 
artificial intelligence 
bauer kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
breiman 
bagging predictors 
machine learning 
shavlik 
growing simpler decision trees facilitate knowledge discovery 
second international conference knowledge discovery data mining 
aaai mit press 
freund schapire 
experiments new boosting algorithm 
proceedings thirteenth international conference machine learning 
morgan kaufmann 
guo uhrig 
genetic algorithms select inputs neural networks 
int 
conf 
genetic algorithms neural networks 
hansen salamon 
neural network ensembles 
ieee transactions pattern analysis machine intelligence 
holland 
adaptation natural artificial systems 
ann arbor mi univ michigan press 
kohavi john 
wrappers feature subset selection 
artificial intelligence 
krogh vedelsby 
neural network ensembles cross validation active learning 
advances neural information processing systems volume 
cambridge ma mit press 
maclin opitz 
empirical evaluation bagging boosting 
proceedings fourteenth national conference artificial intelligence 
providence ri aaai mit press 
mitchell 
genetic algorithms 
mit press 
murphy aha 
uci repository machine learning databases machine readable data repository 
university california irvine department information computer science 
opitz shavlik 
actively searching ective neural network ensemble 
connection science 
opitz shavlik 
connectionist theory refinement searching network topologies 
journal artificial intelligence research 
quinlan 
bagging boosting 
proceedings thirteenth national conference artificial intelligence 
aaai mit press 
shapire freund bartlett lee 
boosting margin new explanation ectiveness voting methods 
proceedings fourteenth international conference machine learning 
morgan kaufmann 
