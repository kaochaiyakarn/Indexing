proceedings iasted international conference artificial intelligence soft computing august honolulu hawaii usa recognizing promoters dna bayesian neural networks ma jason wang department computer information science new jersey institute technology newark nj cis njit edu binary data classification recognize positive data unlabeled test data may contain positive negative data 
propose level approach recognize coli promoters unlabeled dna containing promoter sequences 
level classifiers include bayesian neural networks learn different feature sets 
outputs level classifiers combined second level give final result 
empirical study shows excellent performance proposed approach 
keywords computational biology medicine data mining classification neural networks promoter recognition 
result ongoing human genome project dna protein data accumulated speed growing exponential rate 
mining biological data extract significant information extremely important accelerating genome processing 
classification supervised learning major data mining processes 
classification partition set data categories 
categories called binary classification 
focus binary classification dna sequences 
binary classification training data including positive negative examples 
positive data belongs target class negative data belongs nontarget class 
goal assign unlabeled test data target class non target class 
case test data unlabeled dna sequences positive data promoters negative data non promoters 
goal identify promoters unlabeled dna sequences terms classification recognition interchangeably 
dietterich indicated ensemble classifiers achieve better recognition rate single classifier recognition rate individual classifier ensemble greater ii errors individual classifier uncorrelated 
experimental results show combined classifiers outperform individual classifiers solely bayesian neural networks 
ensemble classifiers process biomolecular data studied 
contrast previous apply bayesian neural networks recognize promoters dna 
bayesian neural network combines constituent neural networks marginalisation 
uncertainty constituent neural network taken account weighting neural network posterior probability 
furthermore bayesian neural network controls model complexity avoid overfitting problem propose level approach recognizing coli promoters dna sequences 
level classifiers include bayesian neural networks trained different feature sets 
outputs level classifiers combined second level give final classification result 
experimental results indicate excellent performance proposed approach 
rest organized follows 
section describes characteristics coli promoters feature extraction methods 
section presents level classification approach 
section presents experimental results 
section concludes 
promoter recognition coli promoters coli promoter located immediately coli gene 
successfully locating coli promoter identifying coli gene 
coli promoters contain binding sites coli rna polymerase kind protein binds 
binding sites box box respectively 
boxes separated spacer 
probable length nucleotides 
promoter sequences position nucleotide upstream transcriptional start site transcriptional start site 
spacer box transcriptional start site variable length 
probable length nucleotides 
salient features binding sites transcriptional start site non salient features regions reported literature 
characteristics coli promoter sequences reported literature explored methods extracting features regions promoters region nucleotides long region nucleotides long region nucleotides long region nucleotides long region nucleotides long transcriptional start region nucleotides long position transcriptional start region 
method maximal dependence decomposition mdd technique second motif method 
region nucleotides long nucleotide long motif region statistically significant apply mdd method extracting features region 
mdd technique mdd technique proposed detect splice site human genomic dna gene prediction software adopted latest version gene prediction software morgan mdd derived position weight matrix pwm described overcome limitation consensus sequence modeling nucleotide distribution position 
disadvantage pwm assumes positions independent 
disadvantage removed weight array model wam generalizes pwm allowing dependencies adjacent positions 
wam essentially order markov chain conditional probability upstream adjacent nucleotide generalized second order markov chain third order markov chain dependencies tries model free parameters model requiring training data appropriately estimate parameters model 
general danger tries complex models free parameters training data estimate free parameters 
instance suppose promoter sequences available estimate parameters pwm represents probability distribution position letter dna alphabet 
equivalently roughly promoter sequences available estimate parameters jx gamma wam jx gamma represents conditional probability distribution position gamma position gamma upstream neighbor position gamma promoter sequences reliably estimate free parameters 
mdd provides flexible solution problem iteratively clustering dataset significant adjacent non adjacent dependencies 
essentially models order secondorder third order higher order dependencies depending amount training data available 
specifically mdd works follows 
set aligned sequences chooses consensus nucleotide position case set includes positive training sequences 
chi square statistic ij calculated measure dependencies nucleotides position 
significant dependencies detected simple pwm 
significant dependencies detected dependencies exist adjacent positions wam 
mdd procedure carried 
mdd procedure iterative process calculate sum ij select position sm maximal decompose dataset disjoint subsets dm containing sequences consensus nucleotide km position gamma dm containing sequences consensus nucleotide km position 
mdd procedure applied recursively dm gamma dm respectively conditions holds decomposition possible significant dependencies positions resulting subsets exist number sequences resulting subsets threshold reliable estimation parameters possible decomposition 
apply mdd method region region region region region region region respectively training promoter sequences 
result region region modeled pwm level decomposition carried regions 
align subsequences region region region region need locate binding sites coli promoters box box 
locating box box done searching best match consensus nucleotides 
different locations best location chosen hierarchical criteria homogeneous conservation regions preferred spacer nucleotides region region preferred spacer nucleotides region transcriptional start site preferred 
motif method calculate motif feature values sequence apply pattern matching tool positive training data coli promoter sequences find weak motifs common subsequences region region region region region respectively sequences 
phase process 
phase creates generalized suffix tree gst sample set sequences traverses gst find segments candidate motifs satisfy minimum length requirement 
set includes subsequences region region positive training sequences 
phase ii ranks candidate motifs occurrence numbers sample evaluates candidate motifs respect entire set sequences 
occurrence number motif segment refers total number sequences set motif occurs 
region length motifs fixed 
study length regions length regions 
minimum occurrence number 
occurrence number motif assigned weight motif 
ensembles neural networks basic classifiers develop basic classifiers classifier classifier classifier 
classifiers bayesian neural network 
classifier classifier bayesian neural network hidden units input units including mdd features described section distance features second classifier classifier bayesian neural network hidden units input units including motif features described section distance features third classifier classifier bayesian neural network hidden units input units including mdd features motif features distance features 
number hidden units determined experimentally 
bayesian neural network hidden layer sigmoid activation functions 
output layer neural network output unit 
output value bounded logistic activation function gammaa neural network fully connected adjacent layers 
bayesian neural network integration bayesian inference neural network 
fx denote data set binary target value output unit 
architecture weights neural network output value uniquely determined input vector model vector parameters output value interpreted jx probability 
likelihood probability data djw pi gamma gammat djw djw gamma log gamma equation objective function maximized non bayesian neural network training process maximum likelihood estimate method assumes possible weights equally 
non bayesian neural network weight decay avoid overfitting training data poor generalization test data adding term ff objective function minimized penalize neural network weights large magnitudes penalizing complex model favoring simple model 
precise way specify appropriate value weight decay parameter ff hyperparameter 
bayesian neural network generalizes previous weight decay term associating weights biases different layers different variances 
hyperparameter vector ff 
hyperparameter ff interpreted parameter model optimized online bayesian learning process 
weight decay term interpreted prior probability weight vector gaussian distribution zero mean standard deviation ff larger neural network weights probable 
ew denote fff denote vector ff 
exp gamma ff zw zw exp gamma ff dw normalizing constant 
get posterior probability wjd ff exp gamma ff djw zm zm exp gamma ff djw dw normalizing constant 
bayesian training neural networks iterative procedure 
current implementation bayesian neural network iteration involves level inferences 
level value hyperparameter ff initialized random value iteration infer probable value weight vector mp corresponding maximum wjd ff neural network training minimizes ff gamma djw 
bayes rule inference level wjd ff djw djff second level hyperparameter ff optimized 
bayes rule second inference level ffjd djff dja lack prior knowledge assume constant value 
normalizing factor dja constant value value ff maximizing posterior ffjd inferred maximizing djff normalizing factor equal djw dw 
integrand approximated gaussian centered mp djff maximized 
new hyperparameter value ff iteration 
process iterates number times specified user 
classification phase bayesian classification models model 
model weighted posterior probability 
new input classification marginalisation classification model jx ff weighted posterior ffjd 
jx jx ff ffjd ffjd ffjd combining basic classifiers basic classifiers described previous subsection combined classifier 
classifier gives final classification result 
explore methods combine classifiers combiner combiner 
combiner unweighted voter 
basic classifiers agree classification results unidentified final result results classifiers classifiers agree classification result final result result classifiers total error rate specificity sensitivity table results basic classifiers 
represent classifier classifier classifier respectively 
combiner combiner total error rate specificity sensitivity table results second level ensemble 
classifiers disagree classification results final result result classifier min gamma output output minimal final result unidentified 
combiner weighted voter 
output weighted sum outputs classifiers 
weight classifier proportional classification rate error rate classifier training phase 
output combiner weight weight weight weight output experiments results data study coli promoter sequences taken coli promoter compilation 
took promoter sequences positive data 
negative data set retrieved known machine learning data repository university california irvine www ics uci edu ai ml 
html 
non promoter dna sequences nucleotides long 
concatenated dna sequences sequence nucleotides 
randomly chose subsequences nucleotides long negative data 
results table gives fold cross validated classification results basic classifiers 
tenfold cross validation data set randomly split mutually exclusive folds approximately equal size 
bayesian neural network trained tested times 
ith time trained gamma tested error rate total number incorrect classifications divided total number sequences test dataset 
false positive non promoter sequence misclassified promoter sequence 
true positive promoter sequence classified promoter sequence 
specificity defined gamma fp ng fp number false positives ng total number negative sequences 
sensitivity defined tp po tp number true positives po total number positive sequences 
classifiers combined table gives results combiner combiner 
table see combiner combiner outperform basic classifiers 
combiner gave best classification rate 

results indicate excellent performance proposed approach 
proposed level ensemble classifiers recognize coli promoter sequences 
classifiers include bayesian neural networks trained different feature sets 
outputs level classifiers combined give final result 
recognition rate achieved 
result better previous similar dataset 
currently extending approach classify protein sequences recognize full gene structure 
acknowledgments dr david mackay sharing bayesian neural network software 
dr dr margalit providing promoter sequences 
dietterich machine learning research current directions ai magazine 
brunak engelbrecht knudsen prediction human mrna donor acceptor sites dna sequence journal molecular biology 
wang marr shasha shapiro chirn lee complementary classification approaches protein sequences protein engineering 
zhang mesirov waltz hybrid system protein secondary structure prediction journal molecular biology 
mackay bayesian interpolation neural computation 
mackay practical bayesian framework backprop networks neural computation 
mackay evidence framework applied classification networks neural computation 
neal bayesian learning neural network lecture notes statistics volume new york springer verlag margalit compilation coli mrna promoter sequences nucleic acids research 
non canonical sequence elements promoter structure 
cluster analysis promoters recognized escherichia coli rna polymerase nucleic acids research 
burge karlin prediction complete gene structures human genomic dna journal molecular biology 
salzberg decision tree system finding genes dna technical report cs johns hopkins 
computer methods locate signals nucleic acid sequences nucleic acids research 
zhang marr weight array method splicing signal analysis computer application biosciences 
wang marr shasha shapiro chirn discovering active motifs sets related protein sequences classification 
nucleic acids research 
hirsh noordewier background knowledge improve inductive learning dna sequences proceedings tenth conference artificial intelligence applications pages 

