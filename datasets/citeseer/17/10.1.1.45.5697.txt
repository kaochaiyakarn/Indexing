neural networks time series processing georg dorffner dept medical cybernetics artificial intelligence university vienna austrian research institute artificial intelligence provides overview common neural network types time series processing pattern recognition forecasting spatio temporal patterns 
emphasis put relationships neural network models classical approaches time series processing particular forecasting 
begins basics time series processing discusses feedforward recurrent neural networks respect ability model non linear dependencies spatio temporal patterns 
world changing 
observe measure physical value temperature price freely traded bound different different points time 
classical pattern recognition large part neural network applications mainly concerned detecting systematic patterns array measurements change time static patterns 
typical applications involve classification input vectors classes discriminant analysis approximate description dependencies observables regression 
changes time taken account additional temporal dimension added 
large extent problem viewed classical pattern recognition terms additional important aspects come play 
field statistics concerned analysing spatio temporal data data spatial temporal dimension usually termed time series processing 
aims introducing fundamentals neural networks time series processing 
tutorial article naturally scratch surface field leave important details untouched 
provides overview relevant aspects form basis field 
guide detailed literature 
basic knowledge neural networks architectures learning algorithms assumed 
time series processing basics formal terms time series sequence vectors depending time components vectors observable variable instance ffl temperature air building ffl price certain commodity stock exchange ffl number births city ffl amount water consumed community theoretically seen continuous function time variable practical purposes time usually viewed terms discrete time steps leading instance point usually fixed size time interval 
speaks time sequence series 
size time interval usually depends problem hand hours days years 
cases observables available discrete time steps price commodity hour day naturally giving rise time series 
cases number births city values accumulated averaged time interval lead number births month obtain series 
domains time continuous temperature place observable measure variable points chosen time interval measuring temperature full hour obtain series 
called sampling 
sampling frequency number points measured resulting chosen time interval crucial parameter case different frequencies essentially change main characteristics resulting time series 
noted field closely related time series processing signal processing 
examples speech recognition detection abnormal patterns automatic staging sleep 
signal sampled sequence values discrete time steps constitutes time series defined 
formal distinction signal time series processing 
differences type prevalent applications recognition filtering signal processing forecasting time series processing nature time series time interval sampled signal usually fraction second time series processing interval hours upwards observation terms prototypical applications clear boundary drawn 
time series processing profit exploring methods signal processing vice versa 
overview neural network applications signal processing 
vector contains component case applications speaks univariate time series multivariate 
depends problem hand univariate treatment lead results respect recognizing patterns 
observables influence air temperature consumption water multivariate treatment analysis observables component indicated 
discussions follow concentrate univariate time series processing 
types processing depending goal time series analysis typical applications distinguished 
forecasting developments time series 
classification time series part thereof classes 
description time series terms parameters model 
mapping time series application type certainly wide spread imminent literature 
econometrics energy planning large number time series problem involve prediction values vector order decide trading strategy order optimize example ieee proceedings series resulting annual conference neural networks signal processing 
production 
formally problem described follows find function thetan dimension obtain estimate vector time values time plus number additional time independent variables exogenous features gamma called lag prediction 
typically meaning subsequent vector estimated take value larger prediction energy consumption days ahead 
sake simplicity neglect additional variables 
keep mind inclusion features size room temperature measured decisive applications 
viewed way forecasting problem function approximation chosen method approximate continuous valued function closely possible 
sense compared function approximation regression problems involving static data vectors methods domain applied see instance 
observation turn important discussing neural networks forecasting 
usually evaluation forecasting performance done computing error measure number time series elements validation test set gamma gamma function measuring single error estimated forecast actual sequence element 
typically distance measure euclidean depending problem function computing cost resulting forecasting incorrectly 
forecasting problems exact value required indication larger rising smaller falling remain approximately 
case problem turns classification problem mapping sequence part thereof classes rising falling constant 
general terms classification time series application type expressed problem finding function thetan assigning classes time series gamma set available class labels 
formally essential difference function approximation problem equation 
words classification viewed special case function approximation function approximated maps continuous vectors binary valued ones 
difference comes way problem viewed separation vectors sought approximation dependencies influence method derive function way performance evaluated 
typically error function takes form gamma ffi expressing percentages inputs correctly assigned desired class 
ffi ij kronecker symbol ffi ij iff 
known class label input distinction approximation regression classification discrimination pattern recognition vectors temporal dimension 
large number results methods domain time series classification 
difference domain time series processing classification exception classification rising falling usually retrospective time lag estimated output prospective forecast 
application type modeling time series implicitly contained instances forecasting classification 
function equation considered model time series capable generating series successively substituting inputs estimates 
useful model fewer parameters degrees freedom estimation elements time series 
number potentially infinite basically means function depend finite fixed number parameters see mean depend bounded number past sequence elements 
forecasting classification model description time series parameters viewed kind features series subsequent analysis subsequent classification time independent features 
compared process modeling aim compressing data vectors purely spatial domain realising auto associative mapping neural network 
modeling form mapping time series finding model parameters time series order reproduce series mapping time series different conceivable application type 
simple example forecasting value series price oil values interest rates 
complex applications involve separate modeling time series finding functional mapping 
simplest form mapping time series special case time series processing discussed complex case common application type discussed 
state space models discussed viewed context 
follows mainly discuss forecasting problems keeping mind application types closely related type 
stochasticity time series considerations implicitly assume theoretically exact model time series minimizes error measure desired degree 
real world applications assumption realistic 
due measuring errors unknown uncontrollable influencing factors assume optimal model lead residual error ffl erased 
usually error assumed result noise process produced randomly unknown source 
equation extended gamma ffl noise ffl included model 
methods assume certain characteristic noise gaussian white noise main describing parameters mean standard deviation included modeling process 
doing forecasting instance give estimate forecast value estimate value disturbed noise 
focus called arch models 
preprocessing time series cases appropriate measured observables processing 
cases necessary pre analyze preprocess time series ensure optimal outcome processing 
hand method employed usually extract certain kinds usually expressed terms vector similarities 
hand necessary remove known hamper performance 
example clear linear non linear trends phenomenon average value sequence elements constantly rising time series showing close linear falling trend 
series consists tick tick currency exchange rates swiss franc 
source ftp ftp cs colorado edu pub time series santafe falling see taken 
replacing time series series consisting differences subsequent values gamma gamma linear trend removed see 
differencing process corresponds differentiation continuous functions 
similarly periodic patterns due periodic influencing factor day week product sales eliminated computing differences corresponding sequence elements gamma gamma time interval days corresponding days week show similar patterns 
identifying trends clearly visible property time series lead prior knowledge series 
statistical problem prior knowledge handled explicitely differencing summation processing obtain original values 
forecasting method mainly attempt model perspicuous characteristics leaving little room fine grained characteristics 
naive forecaster forecast today value plus constant increment probably fare equally 
non linear trends usually parametric model exponential curve assumed elimination done 
reason eliminating trends matter clearly visible known pattern methods time series previous differencing require stationarity time series 
neural nets time series processing authors overview different types neural networks time series processing 
instance distinguishes different neural networks type mechanism deal temporal information 
neural networks previously defined pattern recognition static patterns temporal dimension supplied appropriate way 
distinguishes mechanisms ffl layer delay feedback time windows ffl layer delay feedback ffl unit delay feedback ffl unit delay feedback self recurrent loops bases overview distinction concerning type memory delay akin time windows delays exponential akin recurrent connections gamma memory model continuous time domains :10.1.1.31.3701
give slightly different overview 
discussion time series processing neural networks field mainly seen context function approximation classification 
main neural network types introduced discussed traditional ways sequence processing 
introductions 
extensive treatments neural networks sequence processing book 
perceptrons radial basis function nets autoregressive models wide spread neural networks feedforward networks classification function approximation multilayer perceptrons mlp hidden units sigmoidal transfer functions radial basis function networks rbfn hidden units distance propagation rule gaussian transfer functions 
network types proven universal function approximators see mlp rbfn 
means approximate reasonable function arbitrarily closely mlp jl oe ij gamma gamma oe sigmoid function non linear non polynomial function number hidden units jl ij weights thresholds biases rbf jl gamma ij gamma gamma gamma gaussian function provided sufficiently large 
approximation non linearity done superposition instances basis function sigmoid gaussian 
fixed number hidden units case neural network applications method called semi parametric approximation functions specific assumptions shape function parametric method approximate arbitrarily complex function non parametric note assumed fixed number hidden units proofs require arbitrarily large number units fixed see instance 
observation mlps offer straight forward extension wide spread classical way modeling time series linear autoregressive models 
linear time series modeling see assumes function equation linear combination fixed number previous series vectors 
including noise term ffl ff gamma ffl simplification univariate series assumed replacing vectors scalars gamma gamma ffl previous sequence elements taken speaks ar model time series autoregressive model order 
finding appropriate ar model means choosing appropriate estimating coefficients ff squares optimization procedure see extensive treatment topic 
technique powerful naturally limited assumes linear relationship sequence elements 
importantly assumes stationarity time series meaning main moments mean standard deviation change time mean standard deviation part series independent series part extracted 
clear equations respectively mlp rbfn replace linear function equation arbitrary non linear function nn nn mlp rbf nn gamma ffl non linear function estimated samples series known learning optimization techniques networks backpropagation gradient 
making nn dependent previous sequence elements identical input units fed adjacent sequence elements see fig 

input usually refered time window see section provides limited view part series 
viewed simple way transforming temporal dimension spatial dimension 
non linear autoregressive models potentially powerful linear ones ffl model complex underlying characteristics series ffl theoretically assume stationarity static pattern recognition require care caution linear methods ffl require large numbers sample data due large number degrees freedom ffl run variety problems overfitting sub optimal minima result estimation learning severe linear case overfitting come high value parameter instance nn 
feedforward neural net time window non linear ar model ffl necessarily include linear case trivial way especially point important real world applications limited data available 
linear model preferable cases dependencies non linear 
second point concerns learning algorithm employed 
backpropagation appropriate choice obtain optimal models 
examples feedforward neural networks forecasting numerous papers 
time delay neural networks mechanism supply neural networks memory deal temporal dimension time delays connections 
words delays inputs arrive hidden units different points time stored long influence subsequent inputs 
approach called time delay neural network tdnn extensively employed speech recognition instance 
formally time delays identical time windows viewed autoregressive models 
interesting extension time delays connections hidden output units providing additional memory network 
jordan nets moving average models alternative approach modeling time series assume series generated linear combination noise signals see example proceedings series resulting annual conference neural networks capital markets providing excellent overview forecasting neural networks financial domain 
gamma fi ffl gamma ffl ffl gamma ffl gamma ffl refered moving average ma model order 
approach paradoxical non random time series modeled linear combination random signals 
viewing linear combination discrete filter noise signal ma model viewed noise process usually frequency spectrum containing large number frequencies white noise 
filter ma model cut desired frequency spectrum bounds linearity leading specific non random time series 
combination ar ma components called arma model ff gamma gamma fi ffl gamma ffl gamma gamma ffl gamma ffl gamma ffl ma arma models ar models limited linearity requirement stationarity 
extension non linear case neural networks appropriate 
introduce possibility 
important question answer values ffl taken 
common approach ma modeling difference actual estimated forecast value estimate noise term time justified observation 
assume model near optimal terms forecasting 
difference forecast actual value close residual error noise term equation 
difference estimate ffl noise term ffl equation 
ffl gamma depicts neural network realizing assumption univariate case 
output network identical estimate fed back additional input layer unit receives negative version corresponding actual value available subsequent time step form desired difference 
time window time delay input layer introduced network forms arbitrarily non linear arma model time series nn 
copy neural network output layer feedback realizing nonlinear arma model 
nn gamma gamma ffl gamma ffl gamma ffl observations respect non linear ar model section 
non linear model network potentially powerful traditional arma models 
considered care due large numbers degrees freedom potential limitations learning algorithms 
complication comes fact sequence estimates available 
possible way overcome problem start values update network sufficient estimates computed 
requires certain number cycles learning algorithm applied wasting number sequence elements training 
especially important wants randomize learning choosing arbitrary window time series stepping series sequentially 
network considered special case recurrent network type usually called jordan network 
consists multilayer perceptron hidden layer feedback loop output layer additional input context layer 
addition introduced self recurrent loops unit context layer unit context layer connected weight smaller 
self recurrent loops network forms non linear function past sequence elements past estimates nn gamma gamma gamma gamma non linear arma model discussed said implicitly contained network reformulating equation help equation gamma gamma gamma gamma copy jordan network 
jl oe ij gamma gamma gamma ffl gammai ij gamma gamma gamma nn gamma gamma ffl gamma ffl gamma provided similar derivation 
conventional learning algorithms mlps trivially differences input values terms relevant invariances 
explicit calculation differences viewed inclusion essential pre knowledge network suited implementation clean non linear arma model 
jordan network time series processing extending arma family models realizing functional dependency sequence elements estimates hand forecast value 
examples applications jordan networks 
self recurrent loops jordan network deviation standard arma type models 
help past estimates superimposed way gamma gamma activation function typically sigmoid 
means activations units context layer recursively computed past estimates words activation function past estimates contains information potentially unlimited previous history 
property rise argument recurrent networks exploit information limited time window past values 
practice really exploited 
close unit uses sigmoid transfer function quickly saturates maximum activation additional inputs little effect 

influence past estimates quickly goes applications equation 
fact context layers self recurrent loops limited representing past information 
addition flexibility including past information paid loss explicitness information past estimates accumulated activation value 
way employing self recurrent loops discussed 
elman networks state space models common method time series processing called linear state space models 
assumption time series described linear transformation time dependent state state vector ffl transformation matrix 
time dependent state vector usually linear model gamma matrices noise process just ffl 
model state change version basically arma process 
basic assumption underlying model called markov assumption meaning sequence element predicted state system producing time series matter state reached 
words history series necessary producing sequence element expressed state vector 
vector continuous valued possible state vectors form euclidean vector space model viewed time series modeled terms related mapping time series discussed section 
assume states dependent past sequence vector assumption common instance signal processing see neglect moving average term gamma gamma basically obtain equation describing recurrent neural network type known elman network depicted 
elman network mlp additional input layer called state layer receiving feedback copy activations hidden layer previous time step 
network type forecasting copy elman network instantiation state space model 
equate activation vector hidden layer difference equation fact mlp sigmoid activation function applied input hidden unit oe gamma gamma oe refers application sigmoid logistic function exp gammaa element words transformation linear application logistic regressor input vectors 
leads restriction state vectors vectors unit cube non linear distortions edges cube 
note restricted non linear transformation function represent general form non linear state space models see 
elman network trained learning algorithm mlps backpropagation gradient 
jordan network belongs class called simple recurrent networks srn 
contains feedback connections viewed dynamical system activations spread 
activations layer computed time step presentation sequence vector 
strong relationship classical time series processing exploited introduce new learning algorithms 
instance kalman filter algorithm developed original state space model applied general recurrent neural networks 
similar observations elman recurrent network respect jordan net number time steps needed starting activations suitable activations available state layer learning 
standard learning algorithms backpropagation easy apply cause problems lead non optimal solutions 
type recurrent net really deal arbitrarily long history similar reasons see instance cited :10.1.1.31.3701
examples applications elman networks 
copy mlp rbfn mlp rbfn extension elman network realization non linear state space model hinted general non linear version state space model conceivable 
replacing linear transformation equations arbitrary non linear function obtains ffl gamma previous sections non linear arma models non linear functions modeled mlp rbfn 
resulting network depicted 
example application network 
multi recurrent networks extensive overview additional types time windows time delays neural networks 
combining types feedback delay obtains general network depicted 
feedback hidden output layers permitted 
discussions sections clear viewed state space model state transition modeled kind arma process term equation 
view entirely correct 
estimates additional inputs implicitly introduces estimates noise process ffl equation equation 
secondly input layers actual input state context layer permitted extended time delays introduce time windows past instances corresponding vectors 
essentially means involved processes ar arma respectively larger 
output layer hidden layer input layer context layer output forward propagation layer copy multi recurrent network 
thirdly jordan network self recurrent loops state layer introduced 
weights loops weights feedback copies resulting recurrent connections chosen scale theoretically maximum input unit state layer give weight feedback connections self recurrent loops respectively 
instance total activation unit state layer comes hidden layer feedback comes self state vector tend change considerably time step 
hand come hidden layer feedback self recurrent loops state vector tend remain similar previous time step 
speaks flexible sluggish state spaces respectively 
introducing state layers different weighting schemes network exploit information time steps kind average past time steps longer averaged history 
clear full version contains large number degrees freedom weights requires care models discussed 
empirical studies shown real world applications versions significantly outperform simple forecasting methods 
actual choice feedback delays weightings depends largely empirical evaluations similar iterative estimation algorithms suggested obtaining appropriate parameter values arma models appear applicable 
advantage self recurrent loops evident applications patterns time series vary time scale 
phenomenon called time warping especially known speech recognition different speech patterns vary length relationships segments dependent speaking speed intonation 
autoregressive models fixed time windows distorted patterns lead vectors share sufficient similarities classified correctly 
called temporal invariance problem problem simple network time dependent weight matrix produced second neural net 
recognizing temporal patterns independent duration temporal distortion 
state space model recurrent network self recurrent loops invariances dealt especially sluggish state spaces employed 
states state space model forced similar subsequent time steps events treated equally similarly shifted temporal dimension 
property discussed extensively 
neural nets producing weight matrices time dependent state transitions original state space model approach equations left open possibility making matrices time dependent 
allows modeling non stationary time series series variance noise process changes time 
neural network terms mean time varying weight matrices introduced small neural network model viewed context time dependent transition matrices 
consist feedforward networks mapping input sequence output producing weight matrix network 
state space model ar model input sequence realizes mapping variable matrix weight matrix network 
network learn formal languages parity 
similar example 
context approaches inducing finite state automata neural networks mentioned 
finite state automaton classical model describe time series level level categories continuous valued input 
categorization process assumed model applied time series ones discussed 
automaton defined set states discrete finite concept state space time varying mean varying time scale series 
weight changes due learning considered 
finite state automaton modeling time series model arcs corresponding state transitions taken dependent input 
instance starting left state node encountered automaton jump state encountered remain state 
depending types arcs emanate state prediction respect input element follow provided input grammatical corresponds grammar automaton implements 
especially useful sequences speech language 
shown neural network set trained implement automaton 
concept directly applied real world sequences ones points useful application neural networks especially going finite discrete states continuous state spaces 
exactly elman network model realizes 
topics story 
important topics concerning time series processing neural networks field 
topics covered equal importance ones 
ffl time series applications tackled fully recurrent networks networks recurrent architectures different ones discussed 
special learning algorithms arbitrary recurrent networks devised backpropagation time real time recurrent learning rtrl 
ffl authors combination neural networks called hidden markov models hmm time series signal processing 
hmms related finite automata describe probabilities changing state 
see instance treatment 
ffl unsupervised neural network learning algorithms selforganizing feature map applied time series processing forecasting classification 
application constitutes instance called spatio temporal clustering unsupervised classification time series clusters case clustering sleep eeg sleep stages 
ffl number authors investigated properties neural networks viewed dynamical systems including chaotic attractor dynamics 
examples 
focus introduce widely architectures close relationships classical approaches time series processing 
approaches viewed starting points research potential neural networks especially respect dynamical systems far fully exploited 
mentioned initially overview neural networks time series processing scratch surface lively important field 
attempted introduce basics domain stress relationship neural networks traditional statistical methodologies 
underlined important contribution neural networks elegant ability approximate arbitrary non linear functions 
property high value time series processing promises powerful applications especially subfield forecasting near 
emphasized non linear models problems respect requirement large data bases careful evaluation respect limitations learning estimation algorithms 
relationship neural networks traditional statistics essential live promises visible today 
acknowledgments austrian research institute artificial intelligence supported austrian federal ministry science research arts 
particularly claudia adrian research group prof manfred fischer cooperation basis valuable comments manuscript 
baumann application kohonen network shortterm load forecasting proc 
avignon aug sep 
bengio simard frasconi learning long term dependencies gradient descent difficult ieee trans 
neural networks 
bengio neural networks speech sequence recognition thomson london 
higgins arch models properties estimation testing journal economic surveys 
bishop neural networks pattern recognition clarendon press oxford 
bourlard morgan merging multilayer perceptrons hidden markov models experiments continuous speech recognition int 
computer science institute icsi berkeley ca tr 
box jenkins time series analysis holden day san francisco 
broomhead lowe multivariable functional interpolation adaptive networks complex systems 
chakraborty mehrotra mohan ranka forecasting behavior multivariate time series neural networks neural networks 
grumbach time neural networks sigart bulletin acm press 
chatfield analysis time series chapman hall london th edition 
connor atlas martin recurrent networks modeling moody eds neural information processing systems morgan kaufmann san mateo ca pp 
cottrell munro principal components analysis images backpropagation proc soc photo optical instr 
eng 
cybenko approximation superpositions sigmoidal function math control signals syst 
debar application recurrent network intrusion detection system ijcnn international joint conference neural networks baltimore ieee pp 
comparison kalman filters recurrent neural networks ijcnn international joint conference neural networks baltimore ieee pp 
dorffner koller improving exercise ecg detecting heart disease recurrent feedforward neural nets eds neural networks signal processing iv ieee new york pp 

duda hart pattern classification scene analysis john wiley sons 
elman finding structure time cognitive science 
giles miller chen chen sun lee learning extracting finite state automata second order recurrent neural networks neural computation 
gordon steele predicting trajectories recurrent neural networks dagli eds intelligent engineering systems artificial neural networks asme press new york pp 
girosi poggio networks best approximation property biological cybernetics 
hertz palmer krogh theory neural computation addison wesley redwood city ca 
ho ho wall stochastic neural adaptive control state space innovations model international joint conference neural networks ieee pp 
hornik stinchcombe white multi layer feedforward networks universal approximators neural networks 
jordan serial order parallel distributed processing approach ics ucsd report 
stock price pattern recognition recurrent neural network approach eds neural networks finance investing chicago pp 
kolen exploring computational capabilities recurrent neural networks ohio state university 
kolen pollack observers paradox apparent computational complexity physical systems journal exp theoret 
artificial intelligence 
universal approximation feedforward neural networks gaussian bar units neumann ed proceedings tenth european conference artificial intelligence ecai wiley chichester uk pp 
lee park prediction monthly transition composition stock price index recurrent back propagation aleksander taylor eds artificial neural networks north holland amsterdam pp 
morgan neural networks speech processing kluwer academic publishers boston 
mozer neural net architectures temporal sequence processing predicting understanding past weigend gershenfeld eds time series prediction forecasting understanding past addison wesley publishing redwood city ca :10.1.1.31.3701
pollack induction dynamical recognizers machine learning 
port cummins mcauley naive time temporal patterns human audition port van gelder 
eds mind motion mit press cambridge ma 
chen currency exchange rate neural network design strategies neural computing applications 
ed neural networks capital markets proceedings international workshop neural networks capital markets london nov 
francis stock performance modeling neural networks comparative study regression models neural networks 
electricity demand prediction discrete time fully recurrent neural networks proceedings ca usa 
roberts tarassenko analysis sleep eeg multilayer neural network spatial organisation iee proceedings part 
rohwer time dimension neural network models sigart bulletin acm press 
rumelhart hinton williams learning internal representations error propagation rumelhart mcclelland parallel distributed processing explorations microstructure cognition vol foundations mit press cambridge ma 
schmidhuber local learning algorithm dynamic feedforward recurrent networks connection science 
eds neural networks finance investing chicago 
dorffner rodriguez martin mechanisms handling sequences neural networks dagli eds intelligent engineering systems artificial neural networks vol 
asme press new york 
multi recurrent networks traffic forecasting proceedings twelfth national conference artificial intelligence aaai press mit press cambridge ma pp 
state formation neural networks handling temporal information institut fuer med kybernetik ai univ vienna dissertation 
dorffner lee forecasting fetal heartbeats neural networks appear proceedings engineering applications neural networks eann conference 
weigend rumelhart huberman back propagation weight elimination time series prediction touretzky eds connectionist models morgan kaufmann san mateo ca pp 
weigend gershenfeld 
eds time series prediction forecasting understanding past reading ma addison wesley 
wilson eds neural networks signal processing iv ieee new york 
waibel consonant recognition modular construction large phonemic time delay neural networks touretzky ed advances neural information processing systems morgan kaufmann los altos ca pp 
white economic prediction neural networks case ibm daily stock returns eds neural networks finance investing chicago pp 
widrow stearns adaptive signal processing prentice hall englewood cliffs nj 
williams zipser experimental analysis real time recurrent learning algorithm connection science 
williams training recurrent networks extended kalman filter international joint conference neural networks baltimore ieee pp 
