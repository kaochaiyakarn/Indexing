issn de recherche institut national de recherche en informatique en automatique parameter estimation techniques tutorial application conic fitting zhengyou zhang programme parameter estimation techniques tutorial application conic fitting zhengyou zhang programme image vision projet rapport de recherche sigma pages problems computer vision related form problem estimating parameters noisy data 
tutorial probably commonly techniques parameter estimation 
include linear squares pseudo inverse eigen analysis orthogonal squares gradient weighted squares bias corrected renormalization kalman ltering robust techniques clustering regression diagnostics estimators median squares 
particular attention devoted discussions choice appropriate minimization criteria robustness dioeerent techniques 
application conic tting described 
key words parameter estimation squares bias correction kalman ltering robust regression sum unite de recherche inria sophia antipolis route des lucioles bp sophia antipolis cedex france telephone techniques estimation de param tres un tutorial avec application de sum tous les probl mes en vision par ordinateur sont reli une mani re ou au probl de estimation de param tres partir de donn es es 
dans ce tutorial nous pr des techniques qui sont les plus utilis es pour estimation de param tres 
ce sont la technique des carr lin aires pseudo inverse analyse par la la re normalisation avec correction de le de kalman des techniques clustering par diagnostics la diane des carr 
un est aux discussions sur le un crit re de minimisation appropri sur la des techniques 
le probl de de est utilis pour expos 
mots cl estimation de param tres carr correction de de kalman parameter estimation techniques tutorial contents glance parameter estimation general conic fitting problem squares fitting algebraic distances normalization 
normalization 
normalization 
squares fitting euclidean distances algebraic distances usually satisfactory 
orthogonal distance tting 
gradient weighted squares fitting bias corrected renormalization fitting kalman filtering technique standard kalman filter 
extended kalman filter 
discussion 
iterated extended kalman filter 
application conic fitting 
robust estimation 
clustering hough transform 
regression diagnostics 
estimators 
median squares 
rr sigma zhengyou zhang problems computer vision related form problem estimating parameters noisy data 
examples line tting camera calibration image matching surface reconstruction pose determination motion analysis 
parameter estimation problem usually formulated optimization 
dioeerent optimization criteria possible parameterizations problem solved ways 
purpose show importance choosing appropriate criterion 
inaeuence accuracy estimated parameters eoeciency computation robustness predictable unpredictable errors 
conic tting illustrate aspects ffl simplest problems computer vision hand ffl hand relatively dioecult problem nonlinear nature 
needless say importance conics daily life industry 
glance parameter estimation general parameter estimation discipline provides tools eoecient data aiding mathematically modelling phenomena estimation constants appearing models 
visualized study inverse problems 
parameter estimation related optimization problems ffl criterion choice best function optimize minimize maximize ffl estimation optimization chosen function ffl design optimal design obtain best parameter estimates ffl modeling determination mathematical model best describes system data measured 
mainly concerned rst problems assume model known conic examples 
state parameter vector containing parameters estimated 
dimension say number parameters estimated 
inria parameter estimation techniques tutorial measurement vector output system modeled 
system described vector function relates practice observed measurements available system output corrupted noise ffl ffl usually number measurements system say fy want estimate fy data noisy function valid anymore 
case write function optimized loss generality minimize function 
function usually called cost function objective function 
constraints function rst second partial derivatives necessary conditions minimum mean theta matrix positive 
conic fitting problem problem conic section set points fx 
conic described equation ax cy dx ey simultaneously zero 
practice encounter ellipses impose constraint gamma ac 
constraint usually ignored tting rr sigma zhengyou zhang ffl constraint usually satised data situated section 
ffl computation expensive constraint considered 
data noisy nd set parameters trivial solution 
try estimate minimizing objective function 
squares fitting algebraic distances said noisy data system equation case conic tting hardly hold true 
common practice directly minimize algebraic distance minimize function clearly exists trivial solution 
order avoid normalize 
dioeerent normalizations proposed literature 
describe 
normalization trace zero ellipse arbitrary scale factor conic equation naturally removed normalization 
normalization researchers 
ellipse described vector system equation gamma gamma gammay inria parameter estimation techniques tutorial points vector equation ap function minimize ap gamma ap gamma obtaining partial derivative respect setting zero yield ap gamma solution readily gamma method known pseudo inverse technique 
normalization kpk sum squared zero conic set kpk remove arbitrary scale factor conic equation 
system equation kpk points vector equation ap kpk function minimize ap ap bp subject kpk symmetric matrix 
solution eigenvector corresponding smallest eigenvalue see 
rr sigma zhengyou zhang theta symmetric matrix case decomposed diag em th eigenvalue corresponding eigenvector 
loss generality assume delta delta delta original problem restated find am bp minimized delta delta delta subject delta delta delta 
simple algebra bp delta delta delta problem minimize unconstrained function delta delta delta delta delta delta gamma lagrange multiplier 
setting derivatives respect am yields delta delta delta delta delta delta am am delta delta delta gamma exist solutions 
th solution gammav value corresponding th solution delta delta delta rst solution need squares solution solution original problem eigenvector corresponding smallest eigenvalue 
inria parameter estimation techniques tutorial normalization commonly normalization set 
notations subsection problem minimize function ap ap bp subject sixth element vector seek squares solution ap constraint 
equation rewritten gammaa matrix formed rst gamma columns column vector problem solved technique described sect 

technique solving kind problems ap subject pm eigen analysis consider general formulation theta matrix vector pm element vector function minimize ap ap bp subject pm subsection symmetric matrix decomposed normalize eigenvalue eigenvector element eigenvector im im im element eigenvector element new eigenvector equal 
diag 
original problem rr sigma zhengyou zhang find am bp minimized delta delta delta ame subject delta delta delta am 
simple algebra bp delta delta delta problem minimize unconstrained function delta delta delta delta delta delta am gamma lagrange multiplier 
setting derivatives respect am yields delta delta delta delta delta delta am delta delta delta am gamma unique solution equations ffi solution problem ffi note normalization singularities conics going origin 
method conics require set 
suggest normalizations superior normalization respect singularities 
shown singularity problem overcome shifting data centered origin better results setting obtained setting 
inria parameter estimation techniques tutorial squares fitting euclidean distances section described general techniques solving linear squares problems unconstrained constrained algebraic distances 
section describe techniques usually provide satisfactory results propose conics directly euclidean distances 
algebraic distances usually satisfactory big advantage algebraic distances gain computational eoeciency closed form solutions usually obtained 
general results satisfactory 
major reasons 
ffl function minimize usually invariant euclidean transformations 
example function normalization invariant respect translations 
feature dislike usually know practice best coordinate system represent data 
ffl point may contribute parameter estimation depending position conic 
priori points corrupted amount noise desirable contribute way 
problem data points corrupted dioeerent noise addressed section 
understand second point consider conic normalized system see fig 
ax cy algebraic distance point conic cite bookstein ax cy gammaf gamma distance point center conic distance conic center ray center point 
clear point high curvature sections contributes conic tting point having amount noise low curvature sections 
point high curvature sections large jq small point low curvature sections small rr sigma zhengyou zhang fig 
normalized conic jq higher respect amount noise data points 
concretely methods algebraic distances tend better conic points low curvature sections high curvature sections 
problem usually termed high curvature bias 
orthogonal distance tting overcome problems algebraic distances natural replace orthogonal distances invariant transformations euclidean space exhibit high curvature bias 
orthogonal distance point conic euclidean distance point ti ti conic tangent orthogonal line joining see fig 

points orthogonal distance tting estimate conic minimizing function expression complicated see iterative optimization procedure carried 
techniques readily available including gauss newton algorithm steepest gradient descent levenberg marquardt inria parameter estimation techniques tutorial ti ti fig 
orthogonal distance point conic procedure simplex method 
software written fortran weighted orthogonal distance regression domain public available netlib netlib ornl gov 
initial guess conic parameters supplied obtained techniques described section 
proceed compute orthogonal distance subscript omitted clarity 
refer fig 

conic assumed described gamma gamma gamma gamma gamma point satisfy equations gamma gamma gamma gamma gamma gamma gamma equation merely says point conic says tangent orthogonal vector gamma delta gamma delta gamma delta gammab delta rr sigma zhengyou zhang delta gamma delta gamma gamma ac delta 
delta delta gamma gamma delta delta delta gamma gamma delta substituting value delta equation leads equation delta delta delta gamma ac ce gamma bc gamma gamma gamma squaring equation delta delta delta delta rearranging terms obtain equation degree delta delta delta delta delta gamma gamma gamma gamma gamma gamma real roots closed form 
solution delta obtain delta delta delta inria parameter estimation techniques tutorial delta computed 
eventually comes orthogonal distance gamma gamma delta gamma gamma delta note possibly solutions 
gives smallest distance seeking 
gradient weighted squares fitting squares method described sections usually called ordinary squares estimator ols 
formally equations gamma additive error th equation mean zero variance oe writing matrix form yields ap gamma 


ols estimator tries estimate minimizing sum squared errors ap gamma ap gamma gives seen solution gamma shown see ols estimator produces optimal estimate terms minimum covariance errors uncorrelated oe ffi ij variances constant oe 
examine assumptions valid conic tting 
data points provided signal processing algorithm edge rr sigma zhengyou zhang detection 
reasonable assume errors independent point detecting point usually information points 
reasonable assume errors constant points algorithm edge detection 
note talking errors points equations 
error point gaussian mean zero covariance oe theta identity matrix 
error distribution assumed isotropic directions oe oe oe oe xy 
sect 
consider case point may dioeerent noise distribution 
refer eq 

compute variance function point uncertainty 
true position point certainly gamma expand taylor series gamma gamma gamma gamma delta gamma gamma delta ignoring high order terms compute variance gamma gamma gamma oe rf oe rf just gradient respect ax gamma ay bx clear variance equation ols estimator yield optimal solution 
order obtain constant variance function suoecient original function gradient rf constant variance oe try nd parameters minimizing function rf ap gamma gamma ap gamma inria parameter estimation techniques tutorial diag rf rf rf 
method called gradient weighted squares solution easily obtained setting ap gamma gamma yields gamma gamma gamma note gradient weighted ls general nonlinear minimization problem closed form solution exist 
gave closed form solution ignored dependence computing reality depend seen eq 

solution approximation 
practice run iterative procedure step 
compute ols 
eq 
step compute weight matrix step compute gradient weighted ls 
eq 
step close gamma go step 
superscript denotes iteration number 
bias corrected renormalization fitting consider representation ellipse ax cy dx ey noisy points want estimate ellipse due homogeneity set kpk 
point scalar equation estimated minimizing objective function weighted squares optimization sigma rr sigma zhengyou zhang positive weights 
assume point error distribution mean zero covariance oe oe covariance ax bx cy oe ad bd ce weights chosen inverse proportion variances 
multiplication constant result estimation set oe ad bd ce objective function rewritten gamma sigma delta quadratic form unit vector sigma solution eigenvector associated smallest eigenvalue 
point perturbed noise deltax deltax deltay deltax deltax deltax deltax oe oe matrix perturbed accordingly deltan unperturbed matrix 
deltan estimate statistically unbiased statistically biased perturbation theorem bias deltap deltan 
inria parameter estimation techniques tutorial sigma carry taylor development ignore quantities order higher shown expectation deltan deltan oe cb clear dene sigma gamma deltan sigma gamma cb unbiased unit eigenvector associated smallest eigenvalue unbiased estimate exact solution ideally constant chosen impossible image noise characteristics known 
hand np takes minimum exact solution absence noise 
suggests require np iteration 
current np min update deltac sigma gamma cw gamma sigma deltac min sigma summarize renormalization procedure described rr sigma zhengyou zhang 

compute unit eigenvector sigma gamma cb associated smallest eigenvalue denoted min 
update min sigma recompute new 
return update converged go back step 
implementation dioeerent described kanatani 
implementation uses vectors represent points 
derivation bias assumes perturbation vector deltam ff notations magnitude deltam ff 
unrealistic assumption 
fact rst order deltam ff ff ff deltax ff deltay ff deltam ff deltax ff deltay ff ff ff deltam ff oe ff ff assume perturbation image plane point mean zero standard deviation oe 
method optimal sense 
criterion optimality minimum variance estimation addressed method 
kalman filtering technique nota rst subsections extracted book dynamic scene analysis stereo approach zhang faugeras springer berlin 
kalman ltering pointed lowe applications computer vision general method integrating noisy measurements 
behavior dynamic system described evolution set variables called state variables 
practice individual state variables inria parameter estimation techniques tutorial dynamic system determined exactly direct measurements usually nd measurements functions state variables measurements corrupted random noise 
system may subjected random disturbances 
required estimate state variables noisy observations 
denote state vector denote measurement vector dynamic system discrete time form described delta delta delta delta delta delta vector random disturbance dynamic system usually modeled white noise practice system noise covariance usually determined basis experience intuition guessed 
vector called measurement vector 
practice measurements contain random errors 
assume measurement system disturbed additive white noise real observed measurement expressed ae measurement noise covariance provided signal processing algorithm guessed manner system noise 
general noise levels determined independently 
assume correlation noise process system observation rr sigma zhengyou zhang standard kalman filter linear function able write explicitly linear relationship standard kalman lter directly applicable 
kalman filter ffl prediction states iji gamma gamma gamma ffl prediction covariance matrix states iji gamma gamma gamma gamma gamma ffl kalman gain matrix iji gamma iji gamma gamma ffl update state estimation iji gamma gamma iji gamma ffl update covariance matrix states gamma iji gamma ffl initialization gamma delay oe oe oe iji gamma gamma fig 
kalman lter block diagram inria parameter estimation techniques tutorial block diagram kalman lter 
time system model inherently lter structure generates iji gamma best prediction state previous state estimate gamma previous state covariance matrix gamma extrapolated predicted state covariance matrix iji gamma iji gamma compute kalman gain matrix update covariance matrix system model generates iji gamma best prediction measurement time 
real measurement read measurement residual called innovation gamma iji gamma computed 
residual weighted kalman gain matrix generate correction term added iji gamma obtain updated state kalman lter gives linear unbiased minimum error variance recursive algorithm optimally estimate unknown state linear dynamic system noisy data taken discrete real time intervals 
entering theoretical kalman lter reader referred existing books insist point kalman lter yields optimal estimate optimal sense spread probability density minimized 
words estimate kalman lter minimizes cost function gamma gamma arbitrary positive matrix 
optimal estimate state vector easily understood squares estimate properties 
transformation yields delta delta delta linear 
unbiased sense 
yields minimum variance estimate inverse covariance matrix measurement optimal weight 
inspecting kalman lter equations behavior lter agrees intuition 
look kalman gain matrix manipulation express gain matrix form gamma rr sigma zhengyou zhang gain matrix uncertainty estimate measurement 
measurement uncertain state estimate relatively precise residual resulted mainly noise little change state estimate 
hand uncertainty measurement small state estimate big residual contains considerable information errors state estimate strong correction state estimate 
exactly 
examine covariance matrix state estimate 
inverting replacing explicit form obtain gamma gamma iji gamma gamma equation observe measurement uncertain big covariance matrix decrease little measurement 
measurement contributes little reducing estimation error 
hand measurement precise small covariance decrease considerably 
logic 
described previous paragraph measurement contributes considerably reducing estimation error 
note equation measurements noise free gamma dened 
extended kalman filter linear linear relationship written called extended kalman filter ekf abbreviation applied ekf approach apply standard kalman lter linear systems nonlinear systems additive white noise continually updating linearization previous state estimate starting initial guess 
words consider linear taylor approximation system function previous state estimate observation function corresponding predicted position 
approach gives simple eoecient algorithm handle nonlinear model 
convergence reasonable estimate may obtained initial guess poor disturbances large linearization inadequate describe system 
note usual formulation ekf measurement observation function form gamma 
unfortunately formulation general deal problems addressed monograph 
inria parameter estimation techniques tutorial expand taylor series iji gamma iji gamma iji gamma gamma iji gamma gamma iji gamma gamma gamma iji gamma ignoring second order terms get linearized measurement equation new measurement vector noise vector new measurement linearized transformation matrix 
iji gamma gammaf iji gamma iji gamma iji gamma iji gamma gamma gamma iji gamma clearly iji gamma iji gamma extended kalman lter equations algorithm derivative computed gamma algorithm extended kalman filter ffl prediction states iji gamma gamma ffl prediction covariance matrix states iji gamma gamma gamma ffl kalman gain matrix iji gamma iji gamma gamma ffl update state estimation iji gamma gamma iji gamma iji gamma gamma iji gamma ffl update covariance matrix states gamma iji gamma ffl initialization rr sigma zhengyou zhang discussion kalman lter formalism assumptions system noise process measurement noise process uncorrelated gaussian white noise sequences 
assumptions adequate solving problems addressed monograph 
case noise processes correlated white colored reader referred derivation kalman lter equations 
numerical unstability kalman lter implementation known 
techniques developed overcome problems square root ltering factorization 
see thorough discussion 
exist methods solve parameter estimation problem general minimization procedures weighted squares method bayesian decision theoretic approach 
appendix chapter review squares techniques 
choose kalman lter approach main tool solve parameter estimation problem 
reasons ffl kalman lter takes explicitly account measurement uncertainties ffl kalman lter takes measurements account incrementally ffl kalman lter simple eoecient procedure solve problem computational tractability ffl kalman lter take account priori information 
linearization nonlinear model leads small errors estimates general neglected especially relative accuracy better 
pointed maybank extended kalman lter seriously underestimates covariance 
furthermore current estimate iji gamma dioeerent true rst order approximation anymore nal estimate lter may dioeerent true 
approach reduce eoeect nonlinearities apply iteratively kalman lter called iterated extended kalman lter 
iterated extended kalman filter iterated extended kalman filter iekf applied globally locally 
global iekf applied observed data 
set observations fx delta delta ng 
initial state estimate covariance matrix inria parameter estimation techniques tutorial applying ekf set fx get estimate covariance matrix superscript denotes number iteration 
performing iteration back propagate time denoted iteration initial state estimate original initial covariance matrix initial covariance matrix iteration 
new covariance matrix mean identical sets measurements 
due requirement back propagation state estimate application global iekf limited 
interesting state evolve time 
case back propagation required 
problem estimating motion frames ekf applied spatially applied number matches 
motion state change match global iekf applied 
local iekf applied single sample data nominal trajectory measurement equation 
capable providing better performance basic ekf especially case signicant nonlinearity measurement function 
generated measurement incorporation value serve better state estimate iji gamma evaluating measurement update relations 
state estimate measurement incorporation recomputed iteratively desired 
iekf measurement update relations replaced setting iji gamma superscript denotes number iteration doing iteration iji gamma iji gamma gamma gamma iteration number delta delta delta gamma setting iteration stopped consecutive values preselected threshold 
covariance matrix updated application conic fitting choose normalization see sect 
state vector dened rr sigma zhengyou zhang measurement vector conic parameters points simple system equation noise term zero 
observation function gamma order apply extended kalman lter need compute derivative respect respect gamma gammay robust estimation stated squares estimators assume noise corrupting data zero mean yields unbiased parameter estimate 
noise variance known minimum variance parameter estimate obtained choosing appropriate weights data 
furthermore squares estimators implicitly assume entire set data interpreted parameter vector model 
numerous studies conducted clearly show squares estimators vulnerable violation assumptions 
data contains bad datum squares estimates may completely perturbed 
decades robust techniques proposed sensitive departure assumptions depend 
hampel gives robustness quoted reasons robust procedures 
mainly observations combined give answer 
statistics parametric model implying limited set probability distributions possible common model normally distributed errors exponentially distributed observations 
classical parametric statistics derives results assumption inria parameter estimation techniques tutorial models strictly true 
apart simple discrete models models exactly true 
may try distinguish main reasons derivations rounding grouping ii occurrence measuring wrong decimal points errors copying inadvertent measurement member dioeerent population just went iii model may conceived approximation anyway virtue central limit theorem 
priori knowledge parameters estimated techniques kalman ltering technique test mahalanobis distance yield robust estimate 
describe major approaches robust estimation 
clustering hough transform oldest robust methods image analysis computer vision hough transform 
idea map data parameter space appropriately quantized seek parameter values interpret data clustering 
classical example detection straight lines set edge points 
take example estimating plane rigid motion sets points 
points rst set noted fm points second set noted fm nd rigid transformation sets 
pairing fm fm assumed known 
rigid transformation uniquely decomposed rotation origin translation order 
corresponding parameter space dimensional parameter rotation angle translation vector precisely paired rm cos gamma sin sin cos clear pairings necessary unique estimate rigid transformation 
dimensional parameter space quantized levels necessary required precision 
rotation angle ranges rr sigma zhengyou zhang 
quantization interval rotation angle say units 
translation bounded practice 
assume example translation images exceed pixels direction gamma 
choose interval pixels units direction 
quantized space considered dimensional accumulator cells initialized zero 
pairing points entirely constrain motion dioecult update accumulator constraint motion simple 
match tuples points rst set second set smallest value matching points rst set points second set completely determines motion case 
matches vectors dimension composed points rst second set respectively 
number matches considered order course need consider matches particular problem distance invariance pair points rigid transformation discard infeasible matches 
match compute motion parameters corresponding accumulator cell increased 
matches considered peaks accumulator indicate best candidates motion parameters 
general number data larger number unknowns maximum peak higher peaks may correct data noise parameter space quantization 
hough transform highly suitable problems having data support expected solution 
noise measurements right peak accumulator may blurred easily detected 
accuracy localization simple implementation may poor 
ways improve 
ffl select just maximum peak quadratic hyper surface 
position maximum gives better localization parameter space curvature indication uncertainty estimation 
ffl statistical clustering techniques discriminate dioeerent candidates solution 
inria parameter estimation techniques tutorial ffl integer accumulator uncertainty data taken account propagated parameter estimation considerably increase performance 
hough transform technique follows principle maximum likelihood estimation 
parameter vector example 
xm datum xm example 
assumption data represent complete sample probability density function pr xm law total probability 
maximum considered estimation hough transform described considered approximation 
nature global search hough transform technique robust high percentage gross errors data 
better accuracy localization solution increase number samples dimension quantized parameter space 
size accumulator increases rapidly required accuracy number unknowns 
technique rarely applied solve problems having unknowns suitable conic tting 
regression diagnostics old robust method called regression diagnostics 
tries iteratively detect possibly wrong data reject analysis globally tted model 
classical approach works follows 
determine initial set data squares 

compute residual datum 

reject data residuals exceed predetermined threshold data removed 

determine new remaining data goto step 
clearly success method depends tightly quality initial initial poor computed residuals rr sigma zhengyou zhang meaningless diagnostics outlier rejection 
pointed barnett lewis squares techniques outliers large set havoc 
technique guarantee correct solution 
experiences shown technique works problems moderate percentage outliers importantly outliers having gross errors size data 
threshold residuals chosen experiences example graphical methods plotting residuals dioeerent scales 
better priori statistical noise model data chosen level 
residual th data oe predicted variance th residual characteristics data nose standard test statistics oe 
acceptable corresponding datum rejected 
improvement technique uses inaeuence measures pinpoint potential outliers 
measures asses extent particular datum determining change solution datum omitted 
technique works follows 
determine initial set data squares 

conduct statistic test measure sum square residuals acceptable 

datum delete data set determine new giving measure denoted determine change measure deltaf gamma datum deleted 

delete datum deltaf largest goto step 
shown techniques agrees rst order approximation datum largest residual datum inducing maximum change measure rst order expansion 
dioeerence rst technique simply rejects datum deviates current second technique rejects point exclusion result best iteration 
words second technique looks ahead see improvements materialize 
remarked regression diagnostics approach depends heavily priori knowledge choosing thresholds outlier rejection 
inria parameter estimation techniques tutorial estimators popular robust technique called estimators 
residual th datum dioeerence th observation tted value 
standard squares method tries minimize unstable outliers data 
outlying data give eoeect strong minimization parameters estimated distorted 
estimators try reduce eoeect outliers replacing squared residuals function residuals yielding min ae ae symmetric positive function unique minimum zero chosen increasing square 
solving directly problem implement iterated reweighted squares 
see 
pm parameter vector estimated 
estimator function ae vector solution equations derivative dae dx called inaeuence function 
dene weight function equation 
exactly system equations obtain solve iterated reweighted squares problem min gamma rr sigma zhengyou zhang superscript indicates iteration number 
weight gamma recomputed iteration order iteration 
inaeuence function measures inaeuence datum value parameter estimate 
example squares ae inaeuence function inaeuence datum estimate increases linearly size error non squares estimate 
estimator robust may inferred inaeuence single observation datum yield signicant 
constraints robust estimator meet ffl rst course bounded inaeuence function 
ffl second naturally requirement robust estimator unique 
implies objective function parameter vector minimized unique minimum 
requires individual ae function convex variable necessary requiring ae function unique minimum suoecient 
case maxima considering mixture distribution sum unimodal probability distributions multimodal 
convexity constraint equivalent imposing ae non negative 
ffl third practical requirement 
ae singular objective gradient ae 
avoids having search complete parameter space 
table lists commonly inaeuence functions 
graphically fig 

note functions satisfy requirements 
give indications functions ffl estimators robust inaeuence function bounded 
ffl absolute value estimators stable ae function jxj convex second derivative unbounded solution may result 
ffl estimators reduce inaeuence large errors inaeuence inaeuence function cut ooe point 
inria parameter estimation techniques tutorial table commonly estimators type ae jxj sgn jxj gamma gamma jxj sgn jxj gamma jxj gamma jxj gamma log jxj jxj jxj huber jxj jxj jxj gamma sgn jxj cauchy log geman mcclure gamma exp gamma exp gamma exp gamma tukey jxj jxj gamma gamma gamma delta gamma gamma rr sigma zhengyou zhang squares ae function inaeuence function weight function absolute ae function inaeuence function weight function gamma ae function inaeuence function weight function power ae function inaeuence function weight function fair ae function inaeuence function weight function huber ae function inaeuence function weight function cauchy ae function inaeuence function weight function geman mcclure ae function inaeuence function weight function ae function inaeuence function weight function tukey ae function inaeuence function weight function fig 
graphic representations common estimators inria parameter estimation techniques tutorial ffl gamma estimators take advantage estimators reduce inaeuence large errors estimators convex 
ffl powers function represents family functions 

smaller smaller incidence large errors estimate appears fairly moderate provide relatively robust estimator words provide estimator scarcely perturbed outlying data 
selection optimal investigated estimate may expected 
encountered computation parameter range interest zero residuals troublesome 
ffl function possibilities package see 
dened continuous derivatives rst orders yields unique solution 
asymptotic eoeciency standard normal distribution obtained tuning 
ffl huber function parabola vicinity zero increases linearly level jxj asymptotic eoeciency standard normal distribution obtained tuning constant 
estimator satisfactory recommended situations rarely inferior ae function 
time time encountered may due lack stability gradient values ae function discontinuous second derivative ae dx jxj jxj proposed ae gamma cos jxj gamma jxj 
asymptotic eoeciency standard normal distribution obtained tuning 
ffl cauchy function known function guarantee unique solution 
descending rst derivative function tendency yield erroneous solutions way observed 
rr sigma zhengyou zhang asymptotic eoeciency standard normal distribution obtained tuning 
ffl remaining functions problem cauchy function 
seen inaeuence function inaeuence large errors decreases size 
geman mcclure welsh functions try reduce eoeect large errors tukey function suppress outliers 
asymptotic eoeciency standard normal distribution tukey function obtained tuning constant function 
exist ae functions andrew cosine wave function 
commonly function tri weight jr oe oe jr oe jr oe oe jr oe estimated standard deviation errors 
dioecult select ae function general arbitrary 
rey location regression problems best choice spite theoretical non robustness quasi robust 
computational 
second best function yield nicely converging computational procedures 
eventually comes huber function original modied form 
functions eliminate completely inaeuence large gross errors 
functions unicity reduce considerably eliminate completely inaeuence large gross errors 
proposed huber start iteration process convex ae function iterate convergence apply iterations non convex functions eliminate eoeect large errors 
median squares median squares lmeds method estimates parameters solving nonlinear minimization problem min med inria parameter estimation techniques tutorial estimator yield smallest value median squared residuals computed entire data set 
turns method robust false matches outliers due bad localization 
lmeds problem reduced weighted leastsquares problem 
probably impossible write straightforward formula lmeds estimator 
solved search space possible estimates generated data 
space large randomly chosen subset data analyzed 
algorithm describe robustly estimating conic follows structured chap 
outlined 
points fm 
monte carlo type technique draw random subsamples dioeerent points 
problem hand select points need points dene conic 

subsample indexed techniques described sect compute conic parameters 
technique important exact solution possible dioeerent points 

determine median squared residuals denoted respect set points med number choices residual th point respect conic depending demanding precision computation requirement algebraic distance euclidean distance gradient weighted distance 

retain estimate minimal 
question determine 
subsample consists data points 
assuming set points may contain fraction outliers probability subsamples gamma gamma gamma rr sigma zhengyou zhang requiring near determine values log gamma log gamma gamma implementation assume require 
note algorithm speeded considerably means parallel computing processing subsample done independently 
noted lmeds eoeciency poor presence gaussian noise 
eoeciency method dened ratio lowest achievable variance estimated parameters actual variance provided method 
compensate carry weighted leastsquares procedure 
robust standard deviation estimate oe gamma minimal median 
constant achieve eoeciency squares presence gaussian noise gamma compensate eoeect small set data 
reader referred page details magic numbers 
oe assign weight correspondence oe residual th point respect conic correspondences having outliers taken account 
conic nally estimated solving weighted squares problem min numerous techniques described 
robustly estimated conic outliers detected discarded lmeds method 
said previously computational eoeciency lmeds method achieved applying monte carlo type technique 
points subsample generated may close 
situation avoided estimation conic points highly instable inria parameter estimation techniques tutorial result useless 
waste time evaluate subsample 
order achieve higher stability eoeciency develop regularly random selection method bucketing techniques works follows 
rst calculate min max coordinates points rst image 
region evenly divided theta buckets see fig 
implementation 
bucket attached set points indirectly set matches fall 
buckets having matches attached excluded 
generate subsample points rst randomly select mutually dioeerent buckets randomly choose match selected bucket 
fig 
illustration bucketing technique question remains subsamples required 
assume bad points uniformly distributed space bucket number points random selection uniform formula holds 
number points bucket may quite dioeerent 
result point belonging bucket having fewer points higher probability selected 
preferred bucket having points higher probability selected bucket having points order point probability selected 
realized procedure 
total buckets divide intervals width th interval equal ffi number points attached th bucket see fig 

bucket selection procedure number produced uniform random generator falling th interval implies th bucket selected 
rr sigma zhengyou zhang gamma number matches bucket random variable fig 
interval bucket mapping applied technique matching uncalibrated images :10.1.1.146.990
uncalibrated images available geometric constraint epipolar constraint 
idea underlying approach classical techniques correlation relaxation methods particular implementation nd initial set matches median squares lmeds discard false matches set 
epipolar geometry accurately estimated meaningful image criterion 
matches eventually stereo matching recovered epipolar geometry 
tutorial probably commonly techniques parameter estimation computer vision 
particular attention devoted discussions choice appropriate minimization criteria robustness dioeerent techniques 
hopefully reader nd tutorial useful 
comments extremely welcome 
technique consider important popular computer vision minimum description length mdl principle 
applied solve problem am position 
reader referred 
inria parameter estimation techniques tutorial ayache 
articial vision mobile robots stereo vision multisensory perception 
mit press cambridge ma 
beck arnold 
parameter estimation engineering science 
wiley series probability mathematical statistics 
wiley new york 
chui chen 
kalman filtering real time applications 
springer ser 
info 
sci vol 

springer berlin heidelberg 

reliability analysis parameter estimation linear models application problems computer vision 
comput 
vision graphics image process 
hampel 
robust estimation condensed partial survey 
verw 
gebiete 
huber 
robust statistics 
john wiley sons new york 

stochastic processes filtering theory 
academic new york 
kanatani 
renormalization unbiased estimation 
proc 
fourth int conf 
comput 
vision pages berlin 
leclerc 
constructing simple stable description image partitioning 
international journal computer vision 
li 
minimum description length shape description 
proceedings th proc 
international conference computer vision pages berlin germany may 
ieee computer society press 
lowe 
review vision khatib craig lozano rez editors robotics review pages 
mit press cambridge ma 
maybank 
filter estimates depth 
proc 
british machine vision conf pages university oxford london uk september 
maybeck 
stochastic models estimation control volume 
academic new york 
rr sigma zhengyou zhang maybeck 
stochastic models estimation control volume 
academic new york 
papoulis 
probability random variables stochastic processes 
mcgrawhill new york 

fitting ellipses predicting envelopes bias corrected kalman lter 
image vision computing 
william rey 
robust quasi robust statistical methods 
springer berlin heidelberg 
rissanen 
minimum description length principle 
encyclopedia statistic sciences 
rosin 
note squares tting ellipses 
pattern recognition letters 
rosin west 
segmenting curves elliptic arcs straight lines 
proc 
third int conf 
comput 
vision pages osaka japan 
rousseeuw leroy 
robust regression outlier detection 
john wiley sons new york 
shapiro 
analysis image sequences 
phd thesis dept engineering science oxford university 
zhang deriche faugeras :10.1.1.146.990
luong 
robust technique matching uncalibrated images recovery unknown epipolar geometry 
articial intelligence journal 
appear 
inria research report may 
zhang faugeras 
dynamic scene analysis stereo approach 
springer berlin heidelberg 
inria unite de recherche inria lorraine de nancy campus scientifique rue du jardin bp le nancy unite de recherche inria rennes irisa campus universitaire de beaulieu rennes cedex unite de recherche inria alpes avenue felix grenoble cedex unite de recherche inria rocquencourt domaine de voluceau rocquencourt bp le chesnay cedex unite de recherche inria sophia antipolis route des lucioles bp sophia antipolis cedex inria domaine de voluceau rocquencourt bp le chesnay cedex france issn 
