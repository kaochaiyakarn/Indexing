clustering sequences hidden markov models padhraic smyth information computer science university california irvine ca smyth ics uci edu discusses probabilistic model approach clustering sequences hidden markov models hmms 
problem framed generalization standard mixture model approach clustering feature space 
primary issues addressed 
novel parameter initialization procedure proposed second difficult problem determining number clusters data investigated 
experimental results indicate proposed techniques useful revealing hidden cluster structure data sets sequences 
consider data set consisting sequences fs 
sn 
sequence length composed potentially multivariate feature vectors problem addressed discovery data natural grouping sequences clusters 
analagous clustering multivariate feature space normally handled methods means gaussian mixtures 
trying cluster sequences feature vectors example shows sequences generated different models hidden markov models case 
third came model slower dynamics second fourth details provided 
sequence clustering problem consists sample sequences inferring data underlying clusters 
non trivial sequences different lengths clear meaningful distance metric sequence comparison 
hidden markov models clustering sequences appears position sequence sequences came hidden markov model mentioned juang rabiner subsequently context discovering subfamilies protein sequences krogh 

contains new contributions context cluster method initializing model parameters novel method cross validated likelihood determining automatically clusters fit data 
natural probabilistic model problem finite mixture model sj denotes sequence weight jth model sj density function sequence data component model parameters assume hmms transition matrices observation density parameters initial state probabilities jth component 
sj computed forward part forward backward procedure 
generally component models probabilistic model linear autoregressive models graphical models non linear networks probabilistic semantics forth 
important note motivation problem comes goal building descriptive model data prediction se 
prediction problem clearly defined metric performance average prediction error sample data cf 
rabiner 
speech context clusters hmms meir adler general time series context 
contrast descriptive modeling clear appropriate metric evaluation particularly number clusters unknown 
density estimation viewpoint taken likelihood sample data measure quality particular model 
algorithm clustering sequences clusters assume number clusters known 
model mixture hmms equation 
immediately observe mixture viewed single composite hmm transition matrix model block diagonal mixture model consists components transition matrices represent mixture model single hmm effect hierarchical mixture transition matrix initial state probabilities chosen appropriately reflect relative weights mixture components equation 
intuitively sequence generated model initially randomly choosing upper matrix probability lower matrix probability probability gamma generating data appropriate crossover mixture model data assumed come component 
composite hmm natural approach try learn parameters model standard hmm estimation techniques form initialization followed baum welch maximize likelihood 
note predictive modelling likelihood necessarily appropriate metric evaluate model quality likelihood maximization exactly want seek generative descriptive model data 
assume number states component known priori looking hmm components states known 
obvious extension address problem learning simultaneously dealt 
initialization clustering log likelihood space em algorithm effectively hill climbing likelihood surface quality final solution depend critically initial conditions 
prior information possible problem seed initialization potentially worthwhile 
motivates scheme initializing matrix composite hmm 
fit state hmms individual sequence hmms initialized default manner set transition matrices uniformly set means covariances means algorithm confused number hmm components 
discrete observation alphabets modify accordingly 

fitted model evaluate log likelihood sequences model calculate ij log jm 
log likelihood distance matrix cluster sequences groups details clustering discussed 

having pooled sequences groups fit hmms group default initialization described 
hmms get sets parameters initialize composite hmm obvious way theta block diagonal component mk theta mk set estimated transition matrix jth group means covariances jth set states set accordingly 
initialize equation number sequences belong cluster initialization step complete learning proceeds directly composite hmm matrix usual baum welch fashion sequences 
intuition initialization procedure follows 
hypothesis data generated models 
fit models individual sequence get noisier estimates model parameters sequences cluster parameters clustered manner groups true values assuming model correct 
clustering directly parameter space inappropriate define distance log likelihoods natural way define pairwise distances 
note step requires training sequences individually step requires evaluation distances 
large may impractical 
suitable modifications train small random sample sequences randomly sample distance matrix help reduce computational burden pursued 
variety possible clustering methods step 
symmetrized distance ij jm jm shown appropriate measure dissimilarity models juang rabiner 
results described hierarchical clustering generate clusters symmetrized distance matrix 
furthest neighbor merging heuristic encourage compact clusters worked empirically particular reason method 
refer clustering initialization followed baum welch training composite model hmm clustering algorithm rest 
experimental results consider deceptively simple toy problem 
dimensional feature data generated component hmm mixture states 
observable feature data obey gaussian density state oe oe state component respective mean state component 
sample sequences shown 
top third top sequences slower component stay state switch 
total training data contain sample sequences component length 
problem non trivial data exactly marginal statistics temporal sequence information removed markov dynamics governed relatively similar component making identification somewhat difficult 
hmm clustering algorithm applied sequences 
symmetrized likelihood distance matrix shown grey scale image 
axes ordered sequences clusters adjacent 
difference distances clusters apparent hierarchical clustering algorithm easily separates groups 
initial clustering followed training separately clusters set sequences assigned cluster yielded oe oe subsequent training composite model sequences produced refined parameter estimates basic cluster structure model remained initial clustering robust 
symmetric log likelihood distance matrix model model model model symmetrized log likelihood distance matrix 
comparative purposes alternative initialization procedures initialize training composite hmm 
unstructured method uniformly initializes matrix knowledge fact block diagonal terms zero standard way fitting hmm 
block uniform method uniformly initializes block diagonal matrices sets block diagonal terms zero 
random initialization gave poorer results compared uniform 
table differences log likelihood different initialization methods 
initialization maximum mean standard method log likelihood log likelihood deviation value value log likelihoods unstructured block uniform hmm clustering alternatives run times data run seeds means component initialization changed 
maximum mean standard deviations log likelihoods test data reported table log likelihoods offset mean unstructured log likelihood zero 
unstructured approach substantially inferior problem surprising block diagonal structure true model 
block uniform method closer performance inferior 
particular log likelihood consistently lower hmm clustering solution greater variability different initial seeds 
qualitative behavior observed variety simulated data sets results due lack space 
learning number sequence components background assumed number clusters known 
problem learning best value mixture model difficult practice simpler non dynamic case gaussian mixtures 
considerable prior problem 
penalized likelihood approaches popular log likelihood training data penalized subtraction complexity term 
general approach full bayesian solution posterior probability value calculated data priors mixture parameters priors 
potential difficulty computational complexity integrating parameter space get posterior probabilities various analytic sampling approximations practice 
theory full bayesian approach fully optimal probably useful 
practice ideal bayesian solution approximated obvious approximation affects quality final answer 
room explore alternative methods determining monte carlo cross validation approach imagine large test data set test fitting models 
lk test log likelihood model components fit training data likelihood evaluated test view likelihood function parameter keeping parameters fixed 
intuitively test likelihood useful estimator training data likelihood comparing mixture models different numbers components 
fact test likelihood shown unbiased estimator kullback leibler distance true unknown density model 
maximizing sample likelihood reasonable model selection strategy 
practice usually want reserve large fraction data test purposes cross validated estimate log likelihood 
smyth standard multivariate gaussian mixture modeling standard fold cross validation techniques say performed poorly terms selecting correct model simulated data 
cross validation shao stable data partitioned fraction fi testing gamma fi training procedure repeated times partitions randomly chosen run need disjoint 
choosing fi tradeoff variability performance estimate test set variability model fitting training set 
general total amount data increases relative model complexity optimal fi larger 
mixture clustering problem fi empirically smyth results reported 
experimental results data set described earlier known priori 
sequences randomly partitioned times training test crossvalidation sets 
train test partition value varied value hmm clustering algorithm fit training data likelihood evaluated test data 
mean cross validated likelihood evaluated average runs 
assuming models equally priori generate approximate posterior distribution bayes rule posterior probabilities shown 
cross validation procedure produces clear peak true model size 
general cross validation method tested variety simulated sequence clustering data sets typically converges function number training samples true value 
number estimated posterior probabilities cluster data posterior probability distribution estimated cross validation 
data points grow posterior distribution narrows true value data generated assumed form model posterior distribution tend peaked model size closest distance true model 
results context gaussian mixture clustering smyth shown monte carlo cross validation technique performs better bayesian approximation methods robust penalized likelihood methods bic 
shown model probabilistic clustering generalized feature space clustering sequence clustering 
log likelihood sequence models sequences useful detecting cluster structure cross validated likelihood shown able detect true number clusters 
baldi chauvin hierarchical hybrid modeling hmm nn architectures protein applications neural computation 
krogh hidden markov models computational biology applications protein modeling mol 
bio 
juang rabiner probabilistic distance measure hidden markov models technical journal vol february 
rabiner lee juang hmm clustering connected word recognition proc 
int 
conf 
ac 
speech 
sig 
proc ieee press 
shao linear model selection cross validation am 
stat 
assoc 
smyth clustering monte carlo cross validation proceedings second international conference knowledge discovery data mining menlo park ca aaai press pp 
meir adler time series prediction mixtures experts volume 
