mpi java mpi contrasts comparisons low level communication performance vladimir paul gray sunderam school computer science university ha tp uk dept mathematics computer science emory university atlanta ga usa java receiving increasing attention popular platform distributed collaborative computing 
subject significant performance drawbacks comparison programming languages fortran 
represents current status ongoing project intends conduct detailed experimental evaluation suitability java environments particular focus messagepassing performance data exchange patterns 
emphasize methodology evaluation guidelines order ensure reproducibility sound interpretation comparative analysis performance results 
important parameters characterize communication performance mpi java mpi latency asymptotic bandwidth half investigated 
addition introduce different types pipeline effects intra message inter message significant influence message passing performance 
purpose developed low level message passing benchmark suite evaluate compare different message passing environments ibm sp 
issues facing java programming language acceptance scientific community performance 
active projects underway aim analyze feasibility java computational tasks associated high performance computing 
focus research supported army research office daah department energy 
de fg er national science foundation award nos 
ccr asc 
arisen pursuits concentrated effort bringing java widely accepted parallel programming paradigm mpi 
endeavor development mpi binding java organized collaborations java grande forum develop single mpi api specification java programming language 
purpose development java mpi binding provide java programmers traditional functionality mpi java interface legacy mpi libraries 
mechanism support interface java native interface jni specifies transition java code running java virtual machine jvm native system dependent fortran code 
jni binding native mpi library java offers immediate advantages rapid software development associated java established performance existing mpi libraries 
transition java native code introduces additional time overheads points interest study 
different sets low level benchmarks may defined depending aims particular evaluation exercise 
goal compare communication performance delivered different message passing environments primary purpose low level benchmarks mark range message passing performance 
communication patterns outline messagepassing performance bounds may grouped categories single point point messages single collective communications point point transmissions nodes collective communications nodes 
follow categories order define small comprehensive suite low level message passing benchmarks mpi java mpi 
suite evaluate compare range message passing environments different platforms workstation clusters real supercomputers 
focus ibm sp 
characterization communication performance generally speaking time taken communications represents substantial part performance penalty pay message passing distributed memory computers 
cases communication subsystems machines cut method message subdivided smaller flits 
fast flit buffers hardware routers attached nodes forward flit message soon arrives 
flits message transmitted order inseparable companions pipeline fashion call intra message pipeline effect 
principle behaviour pipeline defined linear relationship time length chunk processed 
communication time modeled simple linear function message length com trans latency message startup time trans transmission time byte message length bytes 
share start time total communication time varies single message transfer initiates differing amounts depending length message buffer 
meaningful elegant way characterizing communication performance extend performance description vector pipelined processors model asymptotic bandwidth half performance message length half parameters 
com power simple formula fact provides single parameter computer system application software 
communication pipeline maximum asymptotic bandwidth occurs infinitely long messages designated parameter characterizes peak throughput communication subsystem 
point view application programmer tell close real bandwidth asymptote comparing average message length code second parameter message length equals real bandwidth half asymptotic maximum order gain better insight message passing overhead distinguish types pipeline effects intra message pipeline effects discussed earlier section pipeline effects 
names suggest intra message pipeline effects occur inside transmission single message inter message pipeline effects observed higher level series messages 
case system wait previous transfer complete starting 
transmission messages may overlap pipeline fashion resulting higher aggregate bandwidth 
communication subsystems tightly coupled parallel computers improving quickly years simple model valid multistage interconnection networks high speed communication switches ibm series parallel computers 
order distinguish specific attributes process environment influence communication tests focus message passing performance group communications data exchange patterns described methodology section 
particular selected tests help values important communication parameters latency asymptotic bandwidth message length trade offs constructing message passing buffers realistic scenarios involve overlapping computation communication 
methodology benchmark suite benchmarks provide insight specific aspects relating overhead associated transition messages various process environments underlying communication layer 
aim suite benchmarks provide details overhead scalability process communication 
benchmarks categorized follows ping pong benchmarks time round trip timing buffer returned buffer sent process process ping pong test 
interest average time required sequential point point passing messages 
simplest measure message passing performance ping pong echo benchmark pair processes described authors 
test see process sends single fixed length buffer turn receives message sends back immediately process 
buffer contains array primitive data types 
buffer size graduated broad range sizes elicit overhead environments commensurate effects latency bandwidth order measure transitional effects mapping primitives languages data types tested include byte arrays integer arrays double precision arrays 
particular interest point point time taken send message length processes 
gather time half average time needed large number round trips 
ping pong benchmarks time process process 
ping pong test timing results show point point efficiency effects inter message pipelining 
ping pong benchmark involves processes 
process responsible sending large number fixed length buffers receiving process 
receiving buffers single acknowledgment ack returned see 
buffers contain arrays primitive data types vary length composition bytes integers double precision primitives 
aim benchmark similar ping pong obtains insight inter message pipeline effect 
inter message pipelining permits overlapping message passing decreasing latency passing multiple messages increasing resultant bandwidth 
ping ping benchmarks time process process buffer returned buffer sent timings ping ping benchmark give indication communication overhead full duplex setting 
ping ping benchmark gives analysis situation full duplex type conditions exist 
involves processes simultaneously send receive messages complement process 
sending receiving done sequentially send immediately followed receive vice versa see 
buffers contain arrays primitive data types vary length composition bytes integers double precision primitives 
full duplex framework benchmark permits investigation effects demanding communication situations simultaneous exchanging boundary information processes domain decomposition calculation 
ping ping benchmarks ping ping benchmark aimed testing computation communication overlap message passing environment 
similar ping ping benchmark processes simultaneously send receive messages complement process 
ping ping benchmark sending receiving allowed asynchronous see 
tasks sending receiving relegated individual threads processes threads busy plain computations 
permits situation time process sending time process process 
ack final send buffer timings ping ping test show effect thread scheduling competition resources asynchronous full duplex communication environment 
thread may sent messages process receiving thread may received messages due thread scheduling competition resources buffers sent processes contain arrays primitive data types vary length composition bytes integers double precision primitives 
benchmark processes contribute timings consist times required send messages pn gamma pn waiting times pn gamma pn total times pn gamma pn denotes process 
times illustrated process 
collective communication benchmarks suite collective communication benchmarks expands results order provide insight overhead scalability associated group communications 
benchmarks consist processes 
primary interest benchmarks scalability message lengths effect graduation buffer lengths enumeration primitive types bytes integers double precision arrays part benchmark suites 
focal point communication single focal point communication benchmarks consist timings broadcast gather type 
types benchmark analogous ping pong ping pong tests described 
ping pong type benchmark focal point communications involves broadcast message process complement set processes processes simply respond kind 
ping pong type code focal point communications involves broadcast messages process followed final acknowledgment processes receiving messages 
concurrent operations concurrent operations benchmarks provide systematic examination cost synchronization multiple processes full duplex communication processes 
costs associated synchronization examined barrier benchmark 
costs associated full duplex communication multicast benchmarks natural ping ping ping ping benchmarks setting depicted respectively 
benchmarks described detail 
mpi barrier mpi barrier time mpi barrier mpi barrier mpi barrier mpi barrier mpi barrier mpi barrier mpi barrier mpi barrier mpi barrier mpi barrier mpi barrier mpi barrier mpi barrier 
process process process 
synchronization processes barrier benchmark tests overhead attributable synchronization processes 
barrier benchmark depicted synchronization overhead determined processes mpi barrier call 
timings obtained calculating time required synchronizations processes 
messages exchanged benchmark communication involved ascending barrier synchronization signal mpi layer layers making upper process layer 
example case java process mpi barrier synchronization signals lam implementation mpi pass mpi layer jni layer jvm executing java bytecode layers may lie 
second multicast suite group communication benchmarks include ping ping test ping ping test multi threaded processes 
ping ping test extends ideas corresponding benchmark see 
process multicasts message receiving identical message processes group process send message 
receiving message message simply returned originating process 
process emits messages initial broadcast receives messages group sends acknowledgments respectively waits acknowledgments message kind group 
measurements large number events total time divided get indication full duplex bandwidth 
ping ping multicast suite consists processes sending messages taken large members group 
simultaneously asynchronously process receives messages 
process receiving messages process sends simple acknowledgment back process total time required acknowledge processes computed result benchmark 
testbed experimental results order distinguish overhead introduced various aspects process environment common benchmarks performed configurations sp 
code native mpi version mpi developed sp ibm ibm mpi 

code lam mpi version lam mpi 

java code jni wrapped version lam mpi java lam mpi 
jvm java compiler part jdk aix version 
execution environment consisted ibm parallel operating environment poe supports loading execution parallel processes nodes ibm sp 
machine thin node power super chip sc processors mbytes memory processor 
communication subsystem sp features high performance switch experiments 
results reflect averaging performance trials 
performance data shown respective graphs represent transmission times messages total byte lengths 
timings ping pong benchmark message length bytes java lam mpi lam mpi ibm mpi timings ping pong benchmark double data types different environments ibm sp 
results ping pong benchmarks 
depicts results message buffer consists double precision dimensional array shows results corresponding integer array message buffer 
mpi processes subsequent benchmarks irrespective environment additional characteristic observed transition messages lengths message length bytes execution time sec java lam mpi lam mpi ibm mpi timings ping pong benchmark integer data types different environments ibm sp 
bytes integer double precision message contents 
attribute behavior mpi eager limit environment variable poe specifies transition eager protocol short messages rendezvous protocol large messages 
eager protocol suitable low latency low bandwidth messages rendezvous protocol intended high latency high bandwidth situations 
eager protocol buffer space preallocated receiver side deal early arrivals messages 
involves extra memory copy receiver internal buffer user buffer 
rendezvous protocol data moved receiver side message receive signal posted suitably sized buffer allocated 
result extra delay latency rendezvous protocol higher 
extra memory copy buffering split sender receiver increases bandwidth 
poe configuration benchmarks buffers short messages consisting bytes eager protocol buffers long messages bytes rendezvous protocol 
notable aspects measurements associated latencies asymptotic bandwidths corresponding environments calculated raw data squares procedure 
double precision ping pong scenario asymptotic bandwidth java lam mpi environment mbyte short messages mbyte long messages 
lam mpi environment asymptotic bandwidth mbyte short mbyte long messages 
ping pong scenario integer messages figures mbyte mbyte respectively 
important parameter benchmarks latency 
ping pong scenario latency java lam mpi environment short messages long messages double precision array messages respectively messages 
contrast lam mpi latency double precision messages messages consisting integer arrays native ibm mpi latency respectively 
timings ping pong benchmark message length bytes execution time sec java lam mpi lam mpi ibm mpi timings ping pong benchmark double data types different environments ibm sp 
results ping pong benchmarks 
depicts results message buffer consists double precision array shows results message length bytes execution time sec java lam mpi lam mpi ibm mpi timings ping pong benchmark integer data types different environments ibm sp 
corresponding integer array message buffer 
particular observation attributable inter message pipeline effect shows repeated sending messages benchmarks lends selective improvements ping pong results 
inter message pipeline effect increases bandwidth decreases latency short messages experiments difference long messages experiments 
rendezvous protocol practically disables inter message pipelining eager protocol allows flexibility overlapping simultaneous transmission short messages 
java lam mpi environment bandwidth figures short messages obtained time measurements squares fit procedure mbyte mbyte double precision integer message buffers respectively 
corresponding figures ibm mpi environment mbyte mbyte inter message pipeline effect doubles bandwidth lam mpi environment 
time latencies environments decreased significantly 
java lam mpi environment value parameter double precision buffers messages integer data type 
corresponding figures lam mpi latencies ibm mpi environment doubles integers 
discussion related benchmarking results depicted previous section derive communication performance parameters linear fit raw data help simple software tool 
basis formulae tool performs squares procedure calculate asymptotic bandwidth latency half message length parameters 
values obtained summarized table short messages table long messages 
table performance parameters short messages scenario bandwidth half latency benchmarks mbyte byte ping pong double java lam mpi lam mpi ibm mpi ping pong integer java lam mpi lam mpi ibm mpi ping pong double java lam mpi lam mpi ibm mpi ping pong integer java lam mpi lam mpi ibm mpi data reflected bring bear questions need addressed 
foremost need identify sources performance penalty message passing paradigm java comparison traditional message passing programming focused performance penalty introduced respective components jni wrapped mpi process 
process layer execution java bytecode takes place jvm layer execution entirely done inside mpi library jni layer permits transition execution java bytecode mpi library calls 
investigation breakdown time spent layers needed order understand sources performance penalty java lam mpi environment 
table performance parameters long messages scenario bandwidth half latency benchmarks mbyte byte ping pong double java lam mpi lam mpi ibm mpi ping pong integer java lam mpi lam mpi ibm mpi ping pong double java lam mpi lam mpi ibm mpi ping pong integer java lam mpi lam mpi ibm mpi way benchmarking tests designed allows ignore time overheads java programming language evaluated taken account separately necessary 
performance penalty introduced mpi libraries relatively studied understood 
evaluated component separately ibm mpi message passing environments 
instrumented java mpi binding gathered additional time measurements 
turns cumulative time spent wrapper software layer typically cases negligible share breakdown total execution time 
decrease asymptotic bandwidth significant order magnitude difference latency comparison corresponding results attributed jni extra data copies particular 
detailed study jvm performance precisely parameters jvm heap size stack size gc mode jit compilation influence performance numa type systems sp obviously interesting scope 
conjunction topics data tables begs question best address bottlenecks message passing performance improve areas latency bandwidth java message passing 
addition faster jvm jni implementations communication performance improvements depend efficient design api java 
area active research development couple years wrappers existing mpi libraries pure java designs 
helped identify usefulness message passing java real applications bringing light aspects java performance need improved order realize high performance communication 
benchmarks completed project go directly questions arise concerning java role scientific high performance computing 
includes results benchmarks taken environments consisting ibm mpi lam mpi java lam mpi ibm sp 
full set benchmarks include java ibm mpi environment subject current 
time ongoing project efforts part activities message passing working group international java grande forum 
experiments carried ibm sp installation san diego supercomputer center support npaci 
particular mary thomas help accessing operating ibm sp platform 
carpenter fox ko lim 
object serialization marshalling data java interface mpi 
acm java grande conference acm press june pp 

carpenter judd skjellum fox 
mpi java position document draft api specification 
technical report tr java grande forum nov 
www org reports htm dongarra otto snir walker 
message passing standard mpp workstations 
cacm 

performance intel ipsc ncube hypercubes 
parallel computing 
flynn hummel high performance parallel programming java exploiting native libraries 
concurrency pract 
exper 
gordon 
essential jni java native interface 
prentice hall 

performance parameters benchmarking supercomputers 
parallel computing 
judd clement snell 
design issues efficient implementation mpi java 
acm java grande conference acm press june pp 

kleinrock 
virtual cut new communication switching technique 
computer networks 
liang 
java native interface programmer guide specification 
addison wesley 
portable message passing java binding mpi 
dongarra eds advances pvm mpi lncs vol 
springer nov pp 

wmin ac uk mpi forum 
mpi message passing interface standard 
international journal supercomputer applications 
reed grunwald 
performance multicomputer interconnection networks 
ieee computer june 
xu hwang 
modeling communication overhead mpi mpl performance ibm sp 
ieee parallel distributed technology 

