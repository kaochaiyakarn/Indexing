high performance parametric modeling nimrod killer application global grid 
david abramson jon giddy lew school computer science crc distributed systems technology software engineering general purpose south building monash university university queensland clayton vic australia st lucia qld australia australian radiation protection nuclear safety agency lower plenty road vic australia examines role parametric modeling application global computing grid explores heuristics possible specify soft real time deadlines larger computational experiments 
demonstrate scheme case study utilizing globus toolkit running gusto testbed 
parametric computational experiments increasingly important science engineering means exploring behavior complex systems 
example engineer may explore behaviour wing running computational model airfoil multiple times varying key parameters angle attack air speed results multiple experiments yield picture wing behaves different parts parametric space 
past years developed specialized parametric modeling system called nimrod 
nimrod uses simple declarative parametric modeling language express parametric experiment provides machinery automates task formulating running monitoring collating results multiple individual experiments 
equally important nimrod incorporates distributed scheduling component manage scheduling individual experiments idle computers local area network 
features mean complex parametric experiments defined run little programmer effort 
cases possible establish new experiment minutes 
nimrod applied range application areas including bioinformatics operations research network simulation electronic cad ecological modelling business process simulation 
whilst nimrod successful suffers limitations considered context global computational grid large number computers linked globally form seamless supercomputer 
uses static set resources discover new ones dynamically 
suited fixed infrastructure type global grid 
second idea user deadlines 
adequate users fixed resource case user estimate time computation take complete adjust experiment meet project milestones 
global grid difficult user compute computation may complete underlying resources provide performance guarantees nimrod provide mechanism allow user specify deadline 
deadline control important nimrod deployed global grid timeliness solution may important user 
third nimrod relies standard unix level security 
global grid owners expensive supercomputing facilities require elaborate security mechanisms 
nimrod support range access mechanisms provided various queue managers 
global grid different queue managers exist supported single solution acceptable 
describe new version nimrod called nimrod addresses shortcomings 
particular focus basic nimrod model extended provide soft performance guarantees dynamic heterogeneous computational grid 
describe simple effective scheduling component nimrod seeks meet constraints dynamic iterative process resource discovery resource acquisition resource monitoring 
introduces nimrod globus toolkit nimrod built 
describes scheduling algorithm illustrates utility system case study 
parametric modeling nimrod nimrod tool manages execution parametric studies distributed computers 
takes responsibility management experiment low level issues distributing files remote systems performing remote computation gathering results 
commercial version research system nimrod 
user describes experiment nimrod develop declarative plan file describes parameters default values commands necessary performing 
system uses information transport necessary files schedule available machine 
plan file composed main sections parameter section tasks section 
shows sample plan simulation discussed 
experiment consists varying thickness parameter execution receives different seed value 
nimrod generates job unique combination parameter values cross product values 
plan jobs generated control computational physics case study 
user invokes nimrod workstation machine known root machine controls experiment 
dispatcher executes code remote platforms known computational node 
experiment conducted root multiple nodes different architecture required 
nimrod supports phases computational experiment 
phases performed experiment phases run distinct parameter set 

experiment pre processing data set experiment 
execution pre processing data prepared particular execution 
execution program executed set parameter values 
execution post processing data particular execution reduced 
experiment post processing results processed example running data interpretation visualization software 
parameter integer range step parameter thick label buc thickness float range step parameter integer compute thick task copy os node copy dummy node 
copy dat node 
copy skel inp node 
task main node substitute skel inp inp node execute copy node op sample plan file phases files may moved root machine cluster processes general parallel computing communication occurs tasks 
example tasks main 
main task executed set parameters 
runs simulation called node passing parameter values program parameter file 
copies number result files back root machine appending name unique identifier 
task corresponds rd phase discussed previous section 
task executes experiment corresponds st phase described previous section 
copies file remote node common simulations skeleton input parameter file 
processed substitute command replaces placeholders actual values 
copies correct binary target operating system 
plan file processed tool called generator 
generator takes parameter values gives user choice actual values 
builds run file contains description job 
run file processed tool called dispatcher responsible managing computation nodes 
dispatcher implements file transfer commands execution model remote node 
nimrod dispatcher allocates machines attempt schedule execution 
nimrod described detail section components generator 
dispatcher completely rewritten support scheduling globus toolkit 
globus toolkit functionality globus grid toolkit collection software components designed support development applications high performance distributed computing environments grids 
globus toolkit implementation bag services architecture provides application tool developers monolithic system set standalone services 
globus component provides basic service authentication resource allocation information communication fault detection remote data access 
different applications tools combine services different ways construct grid enabled systems 
components nimrod globus resource allocation manager gram service provides api requesting computations started computational resource managing computations started 
metacomputing directory service mds provides api discovering structure state availability resources may interest computation 
globus security infrastructure gsi provides single sign run authorized capabilities computations need operate computers 
global access secondary storage gass service provides uniform access mechanisms apis files stored various storage systems 
globus resource management architecture currently extended support advance reservations support differentiated services networks 
experiments advance reservation differentiated services networks conducted context local area testbed argonne esnet testbed linking anl lbnl 
anticipate useful versions nimrod means ensuring computations performed required schedule 
scheduling grid scheduling programs parallel distributed computers topic research years great deal accomplished 
targeted scheduling placing processes parallel computer program represented task graph amount nature computational resource static duration execution 
environment leads naturally scheduling heuristics try produce schedule observes number constraints 
goal minimise execution time task spread load optimally 
computational grid presents challenging domain scheduling performance underlying resource highly variable change application scheduled 
accordingly possible consider grid single computer system control single scheduler 
application domain parameter studies places additional demands scheduler 
studies complex time consuming important receive results timely manner 
traditional parallel computing scheduling systems may attempt minimise execution time application optimise load distribution machines argue allowing user specify absolute soft deadline useful way expressing timeliness computation 
interestingly deadlines commonly applied real time systems basis scheduling 
nimrod grid aware application 
exploits understanding problem domain nature computational grid provide high level interface user 
specifically provides transparent access computational resources implements user level scheduling 
case attempt schedule unrelated tasks user supplied deadline met 
basic nimrod model communication tasks started scheduling problem finding suitable resources executing application 
problem sounds simple dimension scheduling complexity increased due parameters computational economics deadlines usage resources scattered different administrative domains varied machine communication performance 
number researchers considering scheduling problem context applications including ninf apples project netsolve nile 
condor matchmaking match resources tasks parameter studies supported explicitly deadline scheduling performed 
nimrod description architecture nimrod designed operate environment comprises set sites providing access set computers administrative control access resources mediated globus resource allocation manager gram 
information physical characteristics availability resources available globus directory service mds 
illustrated user process acting behalf initiates parametric study local site nimrod organizes mapping individual computations appropriate remote sites nimrod scheduling heuristic 
local site origin process operates master system 
scheduling monitoring logic described encapsulated origin process exists entire length experiment ultimately responsible execution experiment specified time cost constraints 
note structure differs significantly nimrod described section designed support scheduling computations resources scattered globe administrative policy control 
user person process responsible creation experiment 
user interacts origin process client process 
user client required available time 
distinction client origin useful client may tied particular display environment 
user client move environment start client affecting origin process progress experiment 
addition possible multiple clients monitor experiment connecting origin process 
feature allow team melbourne chicago observe computational experiment 
sample gui monitor shown 
example shows number jobs running various gusto resources 
shows deadline cost budget interface 
remote site consists cluster computational nodes 
cluster may single multiprocessor machine cluster workstations single processor 
defining characteristic cluster access nodes provided set resource managers provided globus infrastructure 
globus resource allocation manager gram implements method accessing processing resources 
typically method queue batch queuing system condor lsf process fork shared memory machine 
nimrod architecture sample monitor gui submitting jobs cluster origin process uses globus process creation service start nimrod resource broker cluster 
different entity resource manager 
provides capabilities file staging creation jobs generic experiment process control provided gram 
cost global grid restrictions placed access various resources global grid congested short order 
believe suitable model controlling amount requested fiscal users pay access 
scheme allows resource providers set pricing rates various user client origin nimrod resource broker globus resource manager local site remote site queue machines vary classes machine times day resource demand classes user 
communication cost taken consideration especially data communication cost significant compared computation 
whilst developed complete model cost global grid experimented nimrod defining cost matrix users resources 
example cost resources different users 
particular user finds resource cheaper user find resource cheaper 
asymmetry means initial resources user may different providing example mechanism favoring local machines 
generate cost matrix hand mutual agreement relative costs systems 
expect elaborate costing scheme need devised topic ongoing research 
section describe cost current version nimrod sample nimrod cost matrix scheduling algorithm nimrod scheduler responsible discovering allocating resources required complete experiment subject specified execution time budget constraints 
scheduling problem complicated fact grid environment typically guarantee exclusive immediate access underlying resources 
accordingly traditional scheduling techniques discussed necessarily applicable resource availability execution rates vary unpredictably 
address problem simple heuristic 
discovery number identity lowest cost set resources able meet deadline identified 
cost matrix described section identify low cost resources queries mds directory service determine resource 
output phase set resources jobs submitted ordered cost user 
different users may generate lists sorted different order 

allocation unscheduled jobs maintained pool parameter set allocated candidate resources identified step 

monitoring completion time submitted jobs monitored establishing execution rate resource 

refinement rate information update estimates typical execution times different resources expected completion time job 
refinement process may lead return steps discover new resources drop existing resources candidate set 
scheme continues deadline met cost budget exceeded 
occurs user advised deadline modified accordingly 
consequence way chosen cost cost experiment vary depending load profile users time 
reflects type pricing occurs auction demand allow experiment performed cheaper resources 
shall see initial experiments suggest heuristic works practice applications tend determine quite quickly appropriate job submission rate expensive resources necessary 
case study report experiment conducted evaluate effectiveness nimrod architecture scheduling heuristics real application 
experiment resources provided sites participating international globus ubiquitous supercomputing testbed organization gusto 
gusto sites run standard globus software persuaded resources available grid computing experiments 
show table study reported resources argonne boston university monash university northern illinois university usc information sciences institute university wisconsin 
resources quite diverse terms size availability architecture processing capability power performance scheduling mechanism immediate access fork condor queue lsf queue geographic location 
max number nodes column table indicates number nodes machine average free nodes resources users indicates average number available experimental period april 
table shows currently artificial cost value system 
ionization chamber calibration australian radiation protection nuclear safety agency provides primary australian standards certain radiological quantities air essentially quantifies photon energy deposition terms ionization air 
calibration medical equipment diagnosis treatment standards ensures location radiation doses received patients consistent received locations australia overseas 
ionization chamber essentially isolates certain volume air measures ionization volume 
physical process isolating volume air modifies original photon electron spectrum entering volume 
chamber act primary standard calibration purposes necessary correct measured ionization spectral perturbations due physical presence chamber 
nature problem correction factors determined experimentally 
field radiation monte carlo simulation programs egs package acceptable method simulating response ionization chambers photons electrons alternative means determining appropriate correction factors 
absolute accuracy egs package simulating ion chambers quoted relative accuracy better 
important correction factor primary standard standardization photons emitted source extent photon attenuation scattering front wall chamber 
detailed measurements chamber response function front wall thickness performed quantitative model interactions front wall determine appropriate correction factors 
detailed measurements provide interesting basis comparison monte carlo chamber photon spectrum simulated 
calculations reported concern simulation chamber response function front wall thickness 
nimrod perform parametric variation whilst running model times different random number seeds different front wall thicknesses 
calculated data may normally distributed necessary calculate averages sets data take mean averages obtain mean value components normally distributed particular front wall thickness 
scientific results important output variables model number ion pairs created collecting volume incident photon 
accordingly wish compare simulation data experimental data 
order fit experimental simulation results expression form ae bt effective attenuation coefficient photons effective attenuation coefficient electrons thickness front wall chamber 
comparison fitted coefficients simulation experiment table 
results show egs model quite accurate 
considering data plotted thickness graph model particularly accurate thickness cm 
order improve accuracy model believe require order times number simulations 
results obtained relatively modest number processors order working day achievable easily 
plan perform simulations near 
computational results simulations described run subset gusto testbed resources defined table 
nodes subset available trial 
number sufficient allow testing key features nimrod particular ability schedule tasks time cost constraints 
ionization chamber study involved tasks 
execution time model varied depending platform ranging minutes parameter set sgi machines minutes niu isi 
separate experiments performed deadlines hours hours hours respectively allow evaluation nimrod ability meet soft real time deadlines 
graph shows number nodes function time deadline 
surprisingly nimrod allocates additional resources stringent deadlines 
whilst result appears obvious stressed results obtained real test bed resources discovered load distributed dynamically 
algorithm adapted distribution tasks prior knowledge initial load testbed configuration speed individual machines 
machine location method starting jobs machine type rel 
cost max nodes typ 
avail anl chicago fork ultrasparc ii anl chicago fork ultrasparc ii condor sun lemon anl chicago fork mips anl chicago fork mips flash isi la fork mips jupiter isi la fork mips north illinois fork ultrasparc isi la fork ultrasparc isi la fork ultrasparc isi la fork ultrasparc monash condor pentium ii denali anl chicago fork mips lego boston lsf mips total table gusto machines case studies 
parameter egs calculation experiment ratio table coefficients interest graphs break selected nodes cost time 
graphs show scheduler initially favours cheaper resources time advances finds deadline achieved current resource set expensive machines introduced 
example graph cost unit machines introduced half hour scheduler calculates meet hour deadline cost unit machines 
similarly graph seen cost unit machines introduced hours order meet hour deadline whilst required hour experiment 
predictions measurements real performance machines whilst running ionization code 
table quantifies impact cost different node selections different deadlines hour deadline costs times hour deadline 
reflection distribution resources available time performed particular cost matrix 
easily show dynamic environment nimrod making optimal selections clear nimrod effective selecting expensive nodes system requires meet deadline 
course costs vary depending current load test bed lowest cost resources may available 
experiment total cost cost units hours hours hours table cost experiments discussed evolution particular tool nimrod local computing environment global computational grid 
described various services globus applied building grid aware application 
particular focussed problem providing soft real time deadlines environment 
algorithm simple adaptive changes workload distribution grid incorporates user requirements system ones 
case study illustrated possible build application takes account highly dynamic unpredictable nature grid 
regard efforts area preliminary 
need develop complete model cost environment particular issues storage money deployed 
wish take account ability globus reserve resources incorporate scheduling mechanism 
appear notion privilege priority addition money important particularly user wishes local resources place global grid 
example current scheduling algorithm possible user forced external expensive resources just local ones working user 
number implementation issues need addressed way nimrod explores globus mds search suitable machines 
issues topic ongoing research 
project funded distributed systems technology centre australian government crc program 
wish colleague ian foster valuable contribution support rajkumar buyya proofreading 
acknowledge stuart martin miron livny assistance obtaining access gusto condor resources 
abramson sosic giddy cope laboratory bench distributed computing simulations parallel computing transputers conference nov pp 
abramson sosic giddy hall nimrod tool performing simulations distributed workstations th ieee symposium high performance distributed computing virginia august 
abramson foster giddy lewis sosic white nimrod computational workbench case study desktop metacomputing australian computer science conference macquarie university sydney feb 
active tools www com berman high performance schedulers grid blueprint computing infrastructure pages 
morgan kaufmann publishers berman wolski apples project status report proc 
nec symposium metacomputing 
bestavros load profiling methodology scheduling real time tasks distributed system 
proceedings icdcs ieee international conference distributed computing systems baltimore maryland may 
foster kesselman tuecke gass data movement access service wide area computing systems proc 
acm press 
casanova dongarra netsolve network server solving computational science problems international journal supercomputing applications high performance computing vol number pp 
optimization system server preprint mcs argonne national laboratory argonne il 
fitzgerald foster kesselman von laszewski smith tuecke directory service configuring high performance distributed computations proceedings th ieee symposium high performance distributed computing ieee press 
foster kesselman globus grid architecture 
grid blueprint computing infrastructure pages 
morgan kaufmann publishers 
foster kesselman tsudik tuecke security architecture computational grids acm conference computers security acm press 
cs monash edu au www globus org www platform com lewis abramson sosic giddy parameterisation application perspective computational techniques applications conference melbourne july 
litzkow livny mutka condor hunter idle workstations proceedings th international conference distributed computing systems pp june 
livny high throughput resource management 
grid blueprint computing infrastructure pages 
morgan kaufmann publishers marzullo ricciardi nile wide area computing high energy physics proc 
sigops conf 
new york acm 
matsuoka ninf network information library globally high performance computing methods applications 
nelson rogers egs code system stanford linear accelerator center report slac zhou zheng wang delisle utopia load sharing facility large heterogeneous distributed computer systems software practice experience december 
graph chamber response thickness graph gusto usage hour deadline time processors cus cus cus cus cus cost units cost units graph gusto usage hour deadline time processors cus cus cus cus cus cost units cost units cost units cost units graph gusto usage hour deadline time processes cus cus cus cus cus cost units cost units cost units cost units cost units graph gusto usage ionization chamber study time average processors hour deadline hour deadline hour deadline egs simulation response cavity chamber photons thickness front wall cm ion pairs created collecting volume incident photon calculated data fitted sum exponentials 
