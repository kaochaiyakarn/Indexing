iterative retrieval associative memories threshold control different neural models thomas wennekers friedrich sommer palm department neural information processing university ulm ulm germany investigate retrieval properties hebbian auto associative memories limit sparse coding 
appropriately chosen threshold control strategies finite size associative memories increase completion capacity iterative retrieval fast step retrieval asymptotic capacity extremely large networks 
relate results biologically motivated network consisting coupled cells controlled globally acting inhibitory interneuron 
choosing homogenous coupling matrix different excitatory single neuron types find explicit numerical comparison global behavior spiking neurons general different rate function probabilistic binary units 
show network spiking neurons hebbian coupling matrix able complete segregate distorted patterns simultaneously input 
case global dynamics falls rhythmic activity processes input pattern period behavior related rhythmic cortical activity example visual cats monkeys 

donald hebb idea cell assemblies reasonable internal representation events concepts situations cerebral cortex elaborated considerable detail 
engineering point view closely related auto associative memory stored patterns retrieved iterative pattern completion 
ideas need mechanism threshold control dynamical adjustment global activation parameter control total activity level 
concerned analysis process threshold control 
start simple observation concerning level total activity stabilized equilibrium excitation inhibition 
consider simple threshold neurons 
neuron output binary weighted sum inputs exceeds threshold value herrmann wolf eds supercomputing brain research tomography neural networks pp world scientific singapore 
iterative retrieval associative memories threshold control different neural models ae ij take statistical large randomly connected network average neuron gets excitatory afferents inhibitory afferents strength afferents inputs modelled binary random variables respectively probability delta ne gamma delta answer calculated means elementary probability plotted 
realistic values numbers excitatory inhibitory inputs stable activity possible low total activity 
fig 

iteration curves percentage active neurons randomly connected neural network 
neuron receives average excitatory inhibitory connections 
strength inhibitory connection times excitatory 
activation threshold varied units corresponding strength excitatory connection 
analysis extremely simplified model neurons random connectivity 
section consider structured connectivity matrices ij built auto associative memory 
model compare essentially different strategies dynamical threshold control optimize amount retrieved information 
full dynamical stability problem threshold control studied realistic spiking neural models include low pass filtering time constants time delays control loops 
remaining sections contribute analysis problem 
iterative retrieval associative memories threshold control different neural models 
iterative pattern completion investigate iterative pattern completion optimally filled large auto associative memory 
shows retrieval capacity optimal sparseness represented number components optimal patterns achieve maximal completion capacity varying memory size 
bits synapse 
neurons ffi ffi ffi ffi ffi pi pi pi pi pi ffl ffl ffl ffl ffl 
neurons fig 

optimal capacities optimized 
case 
curves obtained theory step step retrieval 
simulation results plotted step ffi step pi iterative retrieval ffl 
asymptotic capacity value approximately step step iterative retrieval shown horizontal line 
optimal length address pattern corresponding optimal capacity achieved step retrieval strategy tk 
compare different threshold control strategies optimized situation patterns stored auto associative neuron feedback network 
pattern components gamma zero components 
start retrieval iterative retrieval associative memories threshold control different neural models process pattern ones cases ones start pattern form proper subset ones defining stored pattern retrieved 
comparison proper subset completion correction additional wrong ones see 
bits synapse length address ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi pi ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl fig 

capacity step ffi step pi iterative retrieval ffl th step threshold control strategy tk different address patterns solid line shows completion capacity error free retrieval 
fig 

proper choice threshold number ones input pattern 
greyscale represents numerical estimate probability strategy ca chooses certain threshold input patterns fixed activity threshold control strategy abbreviated tk simply chooses min 
idea strategy retrieval step correct ones active neurons threshold help weed additional wrong ones 
strategy improved technically forms intersection pattern retrieval process previous pattern preventing growth pattern strategy iterative retrieval associative memories threshold control different neural models tk 
technically motivated strategy tk realized biologically means specially adjusted low pass behavior neurons 
iterations 
stored patterns ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl bits synapse 
stored patterns ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl fig 

mean iteration time incremental binary hebbian learning rule matrix size 
effective storage range incremental binary hebbian learning 
capacity incremental binary learning rule plotted step retrieval ffi iterative retrieval ffl stopped th step 
surprisingly binary learning achieves higher capacity values incremental learning 
neurally realistic control strategy ca adjusts threshold way pattern close possible set value shows proper choice threshold plotted obviously strategy quite predetermined threshold control rule stabilize total activity set value retrieval needs iteration steps steps capacity improved compared retrieval step iterative retrieval associative memories threshold control different neural models 
contains comparison binary synaptic learning rule simply switches synapse coincident pre postsynaptic activity binary hebbian learning studied willshaw incremental hebbian learning sum outer products rule commonly discussed physics community 
shows biologically realistic retrieval procedure ca efficient optimal technical procedure tk 
results show threshold control total activity fact useful retrieval strategy large auto associative networks sparse activity patterns 
surprising result simulation studies finding retrieval capacity increased iteration process theoretical asymptotic value asymptotic iterative retrieval capacity value finite capacity reaches maximum simulations shown indicate 
bits synapse 
stored patterns ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffl ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi pi pi pi pi pi pi pi pi pi pi pi pi pi pi fig 

capacities achieved iterative retrieval steps different threshold control strategies ca pi tk ffi tk ffl 

threshold control biologically motivated neural network models previous section dealt associative memories technical point view insofar retrieval strategies threshold computations defined mathematical form guided requirements fidelity criteria 
want relate results detailed models neural networks models address possible implementation dynamic thresholds real networks 
furthermore want discuss implications may temporal properties neural activation 
considered case single static patterns memorized 
albeit useful simplified starting point study general learning memory processes previous sections obviously oversimplification iterative retrieval associative memories threshold control different neural models real physical environments 
usually contain objects simultaneously possibly change appearance sensory system time 
case commonly studied paradigm attractor neural networks appropriate assumes pattern distorted noisy part retrieval process 
dynamics network evolves autonomously eventually leading attractor state nearest input pattern kept long time 
unclear networks react continuously changing environment retrieval process effectively decoupled input latched stationary states sustained activation 
results previous section point possible solution problems 
shown retrieval strategies feedback steps chosen time dependent threshold infinitely iterating ones 
memory task essentially done short iteration time result possible 
iterations deteriorate retrieval quality 
represent waste resources physiological point view best choice control network activity suitable chosen global threshold mechanism section switch soon solution 
network restarted new input pattern sensory stream 
stated network reset performed aid dynamical threshold temporally exceeds net activity cells switched 
way system react fast possible moving changing objects give rise different sets input fibers carrying significant activity time input resampled 
defined processing period short compared time needed noticable changes environment input signals appear quasi stationary may applied retrieval period 
accordance biological requirements sensory afferents continuously effect cortical tissue 
advantage cells belonging retrieval patterns 
patterns superimposed input stream serious problem mode operation 
shown retrieval period designed network select complete choose period 
furthermore may happen retrieval fails pattern contained input memorized 
pattern attractor state dynamics stable long time reset network soon memory plays role intermediate error immediately corrected 
wrong pattern general overlap ones possible say net tried alternative interpretation input association psychological sense 
turn question dynamical threshold iterative retrieval associative memories threshold control different neural models realized biological networks 
stated serve different tasks retrieval progress adapt global network activity 
cells fire receive strongest afferent recurrent input 
highest probability belong retrieval pattern 
active cells suppressed completely active state cells firing avoided 
second pattern recognized activity switched break short time allow reset network information processing 
mechanism described type naturally provided inhibitory interneurons measure network activity neighborhood turn inhibit neighboring cells unspecific manner 
kind inhibition equivalent dynamical threshold insofar mechanisms influence firing probability cell way higher inhibition relates higher thresholds 
excitatory inhibitory cells biological neural networks equivalent 
different anatomy physiology probably respect functional role 
excitatory neurons show wider axonal usually utilized long range connections different cortical areas 
evidence synapses related neuronal plasticity widely believed play major role neuronal computations learning memory 
models computer simulations take difference consideration 
network contained connections restricted values greater equal zero 
fact weight sign restriction sense reason threshold control problem appears networks cells mixed synapses balanced ratio excitatory inhibitory interactions hopfield model variants able retrieve constant threshold typically zero 
networks fulfil simple biological fact cell solely positive excitatory negative inhibitory synapses called law 
taken account dynamical behavior network drastically changes 
stored patterns new attractor state appears network purely excitatory synapses corresponds sustained firing cells attracts initial configurations 
section purely excitatory network observed explosive activity increase case low thresholds 
inhibitory neurons contrast excitatory cells smaller axonal dendritic 
approximately cortical cells excitatory probably learn reasonable assume contribute cortical information processing lesser extent excitatory neurons mainly serve support control purposes just example described threshold control 
dendritic tree inhibitory neuron crossed axons large iterative retrieval associative memories threshold control different neural models number cells approx contacts 
true synapses learn assume selection cells contact strength synapses large extent randomly distributed 
due law large numbers membrane potentials essentially represent estimate average network activity 
turn feedback connections excitatory cells random sense sort averaging better excitatory cells small volume containing large number neurons experience similar inhibitory influences global activity control needed model 
question remains open necessary network reset initiated words detected learned pattern 
hopfield model algorithmically done testing fixed point limit cycle reached 
difficult imagine algorithm implemented biologically 
simple alternative solution rise threshold time corresponding iteration steps 
simple low pass characteristic inhibitory cells biologically provided passive membrane properties real neurons suffice 
low pass follow accumulate network activity short phase lag inhibition eventually exceed excitation 
charging time depends excitation switch time extent adapt network activation 
input pattern nearly perfect result shorter switch time quite small fraction learned pattern 
note switch time define length retrieval period excitatory cells feedback missing threshold exceeds excitatory potentials 
means computations suppressed inhibition respectively threshold relaxes cells strongly driven afferents start fire 
idea similar ideas threshold control pump thoughts proposed braitenberg palm 
going details want mention described mechanism supported properties physiological neurons 
side inhibitory cells example distributed membrane time constants firing thresholds 
adapt threshold closer current activation initial period retrieval yield strong rise iterations done increase excitatory activity occurs due amplification pattern 
side excitatory cells refractory mechanism useful leads increase inhibition 
consequence neurons belonging retrieval pattern fire iteration step distribute firing steps 
temporal structure appears neuronal firing provide information stimulus 
lastly patterns contained iterative retrieval associative memories threshold control different neural models input certain amount noise helpful stochastically patterns retrieval completed 
basic model noise retrieve pattern largest initial overlap 
summarize dynamic threshold control follows afferent fibers charge cells physical firing threshold 
noise may favour pattern 
ii generated action potentials distributed network recurrent connections associated cells turn raise threshold 
iii inhibitory cells excited increase global threshold neurons belonging retrieval pattern suppressed 
iv items ii iii iterated inhibitory activity increased exceed excitation 
missing excitatory feedback avoids firing inhibition respectively global threshold relaxed process starts 

extended network model describe extension associative memory model section incorporates stated threshold control mechanism simple form 
similar model studied pay special attention selection appropriate single unit models 
shown possible choices intrinsic properties model neurons dynamically equivalent lead behavior network 
important implications biologically related modeling neural networks 

excitatory units network consists mutual coupled excitatory neurons 
means connection matrix positive zero entries 
contrast previous case single unit dynamics complicated 
consists stages low pass representing neuronal membrane properties input mechanism generation output signals current membrane potentials 
different variants chosen ffl membrane properties low pass characteristic additional white noise strength standard deviation oe 
means total input cell afferent recurrent dx dt gammax wn oe iterative retrieval associative memories threshold control different neural models ffl generation output signals graded response rate function units output cell sigmoid function membrane potential interpreted momentary firing rate cell 
stochastic binary units output time zero 
certain potential value probability having timestep length dt prob dt binary neuron refractoriness zero 
set time dt time potential crosses cell intrinsic time depended threshold gamma ae ffi gamma ffi ffi time spike ffi time ffi spike cell excitable 
relaxes value cell 
comparison single unit types functions chosen accordance rate function spiking neuron model function depends external noise complicated manner general explicit mathematical form 
determined numerically supplied look table furthermore set external noise zero models influence contained supplied oe 
iterative retrieval associative memories threshold control different neural models 
inhibitory control threshold control extended model implemented simple form single graded neuron membrane low pass linear output function 
input neuron sum outputs excitatory cells computes average network 
single inhibitory unit interpreted representative pool cells computing estimate average excitatory activity described section 

network equations network inhibitory excitatory cells described set equations 
delta delta delta dx dt gammax ij gamma ky dy dt gammay outputs computed single unit models 
coupling memory matrix inhibitory coupling strength 
inhibitory membrane constant 
excitatory membrane constant set 

comparison models different single unit types section compare dynamical networks containing units defined types 
simplicity assume unstructured connections value ij interpreted case single learned pattern 
interested memory retrieval general global network behavior set external inputs certain fixed value network invariant permutations cells 
parameters choose oe 
note inhibitory synapses effective excitatory ones case real cortical networks 
cells equivalent model network effectively described set differential equations excitatory inhibitory membrane potentials 
set form equation analyzed possible solutions 
input strength low high unique asymptotically stable fixed points 
intermediate range stable limit cycle appears 
possible show model large exhibits behavior 
limit infinite system size potentials follow exactly equation 
asymptotic behavior networks 
iterative retrieval associative memories threshold control different neural models fig 

network behavior model equations 
steps rate function neurons type step probabilistic binary neurons type rest simulation spiking neurons refractory mechanism type 
upper plot contains spikes excitatory cells middle shows membrane potential spikes dots excitatory cell lower plot presents global threshold 
single unit variants lead stationary global behavior 
fig 

input strength 
models stationary spiking neurons refractory mechanism lead oscillations 
iterative retrieval associative memories threshold control different neural models fig 

input strength 
models oscillatory regime model mode nearly cells fire synchronously 
model difficult analyze 
gerstner shown equivalent stationary state clear conditions 
show explicit numerical comparison longer case non stationary states bifurcation scenario model different models presents simulation runs different input strengths 
contains data model types steps correspond model steps model steps model simulation run output generation excitatory units changed rest computer algorithm remains 
upper plot spike raster cells 
middle plot contains membrane potential spikes small dots unit lower plot gives time course potential neuron global threshold 
step zero external input switched 
appear transients simulation 
low input values network types lead stationary states 
clearly seen potentials type neurons 
type units fluctuations observed inhibitory signal 
due probabilistic generation output cells 
vanish infinitely large networks 
iterations type units excitatory potentials reflect external noise independent network size 
contrast fluctuations seen inhibition time effect 
tend zero 
expectation value inhibition case nets reach stationary state 
iterative retrieval associative memories threshold control different neural models fig 

slowing fluctuations inhibitory signal increasing system size compare 
solid type dashed type units 
fig 

power spectrum inhibitory signal network containing type neurons 

spectrum clearly peaked albeit network stationary state 
fact fluctuations model quite large look uncorrelated short time scale 
power spectrum inhibitory signal steps shown 
clearly peaked 
frequencies units nyquist frequency roughly corresponds hz real systems 
important fact experimental point view 
albeit state network stationary classify usual empirical frequency domain methods oscillatory 
biological networks finite size reduces predictive power theoretical methods iterative retrieval associative memories threshold control different neural models infinite system size limits 
network contained cells glance low 
contrast biological networks completely coupled cortical connectivity percent 
lower connectivity leads larger fluctuations fixed measurable oscillations detected number relevant biological systems order number synapses real neurons typically 
exhibit simulation runs higher input strength 
seen single unit types equivalent show global stationarity oscillations occur vanish large 
global state oscillatory higher input 
network mode nearly cells fire oscillation period 
input increased certain value oscillations disappear remain input strengths larger critical value data shown 
transition points stationary oscillatory behavior different structure 
networks belong different universality classes drawn model take 
context special problem carefully consider spiking neurons rate models appropriate 

rhythmic activity pattern segmentation turn back problem associative retrieval dynamical thresholds 
store patterns coupling matrix single unit model choose type resembles properties physiological neurons shows simulation run excitatory neurons 
patterns stored 
mixture parts patterns input part containing approximately half ones perfect pattern 
upper plot presents spikes cells stored patterns 
cells ordered belonging pattern grouped possible linear array overlapping patterns 
lower plots give time course overlaps patterns network state 
seen pattern contained input half total size recognized perfectly retrieved simulation 
usual task associative memory 
completion done simulation steps 
time activity strongly rises generated spikes concentrated short time interval 
usually bin real networks approximately correspond time needed monosynaptic distribution spikes excitatory cells 
multi unit spike activity appear synchronized resolution scale milliseconds 
obviously raises signal noise iterative retrieval associative memories threshold control different neural models ratio comparison poisson processes areas receiving input considered effectively 
fig 

simulation run associative memory linear inhibitory control 
patterns stored network type units 
patterns partly contained input 
upper plot shows spikes excitatory cells belonging 
lower plots give overlaps current network spike pattern 
fast activity increase furthermore leads strong inhibitory response switches network activity immediately recognition pattern 
synchronous burst activation representing result neuronal computation transmitted areas network recover succeeding time relaxing inhibition 
leads global network behavior 
period usually single input pattern completed 
stored patterns selected mainly depends external noise determines neurons exceeding relaxing threshold 
periods show simultaneous retrieval patterns near steps pose problem appear merged patterns separated post processing area aid dynamical threshold 
mention network optimized pattern segregation frequency merging events drastically lowered sets parameters modified inhibitory control 
iterative retrieval associative memories threshold control different neural models 
starting simple formal model shown properties associative retrieval improved neural networks activity threshold comparison operating constant thresholds 
memory capacity larger explosive increase network activity avoided cases retrieval process switched iteration steps retrieved pattern perfect 
network quickly respond changing environments 
related results real neural networks discussed implemented biologically 
argued answer provided anatomical physiological functional differences excitatory inhibitory neurons 
opinion information processing preprocessing learning memory done cortical sub network excitatory cells inhibitory cells serve support control purposes 
purely excitatory processing network need sort global inhibitory activity control pathological state cells firing avoided self inhibition 
global means averaging large number excitatory cells take place measure mean activity get hint adapting threshold cellular firing sparse 
proposed excitatory associative network controlled acting global inhibition modulates firing thresholds computing neurons 
comparing typical behavior network different types single neuron models rate function probabilistic binary refractory spiking units observations important biological modelling 
different single unit models lead global network behavior 
drawn rate function models general carry utilizing realistic spiking neurons 
results obtained model interpreted cautiously top fact simulation results interpreted carefully 
second artificial real neural networks viewed infinitely large 
appropriate purposes simulations suggest finite size effects may provide plausible explanations biological phenomena 
hebbian connectivity matrix spiking neurons appropriate choice system parameters network able handle inputs environment containing distorted objects 
perform task network falls rhythmic global activity processes input pattern period 
biophysical considerations concerning model parameters estimate average period length roughly ms short allow processing slowly moving objects 
results computations valid short time intervals corresponding enhanced activity 
intervals order membrane time constants postprocessing cells significantly enlarges signal noise ratio quality informa iterative retrieval associative memories threshold control different neural models tion representation processing stage 
want emphasize threshold precisely mechanism leads discharge mass activation neural populations 
underly rhythmic activity eeg local field potentials measured visual areas cat monkey 
oscillations related called binding problem visual information processing coherent signals neuron population link cells respond different parts features objects scene 
objects treated entities subsequent processing operations 
idea reflected nearly synchronous firing responding patterns 
take retrieval step time action potential needs propagate cell influence ms compare mean period observed oscillations range ms find just time iteration steps accordance results optimal iterative retrieval 
acknowledge support deutsche forschungsgemeinschaft pa 

hebb organization behavior 
neuropsychological theory wiley new york 

braitenberg theoretical approaches complex systems eds 
heim palm springer berlin 

palm neural assemblies 
alternative approach artificial intelligence springer berlin 

palm brain theory ed 
elsevier amsterdam 

kohonen self organization associative memory springer berlin 

palm sommer network 

palm psychophysiology 

willshaw buneman longuet higgins nature 

hopfield pnas 

amit phys rev 

amit treves pnas 

parisi phys math gen 

braitenberg anatomy cortex 
statistics geometry springer berlin 

horn usher neural comp 


gerstner van hemmen network 

physiology synapses springer berlin 

wong physica 

palm comp 


palm real brains artificial minds eds 
elsevier amsterdam 

erb palm physica 
iterative retrieval associative memories threshold control different neural models 
gerstner ritz van hemmen biol cybern 


gerstner phys rev 

eckhorn bauer jordan kruse munk biol cybern 


gray konig engel singer nature 

eckhorn bauer 
