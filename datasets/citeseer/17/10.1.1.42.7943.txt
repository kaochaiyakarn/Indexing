weight adjustment schemes centroid classifier shankar george karypis university minnesota department computer science minneapolis mn technical report tr shankar karypis cs umn edu years seen tremendous growth volume text documents available internet digital libraries news sources wide intra nets 
automatic text categorization task assigning text documents pre specified classes topics themes documents important task help organizing finding information huge resources 
similarity categorization algorithms nearest neighbor generalized instance set centroid classification shown effective document categorization 
major drawback algorithms features computing similarities 
document data sets small number total vocabulary may useful categorizing documents 
possible approach overcome problem learn weights different features words document data sets 
report fast iterative feature weight adjustment algorithms centroid classification algorithm 
algorithms measure discriminating power term gradually adjust weights features concurrently 
experimentally evaluate algorithms reuters ohsumed document collections compare rocchio widrow hoff svm 
compared performance terms classification accuracy data sets multiple classes 
data sets compared performance traditional classifiers nn naive bayesian 
experiments show feature weight adjustment improves performance centroid classifier substantially outperforms rocchio widrow hoff competitive svm 
algorithms outperform traditional classifiers nn naive bayesian multi class text document data sets 
supported nsf ccr army research office contract da daag doe asci program army high performance computing research center contract number daah 
access computing facilities provided minnesota supercomputer institute 
related papers available www url www cs umn edu karypis seen tremendous growth volume online text documents available internet digital libraries news sources wide intra nets 
forecasted documents unstructured data predominant data type stored online 
automatic text categorization task assigning text documents pre specified classes documents important task help people find information huge resources :10.1.1.11.6124
text categorization presents huge challenges due large number attributes attribute dependency multi modality large training set 
various document categorization algorithms developed years fall general categories :10.1.1.11.9519:10.1.1.11.6124
category contains traditional machine learning algorithms decision trees rule sets instance classifiers probabilistic classifiers support vector machines directly adapted context document data sets 
second category contains specialized categorization algorithms developed information retrieval community 
examples algorithms include relevance feedback linear classifiers generalized instance set classifiers general class algorithms shown produce document categorization performance similarity 
class contains algorithms nearest neighbor generalized instance set centroid classifiers 
algorithms class new document determined computing similarity test document individual instances aggregates training set determining class class distribution nearest instances aggregates 
major drawback algorithms features computing similarity test document training set instances aggregates 
document data sets relatively small number total features may useful categorizing documents features may affect performance 
possible approach overcome problem learn weights different features words 
approach feature weight associated 
higher weight implies feature important classification 
weights approach feature selection 
refer algorithms feature weight adjustment just weight adjustment techniques 
report presents fast iterative feature weight adjustment algorithms linear complexity centroid classification algorithm 
algorithms measure discriminating power term gradually adjust weights features concurrently 
analysis shows approach gradually eliminates discriminating features document improving classification accuracy 
experimentally evaluate algorithms reuters ohsumed document collection compare performance terms precision recall rocchio widrow hoff support vector machines :10.1.1.11.6124
compared performance terms classification accuracy data sets multiple classes 
data sets described section 
data sets compared performance traditional classifiers nn naive bayesian 
experiments show feature weight adjustment improves performance classifier substantially outperforms rocchio widrow hoff competitive svm 
algorithms outperform traditional classifiers nn naive bayesian text document data sets 
organization report follows 
section describes classification schemes text data section gives brief overview centroid classifier 
section describes weight adjustment schemes discusses computational complexity 
section presents analysis schemes 
section documents results schemes various data sets performance classifiers data sets 
previous linear classifiers linear classifiers family text categorization learning algorithms learn feature weight vector category 
weight learning techniques rocchio widrow hoff algorithm learn feature weight vector training samples 
weight learning algorithms adjust feature weight vector features words contribute significantly categorization large values 
rocchio widrow hoff weight vector classification follows 
test document classified pre defined threshold 
assigned positive class note concept weight vector different rocchio rocchio batch algorithm learn weight vector existing weight vector set training examples 
th component new vector nc nc number training instances set positive training instances nc number positive training instances 
usually rocchio uses positive weights negative weights reset 
widrow hoff widrow hoff algorithm online algorithm runs training examples time updating weight vector 
new weight vector computed follows 
label row negative class positive class 
parameter controls quickly weight vector change influence new example 
may final vector theoretical results suggest better final weight vector average weight vectors computed way 
support vector machines support vector machines svm new learning algorithm proposed vapnik 
algorithm introduced solve class pattern recognition problem structural risk minimization principle 
training set vector space method finds best decision hyper plane separates classes 
quality decision hyper plane determined distance referred margin hyper planes parallel decision hyper plane touch closest data points class 
best decision hyper plane maximum margin 
svm problem solved quadratic programming techniques 
svm extends applicability linearly non separable data sets soft margin hyper planes mapping original data vectors higher dimensional space data points linearly separable 
efficient implementation svm application text categorization reuters corpus reported :10.1.1.11.6124:10.1.1.11.6124
implementation comparison purposes 
nearest neighbor nearest neighbor nn classification instance learning algorithm applied text categorization early days research shown produce better results compared machine learning algorithms ripper :10.1.1.33.4944
classification paradigm nearest neighbors test document computed 
similarities document nearest neighbors aggregated class neighbors test document assigned similar class measured aggregate similarity 
major drawback similarity measure nn uses features equally computing similarities 
lead poor similarity measures classification errors small subset words useful classification 
address problem variety techniques developed adjusting importance various terms supervised setting 
examples techniques include preset weight adjustment mutual information relief variable kernel similarity metric learning 
decision tree widely classification paradigm machine learning data mining 
decision tree model built recursively splitting training set locally optimal criterion records belonging leaf nodes bear class label 
widely decision tree classification algorithm shown produce classification results primarily low dimensional data sets 
unfortunately characteristics document data sets relatively large number features characterize class 
decision tree schemes scenario due overfitting 
fitting occurs number samples relatively small respect number distinguishing words leads large trees limited generalization ability 
results obtained locally modified version algorithm capable handling sparse data sets 
naive bayesian naive bayesian nb algorithm widely document classification shown produce performance 
document naive bayesian algorithm computes posterior probability document belongs different classes assigns class highest posterior probability 
posterior probability class test document computed bayes rule assigned class highest posterior probability class arg max arg max total number classes 
naive bayesian algorithm models document vector term space 
im models presence absence th term 
naive bayesian computes quantities required follows 
approximate class priors computed maximum likelihood estimate set training documents number training documents computed assuming conditioned particular class occurrence particular value statistically independent occurrence value term assumption assumption classifier called naive bayesian 
centroid document classifier centroid classification algorithm documents represented vector space model 
model document considered vector term space 
simplest form document represented term frequency tf vector tf tf tf 
tf tf frequency th term document 
widely refinement model weight term inverse document frequency idf document collection 
motivation weighting terms appearing frequently documents limited discrimination power reason need de emphasized 
commonly done multiplying frequency term log df total number documents collection df number documents contain th term document frequency 
leads tf idf representation document tfidf tf log df tf log df 
tf log df 
order account documents different lengths length document vector normalized unit length tfidf 
rest assume vector representation document weighted tf idf normalized unit length 
vector space model similarity documents commonly measured cosine function cos denotes dot product vectors 
document vectors unit length formula simplifies cos set documents corresponding vector representations define centroid vector vector obtained averaging weights various terms documents refer supporting set centroid analogously documents similarity centroid vectors document centroid vector computed cosine measure 
case cos second case cos dk ck ck 
note document vectors length centroid vectors necessarily unit length 
idea centroid classification algorithm extremely simple 
set documents belonging class compute centroid vectors 
classes training set leads centroid vectors 
centroid th class 
class new document determined follows 
document frequencies various terms computed training set compute tf idf weighted vector space representation scale unit length 
compute similarity centroids cosine measure 
similarities assign class corresponding similar centroid 
class arg max cos ex 
computational complexity learning phase centroid classifier linear number documents number terms training set 
computation vector space representation documents easily computed performing passes training set 
similarly centroids computed single pass training set centroid computed averaging documents corresponding class 
amount time required classify new document km number terms computational complexity algorithm low identical fast document classifiers naive bayesian 
weight adjustment centroid classifier section algorithms improve classification performance achieved centroid classifier adjusting weight various features 
rest section iterative algorithms discuss improve performance case binary classification 
approach weight adjustment feature selection choose small number features adjust weights order improve classification accuracy 
approach shown achieve poor results text domain number important features usually quite large 
mind approach performs simultaneous weight adjustment features 
fixed weight adjustment schemes scheme adjusts weights various features terms perform tasks 
rank various features discriminating power 
second adjust weight various features order emphasize features high discriminating power de emphasize features limited discriminating power 
years number schemes developed measure discriminating power various features information gain entropy gini index statistic 
algorithm discriminating power feature computed measure similar gini index follows 
number different classes 
cm centroid vectors classes 
term 
cm vector derived weight th term centroids norm scaled version discriminating power th term square length vector 
note value range 
lowest value achieved term equally distributed classes highest achieved th term occurs single class 
value close indicates term high discriminating power value close indicates terms little discriminating power 
rest refer purity th term refer vector 
terms purity vector 
having ranked various terms purity measure discriminating power step adjust weights terms higher discriminating power important terms lower discriminating power 
simple way doing scale terms purity 
particular document vector transformed new vector 


set transformed document vectors centroid classification algorithm proceed scale document unit length build new set centroid vectors various classes 
new document classified scaling terms purity vector computing similarity new set centroids 
purity values equal weight various terms transformed document equal smaller original weights 
discussed section renormalization operation performed centroid classification algorithm causes purest terms document gain weight achieving desired feature weight adjustments 
unfortunately simple scheme drawbacks 
weight adjustment approach may cause steep change weights terms 
happens weight document tends get concentrated small number terms 
result loss information negatively affect classification performance 
second cases simple step processes may sufficiently change weights various terms 
consequently new representation similar original change classification accuracy 
reason weight adjustment algorithm adopts somewhat different approach attempts address problems 
algorithms solve problem changing weights various features smaller factor indicated purity 
particular term scale weight 
closer leading smaller changes 
address second problem perform weight adjustment operation multiple times 
data set classification accuracy portion training set validation set order determine times perform weight adjustment 
weight adjustment process stopped classification performance validation set starts decrease 
details algorithm shown 
number weight adjustment iterations computed new test document classified adjusting weights terms going sequence weight adjustment iterations 
split training set training validation 
compute accuracy centroid classifier built 
compute documents 

document 
term 

scale 
compute accuracy centroid classifier built 
accuracy decrease 

goto fixed weight adjustment algorithm 
centroid classification algorithm weight adjusted training set determine class 
process speeded fact applying iterations weight adjustment followed unit length scaling applying single weight adjustment weight term multiplied followed single unit length scaling see appendix proof 
computational complexity algorithm reasonably efficient practical significance 
major advantages centroid classifier linear time classifier outperforms complex algorithms 
section discuss effect feature weight adjustment algorithm computational complexity centroid algorithm 
fixed feature weight adjustment scheme centroid algorithms iterate document term matrix 
matrix sparse matrix usually stored sparse representation 
representations space time complexities nnz nnz number non zeros document term matrix 
representation document term matrix assumed analysis 
fixed feature weight adjustment step classifier learning stage consists steps 
step optimum number iterations determined 
step weight adjustment applied documents documents classified 
applying weights document normalizing linear number terms document classifying document 
iteration nnz 
reasonable assume determining nnz 
experimental observation indicate small constant usually 
assuming constant complexity step rewritten nnz 
second step consists applying weight vector times complete training set 
discussed previous section done iteration document term matrix 
optimization means second step complexity nnz 
final step consists computing centroids transformed data set 
complexity step shown nnz 
putting complexities steps complexity learning phase nnz 
classifying feature weight vector applied tf idf representation test document produce transformed test vector step ex ex number terms document 
classifying document centroid algorithm complexity ex number classes number centroids 
fixed feature weight adjustment affect linear time complexity centroid scheme 
important speed algorithm primary advantages 
variable weight adjustment algorithm section feature weight vector computed original document term matrix 
application weight vector matrix changes 
centroid purity information change result 
section algorithm uses changed purity information recalculate weight vector iteration 
weight vector changes iteration algorithm called variable weight feature adjustment algorithm short 
algorithm summarized 
split training set training validation 
compute accuracy centroid classifier built 

compute documents 
document 
term 

scale 
compute accuracy centroid classifier built 
accuracy decrease 

goto variable weight adjustment algorithm 
test document tf idf representation 
compute transformed test document 
represents test document transformed space computing similarity centroids 
weight vector computed step learning phase added complexity 



result applying weight multiplying term re normalizing associative 
see appendix track vector single vector initialized updating computing step 
computational complexity algorithm described modification algorithm section analysis analysis section 
main difference algorithms optimum number iterations determined algorithm training set weighted iteration 
algorithm weights recomputed iteration iterations need performed 
result complexity step nnz nnz 
assumption constant revised algorithm affect linear time complexity centroid algorithm 
computing transformed test complexity order number terms test document 
affect time complexity classification phase centroid classifier remains ex 
binary classification common classification problem information retrieval developing classifier correctly identify documents belong particular target class large collection documents 
typical binary classification problem try develop model target class versus rest 
weight adjustment scheme described previous sections directly kind problems 
centroid scheme best class contains related documents 
negative class rest diffuse 
result instances negative class tend get classified positive 
propose solution handle 
cluster negative set clusters 
computing centroids purity treat class problem 
classifying compute similarity document negative clusters 
take largest value treat similarity document negative set 
similarity document positive set directly computed centroid positive class 
similarly find class multi modal distribution run clustering algorithm identify sub classes 
treat sub classes separate class 
analysis model weight adjustment process affects documents 
consider terms document sorted decreasing purity 
apply weight adjustment scheme document re normalize term gain weight terms having purity lose weight purer terms 
consider initial document vector 

apply weight vector 

assume loss generality 

new document vector 
rewrite 
know 
denominator greater loses weight 
hand denominator weights equal term gain weight 
assume weights remain constant iteration 
impure term may gain weight due presence terms lesser purity 
iterations performed weight transfer process causes terms lesser weight reduces weight transfer higher terms 
result process initially terms having lowest purity document lose weight 
lose weight terms pure longer able compensate loss weight terms start losing weight 
weight term curve looks 
term having low purity show initial increase purest term exhibit final falling part 
figures shows change weight terms different document iterations 
column weight number iterations assume document model 
consider classes 
class specialized vocabulary addition general vocabulary model illustrated 
assume general case 
consider set words 

reasonable assumption words belonging class affinity particular class 
terms purity close perform weight adjustment process terms tend go zero 
terms discriminant ability loss information weight transfer terms 
terms tend go zero occur equally classes 
process removes terms increasing order discriminating ability 
process stopped new representation starts losing terms important discriminating classes 
validation portion training set needed 
algorithm section updates purity vector iteration 
analysis complicated case 
time weight vector applied change weights terms centroids related term dependencies 
consider terms perfectly correlated 
mean tf idf values document vector centroid values 
apply weight vector re normalize know class class vocabulary vocabulary general vocabulary class document model weight transfer term vice versa 
easy see weight transfer canceled weight transfer correlated net transfer 
term dependencies affect weight transfer document result affect weights centroids 
recomputing purity iteration uses changed weights 
hypothesize allows scheme capture information dependencies caused weight change place 
experimental setup results section experimentally evaluate effect feature weight adjustment schemes classification accuracy centroid classifier 
different sets experiments 
experiments focus evaluating accuracy classifier data sets multiple labels document considering binary classification class 
third evaluates classifier way classifier 
experiments compared performance weight adjustment schemes performance achieved classifiers 
obtained results linear classifiers rocchio respectively learn weight vectors 
case rocchio wh initial vector 

ran svm lite polynomial kernel rbf kernel data sets :10.1.1.11.6124
feature weight adjustment schemes 
classifiers include nearest neighbor naive bayesian classifier 
naive bayesian rainbow multinomial event model 
feature weighting schemes run 
classifiers naive bayesian run tf idf representation documents 
run boolean representation particular entry term occurs document reuters data set reuters text collection 
particular modapte split divide text collection set training documents test documents 
eliminating words removing terms occur times training corpus contains distinct terms 
table shows performance classifiers frequent classes reuters data set 
columns labeled rocchio wh shows performance achieved linear classifier rocchio algorithms respectively learn weights 
columns show performance svm classifier degree polynomial kernel rbf kernel 
fifth column labeled centroid shows performance centroid classifier 
column shows performance centroid classifier feature weights adjusted fixed feature weight adjustment variable feature weight adjustment algorithms respectively 
note algorithms run clustering 
table shows micro average classes micro average top classes macro average top classes 
rocchio wh svm poly svm rbf centroid earn acq money fx grain crude trade interest wheat ship corn micro average top micro average average top table precision recall break points reuters number interesting observations results table 
comparing rocchio widrow hoff basic centroid scheme fastest schemes see centroid scheme performs substantially better rest followed wh rocchio 
top categories centroid scheme best wh dominating remaining 
second see weight adjustment schemes improve performance centroid classifier dramatically 
weight adjustment schemes achieve level performance 
third svm rbf kernel winner doing better schemes 
addition tested effect clustering negative set described section 
results table clusters 
seen clustering dramatic improvement performance scheme 
number clusters slightly affects performance clusters gives best results 
comparing results clustering svm results see svm poly scheme micro average percent weight adjustment 
svm rbf better 
weight adjustment schemes dominate svm rbf top classes 
weight adjustment schemes achieve performance 
earn acq money fx grain crude trade interest wheat ship corn micro average top micro average average top table effect clustering ohsumed results table gives data table ohsumed data set 
ohsumed data documents id title title 
classification task considered assign document multiple categories mesh diseases categories 
entries data set 
training set remaining formed test set 
total documents class label assigned 
documents belonged multiple classes 
comparing rocchio widrow hoff centroid scheme see centroid scheme performs best data set 
surprisingly widrow hoff poor performance data set dominated completely rocchio performing better categories 
centroid scheme dominates categories 
weight adjustment schemes achieve similar performances improve accuracy basic centroid scheme 
clustering schemes achieve higher micro average svm poly scheme perform better classes 
svm rbf performs best 
achieves micro average higher scheme performs better categories 
results weight adjustment schemes clustering shown table 
clustering effect accuracy 
fact cases reduces accuracy 
clusters gives best result data set results lower svm rbf scheme 
interesting result table different levels clustering improves accuracy data sets example clustering gives best results classes clusters gives best result classes 
similarly trends seen classes 
multi class results final set results centroid classifier way classifier 
table shows performance various classifiers 
column nb shows performance naive bayesian classifier 
shows performance standard decision tree classifier knn gives performance nearest neighbor algorithm 
wh svm poly svm rbf centroid average micro average table ohsumed results detailed characteristics various document collections experiments available note data sets list remove common words words stemmed porter suffix stripping algorithm 
furthermore selected documents document class label 
words set classes collected documents class set 
data sets west west west collections legal document publishing division west group described 
data sets tr tr tr tr tr tr tr fbis la la la new derived trec trec trec collections 
data sets re re reuters text categorization test collection distribution 
removed dominant classes earn acq shown relatively easy classify 
divided remaining classes sets 
data sets oh oh oh oh ohsumed collection subset medline database 
data set wap webace project wap 
document corresponds web page listed subject hierarchy yahoo 

table shows performance weight adjustment scheme way classifier 
accuracy measured performing split data cross validation 
numbers bold faces indicate winning algorithms particular data set 
centroid schemes dominate classes compare nn naive bayesian see worst followed nn naive bayesian performs best 
basic centroid scheme dominates weight adjustment tends improve performance 
data sets available www cs umn edu han data tar gz 
average micro average table ohsumed clustering data sets seen scheme appreciably better scheme 
believe scheme mechanisms handling term dependencies change centroid weights resulting change 
results indicate 
part reason purity changes relatively slowly mechanism dampening weight transfer 
number iterations small data sets means changes purity added probably substantial difference classification accuracy 
efficiency advantages weight adjusted centroid scheme speed 
discussed section model learning time linear number non zero terms document term matrix classification time linear number classes 
comparison running time performed svm lite code polynomial kernel rbf kernel centroid scheme reported table :10.1.1.11.6124
times obtained mhz workstation running redhat gig memory similar load conditions 
looking table see times faster svm rbf learning phase times faster classification phase 
nb knn centroid west west west oh oh oh oh re re tr tr tr tr tr tr tr la la la fbis wap new table classification accuracy report showed weight adjustment scheme improves accuracy centroid classifier 
scheme retains power centroid classifier enhancing ability 
retains speed original scheme 
terms clustering shown useful improving accuracy scheme 
clustering needed order handle multi modal distributions scheme handle current form 
automatically determining class needs clustered clusters divided interesting problem 
stands analysis algorithm incomplete 
beneficial rigorous analysis scheme strengths weaknesses 
svm poly svm rbf clusters learn classify learn classify learn classify earn acq money fx grain crude trade interest wheat ship corn table run time comparison times seconds amin shekhar 
generalization neural networks 
proc 
th int conf 
data eng april 
baker mccallum 
distributional clustering words text classification 
sigir 
boley gini gross han hastings karypis kumar mobasher moore 
document categorization query generation world wide web webace 
ai review accepted publication 
cohen 
fast effective rule induction 
proc 
twelfth international conference machine learning 
cohen hirsh 
joins generalize text classification whirl 
proc 
fourth int conference knowledge discovery data mining 
cortes vapnik 
support vector networks 
machine learning 
curran thompson 
automatic categorization documents 
proc 
th asis sig cr classification research workshop tucson arizona 
spiegelhalter michie taylor 
machine learning neural statistical classification 
ellis horwood 
daelemans 
learnability data driven acquisition stress 
technical report tr institute language technology artificial intelligence tilburg university netherlands 
duda hart 
pattern classification scene analysis 
john wiley sons 
goldberg 
genetic algorithms search optimizations machine learning 
morgan kaufman 
han karypis 
centroid document classification algorithms analysis experimental results 
technical report tr department computer science university minnesota minneapolis 
available www url www cs umn edu karypis 
hong han 
text categorization weight adjusted nearest neighbor classification 
phd thesis university minnesota october 
hersh buckley leone 
ohsumed interactive retrieval evaluation new large test collection research 
sigir pages 
iwayama tokunaga 
cluster text categorization comparison category search strategies 
sigir pages 
joachims :10.1.1.11.6124
text categorization support vector machines learning relevant features 
proc 
european conference machine learning 
kira rendell 
practical approach feature selection 
proc 
th international conference machine learning 
kononenko 
estimating attributes analysis extensions relief 
proc 
european conference machine learning 
wai lam chao yang ho 
generalized instance set automatic text categorization 
sigir 
larsen aone 
fast effective text mining linear time document clustering 
proc 
fifth acm sigkdd int conference knowledge discovery data mining pages 
lewis 
naive bayes independence assumption information retrieval 
tenth european conference machine learning 
lewis gale 
sequential algorithm training text classifiers 
sigir 
lewis ringuette 
comparison learning algorithms text categorization 
proc 
third annual symposium document analysis information retrieval 
lewis 
reuters text categorization test collection distribution 
www research att com lewis 
david lewis robert shapire james callan ron papka 
training algorithms linear text classifiers 
proceedings th annual international acm sigir conference research development information retrieval pages pages 
lowe 
similarity metric learning variable kernel classifier 
neural computation pages january 
masand waltz 
classifying news stories memory reasoning 
sigir pages 
mccallum nigam 
comparison event models naive bayes text classification 
aaai workshop learning text categorization 
andrew mccallum 
bow toolkit statistical language modeling text retrieval classification clustering 
www cs cmu edu mccallum bow 
porter 
algorithm suffix stripping 
program 
ross quinlan 
programs machine learning 
morgan kaufmann san mateo ca 
jr rocchio 
smart retrieval system experiments automatic document processing 
gerard salton editor relevance feedback information retrieval 
prentice hall 
salton 
automatic text processing transformation analysis retrieval information computer 
addisonwesley 
trec 
text retrieval conference 
trec nist gov 
nature statistical learning theory 
springer 
weiss kulikowski 
computer systems learn classification prediction methods statistics neural nets machine learning expert systems 
morgan kaufmann san mateo ca 
wettschereck aha mohri 
review empirical evaluation feature weighting methods class lazy learning algorithms 
ai review 
wettschereck dietterich 
experimental comparison nearest neighbor nearest hyperrectangle algorithms 
machine learning 
widrow stearns 
adaptive signal processing 
hall 
yahoo 
yahoo 
www yahoo com 
yang 
expert network effective efficient learning human decisions text categorization retrieval 
sigir 
yang liu 
re examination text categorization methods 
sigir 
yang pederson 
comparative study feature selection text categorization 
proc 
fourteenth international conference machine learning 
associativity weighing wish show applying weight vector 
iteration normalizing vector iteration iterations applying vector 
normalizing 
shown induction 
consider vector 

applying weight vector normalizing gives new vector 

base case 
assume applications gives vector 

applying weight vector 
gives 

magnitude vector normalized vector obtained dividing vector magnitude gives 

having shown statement true true true statement true optimization discussed section special case 

