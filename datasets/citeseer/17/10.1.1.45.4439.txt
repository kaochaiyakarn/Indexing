hq learning discovering markovian subgoals non markovian reinforcement learning technical report idsia marco wiering jurgen schmidhuber idsia corso ch lugano switzerland marco juergen idsia ch www idsia ch october solve partially observable markov decision problems introduce hq learning hierarchical extension learning 
hq learning ordered sequence learning identify solve markovian subtask total task 
agent learns appropriate subgoal intermediate external reinforcement subgoals markovian policy particular subgoal 
experiments demonstrate system easily solve tasks standard learning solve 
solve partially observable mazes states previous pomdp 
quickly solve complex tasks require manipulation environment free blocked path goal 
keywords reinforcement learning hierarchical learning pomdps non markovian interfaces subgoal learning 
problem 
learner optimal action depends current input speak markovian interface learner environment 
widely reinforcement learning rl algorithms td sutton learning watkins watkins dayan depend markovian interfaces fail problem requires memory previous events 
non markovian interfaces common real world markovian environments may appear non markovian learner due lack perfect information current environmental state 
problem controlling system partially observable environments cast partially observable markov decision problem pomdp framework 
number algorithms pomdps schmidhuber mccallum ring kaelbling 
jaakkola 
littman feasible small problems 
presents novel quite different approach appears scale far reasonably 
alternative approaches larger scale pomdps see schmidhuber 
wiering schmidhuber zhao schmidhuber 
basic idea 
realistic environments memory previous events required select optimal action 
necessary memorize entire past general quite infeasible memories corresponding important previously achieved subgoals sufficient 
instance suppose instructions way station follow road traffic light turn left follow road traffic light turn right 
way memories corresponding important subgoals relevant passed traffic light 
subgoals reactive memory independent strategy carry safely 
idea incorporated hq learning novel hierarchical extension watkins learning 
hq learning conquer strategy learns subgoals decompose possibly non markovian task simpler markovian subtasks 
system uses multiple 
agent policy mapping states actions 
time system type short term memory embodied pointer indicating agent active 
agent learns context specific strategy solving subgoal 
policies different agents combined way learned agents 
active agent uses subgoal table hq table generate subgoal instance subgoals represented desired inputs 
follows policy embodied table achieves subgoal 
control passed agent procedure repeats 
goal achieved time limit exceeded agent uses novel learning procedure described section adjust policy subgoal 
agent learns markovian subproblems system learn non markovian tasks impossible learn single lookup tables 
singh system lin hierarchical learning method depend external teacher provides priori information subtasks 
jaakkola method limited finding suboptimal stochastic policies pomdps optimal deterministic solution 
outline 
section describes hq learning details including learning rules hq tables 
section describes experiments relatively complex partially observable mazes 
experiment system solves pomdp standard learning solve automatically decomposing appropriate markovian subtasks 
second experiment solves complex pomdp world states requires finding key open door blocking path goal 
section briefly reviews related mainly tested small problems previous methods scale littman 
section concludes lists directions research 
hq learning pomdp specification 
system life separable trials 
trial consists tmax discrete time steps pomdp specified 
fl finite set environmental states initial state finite set observations function maps states ambiguous observations finite set actions theta ir maps state action pairs scalar reinforcement signals fl discount factor trades immediate rewards rewards theta state transition function 
framework extended non deterministic worlds focus deterministic state transition functions simplicity environmental state time action executed time system goal obtain maximal discounted cumulative reinforcement trial 
architecture 
ordered sequence agents 
cm equipped table hq table transfer control unit cm table see 
agent responsible learning part system policy 
table represents local policy executing action input 
matrix size joj theta jaj joj number different possible observations jaj number possible actions 
denotes value utility action observation agent current subgoal generated help hq table vector joj elements 
hq denotes hq value utility selecting subgoal 
possible observation hq table entry representing estimated value subgoal 
system current policy policy currently active agent 
active time step denote active information agent active represents kind short term memory system 
transfer control transfer control table table table agent agent agent hq table hq table basic architecture 
agents connected sequential way 
agent table hq table transfer control unit agent table 
table stores estimates actual observation action values select action 
hq table stores estimated subgoal values generate subgoal agent active 
solid box indicates second agent currently active agent 
agent achieved subgoal transfer control unit passes control successor 
selecting subgoal 
active 
active hq table select subgoal subgoal chosen probability hq max uniform distribution hq max max ok max gamma max joj max denotes probability max choice rule chooses subgoal maximal hq value subgoal selection 
max returns hq hq 
denotes subgoal selected agent subgoal transfer control defined confused observation 
selecting action 
action choice depends current observation learning time active agent select action probability max boltzmann distribution max max ak max gamma max ak ak function max returns 
temperature adjusts degree randomness involved agent action selection long max 
max boltzmann max uniform distribution values hq values 
distributions prevent exploration policies unstable discussions exploration issues see fedorov schmidhuber thrun cohn dorigo wilson 
distributions easy reduce relative weight exploration opposed exploitation 
learning increase max obtain deterministic policy learning process 
schraudolph 
mixture distributions train td go networks schraudolph personal communication 
transfer control 
transfer control active agent implemented follows 
time executed action transfer control unit checks reached goal 
checks solved subgoal decide control passed denote time agent active system start set 
absorbing state reached current subgoal active active active learning rules line learning updating tables intra trial changes 
learning rules appear similar conventional learning 
major difference agent prospects achieving subgoal tend vary various agents try various subgoals 
learning values 
want approximate system expected discounted reward executing action optimal case jo theta active jo theta denotes probability system state time observation architecture parameters denoted theta information active 
hq learning depend estimating probability world model help speed learning moore 
utility observation agent equal value best action maxa value updates generated different cases tmax denotes total number executed actions current trial ff learning rate active time active time gamma ff ff 
agent active time final action executed ot gamma ff ot ff st 
mentioned update rules resemble normal step learning 
main difference agents trained values see 
learning hq values 
want hq values hq converge expected discounted cumulative reinforcement subgoal current system policy 
optimal case hq fl gammat hv gamma fl gammat discounted cumulative reinforcement time active note time interval states encountered depend subtask hv estimated discounted cumulative reinforcement received trial adjust hq values agents active trial 
hq table updates resemble table updates ff hq denotes learning rate chosen subgoal agent hq invoked agent cn gamma update hq gamma ff hq hq ff hq fl gammat hv 
hq cn gamma hq gamma ff hq hq ff hq fl gammat rn 
hq cn hq gamma ff hq hq ff hq third rules resemble traditional learning rules 
second rule necessary agent cn learned possibly high value subgoal unachievable due subgoals selected previous agents 
comment 
tables hq tables explicitly communicate influence 
results complex dynamics quite different conventional learning 
td modification 
speed learning may td method modify learning rules manner analogous lin 
changes update details follows tables compute desired values ot st fl gamma active update values qn gamma ff ff hq hq tables compute desired hq values hq hq rn hq gamma gamma rn gamma fl gammat rn hq fl gammat gamma hv hq hq update hq values agents min gamma hq gamma ff hq hq ff hq hq experiments test system tasks involving non markovian interfaces learner environment 
task find path start goal partially observable theta maze 
pomdp collectively solved markovian agents 
study system performance agents added 
second quite complex task involves finding key opens door blocking path goal 
optimal solution requires markovian agents takes steps 
learning solve partially observable maze task 
experiment involves partially observable maze shown 
system discover path leading start position goal actions obvious semantics go west go north go east go south 
possible observations agent see adjacent fields blocked 
possible agent positions highly ambiguous inputs 
possible observations occur maze means system may occasionally generate unsolvable subgoals control transferred agent 
deterministic policy solving task 
instance input stands fields left right agents agents agents agents agents optimal agents agents agents optimal partially observable maze pom 
task find path leading start goal possible agent positions different highly ambiguous inputs kind memory necessary disambiguate 
conventional learning fails solve problem 
optimal solution requires steps markovian agents 
shows possible solution costs steps 
asterisks mark appropriate markovian subgoals 
hq learning results partially observable maze agents 
plot average test run length trial numbers means simulations 
system converges near optimal solutions 
required agents tends improve performance 
results agents actions corrupted noise 
cases find goal noisy actions decrease performance 
agent blocked 
optimal action response input depends subtask trial go north go south near go north 
markovian agents necessary solve pomdp 
reward function 
system hits goal receives reward 
reward zero 
discount factor fl 
parameters experimental set 
compare systems agents noise free actions 
compare systems agents actions selected learning testing replaced random actions probability 
experiment consists simulations system 
simulation consists trials 
tmax 
th trial test run actions subgoals maximal table entries selected max set 
system find goal test run trial outcome counted steps 
coarse search parameter space parameters experiments ff ff hq hq tables tables 
max set linearly increased 
table entries initialized 
purposes comparison ran trials actions picked randomly 
results 
plots average test run length trial numbers 
trials systems find near optimal deterministic policies 
learning fails miserably course 
consider table 
largest systems able decompose pomdp markovian subtasks 
average number steps close optimal 
approximately cases optimal path 
cases step solutions 
number step solutions larger number step solutions possible subgoal sequences result surprising 
systems agents performing better system profits having free parameters 
agents don help 
systems perform significantly system av 
steps goal optimal agents agents agents agents agents agents noise agents noise agents noise random table hq learning results random actions replacing selected actions probability 
nd column lists average numbers steps required find goal 
rd column lists numbers simulations goal final trial 
th column lists numbers simulations optimal path final trial 
better random system finds goal step trials 
case noisy actions probability replacing selected action random action systems reach goal simulations see 
final trial simulation systems agents find goal probability 
significant difference smaller larger systems 
studied system adds agents learning process 
agent system solutions agents simulations 
agents tends things easier 
trials agents average 
final trials agents average 
agents tend lead better results 

systems fail solve task subgoals start subgoals successful 
subgoals possibilities compose paths lower probability finding shortest path maze 
key door task 
second experiment involves theta maze shown 
starting system fetch key position move door shaded area normally behaves wall open disappear agent possession key proceed goal different highly ambiguous inputs task difficult pomdp 
optimal path takes steps 
reward function 
system hits goal receives reward 
actions reward 
additional intermediate reward key going door 
discount factor fl 
parameters 
experimental set analogous section 
systems agents systems agents actions corrupted different amounts noise 
ff ff hq 
max linearly increased 
parameters section 
simulation consists trials 
results 
ran step trials system executing random actions 
goal 
ran random system step trials 
shortest path took steps 
observe finding goal negative reinforcement signals extremely difficult 
table show hq learning results noise free actions 
trials deterministic policies simulations 
optimal step paths agents simulations 
agents agents agents agents optimal noise noise noise optimal partially observable maze containing key door grey area 
starting system find key open door proceed goal shortest path costs steps 
optimal solution requires markovian agents 
number possible world states higher pomdps studied authors 
hq learning results key door problem 
plot average test run length trial number means simulations 
trials systems agents find deterministic policies simulations 
hq learning results agent system actions replaced random actions probability 
random actions taken cases agent system finds goal final trials see table 
cases long paths steps 
best solutions steps 
interestingly little noise decrease performance noise lead worse results 
know due hq learning task set 
system av 
steps goal optimal agents agents agents agents agents noise agents noise agents noise random table results hq learning simulations key door task 
second column lists average numbers steps required find goal 
third lists numbers simulations goal final trial 
fourth lists numbers simulations optimal path steps final trial 
hq learning solve task limit steps trial 
random search needed step limit 
previous authors proposed hierarchical reinforcement learning techniques improve performance markov decision problems mdps dayan hinton moore tham 
methods markov assumption 
focus pomdps section limited brief summary previous pomdp approaches specific advantages disadvantages 
recurrent neural networks 
interacting gradient recurrent networks 
model network serves model predict environment uses model net compute gradients maximizing reinforcement predicted model schmidhuber extending ideas nguyen widrow jordan rumelhart 
knowledge presents successful reinforcement learning application simple nonmarkovian tasks learning flipflop 
lin uses combinations controllers recurrent nets 
compares time delay neural networks recurrent neural networks 
despite theoretical power standard recurrent nets run practical problems case long time lags relevant input events 
approaches overcome problem hochreiter schmidhuber reinforcement learning applications 
belief vectors kaelbling 
hierarchically build policy trees calculate optimal policies stochastic partially observable environments 
possible environmental state belief vector represents agent estimate probability currently state 
belief vector updated observation 
operation research algorithms compute optimal actions dynamic programming 
problems approach nature underlying mdp needs known computationally expensive 
mccallum utile distinction memory combines hidden markov models hmms learning 
able solve simple pomdps maze tasks fields splitting inconsistent hmm states agent fails predict utilities experiences quite different returns states 
problem approach solve problems conjunctions successive perceptions useful predicting reward independent perceptions irrelevant 
hq learning problem deals perceptive conjunctions multiple agents necessary 
littman 
compare different pomdp algorithms belief vectors 
report small pomdps states actions pose big problem methods 
larger pomdps states cause major problems 
indicates problems current involve states hardly solved methods 
hq learning contrast computationally complex requires knowledge underlying mdp 
absence prior knowledge significant advantage 
memory bits 
littman uses branch bound heuristics find suboptimal memoryless policies extremely quickly 
deal mazes safe deterministic memoryless policy replaces conventional action actions having additional effect switching memory bit 
results obtained toy problem 
method scale due search space explosion caused adding memory bits 
contrast hq learning depend finding optimal memory bit settings branch techniques uses incremental learning method 
cliff ross describe classifier system pomdps trained bucket brigade genetic algorithms 
memory bits set reset actions 
system reported small problems unstable case memory bit 
usually able find optimal deterministic policies 
wilson described sophisticated classifier system uses prediction accuracy calculating fitness genetic algorithm working environmental niches 
study shows classifiers general accurate 
interesting test system memory solving pomdps 
problem memory bits tried tasks section require long traces memory bit resets 
memory bits critical turned precisely right moment 
instance suppose probability turning memory bit response particular observation low agent observation 
eventually memory bit won remain switched 
learning example tends fail reliably set memory bits learning values changing bit depends luck kaelbling personal communication 
hq learning depend long traces memory bit resets 
memory embodied solely active agent number rarely incremented trial 
stable 
multiple learners 
hq learning learning uses multiple qlearning agents 
major difference agents skills different agents focus different input features receive different rewards 
reward functions genetic algorithms 
important goal learn agent select part input space 
different learning methods implementing cooperative competitive strategies tested complex dynamic environment lead reasonable results 
possibly learning hq learning combined advantageous way 
describes nested learning technique multiple agents learning independent reusable skills 
generate quite arbitrary control hierarchies simple actions skills composed form complex skills 
learning rules selecting skills selecting actions 
may hard deal long reinforcement delays 
experiments system reliably learns solve small maze task 
remains seen system reliably learn decompose solutions complex problems stable skills 
learning control hierarchies 
ring system constructs bottom control hierarchy 
lowest level nodes primitive perceptual control actions 
nodes higher levels represent sequences lower level nodes 
disambiguate inconsistent states new higher level nodes added incorporate information hidden deeper past necessary 
system able quickly learn certain non markovian maze problems able generalize previous experience additional learning optimal policies old new task identical 
hq learning reuse policy generalize previous similar problems 
mccallum tree quite similar ring system 
uses prediction suffix trees branches reflect decisions current previous inputs actions 
values stored leaves correspond clusters instances collected stored entire learning phase 
statistical tests decide instances cluster correspond significantly different utility estimates 
cluster split 
method may viewed decision tree reinforcement learning additions 
mccallum experiments demonstrate algorithm ability improve comparatively large state spaces 
problem depends creation th order markov model size time window sampling observations 
large approach suffer curse dimensionality 
consistent representations 
whitehead uses consistent representation cr method deal inconsistent internal states result perceptual aliasing due ambiguous input information 
cr uses identification stage execute perceptual actions collect information needed define consistent internal state 
consistent internal state identified single action generated maximize discounted reward 
identifier controller adaptive 
limitation method agent access external markov model necessary hq learning 
levin search 
wiering schmidhuber levin search ls program space levin discover programs computing solutions large pomdps 
ls interest amazing theoretical properties broad class search problems optimal order computational complexity 
instance suppose algorithm solves certain type maze task steps positive integer representing problem size 
ls solve task steps 
wiering schmidhuber show ls may substantial advantages reinforcement learning techniques provided algorithmic complexity solutions low 
meta reinforcement learning 
wiering schmidhuber extend ls obtain incremental method generalizing previous experience 
guarantee lifelong history policy changes corresponds lifelong history reinforcement accelerations novel reinforcement learning paradigm called meta reinforcement learning mrl schmidhuber combined ls 
shown lead significant learning speedups 
mrl ls specific general approach allows plugging great variety learning algorithms 
instance additional experiments self referential system embeds policy modifying method policy mrl able solve huge pomdps states schmidhuber 
believe able combine mrl hq learning advantageous way 
summary 
introduced hq learning novel method reinforcement learning partially observable environments 
non markovian tasks automatically decomposed markovian subtasks intermediate external reinforcement subgoals 
done ordered sequence agents discovering local control policy appropriate markovian subgoal 
experiments involve pomdps states pomdps literature 
results demonstrate hq learning ability quickly learn optimal near optimal policies 
believe currently reinforcement learning method solving similar pomdps comparable time 
limitations 
current version restricted linearly ordered subgoal sequences 
complex pomdps generalized hq architectures directed acyclic recurrent graphs may turn useful 
left research 
acknowledgments valuable comments discussions marco dorigo nic schraudolph luca gambardella sa zhao cristina 
dorigo 

training agents 
technical report iridia universit libre de bruxelles 
cliff ross 

adding temporary memory 
adaptive behavior 
cohn 

neural network exploration optimal experiment design 
cowan tesauro alspector editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
dayan hinton 

feudal reinforcement learning 
lippman moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 


emergent hierarchical control structures learning reactive hierarchical relationships reinforcement environments 
maes mataric meyer pollack wilson editors animals animats proceedings fourth international conference simulation adaptive behavior cambridge ma pages 
mit press bradford books 
fedorov 

theory optimal experiments 
academic press 
hochreiter schmidhuber 

long short term memory 
submitted neural computation 


action selection methods reinforcement learning 
maes mataric meyer pollack wilson editors animals animats proceedings fourth international conference simulation adaptive behavior cambridge ma pages 
mit press bradford books 
jaakkola singh jordan 

reinforcement learning algorithm partially observable markov decision problems 
tesauro touretzky leen editors advances neural information processing systems pages 
mit press cambridge ma 
jordan rumelhart 

supervised learning distal teacher 
technical report occasional center cog 
sci massachusetts institute technology 
kaelbling littman cassandra 

planning acting partially observable stochastic domains 
technical report brown university providence ri 
levin 

universal sequential search problems 
problems information transmission 
lin 

reinforcement learning robots neural networks 
phd thesis carnegie mellon university pittsburgh 
littman 

memoryless policies theoretical limitations practical results 
cliff husbands wilson editors proc 
international conference simulation adaptive behavior animals animats pages 
mit press bradford books 
littman cassandra kaelbling 

learning policies partially observable environments scaling 
prieditis russell editors machine learning proceedings twelfth international conference pages 
morgan kaufmann publishers san francisco ca 
mccallum 

overcoming incomplete perception utile distinction memory 
machine learning proceedings tenth international conference 
morgan kaufmann amherst ma 
mccallum 

learning selective attention short term memory sequential tasks 
maes mataric meyer pollack wilson editors animals animats proceedings fourth international conference simulation adaptive behavior cambridge ma pages 
mit press bradford books 
moore atkeson 

prioritized sweeping reinforcement learning data time 
machine learning 
nguyen widrow 

truck backer upper example self learning neural networks 
ieee inns international joint conference neural networks washington volume pages 
ring 

continual learning reinforcement environments 
phd thesis university texas austin austin texas 
schmidhuber 

curious model building control systems 
proc 
international joint conference neural networks singapore volume pages 
ieee 
schmidhuber 

reinforcement learning markovian non markovian environments 
lippman moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
schmidhuber zhao wiering 

simple principles metalearning 
technical report idsia idsia 
schraudolph dayan sejnowski 

temporal difference learning position evaluation game go 
cowan tesauro alspector editors advances neural information processing systems volume pages 
morgan kaufmann san francisco 
singh 

efficient learning multiple task sequences 
moody hanson lippman editors advances neural information processing systems pages san mateo ca 
morgan kaufmann 
hochreiter schmidhuber 

reinforcement driven information acquisition non deterministic environments 
proceedings international conference artificial neural networks paris volume pages 
ec cie paris 
sutton 

learning predict methods temporal differences 
machine learning 
tham 

reinforcement learning multiple tasks hierarchical cmac architecture 
robotics autonomous systems 
thrun 

efficient exploration reinforcement learning 
technical report cmu cs carnegie mellon university 
watkins 

learning delayed rewards 
phd thesis king college london 
watkins dayan 

learning 
machine learning 
whitehead 

reinforcement learning adaptive control perception action 
phd thesis university rochester 
wiering schmidhuber 

solving pomdps levin search eira 
saitta editor machine learning proceedings thirteenth international conference pages 
morgan kaufmann publishers san francisco ca 
wilson 

explore exploit strategies autonomy 
meyer wilson editors proc 
fourth international conference simulation adaptive behavior animals animats pages 
mit press bradford books 
wilson 

generalization 
icml workshop evolutionary computing machine learning 
morgan kaufmann publishers san francisco ca 
zhao schmidhuber 

incremental self improvement life time multi agent reinforcement learning 
maes mataric meyer pollack wilson editors animals animats proceedings fourth international conference simulation adaptive behavior cambridge ma pages 
mit press bradford books 
