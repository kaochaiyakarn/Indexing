self organizing maps clustering visualization arthur austrian research institute artificial intelligence vienna austria arthur ai univie ac 
show number output units selforganizing map som influences applicability clustering visualization 
reviewing appropriate literature theory empirical results demonstrate soms clustering visualization separately simultaneous clustering visualization clustering visualization 
different kinds application som compared statistical approaches 
show som flexible tool various forms explorative data analysis obvious flexibility comes price terms impaired performance 
usage som data mining community covered discussing application data mining tools websom 
self organizing maps som introduced kohonen popular tool range different purposes including clustering visualization high dimensional data spaces 
technique prominent data mining tools algorithms implemented data mining tool heart websom system automatic organization large text document collections see lagus kohonen 
vast literature available concerning soms survey kohonen contains entries far clear apply soms clustering visualization purposes goals relate 
comprehensive monograph kohonen som said project visualize high dimensional data spaces 
fact relation clustering visualization techniques known see balakrishnan kohonen bishop 
theoretical analysis som concentrates issues method convergence commenting som see cottrell survey results 
considerable amount criticism formulated terms empirical theoretical comparison 
balakrishnan waller compare som various clustering algorithms artificial data 
bezdek nikhil compare som principal component analysis sammon mapping series artificial real world data sets 
compares som combined method vector quantization plus sammon mapping codebook multivariate normal data 
empirical studies show som perform equal worse statistical approaches 
exist alternative re formulations original idea soms principled probabilistic frameworks bishop 
bishop criticized soms defining density model optimizing objective error function lack guaranteed convergence property 
albeit wealth done analysing soms considerable amounts criticism formulated missing constructive guidelines clarify soms clustering visualization notions relate context soms 
exactly tries achieve showing number output units som influences applicability clustering visualization 
appropriate literature theory reviewed empirical results compare som statistical approaches 
usage som data mining tools websom discussed 
som clustering ripley clustering algorithms methods divide set observations groups members group alike members different groups groups called clusters 
classical technique achieve grouping lbg algorithm linde cluster solution iteratively improved 
linde noted proposed algorithm similar means approach developed cluster analysis literature starting macqueen 
closely related som online means clustering consisting steps 
initialization number codebook vectors dimensionality vectors number input vectors training sequence fx gamma initial set codebook vectors discrete time coordinate gamma 
fx ng find minimum distortion partition fs ng 
compute usually euclidean distance 

update code book vector minimum distortion gamma ff gamma gamma ff learning parameter defined user 
define replace gamma halt 
go step 
main difference som algorithm fact codebook vectors weight vectors output units ordered line planar grid dimensional output space 
iterative procedure equ 
replaced gamma gamma gamma update computed gives minimum distortion code book vectors neighbourhood line planar grid 
degree neighbourhood amount code book vectors updated gives minimum distortion expressed function decreases distance line planar grid time includes additional learning parameter ff 
degree neighbourhood decreased zero som algorithm equal algorithm 
local convergence guaranteed decreasing ff bottou bengio general proof convergence som nonzero neighbourhood known 
kohonen notes steps som algorithm computed zero neighbourhood order guarantee accurate density approximation input samples 
main problems clustering data decide correct number clusters codebook vectors 
clearly number cluster centers output units equal number clusters data 
duda hart argue compute successive partitions data growing number clusters samples really grouped compact separated clusters expect see error function cluster variance obviously holds average distortion decrease rapidly error functions decrease slowly reach zero comprehensive studies som clustering ability balakrishnan waller soms cluster algorithms set equal number clusters known data 
waller compare som different cluster algorithms artificial data sets 
dimensional soms zero neighbourhood learning consequently soms means clustering perform equally terms data points misclassified better hierarchical cluster methods 
som unsupervised technique built classification number points misclassified wrong cluster center appropriate commonly performance measure cluster procedures true cluster structure known 
members true cluster data space members just cluster obtained partition 
exchanges clusters constitute data points misclassified 
balakrishnan compare som means clustering multivariate normal clustering problems decrease som neighbourhood zero learning 
som performs significantly worse terms data points misclassified additional neighbourhood term tends pull obtained cluster centers away true ones som cluster centers pulled 
kohonen describes effect opposing forces weight vectors output units tend describe density function inputs local interactions output units tend preserve topology 
som simultaneous clustering visualization som just technique cluster data 
appealing property clustering visualization time preserving topological ordering input data reflected ordering codebook vectors cluster centroids dimensional output space 
note order som visualization clustering time necessary number output units equal number clusters data set 
formally topology preserving algorithm transformation phi 
preserves similarities just similarity orderings points input space mapped output space algorithms case number input vectors number output vectors equal transformation phi phi preserves similarities poses strongest possible constraint gamma measure distance 
transformation said produce isometric image 
techniques finding transformations phi various forms multidimensional scaling mds sammon mapping principal component analysis pca see jolliffe som 
sammon mapping doing mds minimizing steepest descent gamma gamma gamma distance output space corresponds distance input space 
som designed heuristically find extremum certain cost energy function theoretical connection mds algorithms remains unclear 
noted som number output vectors limited note mds actual coordinates points input space distances ordering needed 
erwin showed objective function exist som 
number cluster centroids restricted lie planar grid 
restriction entails discretization output space allows different distances theta planar grid gamma different distances theta cluster centroids mapped sammon mapping 
believe existing empirical study som ability doing clustering visualization time compared som combined technique online means clustering plus sammon mapping cluster centroids 
new combined approach consists simply finding set fx ng codebook vectors give minimum distortion partition fs ng algorithm input vectors sammon mapping obtaining dimensional representation minimizing term equ 

contrary som dimensional representation restricted fixed form distances mapped directly correspond original higher dimension 
combined algorithm abbreviated 
proposed similar combined technique difference achieve clustering visualization simultaneously 
empirical comparison done multivariate normal distributions generated procedure standard comparisons cluster algorithms see milligan cooper balakrishnan 
marginal normal distributions gave internal cohesion clusters data lie standard deviations oe 
external isolation defined having dimension non overlapping truncating normal distributions dimension sigma oe defining cluster centroids oe apart 
dimensions clusters allowed overlap setting distance dimension centroids randomly lie sigma oe 
produced data sets number clusters number dimensions 
compared dimensional soms numbers output units set equal numbers clusters known data models corresponding sizes codebooks 
som performed equally recovering structure clusters measured called rand index closely related data points misclassified expected set neighbourhood zero training 
pearson correlation measure topology preserved som 
computed pearson correlation distances input space distances output space possible pairwise comparisons data points 
note som coordinates codebook vectors planar grid compute algorithm preserves distances neighbourhood produce isometric image yield value see bezdek nikhil discussion measures topology preservation 
som performed significantly worse preserving topology obtained correlation som 
direct implication som restriction planar grids described 
nonzero neighbourhood som training warrant significant improvements 
full details study 
som visualization possibility apply som visualization neglecting clustering ability 
necessary try set number output units equal presumed number clusters data 
possible common practice apply som numbers output units multiple number input vectors available training see poverty map example kohonen 
means course soms employing numbers code book vectors comparable multiple number input vectors available visualization purposes 
uses amount codebook vectors input vectors vector quantization codebook vector identical input vectors limit learning 
replaced identical sense terms clustering 
bezdek nikhil empirical study focusing som visualization capability 
compare som principal component analysis sammon mapping artificial data sets different numbers points dimensionality different shapes input distributions anderson iris data 
degree preservation spatial ordering input data measured spearman rank correlation distances points input space distances projections dimensional output space similar pearson correlation described 
traditional techniques preserve distances effectively som performance decreases rapidly increasing dimensionality input data 
study visualization som multivariate data sets described sec 

computed soms consisting theta data sets consisting clusters points theta data sets consisting clusters points code book vectors data sets gave average correlation distances significantly worse error level compared average correlation achieved sammon mapping applied input data directly 
result bezdek nikhil study indicate output units input vectors available really help drawbacks som discretization output space 
rigidity output map clearly visible compares examples output maps fig 

fig 

resulting output representations mapping dimensional clusters sammon mapping left fig 
som right fig 
numbers indicate cluster membership 
som clustering visualization possible application som cluster data visualization 
done visualizing data som output map ones subjective judgement just looking resulting output map counting clusters able see 
reviewing clustering studies employing som quickly shows soms kind clustering visualization 
trying augment cluster visibility som output maps see ultsch bishop 
clear type application soms large amounts output units best suited 
long known clustering community doing clustering visualization bears pitfalls 
shown high probability researcher conclude subset points comprise cluster fact points comprise clusters 
due reduction dimensionality produced mapping output space impairs user ability detect clusters existed space defined original variables 
showed researchers asked determine cluster membership identical dimensional representations inter rater reliability average low 
compares output maps obtained som sammon mapping fig 
clusters clearly visible sammon mapping picture clear som output map 
clusters longer coherent members cluster appear outliers 
data mining applications websom som clustering visualization 
user guide states soms type neural network perform clustering advise reader clustering achieved 
expert training method requires user choose number output units simple training method available automatically chooses parameter 
trained soms function train kohonen simple training method data sets described sec 

data sets data points contained easily separable clusters automatically chose theta grid output units 
means soms simple training method aim clustering simultaneous clustering visualization described secs 
number output units far estimating correct number clusters data 
uses som visualization follow user guide advice soms perform clustering clustering visualization 
websom organizes large collections text documents mapping vectorial representations related word frequencies documents dimensional display som 
example kohonen documents different usenet newsgroups mapped som output units 
clear som clustering visualization huge number output units stands relation assumed number clusters data clusters corresponding different newsgroups 
method ultsch indicate clustering tendency shades gray output grid 
tried notion som data visualization tool concrete showing number output units som influences applicability clustering visualization 
showed number output units set equal number clusters data set som clustering clustering plus simultaneous visualization 
theoretical empirical results clear purposes degree neighbourhood set zero learning som equivalent online means clustering 
empirical results show simultaneous visualization cluster centers output units impaired due som discretization output space 
som visualization clustering visualization number output units order number input vectors multiple 
som visualization ability suffer discretization output space exemplified empirical results 
clustering visualization known literature bears high risk missing true cluster structure 
conclude som flexible tool various forms clustering visualization flexibility comes price terms impaired performance 
concerning som data mining community discussed context websom said tools rely soms ability clustering visualization 
users websom aware possible pitfall missing true cluster structure impaired visualization due discretization output display 
parts done biomed ct project funded ec dg xii 
austrian research institute artificial intelligence supported austrian federal ministry science transport 
author supported doctoral austrian academy sciences 
balakrishnan balakrishnan cooper jacob lewis study classification capabilities neural networks unsupervised learning comparison means clustering psychometrika vol 

bezdek nikhil bezdek nikhil index topological preservation feature extraction pattern recognition vol 
pp 
bishop bishop williams magnification factors som gtm algorithms proc 
workshop self organizing maps helsinki pp 

bishop bishop williams gtm generative topographic mapping neural computation vol 
issue 
bottou bengio bottou bengio convergence properties means algorithms tesauro eds advances neural information processing system mit press cambridge ma pp 
user guide integral solutions limited 
cottrell cottrell fort pages theoretical aspects som algorithm neurocomputing pp 
duda hart duda hart pattern classification scene analysis john wiley sons 
erwin erwin obermayer schulten self organizing maps ordering convergence properties energy functions biological cybernetics 
limitations self organizing maps vector quantization multidimensional scaling mozer eds advances neural information processing systems mit press bradford books pp 
jolliffe jolliffe principal component analysis springer 
kohonen kohonen self organization associative memory springer 
kohonen kohonen self organizing maps springer second extended edition springer series information sciences vol 

kohonen kohonen self organization large document collections state art niklasson eds proceedings th international conference artificial neural networks sweden september springer berlin heidelberg new york tokyo vols pp 
lagus lagus honkela kaski kohonen self organizing maps document collections new approach interactive exploration simoudis han eds kdd proceedings second international conference knowledge discovery data mining aaai press mit press cambridge menlo park pp 
linde linde gray algorithm vector quantizer design ieee transactions communications vol 
com january 
macqueen macqueen methods classification analysis multivariate observations proc 
fifth berkeley symposium math stat 
prob vol 
pp 

evaluating clustering methods psychiatric diagnosis biological psychiatry 
milligan cooper milligan cooper examination procedures determining number clusters data set psychometrika 
ripley ripley pattern recognition neural networks cambridge university press 
sammon sammon nonlinear mapping data structure analysis ieee transactions comp vol 

palm adaptive clustering multidimensional scaling large high dimensional data sets niklasson eds proceedings th international conference artificial neural networks icann springer pp 
risk recognizing clusters distinct classification society bulletin 
ultsch ultsch self organizing neural networks visualization classification opitz eds information classification springer berlin 
waller waller kaiser comparison classification capabilities dimensional kohonen neural network partitioning hierarchical cluster analysis algorithms psychometrika vol 

