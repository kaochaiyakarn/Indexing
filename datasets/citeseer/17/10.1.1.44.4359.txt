compressing relations indexes jonathan goldstein raghu ramakrishnan uri shaft department computer sciences university wisconsin madison dayton st madison wisconsin tel fax email raghu uri cs wisc edu june propose new compression algorithm tailored database applications 
applied collection records especially effective records low medium cardinality fields 
addition new technique supports fast decompression 
gzip typically takes times cpu time decompress relations achieving compression ratio 
algorithm achieves compression numeric fields low medium cardinality fields types handled mapping values small range integers algorithms complemented compression techniques general non numeric text image fields 
promising application domains include decision support systems dss fact tables far largest tables applications contain low medium cardinality fields typically text fields kim 
decompression rates faster typical disk throughputs sequential scans contrast gzip slower 
important dss applications scan large ranges records 
important distinguishing characteristic algorithm contrast compression algorithms proposed earlier decompress individual tuples individual fields full page time 
offers significant improvements cases page stored buffer pool repeatedly probed tuples page stored compressed form retrieving individual tuple extremely fast cpu cost decompression dominates 
compression algorithm improves index structures trees trees significantly reducing number leaf pages compressing index entries greatly increases fanout 
context important feature algorithm compression lossy lossless degree loss compression ratio user specified 
data entries index simply represented pairs compression technique naturally yields benefits optimizations storing rid gamma pairs recognizing runs rids 
traditional compression algorithms lempel ziv sto lz lz basis standard gzip compression package require large portion file small part file required 
example relation containing employee records compressed time current dbms products page worth data uncompressed retrieve single tuple 
page time compression leads compressed pages varying length packed physical pages mapping original pages records physical pages containing compressed versions maintained 
addition compression techniques decompress individual tuples page store page decompressed memory leading poorer utilization buffer pool comparison storing compressed pages 
compression algorithm overcomes problems 
algorithm simple easily added file management layer dbms supports usual technique identifying record pair requires localized changes existing dbms code 
higher layers dbms code insulated details compression technique 
addition new technique supports fast decompression page faster decompression individual tuples page 
contributions page level compression 
describe compression algorithm collections records index entries essentially viewed new page level layout collections records section 
allows decompression level specified field particular tuple proposed compression techniques aware require decompressing entire page 
scenarios illustrate importance tuple level decompression section 
performance study 
performance analysis underscores importance compression database context section 
numbers complete implementation algorithms 
measure compression ratios compression decompression speeds achieved technique range synthetic real datasets compare gzip applied page time basis 
current systems particular sybase iq proprietary variant gzip applied page time 
typically get compression ratios 
low cardinality datasets see compression ratios go high 
numbers comparable gzip low cardinality data compression better gzip gzip applied page time compressed data rounded blocks minimum size sacrificing compression 
algorithm typically times faster gzip decompressing full page orders magnitude faster individual tuple required 
tuples page required decompression times faster sequential contrast gzip times slower sequential application trees trees 
study application technique internal nodes index structures trees trees section 
treat entries index data compress key pointer fields 
comparison traditional techniques storing rid gamma pairs special rid representations exploit runs rids achieve compression comparable additional effort compressing pages algorithm 
trees choose lossy lossless compression key part data way exploits semantics tree entry capability possible compression algorithms 
key represents hyper rectangle lossy compression larger hyper rectangle represent smaller space 
multidimensional bulk loading algorithm 
exploit sort order data gain better compression 
tree tree orders utilized 
data initially unsorted bulk loading algorithm qualities ffl algorithm sorts data packs data pages 
pages high quality leaf level tree 
note pages compressed version data creating rest tree 
clark french sybase iq giving information compression sybase iq 
ffl slightly modified version algorithm ensure overlap leaf pages tree point data 
compressed file produced algorithm set leaf level pages indexing structure tree tree depending sorting algorithm retaining entire tree increases size compressed file typically 
total size compressed file plus clustered index potentially multidimensional search key size original file 
addition compression obtain index little additional cost 
compressing relation part algorithm call page level compression takes advantage common information tuples page 
common information called frame page 
frame field tuple compressed quite significantly tuples stored page technique possible 
compression done incrementally tuples stored bulk loading time run time inserts individual tuples 
ensures additional tuples fit page advantage space freed compression 
conventional page organization information specific tuple stored slot page pageid identify tuple 
retrieve tuple retrieve information corresponding slot conjunction frame information page reconstruct tuple done individual fields tuple 
important feature technique fixed length fields compressed remain fixed length 
compression technique doesn force slot directory 
note information needed uncompress page stored page 
degree compression obtained page level compression technique depends greatly range values field set tuples stored page 
effectiveness compression increased dramatically partitioning tuples file pages intelligent way 
database contains tuples say ways group tuples different groupings may yield drastically different compression ratios 
nr demonstrates effectiveness tree sort order assign tuples pages 
develop connection index sort orders including multidimensional indexes trees improved compression 
describe page level compression technique section bulk loading techniques grouping records pages section 
page level compression frames basic observation follows consider actual range values appear column page smaller range values underlying domain 
example column contains integers smallest value page column largest range smaller range integers represented overflow 
know range potential values represent value range storing just bits distinguish values range 
example remember values range appear column example page specify value range bits represents represents represents represents represents 
consider set points rectangles collect minima maxima dimensions minima maxima provide frame points rectangles lie 
instance theta frame tells range possible values dimension records set instance points axis vary values inclusive points axis vary values 
bits needed distinguish values occur inside frame bits needed distinguish values 
set points represented bit strings number records stored page typically hundreds overhead remembering frame worth example values originally stored bit integers compress points loss information average factor account overhead storing frame 
bits dimension equally spaced values frame lossy compression 
actual point actual point actual point estimated point estimating rectangle point approximation lossy compression 
sufficient represent point rectangle bounding rectangle index levels tree 
case reduce number bits required want trading precision 
idea bits represent equally spaced numbers frame see creating uniform grid coarseness depends number bits represent cuts dimension 
original point rectangle represented smallest rectangle grid contains 
original data consists points new rectangles width dimension represent rectangle simply min value dimension lower left corner dimensions see 
instance bits dimension axes file level compression bulk loading bulk loading algorithms section 
algorithms produces index structure tree tree containing data records leaf pages 
algorithm called str lel bulk loading algorithm trees 
second algorithm variant trades compression obtain higher quality tree index 
third algorithm variant produces tree index search key concatenation attributes sorting bulk loading 
nr discusses tree orderings 
describe bulk loading algorithm detail describe briefly variants 
compression oriented tree bulk loading algorithm bulk loading algorithms partition set points center points data contains rectangles pages 
partitioning problem described follows input 
set multiset points dimensional space 
assume dimension axis space linear ordering values 
output 
partition input subsets 
subsets usually identified index nodes disk pages 
requirements 
partition group points close group possible 
partition unbiased possible 
set specified dimensions 
partition partition partition partition pages example bulk loading sort operator 
partition partition partition partition pages example bulk loading sort operator str 
bold points belong partitions right point 
problem reapplied resulting hyper rectangles corresponding pages produced process 
repeated resulting pages hyper rectangular bounds fit page root 
example dimensions demonstrates algorithm 
consider set points shown small circles 
suppose determine total number pages needed 
sort set dimension ascending order 
define number partitions dimension square root reflects assumption number partitions dimensions equal expect partitions cut partitions dimension 
sort partition dimension ascending order 
second partition sorted descending order third ascending order fourth descending order 
shows partitions 
arrow shows general ordering points dataset 
note linearization generated alternation sort order guarantees page fully packed expense little spatial overlap leaf pages 
description assumed number pages needed known 
number determine number partitions axis 
actuality don know final number pages needed depends compression obtained depends data 
estimate number pages obtained assuming bounding box data values tuples uniformly distributed range 
case low cardinality data want guarantee partition divisions happen changes value dimension cut 
results better division partitions small ranges considers degenerate case low cardinality data 
result narrowing ranges pages dataset value isn partition partitions don contain overlap 
instance note natural partition divisions occurred points value 
partition reduced compression dataset 
see section details 
partitioning technique similar str starting dimension divide data leaf pages strips 
instance shows str decomposes data space pages 
str sorting uncompressed data calculate exactly number pages produced bulk loading algorithm 
case calculate number pages strip 
case determine strips pages 
strips contain pages strip contains points 
strip contains points left case 
strips grouped partitions entries page contains point 
note pages fully packed 
important differences algorithm str arise considerations ffl bulk loading algorithm pack data compressed pages willing trade tree quality increased compression 
ffl degree compression data page number entries page data dependent 
ffl case low cardinality data important cut dimension done value boundary data 
item listed simply means pack pages aggressively expense increased spatial overlap partitions 
creating linearization data allows steal data neighboring partition fill partially empty partition 
particular considers example partially empty page top strip 
pages filled considers effect reversing sort order strip 
results illustrated 
linear ordering achieved data may packed pages linearization 
second point means estimate required number pages 
third point constrains determine partition boundaries 
algorithm detail 
ordering points determined sorting function sort 
arguments array items ordered 
dimensional array integers array index tuple number second identifies particular dimension tuple value jth dimension ith tuple 
notation jaj refer number tuples number elements integer identifying dimension want sort 
addition exists global variable array booleans dimensionality data 
true current sort order dimension ascending descending 
initial value matter 
function sort steps 

reverse sort order dimension 
step ensures linearization space illustrated example section 

sort array dimension sorting order 

estimate number pages needed storing items array 
estimate difficult frame individual page needed determine compression ratio 
currently estimate number retrieving tuples partition frame number tuples get accurate estimate number pages 
set number partitions dimension array 
define list deltak list partition start locations 
lower values particular occurrence 
remove duplicates array 
previous step guarantee dimensions aren important guarantee low cardinality fields 
jlj gamma sort array starting gamma gamma 
sort array starting jlj gamma gamma 
perform bulk loading entire tree maximize compression function sort jaj 
resulting array sorted simply pack pages leaf level maximally introducing entries sequentially array entries packed 
process repeated higher levels tree center points resulting pages leaf level input level 
observe leaf internal nodes compressed resulting tree index 
quality oriented tree bulk loading algorithm perform bulk loading entire tree maximize tree quality deepest level recursion strips writes pages sorting entire array 
combined partitioning changes value results lower page occupancy guarantees overlap leaf pages point data 
note handles low cardinality attributes gracefully str 
tree bulk loading algorithm compress data records ordering tree algorithm sort dimensions dimensions taken treated concatenated search key 
results compressed clustered primary tree concatenated search key 
algorithm case simpler general version runs faster elaborate lack space 
compression applied rectangle indexes indexing structures including trees gut trees bm grid files nhs buddy trees tv trees metric trees bkk consist collections rectangle pointer pairs point data pairs :10.1.1.102.7240:10.1.1.40.9848
compression technique effectively indexing structures especially useful search key contains dimensions 
behavior compression technique index structures similar ways tree prefix compression compression scheme different ffl translate minimum value frame compressing 
ffl lossy compression better bits internal nodes prefix compression bit combinations fall inside frame 
ffl compress leaf level entries prefix compression applied non leaf nodes 
sections discuss impact compression trees 
includes alterations needed dynamic insert delete update 
addition internal nodes compressed lossy compression 
description trees tree gut height balanced tree structure designed specifically indexing multi dimensional spatial objects 
tree node disk page tree internal node leaf node 
entries internal nodes point nodes 
entries leaf nodes contain object ids actual spatial objects optionally objects minimum bounding box mbb object 
internal node entry contains associated mbb guaranteed completely cover subtree pointed entry 
note entries node associated overlap 
means tree searches tree search may explore path root leaf point queries 
oriented orthogonally respect fixed axes tree 
number dimensions greater appropriate number dimensions basic idea remains 
description sufficiently general applies variants tree including trees srf trees bkss 
note non leaf nodes tree nonoverlapping point queries search single path root leaf 
region queries explore paths general 
variations motivated goals improving space utilization search time reducing tree height minimizing mbb overlaps discuss variations 
compression tree internal nodes header entry entry entry compressed tree page normal tree header header frame compressed tree page layout internal nodes trees simply store hyper rectangle page pointer pairs compression compress hyper rectangle stored pair 
result header internal node include frame hyperrectangles page see 
potentially lossy compression global number bits dimension internal nodes ensure fixed size entries internal nodes 
simplifies implementation insert algorithm particular splitting 
representation internal node modifications tree cause change frame internal node 
instance suppose point added region represented tree 
path root leaf created covers space point resides 
involves widening range values pages path 
changes occur happens insert modify delete convert entries old frame new 
done bits dimension dimension entries page boxes array uncompressed hyper rectangles entry th entry node compress box frame returns compressed representation box uncompress box frame inverse compress valid entry number node boxes uncompress entry valid box number boxes page entry compress boxes note algorithm changes done widening rectangles see 
lead large successive approximation error 
alleviate problem steps taken 
steps frames children bounding boxes actual data child 
corrective steps taken 
child pointers tree insert frame child update corresponding entry parent node see 

number times frame changed stored header 
information determine occasional occur cleanup consists getting minimally lossy bounds entries 
minimally lossy bounds determined examining frames children page cleaned 
note cost step virtually nil additional done 
practice step enormous effect reducing successive approximation error 
step expensive involves new frame old frame new point legend old approximation point new approximation point change approximation point frame changes entry child node header precise bound frame normal header parent node conservative pointer header bound normal header frame conservative bounding box parent tied exact bounding box child loading children node cleaned done frequently 
discussion frequently cleanup run deferred 
compression tree leaf nodes legend old frame frame change approximation point old frame approximation point new frame width width width width change change approximation change frame note index secondary index global bits dimension scheme mentioned internal nodes apply leaves 
situation differences leaf nodes internal nodes arise successive approximation error fixed corrective step 
slightly modified version step retrieves actual data database available method correct error 
discussion frequently cleanup run deferred 
index primary index bits dimension page individually determined data page 
cleanup necessary information lost 
leaf nodes secondary indexes point data note index secondary index actual data points changes frame result widening box implicitly approximate point see 
glance indicate box representation point resulting doubling entry size 
reflection unnecessary 
width box represent point exactly number frame changes cleanup 
easy see considering scenario depicted 
performance evaluation section tests effectiveness compression technique variety situations 
section focuses effectiveness compression strategy compress relations 
explore compression real synthetic datasets 
addition appropriateness compression strategy examined trees 
section examines performance gains compression technique applied trees 
section demonstrates compression techniques applied tiger gis dataset 
section discusses cpu costs associated decompression comparison gzip 
section compares compression techniques sybase iq 
section gives examples cheap tuple level decompression significantly enhances performance 
relational compression experiments relational file compression improves space utilization linear scan performance experiments measure size compressed file compared original variety situations 
compression examined tuple partitionings including maximum occupancy random partitioning maximum compression tree str partitioning modified str algorithm maximum quality tree str partitioning tree partitioning 
results shown compressed file size percentage original file size 
example get compression ratio means space taken relation reduced factor 
number attributes dimensionality size original relation size compressed relation size original relation random partitioning tree partitioning tree partitioning sales dataset 
compression achieved varying dimensionality partitioning strategy 
want examine effect partitioning strategy relation compression conducted experiment studied partitioning strategy real dataset 
partitioning strategies random rtree optimized compression tree varied dimensionality datasets keeping number tuples constant 
results shown 
note tree partitioning tree partitioning producing identical compression results significantly better random partitioning 
demonstrates value spatial grouping basis tuple distribution pages 
addition note dimensionality increases effect smart partitioning re duced 
underlying reasons trend clear thinks effect grouping close points order reduce key size percentage reduce size dimension percentage 
suppose percentage determines need reduction bit dimension 
divide range page dimensions factor 
means divide data pages obtain improvement due smart partitioning 
benefit smart partitioning decreases exponentially dimensionality 
result dimensionality increased benefit smart partitioning reduced nil higher dimensions 
synthetic data sets section synthetic datasets test compression scheme variety conditions 
list variables experiments size number tuples relation 
dimensionality number attributes relations 
range range values attributes 
distribution distribution values attribute varied uniform worst case exponential distributions 
partitioning strategy described section strategies 
strategies maximal compression tree partitioning maximal quality tree partitioning tree partitioning 
page size 
varied page size kb kb slight differences experiments performed page size kb 
increasing page size kb slightly negative effect compression ratio 
dataset characteristics ffl attributes particular experiment identical distributions created randomizing process 
ffl attributes statistically independent 
figures section occur pairs left refers experiment uniform attribute distribution right refers experiment exponential distribution 
figures collected shown easy comparison shows compression achieved file size tuples uniform attribute distributions high compression tree partitioning 
varied dimensionality range 
different result lines represent datasets varying attribute range 
assumed attribute values represented original relation bytes 
range attributes decrease frames individual pages narrows compression improves attribute range decreases 
specifically examine top line graph range values covers bits bit integer original dataset 
result expect compress file original size 
note compression significantly better exponentially distributed data 
due large part data falling narrower range resulting smaller frames average 
attributes dimensionality size compressed relation size original relation original relation size attributes range attributes range attributes range attributes range attributes dimensionality size compressed relation size original relation original relation size attributes range attributes range attributes range attributes range results tree bulk loading geared compression 
attributes dimensionality original relation size size original relation size compressed relation attributes dimensionality original relation size size original relation size compressed relation results tree bulk loading geared tree quality 
attributes dimensionality original relation size size original relation size compressed relation attributes dimensionality original relation size size original relation size compressed relation results tree bulk loading 
figures show results exact datasets 
difference partitioning tree quality tree ordering respectively 
note tree partitioning tree high compression partitioning nearly identical tree quality partitioning lags slightly higher dimensions 
note experiments skewed exponential distributions compression worked remarkably 
shows results experiment tuples 
note slightly improved compression 
understandable considers number cuts str size input case results bits reduction total key size 
results previous experiment motivated experiment results shown 
experiment number tuples dimensional dataset repeatedly doubled 
note significant improvement compression doubling 
companies data set section describes compression experiments sample data set 
dataset attributes describe aspects companies stock exchange 
attributes include asset value share price 
sample contained tuples attributes 
experiment indexes partitioned high tree compression 
projected attributes relation 
fields dataset having high range extremely skewed 
shows compression results varied dimensionality page size 
varied dimensionality projection attribute dataset 
tuples experiments 
achieved compressed file sizes 
note asserted previous section page size little effect compression 
attributes dimensionality size compressed relation size original relation original relation size attributes range attributes range attributes range attributes range attributes dimensionality size compressed relation size original relation original relation size attributes range attributes range attributes range attributes range synthetic dataset tuples tree bulk loading geared compression 
left graph uniformly distributed data 
right graph exponentially distributed data 
tuples thousands original relation size size original relation size compressed relation synthetic dataset changing number tuples 
original relation size page size bytes page size kbytes page size kbytes page size kbytes size original relation size compressed relation attributes dimensionality relational compression dataset 
sales data set cardinality dimension low low medium high number attributes dimensionality size original relation size compressed relation size original relation tuples relation tuples relation sales dataset 
compression achieved versus dimensionality 
section describes relational compression experiments done sales dataset 
dataset taken catalog sales eleven attributes 
attributes low cardinality 
attributes created table maps values possible attribute range number possible values 
mapped values relation original values 
experiments sales dataset partitioned tree high compression 
experiment section results shown examined compression dimensionality increased progressively larger projections original dataset 
cardinality added dimension shown dimension 
note low cardinality attribute introduced compression improves high cardinality attribute reduces compressibility 
easily understood low cardinality attributes need fewer bits represent 
addition different data lines represent different dataset sizes 
note effect file size diminished dimensionality 
compression see quite 
comparison gzip uses lempel ziv compression applied file btree sort order able compress file slightly better course maintain notion tuple id guarantee light decoding costs single tuples gzip 
gis data set experiment tiger dataset orange county california 
objects data polygons roads rivers 
tuples 
partitioning tree compression got compression resulting relation size size original data file 
partitioning tree quality got compression 
file unsorted got compression 
comparison gzip compressed original size dataset unsorted original size sorted 
dataset achieved better compression situations 
cpu vs costs section explore cpu costs associated decompressing information compressed pages 
comparison sake cpu costs associated gzip manner consistent sybase iq 
experiments conducted compression techniques extracted information pages dataset 
cases pages memory compressed resulting costs 
measurements included time scan contents full compressed page time scan field tuples full compressed page field time scan tuple full compressed page tuple time count number entries page count 
corresponding throughputs rate disk needs supply information cpu keep cpu busy 
note code experiments endian platform independent bit string package 
numbers improved significantly writing code platform specific assembly language 
results table field tuple count processing time ms ms ms ms throughput mb mb mb mb table compression results gzip experiments performed dividing uncompressed dataset kbyte blocks compressed 
note resulting compressed blocks ranged size kbytes kbytes 
cpu time measured decompressing blocks size category 
ensure overhead starting gzip taken account time decompress number compressed byte blocks subtracted measured times 
note field tuple level decompression isn possible gzip style compression cost associated decompressing individual block measured 
corresponding throughputs rate disk needs supply information cpu keep cpu busy 
results table 
note tables decompression great deal faster gzip 
easily keep linear scan situations approximately mb gzip doesn keep kbytes kbytes kbytes kbytes kbytes processing time ms ms ms ms ms throughput mb mb mb mb mb table gzip compression results measured circumstances 
addition large performance benefit accessing individual tuples fields page gzip unable take advantage situations 
see section information advantages drawbacks gzip style compression comparison 
comparison techniques commercial systems commercial vendors utilizing compression techniques data warehousing products 
discuss sybase iq particular techniques best current commercial systems published information compression commercial dbms products generally lacking 
sybase compresses data technique similar gzip rest section simply say gzip brevity 
note values fields listed order tuple id tuple ids don need stored projection 
sybase uses bit vectors applies gzip compress bit vector 
note gzip advantages drawbacks 
advantage gzip general compression technique compress type field including strings 
drawback gzip support random access tuples page decompression relatively slow 
means pages actively stored memory uncompressed 
drawback applying gzip page time causes complications outline 
applying gzip file time mean need particular tuple page decompress entire file 
important exception problem occurs linear scans 
linear scans processed pages tossed immediately 
consequence applying gzip page time different pages compress different extents result compressing page variable size 
pages buffer pool fixed size need map variable sized compressed pages fixed size logical pages 
sybase iq pages compressed applying gzip compressed page rounded logical page 
logical page uncompressed yields page original file 
means empty space logical page affects compression obtained 
logical pages allocated sequentially disk prefetched increments 
random access page original file desired mapping logical pages disk pages maintained 
summary gzip provides high flexibility type data compressed complicating buffer storage management 
complications lead inefficiencies memory disk utilization 
drawback gzip gunzip sustain throughput mb see section mhz pentium pro 
rate high keep linear scan 
note may improved efficient implementation gunzip sybase iq uses proprietary version gzip standard unix gzip access optimized gzip implementation 
note sybase iq stores relations column wise 
improve compression obtained gzip compression techniques including decision vertically partition relation general orthogonal compression 
vertical partitioning columns stored order original set tuples compression technique compress columns storing tuple ids field value 
hand column reordered tree ordering tuple ids stored field value column case sybase approach case tuple id compressed 
table summarizes resulting size compressed files sybase iq vertical partitioning compression compressed bitmaps compression technique 
cases values tuple field mapped number cardinality field 
sybase experiments gzip uncompressed buffer page size kbytes compressed size kbytes multiple kbytes 
page size experiments kbytes 
experiments done columns stored order original tuples tuple ids weren stored value 
sybase vertical sybase bit vectors low cardinality sales medium cardinality sales na na table sybase iq compression comparison note sybase techniques typically result comparable performance 
note low cardinality sales data compression limit results impedance mismatch buffer pool disk pages limits compression 
importance tuple level decompression section scenarios fast tuple level decompression significantly enhances performance 
example index nested loops join inner relation index compressed tree compressed inner relation fit memory uncompressed inner relation fit 
scenario tuples outer relation brought memory page time 
tuple outer probe inner index cost join dominated cost retrieving matching inner tuples 
inner outer isn sorted join column probe matches tuples randomly distributed inner relation need just tuples page containing matching tuple 
approach rid matching tuple identifies compressed inner page slot containing tuple locating selectively decompressing tuple extremely fast 
page time gzip compression rid tuple uncompressed inner relation mapped logical page containing tuple compressed page containing tuple identified logical page decompressed entirety 
difference cpu costs approach retrieving matching tuple factor see section 
entire compressed inner assumed fit memory costs difference dominates relative costs computing join compression gzip compress inner 
alternatively keep uncompressed inner relation disk bring pages containing matching tuples probe conventional join techniques algorithm outlined superior total just scan outer plus compressed inner plus cpu cost individually decompressing matching tuple seen cpu cost low 
experiment wasn performed medium cardinality attributes bit vectors inappropriate 
second scenario tuple level decompression beneficial occurs multiuser system dataset fits memory compressed uncompressed 
scenario easy imagine people simultaneously accessing random tuples database 
instance people performing exact match queries queries indices retrieve small portions database 
note substantial queries selectivity secondary indices result random access underlying pages relation 
similar scenarios constructed environment different workload 
able store pages compressed extract individual tuples rapidly key performance 
tree compression experiments size query region percent total size region attributes queries point partial match query query uncompressed tree query compressed tree testing quality trees sales dataset 
size query region percent total size region queries point partial match query query uncompressed tree query compressed tree attributes testing quality trees dataset 
section examines impact compressing tree pages tree performance 
note compressing leaf pages indexes results size improvements comparable general relational compression 
internal nodes benefited significantly improved fanout 
experiments section determined compressed pages decompressed fast current disk drives doing sequential access queries trees exhibit random access characteristics bounded costs associated getting pages disk 
result page measured experiments 
experiments real datasets discussed test quality compressed tree tree quality partitioning vs quality uncompressed tree 
purpose performed identical queries compressed uncompressed trees measured page caused query 
purposes making results realistic assumed root tree remains memory doesn incur costs 
query type run times 
page results calculated average relative compressed uncompressed trees 
addition calculated standard deviation 
types queries point queries 
query region random point region tree 
partial match queries 
specified value subset attributes 
query retrieved tuples matched values specified attributes 
query non discriminatory respect unspecified attributes 
range queries 
specified range attribute 
varied hyper volume resulting query region 
queries point query uncompressed tree query compressed tree partial match query size query region percent total size region testing quality trees tiger dataset 
shows results sales dataset 
attributes dataset tuples 
rate improvement page retrieval ratios point queries worst average partial match queries 
clear win quality compressed tree 
shows results dataset 
dimensions tuples 
case got better gains sales dataset case 
shows results tiger dataset 
dimensions tuples 
see results datasets 
related ng ravishankar nr discussed compression scheme similar respects 
particular ffl introduced page level compression decompression algorithm relational data 
ffl explored tree sort order compressed data actual trees data 
note details compression scheme quite different ffl scheme decompresses field tuple basis page basis 
ffl actual information tuples store extra information page basis 
compression technique uses run length encoding stores extra information field tuple basis 
ffl compression scheme easily adapted lossy important compressing index pages trees 
scenarios highlight differences schemes ffl multiuser workloads randomly access individual tuples pages 
clearly approach superior case 
ffl performing range query linear scan data 
bounding box entire page stored data page scheme check see entries page need examined 
ffl performing small probes tree 
compression scheme binary search search page record fixed length 
note serious issue compressed environment entries fit page 
additionally demonstrated application multidimensional bulk loading compression range performance results strongly argue compression database context 
eos rh bas discuss compression techniques run length encoding header compression encoding category values order preserving compression huffman encoding lempel ziv differencing prefix postfix compression support random access tuples page 
compression described section techniques handle kind data introduce buffer storage management problems 
oq discusses query evaluation algorithms compression 
assumes gzip compression techniques examples discussed 
presents new compression algorithm demonstrates effectiveness relational database pages 
compression ratios typical real datasets 
low cardinality datasets particular produced compression ratios high 
decompression costs surprisingly low 
fact cpu cost decompressing relation approximately cpu cost gunzip relation achieved compression ratios comparable 
difference cpu costs means cpu decompression cost sequential cost earlier higher sequential cost 
single tuple required just tuple field decompressed orders magnitude lower cost decompressing entire page 
feasible store pages buffer pool compressed form tuple page required extracted fast 
knowledge compression algorithm allows decompression tuple basis 
compression code localized code manages tuples individual pages making easy integrate existing dbms 
simplifications offers keeping compressed pages buffer pool leading better utilization buffer pool attractive implementation standpoint 
related point applying index pages contain pairs obtain benefits techniques storing rid gamma pairs specialized rid representations exploit runs rids 
comparison techniques gzip compression algorithm disadvantage compresses numeric fields low cardinality fields types mapped numeric fields fact done anyway improves compression attained gzip 
note applied files containing combination numeric non numeric fields achieve compression just numeric fields 
range applicability quite broad example fact tables data warehouses contain bulk data contain numeric low cardinality fields long text fields 
explored relationship sorting compressibility detail 
sorts explored sorts suitable bulk loading multidimensional indexing structures trees 
important sorts worked equally implies compression linear multidimensional indexes significantly better sort 
observation underscores importance sorting data prior compression approximately sorted 
give warm kevin beyer time sounding board asking questions support primary indexes 
clark french sybase providing details concerning compression techniques 
addition patrick neil valuable feedback addressing questions significantly strengthened 
bas 
data compression scientific statistical databases 
ieee transactions software engineering volume se pages october 
bkk stefan berchtold daniel keim hans peter kriegel :10.1.1.102.7240
tree index structure high dimensional data 
mohan editors proc 
th inf 
conf 
vldb pages mumbai india september 
bkss beckmann 
kriegel schneider seeger 
tree efficient robust access method points rectangles 
proc 
acm sigmod int 
conf 
management data pages 
bm bayer mccreight 
organization maintenance large ordered indexes 
acta 
eos susan eggers frank olken arie shoshani 
compression technique large statistical databases 
vldb pages 
gut guttman 
trees dynamic index structure spatial searching 
proc 
acm sigmod int 
conf 
management data pages 

kriegel horn 
performance object decomposition techniques spatial query processing 
proc 
nd symposium large spatial databases lecture notes computer science volume pages 
kim kimball 
data warehouse toolkit 
john wiley sons 

kriegel schneider seeger 
performance comparison point spatial access methods 
ssd pages 

kriegel schneider seeger 
buddy tree efficient robust method spatial data base systems 
proc 
th vldb conf pages 
lel scott leutenegger jeffrey mario lopez 
str simple efficient algorithm tree packing 
technical report mathematics computer science dept university denver 
king ip lin jagadish christos faloutsos :10.1.1.40.9848
tv tree index structure high dimensional data 
vldb journal 
lz lempel ziv 
complexity finite sequences 
ieee transactions information theory 
lz lempel ziv 
universal algorithm sequential data compression 
ieee transactions information theory 
nhs nievergelt hinterberger sevcik 
grid file adaptable symmetric multikey file structure 
readings database systems 
morgan kaufmann 
nr wee ng ravishankar 
relational database compression augmented vector quantization 
yu chen editors ieee th international conference data engineering pages taipei taiwan march 
oq patrick neil quass 
query performance variant indexes 
sigmod pages tucson az march 
rh mark roth scott van horn 
database compression 
sigmod record volume pages september 
srf sellis roussopoulos faloutsos 
tree dynamic index multi dimensional objects 
proc 
th inf 
conf 
vldb pages 
sto james storer 
data compression methods theory 
computer science press research blvd rockville maryland 
