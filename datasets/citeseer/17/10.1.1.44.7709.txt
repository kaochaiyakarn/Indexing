exploiting generative models discriminative classifiers tommi jaakkola david haussler cse ucsc edu department computer science university california santa cruz ca isaac newton institute mathematical sciences university cambridge clarkson road cambridge cb eh generative probability models hidden markov models provide principled way treating missing information dealing variable length sequences 
hand discriminative methods support vector machines enable construct flexible decision boundaries result classification performance superior model approaches 
ideal classifier combine complementary approaches 
develop natural way achieving combination deriving kernel functions discriminative methods support vector machines generative probability models 
provide theoretical justification combination demonstrate substantial improvement classification performance context dna protein sequence analysis 
speech vision text data difficult deal context simple statistical classification problems 
examples classified sequences arrays variable size may distorted particular ways common estimate generative model data bayes rule obtain classifier model 
discriminative methods directly estimate posterior probability class label gaussian process classifiers discriminant function class label support vector machines areas proven superior generative models classification problems 
problem systematic way extract features metric relations examples discriminative methods context difficult data types listed 
propose general method extracting discriminatory features generative model 
features propose generally applicable naturally suited kernel methods discriminative classification 
kernel methods provide brief kernel methods see details 
suppose training set examples corresponding binary labels sigma 
kernel methods define label new example obtained weighted sum training labels 
weighting training label consists parts importance example summarized coefficient measure pairwise similarity expressed terms kernel function 
predicted label new example derived rule sign note class kernel methods includes probabilistic classifiers case rule refers label maximum probability 
free parameters classification rule coefficients degree kernel function pin particular kernel method things need clarified 
define classification loss equivalently optimization problem solve determine appropriate values coefficients slight variations optimization problem take support vector machines generalized linear models 
second important issue choice kernel function main topic 
brief illustration generalized linear models kernel methods 
generalized linear models concreteness consider logistic regression models emphasizing ideas applicable larger class models logistic regression models probability label example parameter vector sjx oe gamma delta specifically applies generalized linear models transfer functions log concave 
assume constant appended feature vector bias term included inner product oe gammaz gamma logistic function 
control complexity model number training examples small assign prior distribution parameters 
assume prior zero mean gaussian possibly full covariance matrix sigma 
maximum posteriori map estimate parameters training set examples maximizing penalized log likelihood log jx log log oe gamma delta gamma sigma gamma constant depend 
straightforward show simply gradient respect parameters solution concave maximization problem written sigmax log oe fi fi fi fi oe gammas note coefficients appear weights training examples definition kernel methods 
inserting solution back conditional probability model gives sjx oe sigmax identifying sigmax noting label maximum probability sign sum argument gives decision rule 
trough derivation written primal parameters terms dual coefficients consequently penalized log likelihood function written entirely terms resulting likelihood function specifies coefficients optimized 
optimization problem unique solution put generic form see details 
form kernel function establishes connection logistic regression model kernel classifier specific inner product form sigmax long examples replaced feature vectors derived examples form kernel function general 
discuss section 
kernel function inner product form sigmax kernel quite general 
general kernel function valid roughly speaking needs positive semi definite see 
mercer theorem corresponds generally legendre transformation loss functions log oe 
possible arise solutions maximum penalized likelihood problem words relevant 
valid kernel function admits representation simple inner product suitably defined feature vectors oe oe feature vectors come fixed mapping oe example previous section kernel function form sigmax simple inner product transformed feature vector oe sigma specifying simple inner product feature space defines euclidean metric space 
consequently euclidean distances feature vectors obtained directly kernel function oe gamma oe gamma addition defining metric structure feature space kernel defines pseudo metric original example space oe gamma oe kernel embodies prior assumptions metric relations original examples 
systematic procedure proposed finding kernel functions finding ones naturally handle variable length examples topic section 
kernels generative probability models fisher kernel key idea derive kernel function generative probability model 
need kernel function clear apparent derived generative model general way 
arrive kernel function different perspectives enhancing discriminative power model discussed appendix attempt find natural comparison examples induced generative model 
perspective section 
seen previous section defining kernel function automatically implies assumptions metric relations examples 
argue metric relations defined directly generative probability model 
goal classification generative model may include classification variable latent variable 
interest difference generative process pair examples say differences posterior probabilities labels computed separately example discrimination purely generative approach 
capture generative process metric examples gradient space generative model 
gradient loglikelihood respect parameter describes parameter contributes process generating particular example 
exponential family distributions natural parameterization gradients normalization constant depends form sufficient statistics example 
gradient space naturally preserves structural assumptions model encodes generation process 
develop idea generally consider parametric class models theta 
class probability models defines riemannian manifold theta local metric fisher information matrix ex ux log expectation see 
simplicity suppressed dependence ux parameter setting equivalently position manifold 
gradient log likelihood ux called fisher score plays fundamental role development 
local metric theta defines distance current model nearby model ffi 
distance ffi ffi ffi approximates kl divergence models sufficiently small ffi 
fisher score ux log maps example feature vector point gradient space manifold theta call fisher score mapping 
gradient ux define direction steepest ascent log example manifold gradient direction ffi maximizes log traversing minimum distance manifold defined ffi 
gradient known natural gradient see obtained ordinary gradient oe gamma ux call mapping oe natural mapping examples feature vectors 
dependence parameter setting 
natural kernel mapping inner product feature vectors relative local riemannian metric oe ioe gamma ux call fisher kernel owing fundamental role played fisher scores definition 
role information matrix significant context logistic regression models matrix appearing middle feature vectors relates covariance matrix gaussian prior show 
asymptotically information matrix immaterial simpler kernel ku ux provides suitable substitute fisher kernel 
metric point view scaled translated kernel ck equivalent kernel operations may important practice 
example logistic regression context additive constant kernel prior variance bias term multiplicative factor relates prior variance remaining parameters natural gradients order information matrix included 
generally emphasize fisher kernel defined provides basic comparison examples defining meant inner product examples examples objects various types variable length sequences 
way kernel function discriminative classifier specified 
fisher kernel directly kernel classifier example amounts finding linear separating hyperplane natural gradient fisher score feature space 
examples may linearly separable feature space natural metric structure fisher kernel 
may advantageous search space quadratic tacitly assumed original examples homogenous coordinates 
higher order decision boundaries equivalent transforming fisher kernel resulting kernel classifier 
ready state properties kernel function form theorem theorem suitably regular probability model parameters fisher kernel gamma ux ux log properties valid kernel function invariant invertible differentiable transformation parameters kernel classifier employing fisher kernel derived model contains label latent variable asymptotically classifier map labeling model property immediate positive definite 
second property follows fact kernel derived manifold theta intrinsic class probability models dependent parameterization 
third property established basis discriminative derivation kernel see appendix 
omit details brevity 
summarize defined generic procedure obtaining kernel functions generative probability models 
consequently benefits generative models immediately available discriminative classifier employing kernel function 
turn experimental demonstration effectiveness combined classifier 
experimental results consider relevant examples analysis compare performance combined classifier best generative models problems 
start dna splice site classification problem objective recognize true splice sites boundaries expressed regions exons gene intermediate regions introns 
data set experiments consisted dna fragments elegans collected jonathon king sanger centre arun jagota ucsc 
true examples sequence dna alphabet fa cg length centered consensus gt splice boundary 
false examples similar sequences centered instances gt occur near twice differentiable likelihood fisher information exists positive definite chosen 
mechanical proof involving jacobians straightforward 
splice sites 
recognition rates report data set averages fold cross validation 
combined classifier setting requires choose generative model purpose deriving kernel function 
order test performance combined depends quality underlying generative model chose poorest model possible 
model dna residue position fragment chosen independently furthermore parameters set fa cg 
model assigns probability examples poor model derive fisher kernel model discriminative 
case logistic regression model quadratic fisher kernel shows recognition performance kernel method poor generative model comparison recognition performance naive bayes model hierarchical mixture model 
comparison summarized roc style curves plotting false positive errors errors accepting false examples function false negative errors errors missing true examples vary classification bias labels 
curves show poor underlying generative model combined classifier consistently better better generative models 
second serious application combined classifier consider known problem recognizing remote homologies evolutionary structural similarities protein sequences low residue identity 
considerable done refining hidden markov models purpose models current achieve best performance stateof art hmms comparison cases sources deriving kernel function 
logistic regression simple kernel ku number parameters hmms 
experiment set follows 
picked particular superfamily tim barrel fold scop protein classification left major families superfamily testing training hmm combined classifier sequences corresponding remaining families 
false training examples discriminative method came sequences fold superfamily 
test sequences consisted left family true examples proteins outside tim barrel fold false examples 
gave scheme fold cross validation 
number training examples varied depending left family 
sequences families extremely different challenging discrimination problem 
shows recognition performance curves hmm corresponding variable length sequences rendering discriminative methods inapplicable 
see med harvard edu jong assess final html false negative rate false negative rate comparison classification performance kernel classifier uniform model solid line mixture model dashed line 
mixture model naive bayes model components class 
false negative rate comparison homology recognition performance hidden markov model dashed line corresponding kernel classifier solid line 
kernel method averaged way cross validation 
combined classifier yields substantial improvement performance hmm 
discussion model kernel function derived provides generic mechanism incorporating generative models discriminative classifiers 
discrimination resulting combined classifier guaranteed superior generative model little additional computational cost 
note power new classifier arises large extent fisher scores features place original examples 
possible features classifier feedforward neural net kernel methods naturally suited incorporating 
note classification guide development kernel function results directly applicable regression clustering interpolation problems easily exploit metric relations examples defined fisher kernel 
acknowledgments tommi jaakkola david haussler gratefully acknowledge support isaac newton institute mathematical sciences acknowledges support doe sloan foundation postdoctoral fellowship support doe de fg er nsf iri 

amari 
natural gradient works efficiently learning 
neural computation 
baldi chauvin mcclure 
hidden markov models biological primary sequence information 
pnas 
eddy 
multiple alignment hidden markov models 
editors ismb pages menlo park ca july 
aaai mit press 
hubbard brenner chothia 
scop structural classification proteins database 
nar jan 
jaakkola haussler 
probabilistic kernel methods 

manuscript preparation 
karplus barrett cline haussler hughey holm sander 
predicting protein structure hidden markov models 
proteins structure function genetics supplement 
krogh brown mian haussler 
hidden markov models computational biology applications protein modeling 
feb 
mackay 
gaussian processes 

available wol ra phy cam ac uk mackay 
vapnik 
nature statistical learning theory 
springer verlag 
wahba 
spline models observational data 
cbms nsf regional conference series applied mathematics 
discriminative derivation model kernel consider class generative probability models label appears latent variable js sj marginal probability label sj functionally independent parameters conditional probabilities js 
model classify new example posterior probability sjx assigns labels 
goal derive kernel function model way kernel classifier employing kernel cases powerful cases powerful original model 
start defining call differential extension original model show maximum posterior probability decisions original model obtained special case extended model 
turn decision rule extended model kernel classifier 
differential extension consider models gamma parameters differentially close gamma 
define differential extension original model associating models sigma values label defining extended joint probability ext ext sigma 
claim posterior probability extended model reduced classifier original model 
define delta sj gamma sj delta sjx sjx gamma sjx denotes binary complement impose differential change marginal sj possible functional independence assumption earlier js ffl delta ffl delta sjx ffl delta second equality straightforward verify 
choose ext ffl delta extended posterior probability ext sjx ext ext ffl delta sjx second equality straightforward obtain details omitted brevity 
delta sjx positive label maximum posterior probability relative sjx evident label ext sjx 
decision rule extended model original model 
model kernel differential extension return general form extended posterior probability rewrite terms logistic function ext sjx oe log log ext ext oe delta logistic function 
parameters gamma differentially close expect taylor expansions log log gamma log log gamma ux differential assumption applies parameters constraints 
accurate 
ux log fisher score 
inserting back posterior probability formulation gives ext sjx oe gamma ux log ext ext oe gamma gamma gamma ux delta assumed clarity ext ext 
resulting decision rule logistic regression model parameter vector gamma gamma examples ux complete derivation assign natural prior distribution new parameters assigns low probability values arise large differences probability models corresponding gamma exp gammak gamma exp ae gamma log gamma dx oe exp ae gamma gamma gamma gamma gamma oe exp ae gamma oe fisher information matrix 
validity approximation follows differential assumption gamma equation shows dual variables logistic regression model prior yields model ext sjx oe gamma ux general form logistic regression fisher score function feature vector fisher kernel gamma ux shows logistic regression fisher kernel powerful classification method underlying generative model motivates choice fisher kernel natural choice perspective differential discrimination respect generative model 
