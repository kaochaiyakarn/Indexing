products experts geo rey hinton gatsby computational neuroscience unit university college london queen square london wc ar www gatsby ucl ac uk possible combine multiple probabilistic models data multiplying probabilities renormalizing 
ecient way model high dimensional data simultaneously satis es di erent lowdimensional constraints 
individual expert model focus giving high probability data vectors satisfy just constraints 
data vectors satisfy constraint violate constraints ruled low probability expert models 
training product models appears dicult addition maximizing probabilities individual models assign observed data necessary models disagree unobserved regions data space ne model assign high probability unobserved region long model assigns low probability 
fortunately individual models tractable fairly ecient way train product models 
training algorithm suggests biologically plausible way learning neural population codes 
di erent generative models data common way combine mixture combined probability distribution weighted arithmetic mean individual distributions 
equivalent assuming generative model data vector generated rst choosing individual generative models allowing individual model generate data vector 
combining models forming mixture attractive easy mixtures tractable models data em gradient ascent individual models di er lot mixture better true distribution data random choice individual models 
unfortunately mixture models inecient high dimensional spaces 
consider example manifold face images 
takes real numbers specify shape pose expression illumination face viewing conditions perceptual systems produce sharp posterior distribution dimensional manifold 
done mixture models tuned dimensional space mixing distributions model broadly tuned order cover dimensional space 
alternative way combine individual expert models multiply probabilities renormalize 
products experts poe advantage produce sharper distributions individual expert models 
example model constrain di erent dimensions high dimensional space product constrain dimensions see gure 
modeling digits low resolution model generate images approximate shape digit local models ensure small image patches contain segments stroke correct ne structure 
modeling sentences expert ensure tenses agree ensure number agreement subject verb 
fitting poe data appears dicult necessary compute derivatives partition function renormalization 
shall see derivatives estimated easily 
learning products experts consider individual expert models tractable compute derivative log probability data vector respect parameters model 
includes mixtures gaussians mixtures gaussian uniform popular models including hidden markov models linear dynamical systems 
combine individual expert models follows dj pm dj pm data vector discrete space parameters individual model pm dj probability model index possible vectors data space 
continuous data spaces sum replaced appropriate integral 
individual expert data give high probability observed data waste little probability possible rest data space 
poe data expert gives high probability unobserved regions data space provided experts disagree unobserved regions probable 
poe set observed iid data vectors need compute derivative log likelihood observed vector poe 
log dj log pm dj log pm second term rhs eq 
just expected derivative log probability expert fantasy data generated poe 
assuming individual experts time series models sequence 
tractable derivative diculty estimating derivative log probability data poe generating correctly distributed fantasy data 
done various ways 
discrete data possible rejection sampling expert generates data vector independently process experts happen agree 
typically inecient 
markov chain monte carlo method uses gibbs sampling typically ecient 
gibbs sampling variable draws sample posterior distribution current states variables 
data hidden states experts updated parallel conditionally independent 
individual experts property components data vector conditionally independent hidden state expert possible update components data vector parallel hidden states experts 
gibbs sampling alternate parallel updates hidden visible variables 
get unbiased estimate gradient poe necessary markov chain converge equilibrium distribution practice brief gibbs sampling works remarkably reasons discussed 
simple example poe data distributions factorized product lower dimensional distributions 
demonstrated gure 
experts mixture uniform single axis aligned gaussian 
tted model tight data cluster represented intersection gaussians elongated di erent axes 
conservative learning rate tting required updates parameters single gibbs iteration sucient estimate derivatives fantasy data 
update parameters computation performed observed data vector 
data calculate posterior probability selecting gaussian uniform dot datapoint 
data tted product experts 
ellipses show standard deviation contours gaussians expert 
experts initialized randomly located circular gaussians variance data 
unneeded experts remain vague mixing proportions gaussians remain high 
expert compute rst term rhs eq 

expert stochastically select gaussian uniform posterior 
compute normalized product selected gaussians gaussian sample get reconstructed vector data space 

compute negative term eq 
reconstructed vector logarithmic opinion pools idea combining opinions multiple di erent expert models weighted average log probability domain far new genest research focussed nd best weights combining experts learned separately training experts cooperatively 
geometric mean set probability distributions attractive property kullback liebler divergence true distribution smaller average divergences individual distributions datapoints generated prolonged gibbs sampling experts tted gure 
gibbs sampling started random point range data parallel iterations annealing 
notice tted model generates data grid point missing real data 
jj mq wm wmd jjq wm mq wm 
individual models identical 
di erence sides log clear bene combining experts comes fact log small disagreeing unobserved data 
learning population code poe ective model expert quite broadly tuned dimension precision obtained intersection large number experts 
shows happens experts type previous example tted dimensional synthetic images contain edges 
edges varied orientation position intensities side edge 
intensity pro le edge sigmoid 
expert learned variance pixel variances varied individual experts specialize small subset dimensions 
image half experts high probability picking gaussian means dimensional gaussians product experts mixture gaussian uniform 
poe tted images contained single intensity edge 
experts ordered hand qualitatively similar experts adjacent 
uniform 
products chosen gaussians excellent reconstructions image 
experts top gure look edge detectors various orientations positions polarities 
experts symmetry locate edge 
di erent sets edges opposite polarities di erent positions 
single gibbs iteration sucient learning 
initializing experts ecient way initialize poe train expert separately forcing experts di er giving di erent training cases training di erent subsets data dimensions different model classes di erent experts 
expert initialized separately individual probability distributions need raised fractional power create initial poe 
necessarily easy probabilities assigned expert model need renormalized involves summing entire data space 
fortunately expert really need assign probability data vector 
long expert assigns positive number data vector number normalized create probability normalization real fantasy data derivative log normalization term cancels 
experts trained cooperatively real requirement expert easy compute derivative respect expert parameters log number assigns vector data space 
comparison directed acyclic graphical models inference poe trivial experts individually tractable hidden states di erent experts conditionally independent data 
relevant models biological perceptual systems able inference rapidly 
alternative approaches directed acyclic graphical models su er explaining away phenomenon 
graphical models densely connected exact inference intractable necessary resort slow iterative techniques approximate inference saul jordan crude approximations ignore explaining away inference rely learning algorithm nd representations inference technique damaging hinton 
unfortunately ease inference poe balanced diculty generating fantasy data 
done trivially ancestral pass directed acyclic graphical model requires iterative procedure gibbs sampling poe 
brief gibbs sampling sucient learning diculty generating unbiased fantasy data major problem 
addition ease inference results conditional independence experts data poe subtle advantage generative models rst choosing values latent variables generating data vector latent values 
model single hidden layer latent variables independent prior distributions strong tendency posterior values latent variables approximately marginally independent model tted data 
lack marginal independence viewed coding ineciency data communicated rst specifying states latent variables inappropriate independent prior specifying data hidden values 
reason little success attempts learn generative models greedy bottom way 
poe experts independent priors latent variables di erent experts marginally dependent high mutual information fantasy data generated poe 
rst hidden layer learned greedily may lots statistical structure latent variables second hidden layer capture 
poe boltzmann machines boltzmann machine learning algorithm hinton sejnowski theoretically elegant slow networks interconnected hidden units su ers strange ects weights driven away regions learning signal zero mean high variance 
unsupervised boltzmann machine visible layer hidden layer connections probability generating visible vector proportional product probabilities visible vector generated hidden units acting 
type boltzmann machine poe expert hidden unit 
inference tractable restricted architecture states hidden units conditionally independent data 
poe learning algorithm exactly equivalent boltzmann learning algorithm case 
consider derivative log probability data respect weight ij visible unit hidden unit notation hinton sejnowski rst term rhs eq second term cancels denotes expected value visible units states determined solely single expert hidden unit major advantage poe boltzmann machine individual experts complex single symmetrically connected hidden unit 
possible take advantage tractability quite complicated experts hidden markov models capture lot structure 
furthermore expert initialised sensibly poe learning algorithm start logarithmic opinion pool sensible experts 
gibbs iteration works absence single convincing argument shows gibbs iteration rely product convincing arguments 
simulations show works 
addition simulations described experts simulations run expert mixture gaussians expert boltzmann machine hidden unit 
simulations single gibbs iteration sucient 
second argument relies fact high dimensional datasets data nearly lies close lower dimensional smoothly curved manifold 
poe needs nd parameters sharp ridge log probability low dimensional manifold 
starting point manifold ensuring point higher log probability typical reconstructions latent variables experts poe ensures probability distribution right local curvature 
possible poe accidentally assign high probability distant unvisited parts data space log surface smooth height local curvature con strained data points 
possible nd eliminate points performing prolonged gibbs sampling data just way improving learning essential part 
third argument individual experts behave quite sensibly estimate derivative second term eq 
just random noise 
case derivative rst term force expert model data random noise simply experts di er randomly useful systematic way 
fourth argument single gibbs iteration move direction equilibrium distribution tend produce right sign rhs eq 
magnitude lot smaller 
discussion previous attempts learn representations adjusting parameters cancel ects brief iteration recurrent network hinton mcclelland seung formulated approximate gradient descent full generative model 
poe unsupervised technique 
classi cation comparing log probabilities separate class speci poe 
normalization term eq 
unknown di erence log normalization terms poe single number easily estimated 
poe experts boltzmann machine single hidden unit learned excellent model realvalued images digit 
images usps cedar rom normalized highly variable style 
poe learned localised features yielded perfect reconstructions 
poe trained models label separate test images 
test image represented coordinates unnormalized log probabilities models 
maximum margin linear separator training data errors test data extremely close decision boundary 
possible de ne expert conditional probability model produces probability distribution output vectors input vector 
expert latent variables poe learning algorithm applied 
poe particularly promising sequential data allow expert tractable fairly powerful model 
static images worth exploring sophisticated tractable individual experts 
example expert tree structured gaussian belief net 
product experts di erent trees provide proper probabilistic image model eliminate block boundary artifacts 
belief nets rst trained separately trained poe 
research funded gatsby foundation 
zoubin ghahramani peter dayan david mackay guy 
andy brown david lowe helpful discussions 
genest 
combining probability distributions critique annotated bibliography 
statistical science 
hinton dayan frey neal 
wake sleep algorithm selforganizing neural networks 
science 
hinton mcclelland 
learning representations recirculation 
anderson editor neural information processing systems american institute physics new york 
hinton sejnowski 
learning relearning boltzmann machines 
rumelhart mcclelland editors parallel distributed processing explorations microstructure cognition 
volume foundations mit press saul jaakkola jordan 
mean eld theory sigmoid belief networks 
journal arti cial intelligence research 
seung 
learning continuous attractors recurrent net 
advances neural information processing systems 
jordan kearns solla eds 
mit press cambridge mass 
