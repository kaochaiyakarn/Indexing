initialization iterative refinement clustering algorithms usama fayyad cory reina bradley microsoft research redmond wa usa fayyad microsoft com cs wisconsin edu iterative refinement clustering algorithms means em converge numerous local minima 
known especially sensitive initial conditions 
procedure computing refined starting condition initial efficient technique estimating modes distribution 
refined initial starting condition leads convergence better local minima 
procedure applicable wide class clustering algorithms discrete continuous data 
demonstrate application method expectation maximization em clustering algorithm show refined initial points lead improved solutions 
refinement run time considerably lower time required cluster full database 
method scalable coupled scalable clustering algorithm address large scale clustering data mining 
background clustering formulated various ways machine learning pattern recognition dh optimization bms si statistics literature kr br 
fundamental clustering problem grouping data items similar 
general approach clustering view density estimation problem br 
assume addition observed variables data item hidden unobserved variable indicating cluster membership data item 
data assumed arrive mixture model mixing labels cluster identifiers hidden 
general mixture model having clusters assigns probability data point follows pr pr called mixture weights 
methods assume number clusters known input 
clustering optimization problem finding parameters associated mixture model parameters components maximize likelihood data model 
probability distribution specified cluster take form 
em expectation maximization algorithm dlr cs known technique estimating parameters general case 
finds locally optimal solutions maximizing likelihood data 
maximum likelihood mixture model parameters computed iteratively em copyright american association artificial intelligence www aaai org 
rights reserved 

initialize mixture model parameters producing current model 
compute posterior probabilities data items assuming current model step 
re estimate model parameters posterior producing new model step 
current new model sufficiently close terminate go 
focus initialization step 
initial condition step algorithms define deterministic mapping initial point solution 
em converges finitely point set parameter values locally maximal likelihood data model 
deterministic mapping means locally optimal solution sensitive initial point choice 
shall assume model represents mixture 
associated data point weight posterior probability generated mixture component 
focus mixture models individual component densities multi variate gaussians exp dimensional mean covariance matrix 
little prior initialization methods clustering 
dh question plagues hill climbing procedures choice starting point 
unfortunately simple universally solution problem 
repetition different random selections dh appears defacto method 
presentations address issue initialization assume randomly chosen starting points dh kr 
recursive method initializing means running clustering problems mentioned dh means 
variant consists mean entire data randomly perturbing times 
method appear better random initialization case em discrete data mh 
bms values initial means coordinate axes determined selecting densest bins coordinate 
methods initialize em include means solutions hierarchical agglomerative clustering hac dh mh marginal noise 
em discrete data initialized hac marginal noise showed improvement random initialization mh 
refining initial conditions address problem initializing general clustering algorithm limit presentation results em 
method initialization exists mh compare defacto standard method initialization randomly choosing initial starting point 
method applied starting point provided 
solution clustering problem parameterization cluster model 
parameterization performed determining modes maxima joint probability density data placing cluster centroid mode 
clustering approach estimate density attempt find maxima bumps estimated density function 
density estimation high dimensions difficult bump hunting 
propose method inspired procedure refines initial point point closer modes 
challenge perform refinement efficiently 
basic heuristic severely subsampling data naturally bias sample representatives near modes 
general guard possibility points tails appearing subsample 
overcome problem estimate fairly unstable due elements tails appearing sample 
shows data drawn mixture gaussians clusters means 
left full data set right small subsample shown providing information modes joint probability density function 
points right may thought guess possible location mode underlying distribution 
estimates fairly varied unstable certainly exhibit expected behavior 
worthy note separation clusters achieved 
observation indicates solutions obtained clustering small subsample may provide refined initial estimates true means centroids data clustering clusters overcome problem noisy estimates employ secondary clustering 
multiple subsamples say drawn clustered independently producing estimates true cluster locations 
avoid noise associated solution estimates employ smoothing procedure 
best perform smoothing needs solve problem grouping points solutions having clusters groups optimal fashion 
shows solutions obtained 
true cluster means depicted 
show points obtained subsample second third fourth 
problem determining grouped 
refinement algorithm refinement algorithm initially chooses small random sub samples data sub samples clustered em proviso empty clusters termination initial centers re assigned sub sample 
sets cm clustering solutions sub samples form set cm 
cm clustered means initialized cm producing solution fm refined initial point chosen fm having minimal distortion set cm 
note secondary clustering means clustering em 
reason goal cluster solutions hard fashion solve problem 
procedures secondary clustering including hierarchical agglomerative clustering 
clustering cm smoothing cm avoid solutions corrupted outliers included subsample refinement algorithm takes input sp initial starting point data number small subsamples taken data algorithm refine sp data cm small random subsample data cm em mod sp cm cm cm fms fm kmeans cm cm fms fms fm fm cm fm likelihood argmax fm return fm define functions called refinement algorithm kmeans em mod likelihood 
kmeans simply call classic means algorithm initial starting point dataset number clusters returning set dimensional vectors estimates centroids clusters 
em mod takes arguments kmeans performs iterative procedure classic em slight modification 
classic em converged clusters checked membership 
clusters membership happens clustering small subsamples corresponding initial estimates empty cluster centroids set data elements likelihood current model classic em called new initial 
true solution solutions trial solutions trial solutions trial solutions trial 
multiple solutions multiple samples 

gaussian bumps full sample versus small subsample 
cluster multiple subsamples subsample multiple sample solutions cluster solutions multiple starts select best solution 
starting point refinement procedure heuristic re assignment motivated termination em empty clusters reassigning empty clusters points likelihood maximizes likelihood step 
likelihood takes set estimates cluster parameters means covariances data set computes likelihood data set model 
scalar measures degree fit set clusters dataset 
em algorithm terminates solution locally optimal likelihood function dlr cs 
refinement process illustrated diagram 
scalability large databases refinement algorithm primarily intended large databases 
working small datasets data sets irvine repository applying classic em algorithm different starting points feasible option 
database size increases especially dimensionality efficient accurate initialization critical 
clustering session data set dimensions tens thousands millions records take hours days 
bfr method scaling clustering large databases specifically targeted databases fitting ram 
show accurate clustering achieved improved results sampling approach bfr 
scalable clustering methods obviously benefit better initialization 
method works small samples data initialization fast 
example sample sizes full dataset size trials samples run time complexity time needed clustering full database 
large databases initial sample negligible size 
data set clustering algorithm requires iter iterations cluster time complexity iter 
small subsample typically requires significantly fewer iteration cluster 
empirically reasonable expect iter iter 
specified budget time user allocates refinement process simply determine number subsamples refinement process 
large small proportion refinement time essentially negligible large desirable property refinement algorithm easily scales large databases 
memory requirement hold small subsample ram 
secondary clustering stage solutions obtained subsamples need held ram 
note assume possible obtain random sample large database 
reality challenging task 
guarantee records database ordered property random sampling expensive scanning entire database scheme reservoir sampling 
note database environment data view may exist physical table 
result query may involve joins groupings sorts 
cases database operations impose special ordering result set randomness resulting database view assumed general 
experiments synthetic data synthetic data created dimension 
value data sampled gaussians elements mean vectors true means sampled uniform distribution 
elements diagonal covariance matrices sampled uniform distribution 
number data points sampled chosen times number model parameters 
gaussians evenly weighted 
goal experiment evaluate close means estimated classic em true gaussian means generating synthetic data 
compare initializations 
refinement random starting point chosen uniformly range data 

refinement starting point refined method 
size random subsamples full dataset number subsamples 

refinement subsample size 
distance truth vs dimension ion ined ref ined ref ined log data model vs ion ion ined ref ined ref ined 
comparing performance dimensionality increases classic em computed solution full dataset initial points described estimated means matched true gaussian means optimal way prior computing distance estimated means true gaussian means 
true gaussian means means estimated em full dataset 
permutation determined quantity minimized score solution computed classic em full dataset simply quantity divided average distance true gaussian means estimated em initial starting point 
results summarizes results averaged random initial points determined uniformly range data 
note em solution computed refined initial points consistently nearer true gaussian means generating dataset em solution computed original initial points 
left summarize average distance true gaussian means 
right show corresponding log likelihood data model 
low dimensions effect initialization strong high dimensions expected 
sampling produces better refinement course costs 
non synthetic data sampling results worse solution effect finding modes subsampling reduced 
results public data computational results publicly available real world datasets 
primarily interested large databases hundreds dimensions tens thousands millions records 
data sets method exhibits greatest value 
reason simple clustering session large database time consuming affair 
refined starting condition insure time investment pays 
illustrate large publicly available data set available reuters news service 
wanted demonstrate refinement procedure data sets uci machine learning repository part data sets easy low dimensional small number records 
small number records feasible perform multiple restarts efficiently 
sample size small sub sampling initialization effective 
data sets interest 
report general experience detailed experience data sets illustrate method advocate useful applied smaller data sets 
emphasize refinement procedure best suited large scale data 
refinement algorithm operates small sub samples database run times needed determine initial starting point speeds convergence full data set orders magnitude total time needed clustering large scale situation 
details see irvine ml data repository www ics uci edu mlearn mlrepository html note cluster labeling associated real world databases correspond clusters assigned em objective maximize likelihood 
evaluation results data easy synthetic data truth known 
datasets uci ml repository evaluated method irvine data sets 
discuss set general comments 
image segmentation data set data set consists data elements dimensions 
instances drawn randomly database outdoor images sky foliage cement window path grass 
images represented instances 
random initial starting points computed sampling uniformly range data 
compare solutions achieved classic em algorithm starting random initial starting points initial points refined method 
best measure case report loglikelihood data extracted model 
results follows refinement method log likelihood increase refined refined irvine datasets evaluated refinement procedure data sets fisher iris star galaxy bright data sets low dimensional sizes small majority results interest 
clustering data sets random initial points refined initial points led approximately equal gain entropy equal distortion measures cases 
observe random starting point leads bad solution refinement takes solution 
admittedly rare cases refinement provide expected improvement 
reuters information retrieval data set demonstrate method real difficult clustering task 
reuters information retrieval data set reuters text classification database derived original reuters data set publicly available part reuters corpus available part reuters corpus reuters carnegie group david lewis data consists documents 
document news article topic earnings commodities acquisitions grain copper 
categories belong higher level categories hierarchy categories 
reuters database consists word counts documents 
hundreds thousands words purposes experiments selected frequently occurring words instance dimensions indicating integer number times corresponding word occurs document 
document ir reuters database classified categories 
clustering see www research att com lewis reuters readme txt details data set 
purposes reflect top level categories 
task find best clustering 
reuters results data set clustering entire database requires large amount time chose evaluate results randomly chosen starting conditions 
results shown follows refinement method log likelihood refined refined chart shows significant decrease log likelihood measure best solution 
case samples better 
normalize results case simply divide size data 
document belongs category categories measure quality achieved clustering measuring gain information categories cluster gives pure clusters informative 
quality clusters measured average category purity cluster 
case average information gain clusters obtained refined starting point times higher information gain obtained refining initial points 
concluding remarks fast efficient algorithm refining initial starting point general class clustering algorithms 
refinement algorithm operates small subsamples database requiring small proportion total memory needed store full database making approach appealing largescale clustering problems 
procedure motivated observation subsampling provide guidance regarding location modes joint probability density function assumed generated data 
initializing general clustering algorithm near modes true clusters follows clustering algorithm iterate fewer times prior convergence 
important clustering methods discussed require full data scan iteration may costly procedure largescale setting 
believe method ability obtain substantial refinement randomly chosen starting points due large part ability avoid empty clusters problem plagues traditional em 
refinement reset empty clusters far points reiterate em algorithm starting point obtained refinement method lead subsequent clustering algorithm bad solution 
intuition confirmed empirical results 
refinement method far context em 
note method generalizable algorithms example method initialize means algorithm bf 
generalization possible discrete data means defined 
key insight algorithm cluster data cluster subsamples 
algorithm produce model 
model essentially described parameters 
parameters continuous space 
stage clusters clusters step algorithm refine section remains means algorithm step 
reason means goal stage find centroid models case harsh membership assignment means desirable 
br banfield raftery model gaussian non gaussian clustering biometrics vol 
pp 

bishop 
neural networks pattern recognition 
oxford university press 
bms bradley mangasarian street 

clustering concave minimization advances neural information processing systems mozer jordan petsche eds 
pp mit press 
bfr bradley fayyad reina scaling clustering algorithms large databases proc 
th international conf 
knowledge discovery data mining kdd 
aaai press 
cs cheeseman stutz bayesian classification autoclass theory results pp 

mit press 
dlr dempster laird rubin maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
dh duda hart pattern classification scene analysis 
new york john wiley sons 
fhs fayyad haussler stolorz 
mining science data 
communications acm 
bf bradley fayyad refining initial points means clustering proc 
th international conf 
machine learning 
morgan kaufmann 
fisher 
knowledge acquisition incremental conceptual clustering 
machine learning 
fukunaga statistical pattern recognition san diego ca academic press 
jones note sampling tape file 
communications acm vol 
kr kaufman rousseeuw 
finding groups data new york john wiley sons 
mh meila heckerman 
experimental comparison clustering methods microsoft research technical report msr tr redmond wa 
rasmussen clustering algorithms information retrieval data structures algorithms frakes eds pp 
new jersey prentice hall 
scott multivariate density estimation new york wiley 
si ismail means type algorithms generalized convergence theorem characterization local optimality 
ieee trans 
pattern analysis machine intelligence vol 
pami 
silverman density estimation statistics data analysis london chapman hall 
thiesson meek chickering heckerman 
learning mixtures bayesian networks microsoft research technical report tr redmond wa 
