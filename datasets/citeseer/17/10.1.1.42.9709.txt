ieee november december phenomenal improvements microprocessor performance place significant demands memory systems requiring low latency high bandwidth stream operands 
researchers point dram access latencies measured processor cycles growing request misses caches may eventually take hundreds cycles satisfy 
researchers proposed techniques mitigate penalties long memory latencies lockup free caches cache conscious load scheduling hardware software prefetching stream buffers speculative loads execution multithreading data value prediction instruction reuse 
techniques reducing impact access latencies cost increasing program bandwidth requirements 
latency tolerance reduction techniques may increase processor memory bandwidth needs causing processor request stream operands time causing processor request data memory 
turn techniques may cause processor stall due queueing memory system 
differentiate intrinsic latency access latency added queueing memory system results limited bandwidth 
long latencies increased bandwidth requirements constitute memory wall eventually inhibit improved microprocessor performance 
designers employ range design decisions new technologies produce balanced cost effective systems 
extent long latency bandwidth requirements affect performance determine techniques technologies affordable worth effort implementing 
solutions trade improved latency increased traffic higher bandwidth increased latency relative effects components memory accesses performance important 
quantify compare performance impacts memory latencies finite bandwidth 
show implementation aggressive latency tolerance techniques aggravates stalls due finite memory bandwidth significant stalls resulting uncongested memory latency 
expect memory bandwidth limitations processor pins drive significant architectural change reasons continuing progress processor design increase issue rate instructions 
advances include architectural innovation wider issue speculative execution forth circuit advances faster denser logic 
extent latency tolerance techniques successful speed retirement rate instructions requiring memory operands unit time 
latency tolerance techniques increase absolute amount memory traffic fetching data needed 
packaging costs power cooling considerations increasingly affect costs resulting slower costly increases chip bandwidth chip processing memory 
factors force architectural system level change 
range tech execution driven simulation measures time spec benchmarks spend stalled memory latency limited memory bandwidth computing 
limited bandwidth affect processor design doug burger james goodman alain university niques improve effective memory bandwidth includes wider faster connections memory larger chip caches traffic efficient requests efficient chip caches logic dram integration memory centric architectures 
earlier version article predicted memory bandwidth particularly processor pins drive significant changes processors memory systems 
special issue advanced memory architectures affirms claims 
ensuing years seen high bandwidth dram interfaces beginnings products integrating memory logic 
articles special issue illustrate change describe high bandwidth dram interfaces rambus describe logic dram integration msm 
decomposing execution time corresponding improvements memory system performance gap processors main memory increases percentage time processor spends stalled memory increase 
due complexity modern processors memory hierarchies simple metric rate example sufficiently quantifies performance impact imperfect memory system 
understand time spent complex processor divide execution time categories processing latency stalls bandwidth stalls 
processor time time processor fully utilized partially utilized stalled due lack instruction level parallelism ilp 
latency time number cpu cycles lost due memory latencies 
latency time mean overhead memory latencies system example latencies reduced adding bandwidth memory hierarchy levels 
bandwidth time number cpu cycles lost contention memory system insufficient bandwidth hierarchy levels 
partitioning scheme superior metric average memory access time separates raw access latency bandwidth restrictions translates directly processor performance 
example simultaneous cache misses lockup free cache appear cache latency processor counted distinct misses calculating average memory access time 
fractions time program spends categories processing latency stalls bandwidth stalls 
number cycles program executes realistic system 
calculate measuring number cycles takes program execute assuming perfect memory hierarchy memory request returned cycle 
number program cycles assuming perfect memory calculate stalling time raw memory latencies measuring system effectively infinite bandwidth amount data may moved level memory hierarchy single cycle 
requests incur access latencies cache memory banks 
time spent run program system number cycles spent stalling memory latency add contention finite bandwidth back measurement program execution time number cycle increase infinite bandwidth case number cycles lost due finite bandwidth 
fraction time spent stalling due insufficient bandwidth table lists latency tolerance optimizations processor improvements shows predict change improvements 
row normalized fraction bandwidth stalls increases optimization 
latency reduction techniques 
improved techniques reducing tolerating memory latency reducing increase percentage execution time spent stalled due insufficient memory bandwidth 
techniques reduce latency related stalls increase total traffic main memory processor 
furthermore reducing increases processor bandwidth rate processor consumes produces operands reducing total execution time 
combination lockup free caches careful scheduling memory operations effectively hides memory latencies 
technique increase amount traffic main memory lockup free caches worsen bandwidth stalls allowing multiple memory requests issue making queueing delays possible memory system 
furthermore presence lockup free caches encourage speculative execution 
ieee micro memory bandwidth table 
estimated effects execution divisions 
effects technique plb latency reduction lockup free caches intelligent load scheduling hardware prefetching software prefetching speculative loads multithreading larger cache blocks processor enhancements faster clock wider issue data value speculation instruction reuse 
speculative threads processing latency stalls bandwidth stalls effect clear software hardware prefetching techniques increase traffic main memory 
may prefetch unneeded data prefetch data evicted may evict needed data cache causing extra cache stream buffers prefetch unnecessary data stream 
falsely identify streams fetching unnecessary data 
speculative fetching techniques lifting loads conditional branches increase unnecessary memory traffic speculation incorrect 
multithreading increases processor throughput switching different thread long latency operation occurs 
frequent switching threads increase interference caches translation look aside buffer tlb causing increase cache misses total traffic 
poorer cache performance resulting increased size threads combined working set may offset performance gains improved latency tolerance 
simultaneous multithreading allows multiple instructions independent threads issued simultaneously increasing contention seen individual jobs 
larger block sizes may decrease cache rates 
rate reduction occurs coarser granularity address space coverage reduced number blocks cache overcomes reduction misses obtained fetching larger blocks 
larger blocks reduce rate increased traffic may cause bandwidth stalls outweigh rate improvements 
advanced processors 
improvements microprocessors latency reduction techniques increase needed bandwidth processor module boundary 
processors get faster consume operands higher rate 
faster processor clocks run programs shorter time increasing chip bandwidth requirements decreasing 
wider issue processors exploit ilp similarly reduce execution time increase needed bandwidth 
speculation increase bandwidth requirements 
data value speculation successful allows computation proceed quickly increasing needed rate operands total number operands data computed loaded memory verify speculation 
instruction reuse conversely replaces computation memory accesses chip table lookups entry listed table reduces microprocessors rely coarse grained speculative threads improve ilp multiscalar processors increase memory traffic squash task incorrect speculation 
multiple distinct execution units processors execute different parts instruction stream simultaneously 
execution may reduce locality shared lower level caches increasing rate total traffic 
emergence single chip multiprocessors increase number data loaded cycle manner similar multiscalar processors 
increased bandwidth requirements result primarily multiple concurrently running contexts threads increase shared cache interference 
primary barrier implementation single chip multiprocessors transistor availability chip memory bandwidth 
processor loses performance due limited pin bandwidth multiple chip processors lose far performance reason 
multiple threads share working sets chip bandwidth issue 
majority microprocessor enhancements discussed aggravate bandwidth limitations exist designers solutions mitigate performance impact limited bandwidth 
execution time decomposition hypothesized bandwidth stalls increase processors memory hierarchies aggressive latency tolerance 
test hypothesis simulated execution time systems benchmarks spec suite 
integer benchmarks compress vortex ijpeg perl floating point benchmarks swim su cor tomcatv 
ran benchmark test input set spec simulating completion 
train input set ijpeg compress reduced number iterations vortex 
simplescalar tool set measure execution time system execution driven simulation 
simplescalar contains detailed order speculative processor simulator executes binaries compiled simplescalar instruction set similar mips 
modified memory system simulate lockup free caches accurate bus contention address translation 
simulations benchmark measured components execution time described earlier 
measure simulated processor load store completes cycle 
measured simulating memory hierarchy buses sufficiently wide transmissions complete bus cycle 
measured simulating full memory system accounting contention 
simulated different processor configurations benchmark evaluate effect various latency tolerance techniques 
latency tolerance techniques evaluated increased cache line sizes lockup free caches order execution speculative loads prefetching 
implemented prefetching scheme tagged prefetch 
assumed blocking caches service hits processing table page lists specific experiments labeled ran benchmark 
experiments order issue way superscalar processor twolevel adaptive gshare branch predictor load store units 
assumes cache blocks twice big level experiment keeping total cache size fixed 
experiment uses lockup free cache 
experiments assume way order issue processor register update unit ruu structure allows loads issue speculatively 
experiments identical tagged prefetching uses aggressive processor core december table lists processor parameters experiment clock speed size branch prediction table load store queue ruu entries forth 
table lists parameters memory hierarchy 
addition parameters assumed multiplexed data address lines main memory bus channels bidirectional memories return critical word 
displays execution times experiment 
execution times benchmark normalized perfect memory system experiment experiment results show simple processors memory systems experiment benchmarks measured spend arithmetic mean cycles stalled memory 
aggressive modern processors experiment incorporate features previously discussed article spend fully cycles stalled memory simulated benchmarks 
aggressive processors spend time stalled limited bandwidth mean total cycles experiment opposed mean experiment limited bandwidth accounts preponderance memory stalls benchmarks experiment latency stalls represent large fraction memory stalls perl tomcatv vortex 
effect due poor instruction cache performance benchmarks 
latency stalls increase perl tomcatv vortex going experiment faster clock experiment instruction cache misses expensive interconnect speed scaled proportionally faster clock absolute memory bank access times changed 
ijpeg differs rest benchmarks spends little time stalled memory 
stream excellent locality data instruction cache rates low level data cache close level instruction cache level unified cache 
benchmarks results show memory bandwidth stalling major dominant component total execution time 
solutions limited chip bandwidth growing relative memory access latencies potential degrade program performance seriously 
results show cases aggressive latency tolerance techniques implemented discretion potential worsen performance memory bandwidth access latencies primary bottleneck program 
potential latency tolerance particularly acute processors rely heavily speculation achieve high performance 
table analysis see similar table range solutions increase actual effective memory bandwidth 
high bandwidth drams increase bandwidth may increase request latency 
larger caches improve average access latency reduce traffic 
traffic ieee micro memory bandwidth compress ijpeg perl su cor swim tomcatv vortex experiment spec benchmark normalized execution time limited stalls raw latency stalls compute time 
performance breakdown execution time 
number atop bar represents table 
processor simulation parameters 
experiment parameter processor order issue order issue clock speed mhz ghz ruu slots entries branch predictor cache blocking lockup free block sizes hw prefetch efficient requests analog latency tolerance techniques reduce traffic may exacerbate effectively may increase decrease 
efficient chip caches reduce may increase overhead maintaining efficient cache increased hit latency outweighs benefit reduction misses 
logic dram integration increase larger caches 
memory centric architectures certainly lend improving memory system performance hampered need implementation acceptance path 
high bandwidth dram interfaces 
obvious solution limited memory bandwidth provide higher bandwidth memory system effectively buying bandwidth 
articles special issue deal high bandwidth dram interfaces rambus synchronous link expected provide memory bandwidth generation highperformance microprocessors 
key question providing memory bandwidth generation possible providing generation cost effective 
particular providing pins low cost significant challenge near 
rate increase processor pins traditionally slower transistor count 
large increases pin counts occurred breakthroughs packaging technology undoubtedly lie horizon issues reliability power especially cost prevent pins sustaining growth numbers commensurate growth rate processor performance 
statement especially true costs package grow number pins 
shows trends pin performance chip bandwidth 
compiled data hand processors original manuals back issues microprocessor report 
axes log scales 
axes linear scale 
plots number pins processor 
see dashed line pin counts increasing year 
striking result plots processor performance pin versus time 
raw performance pin increasing despite rapid increase pin count shown 
packages buses designed provide sufficient chip december table 
memory system simulation parameters 
structure parameters cache kbytes kbytes chip cycle access bus bits wide bus processor clock cache mbytes way set associativity chip ns access memory bus bits wide bus processor clock memory ns access bank conflicts year number pins ultrasparc pentium harp pa year mips pin ultrasparc pentium harp pa year mips pin mbytes harp pentium pa ultrasparc 
physical microprocessor trends pin count increases performance increases pin performance pin bandwidth 
width generation processors 
plots raw performance package bandwidth ratio time shows performance increases quickly growth raw peak package bandwidth 
earlier version article measured traffic ratios range cache configurations spec benchmarks 
traffic ratio number bytes bus traffic generated cache misses divided number bytes traffic cache 
traffic ratio depends cache size mean traffic ratio experiments 
value rough estimate traffic ratios extrapolate pin growth processor performance see sort packages need 
assume annual growth rate sustained microprocessor performance growth rate past decade estimate increases bandwidth requirements 
assuming trends persist chip traffic ratios remain see decade processor package pins 
large package bandwidth requirements pin factor greater today 
processors limited chip bandwidth possibilities exist processor 
industry build cost effective pin packages clocked ghz 
industry build cost effective package pins clock ghz 
techniques may improve effective pin bandwidth today reducing need huge packages 
turn discussion effective pin bandwidth may increased simply paying raw bandwidth mitigating high costs packages 
larger chip caches 
larger caches improve effective bandwidth sending fewer requests misses interconnect 
growing chip caches soon reach multi megabyte range 
caches currently account transistor budget microprocessors fraction increasing 
eventually transistor processors available contain tens hundreds megabytes chip cache memory 
caches large able hold working sets applications programs access patterns aren amenable caching 
programs working sets big fit caches 
today designers consciously large onchip caches increase effective pin bandwidth cutting actual bandwidth needed processor package 
current hewlett packard pa series microprocessors contain chip caches extremely small caches primary cache chip 
organization required large high performance package pins pa 
announced pa reverses philosophy dramatically cache organization includes mbytes chip cache reducing package size contains mere pins 
power cost reduced larger package pa saves die area having fewer pins reducing total pad driver circuitry 
traffic efficient requests 
effective bandwidth may increased sending fewer requests increasing information content utility bytes sent 
requesting smaller blocks processor gets implicit prefetching increasing sends fewer unneeded bytes interconnect reducing previous measured traffic generated minimal traffic cache organized minimize bus traffic small blocks optimal replacement full associativity write validate write back policy 
results showed factors small blocks far largest contributing factor reducing traffic small blocks accounted large differential averaged benchmarks 
sending smaller blocks increases transmitted tag arbitration overheads buses generally better burst transfers factors weighed benefit sending smaller blocks 
implementation conceivably handle smaller requests exploding rate sector cache small subblocks subblock entire address block loaded depending expected block access pattern 
method improving effective bandwidth interconnect compression 
researchers proposed implemented schemes compression data addresses code 
schemes increase effective bandwidth memory expense extra hardware cpu memory case data address compression 
current technology trends computation growing cheaper relative expensive communication compression attractive 
efficient chip caches 
caches quite effective capturing temporal spatial locality dynamic stream crude fashion far optimal buffering active data 
group wood fraction data caches live referenced evicted cache quite small typically twentieth third 
implication result caches potential better managed 
results confirm hypothesis cache optimally managed terms minimizing traffic produced ieee micro memory bandwidth table 
effects memory bandwidth solutions 
effect solution plb high bandwidth drams larger chip caches bandwidth efficient requests efficient caches logic dram integration memory centric architectures times traffic 
huang shen confirmed result study minimal required bandwidths current generation processors 
chip memory naming mapping restrictions available chip buffering chip bandwidth sufficient 
question course extract capability permitting feasible implementation 
way improve efficiency cache cache objects finer granularity cache line exhibit spatial locality 
small blocks increase tag overhead perform poorly larger objects spatial locality 
decoupled sector cache allows finer grain access preventing space unused lines sector wasted storing lines sectors 
conflicts cache may remove lines effectively killing line 
showed reduce number variability conflicts skewed associative caches 
tyson proposed bypassing scheme address capacity misses cache statically dynamically determining data little temporal locality loading data cache logic dram integration 
putting processor die package main memory eliminate need expensive high bandwidth interconnects 
industry steps direction embedded processors graphics controllers see articles special issue msm academic research working area group berkeley iram group looking issues related processor memory integration 
logic dram manufacturing processes fundamentally different 
dram processes typically support multiple layers polysilicon metal layers threedimensional structures maximizing capacitor area 
logic processes tend metal levels optimized transistor switching speed 
dram cells logic process particularly dense estimates vary times dense optimized dram process 
gates dram process slower logic process counterparts 
furthermore types processes diverging 
benefits having denser processor memory cells faster dram gates may drive development hybrid processes don match optimized counterparts density speed improve system performance 
addition average number memory chips lower systems pcs workstations dropping may decrease decade 
processor storage capacities converging average main memory sizes personal computers albeit slowly 
trends conceivable system memory may someday reside single processor chip module personal computers 
question comes play expand system memory add integrated chips just reintroduce dumb ram 
integrated chips added programs run 
memory centric architectures 
processing cheap communication expensive pins increased wire delays designers put functional units storage call memory centric architectures 
challenges architectures distributing data effectively orchestrating flow control 
certain regular fine grained single instruction multiple data simd types codes processing memory approach small processors embedded memory arrays show large performance improvements 
case computational ram cram memory pim research 
application base approach limited 
proposed evaluated memory centric architecture called targets scalar hard parallelize codes 
processors coupled regions main memory processor runs program redundantly broadcasting operands local memory processors 
processor needs operand remote memory bank need request operand 
simply waits operand arrive sent processor remote memory 
currently extending model allow processors select dynamically computational task operands local execute code send updated state bundled register values nodes 
implementing support fine grained data migration aggressive speculation communication control decisions achieve performance gains base model 
architecture match world limited memory bandwidth wire delays chip communication delays dominate performance 
computer system exhibit cost performance designers ensure processor supporting memory system balance 
growing cost communication manifested growing number cycles required main memory accesses caused designers implement latency tolerance techniques prefetching nonblocking caches order execution 
techniques mitigate memory pin bandwidth limitations cases aggravate performance losses increasing chip contention 
paying high cost fast wide path dram part solution techniques reduce bandwidth requirements 
designers implement november december designers ensure processor supporting memory systems balance 
expensive main memory system interface provides processor sufficient rate operands 
large chip caches reduce number chip accesses 
smarter cache management may reduce chip traffic 
moving main memory closer processor moving part processor reduce frequency requests go chip 
farther new memory centric architectures may developed specifically improve memory system cost performance 
solutions eventually synergistically provide sufficiently high bandwidth memory system acceptable cost 
acknowledgments ken including revised special issue steve diamond marie english ieee micro giving opportunity submit 
funding sources national science foundation intel research council sun microsystems generous support possible 

burger goodman memory bandwidth limitations microprocessors proc 
rd ann 
int symp 
computer architecture assoc 
computing machinery new york pp 


sohi breach multiscalar processors proc 
nd ann 
int symp 
computer architecture acm pp 


buffer block prefetching method ibm tech 
disclosure bull vol 
pp 


sohi instruction issue logic high performance interruptible multiple functional unit pipelined computers ieee trans 
computers vol 
pp 


rudolph creating wider bus caching techniques proc 
int symp 
high performance computer architecture ieee computer society press los alamitos calif pp 


park dynamic base register caching technique reducing address bus width proc 
th ann 
int symp 
computer architecture acm pp 


vliw architecture trace scheduling compiler proc 
second symp 
architectural support programming languages operating systems acm pp 


wood hill kessler model estimating trace sample ratios tech 
report tr computer sciences dept univ wisconsin madison wisc 

huang shen limit study memory requirements value reuse profiles proc 
th int symp 
microarchitecture ieee cs press pp 


decoupled caches low tag implementation cost low ratio proc 
st ann 
int symp 
computer architecture acm pp 


tyson modified approach data cache management proc 
th int symp 
microarchitecture ieee cs press pp 


burger goodman architectures proc 
th ann 
int symp 
computer architecture acm pp 

doug burger phd candidate university wisconsin madison 
dissertation topic memory hierarchies advanced microprocessors 
burger received ms university wisconsin madison bs yale university computer science 
intel foundation graduate fellow student member ieee computer society acm 
james goodman professor chair computer sciences university wisconsin madison 
current research focuses high performance memory systems computer systems 
goodman received phd university california berkeley 
early contributor multiprocessor snooping cache literature actively participated development ieee std futurebus scalable coherent interface 
published papers areas cache coherence algorithms sharedmemory multiprocessor architectures database systems interconnection networks virtual memory memory register organization memory systems design 
alain phd candidate university wisconsin madison 
dissertation research concerns efficient synchronization primitives sharedmemory multiprocessors 
received ms university wisconsin madison computer science diploma swiss federal institute technology eth 
send correspondence article doug burger university wisconsin madison computer sciences dept west dayton st madison wi cs wisc edu 
reader interest survey indicate interest article circling appropriate number reader service card 
low medium high ieee micro memory bandwidth 
