adaptive context sequential prediction lossless audio compression abu jaakko astola signal processing laboratory tampere university technology box sf tampere finland submitted signal processing elsevier science name address correspondence abu signal processing laboratory tampere university technology box sf tampere finland contains pages figures tables 
keywords lossless compression adaptive prediction context algorithm audio compression propose adaptive context prediction sequential mode lossless audio compression 
show lossless compression algorithms sequential context prediction achieve better compression results linear prediction 
distinct algorithms proposed evaluated audio signal sampled khz bits sample 
context quantization prediction algorithms similar algorithm previously proposed image compression new solutions provided modelling errors collecting coding statistics 
rst algorithm uses histogram bucketing small number contexts conjunction arithmetic coder 
second algorithm uses parametric modelling errors large number contexts conjunction golomb rice encoding 
keywords lossless compression adaptive prediction context algorithm audio compression lossless compression high quality audio signals signi cant topic research considered desirable feature implemented option new high quality audio applications digital versatile disks 
lossless audio compression may appear necessary audio speech archiving audio signal undergoes multiple encoding decoding operations 
consider lossless compression high quality audio signal sampled khz bits sample 
application classical universal coding methods lempel ziv related algorithms context algorithm original forms provide solution lossless compression audio signal sampled bits sample ective bits sample 
universal algorithms proven asymptotically optimal stationary sources complexity underlying models add extra term best achievable coding rate 
additional term asymptotically decreasing zero important usual sizes les range mbytes 
solution problem constrain underlying model context algorithm apriori information nature characteristics signals compressed 
way universality coding scheme limited particular constrained class signals coding performance improved 
properties scaling extrapolation capability nite dynamic range output signal apriori enforced modelling stage audio signal 
order reduce dimensionality context tree states di er signi cantly lumped prevent attening resulting distribution optimal linear predictors may designed sample site order center distributions lumping states 
combination linear prediction models largely audio speech modelling compression applications powerful context algorithm proven encode optimal rates markov generally tree sources considered lossless audio compression 
studies performed lossless image compression set input signal usually restricted involved alphabet 
major problems arise dealing large size alphabets selection quantized contexts coding mechanism collecting statistics error coding 
examine solutions application context algorithm audio compression prediction model operates sequential mode parameters need transmitted context speci linear predictor updated recursive squares rls algorithm intercept model estimated auxiliary contexts selected order statistics previous samples histogram bucketing modelling error probability mass function pmf arithmetic encoding encode errors estimated pmf alternatively parametric modelling errors conditioned re ned contexts possible golomb rice encoding 
context adaptive prediction context algorithm distribution current symbol selected distributions jx jx jx previous performance code length encoding 
di erent contexts grouped context tree having nodes possible values depth nodes depth nodes depth level dictated memory resources 
tree grown message encoded contexts seen far 
node context associated pmf parameters histogram seen far context 
main power algorithm stems rule selecting contexts associated pmf histogram successful coding performed far 
clearly long context estimate jx poor data seen context context dilution situation short context distribution jx intuitively conditioning meaningful variables entropy decreases 
algorithm optimal encoding markovian sources extremely complex large size alphabets 
context algorithm prediction predicting arg max xjx policy context length 
main challenge image audio compression applications mimic main principles algorithm sources large size alphabet 
di erent solutions considered context algorithm compression graylevel images alternative techniques shown ective context quantization error modelling 
approach take similar 
adaptive linear predictors di erent di erent context 
complexity issue forces select small number contexts quantized versions context mask 
parameters linear predictor associated context updated time context occurs 
decide length context line extensive experiments audio speech material 
subsection describes context selection procedure successful extensive experiments 
context tree model sample audio signal select contextual information processing context mask containing past samples prediction errors 
values context mask increasingly ordered resulting introduce classi cation procedures nding small number classes main contexts sm nm tree classi cation procedure re ecting energy signal prediction errors second nding larger number contexts secondary contexts hasse diagram selection re ecting order statistics equivalently texture samples associate parameters counters adaptive predictor coder nodes finite state machine fsm structure composed tree terminal nodes tree hasse diagram 
fsm parse sequence samples context mask resulting sequence fsm nodes labeled 
parameters associated sequence nodes compute actual prediction 
fsm counters parameters associated nodes time denoted adaptive linear prediction performed recursive squares rls algorithm forgetting factor 
parameters linear predictor order di erent di erent primary contexts np errors incurred primary context reduced account accumulated bias ner context structure 
extend linear predictor extra term intercept sm ss adapted errors resulting prediction np term compensate adaptively bias predictions collected secondary context 
note intercept depends primary secondary contexts parameters depend primary context 
order statistics context selection range decomposition discuss particular representation raw conditional information fx xn call range decomposition 
operator tm 
thresholding level applies integer value yield tm thresholding operation vector xn de ned component wise thresholding tm tm xn 
resulted increasingly ordering vector denoted 
denote binary vector obtained thresholding vector xn level satisfying easy observe binary vector results values vector selection tree primary context entries equal entries equal 
vector hamming weight identity straightforward prove establishes mapping set fx xn set context selection select primary conditioning context predictors quantizing di erent variables accounts order statistics samples inside prediction mask depends error range signal inside prediction mask de ned note average di erences absolute values error preceding position linearly combined give tested performances coding system di erent structures quantization tree tree structure compromise selectivity tree robustness best quantization parameters respect di erent nature audio image les 
selection tree primary context sm leaves sm nm quantization intervals interval limits alphabet size equal set experimentally 
primary context information location inside context mask value occurred neglected 
de ne secondary context takes account order samples inside context mask 
primary context node sm secondary context depend values binary vectors 



hasse cube state transition diagram secondary context 
top bottom path answer questions establish values vectors fsm secondary context node corresponds binary vector note experiments 
recognize structure hasse diagram nodes nodes reached di erent paths starting root 
diagram corresponds large context tree structure nodes lumped 
context fsm obtained concatenating hasse diagram leaf primary context tree 
prediction model generating path fsm follows 
pixel path fsm starting root maximum depth node answering questions branches 
path pixel path sm sm 
selection fsm shown node track occurrence entire sequence vectors associate parameter sm individual node secondary context updating rule node sm visited 
delegate sequence path node sm associated parameter accepting loss contextual information 
natural select favored nodes depth largest number nodes nodes hasse diagram depth odd 
nice feature selection number vector equals number extra odd 
contextual information provided level descriptive texture prediction mask seen level largest resolution 
moving far binary prediction mask lled zeros large ones small 
selection best retaining detailed binary pattern thresholded context mask 
experiments primary conditioning context labeling parameters rls predictors case rst encoding algorithm section labeling histogram prediction error 
thresholds selection tree scaled number bits sample original signal thresholds speci ed context fsm computing prediction bold path corresponds state tree events follows 
sm 
bits sample 
experiments nodes middle layer hasse diagram specify secondary context labeling ne tuning parameters intercepts needed adaptive prediction second algorithm labeling additional parameters golomb rice encoding 
note fully adaptive context selection nodes hasse diagram potential improving compression rate percents unfortunately cost twofold increase complexity algorithm 
context tree previously image compression algorithm described surprisingly optimal behaviour audio compression application best achievable performance obtained keeping context structure parameters optimal values experiments image compression parameters represent quantization thresholds forgetting factors 
fact expresses universality context algorithm due adaptivity important features signal image sound learned encoding 
updating predictor parameters intercept recursive squares algorithm forgetting factor updating parameters predictors various primary contexts shown particular case fsm predictors 
context track power prediction residuals exponential forgetting accumulator threshold updating step omitted 
obtained signi cant speeding algorithm experimenting di erent thresholds appears feedback observed number updates processed sample adapt threshold 
adaptation predictor bias sm ss realized lms algorithm sm ss sm ss adaptation rate 
context modelling coding prediction errors large alphabet size dicult handle nonparametric distributions prediction errors subsection solution combining histogram tracking histogram bucketing provides necessary information needed eciently arithmetic coding algorithm fsm hmb 
direct solution parametric distribution leads second subsection golomb rice coding algorithm pd 
histogram modelling bucketing hmb histogram tracking widely signal compression adaptively track time varying distribution prediction residuals 
audio signals bits sample anymore possible directly adaptive histogram tracking method combined adaptive histogram tracking histogram bucketing method 
value error rst mapped positive integers negative errors odd numbers positive errors numbers 
split value parts transmit di erent ways 
signi cant bits de ne integer integer rst encoded means arithmetic coding observed histogram 
histogram corresponding primary context 
choose adapt size alphabet histogram number symbols observed time period corresponding context 
symbols occurring frequently modeled histogram transmitted esc sequences 
statistics signi cant bits second part encoded various contexts bucketed conditioning respect value signi cant bits context taken account 
quantized contexts condition probability distribution function quantized value quantization error conditioned re ects bucketing principle 
input sample send symbols rst encoded arithmetic encoder take full advantage adaptivity symbol statistics 
gain arithmetic encoder transmitting second type symbols percent total bitrate compared hu man coding xed coding table 
high memory requirement histogram modelling main limiting factor establishing number primary contexts nm experiments 
context parametric distribution modelling prediction residuals parametric distribution modelling distribution prediction residuals solves diculties induced large size alphabet raises question lost conditioning distribution exible histogram 
fortunately parametric modelling residuals number contexts increased signi cantly method practical 
clear loss compared histogram tracking number contexts requires memory 
selecting parametric distribution sided geometric distribution osg encoding fast optimal code distribution known code special class hu man codes 
note similar encoding technique context modelling di erent prediction technique 
variations golomb rice codes introduced series papers associated selection algorithm new jpeg ls standard lossless image compression 
invertible mapping transforming residuals context new residuals distributions close sided geometrical distribution 
set original samples dynamic range prediction residuals reduced rst invertible mapping producing converting remapped errors non negative integers having distribution close osg apply mapping interleaves negative values positive values sequence 
golomb rice coding fast consisting sending bits followed unary representation unary representation terminated bit allow uniquely decoding resulting total number bits 
value parameter computed context sucient statistic con sum absolute values previous errors context con sm con blog ncon con represents times context con visited 
parameters stored context con con previously de ned con sum needed prediction intercept correction con integer correction variable incremented ncon decremented ncon 
experimental results proposed compression algorithms referred fsm hmb fsm context predictor histogram modelling bucketing fsm pd fsm context parametric distribution estimation 
algorithms implemented coding rates reported ratios actual length compressed le number samples le 
audio compression audio les converted bits sample sampled khz di erent lengths mbytes considered experiments content le suggested rst column table 
ect changing predictor order analyzed fsm hmb fsm pd algorithms see table 
results obtained superior results obtained les indicating larger predictor order may improve performance need order selection procedure linear prediction models tting 
mainly complexity implementation dictate order linear prediction 
results proposed algorithm compared results public domain data compression algorithms rst standard unix programs compress pack universal compression algorithms 
third compression procedure shorten algorithm especially designed speech audio lossless compression distributions speech databases cd rom 
method combines forward coding predictor coecients estimated encoded af block new data sequential coding sending sequentially prediction residuals modeled parametric distribution encoded hu man coding 
options prediction stage experimented rst polynomial predictor computing high order di erences input samples second option linear predictor coecients sent encoded error 
results compress pack shorten linear prediction polynomial predictor best algorithms listed table 
best new algorithms outperformed tested methods audio les 
experiments limited nature audio material quite diversi ed expect results ranking respect methods typical situations audio speech compression 
speech compression pitch estimation stage contextual prediction masks small capture long term correlations commonly encountered voiced segments speech signal 
experimented algorithm adding extra stage pitch estimation algorithm goal removing periodical components residual signal lag gain estimation algorithm similar open loop part speech codec range candidate lags selection procedure 
coding scheme operated 
frame length set samples 
best lag best gain estimated frame gain quantized bits frame pitch prediction samples lled periodicity waveform adaptive context prediction algorithm run samples resulting sequence residuals encoded method described section 
denote codelength frame perform encoding sequence method described section denote codelength frame decide pitch estimation bit inform decoder 
decision pitch extraction transmit pitch lag bits pitch gain bits send actual encoded version encoded version transmitted 
experiments better results secondary context stage setting 
results speech les timit database speech khz bits table 
method including pitch estimation proved best tested methods 
fsm hmb fsm hmb fsm hmb fsm pd fsm pd male speech average table comparison compression rates bits sample di erent parameters proposed algorithms 
fsm pd shorten shorten unix unix nm polynomial lp compress pack prediction male speech average table comparison compression rates bits sample best method table lossless compression methods best second best framed 
introduced lossless audio compression method motivated context algorithm including major changes order reduce necessary resources complexity algorithm dealing audio samples represented bits samples 
expected procedure outperforms procedures linear prediction large diversity audio material 
method intended universal practical 
research tting underlying model nature audio signals speci cally inclusion re ned pitch prediction methods may lead improvements compression ratio expense increasing complexity 
fsm pd fsm pd shorten shorten unix unix pitch est 
nm nm polynomial lp compress pack prediction sa wav sa wav si wav si wav si wav sx wav sx wav sx wav sx wav sx wav average table comparison compression rates bits sample compression methods new method including pitch estimation stage speech les timit database best second best framed 
craven 
lossless coding audio discs 
audio 
eng 
soc sept 
gallager van 
optimal source codes geometrically distributed integer alphabets 
ieee transactions information theory mar 
rissanen 
universal data compression system 
ieee transactions information theory sept 
robinson 
shorten simple lossless near lossless waveform compression 
www com html cambridge university engineering department dec 
todd langdon rissanen 
parameter reduction context selection compressing grey scale images 
ibm res 
develop mar 
abu astola 
adaptive boolean predictive modelling application lossless image coding 
spie statistical stochastic methods image processing ii pages san diego california jul 
abu rissanen astola 
adaptive predictors nite state machine context selection 
proc 
icip international conference image processing pages santa barbara california oct 
international telecommunication union 
draft recommendation 
technical report standardization sector dec 
weinberg sapiro 
low complexity context lossless image compression algorithm 
proc 
dcc data compression conference pages snowbird utah mar 
weinberger rissanen 
applications universal context modeling lossless compression gray scale images 
ieee transactions image processing ip apr 
weinberger rissanen feder 
universal nite memory source 
ieee transactions information theory may 
ziv lempel 
universal algorithm sequential data compression 
ieee transactions information theory 
list tables comparison compression rates bits sample di erent parameters proposed algorithms 
comparison compression rates bits sample best method table lossless compression methods best second best framed 
comparison compression rates bits sample compression methods new method including pitch estimation stage speech les timit database best second best framed 
list figures selection tree primary context 
hasse cube state transition diagram secondary context 
top bottom path answer questions establish values vectors 
context fsm computing prediction bold path corresponds state tree events follows 
sm 

