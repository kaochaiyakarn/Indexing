probabilistic visualisation high dimensional binary data michael tipping microsoft research st george house street cambridge cb nh microsoft com probabilistic latent variable framework data visualisation key feature applicability binary categorical data types established methods exist 
variational approximation likelihood exploited derive fast algorithm determining model parameters 
illustrations application real synthetic binary data sets 
visualisation powerful tool exploratory analysis multivariate data 
rendering high dimensional data dimensions generally implying loss information reveals interesting structure human eye 
standard dimensionality reduction methods multivariate analysis notably principal component projection utilised purpose techniques projection pursuit tailored specifically 
current trend larger databases need effective data mining methods visualisation increasingly topical novel developments include nonlinear topographic methods lowe tipping bishop svens en williams hierarchical combinations linear models bishop tipping 
disadvantageous aspect proposed techniques applicability continuous variables methods proposed specifically visualisation discrete binary data types commonplace real world datasets 
approach difficulty proposing probabilistic framework visualisation arbitrary data types underlying latent variable density model 
leads algorithm permits visualisation structure data defining generative observation probability model 
intuitively pleasing result specialisation model continuous variables recovers principal component analysis 
continuous binary categorical data types may combined visualised framework reasons space concentrate binary types 
section outline proposed latent variable approach section consider difficulties involved estimating parameters model giving efficient variational scheme section 
section illustrate application model consider accuracy variational approximation 
latent variable models visualisation ideal visualisation model wish dependencies variables evident visualisation space information lose dimensionality reduction process represent noise independent variable 
principle captured probability density model dataset comprising dimensional observation vectors jx dx dimensional latent variable vector distribution priori specified model parameters 
value location visualisation space observations independent model 
general course model conditional independence assumption hold approximately 
unconditional observation model general capture dependencies variables constraint implied just underlying latent variables 
having estimated parameters data visualised inverting generative model bayes rule xjt tjx 
data point induces distribution latent space purposes visualisation summarise conditional mean value 
form model appropriate visualisation demonstrated bishop tipping showed latent variables defined independent gaussian conditional observation model gaussian jx oe maximum likelihood estimation model parameters fw oe leads model posterior mean equivalent probabilistic principal component projection 
visualisation method binary variables follows naturally 
retaining gaussian latent distribution specify appropriate conditional distribution jx 
principal components corresponds linear model continuous data types adopt appropriate generalised linear model binary case jx oe gamma oe gammat oe exp gammaa gamma parameters maximum likelihood parameter estimation proposed model binary data exists literature various guises historically latent trait model utilised data visualisation 
case probabilistic principal component analysis ml parameter estimates obtained closed form disadvantageous feature binary model jx defined integral analytically intractable computed directly 
fitting latent trait model necessitates numerical integration papers considered gauss hermite monte carlo sampling approximations mackay ryan 
case log likelihood dataset observation vectors ft approximated ln jx samples dimensional latent distribution 
obtain parameter estimates may utilise expectation maximisation em approach noting equivalent form component latent class model component probabilities mutually constrained 
applying standard methodology leads step requires computation theta posterior responsibilities jt logistic regression step unfortunately iterative performed relatively efficiently iteratively re weighted squares algorithm 
difficulties implementation section describe variational approximation likelihood maximised efficiently 
variational approximation likelihood jaakkola jordan introduced variational approximation predictive likelihood bayesian logistic regression model briefly considered dual problem closely related proposed visualisation model 
approach integral approximated jx dx jx oe exp phi gamma gamma psi gamma gamma oe parameters variational parameters approximation property jx jx equality follows 
exponential quadratic integral likelihood computed closed form 
suggests alternative algorithm finding parameter estimates iteratively maximise variational approximation likelihood 
iteration algorithm guaranteed increase lower bound necessarily maximise true likelihood 
hope close approximation accuracy investigated 
step algorithm 
obtain sufficient statistics approximated posterior distribution latent variables observation xn jt 

optimise variational parameters order approximation close possible 
update model parameters increase 
jaakkola jordan give formulae computations include provision biases necessary expressions re derived 
note introduced theta additional variational parameters longer necessary sample compute responsibilities iterative logistic regression step needed 
computing latent posterior statistics 
bayes rule posterior approximation xn jt gaussian covariance mean cn gamma gamma cn gamma optimising variational parameters 
variational approximation optimised maximising respect em methodology obtain updates iw angle brackets deltai denote expectations respect xn jt old earlier necessary posterior statistics cn depend variational parameters cn computed followed update 
iteration stage process guaranteed improve monotonically approximation typically iterations necessary convergence 
optimising model parameters 
em increase variational likelihood approximation respect defining leads updates gamma hb xn gamma gamma hb xn hb xn cn visualisation synthetic clustered data 
firstly give example visualisation artificially generated data illustrate operation features method 
binary data synthesised generating random bit prototype vectors bit set probability 
point dataset generated examples prototype inverting bit probability 
generated second dataset manner probability bit inversion simulating noise prototype 
final values data point plotted 
left plot low noise dataset clusters clear prototype vectors 
right bit noise sufficiently high clusters overlap degree prototypes longer evident 
elucidate information model drawing lines representing jx may considered decision boundaries bit 
offer convincing evidence presence clusters 
visualisation synthetic clustered datasets 
clusters denoted separate glyphs size reflects number examples posterior means located point latent space 
right plot lines corresponding jx drawn 
handwritten digit data 
left visualisation examples derived theta images handwritten digit 
visual evidence natural variability writing styles plot posterior latent means describe approximate horseshoe structure 
right examine nature plotting gray scale images vectors tjx numbered samples visualisation space 
images illustrate expected value bit latent space location demonstrate location indicative style digit notably presence loop 
accuracy variational approximation 
investigate accuracy approximation sampling algorithm section likelihood maximisation implemented applied datasets 
evolution error negative log likelihood data point plotted time algorithms identical 
true error variational approach estimated point monte carlo sample 
typical results shown final running time error sensible stopping criterion datasets table 
example datasets variational algorithm converges considerably quickly sampling case difference final error relatively small particularly larger dimensionality dataset 
approximation posterior distributions xn jt key factor accuracy algorithm 
contours posterior distribution latent space induced digit left visualisation dimensional digit data 
right gray scale images conditional probability bit latent space locations marked 
variational sampling time secs variational sampling time secs error vs time synthetic data left digit data right 
typical data point shown algorithms datasets 
approximation accurate dimensionality increases phenomenon observed datasets true posterior gaussian form 
outlined variational approximation parameter estimation probabilistic visualisation model considered application binary variables extension mixtures arbitrary data types readily implemented 
comparisons shown illustrated approximation appears acceptably accurate particularly data higher dimensionality 
algorithm considerably faster sampling approach permit incorporation multiple models complex hierarchical architecture sort effectively implemented visualisation continuous variables bishop tipping 
synthetic digit time error time error variational sampling table comparison final error running time algorithms 
true posterior approximation true posterior approximation true approximated posteriors single example synthetic data set top digit data bottom 


latent variable models factor analysis 
london charles griffin bishop svens en williams 
gtm generative topographic mapping 
neural computation 
bishop tipping 
hierarchical latent variable model data visualization 
ieee transactions pattern analysis machine intelligence 
jaakkola jordan 
bayesian logistic regression variational approach 
madigan smyth eds proceedings conference artificial intelligence statistics ft lauderdale fl 
lowe tipping 
novel topographic feature extraction radial basis function networks 
mozer jordan petsche eds advances neural information processing systems pp 

cambridge mass mit press 
mackay 

bayesian neural networks density networks 
nuclear instruments methods physics research section 


latent trait latent class model mixed observed variables 
british journal mathematical statistical psychology 
ryan 
latent variable models mixed discrete continuous outcomes 
journal royal statistical society series 
