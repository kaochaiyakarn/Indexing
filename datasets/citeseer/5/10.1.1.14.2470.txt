motivated interest relational reinforcement learning introduce novel representation formalism called logical markov decision programs lomdps integrates markov decision processes logic programs 
lomdps compactly declaratively represent complex relational markov decision processes 
framework develop theory reinforcement learning abstraction states actions plays major role 
various convergence results experiments validate approach 
theory provide sound basis developments relational reinforcement learning 
past years lot extending probabilistic stochastic frameworks abilities handle objects see anderson zeroski friedman kersting de raedt kersting muggleton 
inductive logic programming relational learning point view approaches upgrades propositional representations relational computational logic representations 
various successes direction reported 
friedman kersting de raedt upgrade bayesian networks muggleton upgrades stochastic context free grammars anderson kersting upgrade hidden markov models 
contribution novel representation formalism called logical markov decision programs lomdps combines markov decision processes computational logic 
result flexible expressive framework defining mdps able handle structured objects relations functors 
mdps framework grounded computational logic missing 
boutilier shorter version appear proceedings twentieth international conference machine learning icml washington dc usa august 
logical markov decision programs kristian kersting luc de raedt machine learning lab university freiburg georges koehler building freiburg germany report combining mdps reiter situation calculus 
argue section complex model free reinforcement learning techniques addressed framework 
lomdps share upgrades propositional representations advantages 
logical expressions form clauses rules transitions may contain variables abstraction specific grounded rules transitions 
allows compactly represent complex domains 
secondly abstraction number parameters rewards probabilities model significantly reduced 
turn allows principle speed simplify learning learn level ground level 
fascinating machine learning techniques developed name reinforcement learning rl context mdps decades cf 
sutton barto 
increased attention dealing relational representations objects reinforcement learning see zeroski finney works taken practical perspective developed systems experiments operate relational worlds 
heart systems function approximator logical decision tree able assign values sets states sets state action pairs 
far theory explains approach works lacking 
second important contribution theory 
theory notion states policies represented logical expressions 
state represents set concrete states policy function states actions 
ground states represented state essentially assigned action 
akin happens relational reinforcement learning logical decision trees zeroski leaf decision tree represents state states classified leaf obtain value action 
lomdp framework policies easily represented learned 
value functions assigning values states state action pairs defined average values states state action pairs represent 
show value functions learned concrete episodes convergence results 
turn provides sound theoretical basis relational reinforcement learning approaches 
validate approach experiments blocks world 
proceed follows 
introducing mathematical preliminaries section lomdp framework section 
section defines policies shows compute value policy 
results lq learning algorithm section 
algorithm experimentally evaluated section 
concluding discuss related 
preliminaries logic programs markov decision processes underlying concepts briefly introduce key concepts 
logic order alphabet set relation symbols arity set functor symbols arity 
called constant called proposition 
atom 
tm relation symbol followed bracketed tuple terms ti 
term variable functor symbol immediately followed bracketed tuple terms ti 
tn conjunction set atoms 
conjunction said subsumed conjunction denoted exists substitution term atom clause called ground contains variables 
general unifier mgu atoms denoted mgu 
herbrand base denoted hb set ground atoms constructed predicate functor symbols alphabet 
notation atoms written lower case set atoms upper case sets sets atoms bold upper case highlight resp 
may ground may contain variables write resp 

markov decision processes markov decision process mdp tuple avoid ambiguities index elements set system states propositions 
agent available finite set actions state cause stochastic state transitions 
transition expression form transition denotes probability action causes transition state executed state 
transition agent gains expected reward case reward function probabilistic mean value depends current state action mdp called nondeterministic deterministic 
consider mdps stationary transition probabilities stationary bounded rewards 
stationary deterministic policy set expressions form 
denotes particular course actions adopted agent action executed agent finds state assume infinite horizon agent accumulates rewards associated states enters 
compare policies expected total discounted reward optimality criterion rewards discounted 
value policy shown 
value initial state computed solving system linear equations 
policy optimal policies stationary nondeterministic policy maps state distribution actions 
value expectation distribution 
logical markov decision programs logical component mdp corresponds finite state automaton 
essentially propositional representation state action symbols flat structured 
key idea underlying logical markov decision programs lomdps replace flat symbols symbols 
definition 
state conjunction logical atoms logical query 
states represent sets states 
formally state finite conjunction ground facts alphabet logical interpretation subset herbrand base 
blocks world possible state fl bl bl cl cl fl denotes object cl states clear bl denotes block fl refers floor 
state bl bl 
represents states alphabet blocks 
formally speaking state represents states exists substitution denote set states 
substitution previous example 
able define transitions 
definition 
transition expression form action body head states 
assume range restricted vars vars vars vars transition relies information encoded current state 
semantics transition ditions agent state go state probability performing action receiving expected reward implicitly assume action illustration purposes consider transition moves block floor probability fl cl cl mv fl cl applied state exp fl fl cl cl bl bl bl transition tells execute mv fl successor state fl fl fl cl cl cl bl bl bl probability gaining reward 
see implements kind order variant probabilistic strips operator cf 
hanks mcdermott 
lomdps typically consist set multiple transitions constraints imposed order obtain meaningful lomdps 
set bodies state transitions lomdp modulo variable renaming 
denote set actions lomdp 
require body act 
condition guarantees successor states specified executing action state probabilities sum 
secondly need way cope contradicting transitions rewards 
consider transitions state 
problem transitions transition says execute go probability state second assigns probability state 
essentially ways deal situation 
hand want combine transitions assign probability hand want rule fire 
take second approach allows consider transitions independently 
turn simplify learning yields locally interpretable models 
assume total order action body pairs forward search pairs stopping matching body prolog assume ordered give example definition 
please note employ functor free examples sake simplicity 
states actions transitions include functors 
proofs remain valid 
chose total order sake simplicity 
partial order pairs set pairs founded descending chain elements finite suffices 
conflict resolution strategy select transitions action body pair minimal 
example kersting kind subsumption generality relation employed 
theorems adapted accordingly 
mv fl mv mv fl stacks blocks floor underlying patterns blocks world 
shows situation stacks height 
shows situation stack left 
cuts indicate resp 
top block floor 
able formally define logical markov decision programs 
definition 
logical markov decision process lomdp tuple logical alphabet set actions finite set state transitions actions discount factor holds 
giving semantics lomdps illustrate lomdps stack example blocks world absorb absorb absorb 
fl cl cl mv fl cl cl cl 
cl cl mv cl cl cl 
absorb cl bl 
transition probabilities sum action additional transition staying current state 
order understand lomdp stack understand states govern underlying patterns blocks world cf 

states artificial absorb state excluded order cover possible state action patterns take advantage symmetry blocks world 
transition encodes absorbing state 
transitions cover cases stacks 
transition encodes situation stack goal state stack 
cl bl describe preconditions mv floor moved 
performing action mv state exp see transitions firing 
similar easily encode unstack goal 
note specified number blocks 
lomdp represents possible blocks worlds transitions probability reward parameters number parameters propositional system explodes blocks states blocks states blocks states resulting higher number transitions 
semantics lomdps follows 
theorem 
lomdp specifies discrete mdp 
proo sketch hb hb set ground atoms built states predicates hb hb set ground atoms built action names 
construct follows 
countable state set consists finite subsets hb 
set actions state minimal holds 
probability transition state performing action probability value associated unique transition matching normalized number transitions form 
transition connecting probability zero 
bounded rewards constructed similar way normalized 
theorem puterman theorem follows corollary 
lomdp exists optimal policy ground states 
lomdps generalize finite mdps 
proposition 
finite mdp propositional lomdp relation symbols arity 
policies theorem states lomdp specifies discrete mdp 
furthermore corollary guarantees exists optimal policy mdp 
course policy extensional propositional sense specifies ground state separately action execute 
specifying policies lomdps large state spaces cumbersome learning require effort 
introduce policies intentionally specify action take state sets states 
definition 
policy finite set decision rules form action state meaning decision rule agent state agent perform action denoted 
usually consists multiple decision rules 
apply conflict resolution technique transitions total order decision rules 

set bodies ordered 
call abstraction level 
assume covers possible states lomdp 
total order guarantees forms partition states 
equivalence classes 
induced assume applicable 
inductively defined 
generally coincide proposition holds 
proposition 
policy specifies nondeterministic policy level ground states 
lomdp induced mdp 
define expected reward expected reward taken states 
expected discounted reward policy system state defined lim rt zt ri denotes value time reward received ground level policy induced 
inner expectation conditioned system state time denoted zt outer expectation runs elements 
series converges absolutely reasons mdps 
limit expectations interchangeable rt zt 
function defined analogously 
policy discount optimal abstraction level fixed policies abstraction level 
note optimality abstraction level imply optimality level ground states 
policy specifies expected behaviour set ground states 
problem compute value function 
show lomdp abstraction level exists reduced finite mdp compute value policy abstraction level 
theorem 
lomdp policy abstraction level 

exists finite mdp 
lm tl policy li 
proof sketch proof goes steps 
define show partial sums rt zt lt value reward 
step observe set bodies induce partitions 
resp 

ordered 
state li corresponds 
furthermore ground states belonging set possible transitions 
words forms equivalence class 
transition tl state li lj doing action probability li lj probability function 
value sm probability randomly selected ground state element induces unique probability distribution ground states uniquely specified 
follows theorem 
clearly li lj lj intuition li lj specifies corresponding states 
probabilistic reward li lj depends li chosen mean value equals li li lj li lj lj step consider 
assume produces trace li states 
show trace corresponds trace states transition probabilities 
design traces li specify reward processes expected reward tv 
true follows 
state sl corresponds state 
system starts probability state mgu 
ab transition state subsumed state probability 
state corresponds sl 
steps correspond kind forward chaining step computational logic 
assume 
transition total probability coincides probability making transition see 
iterating forward chaining times get sequence states applying 
state yields sequence states 
ground trace corresponds legal trace probability product corresponding transition probabilities furthermore ground traces unique trace sharing ground instance induce partitions legal traces covered trace 
nondeterministic mdp converted deterministic 
maximizing expected reward depends expected reward state probability distribution rewards 
case li lj 
lq learning principle theorem known algorithm computing optimal policy compute optimal policy abstraction level lomdp 
problem probability function 
problem solved stochastic iterative dynamic programming model free approaches 
sketch lq learning learns function idea combined traditional learning 
point wish stress principle methods mc sarsa actor critic methods adapted 
logical learning abstraction level initialize arbitrarily repeat episode initialize ground state sm repeat step episode choose action qn cf 
action corresponding take action observe successor state resp 
unique state matching resp 
qn max qn set terminal total number times state action pair visited including th iteration 
approximation iterations 
select action probabilistically select action state probability selection proportional qn qn 
common learning 
select uniformly possible ground action get argue lq learning converges correct function 
selection ground state selects unique state li likewise observed uniquely specifies state lj 
rewards stochastic depend 
convergence theorem learning finite nondeterministic mdps applies cf 
watkins dayan 
similar jaakkola convergence results upgraded 
combined theorem yields corollary corollary 
consider lq learning agent lomdp abstraction level 
lq learning agent initializes table arbitrary finite values 
iteration corresponding ith time action applied state 
state action pair state mgu visited infinitely qn probability 
contrast propositional approach ground state visited infinitely 
due abstraction lq learning generalize unseen ground states 
experiments implemented lq learning prolog system sicstus 
task learn policy stack lomdp see 
task motivated experiments relational reinforcement learning rrl zeroski fact blocks world prototypical toy domain requiring relational representations 
key differences experiments reported zeroski exclusively standard predicates cl bl 
zeroski needed background knowledge predicates height stacks directives inductive logic programming function approximator order able learn adequate policies 
difference approach rrl induces relevant states automatically regression tree learner 
discount factor temperature select action increased epoch starting 
agent favors exploration early states learning gradually shifts strategy exploration 
randomly generated blocks world states blocks blocks blocks blocks procedure described slaney thi baux 
note blocks propositional mdp represent states states goal states 
ran lq learning starting states order blocks 
initial function fl cl cl cl bl bl fl cl cl cl bl bl fl cl cl cl bl bl fl cl cl cl bl bl mv fl mv mv mv cl cl mv fl cl cl mv fl cl cl mv fl fl cl cl mv fl cl cl mv cl cl cl mv omitted absorb state front 
experiment repeated times including sampling starting states 
runs learned policy optimal abstraction level mv fl fl cl cl cl 
mv fl cl cl 
mv fl cl cl 
mv cl cl 
learned policy interesting reasons 
uniquely specifies deterministic policy ground states 
second known planning community slaney thi baux 
called unstack stack strategy amounts putting misplaced blocks table building goal state stacking blocks floor single stack 
total number moves worst twice optimal 
third unstack stack perfectly generalizes blocks worlds matter blocks 
learned propositional setting optimal policy encode optimal number moves 
rrl learned policy move block highest stack lq learning 
argued policy described additional background predicates needed approach 
believe rrl difficulties learning unstack stack policy predicates cl bl 
rerunning experiments simpler function omitting values yields policy learning epochs faster proceeded due higher abstraction 
related reinforcement learning rl currently significant interest rich representation languages 
finney investigated methods relational domains 
experimentally studied intermediate language deictic representations drs 
drs avoid enumerating domain variables block floor 
drs led impressive results mccallum whitehead ballard finney results show dr may degrade learning performance relational domains 
finney relational reinforcement learning rrl zeroski way effective learning domains objects 
rrl combination rl inductive logic programming ilp muggleton de raedt 
key idea function approximated relational regression tree learner 
experimental results interesting rrl failed explain theoretical terms rrl works 
precisely theory contribute 
relational regression trees rrl encode policies 
general point view approach closely related decision theoretic regression dtr boutilier 
state spaces characterized number random variables domain specified logical representations actions capture regularities effects actions 
existing dtr algorithms designed propositional representations mdps boutilier proposed order dtr probabilistic extension reiter situation calculus 
language certainly expressive lomdps 
complex 
boutilier order value iteration needs order theorem prover simplify logical partition descriptions generated 
furthermore assumes model 
lomdps assumptions algorithms simpler traditional model free learning methods apply 
idea solving large mdp reduction equivalent smaller mdp discussed dearden boutilier givan ravindran barto 
finite mdps relational order representations 
furthermore great interest abstraction levels state spaces 
abstraction time sutton primitive actions dietterich andre russell useful ways specific subactions time 
research orthogonal applied lomdps 
baum reports solving blocks worlds blocks rl related techniques 
introduced language domain dependent incorporate logic programming 
novel representation framework integrates markov decision processes logic programs 
framework allows compactly declaratively represent complex relational markov decision processes 
functors infinite 
furthermore introduced policies lomdps studied properties shown value functions learned simple upgrades existing rl methods learning 
addition convergence results experiments validate approach 
authors hope framework useful sound theoretical basis developments relational reinforcement learning 
anderson anderson domingos weld 
relational markov models application adaptive web navigation 
hand keim za ne goebel editors proceedings eighth international conference knowledge discovery data mining kdd pages edmonton canada 
acm press 
andre russell andre russell 
programmable reinforcement learning agents 
leen dietterich tresp editors advances neural information processing systems pages 
mit press 
baum baum 
model intelligence economy agents 
machine learning 
boutilier boutilier dearden goldszmidt 
stochastic dynamic programming factored representations 
artificial intelligence 
boutilier boutilier reiter price 
symbolic dynamic programming order mdps 
nebel editor seventeenth international joint conference artificial intelligence ijcai pages seattle washington usa 
morgan kaufmann 
dearden boutilier dearden boutilier 
abstraction approximate decision theoretic planning 
artificial intelligence 
dietterich thomas dietterich 
hierarchical reinforcement learning maxq value function decomposition 
journal artificial intelligence research 
zeroski zeroski de raedt driessens 
relational reinforcement learning 
machine learning 
finney finney kaelbling oates 
thing tried didn deictic representation reinforcement learning 
proceedings eighteenth international conference uncertainty artificial intelligence uai 
friedman friedman getoor koller pfeffer 
learning probabilistic relational models 
dean editor proceedings sixteenth international joint conferences artificial intelligence ijcai pages stockholm sweden 
morgan kaufmann 
givan givan dean greig 
equivalence notions model minimization markov decision processes 
artificial intelligence 
press 
hanks mcdermott hanks mcdermott 
modelling dynamic uncertain world symbolic reasoning change 
artificial intelligence 
jaakkola jaakkola jordan singh 
convergence stochastic iterative dynamic programming algorithms 
neural computation 
kersting de raedt kersting de raedt 
combining inductive logic programming bayesian networks 
line rouveirol mich le sebag editors proceedings th international conference inductive logic programming volume lnai pages 
springer 
kersting kersting kramer de raedt 
discovering structural signatures protein folds logical hidden markov models 
altman hunter jung klein editors proceedings pacific symposium biocomputing pages kauai hawaii usa 
world scientific 
mccallum mccallum 
reinforcement learning selective perception hidden states 
phd thesis department computer science university rochester 
muggleton de raedt muggleton de raedt 
inductive logic programming theory methods 
journal logic programming 
muggleton muggleton 
stochastic logic programs 
de raedt editor advances inductive logic programming pages 
ios press 
puterman puterman 
markov decision processes discrete stochastic dynamic programming 
john wiley sons 
ravindran barto ravindran barto 
model minimization hierarchical reinforcement learning 
proceedings fifth international symposium abstraction reformulation approximation sara volume lncs pages alberta canada 
springer 
slaney thi baux slaney thi baux 
blocks world revisited 
artificial intelligence 
sutton barto sutton barto 
reinforcement learning 
mit press 
sutton sutton precup singh 
mdps semi mdps framework temporal abstraction reinforcement learning 
artificial intelligence 
watkins dayan watkins dayan 
learning 
machine learning 
whitehead ballard whitehead ballard 
learning perceive act trial error 
machine learning 
