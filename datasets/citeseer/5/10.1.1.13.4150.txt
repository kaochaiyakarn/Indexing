pp 
proc 
th belgian dutch conference machine learning utrecht supervised learning bayesian network parameters easy hannes peter gr roos petri ki henry tirri complex systems computation group cwi helsinki inst 
inf 
tech 
hiit box university helsinki nl gb amsterdam netherlands 
helsinki university technology box fin hut finland firstname lastname hiit fi peter grunwald cwi nl bayesian network models widely supervised prediction tasks classification 
usually parameters models determined unsupervised methods maximization joint likelihood 
cases reason clear find parameters maximizing supervised conditional likelihood 
show supervised learning problem solved efficiently large class bayesian network models including naive bayes nb tree augmented nb tan classifiers 
showing certain general condition network structure supervised learning problem exactly equivalent logistic regression 
hitherto known naive bayes models 
logistic regression models concave loglikelihood surface global maximum easily local optimization methods 
years recognized supervised prediction tasks classification supervised learning algorithm supervised conditional likelihood maximization friedman greiner ng jordan kontkanen greiner zhou :10.1.1.30.9978
related applications model parameters determined unsupervised methods maximization unsupervised joint likelihood ordinary unsupervised bayesian methods 
main reasons discrepancy difficulty finding global maximum supervised likelihood 
show problem solved bayesian network models long satisfy particu lar additional condition 
condition satisfied existing bayesian network classifiers including naive bayes nb tan tree augmented nb diagnostic classifiers kontkanen 
find maximum supervised likelihood parameters models different manner roughly speaking parameters parametrization correspond logarithms parameters standard bayesian network parametrization 
way conditional bayesian network model mapped logistic regression model 
cases parameters logistic regression model allowed vary freely 
words bayesian network model corresponds subset logistic regression model full logistic regression model 
provide general condition network structure prove bayesian network model mapped full logistic regression model freely varying parameters 
supervised log likelihood logistic regression models concave function parameters 
condition parameters allowed vary freely condition implies new parametrization supervised loglikelihood concave function parameters convex set 
implies find global maximum supervised likelihood parameters simple local optimization techniques hill climbing 
viewing bayesian network classifiers logistic regression models new earlier papers heckerman meek ng jordan greiner zhou 
concavity log likelihood surface logistic regression known 
main contribution sup ply condition bayesian network models correspond logistic regression completely freely varying parameters 
case guarantee local maxima likelihood surface 
direct consequence result show time supervised likelihood instance tree augmented naive bayes tan model local maxima 
organized follows 
section introduce bayesian networks alternative called parametrization 
section show parametrization allows consider bayesian network classifiers logistic regression models 
earlier results logistic regression conclude parametrization supervised log likelihood concave function 
section main result giving conditions parametrizations correspond exactly conditional distributions parametrization preserves independence assumptions encoded network structure 
summarized section 
bayesian networks assume reader familiar basics theory bayesian networks see pearl 
consider random vector 
xm xi takes values 
ni 
bayesian network structure factorizes xi ai ai 
xm parent set variable xi interested predicting class variable xm 
conditioned xi loss generality may assume class variable children 
xm instance called naive bayes model leftmost picture children class variable independent value 
bayesian network model corresponding set distributions satisfying conditional independencies encoded usually parametrized vectors components form defined xi pai xi pai xi xi ai pai pai configuration set values parents ai xi 
want emphasize pai determined complete data vector 
xm write pai denote configuration ai vector data vector 
xm need consider modified vector replaced entries remain 
write pai ration 
xm 
set conditional distributions 
xm corresponding distributions 
xm satisfying conditional independencies encoded conditional distributions written 
xm pa xi pai pa xi pai extended outcomes independence 
complete data matrix 
supervised log likelihood parameters log 
xm 
note standing nodes xi pai class variable children cancel terms pai pai relevant parameters determining conditional likelihood form xi pai xi ni pai configuration values ai 
order parameters lexicographically define set vectors constructed way xi pai ni xi 
xi xi pai values configurations pai 
note require parameters strictly positive 
model mb contain notion unsupervised distributions probabilities xi ai undefined interested 
task prediction 
xm 
heckerman meek call models mb bayesian regression classification brc models heckerman meek heckerman meek 
arbitrary supervised bayesian network model mb define called set conditional distributions 
xm 
model denote ml parametrized vectors set closely resembles different mb give rise corresponding ml necessarily mb ml component vector xi pai corresponding component xi pai vectors components xi pai take values range 
vector defines conditional distribution 
xm exp pa exp xi pai exp pa exp xi pai 
model set conditional distributions 
xm indexed extended outcomes independence 
data matrix supervised log likelihood parameters defined analogously place 
theorem 
proof 
theorem immediate doing log parameter transformation setting xi pai log xi pai xi pai 
words conditional distributions represented parameters represented parameters usual parametrization require ni xi xi pai 
pai corresponding condition parameters model 
consequently converse theorem ml mb true additional conditions network structure 
return topic section take closer look model 
model viewed logistic regression model closely related cases formally identical bayesian network classifiers interpreted terms logistic regression 
think conditional model predictor combines information attributes called softmax rule bishop heckerman meek ng jordan 
gives interpretation depicting naive bayes model corresponding model bayesian network guises 
terms logistic regression model binary regressor variable configuration parent sets ai binary output variable possible value class variable 
having established model logistic regression model may known fact holds logistic regression models general 
supervised log likelihood parametrization concave function parameters theorem 
duffy parameter set convex supervised log likelihood concave strictly concave 

xm 
xm standard naive bayes structure left corresponding model right 
arcs network reversed resulting product distribution replaced softmax denoted 
proof 
part obvious parameter take values 
concavity direct consequence fact logistic regression model see duffy 
example supervised log likelihood strictly concave see 
theorem directly obtain corollary 
corollary 
local non global maxima likelihood surface model 
conditions global maximum exists discussed duffy 
possible solution cases maximum exists assign prior model parameters maximize supervised posterior gr likelihood 
global supervised maximum likelihood parameters obtained training data prediction data 
addition discussed heckerman meek perform model selection competing model structures bic schwarz approximate mdl rissanen criteria 
heckerman meek stated general supervised bayesian network models may difficult determine global maximum gradient methods 
locate local maxima 
theorem shows dealing models global maximum exists 
original parametrization loglikelihood surface necessarily concave example shows 
example 
consider bayesian network class variable child variables take values 
training data supervised log likelihood example peaks twice original parametrization line defined 
set parameters follows 
shows supervised log likelihood data function 
shows bimodal curve clearly violates concavity 
network structure models equivalent find global maximum supervised likelihood parameterization local optimization method 
continuous follows calculus case local maxima supervised likelihood global maxima original parametrization main result theorems suggest order find parameters maximizing supervised likelihood bayesian network model model optimization easy 
resulting parameters may violate sum constraint may ni xi exp xi pai 
pai 
correspond simple bayesian network class variable denoted satisfying condition left network satisfy condition right 
violation independence assumptions model structure demonstrated example 
structures show ni xi exp xi pai 
pai conditional distribution indexed mb main result independence assumptions encoded guaranteed preserved model satisfies condition condition exists xi aj 
xm aj ai xi 

condition holds seen induction parent set aj child xj class conditionally fully connected fully connected modulo arcs parents effect conditional aj 
necessary sufficient condition class moral node common child node directly connected see 
example 
consider bayesian networks depicted 
leftmost network satisfies condition rightmost network 
mb denote usual model corresponding indexed parameters denote corresponding model 
parameters define xi varies 
ni 
consider distributions ml indexed exp 
distri butions violate independence assumptions mb tree augmented naive bayes tan model satisfying condition left network tan right 
note cases class variable moral node network right satisfy condition 
stance suppose summing values gives summing values gives 
correspond situation probable depen dent 
follows case ml contains conditional distributions satisfy independence assumptions encoded 
happen leftmost network structure allows conditional distributions 
examples network structures satisfy condition mention naive bayes nb tree augmented naive bayes tan models friedman :10.1.1.30.9978
generalization children class variable allowed form tree structures see 
proposition 
condition satisfied naive bayes tree augmented naive bayes structures 
proof 
naive bayes aj 

tan models children class variable parents 
children parent class variable argument nb case 
child xj parents xi parent class variable 
xi child class variable aj ai xi 
condition automatically satisfied incoming arcs diagnostic classifiers see kontkanen 
bayesian network structures condition hold add arrows arrive structure condition hold instance add arrow rightmost network fig ure 
model mb submodel larger model mb condition holds 
reasons regard condition relatively mild 
ready main result theorem 
satisfies condition corollary shows condition holds supervised likelihood surface local maxima 
proposition implies example supervised likelihood surface tan classifiers local maxima 
maximum local optimization techniques 
proof 
speak parent configuration pa 
case parents empty set pa constant respect 
xm 
introduce notation 

mj maximum number 
aj aj 
mj exists condition 
condition implies completely determined pair 
introduce functions qj mapping corresponding 

xm 
qj 
introduce 
configuration pai ai constant pai define xi pai xi pai pai cj qj xi pai 
mj easy see case maximum supervised likelihood parameters may determined analytically 
parameters constructed way xi pai combined vector clearly member introducing additional notation proceed stage proof 
stage stage proof show matter choose constants pai corresponding 
show possible vectors corresponding parent configurations matter ci pai chosen holds xi pai xi pai pa 
derive substitute terms definition 
clearly xi pai 
exactly term form cj appears sum positive sign 

exists exactly 
pj case 
term form cj qj xi pai appears exactly sum negative sign 
cj qj xi pai cj terms cj appear positive sign appear negative sign 
follows pa terms cancel 
establishes 
plugging cj follows sl sl concludes stage proof 
stage set xi pai xi pai exp 
xi pai stage show determine constants ci pai 
pai sum constraint satisfied ni xi xi pai 
achieve sequentially determining values ci pai particular order 
need terminology say ci determined configurations pai ai determined ci pai say ci undetermined determined ci pai configuration pai ai 
say ci ready determined ci undetermined time cj pj determined 
note long ci 
undetermined exist ci ready determined 
see take 
ci undetermined 
ci ready determined case done exists 
pj xi aj cj undetermined 
cj ready determined done 
exist xj ak ck undetermined 
repeat argument move forward bayesian network structure restricted 
xm find cl ready determined 
acyclic find cl steps 
describe algorithm sequentially assigns values ci pai satisfied 
start ci undetermined repeat steps exists 
ci undetermined 
pick largest ci ready determined 

set configurations pai ai ci pai ni xi holds 
xi pai done algorithm loop times halt 
step affect values cj cj determined 
algorithm halts holds 
concludes stage proof 
choice constants pai determines corresponding vector components 
turn determines corresponding vector components 
stage showed take ci pai holds 
choice pai adopt 
particular choice indexes distribution applying log transformation components find length denotes supervised log likelihood summing logarithm 
result stage implies indexes conditional distribution chosen arbitrarily shows theorem concludes proof 
concluding remarks showed parameter transformation described effectively find parameters maximizing supervised conditional likelihood nb tan bayesian network models 
arbitrary bayesian network transformation may yield slightly powerful model class remove independence assumptions network structure 
gave condition transformation change class models considered 
test runs naive bayes case shown maximizing supervised likelihood contrast usual practice maximizing unsupervised joint likelihood feasible yields greatly improved classification 
intend study complicated models model selection 
acknowledgments 
research supported national technology agency academy finland 
authors wray buntine useful comments 
bishop 

neural networks pattern recognition 
oxford university press 
friedman geiger goldszmidt 

bayesian network classifiers 
machine learning 
greiner zhou 

structural extension logistic regression discriminant parameter learning belief net classifiers 
proceedings th annual national conference artificial intelligence aaai pages edmonton 
greiner grove schuurmans 

learning bayesian nets perform 
pro ceedings th conference uncertainty artificial intelligence uai pages 
morgan kaufmann publishers san francisco ca 
gr kontkanen ki roos tirri 

supervised posterior distributions 
seventh valencia international meeting bayesian statistics spain 
heckerman meek 

embedded bayesian network classifiers 
technical report msr tr microsoft research 
heckerman meek 

models selection criteria regression classification 
proceedings th conference uncertainty artificial intelligence uai pages 
morgan kaufmann publishers san francisco ca 
kontkanen ki tirri 

classifier learning supervised marginal likelihood 
proceedings th conference uncertainty artificial intelligence uai pages 
morgan kaufmann publishers san francisco ca 
ng jordan 

discriminative vs generative classifiers comparison logistic regression naive bayes 
advances neural information processing systems 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann publishers san mateo ca 
rissanen 

modeling shortest data description 
automatica 
duffy 

statistical analysis discrete data 
springer verlag new york 
schwarz 

estimating dimension model 
annals statistics 
gr roos ki tirri 

supervised learning bayesian network parameters 
technical report hiit helsinki institute information technology hiit 
available hiit fi articles hiit ps 

