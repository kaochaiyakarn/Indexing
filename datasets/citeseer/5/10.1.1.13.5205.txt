appears proceedings th international workshop active middleware services seattle wa june jagr autonomous self recovering application server george candea steve zhang pedram armando fox stanford university computer systems lab stanford ca usa candea fox cs stanford edu demonstrates dependability generic evolving ee applications enhanced combination recovery oriented techniques 
goal reduce downtime automatically efficiently recovering broad class transient software failures having modify applications 
describe integration new techniques jboss open source ee application server 
resulting system jagr jboss application generic recovery self recovering execution platform 
jagr combines application generic failure path inference path failure detection microreboots 
uses controlled fault injection observation infer paths faults follow ee application 
path failure detection uses tagging client requests statistical analysis identify anomalous component behavior 
micro reboots fast reboots perform sub application level recover components transient failures selectively rebooting components necessary repair failure reduce recovery time 
techniques designed autonomous application generic making wellsuited rapidly changing software internet services 

internet applications able automatically recover variety transient failures freeing system operators focus higher leverage tasks requiring fewer operators oversee systems 
recovery oriented computing roc project embraces philosophy failures happen due hardware software operator error fast recovery key maintaining high availability face failures 
roc researchers investigating techniques failure detection recovery external application rely priori fault models models application semantics examples include recursive restarts form recovery anomaly detection runtime path analysis form failure detection 
emergence popular middleware platforms internet web applications java enterprise edition ee believe techniques applied today middleware platform implementation applications written platform benefit 
describe progress just goal integrated system leverages pieces prior 
augmented jboss open source implementation ee middleware platform 
adding plug architecture failure monitoring recovery manager reacts failures component level micro reboots recovery enable jboss ee compliant application running top automatically detect recover variety transient failures 
result system application generic respect class failures 
past painted grim picture regarding application generic recovery showing general purpose transparent recovery 
derives part fact fully generic recovery system assumptions application structure constitutes safe correctness preserving generic recovery 
argue specific class applications re targeting interactive internet services deployed traditional configuration assumptions structural properties assumptions possible obtain application generic benefits solely modifying middleware 
exploit applicationspecific fault propagation information guide recovery process collecting information automatic relatively fast perform order hours 

ee jboss ee standard constructing enterprise applications reusable java modules called enterprise java beans ejbs 
ejb java component conforms particular programmatic interface provides services local remote clients 
ejb architecture runs distributed components special container provided serviced application server see 
ee application server provides standard environment set runtime services specified ee including naming directory services authorization authentication state management integration web server run ee application boot operating system start ee application server jboss start necessary additional components required application database persistent state storage instruct application server deploy instantiate beans associated application 
chose ee widely enjoys developer infrastructure 
ee apis provide useful level indirection applications underlying platform exploit instrumentation recovery applications benefit 
various commercial open source implementations ee application servers chose jboss open source performs popular downloaded sourceforge times received javaworld editors choice award product category commercial competitors 
corporations including dow jones jboss demanding computing tasks 

jboss jagr shown augmented jboss number ways added modular monitoring plugin infrastructure shows sample monitors recovery manager recovery agent stall proxy fault injector automated failure path inference 
changes ee execution platform ee compliant application 
accommodating multiple failure monitors reflects observation single type monitoring detect types failures failure monitors imperfect 
report implemented failure monitors tracking exceptions middleware failures may cause detecting behavioral anomalies application relative historical data pp ejb ejb ejb ejb ejb ejb application server ee application server servlet jsp container fault map recovery mgr stall proxy web browsers injector ejb containers naming dir messaging security mgr recovery agent txn svc database clients 
jboss application generic recovery jagr ee application server self recover 
dark gray boxes indicate components added jboss 
mon detecting user visible failures 
plug failure monitor responsible sending failure notifications recovery manager rm 
modified jboss allow request tracing exception tracking 
request tracing allows follow path request system recording ejb invoked satisfying request 
exception tracking allows observe exceptions thrown ejb report recovery manager attempt immediate micro rebooting 
recovery manager recovery process resides outside application server proper design choice allows important degree fast recovery application server depends accurate rm actions 
rm suspects components failed instructs recovery agent inside jboss application server micro reboot suspect components 
keep clients failing parts system rebooting stall proxy intercept new client connections force wait seconds recovery process completes 
added fault injection mechanism automatic failure path inference algorithm obtain graph guides rm deciding specific components ee services recover 
call graph failure propagation map map 
sections describe extensions jboss detail section describes failure monitoring infrastructure section presents recovery manager agent automated failure path inference procedure 
section describes microreboots stall proxy 
section show experimental evidence jagr efficient self recovery abilities 
conclude related directions 

testbed applications ee applications rubis test described 
freely available sample application implements ecommerce site complete personalized web pages product catalogs shopping carts 
consists java files lines code 
uses database tables contains different kinds items pets suppliers rubis implements web auction service modeled ebay 
features include user accounts customized summary information item bidding comments pages 
rubis contains java files lines code uses mysql database back stores tables 

failure monitoring step recovery process detecting failure occurred components require recovery action 
recover failure solving harder problem deducing root cause fault 
individual monitors required perfect cross reports multiple failure monitors 
trying mask specific failure user require application specific knowledge goal report failure initiate generic recovery actions eliminate quickly failure impacts large number users 
approach sacrificing small number user sessions majority user population appropriate internet services large sites expect number concurrent users thousands servers handle hundreds thousands requests second 

exception monitor failure monitor implemented tracks reports java exceptions 
monitor instruments jagr internals intercept java exceptions thrown application platform components 
exception intercepted exception monitor reports error offending component recovery manager 
kinds failures exception monitor detect include application level failures dereferencing null pointers resource failures memory errors network failures 
course exception monitor detect failures manifest java exceptions 
includes failures cause java virtual machine crash hang 
previous experiments involving fault injection confirmed monitor correctly tracks faults component boundaries 
showed variety low level failures unrelated application hard failure remote node hosting needed service ultimately manifest java exceptions 
developing exceptions catches definitive signs failure 
exceptions thrown caught handled part normal behavior testbed applications 
example client tries log rubis auction site non existent user name database ejb thrown 
exception caught servlet suitable informational page generated correct behavior 
distinguish acceptable exceptions exceptions cause failure combine reports failure monitors 

failure monitor failure monitor module simulates real client fact current implementation colocated client emulator 
submits requests application web front trace requests specific application consideration 
monitor detects failures experienced user different levels reports failures recovery manager 
monitor reports request appears failing tell components system fault 
reports give information validate component failures reported 
current implementation rubis applications minimal modifications sophisticated 
levels detects failures network level connection client jagr closed connection refused timeout reading data server level inspects return codes reports indicate failure forbidden internal server error 
html level empty html page returned page contains certain keywords observed correspond failure error pages case fail 
definition monitor entirely application generic 
example generic approach searching keywords needs tailored application consideration 
searches fail string prefixes catching keywords error erroneous failed failure failing help page contains keywords encounter error sign needed special case page signal failure 
sophisticated detection gets application specific monitor 
instance placing bid rubis verifying correctly reflected item bid history requires semantic understanding auction site implemented functionality 
certain failures catch 
generally masked failures application attempts hide failure plausible response byzantine failures subtly corrupt data 
example failure ejb results web catalog masking failure innocuous incorrect stock message 
regardless level sophistication embed kinds failures reliably diagnosed human user difficulty realizing went wrong 
failure monitor described section notice failures looking anomalous behavior application 

pinpoint monitor third failure monitor implemented pinpoint 
detect failures pinpoint uses coarsegrained tagging client requests path system tracing component resource usages 
statistical data mining techniques pinpoint analyzes traced client requests paths capture aggregate behavior structure system 
pinpoint compares behavior structure historically observed behavior system 
looking anomalies relative past behavior pinpoint able detect failures requiring application specific knowledge semantics 
pinpoint traces client request separately explicitly compensate changes system structure performance caused variations workload mix 
detect failures pinpoint studies communication patterns components looking components calling 
pattern calls component changes significantly pinpoint tags component anomaly reports recovery manager failing component 
evaluated standalone pinpoint injecting faults various ejbs correctly identified faulty components cases 
injected declared exceptions time pinpoint correctly identified failed components undeclared exceptions fail silent component behavior failures detected 
remainder mis diagnosed detected 
modifications jboss trace requests pinpoint adds latency penalty ms client request depending number components degrades throughput mixed workload 
pinpoint analysis engine operates separate machine directly impact application performance 

recovery manager recovery agent shown recovery manager rm entity external application server 
attempts automated recovery involves system administrators automated recovery unsuccessful 
rely micro reboots recovery micro rebooting way recover transient failures internet systems 
recovery manager listens udp port failure notifications monitors 
failure information builds representation failure propagation paths system form graph structure described section 
updating graph recovery manager decides components affected determining connected graph component failed component resides 
rm activates stall proxy simultaneously reboots nodes connected component deactivates proxy 
effect reboots recovery manager sends reboot signals recovery agent 
recovery agent moment resides inside jagr 
charge micro rebooting ejbs receiving orders rm 
general case expect recovery agents various domains control charge rebooting entities run jurisdiction 
entities range ejbs entire java virtual machines jvms real machines case recovery agent ip addressable power supply 
targetted reboot recovery framework designed employ notion recursive reboots 
recovery manager responsible initiating coarser grained reboots recognizes previous reboots cured failure 
recovery manager ends rebooting entire system avail notify sysadmin pager email right levels rebooting micro reboot small subset ejbs restart entire application 
rm keeps track previously rebooted subset ejb new subset chooses restart entire application subset ejbs 
limited experiments encountered need recursive reboots 
rm charge recognizing repeating patterns preventing jagr going infinite reboot loops 
seen occur testbed possible components form undetected reboot failure cycle rebooting component causes second fail rebooting second component causes original component fail repeating process 
encountered situation practice implemented detection avoidance reboot loops 

determining failure propagation paths rm reboots failed components components thinks fault may propagated needs representation system fault propagation paths 
relying humans identify ways faults propagate systems unreliable large scale systems notorious exhibiting unexpected failure modes 
section describes detail rm obtains failure dependency graph application generic fashion human intervention 
technique automatic failure path inference uses systematic injection java exceptions discover dependencies ejbs 
stage process 
stage invasive relies controlled systematic fault injection cause system components fail takes order hours run requires priori knowledge application 
monitors report observed failures recovery manager constructs graph containing components nodes inter component fault propagation edges 
second non invasive stage continues passively monitor production system fault injection completed updates graph new failures observed 
second phase essentially performance overhead 
phase monitors rm aware faults injected purposely react faults naturally occurring exactly second phase 
time rm receives failure notification augments fmap decides reboot updated map 
recovery agent selectively disabled initial phase 
applying ee injected java exceptions applications rubis 
graphs exception propagation detailed accurate derived timeconsuming manual inspection analysis static application descriptions 
control control control control control control language 
generated failure propagation map 
complete accurate map obtained application deployment descriptors 
boxes diagram ejbs java server pages servlets 
databases shown bottom 
show maps obtained commerce application 
able find components edges represented static deployment information came application 
addition ignored edges incorrectly contained deployment information propagate faults components involved 
obtained similar results applying rubis 
illustrate additional information recovery manager maintains map node keeps track times corresponding component micro rebooted number times components failed 
edge fmap rm keeps track times fault detected having propagated edge 
historical information versions recovery manager avoid previously encountered reboot mistakes 

micro reboots rebooting single component opposed entire system refer micro reboot 
tier internet systems general ee applications particular structural properties rebooting relatively safe important persistent state managed separately temporary state workload characterized short lived independent requests 
managing persistence important state exclusively dedicated persistence tier tier internet systems guarantee safety restarting nodes tiers service 
lets simply safely recover range transient failures relatively complicated software presentation application tiers 
workload internet service consists short lived independent requests service partitioned requests fail clients unaffected 
additionally underlying protocol application logic stateless marked non idempotent requests users safely retry failed requests succeed 
lets reboot components system knowing users affected face minor inconvenience 
micro recovery vs system recovery meant reduce time recovery internet systems important increasing time failures 
showed micro reboots reduce recovery time small java system factor 
see similar benefit observed ee applications augmented existing ejb coarser reboot boundaries splitting application server separate subsystems naming service database remainder jagr 
informal empirical observations system administrators running large scale ee installations frequent failure mode jvms run memory recover failure necessary reboot affected node 
performed simple experiment compared time reboot server node times reboot various combinations subsystems plus application results shown table 
measurements way multiprocessor mhz intel xeon gb ecc ram scsi disks 
performed trials measurement variance 
restarted unit duration fraction reboot server jagr seconds restart jagr seconds restart seconds micro reboot ejb table 
restart times various granularities micro rebooting smallest application component ejb orders magnitude faster rebooting server order magnitude faster restarting application 

delaying client requests recovery process delay incoming requests stall proxy 
keeps clients seeing failures due directly recovery process failure clients perceive increased latency request 
induced latency finite stall requests maximum seconds consistent distraction thresholds identified return failure client recovery completed request admitted system 
feature directly part self management aspect jagr needed correctness 
allows automatic micro reboots happen minimal impact user 
failures resulting recovery masked short delays user request triggered failure experience error users may experience performance 

experimental validation jagr validate approach building self recovering application server fault injection trigger failures allow jagr recover faults injected java exceptions high level manifestation wide range underlying faults 
deliberately inject application level failures contrast injects low level hardware faults 
determining application visible failures result particular low level hardware faults requires construction fault dictionary proven difficult 
construction dictionary complicated software layers low level hardware application 
applying system replay workload exercises application consideration 
order gather workloads replay combination recording proxy load generator 
proxy intercepts interaction human user application web site records interactions workload trace file 
load generator plays back recorded traces simulating number concurrent clients forking separate thread 
clients think time inbetween requests soon response received request issued 
trace experiments shown captures multi hour interaction pet store comprising account creation operations purchases account updates browsing find publicly available request traces application experimental data graphs available www cs stanford edu candea papers jagr 
colleagues created interactively performing range operations typical commerce site interactions claim resulting mix particularly representative benchmarking sense commerce workload 
rubis recorded traces played rubis load generator comes software 
workload load generator simulates trace distinct user sessions executed serially 
session consists user logging performing various operations leaving site 
individual operation request service success failure unambiguously detected 
request fails load generator retries request maximum times experiments second interval retries 
attempts fail load generator abandons current session moves simulating customer re logs new customer arriving 
rest section describe categories experiments section provides quantitative evidence jagr able recover automatically application generic fashion rubis results section suggest recovery improve users experience 

application generic recovery goal determine jagr able recover correctly observed failures 
chose different exercise different parts injected unmodified vanilla jboss jagr 
consisted faults injected fault minute 
illustrates instantaneous availability systems placed workload 
time runs left right show total timelines 
top timeline vanilla jboss bottom timeline jagr 
bands indicate test client perceived system point time solid band indicates system appears available successfully executing client requests gap indicates server perceived 
experiment suggests jagr able allow clients continue doing useful recovery 
jagr able fully recover injected fault vanilla jboss longer functioned correctly injected fault 
human intervention coarse reboot higher latency self recovery needed resolve failures 
sparse impulses availability jboss post failure timelines correspond requests completing successfully failure requests static html 
client perceived instantaneous availability jboss jagr different 
faults injected minute jagr manages successfully autonomously recover time 
pages images served directly front 
requests involved application logic embedded ejbs failed means user able useful despite sporadic successful requests 
verify approach ran similar experiments rubis line auction application running automated inference algorithm obtain corresponding map 
making modifications applications able obtain encouraging results 
report interesting case 
experimentation noticed workload traces cause rubis deadlock especially large number users accessed application simultaneously 
hangs resulted application unresponsive client connections timing 
investigate specific operations workload caused hangs relied jagr autonomously recover hangs 
chosen form recovery micro reboot ejbs application due way rubis packaged ejbs jar file 
show results running concurrent clients rubis client ran dedicated cpu 
experiment shown inject faults wait application naturally fail allow jagr recover 
similar illustration method show vanilla jboss rubis behavior top part jagr rubis behavior bottom part 
diagram timelines client shown 
workload identical 
jagr recovers naturally occurring deadlocks rubis 
top half shows instantaneous service availability jboss rubis perceived clients bottom half shows jagr rubis 
deadlock occurred times running jagr able recover time jboss rubis hung right deadlock denying application access clients remainder run 
runs precise sequence events inside server application nondeterministic interleaving client requests potential race conditions inside application 
explains occurrence deadlock line different systems 
condition causes hangs time server logs indicate hang jboss rubis case type hangs experienced jagr rubis case 

self recovery improves user experience experiments previous section illustrated jagr ability recover suggested users may able get done server recovers 
section try see just better self recovering application server 
rubis runs encountered case user detection failure enabled complete masking failure users 
shows timelines corresponding concurrent clients 
injected fault server specifically nullpointerexception simulate real life data corruption error requests 
embedded load generator failure detected reported recovery manager 
jagr improve user experience offering illusion continuous availability 
fast effective recovery helps mask injected fault clients 
noticed exception 
recovery manager instructed stall proxy hold requests recovery completed stalled requests admitted system user successfully retried failed request 
net effect users delay seconds serving requests 
second distraction threshold discussed section fair say users notice failure 
fast recovery conjunction brief stall able mask failure system users 
heavy weight non micro reboot recovery employed delay resulted visible failures 
characterize effects general terms compared performability jagr rubis jboss rubis 
measured successful request throughput goodput system load experiencing faults 
platforms faults injected minutes 
workload consisted users executing exclusively read requests avoid deadlock problems described section 
plot goodput jboss left side jagr right side 
computed average throughput second intervals amortize variability resulting relatively small user population 
area throughput curve represents number requests successfully completed interval 
vanilla jboss case soon monitors detect problem jboss server automatically gets rebooted 
human administered system optimistic detection time practically zero assumes administrator system rebooting 
jagr system time ramp back original performance reboot 
part due rebooted server having page java classes recompile servlets due clients difficulty picking left highly disruptive reboot 
jagr automati time minutes jboss manual recovery time minutes jagr automatic self recovery 
jagr improves service performability 
shaded area curve represents total number requests succeeded jagr rubis better jboss rubis maintained goodput requests sec face faults 
cally micro reboots rubis ejbs affecting server notice jagr goodput drops requests sec recovery fast stall proxy maintains illusion availability part client population 
compute total number requests successfully completed observation period simply need compute area throughput curve 
yields total requests jboss managed rubis requests jagr managed rubis represents improvement particular experiment 
assume increase successful requests translates increase user satisfaction argue running ee applications jagr enhanced version jboss offer service providers competitive advantage 

related redundancy failover internet services popular tool reducing downtime 
techniques complementary strategy failed nodes eventually recovered restore system throughput close window vulnerability associated operating partial failure 
cnn com example slow node level recovery time lead entire service collapsing 
armor provides application generic services liveness monitoring reboot distributed applications 
provides checkpoint recovery applications written armor micro checkpointing api including armor armor middleware modules recover checkpoints 
compared armor attempting detect classes failures different types plug failure monitors collecting place recovery manager policy decisions rebooted attempt recovery 
interested supporting unmodified ee applications written checkpointing api recovery generic successively reboot larger subsystems fault goes away rebooted entire system failure detection offers room refinement 
seda project recognized value moving certain behaviors admission control load balancing runtime system applications running platform benefit 
seda case applications written event driven continuationpassing style admission control load balancing done implicitly seda middleware 
requires recoding application somewhat programming style 
jagr application modifications required 
implemented pluggable monitoring framework jagr monitors strictly complementary failure detection techniques 
common techniques detecting failures internet services low level monitoring heartbeats pings periodic high level application checks 
heartbeats pings advantage simple implement easy maintain 
lack ability detect application level failures 
complex tests detailed application semantics able detect applicationlevel failures individual applications expensive build 
addition require significant maintenance keep date rapidly evolving applications 

initial implementation jagr jboss application generic recovery 
jagr com application generic failure path inference path failure detection micro reboots 
uses controlled fault injection observation infer paths faults follow ee application 
path failure detection uses tagging client requests statistical analysis identify anomalous component behavior 
microreboots fast reboots perform sub application level recover components transient failures selectively rebooting components necessary repair failure reduce recovery time 
techniques designed autonomous making suited rapidly changing software internet services 
shown jagr improve availability ee applications 
monitors entirely application generic jagr infrastructure application agnostic 
expect plug ins developed able broaden class failures automated recovery possible 
intend take jagr realm clusters explore benefits offer context 
want enable recovery manager better utilize historical information actions rebooting component cured failures past decision making 
internet applications domain choice increasing complexity rapid pace subject continuous evolution deployment 
circumstances hope jagr self healing property insulate service administrators users intricacies applications run 
hope bring self recovering software wider user community making jagr available jboss release 
bhatti 
integrating quality web server design 
proc 
th international world wide web conference amsterdam holland 
candea cutler fox 
improving availability recursive micro reboots soft state system case study 
performance evaluation journal summer 
appear 
candea delgado chen fox 
automatic failure path inference generic introspection technique software systems 
proc 
rd ieee workshop internet applications san jose ca 
candea fox 
recursive turning reboot scalpel 
proc 
th workshop hot topics operating systems germany 
zwaenepoel 
performance scalability ejb applications 
proc 
th conference object oriented programming systems languages applications seattle wa 
chandra chen 
generic recovery application faults 
case study open source software 
proc 
international conference dependable systems networks new york ny 
chen fox brewer 
runtime paths macro analysis 
proc 
th workshop hot topics operating systems hawaii 
chen brewer fox 
pinpoint problem determination large dynamic internet services 
proc 
international conference dependable systems networks washington dc june 
fox patterson 
fast recovery trump high reliability 
proc 
nd workshop evaluating architecting system dependability san jose ca 
fu compiler directed program fault coverage highly available internet applications 
technical report rutgers university computer science dept 
jboss 
homepage 
www jboss org docs 
iyer ries patel lee xiao 
hierarchical simulation approach accurate fault modeling system dependability evaluation 
ieee transactions software engineering september october 
lefebvre 
cnn com facing world crisis 
th usenix systems administration conference 
invited talk 
lowell chandra chen 
exploring failure transparency limits generic recovery 
proc 
th usenix symposium operating systems design implementation san diego ca 
marcus stern 
blueprints high availability 
john wiley sons new york ny 
meyer 
evaluating performability computer systems 
ieee transactions computers aug 
miller 
response time man computer conversational transactions 
proc 
afips fall joint computer conference volume 
pai cox pai zwaenepoel 
flexible efficient application programming interface customizable proxy cache 
proc 
th usenix symposium internet technologies systems seattle wa 
patterson brown candea chen cutler fox oppenheimer sastry tetzlaff 
recovery oriented computing roc motivation definition techniques case studies 
technical report ucb csd uc berkeley berkeley ca march 
sourceforge net 
world largest open source software development repository 
www sourceforge net 
sun microsystems 
ee platform specification 
java sun com ee 
sun microsystems 
java pet store demo 
developer java sun com developer releases 
welsh culler brewer 
seda architecture conditioned scalable internet services 
proc 
th acm symposium operating systems principles banff canada 
bagchi srinivasan iyer 
incorporating reconfigurability error detection recovery chameleon armor architecture 
technical report crhc university illinois urbanachampaign 

