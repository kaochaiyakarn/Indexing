pp 
step intelligence art natural artificial artificial proceedings th finnish artificial intelligence conference edited ala kaski 
finnish artificial intelligence society 
complex systems computation group helsinki institute information technology hiit university helsinki helsinki university technology box fin hut finland cwi box nl sj amsterdam netherlands 
bayesian network models widely supervised prediction tasks classification 
naive bayes nb classifier particular successfully applied fields 
usually parameters determined unsupervised methods likelihood maximization 
lead seriously biased prediction independence assumptions nb model rarely hold 
clear find parameters maximizing supervised likelihood posterior globally 
show supervised learning problem solved efficiently 
introduce alternative parametrization supervised likelihood concave 
result follows maximum easily local optimization methods 
test results show feasible highly beneficial 
years recognized supervised prediction tasks classification supervised learning algorithm supervised conditional likelihood maximization :10.1.1.19.9829:10.1.1.30.9978
applications related type task model parameters determined unsupervised methods ordinary likelihood maximization 
main reasons discrepancy difficulty finding global maximum supervised likelihood 
show problem solved naive bayes nb classifier 
find supervised maximum likelihood parameters nb model different manner take logarithms original parameters drop sum constraints 
new parametrization remarkable property supervised likelihood concave function parameters 
find global maximum supervised likelihood parameters simple local optimization techniques hill climbing 
experimental part demonstrate usefulness idea applying infer supervised naive bayes distributions variety real world data sets 
data sets supervised nb classifiers lead substantially better predictions obtained ordinary unsupervised nb classifiers 
organized follows 
section review standard unsupervised naive bayes classifier supervised version 
show model parametrized usual way supervised likelihood concave function parameters hinders optimization 
section introduce model 
model looks different supervised nb section show models fact represent exactly conditional distributions 
supervised likelihood data function parameters model concave parameter set convex 
section provides alternative interpretations model 
section argue technical reasons useful equip models prior effectively maximize supervised bayesian posterior plain supervised likelihood 
section compare supervised nb standard nb variety real world data sets 
outlook research section 
supervised naive bayes model 
xm discrete random vector variable xi takes values 
ni 
loss generality class variable want predict remaining 
xm predictor variables attributes 
training data set consists vectors containing entries 
dn dj dj 
djm 
classification task goal build training data model predicts value class variable values predictors 
standard multinomial naive bayes classifier nb see consists pa rameters 
kil 


ni 
default distribution class ki xi distribution values xi class 
restrict parameters lie set defined 
ni 
kil kil note closure set parameter vectors correspond naive bayes distribution 
set parameter vectors corresponding naive bayes distribution strictly positive probabilities 
essential loss generality may restrict parameters shall see section 
unsupervised log likelihood defined log log dj dj dj dj equality refers 
independent identically distributed assumption inherent naive bayes model 
eq 
rewritten log hk log ni log kil hk data frequency counters hk number vectors dj class dj number class vectors standard nb classifier data infers maximum likelihood ml parameters maximizing 
inferred parameters usually supervised prediction tasks 
xm xm wants predictions value 
done conditional distribution 
xm 
distribution looks follows 
xm xm ixi 
argued prediction task supervised score function determine parameters model supervised conditional 
leads supervised log likelihood ss defined follows 

xm single data vector 
log 
xm log sample 
dn dj hk log log ni ixi dj log kil log 

interested parameter vectors maximizing supervised log likelihood 
generally different commonly ml parameters arrived maximizing eq 
analytically exactly proportional corresponding training data frequency vectors characterization complicated see section 
interested conditional supervised likelihood restrict attention set conditional distributions 
formally define supervised naive bayes model set conditional distributions 
xm defined eq 

xm 
conditional distributions extended outcomes independence 
sample parameters results supervised log likelihood 
example parametrization 
consider domain binary variables 

values supervised score vector sx word score order stress log likelihood objective optimized 
constant wrt 
shows exist 
index different joint distribution index conditional distribution 
problem maximizing supervised likelihood conventional nb parametrization concave 
simple example shows supervised score may peak line contradicting concavity 
example non concavity supervised score 
consider domain previous example 
possible data vectors appear exactly data set set 
shows plot supervised log likelihood peaks twice 
theta phi supervised log likelihood peaks twice varies 
non concavity complicated optimization methods maximize supervised score contrast unsupervised nb case solve problem analytically 
algorithms may converge slowly due non concavity score 
supervised model introduce model set conditional distributions shall see just supervised nb disguise distribution defined terms parameter vector kil indexed 
set parameter vectors denoted formally define set nm 
indexes conditional distribution 
xm follows 
data vector 
xm define 
xm xm exp exp exp exp ixi 
distributions 
xm extended outcomes independence product distributions 
immediately verifies 
xm 
xm term sum positive 
confirms 
xm defines conditional distribution 
xm 
supervised log likelihood corresponding conditional distribution denoted 
course just log log exp extended sample 
dn independence ixi dj 
define supervised model ml set conditional distributions indexed 
xm model parameters mapping parameters models proposition 
vector set kil index conditional distribution proof plug 
supervised conditional models indexed corresponding conditional nb distributions indexed corresponding conditional distributions 
show seemingly different conditional models fact equal 
equality concavity see related define log transformation follows 
parameter vector corresponding transformed parameters defined log kil log kil plugging see 
xm 
xm 
shows parameter set indexing distribution transformed parameter set indexing conditional distribution result may tempted view simply parametrization terms logarithms original parameters 
complicated parameters allowed just exponentiated interpreted probabilities sum respectively 
theorem proof shown show mn vector components 

cn 
cn 
define 
kil kil 
infer mn 
see holds just substitute left hand side see cancel 
define kil exp kil exp kil exp exp 
evidently choose ni subsequently 
implies substituting find equation implies 
equality proved think parametrization supervised naive bayes model call parametrization saw supervised log likelihood concave standard supervised nb 
main theorem remarkably concave parametrization theorem convex set 
ii sample length concave strictly concave 
function proof item immediate proof item ii see section technical report 
items ii demonstrate finding nb distribution maximizing supervised likelihood parametrization finding maximum concave function convex set 
simple local optimization method hill climbing 
log likelihood local maxima standard parametrization see concave ripples wrinkles 
greiner zhou parametrization report worked better standard parametrization 
results explain 
example surface 
look domain consisting binary variables time choose model 
set log log 
gives clue possible objective peak twice example 
theta phi supervised log likelihood concave function new parameters pointed line shows transform 
phi alternative views model parametrization allows think naive bayes classifier discriminative diagnostic generative sampling model see 
formally identical supervised naive bayes model interpreted terms logistic regression neural networks models 
discrete supervised logistic regression 
think conditional model predictor combines information attributes softmax 
usually done continuous binary case linear softmax 
gives interpretation depicting naive bayes model bayesian network guises 

xm 
xm standard nb net left model right 
arcs reversed resulting product distribution replaced softmax denoted 
technically get logistic regression model model create binary regressor variable possible value attribute binary output variable value class 
looking model light proves concavity theorem ii supervised log likelihood known concave logistic regression models see 
neural networks 
conditional distribution equivalent single layer hidden units linear feed forward neural network logistic sigmoid softmax activation function see 
type network inputs outputs encoded called encoding binary node variable value combination 
logistic activation function applied linear function resulting set indicator variables activation value output nodes interpreted probabilities corresponding class values 
terms represent default classification model implemented adding called bias node node constant input network 
parameters neural network usually optimized maximize conditional likelihood equivalently called cross entropy 
follows theorem objective function neural network concave 
calibration 
model interesting property derivative zero holds dj 
djm hk dj 
djm 
parameters supervised task exactly calibrated wrt 
subsets dil dj sense 
optimizing sl means ni calibration tests simultaneously 
assumption model saves incoherent recalibrate see 
spin find model solve calibration problem form dj 
djm dj dj dj collection indicator functions computable 
xm local optimization methods 
long run unlimited amount data available calibrated respect calibration tests see 
limited data availability calibration tests implicit naive bayes model fnb fil fil di sensible choice cases 
choices need correspond bayesian model 
order avoid fitting may instance prune nb model demanding calibration sets certain minimal size arriving fc fil dil 
small data sets resulting model may consist considerably fewer parameters depending 
need prior problem practical applications sample typically frequency counters 
case supervised likelihood ordinary parameterization maximized parameter vector parameters conditional class probabilities equal 
poses problem supervised likelihood optimization model maximized kil supervised likelihood sl maximized kil sl maximum optimization task hard perform 
problem arise subtle situations illustrated example example divergence 
consider domain binary variables 
maximized cf 
example 
seen follows 
vectors conditional likelihood improved pair contradicting class 
observe 
avoid problems introducing bayesian parameter priors 
impose strictly concave prior goes parameter 
introduce set constraints parameters ensuring existence single maximum new objective log 
xm log 
restricted parameter space 
note maximizing equivalent bayesian maximum posteriori map estimation conditional model ml prior 
shown earlier ordinary unsupervised naive bayes danger fitting training data small sample sizes data predictions greatly improved imposing prior parameters bayesian map bayesian evidence ml prediction 
supervised nb inclined worse fitting unsupervised nb uses amount parameters model smaller domain 
experiments reported section decided strictly technical prior draws parameters little bit closer zero zero influence fitting 
prior simply normalized product parameters exp exp empirical evaluation exp kil exp il 
want illustrate usefulness method reporting results test runs 
globally optimal supervised parameters obtained maximizing gradient ascent standard line search 
test bed took realworld data sets uci repository 
continuous data discretized discretizations www cs helsinki fi data 
cross validation method leave loo avoiding variance due random splits 
table lists data sets ordered size log score percentage correct predictions obtained standard naive bayes uniform prior evidence prediction supervised method 
winner scores boldfaced 
observe cases supervised method produced better log score 
small data sets apparently fitted training data 
larger data sets consistently outperformed standard nb cases quite margin 
contrast smaller data sets standard nb outperformed supervised nb smaller margins 
exactly type behavior expected 
completeness mention loss supervised method won score 
wins larger data sets agreement results 
showed parameter transformation described effectively find parameters maximizing global supervised likelihood naive table leave cross validation results 
name size data set prediction loss unsupervised vs supervised naive bayes log score percentage correct predictions 
best scores boldfaced 
data set size uns 
nb sup 
nb mushrooms page bl 
abalone segment 
yeast german cr 
tictactoe vehicle annealing diabetes bc wisc 
cr 
balance sc 
voting mole fever 
ionosphere liver pr 
tumor ecoli soybean hd cleve hd hung 
breast hd stats thyroid glass id wine hepatitis iris plant 
postop 
bayes model 
empirical results reported suggest technique improving accuracy naive bayes classifier cases considerable amount 
extend theoretical results general classes bayesian network models including tan tree augmented nb models 
intend perform experiments involve complicated models 
plan investigate prevent fitting small data samples theoretically elaborate parameter priors simple technical prior 

research supported national technology agency academy finland 
bishop 
neural networks pattern recognition 
oxford university press 
dawid 
properties diagnostic data distributions 
biometrics 
dawid 
calibrated bayesian 
journal american statistical association 
friedman geiger goldszmidt 
bayesian network classifiers 
machine learning 
greiner grove schuurmans 
learning bayesian nets perform 
proceedings th conference uncertainty artificial intelligence uai providence august 
greiner zhou 
discriminant parameter learning belief net classifiers 
www cs ualberta ca greiner 
heckerman meek 
models selection criteria regression classification 
proceedings th conference uncertainty artificial intelligence uai providence august 
kontkanen ki tirri gr 
predictive distributions bayesian networks 
statistics computing 
kontkanen ki tirri 
classifier learning supervised marginal likelihood 
breese koller editors proceedings th international conference uncertainty artificial intelligence uai 
morgan kaufmann publishers 
ng jordan 
discriminative vs generative classifiers comparison logistic regression naive bayes 
advances neural information processing systems 
duffy 
statistical analysis discrete data 
springer verlag new york 

calibration bayesian learning 
org research wp wp ps 
gr roos ki tirri 
supervised learning bayesian network parameters 
technical report helsinki institute information technology hiit 
hiit fi articles hiit ps 

