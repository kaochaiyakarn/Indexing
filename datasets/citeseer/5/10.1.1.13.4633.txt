affinity scheduling staged server architectures stavros ailamaki march cmu cs school computer science carnegie mellon university pittsburgh pa modern servers typically process request streams assigning worker thread request rely round robin policy context switching 
programming paradigm intuitive oblivious execution state ignores software module affinity processor caches 
result resumed threads execution suffer additional delays due conflict compulsory misses populating caches evicted working sets 
alternatively staged programming paradigm divides computation stages allows stage request thread cohort scheduling improves module affinity 
technical report introduces novel cohort scheduling techniques staged software servers follow production line model operation mathematical framework methodically quantify performance trade offs techniques 
markov chain analysis scheduling techniques matches simulation results 
model staged database server proposed policies exploit data instruction locality wide range workload parameter values outperform traditional techniques fcfs processor sharing 
consequently results justify restructuring wide class software servers incorporate staged programming paradigm 
email stavros cs cmu edu stavros partially sponsored lilian foundation fellowship gratefully acknowledges sup port 
keywords dbms databases performance staged server scheduling cache conscious 
modem application servers typically accept streams requests assign request execution thread 
context switching threads discretion operating system typically employ time sharing context switching policy 
programming paradigm elegant intuitive request spends different amount time software module 
round robin time shar ing context switching oblivious execution state ignores module affinity memory hierar chy resources 
consequently resumed thread suffers additional delays re populating processor caches evicted working set 
processor memory speed gap continues increase cache misses increas ingly important factor server performance 
research shown gap affects commercial database server performance significantly affects engineering scientific desktop applications 
reason database applications access memory subsystem far desktop engineering workloads 
database workloads exhibit large instruction footprints fight data dependencies reduce instruction level parallelism opportunity incur data instruction transfer delays 
alleviate memory delays numerous data placement techniques cache conscious algorithms proposed exploit spatial temporal locality 
techniques improve locality request limited effects locality requests 
database servers typically assign threads queries context switching concurrent requests destroy data instruction locality caches 
running oltp workloads instance misses occur due conflicts threads working sets replace cache 
incoming requests go different modules server code cpu switches execution concurrent requests 
illustrates example successive executions module interleaved executions modules forcing module content repeatedly evicted restored memory hierarchy 
systems expected deeper memory hierarchies adaptive programming solution necessary best utilize available memory resources 
staged server programming paradigm divides computation server performs response request stages schedules request execution stage time improve locality 
staged server defers executing stage queue accumulates operands processes entire queue repeatedly executing operation 
queue empty processor keeps tra list stages going forward backward 
authors demonstrated approach improve performance simple custom built web server reducing frequency cache misses application operating system code 
web server successful time incoming requests cpu context affinity poor cache performance cpu cpu context affinity high cache performance uncontrolled context switching lead poor cache performance 
experiment remain significant scheduling trade offs solved 
instance clear circumstances policy delay execution individual requests moving module locality benefit offset 
addition forward backward list traversal scheduling algorithm proposed techniques may better variety application workloads 
staged server paradigm improve performance application server application bound delays memory hierarchy requests executed defined modules mod ule topology relatively simple minimize scheduling complexity 
database management systems meet requirements excellent candidate staged programming paradigm 
discussion section indicates database applications suffer memory hierarchy delays types workloads increases potential performance gains 
furthermore database servers naturally modular queries follow sequence parser optimizer transaction manager 
different requests benefit locality module access common data structures code 
addition flow execution purely sequential search space scheduling policies conveniently manageable possi ble policies include combination number requests receive service sev eral time receive service module completion cutoff value order visiting modules 
attempt extensively study scheduling trade offs apply ing staged server paradigm context complex application server database system 
contributions technical report twofold 
introduces novel cohort scheduling techniques staged software servers follow production line model operation 
presents mathematical framework methodically quantify performance trade offs tech niques 
model proposed policies markov chain validate solution sim ulation scripts 
analysis applies wide class systems production line model operation may benefit stage divided service repeatedly performed batches jobs 
context simulated database system compare scheduling alternatives traditional processor sharing model plain fcfs proposed scheduling techniques 
order show performance gains possible scenario vary parameters importance module loading time compared execution time load system 
results show pro posed policies exploit data instruction locality wide range workload parameter values perform traditional techniques fcfs processor sharing 
rest report organized follows 
section reviews additional related 
section pre sents framework assumptions defines problem describes existing pro posed scheduling policies 
section presents queueing model incorporates locality aware execution analysis 
section discusses results measuring comparing formance policies various scenarios 
section discusses system assumptions simula tion 
section concludes report results 
related term affinity scheduling widely shared memory multiprocessor systems 
environments beneficial schedule task certain processor contains relevant data local cache 
type affinity similar staged program ming paradigm tries leverage approaches problem restructuring single application exploit locality improve locality collection tasks generic setting 
server architectures adopted processor sharing ps scheduling fair scheduling policy 
cpu spend fixed amount time typically order lms active process keep switching processes round robin fashion 
tries argue favor shortest remaining processing time scheduling case single server 
authors shown traditional assumptions workload modeling accurate 
specifically shown unix jobs sizes follow pareto heavy tailed distribution exponential 
effects account distribution counter intuitive discussed 
database community significant research improving dbms performance making database systems components cache conscious 
relevant studies target part query execution lifetime query processing algorithms index manipulation cpu schedule module task fio 
mapping daa schemes 
proposed ake daa instruction seer modules decrease cache misses memo delays execution program 
authors proposed ven seda highly services 
seda decomposes complex driven application imo se sages connected queues 
design avoids high overhead wih hread models enables ser vices conditioned load preventing resources demand exceeds service 
potential benefits design orthogonal benefits cache awe scheduling policy provided ha sage definition done wih ing la er ino problem formulation proposed methods consider case single cpu database server memory resident workload 
query sub server passes stages execution stages corresponds server module 
module defined relatively autonomous terms data structures owned accessed part database server performs response request 
example module parser optimizer database shown 
data structures referenced code executed different queries execution single module overlap variable amount instructions data 
instance different queries may different execution operators produce plans access different tuples common code buffer pool code index look common data symbol table database catalog 
common data structures instructions module accessed loaded cache subsequent executions different requests module significantly reduce memory delays reduce cycles instruction cpi rate 
model assumes loss generality entire set module data structures fit higher levels memory hierarchy total eviction takes place cpu switches different module 
results drawn model may easily transferred processor cache memory cpu memory disk hierarchy 
processor sharing ps fails reuse cache contents switches query query random way respect query current execution module 
exploit data locality propose tech niques scheduling queries different modules evaluate processor sharing 
order compare different scheduling policies assume poisson stream queries query service time follows exponential distribution 
query starts execution certain mod ule common data structures cache cpu previously working different module query charged additional fixed cpu demand module 
extra cpu demand represents time spent memory stalls due fetching common data structures main memory cache query 
ps stalls default ones lies opportunity better exploiting data locality 
clearly assumptions related system architecture single cpu memory dbms context switch costs common data structure sizes cache behavior simplifications stand problem create framework evaluating scheduling policies 
relaxing assump tions affect generality results discussed detail section 
assumption respect query sizes interarrival times match workloads universally provides tractable model comparing different policies 
specific problem consideration bursty arrivals increase potential cache conscious scheduling poisson arrivals adequate qualitatively comparing different scheduling policies 
experimentation section departs exponentially distributed query sizes exam ines happen case highly variable distribution small fraction load consists really large queries 
shown distribution closely model real workloads 
assumptions mentioned exact problem definition follows 
problem definition queries arrive dbms server poisson process rate query passes order series modules 
module separate queue 
cpu system 
query arrives server service time drawn exponential distribution mean query executes module spends time previous query execution happened different module cpu spends time fixed load module common contents cache 
goal devise scheduling policy minimizes average query response time 
table symbol definitions symbol explanation value number modules fraction query total execution time ai ai spent module set query arrival rate tion section mean query service time common data code cache total time query spends loading cache common data code mean query service time module common data code cache time query spends module loading cache common data code percentage query execution time spent average servicing common instruction data cache misses system load baseline scheduling policies come serve fcfs prevailing ps 
fcfs query arrives module executes continues module exe modules 
new queries arrive module wait current query exits system 
cpu queue accumulates input entrance module 
ps fcfs oblivious module structure locality aware newly loaded module previous cache 
proposed scheduling policies gated dynamic gated 
policy dynamically imposes gate incoming queries executes admitted group queries batch module completion 
execution takes place come served basis queue module query queue module starts execution fetches common data structures module cache 
rest queries form current batch pass module paying penalty loading common data structures cache 
query batch finishes execution module cpu shifts fcfs fcfs fcfs gate empty empty illustration dynamic gated modules queue module processes batch fcfs fashion 
incoming que ries database server queued module queue 
eventually current batch moves module query leaves system immediately execution 
cpu shifts module marks new batch admitted queries 
queries lated far module queue 
point time gate imposed incoming queries duration batch execution 
gate defines batch size time module resumes execution call policy dynamic gated gated 
process illustrated modules 
cpu shifts module queue empty gated reduces plain fcfs 
note gated cache conscious scheduling policy pays penalty loading module cache batch queries 
gated threshold gated 
policy works similarly gated way specifies size admitted batch queries 
gated explicitly defines upper threshold number queries pass module form batch maximum size queries queued module cpu finishes previous batch gated admit just queries rest considered batch 
policy reduces fcfs 
non gated 
policy admits queries queued module works module queue empty 
point cpu moves module proceeds fashion gated gated 
current batch exits system cpu shifts module keeps admitting queries done module 
non gated sensitive star vation continuous stream queries cause server indefinitely module 
analytic workloads didn produce behavior time mechanism necessary real implementation 
policy similar described applied framework 
gated cutoff gated 
possible issue previous policies large query essentially block way smaller ones lead momentarily higher response times 
exponential distribution query sizes problem surface average response fcfs fcfs gate empty size cutoff size cutoff empty size cutoff illustration cutoff gated modules time measured 
majority system load attributed relatively small query sizes close mean 
heavy tailed distribution hand typically involves half system load infrequent large queries 
scenario fcfs policies lead unreasonably high response times compared processor sharing 
gated tries bridge cache awareness gated gated exhibit fairness presence large queries ps shows 
gated apart imposed gate incoming queries module dynamically predefined threshold additional cutoff value applies time cpu spends query module 
cpu exceeds cutoff value switches execution rest queries queue module leaving large query unfinished 
query rejoin batch eventually resume execution 
remaining cpu demand cur rent module drops cutoff value advance module 
way small large queries progress benefiting increased data locality 
cutoff technique foreground background scheduling policy unix large jobs identified pushed separate queue receive service small jobs default queue 
difference gated large queries receive service presence small queries batch passing 
analysis ps fcfs fcfs ps query sees modules server mean service time mean module service time plus module load time 
query serve time units plus variable amount time drawn exponential distribution mean ps mean response time expected time system server ps rs fcfs pollaczek formula applies expected time system server tr rol si formula needs moments general query size distribution exp dx lx dx lm expected response time query problem definition fcfs lm fcfs analysis queue staged locality aware policy section considers special case queue notion data locality incorporated job service times 
cpu assume job undergoes series execution steps modules 
modules service time affected cache hit ratio 
job moves execution module new data structures new instructions need loaded cache 
second job preemptively scheduled execute module previ ous job working cache hit ratio increases service time second job reduced 
analysis described context staged server paradigm apply wider class servers follow production line model operation 
instance imagine single robotic arm part chain performing tasks incoming items 
suppose tasks require time special time consuming preparation behalf robotic arm switching position functionality 
efficient robotic arm treated incoming items batch performed task batch paying penalty tion incoming batch 
system notion cache common data structures replaced preparation robotic arm common task 
order model queue assume jobs pass execution modules 
breakdown service requirement modules common data associated module fit entirely cache higher levels memory hierarchy general 
total ser vice time job suffers default cache ratio fcfs case drawn exponential distribution mean job executes module just job brought common data module cache requires total service time drawn tial distribution mean markov chain states jobs queue size admitted batch jobs completed batch existing scheduling policies preemptive non preemptive queue oblivious cache performance 
exponential service times non size policies result expected time system job arrival rate 
rest section analyses gated queue just described 
equivalent way think proposed cache conscious execution modified fcfs policy 
policy job queue group gets uninterrupted service rate waits jobs group finish 
jobs turn executes fcfs fashion accelerated rate waits rest note defined zero 
job group finishes execution jobs leave system time cpu looks queue jobs wait consist group jobs queue job arrive system consist group case cache conscious execution reduces true fcfs policy 
modified fcfs scheme behaves proposed cache conscious execution terms number jobs system time 
interarrival service times exponential markov chain written modified fcfs scheme illustrated 
state chain defined num ber jobs waiting queue admitted number jobs consisting current batch number jobs current batch finished execution normal fcfs wait members batch complete execution 
batch comes completion departure rate batch size rate batch size state form departure leads state jobs waiting queue consist batch 
solution markov chain 
write balance equation state rate leave state equals rate enter state 
balance equation convention write probabilities form po goal express state probabilities functions 
balance equation pm lpm pm substituting solving derive 
give solving 
conclude 
function pm kp xy pm pn convention sum kt iz iz il thought sequence non negative integer recursive definition written useful want evaluate probabilities 
need compute probabilities 
balance equation rewrite 
ea write balance equation xp written probability state system idle equals minus sum probabilities possible state re lz zd ia im oz equations see probabilities part set infinite number linear equations value obtained able solve linear system infinite equations 
solution general representation quantity defined ii lkl 
evaluation 
solution approximated solving finite job batch sees system simple fcfs server system stable sum state probabilities goes goes infinity 
dynamic programming efficiently evaluate aim matlab solve linear equations range 
results matched exactly simulation scripts combinations tried 
experiments experiments set number modules equal 
simplicity equal percent age service time breakdown assigned different modules query spends equal time modules 
ps fcfs affected number modules service time breakdown 
gated algorithms benefit biased assignment service times different modules 
happens queries spend significant amount time module 
average queries leave server faster execute fcfs fashion delayed queries batch benefit module locality time 
typically case real dbms number modules service time breakdown remain experiments 
time takes module load common data structures instructions equal que ries varies experiments percentage total expected mean value execution time query 
mean query execution time cpu demand case common instructions data structures need loaded cache case fcfs ps looms 
value consists fixed component time takes modules load common data code variable query service time common code data cache 
variable component takes values experiments distributions exponential bounded pareto highly variable distribution 
values total mean cpu demand resulted relative dif ferences response times policies tested performance graphs value 
results ps fcfs derived analytical formulas gated policies simulation scripts 
confidence intervals tight omitted graphs better readability 
table shows experimentation parameters value range 
table simulation parameters parameter variance value range number modules fixed ai fraction execution time module fixed query arrival rate poisson queries sec re query service time module loaded see mean looms query service time modules loaded exponential bounded pareto mean looms common data code loading time equal queries looms effect various degrees data locality different module loading times experiment variable component mean query cpu demand drawn tial distribution mean range looms 
initially query arrival rate set produce sys tem load 
experiment compares mean response time query ps fcfs gated non gated gated various module loading times time takes modules fetch com mon data structures code cache 
time varies percentage mean query cpu demand mean query service time corresponds private data instructions adjusted accordingly re looms 
value viewed percentage exe system load 
gated non gated fcfs execution time spent fetching common data code system load 
gated non gated fcfs 
execution time spent fetching common data code mean response times system load left system load right cution time spent servicing cache misses attributed common instructions data default server configuration ps 
results 
left graph shows gated family algorithms performs better ps mod ule loading times account query execution time 
response times twice fast improve module load time significant 
hand module loading times correspond execution time non gated gated policies show worse response times 
gated policies gated performs best 
gated forms consistently outperforming ps configurations 
reason gated forms better gated non gated closer approximates fcfs gated query batch queries delayed query 
benefits cache hits reduced important query delayed queries 
note response time fcfs drops percentage module loading time increases 
fixed part service time reduces variability queueing time delays 
gated performed better scenario threshold value 
right graph experimental setup arrival rate set create system load 
system load increases trends gated family policies change 
performance ps fcfs drops significantly result gated policies form better values module loading time percentages 
gains gated poli cies increase times reduced response times 
gated policy choice loses ps ability improving performance compared ps com mon memory low outweigh slightly worse performance gated non gated high percentages common memory 
execution time spent fetching common data code direct comparison ps gated direct comparison gated ps experiment directly compares ps gated gated set plotting area policies results lower response times 
exponential distribution query sizes 
graph produced varying system load module loading times 
shows relative speedup gated ps wide range different locality scenarios 
axis percentage execution time eliminated query finds common data structures module cache 
value varies 
axis server load varied arrival rate achieve server loads 
right axis denote areas relative speedup gated ps certain range 
areas darker color correspond higher speedup white area corresponds combinations poli cies perform 
ps able perform better gated small area left graph relative speedup ps area exceed 
effect large query sizes experiment study effect highly variable distribution 
exponential distribution assumes remaining query service times independent cpu time far 
true real workloads small queries case large ones increasingly take cpu time 
exactly property called decreasing failure rate characteristic heavy tailed distribution 
distribution example pareto distribution small fraction largest queries comprise half system load 
ps fair policy insensitive different distributions need mean service time arrival rate compute mean response time rest policies 
really large query system load gated exponential gated bounded pareto gated exponential 
execution time spent fetching common data code highly variable distribution bounded pareto block way smaller ones result higher mean response times 
section test gated push small queries system fast exploiting data locality 
distribution bounded pareto variable component total query size fixed mean range looms depending choice mean query cpu demand remains looms 
parameter denotes maximum query size set sec 
parameter smallest query size set ms value computed rest parameters set mean 
parameter typically ranges defines degree variability way pareto 
approaches zero distribution variable 
bounded pareto differs pareto moments finite produces query sizes high variance 
chose set equal 
probability mass function bounded pareto defined ak gated policy manually set fixed cutoff value looms mean cpu demand 
real environment implemented monitoring query sizes aver age value cutoff 
repeated experiment time tested gated exponential bounded pareto distributions 
fcfs non gated gated gated explode bounded pareto result large response times don appear plot 
show gated response times exponential distribution 
ps defini tion behaves exactly distributions 
results 
case gated bounded pareto confidence intervals shown wider due highly vari able distribution 
shows gated bounded pareto wins ps module loading times mean query execution time 
gated bounded pareto worse compared gated exponential distribution manages exploit data locality cache conscious scheduling avoid pitfall working indefinitely large queries 
interesting gated exponential distribution slightly worse gated 
means gated policy choice apriori knowledge workload char 
increasing system load trends fight graph observed 
gains gated family ps increased 
relaxing model assumptions order approach problem low cache performance today servers particularly dbmss due uncontrolled swapping concurrent requests cpu assumptions 
served purpose formulating framework existing proposed solutions compared understood 
section discusses assumptions related architecture system affect solution experimental results single cpu 
high commercial database system installations typically run high performance multiprocessor systems 
essentially adds degree freedom space possible sched policies straightforward extend proposed approach include execution multiple cpus 
divide modules groups assign group processor 
analysis report holds individual cpu 
additional opportunity cache con execution assigning specific modules cpu scheduling queries accordingly higher number queries pass module cpu higher performance gains expected illustrated increased system load 
memory dbms 
model discussed processor cache memory hierarchy results easily applied cpu memory disk hierarchy exploiting memory locality 
inter cause suspension thread execution time slice assigned thread 
real servers mechanism overlap mask latencies various system devices 
assumption memory dbms removes need premature thread preemption synchronization purposes allows simpler execution model 
blocking threads affect way proposed policies 
thread unblocks joins batch queries passing module executes cpu idle 
context switch cost 
cost applies policies come serve inter doesn affect relative performance gains 
context switches tend hap pen frequently gated family policies query tries execute entirely interruptions possible module 
common data structures cache benefits model 
proposed framework models benefit cache hits due staged execution single parameter module percentage execution time spent servicing cache misses attributed common instructions code queries includes possible code data overlap queries executing module 
number combination overlaps data categories simplifies presentation son scheduling policies 
modern servers especially commercial database servers run current platforms typically suffer high processor memory data instruction transfer delays 
despite ongoing effort create locality aware algorithms interference caused context switching results high penalties due additional conflict compulsory cache misses 
preserve data locality execution threads staged server programming model allows incoming queries queue database server modules appropriate scheduling policies minimize conflicts different modules working sets 
paradigm ideal database systems due modular memory demanding nature 
contributions technical report twofold 
introduces novel cohort scheduling techniques staged software servers follow production line model operation 
presents mathematical framework methodically quantify performance trade offs tech niques 
markov chain analysis apply wide class systems may benefit stage divided service repeatedly performed batches jobs 
context simulated database sys tem compare scheduling alternatives traditional processor sharing model plain fcfs proposed scheduling techniques 
order show performance gains possible scenario vary parameters importance module loading time compared execution time load system 
experimental results show new scheduling policies outperform processor sharing delivering fold response time improvement configurations exploit data instruction locality wide range workload parameter values 
expected gains real large staged system lower due additional implementation related overheads report shows scheduling trade delaying requests forming batch justifiable pays low degrees inter request locality 
james larus michael parkes 
cohort scheduling enhance server performance microsoft research technical report msr tr march 
chen phillip gibbons todd mowry 
improving index performance prefetching proceedings sigmod conference may 
chilimbi larus hill 
making pointer data structures cache conscious 
ieee computer december 
goetz graefe ke larson 
tree indexes cpu caches 
proceedings international conference data engineering pp 
kant naughton 
cache conscious algorithms relational query processing 
proceedings th international conference large data bases vldb pp 
september 
ailamaki hill 
weaving relations cache performance 
proceedings th international conference large databases vldb roma italy september ailamaki dewitt hill wood 
dbmss modem processor time go proceedings th international conference large databases vldb edinburgh scotland september 
kumar thread cache analysis modified tpc workload proceedings second workshop computer architecture evaluation commercial workloads 
orlando fl 
rosenblum bugnion herrod witchel gupta impact architectural trends operating system performance proceedings fifteenth acm symposium operating system principles 
copper resort pp 

keeton patterson raphael baker performance characterization quad pentium pro smp oltp workloads proceedings th annual international symposium computer architecture barcelona spain pp 

mark crovella mor harchol balter cristina task assignment distributed system improving performance load proceedings acm sigmetrics madison wi 
hennessy patterson 
computer architecture quantitative approach nd edition morgan kaufmann nikhil bansal mor harchol balter 
analysis scheduling investigating unfairness 
appear proceedings acm sigmetrics conference measurement modeling computer systems 
matt welsh david culler eric brewer 
seda architecture conditioned scalable intemet services proceedings eighteenth symposium operating systems principles sosp banff canada october 
maynard 
contrasting characteristics cache performance technical multi user commercial workloads 
proceedings th international conference architectural support programming languages operating systems san jose california october 
lazowska 
processor cache affinity information shared memory multiprocessor scheduling 
ieee transactions parallel distributed systems vol 
february subramaniam eager 
affinity scheduling unbalanced workloads 
proceedings supercomputing november 
