optimal algorithm online reorganization replicated data technical report ucsc crl cs ucsc edu ethan miller elm cs ucsc edu storage systems research center jack school engineering university california santa cruz santa cruz ca ssrc cse ucsc edu november storage systems scale thousands disks data distribution load balancing increasingly important 
algorithm allocating data objects disks system grows disks hundreds thousands 
client algorithm locate data object microseconds consulting central server maintaining full mapping objects buckets disks 
despite requiring little global configuration data algorithm probabilistically optimal distributing data evenly minimizing data movement new storage added system 
algorithm support weighted allocation variable levels object replication needed efficiently support growing systems technology changes 
prevalence large distributed systems clusters commodity machines grown significant research devoted designing scalable distributed storage systems 
scalability systems typically limited allowing construction large system single step slow time components large system 
bias reflected techniques ensuring data distribution reliability assume entire system configuration known object written disk 
modern storage systems configuration changes time new disks added supply needed capacity bandwidth 
increasing popularity network attached storage devices allow thousands smart disks directly attached network complicated storage system design 
nasd systems disks may added connecting network efficiently utilizing additional storage may difficult 
systems rely central servers doing introduce scalability reliability problems 
impossible client maintain detailed information entire system number devices involved 
research addresses problem providing algorithm client map object disk small amount infrequently updated information 
algorithm distributes objects disks evenly redistributing objects possible new disks added preserve distribution 
algorithm fast scales number disk groups added system 
example disk system disks added time run time proportional 
system modern client require map object disk 
central directory clients computation parallel allowing thousands clients access thousands disks simultaneously 
algorithm enables construction highly reliable systems 
objects may arbitrary adjustable degree replication allowing storage systems replicate data sufficiently reduce risk data loss 
replicas distributed evenly disks system load failed disk distributed evenly disks system 
result little performance loss large system loses disks 
benefits algorithm simple 
requires fewer lines code reducing likelihood bug cause object mapped wrong server 
client need keep table servers system storing network address bytes additional information server 
system thousands clients small simple distribution mechanism big advantage 
related litwin describe class data structures algorithms data structures authors dubbed scalable distributed data structures 
main properties data structure meet order considered 

file expands new servers gracefully servers efficiently loaded 

master site object address computations go access centralized directory 

file access maintenance primitives search insertion split require atomic updates multiple clients 
second third properties clearly important highly scalable data structures designed place objects hundreds thousands disks property stands considered limitation 
essence file grows new servers storage demands resource availability difficult administration problem 
administrator wants add disks storage cluster immediately rebalance objects cluster take advantage new disks increased parallelism 
administrator want wait system decide take advantage new resources algorithmic characteristics parameters understand 
fundamental flaw lh variants 
furthermore linear hashing lh variants split buckets disks case half average half objects split disk moved new empty disk 
moving half objects disk causes wide differences number objects stored different disks cluster results suboptimal disk utilization 
splitting lh result hot spot disk network activity splitting node recipient 
algorithm hand moves statistically optimal number objects disk system new disks disk disk 
lh variants lh lh lh lh sa lh rs describe techniques increasing availability data storage efficiency mirroring striping checksums reed solomon codes standard techniques conjunction basic lh algorithm 
algorithm easily take advantage standard techniques focus 
furthermore lh variants provide mechanism weighting different disks take advantage disks heterogeneous capacity throughput 
reasonable requirement storage clusters grow time want add highest performance highest capacity disks cluster 
algorithm allows weighting disks 
kr ll widmayer propose call distributed random trees 
optimized complex queries range queries closest match simple primary key lookup supported algorithm lh 
additionally support server weighting 
difficulties data driven reorganization opposed administrator driven reorganization lh variants 
addition authors algorithm data replication metadata replication discussed extensively 
provide statements regarding average case performance data structure drt worst case performance linear number disks cluster 
authors prove lower bound average case performance tree number objects stored system 
algorithm performance nlog number groups disks added disks added large groups case performance nearly constant time 
weikum discuss distributed file organization resolves issues disk utilization load lh 
propose solution data replication 
peer peer systems cfs past gnutella freenet assume storage nodes extremely unreliable 
consequently data high degree replication 
furthermore systems attempt guarantee long term persistence stored objects 
cases objects may garbage collected time users longer want store particular objects node objects seldom automatically discarded 
unreliability individual nodes systems replication high availability concerned maintaining balanced performance entire system 
large scale persistent storage systems farsite oceanstore ensure file system semantics 
objects placed file system guaranteed probability client cpu obsd client client client network obsd obsd obsd client typical nasd storage system failure remain file system explicitly removed removal supported 
oceanstore guarantees reliability high degree replication 
inefficiencies introduced peer peer wide storage systems order address security reliability face highly unstable nodes client mobility things introduce far overhead tightly coupled mass object storage system 
distributed file systems afs client server model 
systems typically replication storage node raid client caching achieve reliability 
scaling typically done adding volumes demand capacity grows 
strategy scaling result poor load balancing requires maintenance large disk arrays 
addition solve problem balancing object placement 
object placement algorithm developed object placement algorithm organizes data optimally system disks servers allowing online reorganization order take advantage newly available resources 
algorithm allows replication determined object basis permits weighting distribute objects unevenly best utilize different performance characteristics different servers system 
algorithm completely decentralized minimal storage overhead minimal computational requirements 
object storage systems nasd storage systems built large numbers relatively small disks attached high bandwidth network shown 
nasd disks manage storage allocation allowing clients store objects blocks disks 
objects size may bit name allowing disk store object find space 
object name space partitioned clients clients store different objects single disk need distributed locking 
contrast blocks fixed size stored particular location disk requiring distributed locking scheme control allocation 
nasd devices support object interface called object storage devices 
assume storage system algorithm runs built 
discussion algorithm assumes object mapped key object unique identifier system key algorithm unique object 
may called object disks 
object mapped seed random number generator object key advance random number generator steps 
generate random number map object server mod algorithm mapping objects servers replication weighting 
objects mapped set may contain hundreds thousands objects share key having different identifiers 
algorithm located set object resides set may searched desired object search done locally obsd object returned client 
restricting magnitude relatively small number object balancing described section simpler implement losing desirable balancing characteristics algorithm 
previous assumed storage static storage added additional capacity 
believe additional storage necessary additional performance capacity requiring objects redistributed new disks 
objects rebalanced storage added newly created objects stored new disks 
new objects referenced leave existing disks underutilized 
assume disks added system clusters jth cluster disks containing disks 
system contains objects mi disks adding disks require relocate objects new disks preserve balanced load 
algorithms assume existing clusters numbered adding cluster cth cluster contains mc disks nc disks system 
basic algorithm call disks servers algorithm distribute data object database complex service 
algorithm operates basic principle order move statistically optimal number objects new cluster servers object simply pick pseudo random integer nc mc 
mc object question moves new cluster 
algorithm applied recursively time add new cluster servers add step lookup process 
find particular object backwards clusters starting added deciding object moved cluster 
basic algorithm determining placement object key making considerations object replication weighting shown 
uniform random number generator allows jump ahead numbers generated generator skipped st number generated directly 
generator advanced steps log time currently exploring generators generate parametric random numbers time described section 
simple inductive proof show expected number objects placed new cluster basic algorithm mc objects randomly distributed uniformly nc mc servers reorganization 
base case objects clearly go cluster meaning furthermore comes uniform distribution object placed server mod mod probability choosing server server equal probability chosen objects distributed uniformly servers placing cluster 
induction step assume objects randomly distributed uniformly nc servers divided clusters add cluster containing mc servers 
optimally place mc objects nc mc cluster random number nc mc equally probability mc moving nc mc object server cluster objects total number objects moved server cluster mc optimal value 
nc mc objects system distributed uniformly nc servers inductive hypothesis relocated object equal probability coming nc servers 
expected number mc objects moved server nc expected number nc mc nc objects remaining server nc mc nc mc expected number objects nc mc placed cluster expected number objects placed server cluster mc mc nc mc nc mc mc nc mc expected number objects server system nc mc distribution objects system remains uniform 
decision regarding objects move move pseudo random process distribution objects system remains random 
cluster weighting replication simply distributing objects uniform clusters sufficient large scale storage systems 
practice large clusters disks require weighting allow newer disks faster larger contain higher proportion objects existing servers 
clusters need replication overcome frequent disk failures occur clusters thousands servers 
cluster weighting systems clusters servers different properties newer servers faster capacity 
add weighting algorithm allow server clusters contain higher proportion objects 
accomplish weight adjustment factor cluster factor number describes power capacity throughput combination server 
example clusters weighted capacity drives drive cluster gigabytes drive second cluster gigabytes initialized initialized 
jw place place 
object cluster selected mapped server done basic algorithm 
bit integers arithmetic allows large systems terabyte system weights gigabytes total weight 
weights naturally fractional bandwidth scaled constant factor cw ensure remain integers 
replication object mapped seed random number generator object key advance generator steps jw generate random number choose random prime number mod map object server mod mod map object server mod 
algorithm mapping objects servers replication weighting 
algorithm slightly complicated add replication guarantee replicas object placed server allowing optimal placement migration objects new server clusters 
version algorithm shown relies fact multiplying number mod prime larger defines bijection ordered set permutation 
furthermore number unique bijections equal number elements relatively prime words multiplying prime larger permutes elements ways euler phi function described section 
key object placed number clusters total number servers clusters number servers cluster 
equal maximum degree replication object replica number object question 
pseudo random values algorithm 
algorithm assumes number servers cluster large maximum degree replication 
intuitive sense true sufficient number servers available accommodate replicas object system brought online 
case algorithm intuitively speaking cluster size selects object replicas allocated servers imaginary cluster servers 
way avoid mapping replica server 
number objects get mapped cluster factor cancels completely 
total weight system 
fraction total weight possessed server cluster wi show expected number object replicas owned server show replicas object get placed server 
prove facts induction 
omit proof objects remain distributed mod mod mod mapping order set integers 
permutation set function mod clusters weights argument essentially identical basic algorithm described section 
base case modulus 
require cluster servers map object server mod mod cluster described 
pseudo random number pseudo random object chance placed servers cluster 
expected number objects placed server cluster wanted prove 

examine mod term mod term separately 
recall key object 
value potentially different object replica object viewed defining random offset servers cluster start placing objects 
relatively prime chinese remainder theorem unique solution mod 
words defines bijection ordered set 
permutation set 
think mod denoting permutation set 
shifted mod 
words rotate element position times set defined mod 
permutation 
replica object maps unique server shown 
induction step assume objects randomly distributed uniformly nc servers divided clusters 
furthermore assume cluster weighted server unnormalized weight object replicas system distributed randomly servers server respective weight defined server cluster 
add cluster containing servers total weight allocated cluster object replica placed cluster probability expected number objects placed cluster jm base case object replicas distributed servers cluster uniformly expected number number object replicas allocated server cluster wanted show 
number unique permutations obtained multiplying coprime equal euler phi function described section 
defines bijection ordered set 
permutation set replica gets placed cluster gets placed unique server 
note replicas object placed cluster replicas mapped mod values greater equal algorithm moves optimal number algorithms cluster replicas object get placed server 
choosing prime numbers algorithm uses random prime number known server client system 
sufficient choose random prime large pool primes 
prime relatively prime modulus mod furthermore choosing random prime computing mod statistically equivalent making uniform draw set integers set 
relatively prime proof scope 
number integers set 
relatively prime relatively prime integers called remainder section described euler phi function means set factor 

number bijections described set smaller general number possible permutations set integers 

scope show precise statistical impact difference 
practical impact difference seen 
performance operating characteristics theoretical complexity section demonstrate algorithm time complexity number server additions time takes generate appropriate random number 
algorithm currently generate random numbers takes logn time 
theoretically reduced 
noted section appropriate prime numbers chosen time rest operations related generating random numbers arithmetic operation generating random numbers runs time 
algorithm seeding generating random numbers constant time 
algorithm advancing random number generator number steps calculating intermediate values takes logn time 
specifically algorithm requires modular exponentiation known run logn time 
jump ahead cluster group number iteration iteration algorithm takes average logn time 
worst case object replica placed server cluster case algorithm examine cluster determine object belongs 
average case depends size weighting different clusters metric performance 
weight clusters sizes distributed evenly clearly need average iterations 
believe newer clusters tend higher weight average case need calculate logn iterations 
time seconds number clusters algorithm linear log time lookup compared linear nlog functions time seconds number clusters distribution weighting weighting time lookup weighting exponential weighting time looking object versus number server clusters system 
times computed intel pentium iii 
generate statistically random hash values parameterized server cluster number examined approach parametric random number generators 
random number generators popular distributed random number generation 
parameterizing generated sequence generators assign different parameter processor cluster seed 
guarantees unique deterministic pseudo random number sequences processor 
simple method linear congruence generators allows parameterization occur time 
notorious generating numbers lie higher dimensional hyperplane strongly correlated purposes 
unfortunately correlation results poor distribution objects algorithm unusable 
currently examining sophisticated generators final note algorithm support operation theoretical interest 
operation achieved follows iteration seed generator advance steps normally done 
re seeding generator advancing steps retain state generator advance period generator case maximum value unsigned long integer minus 
period generator known quantity depend done time 
course hard imagine distributed storage device nodes apart sort nano scale device classification academic interest 
performance order understand real world performance algorithm tested average time lookup different configurations 
ran test object replicas placed configurations starting servers single cluster isolate effect server addition 
timed lookups added clusters servers servers time timed lookups new server organization 
see line lookups configuration grows faster linear slower nlog lines grow approximately logarithmically 
disk capacity growing exponentially consider performance algorithm weight number object assigned new clusters grows exponentially 
bottom line illustrates growth capacity cluster additions middle line represents growth 
number objects server id server fails system evenly weighted clusters servers number objects server id server fails system clusters servers cluster servers 
failed server cluster servers 
number objects server id server fails system clusters servers cluster having increasing weight number objects server id server fails system clusters servers object replicas distributed adjacent servers distribution replicas objects stored failed server server fails different system configurations 
total objects stored system 
weighting new servers significantly improve performance algorithm 
consistent predictions section 
failure resilience server fails clients read write servers objects stored failed server 
replicas particular server stored set servers replicas objects server stored server server server failure cause read load mirror servers increase factor degree object replication 
value assumes replicated clients quorums reads case mirrors participate reads increase load 
false benefit achieved resources inefficiently normal operation severe burden large scale systems 
order minimize load servers failure algorithm places replicas objects pseudo randomly server fails load failed server absorbed server system 
shows histogram distribution objects replicate objects server 
case load uniform weight server cluster increases 
see spikes servers replicas objects server 
occurs cluster server added size composite number 
depending degree replication number distinct prime factors size cluster size cluster composite empty spots may occur cluster 
number composite number objects distributed relatively uniformly servers 
clearly distribution far superior simplistic sequential distribution illustrated servers system degree replication exact take load failed server 
algorithm distributes load failed servers close uniformly working servers system 
operational issues algorithm easily supports desirable features large scale storage systems online reconfiguration load balancing variable degrees replication different objects 
online reconfiguration algorithm easily allows load balancing done online system servicing object requests 
basic mechanism identify sets move existing disk new done iterating possible values identify sets move 
note balancing algorithm move objects existing disk existing disk objects moved new disks 
identification pass quick particularly compared time required copy objects disk 
process adding disks basic reasons client locate object correct server 
server clusters may reconfigured client may updated algorithm configuration server map 
case client receive updated configuration server requested object question re run algorithm new configuration 
second client may configuration desired object moved correct server 
case client thought object replica located cluster find simply continue searching cluster added 
finds object write object correct location delete old 
different semantics object locking configuration locking necessary depending parameters system commit protocol algorithm equally suited online batch reorganization 
adjustable replication algorithm allows degree replication object objects vary time constraint system initially configured administrator set maximum degree replication 
value size initial cluster unique location place replicas 
client decide object basis replicas place 
places fewer maximum number possible spots remaining replicas higher degree replication desired time 
practically speaking client file metadata determine degree replication different objects compose file obsd 
algorithm proposed exhibits excellent performance distributes data highly reliable way 
provides optimal utilization storage allows heterogeneous clusters servers 
addition replica identifiers indicate location different stripes object algorithm place stripes reed solomon coding similar striping data protection schemes 
improvements currently investigating 
studying efficient parameterizable random number generation hashing function worst case performance algorithm 
addition studying modification algorithm allow cluster removal 
exchange capability algorithm need look replicas 
significantly effect performance locations cached calculated 
considering exact protocols distribution new cluster configuration information 
protocols require global locks clients cases optimistic locking semantics acceptable require locks 
considering different read write semantics different types storage systems integrating algorithm massively scalable cluster file system 
considering fast recovery technique automatically creates extra replica objects effected failure order significantly increase mean time failure degree replication 
ethan miller supported part lawrence livermore national laboratory los alamos national laboratory sandia national laboratory contract 
adya bolosky castro douceur howell lorch theimer wattenhofer 
farsite federated available reliable storage incompletely trusted environment 
proceedings th symposium operating systems design implementation osdi boston ma dec 
usenix 
breitbart weikum 
load control scalable distributed file structures 
distributed parallel databases 
chen lee gibson katz patterson 
raid high performance reliable secondary storage 
acm computing surveys june 
clarke sandberg wiley hong 
freenet distributed anonymous information storage retrieval system 
lecture notes computer science 

random number generators parallel computers 
review 
cormen leiserson rivest stein 
algorithms second edition 
mit press cambridge massachusetts 
dabek kaashoek karger morris stoica 
wide area cooperative storage cfs 
proceedings th acm symposium operating systems principles sosp pages banff canada oct 
acm 
douceur wattenhofer 
large scale simulation replica placement algorithms serverless distributed file system 
proceedings th international symposium modeling analysis simulation computer telecommunication systems mascots pages cincinnati oh aug 
ieee 
gibson nagle amiri butler chang gobioff hardin riedel zelenka 
cost effective high bandwidth storage architecture 
proceedings th international conference architectural support programming languages operating systems asplos pages san jose ca oct 
hennessy patterson 
computer architecture quantitative approach 
morgan kaufmann publishers rd edition 
howard kazar menees nichols satyanarayanan sidebotham wes 
scale performance distributed file system 
acm transactions computer systems feb 
kr ll widmayer 
distributing search tree growing number processors 
proceedings acm sigmod international conference management data pages 
acm press 
kr ll widmayer 
balanced distributed search trees exist 
proceedings th international workshop algorithms data structures pages 
springer aug 
kubiatowicz bindel chen eaton geels gummadi rhea weatherspoon weimer wells zhao 
oceanstore architecture global scale persistent storage 
proceedings th international conference architectural support programming languages operating systems asplos cambridge ma nov 
acm 
litwin menon risch 
lh schemes scalable availability 
technical report rj ibm research almaden center may 
litwin neimat levy schwarz 
lh high availability high security scalable distributed data structure 
proceedings th international workshop research issues data engineering pages birmingham uk apr 
ieee 
litwin 
neimat 
high availability lh schemes mirroring 
proceedings conference cooperative information systems pages 
litwin 
neimat schneider 
lh scalable distributed data structure 
acm transactions database systems 
litwin risch 
lh high availability scalable distributed data structure record grouping 
ieee transactions knowledge data engineering 
litwin schwarz 
lh rs high availability scalable distributed data structure reed solomon codes 
proceedings acm sigmod international conference management data pages dallas tx may 
acm 
ripeanu iamnitchi foster 
mapping gnutella network 
ieee internet computing aug 
rowstron druschel 
storage management caching past large scale persistent peerto peer storage utility 
proceedings th acm symposium operating systems principles sosp pages banff canada oct 
acm 
weber 
information technology scsi object storage device commands osd 
technical council proposal document technical committee aug 
hill 
algorithm efficient portable pseudo random number generator 
applied statistics 

