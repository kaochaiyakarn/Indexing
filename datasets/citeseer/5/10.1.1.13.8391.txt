semi supervised learning trees kemp griffiths tenenbaum department brain cognitive sciences mit cambridge ma sean mit edu describe nonparametric bayesian approach generalizing labeled examples guided larger set unlabeled objects assumption latent tree structure domain 
tree distribution trees may inferred unlabeled data 
prior concepts generated mutation process inferred tree allows efficient computation optimal bayesian classification function labeled examples 
approach performs real world datasets extends naturally handle difficult problems learning sparse data learning positive examples 
people remarkable abilities learn concepts limited data just labeled examples class 
semi supervised learning approaches machine learning try match ability extracting strong inductive biases larger sample unlabeled data 
general strategy adopt metric space model underlies observed features unlabeled data new concept learned 
unlabeled data identify model assumption smooth respect bayesian terms assigned strong prior conditional provides inductive bias needed generalize successfully labeled examples 
existing approaches different assumptions framework 
transductive support vector machines take metric space high dimensional feature space defined kernel maximizing margin unlabeled data effectively assume maximally smooth respect density unlabeled data feature space 
laplacian method belkin niyogi assumes data concentrate low dimensional riemannian manifold smooth manifold 
manifold geometry estimated unlabeled data simple bottom techniques graph nearest neighbors 
approach performs quite classifying data natural manifold structure handwritten digits 
methods combine aspects approaches 
domains structured generic kernel approaches assume underlying low dimensional riemannian geometry 
design semisupervised approaches exploit kinds metric structures 
particular trees arise prominently natural human generated domains biology language information retrieval 
propose approach semi supervised learning embedding data ultrametric space rooted tree data located leaf nodes equidistant root 
concept generated stochastic mutation process oper ating branches tree inferred unlabeled data bottomup methods agglomerative clustering complex probabilistic methods 
mutation process defines prior possible transductive concepts possible labelings unlabeled data favoring maximize tree specific notion smoothness 
illustrates tree bayes tbb approach 
tbb classifies unlabeled data integrating hypotheses defined data set instance optimal bayesian concept learning 
general optimal bayes theoretical interest sum hypotheses intractable difficult specify sufficiently powerful noise resistant priors real world domains 
working semi supervised setting defining prior terms tree mutation process approach efficient empirically successful data strongly tree structured 
schematic illustration tree bayesian approach semi supervised learning 
observe set unlabeled objects small points latent hierarchical structure gray ellipses positive negative examples new concept black white circles 
inferring latent tree treating concept generated mutation process tree classify unlabeled objects 
section describes tbb simple heuristic method tree nearest neighbor tnn show approximates tbb limit high mutation rate 
section presents experiments comparing algorithms approaches range data sets 
section illustrates bayesian formulation extends handle impoverished forms data produced single class sampling labeled data drawn just class sparse sampling missing feature values 
tree bayes domain objects 
xt 
object xi associated vector observable features denoted xi 
labels yl 
ys objects goal infer labels yu ys 
yt unlabeled points 
refer xl 
xs labeled data complement xu xl unlabeled data 
assume binary classification problem yl yu generalization multi class straightforward 
assume ultrametric binary tree objects leaves 
expressed terms tree learning problem find indicator function leaves 
intuition domains exists tree natural concepts tend assign label pairs nearby leaves tree bayes tbb approach grounds tree proximity principle generative model object features tree structured domains 
show define ingredients bayesian concept learning terms particular tree extending previous :10.1.1.10.4037
consider relevant case semi supervised learning unknown may inferred help unlabeled data 
hypothesis space 
approach non take set possible binary labelings objects domain define xl yl denote labeled examples consisting labeled objects xl labels yl 
unlabeled object xi xu want compute yi probability xi positive instance concept examples tree summing hypotheses yi yi 
define subset hypotheses label xi positive 
yi yi 
yl subset hypotheses consistent labels yl version space 
hypotheses yl likelihood equals zero 
hypotheses consistent yl xl 
expanding denominator write equation yl yi xl yl xl 
likelihood xl depends labeled set xl chosen unknown 
typical assumption xl drawn randomly objects domain xl independent cancels equation leaving yi 
yl yl probability xi positive instance reduces fraction hypotheses consistent examples label xi positively measured prior probability 
sampling paradigms different likelihood functions xl equation adopted learning small training sets 
consider identifying genetic markers disease person 
training set problem constructed balanced sampling data patients disease healthy subjects 
randomly sampling subjects entire population require huge training set chance including disease 
real world data sets constructed manner closer balanced sampling analysis tractable random sampling equation 
assume random sampling experiments explore paradigms section consider alternative sampling paradigm equation labeled data come just single class 
bayesian classification mutation model tree structured domains biology fields natural think entities result evolutionary process think features concepts arising history stochastic events mutations 
independent naturalness mutation process induces sensible prior enables efficient computation equation belief propagation bayes net 
mutation model combines aspects previous proposals probabilistic learning trees :10.1.1.10.4037:10.1.1.10.4037
feature corresponding class label 
suppose defined nodes just leaves 
model development poisson arrival process parameter called mutation rate 
probability feature changes value branch length probability odd number arrivals branch changes 
note mutations assumed symmetric feature just switch 
approach readily accommodate probabilistic models mutation 
prior probability generating indicator function leaves mutation process 
resulting distribution favors labelings smooth respect regardless stay switch value branch equation 
labelings require mutations preferred hypotheses assign label leaf nodes receive weight 
mutations occur longer branches prior favors hypotheses label changes occur clusters branches tend longer clusters branches tend shorter 
independence assumptions implicit mutation model allow right side equation computed efficiently 
value child tree depends value parent number mutations occurred parent child 
set bayes net topology captures joint probability distribution nodes 
associate branch conditional probability table specifies value child conditioned value parent equation set prior probabilities root node uniform distribution 
evaluating equation reduces standard problem inference bayes net computing marginal probability group evidence nodes corresponding labeled examples denominator labeled examples plus xi numerator 
tree structure computation efficient allows specially tuned inference algorithms 
tree nearest neighbor bayesian formulation mutation process provides principled approach learning trees imagine simpler algorithms instantiate similar intuitions 
instance build nearest neighbour classifier metric distance tree clear tree nearest neighbor tnn algorithm reflects assumption nearby leaves label necessarily clear simple approach 
analysis tree bayes provides insight 
show algorithms tbb tnn equivalent parameter tbb set sufficiently high 
theorem ultrametric tree tnn tbb produce identical classifications examples unique nearest neighbour 
full proof available www mit edu nips pdf sketch main steps 
skeleton subtree consisting paths labeled leaf nodes root 
suppose labeled node na ancestor labeled descendants 
establish theorem showing node skeleton na posterior distribution favours grows large 
assume na done distance na 
mutation model previous section show skeleton node posterior favours log 
worst case analysis shows lim 
equivalence tnn superior algorithm high mutation rate appropriate 
faster numerically stable 
large values probabilities manipulated tbb close variables different may indistinguishable limits computational precision 
implementation tbb uses tnn sufficiently high value required 
computing tree far computing yi assumed tree known 
generally semi supervised learning know best tree 
need compute yi xu classification xi conditioned labeled objects labels xl yl unlabeled data xu 
proceed full data infer tree 
principle requires sum trees yi xu yi xu xu 
sum trees intractable consider approaches approximation 
markov chain monte carlo mcmc techniques approximate similar sums trees bayesian 
mcmc trees quite costly show section offer significant benefits data sparse 
simpler approach assume probability xu concentrated near probable tree approximate equation simply yi xu 
expression just computed equation known tree assuming new concept conditionally independent features objects xu 
estimate sophisticated means 
experiments section greedy bottomup method average link agglomerative clustering full data comparable complexity neighborhood graph constructions 
experiments tree versus manifold approaches compared algorithms tbb tnn laplacian approach belkin niyogi generic nearest neighbor 
data sets expected tree structure arose biological taxonomies worms respective sizes 
sets expected low dimensional manifold structure arose human motor behaviors digits vowels respectively 
tree set describes external anatomy group species data available biodiversity uno edu delta 
feature set example indicates beetle body strongly flattened slightly flattened moderately convex strongly convex tree sets include class labels chose features random stand class label 
averaged choices dataset 
digits set subset mnist data vowels taken uci repository 
experiments focused learning small labeled sets 
number labeled examples set small multiple total number classes 
algorithms compared random balanced sampling training sets sampled replacement 
training set size averaged random splits data xl xu 
free parameters tbb laplacian number nearest neighbors number eigenvectors chosen randomized cross validation 
shows performance algorithms random sampling 
tbb outperforms algorithms tree sets difference tbb nearest neighbor small 
results suggest substantial advantage tbb laplacian presumably tree structured domains 
expected pattern reversed digits set encouraging tree methods improve nearest neighbour datasets normally associated trees 
surprisingly tbb performs little better laplacian vowel set method beats nearest neighbor 
shows balanced sampling better able discriminate algorithms 
clear advantage tbb tree sets 
algorithms tbb copes class proportions training set match proportions population turns features taxonomic datasets unbalanced 
digits vowels sets classes approximately equal size results balanced sampling similar random sampling 
conclusive results suggest tbb may method choice treestructured datasets robust data clearly tree structured 
error rate error rate worms digits tbb nn tnn laplacian vowels error rates random sampling datasets described text 
total number labeled examples multiplied number classes 
mean standard error bars data set shown upper right corner plot 
extensions shown optimal bayesian concept learning tractable semi supervised setting assuming latent tree structure inferred unlabeled data defining prior concepts mutation process tree 
bayesian framework supports number extensions basic semi supervised paradigm 
close briefly considering extreme cases impoverished data learning labeled examples drawn single class learning sparsely observed data 
discriminative techniques nearest neighbour laplacian approaches naturally apply labeled data include positive examples single class 
bayesian approach adapted setting choosing xl equation appropriately 
restriction positive examples means xl proportional number objects labeled positive number ex error rate error rate worms digits tbb nn tnn laplacian vowels results balanced sampling labeled examples class 
amples assuming sampling replacement 
classification probability yi longer computed closed form belief propagation 
greedy search find small number hypotheses high posterior positing minimal mutations extended skeleton labeled examples mutations paths examples root mutations paths 
approximate equation summing just hypotheses 
shows extended skeleton concept uci zoo data set 
shows comparison approach min simpler algorithm chooses single smallest subtree consistent positive examples 
concept learned corresponds single subtree methods min dramatically 
tbb converges rapidly true concept 
error rate tbb min error rate ideal mcmc 
tbb nn nodes dominating members single class zoo dataset approximating equation 
performance min algorithms labeled examples class 
results inferring tree tbb extremely sparse data different methods compared nn 
tree structured domains interest biology may sufficient data allow single tree structure discovered simple greedy clustering 
exploit tree structure mcmc techniques approximate sum trees equation 
objects binary discrete features define generative distribution features mutation process generated prior concepts section 
take distribution trees xu proportional effectively assuming uniform prior ignoring negligible contribution labels test mcmc conditions hard identify generated artificial data sets consisting objects 
data set true tree objects xi leaves 
object xi represented vector binary features generated mutation process high 
feature values missing average algorithms saw features object 
data set created test concepts mutation process 
algorithm saw labeled examples test concept infer labels objects 
experiment repeated random 
mcmc approach inspired algorithm reconstruction phylogenetic trees uses metropolis hastings tree topologies kinds proposals local nearest neighbor interchange global subtree pruning 
shows mean classification error rate samples burn iterations 
shows results nearest neighbor versions tbb single tree tree constructed agglomerative clustering ignoring missing features ideal true tree 
ideal learner beats true tree impossible identify sparse data 
mcmc trees brings tbb substantially closer ideal simpler alternatives ignore tree structure nn consider single tree 
advocates bayesian approaches traditional supervised classification tasks emphasized framework supports useful auxiliary inferences active learning selecting unlabeled points labels informative estimating confidence inferred class labels 
issues pressing learning labeled examples semi supervised setting expect handled naturally extensions bayesian approach 
generally valuable formulate bayesian approaches semi supervised learning kinds metric space structures including limited riemannian manifolds 
necessary computations surely tractable approximate techniques useful 
faced new domain data form underlying structure unknown bayesian methods model selection sufficient unlabeled data choose approaches assume trees manifolds canonical representational forms 
vapnik 
statistical learning theory 
wiley 
belkin niyogi 
semi supervised learning riemannian manifolds 

submitted journal machine learning research 
szummer jaakkola 
partially labeled classification markov random walks 
nips volume 
chapelle weston sch lkopf 
cluster kernels semi supervised learning 
nips volume 
mitchell 
machine learning 
mcgraw hill 
haussler kearns schapire 
bounds sample complexity bayesian learning information theory vc dimension 
machine learning 
tenenbaum 
bayesian models inductive generalization 
nips volume 
kemp tenenbaum 
theory induction 
proceedings th annual conference cognitive science society 
appear 
shih karger 
learning classes correlated hierarchy 

unpublished manuscript 

vert 
tree kernel analyze phylogenetic profiles 
bioinformatics 
rattray higgs 
bayesian rna substitution model applied early mammalian evolution 
molecular biology evolution 
