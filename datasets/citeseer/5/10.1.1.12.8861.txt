efficient margin maximizing boosting gunnar australian national canberra act australia manfred warmuth california santa cruz santa cruz ca usa gunnar anu edu au manfred cse ucsc edu editor leslie pack kaelbling adaboost produces linear combination base hypotheses predicts sign linear combination 
observed generalization error algorithm continues improve examples classified correctly current signed linear combination viewed hyperplane feature space base hypotheses form features 
improvement attributed experimental observation distances margins examples separating hyperplane increasing training error zero examples correct side hyperplane 
give new version adaboost called adaboost explicitly maximizes minimum margin examples precision 
algorithm incorporates current estimate achievable margin calculation linear base hypotheses 
number base hypotheses needed essentially number needed previous adaboost related algorithm required explicit estimate achievable margin 

common version boosting algorithm fixed set labeled training examples 
stage algorithm produces probability weighting examples 
base hypothesis weighted error probability wrong classification slightly 
base hypothesis update distribution examples 
intuitively hard examples receive high weights 
stage base hypothesis added linear combination sign linear combination forms current hypothesis boosting algorithm 
known boosting algorithm adaboost freund schapire :10.1.1.32.8918
linear coefficient base hypothesis depends weighted error base hypothesis time base hypothesis added linear combination 
earlier boosting includes schapire freund 
adaboost interesting properties 
earlier boosting algorithms schapire property training parts done fraunhofer berlin uc santa cruz 
partially funded dfg contract ja ja mu eu neurocolt ii project 
warmuth visits uc santa cruz partially funded nsf ccr 
mika 
discussions 
gunnar manfred warmuth 
error converges exponentially fast zero 
precisely weighted training error th upper bound training error signed linear com base hypothesis bination reduced factor stage second observed experimentally adaboost continues learn training error signed linear combination zero schapire experiments generalization error continuing improve 
training error zero examples right side signed linear combination viewed hyperplane feature space threshold zero base hypothesis feature dimension 
margin example signed distance times label 
soon training error zero examples right side positive margin 
observed margins examples continue increase training error zero 
theoretical bounds generalization error linear classifiers schapire breiman improve margin classifier defined size minimum margin examples 
fact margins improve experimentally explain adaboost learns training error zero 
shortfall argument 
adaboost proven maximize margin final hypothesis 
fact experiments section observe adaboost maximize margin 
breiman proposed modified algorithm called arcing game value suitable task showed maximizes margin similar results shown grove schuurmans bennett 

give algorithm produces final hypothesis margin lies maximum margin achievable linear combination base hypotheses precision parameter 
main problems regarding finding linear combination maximum margin 
know close lower bound value maximum achievable margin 
problem solved known adaptation adaboost called adaboost cf 

warmuth guaranteed find log hypotheses 
note margin parameter linear combination algorithm margin obtained algorithm 
give slightly improved bound size linear combination produced adaboost 
challenging second problem consists case known 
related conference warmuth adaboost binary search fashion 
required log calls obtain value margin range 
call adaboost essentially aborted call log hypotheses 
provided linear combination greatly simplified answer case unknown 
new algorithm pass produces linear combination margin range size linear combination note bound size produced linear combination bound best algorithm adaboost start estimate achievable margin 
new algorithm uses current estimate achievable margin computation linear coefficients base learners needs know precision parameter recomputing estimate iteration 
algorithm previous conference result fast convergence boosting algorithm maximum margin solution works 
previous results show adaboost asymptotically converges final hypothesis margin cf 
corollary subtle conditions chosen base hypotheses satisfied 
adaboost designed find final hypothesis margin zero 
algorithm maximizes margin values 
includes inseparable case minimizes overlap classes 
algorithm useful hypotheses boosting algorithm strong sense separate data margin greater zero 
case linear coefficients hypotheses unbounded 
new algorithm maximizes margin combining strong learners 
structured follows section extend original adaboost algorithm leading adaboost requires guess maximal achievable margin 
propose adaboost similar adaboost adapts precision parameter section give detailed analysis algorithms 
prove weighted training error th base upper bound fraction examples margin smaller reduced factor stage adaboost cf 
section slightly improved factor shown case 
achieve large margin needs assume guess smaller 
case prove exponential convergence rate adaboost 
discuss way automatically tuning depending error base hypotheses precision parameter show adaboost able achieve theoretical performance knowing size maximal achievable margin 
extends previous result warmuth additional log factor number times weak learner called worse constants 
complete experiments confirming theoretical analysis section 

preliminaries basic notation consider standard class supervised machine learning setup set training examples xn xn learn function able generalize unseen data generated distribution training data 
case ensemble learning boosting fixed underlying set base ses ensemble built 
assume finite show section assumption dropped cases analysis applies case infinite hypothesis sets 
boosting algorithms iteratively form linear combinations hypotheses iteration base hypothesis ht added linear combination 
algorithm selects base hypothesis iteration called weak ensemble algorithms generate combined hypothesis form sign fa fa adaboost shown weak learner selects base hypotheses weighted error bounded slightly away combined hypothesis consistent training set small number iterations freund schapire :10.1.1.32.8918
discuss bounds number needed base hypotheses detail section 
suffices say point size combined hypothesis enters pac analysis generalization error schapire freund 
research schapire 
shown bound generalization error decreases size called margin final hypothesis margin single example xn yn defined xn 
margin quantifies far example yn side hyperplane margin combined hypothesis minimum margin examples 
goal find small linear combination base hypotheses margin close maximum achievable margin 

marginal boosting adaboost original adaboost designed find consistent hypothesis linear combination margin greater zero 
start slight modification adaboost finds possible linear combination base learners margin pre specified cf 
algorithm 
call algorithm adaboost naturally generalizes adaboost case target margin original adaboost algorithm 
algorithm adaboost algorithm margin parameter 
input xl yl xn 
initialize 

train classifier dr obtain hypothesis calculate edge rtl cr ct sin break log update weights dt exp dt 
ht 
output 
original adaboost algorithm formulated terms weighted training error base hypothesis 
cf 
section 
equivalent convenient formulation terms edge algorithm adaboost known unnormalized arcing breiman ada boost type algorithm 
related algorithms proposed freund schapire zhang 
difference adaboost choice hy log appears expression ctt 
term coefficients ctt additional term vanishes 
constant seen guess maximum margin 
chosen properly slightly converge exponentially fast combined hypothesis near maximum margin see section details 
example illustrates works 
assume weak learner returns constant hypothesis ht 
error hypothesis sum negative weights dtn edge 
parameter chosen edge ht respect new distribution exactly original adaboost 
choice assures edge valued base hypotheses 
general base hypothesis ht continuous range choosing zt function minimized guarantees edge ht respect distribution see schapire singer similar discussion 
choosing step approximately minimizes zt range ht 
kivinen warmuth lafferty standard boosting algorithms interpreted approximate solutions optimization problem choose distribution maximum entropy subject constraints edges previous hypotheses equal zero 
reasoning uses inequality constraints edges previous hypotheses ctt function lagrange multipliers inequality constraints 
increasing function tt iff 
log log note adding ht ht leads distribution 
symmetry broken 
know value optimum margin needs find 
warmuth marginal adaboost algorithm converging fast way find real value certain constructs sequence accuracy interval binary search needs log search steps 
previous marginal adaboost algorithm uses adaboost algorithm decide current guess larger smaller 
depending outcome chosen region uncertainty roughly cut half 
previous algorithm log iterations adaboost aborted inefficient 
propose different algorithm called adaboost 
precision parameter 
algorithm finds linear combination margin lies range 
arc gv breiman new algorithm essentially runs adaboost having fixed margin estimate updates appropriate way 
shall show guarantees algorithm adaboost known arc gv 
produces essentially monotonically increasing sequence margin estimates adaboost 
knows needs log steps 

original formulation sequence necessarily increasing showed leads result easier proofs restricts monotonically increasing 
cally decreasing sequence 
improved sequence estimates new theoretical insights developed section 
surprisingly size linear combination new adaboost algorithm see algorithm pseudo code small possible 
finding margin estimate range call adaboost executed previous algorithm produced linear com log surprisingly size linear combination new pass bination size adaboost algorithm algorithm adaboost algorithm accuracy parameter input xl yl xn yn 
iterations desired accuracy initialize 
train classifier obtain hypothesis ht calculate edge ht 
oz sign break pt yt log pt set pt update weights dtn dl exp cty 
ht zt din output 
detailed analysis weak learning margins standard assumption weak learning algorithm pac analysis boosting algorithm weak learner returns hypothesis fixed set slightly better random guessing 
formally means error rate consistently smaller easily reached fair coin assuming classes note error rate prior probabilities 
formally error hypothesis defined fraction examples misclassified 
boosting extended weighted example sets 
pac setting weak learner allowed fail probability sake simplicity assume weak learner fails 
algorithm extended case 
defines error eh dn yn sign xn hypothesis returned weak learner indicator function true false weighting dl dn examples dn zn dn 
convenient quantity measuring quality hypothesis edge xn affine transformation en case fn 
recall section margin example xn yn defined xn 
set weak learner chooses base hypotheses 
suppose combine possible hypotheses assuming moment finite cf 
section known theorem establishes connection margins edges seen connection boosting freund schapire breiman theorem min max theorem yon neumann min xn max min yn xn ot ihi :10.1.1.133.1040:10.1.1.133.1040
denotes dimensional probability simplex 
minimal edge achieved possible weightings training set equal maximal margin linear combination hypotheses non optimal weightings min xn 
hh particular weak learning algorithm guarantees return hypothesis edge weighting particular optimal weighting duality exists combined hypothesis margin lower bound largest possible exists combined hypothesis margin exactly hypotheses returned weak learner response certain weightings examples 
discussion derive sufficient condition weak learning algorithm reach maximal margin case finite weak learner returns hypotheses edges exists linear combination hypotheses margin 
prove adaboost algorithm efficiently finds linear combination cf 
theorem margin close 
constraining edges previous hypotheses equal zero done totally corrective algorithm kivinen warmuth leads problem solution satisfying constraints 
considers edge restricted problem hypotheses generated far hi ht 
duality min equality constraints edges eventually minimizing qt 
contrast adaboost motivated system inequality constraints xn adapted 
lagrange multipliers may diverge infinity 
new algorithm adaboost wc start large decrease necessary 
choice precision parameter 
convergence properties analyzing generalized version fixed adapted iteration 
extension necessary analysis adaboost 
consider pt specified running algorithm algo sequences rithm 
instance algorithm arc gv breiman chooses pt min xn 
shown breiman arc gv asymptotically converging maximum margin solution see discussion section 
answering question adaboost able increase margin bound fraction examples margin smaller say start useful lemma schapire schapire singer ht lemma convex combination fa zi xn zt step :10.1.1.31.2869
proof directly follows simple extension theorem schapire singer see schapire 

leads result generalizing theorem freund schapire case target margin zero :10.1.1.32.8918
recall pt denotes parameter algorithm margin bound 
lemma 
edge ht th step adaboost 
assume exp pat 
algorithm progress right hand side rhs 
smaller 
suppose reach margin training examples obviously need assume find combined hypothesis iterations possible 
rhs 
minimized pt independent constant choice 
assume analysis held constant run adaboost 
reconsider lemma special case pt exp ctt log zt log log binary relative entropy 
means rhs 
reduced factor ex bounded inspecting taylor expansion exp second ment noticing higher order terns negative exp fit need assume fit 
note fore bound obtains original adaboost fraction points zero reduced fft fit 
additional factor speeds convergence ifs 
upper bound number iterations needed adaboost achieving examples maximum corollary assume weak learner achieves edge 
ifo adaboost converge solution margin examples steps 
proof lemma bound examples rhs 
smaller og lw iterations proves statement 
result case number iterations bounded log 
asymptotical margin methods shown far analyze value maximum margin original adaboost algorithm converges asymptotically 
state lower bound margin achieved 
gap lower bound upper bound theorem 
second part consider experiment shows depending subtle properties weak learner margin combined hypotheses generated adaboost converge quite different values maximum margin kept constant 
observe previously lower bound margin tight empirical cases 
long factor rhs 
eq 
smaller bound decreases 
factor rhs 
converges exponentially fast zero 
corollary considers asymptotical case gives lower bound margin 
corollary assume adaboost generates hypothesis hi edges coefficients ctl ct 
mint 
assume tht xn achieved margin th iteration 
margin combined hypothesis bounded log log log log om understand interaction difference ff small hs 
small 
ff choosing results training examples 
expansion hs 
sees lowe bounded known lowe bound theorem section reasoned 
adaboost chosen small asymptotical marin 
original adaboost adaboost achieves ff gap theory motivates adaboost algorithm 
experimental illustration corollary illustrate mentioned gap perform experiment showing tight 
analyze different settings weak learner selects hypothesis largest edge hypotheses best case ii hypothesis minimal edge hypothesis edge larger worst case 
note corollary holds cases weak learner allowed return hypothesis edge larger 
random data training examples drawn uniformly 
labels drawn random binomial distribution equal probability 
hypothesis set random hypotheses range 
choose parameter uniformly 
label hypothesis example chosen agree label example probability compute solution margin lp problem left hand side lhs 

compute combined hypothesis generated adaboost iterations best worst selection strategy respectively 
depends 
chose hypothesis sets random draws random choice ensures cases small large optimal margins 
hypothesis set runs adaboost best worst selection strategy 
result run represented point 
abscissa maximal achievable margin run 
ordinate margin adaboost best green worst strategy red 
observe great difference selection strategies 
margin worst strategy tightly lower bounded best strategy near maximal margin 
experiments show obtains different results changing selection strategy weak learning algorithm 
lower bound holds selection strategies 
looseness bounds problem able predict adaboost converging 
note moving closer reduces gap see right 

allow duplicate hypotheses hypotheses agrees labels examples 

able construct cases outputs converging 
gap best case 
rst cas owe bound maximal margin maximal margin achieved margins best green worst red selection random right abscissa maximal achievable margin data left ordinate margin achieved data realization 
comparison line upper bound bound lower bound plotted 
interval clear gap performance worst best selection strategies 
margin worst strategy tightly lower bounded best strategy near maximal margin 
chosen slightly maximum achievable margin gap reduced 
decreasing step size breiman conjectured inability maximizing margin due fact normalized hypothesis coefficients may circulate endlessly convex set defined lower bound margin 
fact motivated previous experiments possible implement weak learner appropriately switches optimal worst case performance leading non convergent normalized hypothesis coefficients 
rosset 
shown adaboost infinitesimal small step sizes may maximize margin weak learner uses best selection strategy similar empirically normal finite step sizes 
motivates analyze adaboost step sizes chosen follows tt 
obviously recover adaboost 
proof technique corollary show case holds conditions corollary log exp exp log log note goes zero fi interestingly independent choice 
weak learner returns hypotheses edges maximum margin min max theorem margin maximized goes zero 
guarantees convergence speed 
convergence adaboost adaboost algorithm insights discussion adaboost converges fastest combined hypothesis margin chooses pt close possible distribution examples hard weak learner edge close 
idea choosing pt minr concentrate hardest distribution generated far find close estimate helps converging faster larger margin generating distributions weak learner return low edges 
note weak learner returns hypotheses edge worst case sense section immediately pt smallest step size taken iteration monotonically increasing 
size step desired accuracy 
matches intuition decreasing step size achieve larger margins discussed section 
state prove main theorem theorem assume weak learner achieves edge 
adaboost algorithm find combined hypothesis maximizes margin accuracy log log calls weak learner 
final hypothesis combines base hypotheses 
proof margin achieve 
assumption performance weak learner construction step algorithm pt lemma lemma li xn log log 
rewrite rhs 
og og pat pt margin examples rhs 
smaller og iterations proves theorem 
theorem improved factor pt iteration assumes pt exploited additional factor pt lemma 
pt obtain bound factor matter large margins 
infinite hypothesis sets far implicitly assumed hypothesis space finite 
section show assumption necessary 
note output hypotheses discrete hypothesis space effectively finite 
infinite hypothesis sets theorem restated weaker form theorem weak min max hash minsup xn sup min yn xn ot oq ihi finite support 
call duality gap 
particular pn en xn dn finite support yn aq xn theory duality gap may nonzero 
lemma theorem assume finite hypothesis sets show margin converge arbitrarily close long weak learning algorithm return hypothesis iteration edge smaller 
words duality gap may result fact sup left side replaced max exists single hypothesis edge larger equal 
assuming weak learner able pick hypotheses automatically gets lemma 
certain conditions maximum exists strong duality holds details see nash theorem strong min max set label vectors xl xn com pact 
general requirement fulfilled weak learning algorithms outputs continuously depend distribution furthermore outputs hypotheses need bounded cf 
step adaboost 
requirement problem weak learning algorithms variants decision stumps decision trees 
simple trick avoid problem roughly speaking point discontinuity adds hy limit points ds ds sc arbitrary sequence converging denotes hypothesis returned weak learning algorithm weighting training sample 
procedure closed 

experimental illustration marginal adaboost note aware fact maximizing margin ensemble lead cases improved generalization performance 
fairly noisy data sets opposite reported cf 
breiman grove schuurmans 
breiman reported example margins examples larger ensemble generalized considerably better 
discriminative dimensions separable dimensional data set 
theoretical bounds generalization error linear classifiers improve margin 
expect able measure differences generalization error function approximately maximizes margin function small margin 
similar results obtained schapire 
multi class optical character recognition problem 
report experiments artificial data illustrate algorithm works compares adaboost 
data dimensional contains nuisance dimensions uniform noise 
dimensions plotted exemplary 
training examples obviously need carefully control capacity ensemble 
weak learning algorithm decision trees provided option control number nodes tree 
set generates trees nodes 
weak learner training examples correctly fits data 
furthermore case margin maximal equal boosting algorithms 
need limit complexity weak learner agreement bounds generalization error schapire 
deal fact weighted samples 
weighted bootstrapping 
amplifies problem resulting hypotheses cases small edge smaller maximal margin happen min max theorem weak learner performs optimal 
deal problem repeatedly calling edge smaller margin current linear combination 
furthermore adaboost small edge hypothesis spoil margin estimate pt 
reduce problem resetting pt pt pt margin currently combined hypothesis 
see typical run adaboost marginal adaboost adaboost arc gv 
comparison plot margins hypotheses generated adaboost cf 
left 
observes able achieve large margin efficiently iterations 
marginal adaboost proposed warmuth proceeds stages tries find estimate margin binary search 
calls times 
call stops iterations generated consistent combined hypothesis 
lower bound computed algorithm upper bound 
second time chosen middle interval adaboost reaches margin iterations 
interval 
length interval small marginal adaboost leaves loop exit condition calls adaboost time achieves margin 
run arc gv iterations observe margin combined hypothesis new algorithm adaboost find 
case margin adaboost largest methods iterations 
starts slightly lower margins catches due better choice margin estimate 
adaboost adaboost illustration achieved margin adaboost left marginal adaboost middle arc gv adaboost right iteration 
marginal adaboost calls times adapting dash dotted 
plot values marginal adaboost dashed 
details see warmuth adaboost achieves larger margins adaboost 
compared arc gv starts slower catches iterations 
correct choice parameter important 
adaboost marginal adaboost adaboost table estimated generalization performances margins confidence intervals decision trees adaboost marginal adaboost adaboost toy data 
numbers averaged splits training test examples 
table see average performances classifiers 
adaboost adaboost combined hypotheses final prediction 
marginal adaboost algorithm combine hypotheses final prediction get fairer comparison 
see large improvement ensemble methods compared single classifier 
slight test confidence level significant difference generalization performances adaboost marginal adaboost adaboost adaboost 
note margins combined hypothesis achieved marginal adaboost adaboost average twice large adaboost 
differences generalization performance adaboost marginal adaboost statistically significant 
differences achieved margins algorithms slightly significant 
slightly larger margins generated marginal adaboost attributed fact uses calls weak learner adaboost estimate achievable margin available starts optimizing linear combination estimate 
natural pass algorithm pass adaboost get estimate margin interval estimate run adaboost 
hypothesis produced second pass better quality larger margin fewer bases hypotheses 

analyzed generalized version adaboost context large margin algorithms 
yon neumann min max theorem maximal achievable margin weak learner returns hypothesis weighted classification error asymptotical analysis lead lower bound margin combined hypotheses generated limit shown tight empirical cases 
results indicate adaboost generally maximize margin achieves reasonable large margin 
overcome problems proposed algorithm shown fast convergence maximum margin solution 
achieved decreasing iteratively gap best worst case arbitrarily small 
analysis need assume additional properties weak learning algorithm 
simulation experiment illustrated validity analysis 
bennett demiriz shawe taylor 
column generation algorithm boosting 
langley editor proceedings th icml pages san francisco 
morgan mann 
breiman 
margins relevant voting 
talk workshop large margin classifiers december 
breiman 
prediction games arcing algorithms 
neural computation 
technical report statistics department university california berkeley 
freund 
boosting weak learning algorithm majority 
information computation september 
freund schapire 
experiments new boosting algorithm 
proc 
th international conference machine learning pages 
morgan kaufmann 
freund schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences 
freund schapire 
adaptive game playing multiplicative weights 
games economic behavior 
grove schuurmans 
boosting limit maximizing margin learned ensembles 
proceedings fifteenth national conference artifical intelligence 

semi infinite programming theory methods applications 
siam review september 
kivinen warmuth 
boosting entropy projection 
proc 
th annu 
conference cornput 
learning theory pages 
acm press new york ny 
lozano 
new bounds generalization error combined classifiers 
advances neural information processing systems volume 
lafferty 
additive models boosting inference generalized divergences 
proc 
th annu 
conf cornput 
learning theory pages new york ny 
acm press 
nash 
linear nonlinear programming 
mcgraw hill new york ny 
quinlan 
programs machine learning 
morgan kaufmann 
quinlan 
boosting order learning 
lecture notes computer science 

robust boosting convex optimization 
phd thesis university potsdam potsdam germany october 
demiriz bennett 
sparse regression ensembles infinite finite hypothesis spaces 
machine learning 
special issue new methods model selection model combination 
neurocolt technical report nc tr 
onoda 

soft margins adaboost 
machine learning march 
neurocolt technical report nc tr 
warmuth 
maximizing margin boosting 
proc 
colt volume lnai pages sydney 
springer 
rosset zhu hastie 
boosting regularized path maximum margin separator 
technical report department statistics stanford university 
schapire 
design analysis efficient learning algorithms 
phd thesis mit press 
schapire freund bartlett lee 
boosting margin new explanation effectiveness voting methods 
annals statistics october 
schapire singer 
improved boosting algorithms confidence rated predictions 
machine learning december 
proceedings th workshop computational learning theory pages 
yon neumann 
zur theorie der 
math 
ann 
zhang 
sequential greedy approximation certain convex optimization problems 
technical report ibm watson research center 

