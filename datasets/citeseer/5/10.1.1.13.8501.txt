part speech induction scratch hinrich center study language information ventura hall stanford ca schuetze csli stanford edu presents method inducing parts speech language part speech labels individual words large text corpus 
vector representations part speech word formed entries near lexical neighbors 
dimen reduction creates space represent ing syntactic categories unambiguous words 
neural net trained spa tial representations classifies individual con texts occurrence ambiguous words 
method classifies ambiguous unam words correctly high accuracy 
part speech information individual words necessary kind syntactic higher level processing natural language 
easy obtain lists part speech labels frequent english words information available common languages 
en glish categorization words tailored particular genre may desired 
rare words need categorized fre quent words covered available electronic dictionary 
presents method inducing parts speech language part speech labels individual words large text cor pus 
little language specific knowledge applicable language principle 
part speech representations derived corpus resulting highly text specific doesn contain categories inappropriate genre question 
method efficient vo tens thousands words ad dressing problem coverage 
problem syntactic categories induced theoretical interest language acquisition learnability 
syntactic category information part basic knowledge language children learn acquire complicated structures 
claimed properties child detect input serial positions adjacency occurrence relations words general linguistically irrelevant 
pinker shown relative position words respect suf ficient learning major syntactic categories 
part derivation iterations massive linear approximation cooccurrence counts categorize unambiguous words 
neural net trained words classifies indi vidual contexts occurrence ambiguous words 
evaluation suggests method classi fies ambiguous unambiguous words cor rectly 
differs previous effi ciency applicability large vocabularies linguistic knowledge step theoretical assumptions don hold language sublanguage min influence classification 
sections describe linear ap proximation neural network classification ambiguous words 
section discusses results 
category space goal step induction com pute multidimensional real valued space called category space syntactic category word represented vector 
proximity space related similarity syntactic cat 
vectors space input target vectors connectionist net 
vector space bootstrapped collecting relevant distributional information words 
frequent words months new york times news service june october selected experiments 
pair words wi num ber occurrences wi immediately left wj hi number occurrences wi right cij number occur wl distance word left wj ai number occurrences distance word right wj counted 
sets counts collected matrices respectively 
matrices combined large ma trix shown 
shows words cooccurrence counts located matrix 
experiments resistance 
marks posi tions counts indicate resistance oc positions respect 
element rows matrix directly compute syntactic similar ity individual words cosine angle vectors pair words measure similarity 
computa tions large vectors time consuming 
singular value decomposition formed matrix 
fifteen singular values computed sparse matrix algorithm berry 
result words represented vector real num bers 
original component vectors words corresponding rows ma trix similar collocations similar holds reduced vectors singular value decomposition finds best square approximation orig inal vectors dimensional space pre serves similarity vectors 
see deerwester definition svd appli cation similar problem 
close neighbors dimensional space generally syntactic category seen table 
problem method scale large number words 
singular value tion time complexity quadratic rank matrix treat small part total vocabulary large corpus 
alternative set features con sidered classes words dimensional space 
counting number occur individual words count cosine vectors corresponds normalized correlation coefficient cos ff number occurrences members word classes 
space clustered buckshot linear time clustering algorithm described cut ting 
applies high quality quadratic clustering algorithm random sam ple size number desired cluster centers number vectors clustered 
remaining vec tors assigned nearest cluster center 
high quality quadratic clustering algorithm truncated group average agglomeration cut ting 
clustering algorithms generally con struct groups just member 
closed class words auxiliaries prepositions shouldn thrown open classes verbs nouns 
fore list closed class words essentially words highest frequency set aside 
remaining words classified classes buckshot 
resulting classes high frequency words clusters features matrix shown 
number features greatly reduced larger num ber words considered 
second matrix words occurred times months new york times news service may october selected 
submatrices corresponding relative positions 
example entries aij part matrix count member class occurs distance word left word singular value decomposition performed matrix time singular values computed 
note element rows matrix reduced dimensions second matrix element columns reduced dimensions 
table shows randomly selected words nearest neighbors category space order proximity head word 
seen table proximity space predictor similar syntactic category 
near est neighbors athlete clerk declaration dome singular nouns nearest neighbors bowers gibbs family names near est neighbors desirable sole adjectives nearest neighbors plu ral nouns case exception 
neighborhoods nouns nai northwestern np initial modifiers fail respect finer grained syntactic cf 
brown idea improving generalization accuracy looking word classes individual words 
setup singular value decomposition 
table random selected words nearest neighbors category space 
word accompanied causing classes directors goal japanese represent think york oil features features features features nearest neighbors submitted banned financed developed authorized headed canceled awarded barred virtually merely formally fully quite officially just nearly reflecting forcing providing creating producing carrying particularly elections courses payments losses computers performances violations levels pictures professionals investigations materials competitors agreements papers transactions mood roof eye image tool song pool scene gap voice chinese american western foreign european federal soviet indian reveal attend deliver reflect choose contain impose manage establish retain believe wish know realize wonder assume feel say mean bet angeles francisco sox rouge kong diego zone vegas layer may helps everybody words setup matrix second singular value decomposition 
table random selected words category space 
word athlete nal bowers clerk cruz declaration desirable dome equally gibbs northwestern oh sole transports walks claims oil nearest neighbors landmarks coordination prejudices secrecy virus scenario event audience disorder organism candidate procedure epidemic sri cuny multimedia jacobs levine cart hahn schwartz adams dershowitz fitzpatrick peterson salesman psychologist photographer mechanic dancer lawyer trainer wrinkles streams icons endorsements friction antonio clara pont monica paulo rosa mae attorney palma sequence mood profession marketplace concept facade re loyal devastating ng troublesome awkward blackout furnace quartet citation chain countdown thermometer shaft somewhat progressively acutely enormously excessively unnecessarily largely scattered endeavors raids stalls offerings occupations philosophies adler reid webb jenkins stevens carr dempsey hayes farrell volatility insight hostility dissatisfaction stereotypes competence residues harvard ubs humboldt laguna granada gee ah hey appleton dolly boldface lo lengthy vast rudimentary meager spokesman alloy dal confidently streaming spontaneously floats jumps collapsed sticks peaked runs crashed credits promises forecasts shifts searches trades practices processes supplements controls won doesn may everybody distinctions reasonable representations syntactic category 
neighbors cruz sec ond components names equally adverbs include words wrong category correct part 
order give rough idea density space different locations symbol placed neighbor table correlation head word 
seen table re occupied nouns proper names dense adverbs adjectives distant nearest neighbors 
attempt find fixed threshold separate neigh category syntactically dif ferent ones 
instance neighbors oh correlation higher neighbors threshold region plural nouns 
density space different different re general threshold syntactic categories 
neighborhoods transports walks homogeneous 
words ambiguous third person singular tense plural noun 
ambiguity problem vector representation scheme cause components ambiguous vector add way chance simi lar unambiguous word different syntactic category 
call distributional vector fi words category profile category word wl frequency category cl frequency category weighted sum profiles corresponds column word wl may turn profile unrelated third category probably happened cases transports walks 
neighbors claims demonstrate homogeneous am regions space words ambiguity fre quency ratio categories walks floats jumps sticks runs frequency ratios fl dif ferent claims ended dif ferent regions 
lines table indicate func tion words prepositions auxiliaries nominative pronouns quantifiers occupy regions separated open classes 
network part speech prediction straightforward way take advantage vector representations part speech cluster space assign part speech labels clusters 
done buckshot 
resulting clusters yielded results unambiguous words 
reasons discussed linear combination profiles different categories clustering successful ambiguous words 
fore different strategy chosen assigning category labels 
order tease apart differ ent uses ambiguous words go back individual contexts 
connectionist network analyze individual contexts 
idea network similar elman re current networks elman elman network learns syntactic structure language trying predict word context units previous step current word 
network novel features uses vectors second singular decomposition input target 
note distributed vector representations ideal connectionist nets connection ist model appropriate predic tion task 
second innovation net 
left right 
detail network input consists word left tn left context previous time step word right tn right context rn time step 
second layer context units current time step 
feed hidden units turn produce output vector 
target current word tn 
output units linear hidden units 
network trained stochastically truncated backpropagation time bptt rumelhart williams peng 
purpose left context units un folded time steps left right con text units time steps right shown 
blocks weights connections ln linked ensure identical mapping time step 
connections right side linked way 
train ing set consisted words new york times newswire june 
train ing step words left target word tn tn tn words right target word tn tn tn unfolded network train ing 
input unfolded network 
tar get word tn 
modification bp pdp package learning rate recurrent units units momentum 
training network applied category prediction tasks described choosing part text unknown words computing left contexts left right computing right contexts right left predicting desired category word precomputed contexts rn 
order tag occurrence word retrieve word category space vector closest output vector computed network 
give rise variety category labels 
illustrate con sider prediction category noun 
network categorizes occurrences nouns correctly region declaration slightest variation output change nearest neighbor output vector ration nearest neighbors sequence mood see table 
confusing hu man user categorization program 
output vectors network day june clustered output clusters buckshot 
output cluster labeled words closest centroid 
table lists labels output clusters occurred ex periment described 
easily minimal linguistic knowledge examples show 
cat egories thi 
needs look couple instances get feel mean lo table labels output clusters 
output cluster label prompt select cares office staff promotion trauma famous talented publicly badly ing 
architecture network part speech intransitive verb base form transitive verb base form 
person sg 
tense noun noun adjective adverb np initial syntactic distribution individual word accurately determined algorithm compute output vector position text target word occurs 
output vector determine centroid cluster closest compute correlation coefficient put vector centroid output cluster score si cluster vector assign zero scores clusters vector cluster compute final score fi sum scores sij fi si normalize vector final scores unit length algorithm applied june 
word sum unnormalized final scores corresponding roughly occurrences june word dis 
table lists highest scoring categories random words selected ambiguous words 
categories score listed 
network failed learn distinctions tween adjectives intransitive participles past participles frame non np 
reason adjective close participle past ple shot classified belonging cate gory struggling traveling 
ples successfully discriminated frame np see winning table classified progressive form transitive verb holding promising 
place linguistic knowledge injected form rules word struggling traveling mor participle past participle assign category cat adjective predicative 
word noun category morpho logical plural assign noun plural noun singular 
rules major categories algorithm particular major categories am words better adjective adverb close verb adjective noun base form verb hopes noun third person singular noun participle shot noun past par 
clear errors contain advisory rank table 
table word adequate admit appoint consensus contain genes language legacy thirds better close hospital buy hopes shot winning highest scoring categories random selected words 
highest scoring categories universal martial excel depart prompt select office staff gather propose promotion trauma office staff promotion trauma promotion trauma hand shooting famous talented famous talented gather propose promotion trauma gather propose promotion trauma promotion trauma hand shooting cto famous talented results promising fact context vectors consist units 
naive believe syntactic informa tion sequence words left right expressed small number units 
larger experiment hidden units context vector hopefully yield better results 
discussion brill marcus describe approach simi lar goals brill marcus 
method requires initial consultation native speaker couple hours 
method short consultation native speaker nec essary occurs step category induction 
advantage avoiding bias initial priori classification 
finch chater approach cat induction starts offset counts proceeds classifying words ba sis counts goes back lo cal context better results finch chater 
mathematical computational techniques efficient accurate finch chater applicable vocabularies realistic size 
important feature step pro cedure neural network linguist browse space output vectors word get sense syn tactic distribution instance uses better adverb improve classification struggling traveling gather propose gather propose promotion trauma prompt select promotion trauma office staff office staff famous talented struggling traveling promotion trauma office agent prompt select cares sounds struggling struggling traveling holding promising numerous prompt select hand shooting fantasy ticket route style office agent iron pickup badly famous talented remain want fantasy ticket remain want windows pictures promotion trauma advisory iron pickup stance splitting induced category coarse 
algorithm cate unseen words 
possible long words surrounding known 
procedure part speech categorization introduced may interest words part speech labels known 
di reduction global distribu tional pattern word available profile con dozen real numbers 
compactness profile effi ciently additional source information improving performance natural language processing systems 
example adverbs may lumped category lexicon processing system 
category vectors adverbs different positions completely mainly pre adjectival normally mainly pre verbal differently mainly post verbal different different dis properties 
information exploited parser category vectors available additional source information 
model implications language acquisition 
pro pose absolute position words sen tences important evidence children learn ing categories 
results show relative position sufficient learning major syntactic categories 
suggests rel ative position important information learning syntactic categories child language ac 
basic idea collect large amount distributional information con word cooccurrence counts com pute compact low rank approximation 
approach applied sch forth coming induction vector representations semantic information words differ ent source distributional information 
graded information multi dimensional space vector representa tions particularly suited integrating different sources information tion 
summary algorithm introduced pro vides language independent largely automatic method inducing highly text specific syntactic categories large vocabulary 
hoped method distributional analysis pre sented easier computational traditional lexicographers build ies accurately reflect language 
acknowledgments indebted mike berry marti hearst jan pedersen reviewers helpful comments 
partially supported national cen ter supercomputing applications bns 
berry michael 
large scale sparse lar value computations 
international jour nal supercomputer applications 
brill eric mitch marcus 

tagging unfamiliar text minimal human sion 
working notes aaai fall sym probabilistic approaches natural language ed 
robert goldman 
aaai press 
brown peter vincent della pietra pe ter desouza lai robert mercer 

class gram models natural language 
computational linguistics 
cutting douglas jan pedersen david karger john tukey 

ter gather cluster approach browsing large document collections 
pro ceedings sigir 
deerwester scott susan dumais george furnas thomas landauer richard harshman 

indexing latent semantic analysis 
journal american society information science 
elman jeffrey 
finding structure time 
cognitive science 
elman jeffrey 
distributed repre sentations simple recurrent networks grammatical structure 
machine learning 
finch steven nick chater 

boot strapping syntactic categories cal methods 
background experiments machine learning natural language ed 
walter daelemans david powers 
tilburg university 
institute language technology ai 


inter nal language children syntax sis representation syntactic categories 
children language ed 
nelson 
new york gardner press 
pinker steven 

language learnability language development 
cambridge ma har university press 
rumelhart hinton williams 

learning internal representa tions error propagation 
parallel dis tributed processing 
explorations mi cognition 
volume tions ed 
david rumelhart james mc pdp research group 
cam bridge ma mit press 
hinrich 
forthcoming 
word space 
advances neural information processing sys tems ed 
stephen hanson jack cowan lee giles 
san mateo ca morgan mann 
williams ronald jing peng 

ef ficient gradient algorithm line training recurrent network trajectories 
neural computation 
