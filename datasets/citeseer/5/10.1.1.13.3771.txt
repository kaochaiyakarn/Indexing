appeared tools ai pages received ieee tai ramamoorthy best award minor modifications nov data mining mlc machine learning library www sgi com technology mlc ron kohavi data mining visualization silicon graphics shoreline blvd mountain view ca ronnyk engr sgi com dan sommerfield data mining visualization silicon graphics shoreline blvd mountain view ca engr sgi com james dougherty platform group sun microsystems ms garcia ave mountain view ca eng sun com data mining machine learning statistical analysis pattern recognition techniques greatly improve understanding data warehouses widespread 
focus classification algorithms review need multiple classification algorithms 
describe system called mlc designed help choose appropriate classification algorithm dataset making easy compare utility different algorithms specific dataset interest 
mlc provides workbench comparisons provides library classes aid development new algorithms especially hybrid algorithms multi strategy algorithms 
algorithms generally hard code scratch 
discuss design issues interfaces programs visualization resulting classifiers 
data warehouses containing massive amounts data built decade 
organizations find unable understand interpret extrapolate data achieve competitive advantage 
machine learning methods statistical methods pattern recognition methods provide algorithms mining databases order help analyze information find patterns improve prediction accuracy 
problem users analysts face trying uncover patterns build predictors cluster data algorithms available hard determine 
detail system called mlc machine learning library designed aid algorithm selection development new algorithms 
mlc project started stanford university summer currently public domain software including sources 
brief description library plan kohavi john long manley pfleger 
distribution moved silicon graphics late 
dozens people people mailing list 
motivation mlc library fact single best learning algorithm tasks 
proven theoretically shown experimentally 
recommendation users run different algorithms 
developers mlc create new algorithms suitable specific tasks 
second part describe mlc system dual role system users algorithm developers 
show large comparison algorithms large datasets uc irvine repository murphy aha 
study magnitude extremely hard conduct tool 
matter cpu cycles scripts parse output 
study shows behavior different algorithms different datasets stresses fact clear winners algorithms practice better datasets 
importantly specific task relatively easy choose best algorithms accuracy estimation utility measures comprehensibility 
third part discuss software development process hindsight 
forced choices way briefly describe library evolved 
conclude related 
best algorithm task theory difference theory practice practice chuck reid theoretical results show single algorithm uniformly accurate domains 
theorems limited applicability practice little known algorithms choose specific problems 
claim organization specific background knowledge help choose algorithm tailor algorithm specific needs simply try pick best task 
single best algorithm theoretical result single learning algorithm outperform performance measure expected generalization accuracy 
result called free lunch theorem conservation law wolpert schaffer assumes possible targets equally 
practice course user data mining tool interested accuracy efficiency comprehensibility specific domain just car buyer interested power gas mileage safety specific driving conditions 
averaging algorithm performance target concepts assuming equally averaging car performance possible terrain types assuming equally 
assumption clearly wrong practice domain clear concepts equally probable 
medical domains measurements attributes doctors developed years tend independent attributes highly correlated attribute chosen 
domains certain class learning algorithms outperform 
example naive bayes performer medical domains kononenko 
quinlan identifies families parallel sequential domains claims neural networks perform parallel domains decision tree algorithms perform sequential domains 
single induction algorithm build accurate classifiers situations algorithms clear winners specific domains just cars clear winners specific driving conditions 
usually option test drive range cars obvious car best purpose 
true data mining algorithms 
ability easily test drive different algorithms factors motivated development mlc take algorithm test drive decide type vehicle large luxury car small economy model practical family sedan coupe 
point re ready trip test drive 
consumer reports buying guide buy new car organizations mine databases different reasons 
note relevant classification algorithms classification accuracy accuracy predictions instance 
example customer able pay loan respond credit card offer 
methods holdout bootstrap cross validation weiss kulikowski efron tibshirani kohavi estimate prediction accuracy unseen data quite practice 
comprehensibility ability humans understand data classification rules induced learning algorithm 
classifiers decision rules decision trees inherently easier understand neural networks 
domains medical black box approach offered neural networks inappropriate 
handwriting recognition important understand prediction long accurate 
compactness related comprehensibility necessarily imply 
perceptron single neuron compact classifier instance may hard understand labelling process 
alternatively decision table kohavi may large labelling instance trivial simply looks table 
training classification time time takes classify versus training time 
classifiers neural networks fast classify slow train 
classifiers nearest neighbor algorithms lazy algorithms see aha appear details usually fast train slow classification 
factors define utility function rank different algorithms fayyad piatetsky shapiro smyth 
step test drive algorithms note utility specific domain problem 
believe rules thumb choosing algorithms choosing classifier done testing different algorithms just best test drive car 
mlc friendly car dealer honest 
mlc users mlc useful writing new algorithms users simply test different learning algorithms 
pressure reviewers compare new algorithms led interface external inducers induction algorithms written people 
mlc provides appropriate data transformations interface external inducers 
find type car provide shuttle service take easily test cars 
inducers induction algorithms implemented mlc id decision tree algorithm quinlan 
nearest neighbor classical nearest neighbor options weight setting normalizations editing dasarathy aha wettschereck 
naive bayes simple induction algorithm assumes conditional independence model attributes label domingos pazzani langley iba thompson duda hart 
read decision graph induction algorithm described kohavi 
lazy decision trees algorithm building best decision tree test instance described friedman kohavi yun 
algorithm described holte 
decision table simple lookup table 
simple algorithm useful feature subset selection 
perceptron simple perceptron algorithm described hertz krogh palmer 
winnow multiplicative algorithm described littlestone 
const constant predictor majority 
external inducers interfaced mlc decision tree induction quinlan 
rules trees rules induction algorithm quinlan 
cn direct rule induction algorithm clark niblett clark boswell 
ib set instance learning algorithms aha 
oc oblique decision tree algorithm kasif salzberg 
pebls parallel exemplar learning system cost salzberg 
level error minimizing decision tree auer holte maass 
appropriate tasks 
example perceptron winnow limited class problems reduces usefulness problems encounter including tested section 
wrappers hybrid algorithms algorithms encapsulated objects mlc able build useful wrappers 
wrapper algorithm treats algorithm black box acts output 
algorithm written mlc wrapper may applied extra 
important wrappers mlc accuracy estimators feature selectors 
accuracy estimators range methods holdout crossvalidation bootstrap estimate performance inducer kohavi 
feature selection methods run search inducer determine attributes database useful learning 
wrapper approach feature selection automatically tailors selection inducer run john kohavi pfleger 
voting wrapper runs algorithm different portions dataset lets vote predicted class wolpert breiman perrone ali 
discretization wrapper pre discretizes data allowing algorithms support continuous features handle properly 
parameter optimization wrapper allows tuning parameters algorithm automatically search parameter space optimizes accuracy estimate different parameters 
inducers created combinations induction decision tables majority 
feature subset selection wrapper top decision tables kohavi kohavi 
auto automatic parameter setting kohavi john 
fss naive bayes feature subset selection top naivebayes kohavi sommerfield 
nbtree decision tree hybrid naive bayes leaves kohavi 
ability create hybrid wrapped algorithms important powerful approach multistrategy learning 
mlc implement algorithms just decide integrate wrap 
mlc utilities mlc utilities set individual executables built mlc library 
designed users little programming knowledge 
utilities employ consistent interface options may set command line environment variables 
utilities centered induction 
inducer 
inducer simply runs induction algorithm choice dataset testing resulting classifier test file 
estimates performance induction algorithm dataset accuracy estimation techniques provided holdout cross validation bootstrap 
builds graphical representation learning curve algorithm running algorithm differently sized samples dataset 
output displayed mathematica gnuplot 
remaining utilities provide dataset operations 
info utility generates descriptive statistics dataset including counts number attributes class probabilities number values attribute 
project utility performs equivalent database select operation allowing user remove attributes dataset 
discretization utility converts real valued attributes nominal valued attributes number supervised discretization methods supported library 
conversion utility changes multi valued nominal attributes local binary encodings may useful nearest neighbor neural network algorithms 
visualization induction algorithms support visual output classifiers 
graph algorithms support display dimensional representations graphs dot dotty koutsofios north 
dotty capable showing extra information node class distributions 
shows graph 
decision tree algorithms id may generate output silicon graphics tm product 
tree visualizer provides dimensional view decision tree interactive fly capability 
shows snapshot display 
mlc provides utility displaying general logic diagrams classifiers implemented mlc general logic diagrams graphical projections multi dimensional discrete spaces dimensions 
similar maps generalized non boolean inputs outputs 
provides way displaying dimensions graphical representation understood humans 
described michalski thrun 
michalski compare algorithms 
naive bayes algorithm may visualized dimensional bar chart log probabilities 
threedimensional view implemented virtual reality modeling language vrml standard viewed web browsers 
height bar represents evidence favor class single attribute set specific value 
shows snapshot visualization 
global comparison statlog taylor michie large project compared different learning algorithms datasets 
project funded esprit program european community lasted october physician fee freeze education spending mx missile democrat democrat democrat republican republican republican democrat democrat republican 
dot display decision tree congressional voting dataset 

snapshot tm tree visualizer fly decision tree 

snapshot naive bayes view 
june 
tool mlc organization specific domain problems easily repeat effort order choose appropriate algorithms 
converting data formats learning different algorithms running usually question 
mlc easily provide comparison 
section comparison large datasets uc irvine repository murphy aha 
take opportunity correct misunderstandings results holte 
table shows basic characteristics chosen domains 
instances unknown values removed original datasets 
holte showed small datasets commonly researchers machine learning circa simple classifiers perform surprisingly 
specifically holte proposed algorithm called learns complex rule intervals single attribute 
claimed simple rule performed surprisingly 
sixteen datasets uci repository error rate difference 
surprising single attribute different way look result look relative error increase error 
increase significant organization pay large amount money mistake 
figures show comparison algorithms large datasets uc irvine repository 
results show different domains different algorithms perform differently single clear winner 
show larger real world datasets uc irvine algorithms generally pretty bad general 
specifically algorithms auto nbtree fss naivebayes best performers datasets algorithms consistently worst performers 
fact worst set datasets run datasets required mb memory 
little theory select algorithms advance simply running algorithms looking output accuracy practical solution mlc mlc software developers mlc source code public domain including sources 
mlc contains lines code lines regression testing code 
mlc utilities lines code library 
advantages mlc software developers provides high encourages high quality code development 
providing large number general purpose classes large set classes specific machine learning mlc ensures coding remains small part development process 
mature coding standards shallow class hierarchy insure new code robust helpful developers 
take longer write fully tested code mlc ultimately mlc greatly decreases time needed complete robust product 
mlc includes library general purpose classes independent machine learning called 
classes include strings lists arrays hash tables input output streams built mechanisms dealing initialization order options temporary file generation cleanup interrupt handling 
classes provide wide range functions provide solid tests functions reducing time takes build complex operations 
general purpose classes full library exception graph classes leda library 
classes built mlc library tied machine learning may general purpose library 
separate library may linked independently library 
mlc classes arranged library independent units tree forest set modules requiring multiple link compile stages 
ml mind modules follow philosophy series classes encapsulate basic concepts machine learning 
inheritance helps programmer 
resisting temptation maximize objects able provide set basic classes clear cut interfaces developing machine learning algorithm 
important concepts provided library include instance lists classifiers inducers 
instance lists supporting classes hold database learning algorithms run 
known classifiers inducers induction algorithms represent algorithms inducer machine learning algorithm produces classifier turn assigns class instance 
replace real valued attributes database attributes algorithms discrete values 
mlc provides discretization methods dougherty kohavi sahami 
programming mlc generally requires little coding 
mlc contains tested modules utility classes bulk mlc programming determining existing code base implement new functionality 
learning large code base takes time find code write algorithm adult algorithm adult best algorithms worst algorithms algorithm chess algorithm chess best algorithms worst algorithms algorithm dna algorithm dna best algorithms worst algorithms algorithm led algorithm led best algorithms worst algorithms auto rules voted id voted id nbtree cn fss naive bayes decision table naive bayes nearest neighbor nearest neighbor oc pebls 
error relative errors learning algorithms 
algorithm letter algorithm letter best algorithms worst algorithms algorithm shuttle algorithm shuttle best algorithms worst algorithms shown left range algorithm satimage algorithm satimage best algorithms worst algorithms algorithm waveform algorithm waveform best algorithms worst algorithms auto rules voted id voted id nbtree cn fss naive bayes decision table naive bayes nearest neighbor nearest neighbor oc pebls 
error relative errors learning algorithms 
needed mb memory letter shuttle datasets 
table 
datasets number attributes training test set sizes baseline accuracy majority class 
dataset attributes train test majority dataset attributes train test majority size size accuracy size size accuracy adult chess dna led letter shuttle satimage waveform easier test debug leverage testing debugging done rest library 
additionally forcing developer break problem mlc concepts insures mlc maintains consistent design turn insures consistent code quality 
class mlc testable independently little reliance classes 
built large suite regression tests insure quality class library 
tests immediately determine code change created problem 
goal tests force bugs show close sources possible 
likewise porting library greatly simplified tests provide clear indication problems port 
unfortunately compilers support full ansi templates required compile mlc mlc development hindsight hindsight billy wilder decisions development mlc tackled issues language libraries users know code order stress efficiency versus safety 
decided mlc useful users programmed want program lisp packages machine learning require writing code just load data 
realized gui useful concentrated adding functionality providing simple general environment variable command line interface 
provided source code people interface 
mlc teach machine learning courses stanford changes library student feedback 
chose number reasons 
general purpose language useful features imposes style requirements 
features templates allowed maintain generality reusability safety strong typing static checking 
second language object oriented features allowed decompose library set modular objects tested independently 
time relative flexibility allowed avoid object oriented paradigm needed 
third language imposed barriers efficiency functions inline objects placed stack faster access 
gave ability optimize code critical sections 
widely accepted language gaining popularity quickly 
project created stanford extremely useful language everybody wanted learn 
decision best compiler time centerline available features 
assumed gnu compiler catch 
mike gave animated interesting presentation compiler just put compiler writers business 
sadly gnu compiler weaker commercial grade compilers handle templates 
compilers compile mlc hope change bit operating systems mature compilers improve 
quickly chose leda graph manipulation algorithms gnu provided basic data structures 
unfortunately gnu library deficient respects 
hasn kept emerging standards issues templates slowly rewrote classes 
today mlc 
starting project today standard template library stl provides large set solid data structures algorithms static safety templates widely accepted standard 
current disadvantage class libraries including mlc developers learn large code base standard data structures algorithms 
greatly increases learning curve library 
standard library stl flatten curve 
done mlc motivated research interests author 
resulted skewed library symbolic algorithms neural networks statistical regression algorithms 
neural network statistical algorithms algorithms existed little need write contribute immediate research interests 
today focus shifting data mining library increasingly balanced 
related large efforts describe data mining algorithms knowledge discovery data 
refer reader url info gte com kdd html pointers description 
briefly mention systems provide multiple tools goals similar mlc tm silicon graphics data mining visualization product 
release uses mlc base induction classification algorithms 
classification models built shown visualization tools 
tm gui interface accesses commercial databases including oracle sybase informix 
classification provides association algorithm 
collection publicly available algorithms re implementations 
algorithms share common code interface 
collection methods statistical pattern recognition especially classification 
contains algorithm nearest neighbor radial basis functions parzen windows feature selection extraction limited continuous attributes missing values 
multistrategy tool integrates manual knowledge acquisition techniques automatic order learning algorithms 
entire system rulebased existing knowledge stored syntax similar predicate logic 
emerald research prototype george mason university 
main features learning programs gui 
lisp system capable learning rules translated english spoken speech synthesizer 
algorithms include algorithms learning rules examples learning structural descriptions objects conceptually grouping objects events discovering rules characterizing sequences learning equations qualitative quantitative data 
machine learning knowledge engineering tool package implemented cart id segmentation chi aid algorithms classification 
runs real discrete valued data sets oriented building testing expert systems 
reads ascii lotus format files 
commercial tools data mining include ibm intelligent data miner darwin 
darwin share common underlying library 
ibm intelligent data miner provides variety knowledge discovery algorithms classification associations clustering sequential pattern discovery 
includes neural network interface decision tree rule induction 
strong dataflow gui simple visualizations 
darwin suite learning tools developed thinking machines 
intended large databases uses parallel processing speedup 
algorithms include classification regression trees cart neural networks nearest neighbor genetic algorithms 
darwin includes visualization 
summary mlc machine learning library greatly evolved years 
provides developers substantial piece code organized useful hierarchy 
research project managed keep code quality high regression tests 
library provides users ability easily test drive different induction algorithms datasets interest 
accuracy estimation visualization classifiers allow pick best algorithm task 
silicon graphics new data mining product tm classifiers built top mlc gui interfaces commercial databases 
hope open machine learning data mining wider audience 
acknowledgments mlc project started summer continued years stanford university 
late distribution support moved silicon graphics 
nils nilsson yoav shoham provided support project initial stages 
wray buntine george john pat langley ofer karl pfleger scott roy contributed design mlc students stanford worked mlc including robert allen brian james dougherty steven ronny kohavi alex jamie chia hsin li richard long david manley nestorov mehran sahami dan sommerfield yun 
mlc partly funded onr nsf iri 
eric bauer clayton kunz working extending mlc stanford support silicon graphics 
ronny kohavi dan sommerfield continue project silicon graphics part tm product 
aha 
tolerating noisy irrelevant novel attributes instance learning algorithms international journal man machine studies 
aha 
appear ai review journal special issue lazy learning 
ali 
learning probabilistic relational concept descriptions phd thesis university california irvine 
www ics uci edu ali 
auer holte maass 
theory applications agnostic pac learning small decision trees prieditis russell eds machine learning proceedings twelfth international conference morgan kaufmann publishers breiman 
bagging predictors technical report statistics department university california berkeley 
clark boswell 
rule induction cn improvements kodratoff ed proceedings fifth european conference ewsl springer verlag pp 

clark niblett 
cn induction algorithm machine learning 
cost salzberg 
weighted nearest neighbor algorithm learning symbolic features machine learning 
dasarathy 
nearest neighbor nn norms nn pattern classification techniques ieee computer society press los alamitos california 
domingos pazzani 
independence conditions optimality simple bayesian classifier saitta ed machine learning proceedings thirteenth international conference morgan kaufmann publishers pp 

dougherty kohavi sahami 
supervised unsupervised discretization continuous features prieditis russell eds machine learning proceedings twelfth international conference morgan kaufmann pp 

duda hart 
pattern classification scene analysis wiley 
efron tibshirani 
cross validation bootstrap estimating error rate prediction rule technical report stanford university statistics department 
fayyad piatetsky shapiro smyth 
data mining knowledge discovery overview advances knowledge discovery data mining aaai press mit press chapter pp 

friedman kohavi yun 
lazy decision trees proceedings thirteenth national conference artificial intelligence aaai press mit press 

estimation probabilities essay modern bayesian methods press 
hertz krogh palmer 
theory neural computation addison wesley 
holte 
simple classification rules perform commonly datasets machine learning 
john kohavi pfleger 
irrelevant features subset selection problem machine learning proceedings eleventh international conference morgan kaufmann pp 

kohavi 
power decision tables lavrac wrobel eds proceedings european conference machine learning lecture notes artificial intelligence springer verlag berlin heidelberg new york pp 

kohavi 
study cross validation bootstrap accuracy estimation model selection mellish ed proceedings th international joint conference artificial intelligence morgan kaufmann publishers pp 

kohavi 
wrappers performance enhancement oblivious decision graphs phd thesis stanford university computer science department 
stan cs tr ftp stanford edu pub ronnyk ps 
kohavi 
scaling accuracy naive bayes classifiers decision tree hybrid proceedings second international conference knowledge discovery data mining appear 
kohavi john 
automatic parameter selection minimizing estimated error prieditis russell eds machine learning proceedings twelfth international conference morgan kaufmann publishers pp 

kohavi john long manley pfleger 
mlc machine learning library tools artificial intelligence ieee computer society press pp 

www sgi com technology mlc 
kohavi sommerfield 
feature subset selection wrapper model overfitting dynamic search space topology international conference knowledge discovery data mining pp 

kononenko 
inductive bayesian learning medical diagnosis applied artificial intelligence 
koutsofios north 
drawing graphs dot 
available anonymous ftp research att com dist ps langley iba thompson 
analysis bayesian classifiers proceedings tenth national conference artificial intelligence aaai press mit press pp 

littlestone 
learning quickly irrelevant attributes abound new linear threshold algorithm machine learning 
michalski 
planar geometric model representing multidimensional discrete spaces multiple valued logic functions technical report uiucdcs university illinois champaign 
murphy aha 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
murthy kasif salzberg 
system induction oblique decision trees journal artificial intelligence research 

leda library efficient data types algorithms edn max planck institut fuer informatik im stadtwald frg 
www mpi sb mpg de leda leda html 
perrone 
improving regression estimation averaging methods variance reduction extensions general convex measure optimization phd thesis brown university physics dept quinlan 
induction decision trees machine learning 
reprinted shavlik dietterich eds 
readings machine learning 
quinlan 
programs machine learning morgan kaufmann publishers los altos california 
quinlan 
comparing connectionist symbolic learning methods hanson rivest eds computational learning theory natural learning systems vol 
constraints prospects mit press chapter pp 

schaffer 
conservation law generalization performance machine learning proceedings eleventh international conference morgan kaufmann publishers pp 


design evolution addison wesley publishing 
taylor michie 
machine learning neural statistical classification paramount publishing international 
thrun 
monk problems performance comparison different learning algorithms technical report cmu cs carnegie mellon university 
weiss kulikowski 
computer systems learn morgan kaufmann publishers san mateo ca 
wettschereck 
study distance machine learning algorithms phd thesis oregon state university 
michalski 
hypothesis driven constructive induction aq hci method experiments machine learning 
wolpert 
stacked generalization neural networks 
wolpert 
relationship pac statistical physics framework bayesian framework vc framework wolpert ed generalization addison wesley 

