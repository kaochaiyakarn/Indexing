hierarchical multi classi cation hendrik blockeel maurice bruynooghe sa zeroski jan ramon jan katholieke universiteit leuven department computer science celestijnenlaan leuven belgium blockeel maurice bruynooghe jan ramon jan cs kuleuven ac jo stefan institute si ljubljana slovenia saso dzeroski ijs si 
problem hierarchical multi classi cation considered 
setting set classes assigned single instance possible classes structured hierarchy 
example application domains functional genomics text classi cation 
algorithm solve hierarchical multi classi cation tasks 
decision tree induction algorithm notion predictive clustering trees suitable distance measure plugged 
preliminary results algorithm data sets functional genomics text classi cation reported discussed 
data mining algorithms construct predictive models target variable dimensional nominal classi cation tasks numerical regression tasks 
algorithms handle task making predictions structural kind 
relational data mining algorithms usually focus learning classi cation regression models structural properties prediction predicted value structured 
multi classi cation simple kind structure prediction target variable set classes 
setting occurs relatively frequently document classi cation document typically relevant topics may belong classes 
problem tackled learning separate models class indicating single instance belongs class learning single model classes advantage total size predictive theory typically smaller dependencies di erent classes membership taken account may 
advantages learning single model multiple related prediction tasks reported times literature see decision trees neural networks text classi cation :10.1.1.50.3353
setting assigning set classes instance look detail case classes ordered hierarchy 
hierarchy concisely conveys relevant information similarity di erences classes expresses constraint object belonging class belongs parent class 
hierarchy prediction represented subtree hierarchy predicted classes leaves internal nodes 
boils special case prediction structured values extension multi classi cation problem call hierarchical multi classi cation 
example useful document classi cation document classi ed number newsgroups newsgroups form hierarchy 
remainder text structured follows 
section describe general setting distance multi classi cation hierarchical multi classi cation section special case 
section brie reviews ideas predictive clustering trees algorithm 
section presents distance measure plugged decision tree learner order obtain multi classi cation 
section presents experiments algorithm 
section discusses related section concludes 
distance multi classi cation approach hierarchical multi classi cation placed general context distance multi classi cation describe rst 
multi classi cation context individual belongs classes new instance wish predict classes belongs 
important element evaluate quality predictions 
distinguish possibilities general speci cost 
cost associated including class prediction ci omitting class included substituting class class cs ij 
costs positive 
distance 
similar cost approach imposes constraint ci cs ij cs ji classes due symmetry distances 
distances similar called edit distances 
accuracy 
substitutions allowed modelled inclusions omissions ci cs ij unde ned 
criterion amounts kind weighted average accuracy di erent classes predicted 
example 
consider task predicting article newsgroups relevant 
assume relevant newsgroups article autos science 
predictions autos science sport science respectively cost making vague prediction rec rec autos higher making speci wrong guess rec sport 
mistake classi cation high level worse lower level 
costs chosen answers questions may depend application 
example 
consider task composing ice cream cone 
choose set person 
penalty omitting chocolate person likes 
assume person likes normally choose 
similarity penalty substituting probably smaller sum penalties including omitting 
situation modelled accuracy method 
current studies hierarchical multi classi cation distances classes de ned position hierarchy 
instance distance setting mentioned 
hierarchical multi classi cation represent hierarchy set values tree de ned root element parent function maps children tree node node de ned root element 
valid set set values closed respect parent function parent root 
denotes powerset denotes set valid sets problem hierarchical multi classi cation stated follows 
instance space class space hierarchy set examples quality criterion find function maps instance valid set classes maximizes quality criterion represent hierarchy tree single prediction subtree graph sense 
illustrates 
case predicting single leaf class hierarchy subtree reduces single branch 
assume subtree hierarchy semantically correct meaningful prediction 
speci applications constraints may hold example classes incompatible 
information better exploited making prediction 
consider possibility 
putting hierarchical multi classi cation distance context means need method deriving distance hierarchy 
intuitively distance classes smaller closer hierarchy 
siblings node equidistant distance node parent nodes level 
natural distance de nitions ful lling criteria 
quality criterion need distance 
instance just average accuracy di erent science rec electronics medical autos sport baseball hockey science rec electronics medical sport baseball hockey autos fig 

left class hierarchy right prediction sport science depicted forms subtree hierarchy indicated bold 
classes predicted take account fact predicting rec sport hockey smaller mistake article belongs class rec sport baseball article sport 
nally representing prediction subtree hierarchy natural constraints class membership belonging automatically belongs automatically 
guaranteed independent models learnt di erent classes 
problem clearly de ned question construct algorithm learns predictive models setting 
follow predictive clustering approach argued provides general approach predictive modelling :10.1.1.50.3353:10.1.1.50.3353
predictive clustering trees variety algorithms predictive modeling exists 
better known algorithms induce decision trees 
compared known techniques neural networks decision trees advantage interpretable clearly factors uence outcome strongly 
decision trees context classi cation regression represent model value single variable predicted 
decision tree naturally identi es partitions data course grained top tree ne grained bottom consider tree hierarchy clusters 
cluster hierarchy individuals cluster similar respect number observable properties 
leads simple method building trees allow prediction multiple target attributes 
de ne distance measure tuples target variable values build decision trees multi target prediction 
similarly distance structured target values de ned build decision trees prediction structural target variables 
methodology successfully variety applications conceptual clustering simultaneous prediction multiple parameters ranking tasks :10.1.1.50.3353:10.1.1.50.3353
algorithm inducing trees essentially standard tdidt topdown induction decision trees algorithm id 
general idea recursively partition set data clusters way intracluster variance minimized 
words heuristic selecting tests include node tree variance 
intra cluster variance de ned sum squared distances members cluster prototype de ned arg min roughly point closest instances cluster distance de ned 
prototype may valid prediction 
instance prediction prototype mean target values making prediction speci instance converted valid prediction 
result induction process decision tree leaf contains prediction derived prototype examples covered leaf 
detailed description algorithm :10.1.1.50.3353:10.1.1.50.3353
main point proposed method inducing predictive clustering trees relies entirely de nition distance measure prototypes mapping prototypes valid predictions 
issues focus section 
distance measure introduce distance measure speci purpose hierarchical multi classi cation fact generally useful multi classi cation 
idea distances individual classes input method distance measure sets classes de ned compatible individual distances 
done mapping sets vectors euclidean space individual classes base vectors 
depending distances base may may orthogonal 
intuitively classes close base vectors point direction 
rst discuss alternatives deriving distance classes hierarchy discuss upgraded distance sets classes 
distance nodes hierarchy current hierarchical classi cation considers distance nodes tree 
examples shortest path distance spd considering tree graph number edges shortest path nodes distance nodes 
computed depth depth depth deepest common ancestor dca weighted shortest path distance wspd similar previous edges weights 
edges deeper tree typically lower weights 
weighted penalty wp weights assigned nodes distance nodes weight dca 
nodes deeper tree lower weights 
di erence wspd wp distances mainly rst type focuses dissimilarities nodes parts dca second similarities part dca larger smaller dca weight 
note distance measures generalise ilp context 
distances normally depend application intention general claims 
method suciently exible cater wide variety distance measures wide range applications 
assume distance measure individual nodes tree need sets nodes 
continue describe distance nodes upgraded distance sets nodes combinations nodes shall see 
distance combinations nodes assume nite domain elements fe classes 
metric need subsets metric elements domain metrics subsets domain de ned hausdor metric metric proposed matchings 
decision tree algorithm able compute prototypes eciently 
existing metrics sets straightforward choose approximative method originates kernel methods 
empty set subset called origin 
assume application allows de ne distance singletons origin di erent singletons 
note distances respectively correspond earlier mentioned ci cs ij cs ji follow procedure proposed 
set converted binary vector th component indicates element set 
note singletons fe base vectors space th component components 
dimensional feature space kernel corresponding euclidean distance de ned compatible assumed distances 
de ne function ir inner product euclidean space 
element feature space ir written linear combination base vectors element denoted denoted 
de ne function ir ir ir ir discussed positive semi de nite ir verify kernel 
metric ir ir ir induced de ned agrees positive semi de nite non negative metric 
consider set fa dg 
euclidean space center line connecting 
di erent impossible map euclidean space 
re ected fact kernel positive semi de nite 
positive eigenvalues matrix average larger absolute value negative ones starts basic distance function positive pairs base vectors 
discussed kernel satisfy required theoretical properties metric turns behave relatively practice understood 
vector notation say constructed matrix elements ij linear combinations elements represented vectors inner product distance 
summarizing distance compute kernel de ne euclidean distance metric sums class values compatible distances sense class values holds 
sets fe fe class values jn distance sets 
show section easy de ne prototypes 
prototype euclidean vector space prototype set vectors just arithmetic mean vectors set ecient easy compute 
note non euclidean distances manhattan distances prototypes uniquely de ned dicult compute 
mapping prototype prediction suppose learned decision tree sorts new example leaves 
tree predictions class es examples 
look prototype training examples leaf assigned new example 
discussed previous section prototype vector euclidean space class values mapped 
vector necessarily valid prediction set examples mean components rounded 
just rounding individual components whichever nearest necessarily give optimal solution base vectors may orthogonal dependencies components 
iterative procedure construct valid prediction 
start valid prediction just empty set consecutive steps greedily add base vector maximally decreases error criterion 
th component prototype vectors exist 
preliminary experiments experiments describe preliminary 
implemented version distances mentioned run experiments easily obtainable data sets trying gain insight behaviour proposed method 
algorithm experiments clus system designed building predictive clustering trees discussed section 
fact propositional version tilde system :10.1.1.50.3353
added clustering mode clus distance described section 
distance base vectors weighted shortest path distance 
weights distance decrease exponentially hierarchy depth weight edge depth node edge originates parameter 
experiments arbitrarily chosen 
text classi cation experimental setup rst experiment apply clus task classifying usenet newsgroup articles 
aware decision trees optimal tool document classi cation support vector machines usually perform better reasonable application domain aim learn approach 
articles assigned leaves newsgroup hierarchy 
articles belong multiple newsgroups data set obtained articles belong single newsgroup 
experiment useful mainly clus available authors request 
comp graphics rec autos comp os ms windows misc rec motorcycles comp sys ibm pc hardware rec sport baseball comp sys mac hardware rec sport hockey comp windows sci crypt talk politics guns sci electronics talk politics mideast sci med talk politics misc sci space talk religion misc alt atheism soc religion christian misc table 
usenet newsgroups newsgroup data investigate bene hierarchical information building cation tree 
compare settings clus hierarchical multi classi cation 
classi cation denote setting leaf classes equidistant information hierarchy taken account 
data set collected ken lang 
contains articles taken di erent newsgroups 
data set compare di erent algorithms perform single classi cation 
data preparation involved 
original hierarchy data set ideal parts degenerate class space hierarchy contains subclass category soc 
order increase hierarchical structure class space removed classes alt atheism soc religion christian misc 
cross posted articles appear multiple examples data set setting form single multi class example 
removing classes merging cross posted examples cleaning data set bit removing articles bodies unsubscribe data set containing examples multi class 
example selected features attributes clus statistical text processing system rainbow 
features selected information gain criterion feature corresponds number occurrences word article 
passing article rainbow removed header elds subject header elds may reveal class article 
experiments run follows 
rst randomly selected available examples test examples 
remaining sampled training sets di erent sizes ranging examples 
training set compare hierarchical clus distances de ned newsgroup hierarchy clus distance criterion classes equidistant simulated rede ning hierarchy classes level directly root 
results evaluated calculating mean squared weighted shortest path distance actual predicted set classes test training set size newsgroup classification default flat hierarchical fig 

evolution shortest path error criterion training set size 
hierarchical clustering consistently performs slightly better clustering 
examples true hierarchy 
experiment repeated times average results reported 
results shows average results obtained 
shows line marked default corresponds predicting set classes corresponding prototype training set 
corresponds decision tree just leaf 
training sets low number examples method scores worse compared hierarchical default 
scores worse default generates sub optimal predictions introduce test just predict leaf set classes predicted leaf di er default hierarchical information calculating prototypes 
number training examples increases hierarchical approach slightly better compared approach 
slightly better somewhat disappointing hierarchical approach uses relevant information approach fact heuristic directly related error criterion approach rough approximation possible explanation small di erence hierarchy data set small method nd useful tests depending hierarchical information 
obvious extension course retrieve larger hierarchy usenet newsgroups repeat experiment 
note allow algorithm predict leaves hierarchy 
means predict comp graphics comp 
allow internal nodes decreased hierarchical 
internal nodes valid newsgroups 
functional genomics experimental setup second experiment apply clus cation task introduced clare king :10.1.1.20.3909
modi ed version decision tree learner capable learning cation trees exploit hierarchical information 
phenotype data set contains examples 
example corresponds mutant strain obtained removing speci gene cell case yeast cerevisiae cells 
mutant cells grown di erent conditions growth media 
growth medium corresponds attributes data set 
attribute valued wild type means growth mutant equal original cells wildtype sensitive means growth mutant resistance means better growth mutant 
data set lot missing values attribute values missing possible growth experiments carried 
example attribute discretized number growth media mutant di ers wild type 
target value set functional classes removed gene 
functional class belongs hierarchy depth classes level metabolism energy transcription leaves 
resampling approach nd accurate stable rules selected rules :10.1.1.20.3909
preliminary experiment just train test split examples 
clus hierarchical multi classi cation setting build model training set see compares rules :10.1.1.20.3909
results shows part clustering tree obtained 
tree small contains tests 
apparently tests considered signi cant probably huge number missing values data 
clus shows test lines percentage examples test succeeds percentage missing values attribute test 
leaf shows size expected number training examples test examples leaf number test examples certainly belong leaf missing values attributes tests leaf 
furthermore leaf outputs list classes expected frequency test examples possibly leaf test examples certainly leaf test examples 
instance rst leaf covers examples white resistance 
expected number test examples leaf available users aber ac uk ajc phenotype real numbers examples missing values belong certain probability leaf 
white resistance size 
size 

size 
fig 

clustering tree grown phenotype data 
test examples certain belong leaf 
information class compound metabolism indicates examples sorted leaf belong class 
examples certainly belong leaf belong class 
test set belongs class random examples test set chance class 
rule white resistance gene function validated high signi cance 
examples guaranteed belong leaf see examples may may belong leaf weight increased percentage belongs class compared 
accuracy con dence rule comparable rules :10.1.1.20.3909
rule gene resistant white class cell wall cell envelope 
single condition rule corresponds condition top node tree 
rule covers average examples resampling strategy examples class compared data set :10.1.1.20.3909
note predicting classes single level hierarchy dicult see condition really related class superclass subclasses tree hand gives information classes di erent levels hierarchy 
statistics class compared subclasses see level deviations truly occur 
instance looking rst leaf deviation class certainly covered examples belong entirely attributed class certainly covered examples belong deviation attributed single subclasses cover example 
current implementation left user perform comparisons easily automated 
related authors generalised decision trees multi classi cation setting see suzuki decision trees functional genomics application clare king multi classi cation upgrade :10.1.1.20.3909
cases notion class entropy extended cover cation 
main di erence approach clustering approach naturally allows take account hierarchical information 
hierarchical approaches area text classi cation include approach association rules builds separate predictive model node :10.1.1.50.3353
aware existing approaches learn single model context hierarchical multi class 
prediction structural information extent related case reasoning seen extension instance learning structural prediction setting see :10.1.1.50.3353
structure prediction includes ramon de raedt instance function learning 
distance measure propose possible mentioned distance measures structural values proposed see 
discussed task hierarchical multi classi cation extends multi classi cation hierarchical classi cation believe cover interesting range applications 
algorithm extends decision tree approach hierarchical multi classi cation 
main assumption approach hierarchy natural distance elements hierarchy de ned 
approach special case general distance approach multi classi cation 
experimentally validated approach di erent data sets area document classi cation area functional genomics 
data sets turn somewhat limited respect information may give rst simple hierarchy contains instances multiple classes relatively examples missing values compared complexity hierarchy 
experiments give indication usefulness approach especially functional genomics data set interpretation results suggests promising approach clear experiments needed 
obviously include thorough evaluation approach variety test sets evaluation distances application domains ones relatively ad hoc 
expect continue focus attention decision tree induction approach extended rule learners 
obvious direction research incorporating proposed kernel support vector machine see extent performance svm text classi cation improve hierarchical information 
hierarchical multi classi cation points possible direction research regarding general problem prediction sets values distance measure individual values de ned generally cost certain types errors 
jan research assistant hendrik blockeel post doctoral fellow fund scienti research flanders fwo 
jan ramon supported flemish institute advancement scienti technological research industry iwt 
research supported research fund leuven 

bakker heskes 
task clustering learning learn 
kr de rijke schreiber van someren editors proceedings th belgium netherlands conference arti cial intelligence pages amsterdam 

bishop 
neural networks pattern recognition 
university press oxford 

blockeel de raedt 
top induction rst order logical decision trees 
arti cial intelligence june 

blockeel de raedt ramon 
top induction clustering trees 
proceedings th international conference machine learning pages 

blockeel zeroski simultaneous prediction multiple chemical parameters river water quality tilde 
proceedings rd european conference principles data mining knowledge discovery volume lecture notes arti cial intelligence pages 
springer 

breiman friedman olshen stone 
classi cation regression trees 
wadsworth belmont 

caruana 
multitask learning 
machine learning 

clare king 
knowledge discovery multi label phenotype data 
de raedt siebes editors th european conference principles data mining knowledge discovery pkdd volume lecture notes arti cial intelligence pages 
springer verlag 

hutchinson 
metrics terms clauses 
proceedings th european conference machine learning lecture notes arti cial intelligence pages 
springer verlag 

joachims 
probabilistic analysis rocchio algorithm tfidf text categorization 
proceedings icml th international conference learning 

koller sahami 
hierarchically classifying documents words 
proceedings icml th international conference machine learning pages nashville tn usa 
morgan kaufmann 

langley 
elements machine learning 
morgan kaufmann 

andrew mccallum 
bow toolkit statistical language modeling text retrieval classi cation clustering 
www cs cmu edu mccallum bow 

mitchell 
machine learning 
mcgraw hill 


cheng 
distance herbrand interpretations measure approximations target concept 
proceedings seventh international workshop inductive logic programming lecture notes arti cial intelligence 
springer verlag 

pavel robert duin 
generalized kernel approach dissimilarity classi cation 
journal arti cial intelligence research december 

quinlan 
induction decision trees 
machine learning 

quinlan 
programs machine learning 
morgan kaufmann series machine learning 
morgan kaufmann 

ramon bruynooghe 
polynomial time computable metric point sets 
acta informatica 

ramon bruynooghe van laer 
distance measures atoms 
proceedings area meeting computational logic machine learning pages 

ramon de raedt 
instance function learning 
proceedings ninth international workshop inductive logic programming lecture notes arti cial intelligence pages 
springer verlag 

suzuki gotoh 
decision trees multi objective classi cation 
de raedt siebes editors principles data mining knowledge discovery proceedings th european conference volume lecture notes arti cial intelligence pages 
springer verlag 

todorovski blockeel zeroski 
ranking predictive clustering trees 
proceedings th european conference machine learning volume lecture notes arti cial intelligence pages 
springerverlag 

ke wang zhou chen 
building hierarchical classi ers class proximity 
atkinson orlowska valduriez zdonik brodie editors vldb proceedings th international conference large data bases september edinburgh scotland uk pages 
morgan kaufmann 
