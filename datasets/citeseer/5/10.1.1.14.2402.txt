learning options reinforcement learning martin precup school computer science mcgill university montreal qc canada cs mcgill ca www home page www cs mcgill ca 
temporally extended actions macro actions proven useful speeding learning ensuring robustness building prior knowledge ai systems 
options framework precup sutton precup singh provides natural way incorporating actions reinforcement learning systems leaves open issue options identified 
empirically explore simple approach creating options 
underlying assumption agent asked perform different goal achievement tasks environment time 
approach intuition bottleneck states states frequently visited system trajectories prove useful subgoals mcgovern barto iba 
empirical studies approach gridworld navigation tasks 
environments explored contains bottleneck states algorithm finds states expected 
second environment empty gridworld obstacles 
environment contain bottleneck states approach finds useful options essentially allow agent travel environment quickly 
temporally extended actions macro actions proven useful speeding learning planning ensuring robustness building prior knowledge ai systems fikes hart nilsson newell simon sacerdoti korf laird minton iba 
topic explored context markov decision processes mdps reinforcement learning rl singh parr dietterich precup 
options framework precup sutton precup singh provides natural way incorporating extended actions reinforcement learning systems 
option specified set states option initiated internal policy termination condition 
initiation set termination condition specified traditional reinforcement learning methods learn internal policy option 
interesting issue left open options framework automatically finding initiation sets termination conditions options 
empirically explore simple approach creating options 
underlying assumption agent asked perform different goal achievement tasks environment time 
approach intuition bottleneck states states frequently visited system trajectories prove useful subgoals mcgovern barto iba 
agent allowed explore environment gather statistics chooses potential subgoals initiation states options 
second phase agent learns internal policies options 
learned agent options solve goal achievement tasks 
empirical studies approach gridworld navigation tasks 
environments explored contains bottleneck states algorithm finds states expected 
second environment empty gridworld obstacles 
environment contain bottleneck states approach finds useful options essentially allow agent travel environment quickly 
organized follows 
section introduce basic reinforcement learning notation 
section contains definition options smdp learning main algorithm learning choose options 
section describe algorithm automatically creating options 
section contains empirical results algorithm gridworld tasks discussion behavior observed 
reinforcement learning reinforcement learning rl computational approach automating goal directed learning decision making sutton barto 
encompasses broad range methods determining optimal ways behaving complex uncertain stochastic environments 
current rl research theoretical framework markov decision processes mdps puterman 
mdps standard general formalism studying stochastic sequential decision problems 
framework agent takes sequence primitive actions paced discrete fixed time scale 
time step agent observes state environment st contained finite discrete set chooses action finite action set possibly dependent st 
time step agent receives reward rt environment tran sitions st state 
state action expected value immediate reward ra transition new state probability pa regardless ss path taken agent state way behaving policy defined probability distribution picking actions state goal agent find policy maximizes total reward received time 
policy state value action state policy denoted expected discounted reward starting henceforth def rt rt st optimal action value function def max mdp exists unique optimal value function optimal policy corresponding value function iff popular reinforcement learning algorithms aim compute implicitly observed interaction agent environment 
widely learning watkins allows learning directly interaction environment 
agent updates value function follows st st rt max st st learning shown converge limit probability optimal value function standard stochastic approximation assumptions 
options options precup sutton precup singh generalization primitive actions include temporally extended courses action 
options consist components policy termination condition input set option available state option taken actions selected option terminates stochastically 
state st action selected probability distribution st environment transition state st option terminates probability st continues determining st possibly terminating st st 
option terminates agent opportunity select option 
note primitive actions viewed special case options 
action corresponds option available available selects 
consider agent choice time entirely lasts exactly step options persist single time step temporally extended 
natural generalize action value function option value function 
define value option state policy 
rt rt rt denotes policy follows terminates initiates resulting state denotes execution state starting time options closely related actions special kind decision problem known semi markov decision process smdp see puterman 
fact mdp fixed set options smdp 
accordingly theory smdps provides important basis theory options 
problem finding optimal policy set options addressed learning methods 
mdp augmented options smdp apply smdp learning methods developed bradtke duff parr russell mahadevan mcgovern sutton fagg 
execution option started state jump state terminates 
experience option value function updated 
example smdp version step learning bradtke duff updates value function option termination max denotes number time steps denotes cumulative discounted reward time implicit step size parameter may depend arbitrarily states option time steps 
estimate con optimal value function options conditions similar conventional learning parr 
finding options initiation set termination condition specified internal policies options learned standard rl methods learning precup 
key issue remains select termination conditions initiation sets 
intuitively clear different heuristics different kinds environments 
assume agent confronted goal achievement tasks take place fixed environment 
setup quite natural thinking human activities 
instance day cooking different breakfast kitchen layout day day 
allow agent explore environment ahead time order learn options 
option finding process posing series random tasks environment letting agent solve 
time agent gathers statistics regarding frequency occurence different states 
algorithm intuition states occur frequently trajectories represent solutions random tasks states may important 
ill learn options states targets 
options finding algorithm described detail 

select random number start states target states generate random tasks agent 

pair perform episodes learning learn policy going perform episodes greedy policy learned 
states count total number times state visited trajectories 

repeat desired number options reached pick state tmax target state option compute tmax number times state occurs paths go tmax compute tmax tmax select states tmax tmax part initiation set option 
complete initiation set interpolating selected states 
interpolation process domain specific 
option learn internal policy achieved giving high reward entering tmax rewards 
agent performs learning performing episodes start random states tmax reached agent exists options smdp learning described previous section order learn policy options 
note approach quite similar spirit mcgovern barto 
data mining techniques particular diverse density learn bad trajectories 
treat trajectories way 
approach probably simplest implementation bottleneck state idea 
experimental results experimented algorithm finding options simple gridworld navigation tasks 
rooms environment depicted left panel 
state current cell position 
deterministic primitive actions move agent left right 
agent attempts move wall stays position penalty incurred 
discount factor intermediate rewards 
agent obtain reward entering designated goal state 
assume goal state may move location time 
hallway states obvious bottleneck states environment trajectories passing room pass hallways 
expected option finding context finding options going hallways 
room hallways primitive actions left right fig 

rooms environment example option episode length performed independent runs algorithm 
run option finding algorithm find options 
option finding stage states potential start goal states random tasks 
episodes learn values pair start goal states 
learning episodes generate state visitation counts 
initiation sets constructed creating rectangular box selected initiation states option 
approach specific gridworld navigation expected domains 
initiation sets targets options selected internal policy option learning trials 
example option algorithm depicted right panel 
target state option close hallways exactly hallway 
behavior consistent runs due fact states near hallways traversed trajectories going rooms trajectories inside room 
behavior consistent earlier findings mcgovern 
shows option policy learned initiation states states suboptimal policy visited random policy 
primitives options primitives episodes fig 

results rooms environment episode length primitives primitives options episodes options learned compared performance learning agent primitive actions performance agent primitive actions options 
agents learning rate greedy policy generating behavior picked different goal locations 
agents trained episodes fixed goal location 
episode consists starting randomly chosen location performing navigation actions goal reached 
training episodes performed test episodes starting random states greedy policy choose actions 
episodes goal moved values agent retained process repeated 
results rooms environment depicted 
left graph shows performance training right graph shows performance greedy episode length policy learned 
expected temporally extended options provides advantage primitive actions number actions learn increased 
preliminary results indicate advantage larger efficient learning method intra option learning 
second experimented empty gridworld obstacles 
bottleneck states 
obvious option finding heuristic 
expectation options slow learning 
primitives options primitives episodes episode length primitives primitives options episodes fig 

results empty gridworld environment 
left panel shows performance training right panel shows performance greedy policy learned 
applied algorithm environment manner described 
results depicted 
training clear temporally extended options helpful 
result clear greedy policy 
order understand better behavior algorithm looked options environment 
run algorithm couple options leading central states environment 
state visitation counts decreased eliminating trajectories went center states peak counts spread states different areas grid 
virtually run algorithm options navigating different parts grid 
course options useful case goal state moves environment 
example goal states depicted 
goals numbered order selected 
lot directions explored option creation 
currently investigating sophisticated learning execution methods early termination option finding heuristics 
note fig 

option target states run empty gridworld 
algorithm currently assumption tabular representation value functions possible 
looking way lifting assumption dietterich 

hierarchical reinforcement learning maxq value function decomposition 
seventeenth international conference machine learning icml 
fikes hart nilsson 
learning executing generalized robot plans 
artificial intelligence vol 
pp 
iba 

heuristic approach discovery macro operators 
machine learning korf 

macro operators weak method learning 
artificial intelligence laird rosenbloom newell 

chunking soar anatomy general learning mechanism 
machine learning mcgovern sutton 

macro actions reinforcement learning empirical analysis 
technical report university massachusetts department computer science 
mcgovern barto 

automatic discovery subgoals reinforcement learning diverse 
proceedings eighteenth conference machine learning icml pp 

parr russell 

reinforcement learning hierarchies machines 
nips 
sutton precup singh 

mdps semi mdps framework temporal abstraction reinforcement learning 
artificial intelligence 
