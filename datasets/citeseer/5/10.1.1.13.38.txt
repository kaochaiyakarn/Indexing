pattern recognition global means clustering algorithm nikos vlassis 
verbeek www elsevier com locate department computer science university ioannina ioannina greece computer science institute university amsterdam kruislaan sj amsterdam netherlands received march accepted march global means algorithm incremental approach clustering dynamically adds cluster center time deterministic global search procedure consisting size data set executions means algorithm suitable initial positions 
propose modi cations method reduce computational load signi cantly ecting solution quality 
proposed clustering methods tested known data sets compare favorably means algorithm random restarts 
pattern recognition society 
published elsevier science rights reserved 
keywords clustering means algorithm global optimization trees data mining 
fundamental problem frequently arises great variety elds pattern recognition image processing machine learning statistics clustering problem 
basic form clustering problem de ned problem nding homogeneous groups data points data set 
groups called cluster de ned region density objects locally higher regions 
simplest form clustering partitional clustering aims partitioning data set disjoint subsets clusters speci clustering criteria optimized 
widely criterion clustering error criterion point computes squared distance corresponding cluster center takes sum distances points data set 
popular clustering method minimizes clustering error means algorithm 
means algorithm local search procedure known su ers serious drawback performance heavily depends corresponding 
author 
tel fax mail address cs gr 
initial starting conditions 
treat problem techniques developed stochastic global optimization methods simulated annealing genetic algorithms 
noted techniques gained wide acceptance practical applications clustering method means algorithm multiple restarts 
propose global means clustering algorithm constitutes deterministic ective global clustering algorithm minimization clustering error employs means algorithm local search procedure 
algorithm proceeds incremental way solve clustering problem clusters intermediate problems clusters sequentially solved 
basic idea underlying proposed method optimal solution clustering problem clusters obtained series local searches means algorithm 
local search cluster centers initially placed optimal positions corresponding clustering problem clusters 
remaining mth cluster center initially placed positions data space 
optimal solution known iteratively apply procedure nd optimal solutions clustering problems addition effectiveness method deterministic depend pattern recognition society 
published elsevier science rights reserved 
pii pattern recognition initial conditions empirically adjustable parameters 
signi cant advantages clustering approaches mentioned 
section start formal de nition clustering error brief description means algorithm describe proposed global means algorithm 
section describes modi cations basic method require computation expense slightly ective 
section provides experimental results comparisons means algorithm multiple restarts 
section provides describes directions research 

global means algorithm suppose data set xn xn clustering problem aims partitioning data set disjoint subsets clusters cm clustering criterion optimized 
widely clustering criterion sum squared euclidean distances data point xi centroid mk cluster center subset ck contains xi 
criterion called clustering error depends cluster centers mm mm xi ck xi mk ifx true 
means algorithm nds locally optimal solutions respect clustering error 
fast iterative algorithm clustering applications 
point clustering method starts cluster centers initially placed arbitrary positions proceeds moving step cluster centers order minimize clustering error 
main disadvantage method lies sensitivity initial positions cluster centers 
order obtain near optimal solutions means algorithm runs scheduled di ering initial positions cluster centers 
global means clustering algorithm proposed constitutes deterministic global optimization method depend initial parameter values employs means algorithm local search procedure 
randomly selecting initial values cluster centers case global clustering algorithms proposed technique proceeds incremental way attempting optimally add new cluster center stage 
speci cally solve clustering problem clusters method proceeds follows 
start cluster nd optimal position corresponds centroid data set order solve problem clusters perform executions means algorithm initial positions cluster centers rst cluster center placed optimal position problem second center execution placed position data point xn 
best solution obtained executions means algorithm considered solution clustering problem general denote nal solution clustering problem 
solution clustering problem try nd solution clustering problem follows perform runs means algorithm clusters run starts initial state xn 
best solution obtained runs considered solution clustering problem 
proceeding fashion nally obtain solution clusters having solutions clustering problems characteristic advantageous applications aim discover correct number clusters 
achieve solve clustering problem various numbers clusters employ appropriate criteria selecting suitable value 
case proposed method directly provides clustering solutions intermediate values requiring additional computational ort 
may concern computational complexity method requires executions means algorithm value 
depending available resources values algorithm may attractive approach experimental results indicate performance method excellent 
show modi cations applied order reduce computational load 
rationale proposed method assumption optimal clustering solution clusters obtained local search means starting initial state centers placed optimal positions clustering problem remaining kth center placed appropriate position discovered 
assumption natural expect solution clustering problem reachable local search solution clustering problem additional center placed appropriate position data set 
reasonable restrict set possible initial positions kth center set available data points 
noted computational heavy assumption options examining fewer initial positions may considered 
assumptions veri ed experimentally experiments values solution obtained proposed method obtained numerous random restarts means algorithm 
spirit cautiously state proposed method experimentally optimal di cult prove theoretically 

speeding execution general idea global means algorithm heuristics devised reduce computational load signi cantly ecting quality solution 
subsections modi cations proposed referring di erent aspect method 

fast global means algorithm fast global means algorithm constitutes straightforward method accelerate global means algorithm 
di erence lies way solution clustering problem obtained solution clustering problem 
initial states xn execute means algorithm convergence obtain nal clustering error en 
compute upper bound en bn resulting error en possible allocation positions xn error clustering problem bn de ned eq 

initialize position new cluster center point xi minimizes en equivalently maximizes bn execute means algorithm obtain solution clusters 
formally bn max xn xj arg max bn squared distance xj closest center cluster centers obtained far center cluster xj belongs 
quantity bn measures guaranteed reduction error measure obtained inserting new cluster center position xn 
suppose solution clustering problem new cluster center added location xn 
new center allocate points xj squared distance xn smaller distance previously closest center 
data point xj clustering error decrease xn xj summation data points xj provides quantity bn speci insertion location xn 
means algorithm guaranteed decrease clustering error step bn upper bounds error measure obtained run algorithm convergence inserting new center xn error measure global means algorithm 
experimental results see section suggest data point minimizes bound leads results pattern recognition provided global means algorithm 
cluster insertion procedure ciently implemented storing matrix pairwise squared distances points algorithm starts matrix directly computing upper bounds 
similar trick related problems greedy mixture density estimation em algorithm principal curve tting 
may apply method global means algorithm consider data point xn possible insertion position new center smaller set appropriately selected insertion positions 
fast sensible choice selecting set positions dimensional trees discussed 

initialization trees tree multi dimensional generalization standard dimensional binary search tree facilitates storage search data sets 
tree recursive partitioning data space disjoint subspaces 
node tree de nes subspace original data space consequently subset containing data points residing subspace 
nonterminal node successors associated subspaces obtained partitioning parent space cutting hyperplane 
tree structure originally speeding distance search operations nearest neighbors queries range queries case variation original tree proposed ref 

cutting hyperplane de ned plane perpendicular direction principal component data points corresponding node algorithm regarded method nested recursive principal component analysis data set 
recursion usually terminates terminal node called bucket created containing prespeci ed number points called bucket size prespeci ed number buckets created 
turns algorithm nearest neighbor queries merely construction tree provides preliminary clustering data set 
idea bucket centers fewer data points possible insertion locations algorithms previously 
fig 
average performance results shown data sets consisting data points drawn mixture gaussian components 
components gaussian mixture separated exhibit limited eccentricity 
compare results methods clustering problem centers dashed line depicts results fast global means algorithm data points constituting potential insertion locations 
average clustering error data sets pattern recognition clustering error number buckets fig 

performance results data drawn gaussian mixture components 
standard deviation 
ii solid line depicts results standard means algorithm run data set conducted 
run cluster centers initially positioned centroids buckets obtained application tree algorithm buckets created 
average clustering error data sets standard deviation 
iii solid line error bars indicating standard deviation mean value shows results fast global means algorithm potential insertion locations constrained centroids buckets tree 
horizontal axis vary number buckets tree method 
computed theoretical clustering error data set error computed true cluster centers 
average error value data sets standard deviation 
results close results standard fast global means include gure 
conclude experiment fast global means approach gives rise performance signi cantly better starting centers time initialized tree method restricting insertion locations fast global means tree data points signi cantly degrade performance consider large number buckets tree general larger number clusters 
obviously possible employ tree approach global means algorithm 

experimental results tested proposed clustering algorithms known data sets iris data set synthetic data set image segmentation data set 
data sets conducted experiments clustering problems obtained considering feature vectors ignoring class labels 
iris data set contains dimensional data points synthetic data set dimensional data points image segmentation data set consider dimensional data points obtained pca original dimensional data points 
quality obtained solutions evaluated terms values nal clustering error 
data set conducted experiments run global means algorithm 
run fast global means algorithm 
means algorithm 
value means algorithm executed times number data points starting random initial positions centers computed minimum average clustering error standard deviation 
data sets experimental results displayed figs 
respectively 
gure plot displays clustering error value function number clusters 
clear global means algorithm clustering error clustering error pattern recognition number clusters fig 

performance results iris data set 
global means fast global means means min means number clusters global means fast global means means min means fig 

performance results synthetic data set 
pattern recognition clustering error ective providing cases solutions equal better quality respect means algorithm 
terms fast version algorithm encouraging executing signi cantly faster provides solutions excellent quality comparable obtained original method 
constitutes cient algorithm terms solution quality computational complexity run faster trees employed explained previous section 
subsections provide extensive experimental results comparing fast versions algorithm conventional means algorithm random initialization 

texture segmentation clustering experiment objective cluster pixel image patches extracted set brodatz texture images 
complete texture image consists pixels patches texture extracted randomly selecting windows 
expected patches originating texture image form individual cluster clusters 
number textures randomly constructed data sets 
data set created rst randomly selecting textures selecting patches texture resulting patches data set 
reported results averages data sets 
compared performance algorithms means initialized uniformly selected random number clusters global means fast global means means min means fig 

performance results image segmentation data set 
fig 

results texture segmentation problem clusters textures 
subset data fast global means fast global means insertion locations limited top nodes corresponding tree 
evaluate different methods considered mean squared clustering error mse patches closest mean 
rst experiment number clusters considered equal number textures selected create data set 
results fig 
provide corresponding execution times seconds 
bold values tables indicate method best performance terms clustering error 
conducted second series experiments twice clusters textures 
results displayed fig 

observed average means algorithm random initialization gives worst results number clusters textures 
interesting note fast global means uses fig 

results texture segmentation problem twice clusters textures 
top nodes tree insertion candidates faster generic fast global means algorithm provides slightly better results terms clustering error 

arti cial data subsection provide extensive comparative experimental results arti cially created data sets 
purpose compare randomly initialized means algorithm fast global means algorithm uses top nodes corresponding tree candidate insertion locations 
data drawn randomly generated gaussian mixtures varied number sources mixture components dimensionality data space separation sources mixture 
ref 
separation gaussian mixture satis es max trace ci trace cj denote respectively means covariance matrices mixture components 
experiments considered mixtures having separation range values correspond exclusively weakly separated clusters 
number data pattern recognition points data set number sources 
example data sets shown fig 

considered problems corresponding combination values 
problem data sets created 
results displayed figs 

data set greedy fast global means tree initialization algorithm applied rst 
randomly initialized means algorithm applied times possible run time greedy algorithm 
evaluate quality solutions speci data set rst mean clustering error computed runs performed means algorithm 
minimum clustering error value corresponding runs means algorithm provided min columns min standard deviation column 
gr column provides clustering error egr obtained fast global means algorithm form egr 
trials column indicates runs means algorithm allowed run time greedy algorithm 
noted row table displays averaged results data sets constructed speci values bold values tables indicate clustering error greedy algorithm minimum error achieved runs means algorithm smallest 
clear experiments bene greedy method larger clusters separation larger dimensionality gets smaller 
striking cases greedy algorithm gives better results 
cases greedy method superior methods yield results relatively close average means result 
thing note clusters created data sets poorly separable 
interesting note number trials allowed random means algorithm grows slowly number clusters increases 
top bottom number clusters increased factor fig 

example dimensional data sets sources separation left right 
pattern recognition number trials doubled average 
inspection number trials function indicates dependence number clusters 
fig 
indicates average number trials corresponding variance increase number clusters increases 
fast global means tree building algorithms downloaded carol wins uva nl software 
fig 

experimental results arti cial data sets 
discussion global means clustering algorithm constitutes deterministic clustering method providing excellent results terms clustering error criterion 
method independent starting conditions compares favorably means algorithm multiple random restarts 
deterministic nature method particularly important cases fig 

number allowed trials randomly initialized means function number clusters 
pattern recognition fig 

experimental results arti cial data sets 
clustering method specify initial parameter values methods example rbf training constitutes module complex system 
case certain employment global means fast variants provide sensible clustering solutions 
evaluate complex system adjust critical system parameters having worry dependence system performance clustering method employed 
pattern recognition advantage proposed technique order solve clustering problem intermediate clustering problems solved may prove useful applications seek actual number clusters clustering problem solved values developed fast global means algorithm signi cantly reduces required computational ort time providing solutions quality 
proposed modi cations method reduce computational load signi cantly ecting solution quality 
methods employed nd solutions clustering problems thousands high dimensional points primary aims test techniques large scale data mining problems 
direction related parallel processing accelerating proposed methods executions means algorithm independent performed parallel 
research direction concerns application proposed method types clustering example fuzzy clustering topographic methods som 
important issue deserves study related possible development theoretical foundations assumptions method 
possible employ global means algorithm method providing ective initial parameter values rbf networks data modeling problems gaussian mixture models compare ectiveness obtained solutions training techniques gaussian mixture models 

summary global means algorithm incremental approach clustering dynamically adds cluster center time deterministic global search procedure consisting size data set executions means algorithm suitable initial positions 
basic idea underlying proposed method optimal solution clustering problem clusters obtained series local searches means algorithm 
local search cluster centers initially placed optimal positions corresponding clustering problem clusters 
remaining mth cluster center initially placed positions data space 
optimal solution known iteratively apply procedure nd optimal solutions clustering problems proposed method deterministic depend initial positions cluster center contain empirically adjustable parameters eliminating problems characterizing means algorithm stochastic extensions 
addition modi cations method reduce computational load signi cantly ecting solution quality 
rst modi cation called fast global means algorithm de nes fast computed bound clustering error local searches 
second modi cation related partitioning data space tree structure order reduce number examined insertion positions new cluster center 
proposed clustering methods tested known data sets compare favorably means algorithm random restarts 
addition modi ed versions lead implementations fast exhibit equal performance 
vlassis verbeek supported dutch technology foundation stw project 
murty jain flynn data clustering review acm comput 
surv 

lozano pena empirical comparison initialization methods means algorithm pattern recognition lett 

milligan cooper examination procedures determining number clusters data set psychometrika 
vlassis greedy em algorithm gaussian mixture learning neural process 
lett 

verbeek vlassis segments algorithm nd principal curves technical report computer science institute university amsterdam netherlands november ias uva 
bentley multidimensional binary search trees associative searching commun 
acm 
sproull re nements nearest neighbor searching dimensional trees algorithmica 
blake merz uci repository machine learning databases university california irvine department information computer sciences 
ripley pattern recognition neural networks cambridge university press cambridge uk 
brodatz textures photographic album artists designers dover new york usa 
dasgupta learning mixtures gaussians proceedings ieee symposium foundations computer science new york october 
mclachlan peel finite mixture models wiley new york 
vlassis kurtosis dynamic approach gaussian mixture modeling ieee trans 
syst 
man cybern part 
pattern recognition author received diploma degree electrical engineering ph degree electrical computer engineering national technical university athens 
department computer science university ioannina greece currently assistant professor 
research interests include neural networks pattern recognition machine learning automated diagnosis 
author nikos vlassis received sc 
degree electrical computer engineering ph degree arti cial intelligence national technical university athens greece 
joined intelligent autonomous systems group university amsterdam netherlands holding assistant professor position 
visiting researcher electrotechnical laboratory currently aist japan scholarship japan industrial technology association 
holds dimitris foundation prize switzerland young researchers engineering technology 
research focusses probabilistic statistical learning methods intelligent systems 
author jakob verbeek received cum laude masters degree arti cial intelligence cum laude master logic degree university amsterdam netherlands 
september ph student advanced school computing imaging working intelligent autonomous systems group university amsterdam 
current research activities take place project nonlinear feature extraction 
research interests include principal curves surfaces nonparametric density estimation machine learning general 
