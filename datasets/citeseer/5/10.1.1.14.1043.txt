machine learning fl kluwer academic publishers boston 
manufactured netherlands 
learning classify text labeled unlabeled documents kamal nigam cs cmu edu andrew mccallum zy mccallum com sebastian thrun thrun cs cmu edu tom mitchell mitchell cs cmu edu school computer science carnegie mellon university pittsburgh pa just research henry street pittsburgh pa editor 
shows accuracy learned text classifiers improved augmenting small number labeled training documents large pool unlabeled documents 
significant important text classification problems obtaining classification labels expensive large quantities unlabeled documents readily available 
theoretical argument showing common assumptions unlabeled data contain information target function 
introduce algorithm learning labeled unlabeled text combination expectation maximization naive bayes classifier 
algorithm trains classifier available labeled documents probabilistically labels unlabeled documents 
trains new classifier labels documents iterates 
experimental results obtained text different real world tasks show unlabeled data reduces classification error 
keywords text classification expectation maximization combining supervised unsupervised learning bayesian learning 
consider problem training computer automatically classify text documents 
growing volume online text available world wide web internet news feeds electronic mail digital libraries problem great practical significance 
statistical text learning algorithms trained approximately classify documents sufficient set labeled training examples 
text classification algorithms automatically catalog news articles web pages automatically learn reading interests users automatically sort electronic mail :10.1.1.22.6286:10.1.1.35.6633:10.1.1.16.3103
key difficulty current algorithms issue addressed require large prohibitive number labeled training examples learn accurately 
take example task learning newsgroup articles interest person reading usenet news examined lang 
reading classifying articles precision learned classifier top documents ranked classifier 
practical user filtering system obviously prefer learning algorithms nigam mccallum thrun mitchell provide accurate classifications hand labeling dozen articles thousands 
describe algorithm learns classify text documents accurately unlabeled documents augment available labeled training examples 
example labeled documents just articles read judged user interesting 
learning algorithm vast multitude unlabeled articles available usenet augment labeled examples 
text domains especially involving online sources collecting unlabeled examples trivial labeling expensive 
experimental results showing unlabeled data boost learning accuracy text classification domains newsgroup articles web pages newswire articles 
example identify source newsgroup usenet article classification accuracy algorithm takes advantage unlabeled examples requires labeled examples hand traditional learner requires labeled examples achieve accuracy 
task technique reduces need labeled training examples factor 
unlabeled examples boost learning accuracy 
brief provide information joint probability distribution words documents 
suppose example labeled data determine documents containing word learn tend belong positive class 
fact estimate classification unlabeled documents find word teach occurs frequently unlabeled examples believed belong positive class 
occurrence words learn teach large set unlabeled training data provide useful information construct accurate classifier considers learn teach indicators positive examples 
specific approach describe combination wellknown learning algorithms naive bayes classifier expectation maximization em algorithm 
naive bayes algorithm class statistical text classifiers uses word frequencies features 
examples include tfidf rocchio regression models nearest neighbor support vector machines 
em class iterative algorithms maximum likelihood estimation problems incomplete data 
result combining algorithm extends conventional text learning algorithms em dynamically derive pseudo labels unlabeled documents learning providing way incorporate unlabeled data supervised learning 
previous supervised algorithms learning classify text incorporate unlabeled data 
similar approach miller uyar non text data sources 
adapt approach naive bayes text classifier conduct thorough empirical analysis 
show theoretically unlabeled data carries information useful improving parameter estimation certain restrictive conditions survey results show consequently improves classification 
learning classify text labeled unlabeled documents sections provide sufficient conditions unlabeled data expected improve classification accuracy 
probabilistic setting naive bayes combination expectation maximization algorithm 
empirically demonstrate significantly improved performance text data sets 
discuss directions research 
argue results significant practical importance text learning domains web unlabeled data available free labeling data truly expensive 

probabilistic framework ground theoretical aspects provide setting algorithm section presents probabilistic framework characterizing nature documents classifiers 
introduce classifier show unlabeled data improve classification 
framework follows commonly assumptions data text produced mixture model correspondence mixture components classes :10.1.1.144.7475
setting document generated probability distribution mixture model parameterized 
mixture model consists mixture components fc jcj component parameterized disjoint subset 
document created selecting component priors second having mixture component generate document parameters distribution jc 
characterize likelihood document sum total probability mixture components jcj jc document class label 
assume correspondence classes mixture model components indicate jth mixture component jth class 
class label document written document generated mixture component say class label may may known document 

proof value unlabeled data section show setting documents unknown class labels useful learning concept classes 
learning concept classes setting equivalent estimating parameters unknown mixture model produced training documents 
provide sufficient condition unlabeled documents estimate improve classification accuracy 
argue condition fulfilled context nigam mccallum thrun mitchell high dimensional mixture models 
loss generality assume section classification task binary jcj 
unlabeled data carry information parameters sufficient 
learning task degenerate jc jc random variable documents random variable classes events 
jd jd number labeled training documents 
assumption excludes tasks learning impossible training data class labels reason parameterizations yield equivalent results 
case knowledge parameters help aid prediction data 
second assumption excludes cases labeled data available case unlabeled data improve classification accuracy 
excludes cases labeled data sufficient estimate parameters 
easy show high dimensional mixture models unknown parameters meet condition non degenerate 
show knowledge unlabeled documents carries information parameters need demonstrate conditional dependence jd conjecture holds direct implication unlabeled data contain information parameters 
way unlabeled data help supervised learning 
provide proof contradiction 
temporarily assume independent 
negating equation applying bayes rule write dj direct equation parameterizations provide class probabilities sample 
substitution equation gives jcj jc jcj jc straightforward exercise construct document parameterizations generate contradiction making non degeneracy learning classify text labeled unlabeled documents assumption 
assumption non degeneracy requires document individual terms equation differ construct total probability document differs assumption conditional independence contradicted parameterizations conditionally dependent documents 
signifies unlabeled data contain information parameters generative model 
knowledge aid classification exclude extreme cases jd jd 
labeled data unlabeled data improve classification shown 
infinite amounts labeled data parameters recovered probability labeled data resulting classifier bayes optimal unlabeled data improve classification accuracy 
note argument immediately motivate algorithm extracting information unlabeled data 
additionally show better parameter estimation yield better classification 
sections describe way guaranteed improve parameter estimates 
section contains survey related ties improvements parameter estimates increases available training data improvements classification 

naive bayes text classification section naive bayes classifier known probabilistic algorithm classifying text special case mixture model 
algorithm forms foundation incorporate unlabeled data 
continue assumptions data generation documents produced mixture model correspondence classes mixture components 
motivational result previous section holds unlabeled documents beneficial 
naive bayes additional assumption probability seeing word document independent context position :10.1.1.144.7475
learning task set training documents order form estimates parameters generative model 
naive bayes forms bayes optimal estimates parameters uses estimated model classify new documents 
generative model describe full generative model documents learning text classifiers 
specialization mixture model section 
document considered ordered list word events 
write ik word position document subscript indicates index vocabulary hw jv document generated mixture component selected document length chosen independently component 
note assumes document length independent class 
selected mixture component generates sequence words specified length 
expand second term equa nigam mccallum thrun mitchell tion express probability document mixture component probability document length product probabilities individual word events sequence 
note general setting probability word event conditioned words precede 
jc hd ijd jd jd ik jc iq standard naive bayes assumption words document generated independently context independently words document class label 
assume probability word independent position document example probability seeing word dog position document seeing position 
express assumptions ik jc iq ik jc combining equations gives complete naive bayes expression probability document class jc jd jd ik jc parameters individual mixture component collection word probabilities jc jc jv jg jc 
assume classes document length uniformly distributed need parameterized 
parameters specified model class priors indicate probabilities selecting different mixture components 
training classifier underlying assumptions data produced task learning text classifier consists forming estimate set data associated class labels 
estimate written 
labeled training documents fd jdj calculate bayes optimal estimates parameters model generated documents 
calculate probability word class jc simply count fraction times word occurs data class augment fraction bayes optimal smoothing primes count word pseudo occurrence 
smoothing referred laplacean prior necessary prevent probability zero probabilities infrequently occurring words 
word probability estimates jc learning classify text labeled unlabeled documents jc jc jdj jd jv jv jdj jd count number times word occurs document jd class label 
class prior probabilities estimated fashion counting smoothing jdj jd jdj estimates parameters calculated training documents possible turn generative model side calculate probability particular mixture component generated document 
formulate application bayes rule substitutions equations 
jd jc jd ik jc jcj jd ik jc note document lengths class independent document length terms equation cancel appear 
task classify test document single class simply select class highest posterior probability arg max jd 
note assumptions generation text documents mixture model correspondence mixture components classes word independence document length distribution violated practice 
documents fall overlapping categories 
words document independent grammar topicality ensure 
despite violations empirically naive bayes classifier job classifying text documents :10.1.1.21.7950:10.1.1.35.6633
paradox explained fact classification estimation function sign binary cases function estimation function approximation poor classification accuracy remains high :10.1.1.144.7475
formulation naive bayes assumes generative model accounts number times word appears document 
equivalent multinomial event model factorial terms account event ordering 
formulation numerous practitioners naive bayes text classification :10.1.1.21.7950:10.1.1.16.3103
formulation naive bayes text classification assumes generative model document representation word vocabulary binary feature modeled bernoulli trial :10.1.1.21.988
empirical comparisons show multinomial formulation yields higher accuracy classifiers 
nigam mccallum thrun mitchell 
em incorporate unlabeled data naive bayes just small set labeled training data classification accuracy suffer variance parameter estimates generative model high 
augmenting small set large set unlabeled data combining sets em improve parameter estimates 
em concurrently generates probabilistically assigned labels unlabeled documents probable model smaller parameter variance predicts probabilistic labels 
section describes em probabilistic framework previous section 
special case general missing values formulation :10.1.1.56.6066
theory em works particularly simple resulting algorithm straightforward 
algorithm outlined table 
set training documents task build classifier form previous section 
previously section assume subset documents come class labels rest documents subset class labels unknown 
disjoint partitioning consider probability training data probability data simply product documents document independent model 
equation probability data dj jd jcj jc theta jd jc unlabeled documents direct application equation 
labeled documents generative component label need sum class components 
learning classifier corresponds calculating maximum likelihood estimate finding parameterization training data arg max jd 
bayes rule jd dj 
constant maximum likelihood estimation assumes constant log define log 
maximizing log likelihood maximizing likelihood 
equation bayes rule write log likelihood jd log jd learning classify text labeled unlabeled documents build initial classifier estimating labeled documents equations 
loop classifier parameters change current classifier probabilistically label unlabeled documents equation 
recalculate classifier parameters probabilistically assigned labels equations 
table 
algorithm 
jd jd log jcj jc jd log jc line equation log sums computable closed form 
knew class labels avoid 
access class labels represented matrix binary indicator variables hz ij iff ij express complete log likelihood parameters jd log sums jd jdj jcj ij log jc formulation log likelihood readily computable closed form 
dempster uses insight expectation maximization algorithm finds local maximum likelihood iterative procedure recomputes expected value maximum likelihood parameterization note labeled documents known 
estimated unlabeled documents 
denote expected value iteration find local maximum jd iterating steps ffl step set zjd 
ffl step set arg max jd 
practice step corresponds calculating probabilistic labels jd document current estimate equation 
corresponds calculating new maximum likelihood estimate current estimates jd equations 
see table outline algorithm 
em finds locally maximizes probability data labeled unlabeled 
nigam mccallum thrun mitchell 
experimental results section give empirical evidence algorithm table labeled unlabeled documents outperforms naive bayes unlabeled documents 
experimental results different text corpora domains usenet news articles newsgroups web pages webkb newswire articles reuters 
datasets protocol newsgroups data set collected ken lang consists articles divided evenly different usenet discussion groups :10.1.1.21.7950
words stoplist common short words removed unique words occur 
categories fall confusable clusters example comp discussion groups discuss religion 
tokenizing data skip usenet headers discarding subject line tokens formed contiguous alphabetic characters left unstemmed 
best performance obtained feature selection normalizing word counts document length 
accuracy results reported averages test train splits documents randomly selected placement test set 
webkb data set contains web pages gathered university computer science departments :10.1.1.35.6633
departments web pages included additionally pages assortment universities 
pages divided categories student faculty staff course project department 
populous non categories student faculty course project containing pages 
stemming stoplist stoplist hurt performance example fourth ranked word information gain excellent indicator student homepage 
done previously informative words measured average mutual information class variable :10.1.1.35.6633:10.1.1.35.6633
feature selection method commonly text :10.1.1.21.7950
accuracy results average test train splits randomly holding documents testing 
modapte train test split reuters distribution data set consists articles topic categories reuters newswire 
studies populous classes build binary classifiers class 
words inside text 
tags including title remove reuter tags occur top bottom document 
stoplist stem 
vocabulary selection performed average mutual information class variable 
standard modapte split documents test set training set 
results reported average results randomly selected training sets 
complete modapte test set calculate precision recall breakeven points standard information retrieval measure binary classification 
learning classify text labeled unlabeled documents number labeled documents unlabeled documents unlabeled documents 
classification accuracy newsgroups data set unlabeled documents 
narrow error bars data point twice standard error 
small amounts training data em yields accurate classifiers 
limit methods converge 
number unlabeled documents number unlabeled documents 
effect varying number unlabeled data 
classification accuracy shown newsgroups data set labeled documents varying amounts unlabeled data 
unlabeled data helps 
experiments em initial estimated labeled data em iterations progressed table 
experiments performed em iterations significant changes occur iterations 
classification accuracy improve eighth iteration 
results shows effect em unlabeled data newsgroups data set 
vertical axis indicates classifier accuracy test sets horizontal axis indicates amount labeled data training 
vary amount labeled training data compare classification accuracy traditional naive bayes unlabeled data em learner access nigam mccallum thrun mitchell table 
lists words predictive course class webkb data set change iterations em specific example 
second iteration em common course related word high weights 
symbol indicates arbitrary digit 
iteration iteration iteration intelligence dd dd dd artificial lecture lecture understanding cc cc dd dd dist dd dd due identical handout rus due homework arrange problem assignment games set handout dartmouth tay set natural hw cognitive exam logic homework problem proving kfoury prolog sec postscript knowledge postscript solution human exam quiz representation solution chapter field ascii unlabeled documents 
em performs significantly better 
example labeled documents documents class naive bayes reaches accuracy em achieves 
note em performs small number labeled documents documents single labeled document class naive bayes gets em 
expected lot labeled data naive bayes learning curve reached plateau learner saturated having unlabeled data help 
hold number labeled documents constant vary number unlabeled documents horizontal axis 
naturally unlabeled data helps 
results demonstrate em finds model probable parameter estimates improved estimates reduce classification accuracy need labeled training examples 
example get classification accuracy em requires labeled examples naive bayes requires labeled examples achieve accuracy 
gain intuition em works detailed trace example 
table provides window evolution classifier course em iterations example 
webkb data set column shows ordered list words model believes predictive course class 
word judged predictive weighted log likelihood ratio 
iteration parameters estimated randomly chosen single learning classify text labeled unlabeled documents number labeled documents unlabeled documents unlabeled documents 
classification accuracy webkb data set unlabeled documents averaged trials data point 
small amounts labeled documents em helps limit degrades performance slightly indicating misfit data assumed generative model 
labeled document class 
notice course document specific artificial intelligence course dartmouth 
em iterations unlabeled documents see em unlabeled data find words generally indicative courses 
classifier corresponding column gets accuracy eighth iteration classifier achieves accuracy 
graph shows benefits unlabeled documents webkb data set 
em improves accuracy significantly especially amount labeled data small 
labeled documents class traditional naive bayes attains accuracy em reaches 
lot labeled data em hurts performance slightly 
labeled documents naive bayes obtains accuracy em gets 
varying weight unlabeled data hypothesize reason em hurts performance data fit assumptions model newsgroups mixture components best explain unlabeled data correspond class labels 
words em places strong assumptions generative process documents optimizes parameters subject assumptions data 
assumptions hold data optimization may longer beneficial classification 
em little labeled training data parameter estimation desperate guidance em helps spite somewhat violated assumptions labeled training data labeled data sufficient parameter estimation estimates modestly thrown em inclusion unlabeled data 
surprising unlabeled data throw parameter estimation considers number unlabeled nigam mccallum thrun mitchell probability mass assigned unlabeled documents labeled documents labeled documents 
effects varying relative importance labeled unlabeled documents webkb data different amounts labeled data 
cross validation automatic selection picks near optimal values ff indicated cross diamond 
note magnified vertical scale 
documents greater number labeled documents versus points largest amounts labeled data great majority probability mass step estimate classifier parameters comes unlabeled data 
insight suggests simple fix 
add learning parameter varies relative contributions labeled unlabeled data parameter estimation step 
implementation parameter embodied factor ff reduces weight unlabeled documents estimation jc equation 
essence unlabeled document count fraction ff document correctly balancing mass labeled unlabeled documents optimize performance 
build models varying values ff choose best leave cross validation training data tune parameter em iterated convergence 
empirically cross validation picks optimal value time near optimal value 
plots classification accuracy varying ff horizontal axis different amounts labeled training data 
bottom curve obtained labeled documents vertical slice point naive bayes em curves cross 
top curve obtained labeled documents slice crossover 
note magnified vertical scale facilitate interpretation data 
second remembering rightmost point corresponds em weighting generate left regular naive bayes note best performing values ff extremes 
paired test indicates maxima statistically significantly higher point 
third note select optimal near optimal values automatically crossvalidation 
plots show performance held test set values ff selected leave cross validation indicated diamond learning classify text labeled unlabeled documents number labeled documents unlabeled documents weight tuning unlabeled documents 
classification accuracy webkb data set ff optimization selected crossvalidation compared unlabeled data 
note accuracy unlabeled data degrade large amounts labeled data maintains large benefits small training sets 
cross bottom top curves respectively 
trend amounts labeled data labeled data unlabeled data gets weight 
compares performance naive bayes em ff tuned cross validation 
fixed ff em strictly dominates naive bayes 
indicates automatically avoid degradation accuracy em preserve benefits seen small training set 
multiple mixture components class faced data fit assumptions model ff tuning approach described addresses problem allowing model incrementally ignore unlabeled data 
direct approach change model naturally fits data 
hypothesized data violates assumption correspondence mixture components classes mixture model components em correspond class labels 
flexibility added mapping mixture components class labels allowing multiple mixture components class expect improve performance data class multi modal 
eye testing hypothesis applied em reuters corpus 
documents data set multiple class labels category traditionally evaluated binary classifier 
negative class covers distinct categories expected task strongly violate assumption data negative class generated single mixture component 
experiments randomly selected positively labeled documents negatively labeled documents unlabeled documents 
uneven labeling justified binary reuters classification tasks negative class frequent positive class 
nigam mccallum thrun mitchell table 
precision recall breakeven points showing performance binary classifiers reuters traditional naive bayes em mixture component class em varying multi component models negative class 
best multi component model noted bold difference performance naive bayes noted rightmost column 
results shown optimal vocabulary size indicated parentheses 
note performance poor single component class em data model fit poor 
natural multi component model negative class em improves naive bayes 
category nb em em em em diff acq corn crude earn grain interest money fx ship trade wheat left column table shows average precision recall breakeven points trials experiment naive bayes 
numbers best vocabulary size task indicated parentheses 
classifiers different categories performed best widely varying vocabulary sizes 
variance optimal vocabulary size unsurprising 
previously noted categories wheat corn known strong correspondence words categories categories acq known subtle class definition :10.1.1.21.7950
categories narrow definitions require small vocabularies best classification broader definition require large vocabulary capture category 
second column table shows results performing em data single negative centroid previous experiments 
expected fit assumed model reuters data poor results em dramatically worse simple naive bayes 
negative class truly multi modal fitting single naive bayes class em data accurately capture distribution 
choosing appropriate multicomponent model run em get results improve naive bayes 
remainder table shows effects different multi component models conjunction em 
negative class modeled negative centroids 
initializing centroids running em initialized randomly assigned negative documents 
best performer noted bold difference naive bayes noted difference column 
paired test trial categories shows improvement average breakeven point statistically significant 
note cases em best components confirming learning classify text labeled unlabeled documents hypothesis complex multi component model accurately represents reuters data 
results indicate correct model selection crucial em data sets naturally modeled small number generative components 
data accurately modeled gains em readily seen 
obvious question select best model representation 
cheeseman stutz investigate clustering tasks labeled data explicitly compare probability data different models select best match prior prefers smaller models 
classification tasks may beneficial select appropriate classification oriented criteria 
consider number components equals number examples data modeled perfectly 
poor generalization 
possibility leave cross validation manner tuning ff 

survey related theoretical previous section provides empirical evidence unlabeled data conjunction labeled data help improve classification accuracy 
built theoretical showing unlabeled data improve parameter estimates 
provide link survey literature convergence error bounds describing degree labeled unlabeled data improve classification 
section assumes high dimensional mixture model naive bayes special case 
recall denote parameters mixture model 
denote number different words induces parameter mixture component 
certain asymptotic cases relative value unlabeled data learning classification understood 
ffl unlabeled data 
consider efficiency estimating pool labeled data estimating maximum likelihood estimator subject error bound gamma djc maximum likelihood estimates 
follows parameter estimation error gamma converges zero rate jd 
likewise classification error converges bayes optimal classifier rate 
ffl infinite unlabeled data 
infinite amounts unlabeled data available parameters mixture components recovered unlabeled data assignment mixture components classes 
estimation problem reduces problem learning permutation matrix assigns labels different mixture components 
nigam mccallum thrun mitchell labeled data permutation parameters known classification error reduced random guessing 
shown infinite unlabeled data classification error approaches bayes optimal solution exponential rate number labeled examples 
infinite amounts unlabeled data available convergence rate learning labeled data changed exponential factor 
ffl trade 
shown labeled data exponentially valuable unlabeled data reducing probability classification error non degenerate bayesian classifiers 
analysis investigates restricted estimation problem individual mixture components known things aren priori likelihood mixture component assignment mixture components class labels 
situation classification error essentially dominated number unlabeled documents number unlabeled data grows faster exponential function number labeled documents case classification error essentially determined number labeled samples 
result assumes parameters individual mixture components known little known general case unlabeled data estimate 
shahshahani landgrebe investigates utility unlabeled data supervised learning quite different results 
analyze convergence rate assumption unbiased estimators available labeled unlabeled data 
bounds fisher information gain show linear exponential value labeled vs unlabeled data 
unfortunately analysis assumes unlabeled data sufficient estimate parameter vectors assume target concept recovered target labels 
assumption unrealistic 
shown unlabeled data improve classification results absence labeled data 
shahshahani landgrebe analysis investigate classification error 
unfortunately results rest restrictive assumptions usually violated text classification domains 
asymptotic characterize importance labeled unlabeled documents limit 
little known non asymptotic case 
second assume underlying mixture model correct exists parameter dj identical distribution generated data generated 
unfortunately assumption violated estimators maximum likelihood estimator may generate poor results 
shown conditions maximum likelihood estimator easily fail minimize classification error training set 
learning classify text labeled unlabeled documents 
related studies em combine labeled unlabeled data classification 
naive bayes shahshahani landgrebe mixture gaussians miller uyar mixtures experts 
demonstrate experimental results non text data sets features 
contrast textual data sets orders magnitude features 
example applying em fill missing values missing values class labels unlabeled training examples 
ghahramani jordan em mixture models fill missing values :10.1.1.56.6066
emphasis missing feature values focus augmenting small complete set labeled data 
autoclass project investigated combination em algorithm underlying model naive bayes classifier 
emphasis research discovery novel clusterings unsupervised learning unlabeled data 
autoclass applied text classification 
approaches reducing need labeled training examples active learning algorithm iteratively selects unlabeled example asks human labeler classification rebuilds classifier 
approaches differ methods selecting unlabeled example request label 
examples relevance sampling uncertainty sampling query committee approach 
statistical text classifiers variety domains naive bayes strong probabilistic foundation em efficient large data sets :10.1.1.11.6124:10.1.1.14.6535
thrust straightforwardly demonstrate value unlabeled data similar approach apply unlabeled data complex classifiers 

summary explored question unlabeled data may supplement scarce labeled data machine learning problems especially learning classify text documents 
important question text learning high cost hand labeling data availability huge volumes unlabeled data 
theoretical model algorithm experimental results show significant improvements unlabeled documents training classifiers real world text classification tasks 
theoretical model characterizes setting unlabeled data boost accuracy learned classifiers probability distribution generates documents described mixture distribution mixture components correspond class labels 
conditions fit exactly model naive bayes classifier 
complexity natural language text soon completely captured statistical models 
interesting consider sensitivity nigam mccallum thrun mitchell classifier model data inconsistent model 
data inconsistent assumptions model method adjusting weight contribution unlabeled data results webkb prevents unlabeled data hurting classification accuracy 
results reuters study ways improve model better matches assumptions mixture models correspondence components classes 
results show improved classification accuracy suggest exploring complex mixture models better correspond textual data distributions 
believe algorithm unlabeled data require closer match data model labeled data intended target concept model differ actual distribution data unlabeled data hurt help 
intend closer theoretical empirical study tradeoffs unlabeled data inherent model inadequacies text learning algorithms 
see interesting directions unlabeled data 
learning task formulations benefit em active learning approach uses explicit model unlabeled data incorporate em iterations stage improve classification better select data request class labels labeler incremental learning algorithm re trains testing phase unlabeled test data received early testing phase order improve performance test data 
problem domains share similarities text domains abundant unlabeled data limited expensive labeled data 
robotics vision information extraction domains 
applying techniques improve classification areas 
acknowledgments larry wasserman extensive help theoretical aspects 
doug baker help formatting reuters data set 
research supported part darpa hpkb program contract 
notes 
assumption relaxed section making correspondence 
relaxes assumption fashion 

previous naive bayes formalizations include document length effect 
general case document length modeled parameterized 

data sets available internet 
see www cs cmu edu www research att com lewis 
learning classify text labeled unlabeled documents 
weighted log likelihood ratio rank words jc log jc understood information theoretic terms word contribution average inefficiency encoding words class code optimal distribution words sum quantity words kullback leibler divergence distribution words distribution words 

castelli cover 
exponential value labeled samples 
pattern recognition letters january 

castelli cover 
relative value labeled unlabeled samples pattern recognition unknown mixing parameter 
ieee transactions information theory november 

peter cheeseman john stutz 
bayesian classification autoclass theory results 
fayyad editor advances knowledge discovery data mining 

cohen singer 
context sensitive learning methods text categorization 
proceedings acm sigir conference 

thomas cover joy thomas 
elements information theory 
john wiley 

craven dipasquo freitag mccallum mitchell nigam slattery 
learning extract symbolic knowledge world wide web 
proceedings aaai 

dempster laird rubin 
maximum likelihood incomplete data em 
algorithm 
journal royal statistical society series 

devroye lugosi 
probabilistic theory pattern recognition 
springer verlag berlin 

domingos pazzani 
independence conditions optimality simple bayesian classifier 
machine learning 

jerome friedman 
bias variance loss curse dimensionality 
data mining knowledge discovery 

zoubin ghahramani michael jordan 
supervised learning incomplete data em approach 
advances neural information processing systems nips 
morgan kauffman publishers 

hanson cheeseman stutz 
bayesian classification theory 
technical report technical report fia nasa ames research center 

thorsten joachims 
probabilistic analysis rocchio algorithm tfidf text categorization 
international conference machine learning icml 

thorsten joachims 
text categorization support vector machines learning relevant features 
technical report ls report university dortmund november 

croft 
new probabilistic model text classification retrieval 
technical report ir university massachusetts center intelligent information retrieval 
ciir cs umass edu publications index shtml 

daphne koller mehran sahami 
hierarchically classifying documents words 
proceedings fourteenth international conference machine learning 

ken lang 
newsweeder learning filter netnews 
international conference machine learning icml pages 

larkey bruce croft 
combining classifiers text categorization 
sigir 

lewis gale 
sequential algorithm training text classifiers 
proceedings acm sigir conference 
nigam mccallum thrun mitchell 
david lewis marc ringuette 
comparison learning algorithms text categorization 
third annual symposium document analysis information retrieval pages 

david lewis 
evaluation phrasal clustered representations text categorization task 
sigir 

david lewis 
sequential algorithm training text classifiers corrigendum additional data 
sigir forum 

david lewis kimberly knowles 
threading electronic mail preliminary study 
information processing management 

hang li kenji yamanishi 
document classification finite mixture model 
proceedings th annual meeting association computational linguistics 

liere tadepalli 
active learning committees text categorization 
aaai 

andrew mccallum kamal nigam 
comparison event models naive bayes text classification 
submitted aaai workshop learning text categorization 
www cs cmu edu mccallum 

andrew mccallum kamal nigam 
employing em pool active learning text classification 
submitted icml 
www cs cmu 

andrew mccallum ronald rosenfeld tom mitchell andrew ng 
improving text shrinkage hierarchy classes 
submitted icml 
www cs cmu 

mclachlan basford 
mixture models 
marcel dekker new york 

david miller hasan uyar 
mixture experts classifier learning labelled unlabelled data 
advances neural information processing systems nips 

tom mitchell 
machine learning 
mcgraw hill 

pazzani muramatsu billsus 
syskill webert identifying interesting web sites 
aaai 

robertson sparck jones 
relevance weighting search terms 
journal american society information science 

rocchio 
relevance feedback information retrieval 
smart retrieval system experiments automatic document processing chapter pages 
prentice hall 

salton 
developments automatic text retrieval 
science 

shahshahani landgrebe 
effect unlabeled samples reducing small sample size problem mitigating hughes phenomenon 
ieee trans 
geoscience remote sensing sept 

vapnik 
estimations dependences statistical data 
springer publisher 

yiming yang christopher chute 
application squares fit mapping text information retrieval 
proceedings sixteenth annual international acm sigir conference 

yiming yang jan pederson 
feature selection statistical learning text categorization 
icml pages 
