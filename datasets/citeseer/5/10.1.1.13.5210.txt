arc self tuning low overhead replacement cache usenix file storage technologies conference fast march san francisco ca nimrod megiddo modha ibm almaden research center harry road san jose ca consider problem cache management demand paging scenario uniform page sizes 
propose new cache management policy adaptive replacement cache arc advantages 
response evolving changing access patterns arc dynamically adaptively continually balances recency frequency components online fashion 
policy arc uses learning rule adaptively continually revise assumptions workload 
policy arc empirically universal empirically performs certain fixed replacement policy uses best workload specific tuning parameter selected offline fashion 
consequently arc works uniformly varied workloads cache sizes need workload specific priori knowledge tuning 
various policies lru lrfu lirs require user defined parameters unfortunately single choice works uniformly different workloads cache sizes 
policy arc simple implement lru constant complexity request 
comparison policies lru lrfu require logarithmic time complexity cache size 
policy arc scan resistant allows time sequential requests pass polluting cache 
real life traces drawn numerous domains arc leads substantial performance gains lru wide range cache sizes 
example spc synthetic benchmark gb cache lru delivers hit ratio arc achieves hit ratio 
forced recognize possibility constructing hierarchy memories greater capacity preceding quickly accessible 
burks von neumann preliminary discussion logical design electronic computing instrument part vol 
report prepared army ord 
dept june 
problem caching oldest fundamental metaphor modern computing 
widely storage systems example ibm ess emc databases web servers middleware processors file systems disk drives raid controllers operating systems varied numerous applications data compression list updating 
substantial progress caching algorithms affect entire modern computational stack 
consider system consisting memory levels main cache auxiliary 
cache assumed significantly faster auxiliary memory significantly expensive 
size cache memory usually fraction size auxiliary memory 
memories managed units uniformly sized items known pages 
assume cache receives continuous stream requests pages 
assume demand paging scenario page memory paged cache auxiliary memory request page page cache 
particular demand paging rules pre fetching 
full cache new page brought existing pages paged 
victim page selected cache replacement policy 
demand paging model replacement policy algorithm interest 
important metric cache replacement policy hit rate fraction pages served main memory 
rate fraction pages paged cache auxiliary memory 
important metric cache replacement policy overhead low 
problem cache management design replacement policy maximizes hit rate measured long trace subject important practical constraints minimizing computational space overhead involved implementing policy 
contributions main themes design replacement policy high hit ratio paying attention implementation complexity 
equally important theme real life workloads possess great deal richness variation admit size characterization 
may contain long sequential os moving hot spots 
frequency scale temporal locality may change time 
may fluctuate stable repeating access patterns access patterns transient clustered 
static priori fixed replacement policy access patterns 
seek cache replacement policy adapt line fly fashion dynamically evolving workloads 
propose new cache replacement policy adaptive replacement cache arc 
basic idea arc maintain lru lists pages 
list say contains pages seen list say contains pages seen twice 
items seen twice short time low inter arrival rate thought high frequency 
think capturing recency capturing frequency 
endeavor keep lists roughly size cache size lists remember exactly twice number pages fit cache 
words arc maintains cache directory remembers twice pages cache memory 
time arc chooses variable number pages keep precise number pages drawn list tunable parameter adaptively continually tuned 
frc denote fixed replacement policy attempts keep pages pages cache times 
time policy arc behaves frc fixed arc may behave frc time frc time 
key new idea adaptively response evolving workload decide top pages list maintain cache time 
achieve line fly adaptation learning rule allows arc track workload quickly effectively 
effect learning rule induce random walk parameter intuitively learning past arc attempts keep pages cache greatest likelihood near 
acts filter detect track temporal locality 
part workload recency resp 
frequency important arc detect change configure exploit opportunity 
think arc dynamically adaptively continually balancing recency frequency online self tuning fashion response evolving possibly changing access patterns 
empirically demonstrate arc works policy frc assigns fixed portion cache pages remaining fixed portion frequent pages uses best fixed offline workload cache size dependent choice parameter sense arc empirically universal surprisingly arc completely online delivers performance comparable lru lrfu lirs policies best tuning parameters selected offline fashion 
policy arc compares favorably online adaptive policy mq 
implement arc need lru lists 
arc difficult implement lru constant time complexity request requires marginally higher space overhead lru 
reallife implementation space overhead arc cache size 
say arc low overhead 
contrast lru lrfu require logarithmic time complexity request 
result simulations lru factor slower arc lru lrfu factor slower arc lru 
policy arc scan resistant allows time sequential read requests pass cache flushing pages temporal locality 
argument effectively handles long periods low temporal locality 
large number real life workloads drawn workstation disk drives commercial erp system spc synthetic benchmark web search requests demonstrate arc substantially outperforms lru 
anecdotal evidence workstation disk drive workload mb cache lru delivers hit ratio arc achieves hit ratio spc benchmark gb cache lru delivers hit ratio arc achieves hit ratio 
brief outline section ii briefly review relevant provide context arc section iii introduce class replacement policies show class contains lru special case 
section iv introduce policy arc section experimental results workloads 
section vi 
ii 
prior brief review offline optimal priori known page stream belady min replaces page greatest forward distance known optimal terms hit ratio 
policy min provides upper bound achievable hit ratio line policy 
recency policy lru replaces page 
dates back may fact older 
various approximations improvements lru abound see example enhanced clock algorithm 
known workload request stream drawn lru stack depth distribution sdd lru optimal policy 
lru advantages example simple implement responds changes underlying sdd model 
sdd model captures recency capture frequency 
quote significance long run page equally referenced model useful treating clustering effect locality nonuniform page referencing 
frequency independent model irm provides workload characterization captures notion frequency 
specifically irm assumes page drawn independent fashion fixed distribution set pages auxiliary memory 
irm model policy lfu replaces frequently page known optimal 
lfu policy drawbacks requires logarithmic implementation complexity cache size pays attention history adapt changing access patterns accumulates stale pages high frequency counts may longer useful 
relatively algorithm lru approximates lfu eliminating lack adaptivity evolving distribution page frequencies 
significant practical step forward 
basic idea remember page times requested replace page penultimate 
irm assumption known lru largest expected hit ratio line algorithm knows page 
algorithm shown traces :10.1.1.34.2641
lru practical limitations needs maintain priority queue requires logarithmic implementation complexity ii contains crucial tunable parameter correlated information period cip roughly captures amount time page seen kept cache :10.1.1.34.2641
practice logarithmic implementation complexity severe overhead see table limitation mitigated reduces implementation complexity constant request :10.1.1.34.2641
algorithm uses simple lru list priority queue lru similar lru 
policy arc computational overhead similar better lru see table table ii shows choice parameter cip lru arc lru lrfu table comparison computational overhead various cache algorithms trace collected workstation running windows nt captures disk requests 
details trace see section 
cache size represents number byte pages 
obtain numbers reported assumed costs hit 
focuses attention entirely book keeping overhead cache algorithms 
timing numbers seconds obtained clock subroutine time gnu compiler 
seen computational overhead arc essentially lru 
seen lru roughly double overhead lru lrfu large overhead compared lru 
general results hold traces examined 
crucially affects performance lru 
seen single fixed priori choice works uniformly various cache sizes judicious selection parameter crucial achieving performance 
furthermore single priori choice works uniformly various workloads cache sizes examined 
example small value cip parameters stable workloads drawn irm larger value works workloads drawn sdd 
previously noted difficult model algorithm exactly :10.1.1.34.2641
underscores need line fly adaptation 
unfortunately second limitation lru persists 
authors introduce parameters kin note fixing parameters potentially tuning question :10.1.1.34.2641
parameter kin essentially parameter cip lru 
noted kin predetermined parameters need carefully tuned sensitive types workloads 
due space limitation shown table ii lru observed similar dependence workload cip table ii 
hit ratios percentages achieved algorithm lru trace various values tunable parameter cip various cache sizes 
trace collected workstation running windows nt captures disk requests details trace see section 
cache size represents number byte pages 
cache size 
theoretically analyzing set parameter concluded formula requires priori estimate rate little practical tuning :10.1.1.34.2641
suggested choice 
algorithm low inter recency set lirs 
algorithm maintains variable size lru stack lru page lirs th page seen twice lirs parameter 
pages stack algorithm keeps lirs pages seen twice cache pages seen 
parameter similar cip lru kin 
authors suggest setting cache size 
choice stable workloads drawn irm lru friendly workloads drawn sdd 
just cip affects lru kin affects parameter crucially affects lirs 
limitation lirs requires certain stack pruning operation worst case may touch large number pages cache 
implies overhead lirs expected sense worst case lru 
lirs stack may grow arbitrarily large needs priori limited 
idea separating items seen seen twice related similar ideas lru lirs 
precise structure lists self tuning adaptive nature algorithm analogue papers 
recency frequency years interest focussed combining recency frequency 
papers attempted bridge gap lru lfu combining recency frequency various ways 
shall mention heuristic algorithms direction 
frequency replacement fbr policy maintains lru list divides sections new middle old 
page cache counter maintained 
cache hit hit page moved mru position new section hit page middle old section count incremented 
key idea known factoring locality hit page new section count incremented 
cache page old section smallest count replaced 
limitation algorithm prevent cache pollution due stale pages high count usage algorithm periodically resize rescale counts 
algorithm tunable parameters sizes sections parameters cmax amax control periodic resizing 
lru different values tunable parameters may suitable different workloads different cache sizes 
historical importance fbr stems fact earliest papers combine recency frequency 
shown performance fbr similar lru lrfu 
class policies frequently lrfu subsume lru lfu studied 
initially assign value page time update referenced time tunable parameter 
hindsight easily recognize update rule form exponential smoothing widely statistics 
lrfu policy replace page smallest value 
intuitively approaches value simply number occurrences page lrfu collapses lfu 
approaches due exponential decay value emphasizes recency lrfu collapses lru 
performance algorithm depends crucially choice see 
adaptive version adaptive lrfu dynamically adjusts parameter 
lrfu fundamental limitations hinder practice lrfu require additional tunable parameter controlling correlated 
choice parameter matters see 
ii implementation complexity lrfu fluctuates constant request logarithmic cache size request 
due multiplications exponentiations practical complexity significantly higher lru see table seen small values lrfu times slower lru arc overhead potentially wipe entire benefit higher hit ratio 
temporal distance distribution studied multi queue replacement policy mq 
idea typically lru queues qm contains pages seen times times 
algorithm maintains history buffer page hit page frequency incremented page placed mru position appropriate queue set currenttime ime ime tunable parameter 
access lru page queue checked currenttime page moved mru position lower queue 
optimal values ime length depend workload cache size 
algorithm mq designed storage controllers 
due stream caches consecutive accesses single page relatively long temporal distance 
algorithm assumes temporal distance possesses hill shape 
setting recommended value ime parameter peak temporal distance dynamically estimated workload 
algorithm arc stringent assumption shape temporal distance distribution robust wider range workloads 
mq adjust workload evolution measurable change peak temporal distance detected arc track evolving workload adapts continually 
mq constant time overhead needs check time stamps lru pages queues request higher overhead lru arc 
example context table overhead ranged seconds various cache sizes 
caching multiple experts proposed master policy simulates number caching policies particular policies time adaptively dynamically chooses competing policies winner switches winner 
noted develop new caching policy select best policy various competing policies 
arc policies discussed competing policies approaches entirely complementary 
practical standpoint limitation master policy simulate competing policies requires high space time overhead 
applied ideas distributed caching 
ghost caches maintain larger cache directory needed support underlying cache 
directories known shadow cache ghost cache 
previously ghost caches employed number cache replacement algorithms mq lru lirs remember evicted cache pages 
studying storage array cache exclusive client caches ghost caches simulate lru lists disk read blocks client demoted blocks 
hits rates ghost lru lists adaptively determine suitable insertion points type data lru list 
summary contrast lru lirs fbr lrfu require offline selection tunable parameters replacement policy arc online completely self tuning 
importantly arc empirically universal 
policy arc maintains frequency counts lfu fbr suffer periodic rescaling requirements 
lirs policy arc require potentially unbounded space overhead 
contrast mq policy arc may useful wider range workloads adapts quickly evolving workloads computational overhead 
arc lirs mq fbr constant time implementation complexity lfu lru lrfu logarithmic implementation complexity 
iii 
class replacement policies denote cache size number pages 
cache replacement policy write want emphasize number pages managed policy 
introduce policy dbl manage remember twice number pages cache 
respect policy dbl introduce new class cache replacement policies 
double cache replacement policy suppose cache hold pages 
describe cache replacement policy dbl manage cache 
construct motivate development adaptive cache replacement policy cache size cache replacement policy dbl maintains variable sized lists containing pages seen second containing pages seen twice 
precisely page requested exactly time removed requested removed similarly page requested time removed requested removed replacement policy replace lru page contains exactly pages replace lru page replacement policy attempts equalize sizes lists 
exhibit complete algorithm captures dbl pictorially illustrate structure 
sizes lists fluctuate algorithm ensures invariants hold jl jl jl jl new class policies propose new class policies 
intuitively proposed class contain demand paging policies track items cache size managed dbl physically keep cache time 
lists associated dbl 
denote class demand paging cache replacement policies policy exists dynamic partition list top portion bottom portion dynamic partition list top portion bottom portion dbl input request stream 
initialization set 
cases occur 
case cache hit occurred 
mru page case ii cache occurred 
cases occur 
case exactly pages 
delete lru page room new page mru page case pages 
cache full jl jl delete lru page room new page 
insert mru page fig 

algorithm cache replacement policy dbl manages pages cache 
lru dbl 
mru mru 
lru fig 

general structure cache replacement policy dbl 
cache partitioned lru lists contains pages seen contains pages seen twice 
visually list inverted compared allows think items lists closer diagram 
replacement policy deletes lru page contains exactly pages replaces lru page bottom lru 
top mru top mru 
bottom lru fig 

general structure generic cache replacement policy 
lists exactly 
list partitioned top portion bottom portion similarly list partitioned top portion bottom portion policy maintains pages cache 
lists disjoint lists disjoint jl empty 
jl contain exactly pages 
resp 
resp 
empty lru page resp 
mru page resp 

traces time contain exactly pages maintained cache policy 
generic structure policy depicted 
follows conditions pages contained cache size managed subset pages managed dbl 
policy track pages dbl require essentially double space overhead lru 
iii condition implies page resp 
kept pages resp 
kept cache 
policy thought skimming top pages suppose managing cache suppose cache full jt follows condition trace cache actions available policy replace lru page ii replace lru page lru show policy lru contained class 
particular show pages contained dbl 
see fact observe dbl deletes lru item lru item case contain exactly items see case ii 
second case contain items see case ii 
dbl deletes seen pages contains pages contained lru cache items 
follows exist dynamic partition lists lists lru lru lru lru conditions hold 
policy lru contained class claimed 
conversely consider dbl positive integer pages need dbl 
example consider trace 
trace hit ratio lru approaches size trace increases hit ratio dbl zero 
remarks shed light choice size cache directory dbl 
iv 
adaptive replacement cache fixed replacement cache describe fixed replacement cache frc class tunable parameter policy frc satisfy conditions 
brevity write crucial additional condition frc satisfies attempt keep exactly pages list exactly pages list words policy frc attempts keep exactly top pages list top pages list cache 
intuitively parameter target size list light iii replacement policy jt replace lru page jt replace lru page jt missed page resp 
replace lru page resp 

replacement decision somewhat arbitrary differently desired 
subroutine replace consistent 
policy introduce adaptive replacement policy arc class 
time behavior policy arc completely described certain adaptation parameter known 
value arc behave exactly frc arc differs frc single fixed value parameter entire workload 
policy arc continuously adapts tunes response observed workload 
arc arc arc arc denote dynamic partition corresponding arc brevity write arc arc arc arc policy arc dynamically decides response observed workload item replace time 
specifically light iii cache arc adaptively decides replace lru page replace lru page depending value adaptation parameter time 
intuition parameter target size list close resp 
algorithm thought favoring resp 

exhibit complete policy arc 
adaptation steps removed parameter priori fixed value resulting algorithm exactly frc algorithm implicitly simulates lists lists obtained fashion identical lists 
specifically cases ii iii correspond case 
similarly case iv corresponds case ii 
learning policy continually revises parameter fundamental intuition learning hit increase size hit increase size hit increase target size hit decrease increase resp 
decrease implicitly decrease resp 
increase target size precise magnitude revision important 
quantities control magnitude revision 
quantities termed learning rates 
learning rates depend sizes lists hit increment size size increment jb jb increments subject cap smaller size larger increment 
similarly hit decrement size size decrement jb jb decrements subject minimum 
smaller size larger decrement 
idea invest list performing best 
compound effect number small increments decrements quite profound demonstrate section 
roughly think learning rule inducing random walk parameter observe arc stops adapting 
workload suddenly change stable irm generated transient sdd generated vice versa arc track change adapt accordingly exploit new opportunity 
algorithm continuously revises invest past 
scan resistant observe totally new page page put mru position gradually way lru position affects evicted long sequence time reads pass flushing possibly important pages sense arc scan resistant flush pages flush pages furthermore scan begins arguably hits encountered compared effect learning law list grow expense list resistance arc scans 
extra history addition pages cache algorithm arc remembers evicted pages 
interesting question incorporate history information arc improve 
suppose allowed remember extra pages 
demonstrate carry step 
define lru list contains pages discarded list case iv algorithm 
arc input request stream 
initialization set set lru lists empty 
cases occur 
case cache hit occurred arc dbl 
move mru position case ii cache resp 
hit occurred arc resp 
dbl 
adaptation update min fp cg jb jb jb jb replace 
move mru position fetch cache 
case iii cache resp 
hit occurred arc resp 
dbl 
adaptation update max fp jb jb jb jb replace 
move mru position fetch cache 
case iv cache occurred arc dbl 
case exactly pages 
jt delete lru page replace 
empty 
delete lru page remove cache 
endif case pages 
jt jt jb jb delete lru page jt jt jb jb 
replace 
endif fetch cache move mru position subroutine replace jt empty jt exceeds target jt delete lru page remove cache move mru position delete lru page remove cache move mru position endif fig 

algorithm adaptive replacement cache 
algorithm completely self contained directly basis implementation 
tunable parameters needed input algorithm 
start empty cache empty cache directory 
arc corresponds dbl corresponds pages discarded list put list 
list variable timedependent size 
time longest list jzj page page list list related lirs stack 
list constructed follows 
lru page discarded case iv discarded page mru page list discard lru page jzj lru page discarded case iv ensure page new page list discard pages list condition satisfied 
step may discard arbitrarily large number pages resulting algorithm expected sense 
hit list move hit page top list adaptation takes place hit refer resulting algorithm arc 
experiments focus arc arc 
experimental results traces table iii summarizes various traces 
traces capture disk accesses databases web servers nt workstations synthetic benchmark storage controllers 
traces filtered stream caches representative workloads seen storage controllers disks raid controllers 
trace name number requests unique pages oltp concat merge ds spc merge table iii 
summary various traces 
number unique pages trace termed footprint 
trace oltp :10.1.1.34.2641
contains database period 
traces collected workstations running windows nt captures disk operations device filters 
traces gathered months see 
page size traces bytes 
trace concat obtained concatenating traces 
similarly trace merge obtained merging traces time stamps requests 
idea synthesize trace may resemble workload seen small storage controller 
trace ds taken database server running commercial site running erp application top commercial database 
trace days long see 
captured trace spc synthetic benchmark contains long sequential scans addition random accesses 
precise description mathematical algorithms generate benchmark see 
page size trace kbytes 
consider traces disk read accesses initiated large commercial search engine response various web search requests 
trace captured period hour captured roughly hours captured roughly hours 
page size traces kbytes 
trace merge obtained merging traces time stamps requests 
traces considered read requests 
hit ratios reported cold start 
report hit ratios percentages 
oltp studied trace table iv compare arc number algorithms 
tunable parameters fbr lirs set original papers 
tunable parameters lru lrfu selected offline fashion trying different parameters selecting best result cache size 
parameter ime mq dynamically estimated history size set cache size resulting space overhead comparable arc algorithm arc self tuning requires user specified parameters 
seen arc outperforms online algorithms comparable offline algorithms lru lrfu 
lfu fbr lru lrfu min numbers exactly reported 
traces traces table display comparison hit ratios achieved arc lru lru lrfu lirs 
tunable parameters lru lrfu lirs selected offline fashion trying different parameters selecting best result trace cache size 
seen arc outperforms lru performs close lru lrfu lirs algorithms best offline parameters 
brevity exhibited results traces general results continue hold traces examined arc table compared arc best fixed offline choice tunable parameter kin 
table vi compare arc online forced reasonable values tunable parameters specifically kin :10.1.1.34.2641
seen arc outperforms 
oltp lru arc fbr lfu lirs mq lru lrfu min online offline table iv 
comparison arc hit ratios various cache algorithms oltp trace 
hit ratios reported percentages 
seen arc outperforms lru lfu fbr lirs mq performs lru lrfu algorithms best offline parameters 
lru mq arc lru lrfu lirs online offline lru mq arc lru lrfu lirs online offline table comparison arc hit ratios various cache algorithms traces 
hit ratios reported percentages 
seen arc outperforms lru performs close lru lrfu lirs algorithms best offline parameters 
trace arc outperforms mq cache sizes mq outperforms arc cache sizes 
trace arc uniformly outperforms mq 
arc mq seen table trace arc outperforms mq cache sizes mq outperforms arc cache sizes 
furthermore seen arc uniformly outperforms mq trace 
workloads spc merge represent requests storage controller 
table vi compare hit ratios lru mq arc workloads 
seen mq outperforms lru arc outperforms mq lru 
results quite surprising algorithm mq designed especially storage servers 
show arc quickly track evolving workload fluctuates extreme recency frequency 
trace table vii compare hit ratios lru mq arc clearly seen arc outperforms algorithms 
lru designed recency mq designed workloads stable temporal distance distributions design meaningfully track workload 
taken tables vi vii imply arc effective wider range workloads mq 
complexity arc smaller constant overhead 
adaptation arc requires tuning single scalar adaptation mq requires maintaining histogram observed temporal distances 
cache size number byte pages hit ratio arc lru cache size number byte pages arc lru cache size number byte pages arc lru cache size number byte pages hit ratio arc lru cache size number byte pages arc lru cache size number byte pages hit ratio arc lru fig 

plot hit ratios percentages achieved arc lru 
axes logarithmic scale 
cache size number byte pages arc lru cache size number byte pages arc lru cache size number byte pages concat arc lru cache size number byte pages hit ratio merge arc lru cache size number byte pages ds arc lru cache size number byte pages arc lru fig 

plot hit ratios percentages achieved arc lru 
axes logarithmic scale 
spc lru mq arc online merge lru mq arc online table vi 
comparison hit ratios lru mq arc traces spc merge 
hit ratios reported percentages 
algorithm forced reasonable values tunable parameters specifically kin 
page size kbytes traces 
largest cache simulated spc gbytes merge gbytes 
lru mq arc online table vii 
comparison hit ratios lru mq arc trace 
hit ratios reported percentages 
seen arc outperforms algorithms 
arc lru focus lru single widely cache replacement policy 
traces listed section plot hit ratio arc versus lru figures 
traces spc merge plotted discussed various tables 
traces plotted results virtually identical merge 
traces plotted space consideration traces arc uniformly better lru 
clearly seen arc substantially outperforms lru virtually traces cache sizes 
workload space lru arc frc mb offline concat merge ds spc merge table viii 
glance comparison hit ratios lru arc various workloads 
hit ratios reported percentages 
seen arc outperforms lru quite dramatically 
arc online performs close frc best fixed offline choice parameter table viii glance comparison arc lru traces trace selected practically relevant cache size 
trace spc contains long sequential scans interspersed random requests 
seen trace arc due scan resistance continues outperform lru 
arc self tuning empirically universal surprising intriguing results 
table viii seen arc tunes performs better policy frc best fixed offline selection parameter result holds traces 
sense arc empirically universal 
seen table viii arc outperform offline optimal frc happens example trace 
trace shows parameter fluctuates entire range 
arc adaptive tracks variation workload dynamically tuning contrast frc single fixed choice entire workload offline optimal value 
performance benefit arc hand arc slightly worse offline optimal frc happens example trace 
trace arc maintains small value parameter favors frequency entire workload 
case arguably exists small range optimal parameters 
due constant adaptation arc lock single fixed parameter keeps fluctuating optimal choice 
fluctuations cost arc slightly offline optimal frc terms hit ratio 
stable workloads expect arc slightly worse offline optimal frc theoretical construct available practice 
stable workloads value best fixed offline parameter depends workload cache size 
policy arc provides reasonable online approximation offline optimal frc requiring priori workload specific cache size specific tuning 
closer examination adaptation arc study adaptation parameter closely 
trace cache size pages plot parameter versus virtual time number requests 
close zero arc thought emphasizing contents list close cache size arc thought emphasizing contents list seen parameter keeps fluctuating extremes 
seen touches extremes 
result arc continually adapts reconfigures 
quite dramatically policy arc fluctuate frequency recency back single workload 
fluctuations may occur times dictated nature workload priori knowledge offline tuning 
time tune played workload 
continuous adaptation response changing evolving workload allows arc outperform lru 
vi 
reviewed various cache replacement algorithms lru lrfu lirs 
performance algorithms depends crucially respective tunable parameters 
single universal rule thumb priori select algorithms 
addition demonstrated computational overhead lru lrfu practically attractive 
new cache replacement policy arc online self tuning 
empirically demonstrated arc adapts workload dynamically 
empirically demonstrated arc performs better frc best offline choice parameter virtual time request number target size list fig 

plot adaptation parameter target size list versus virtual time trace 
cache size pages 
page size bytes 
workload cache size 
similarly shown arc online performs lru lrfu lirs algorithms best offline values respective tuning parameters 
demonstrated arc outperforms forced online reasonable values tunable parameters 
demonstrated performance arc comparable better mq specific domain designed 
contrast mq shown arc robust wider range workloads overhead 
demonstrated arc overhead comparable lru low overhead policy 
argued arc scan resistant 
importantly shown arc substantially outperforms lru virtually uniformly numerous different workloads various different cache sizes 
results show considerable room performance improvement modern caches adaptation cache replacement policy 
hope arc seriously considered cache designers suitable alternative lru 

universal motivated similar data compression coding scheme termed universal knowledge source generated string asymptotically compresses string coding scheme knows statistics generating source 

legally correct term spc benchmark specification term spc quote performance numbers interest 

due extremely large overhead numerical instabilities able simulate lrfu lru larger traces concat merge larger cache sizes mbytes 
acknowledgment grateful jai menon holding contrarian belief cache replacement policies improved 
second author grateful managers bill tetzlaff constant support encouragement 
grateful brent beardsley goyal robert morris william tetzlaff tewari honesty young valuable discussions suggestions 
grateful bruce tewari spc trace windsor hsu traces ruth azevedo trace ds gerhard weikum trace ken bates bruce traces song jiang sharing lirs simulator sang min lee sharing lrfu simulator various numbers reported table iv 
peter chen fast shepherd anonymous referees valuable suggestions greatly improved 
teng managing ibm database buffers maximize performance ibm sys 
vol 
pp 

cao irani cost aware www proxy caching algorithms proc 
usenix symp 
internet technologies systems monterey ca 
iyengar middleware system intelligently caches query results middleware vol 
lncs pp 

gee hill smith cache performance spec benchmark suite tech 
rep cs tr university california berkeley 
nelson welch ousterhout caching sprite network file system acm transactions computer systems vol 
pp 

smith disk cache ratio analysis design considerations acm trans 
computer systems vol 
pp 

chen lee gibson katz patterson raid high performance reliable secondary storage acm computing surveys vol 
pp 

bach design unix operating system 
englewood cliffs nj prentice hall 
bentley sleator tarjan wei locally adaptive data compression scheme comm 
acm vol 
pp 

sleator tarjan amortized efficiency list update paging rules comm 
acm vol 
pp 

ziv lempel universal algorithm sequential data compression ieee trans 
inform 
theory vol 
pp 

belady study replacement algorithms virtual storage computers ibm sys 
vol 
pp 

mattson traiger evaluation techniques storage hierarchies ibm sys 
vol 
pp 

denning working sets past ieee trans 
software vol 
se pp 

carr hennessy simple effective algorithm virtual memory management proc 
eighth symp 
operating system principles pp 

coffman denning operating systems theory 
englewood cliffs nj prentice hall 
aho denning ullman principles optimal page replacement acm vol 
pp 

neil neil weikum lru page replacement algorithm database disk buffering proc 
acm sigmod conf pp 

neil neil weikum optimality proof lru page replacement algorithm acm vol 
pp 

johnson shasha low overhead high performance buffer management replacement algorithm proc :10.1.1.34.2641
vldb conf pp 

jiang zhang lirs efficient low inter recency set replacement policy improve buffer cache performance proc 
acm sigmetrics conf 
robinson data cache management frequency replacement proc 
acm sigmetrics conf pp 

lee choi 
kim noh min cho kim lrfu spectrum policies subsumes frequently policies ieee trans 
computers vol 
pp 

lee choi 
kim noh min cho kim existence spectrum policies subsumes lru frequently lfu policies proc 
acm sigmetrics conf pp 

zhou philbin multi queue replacement algorithm second level buffer caches proc 
usenix annual tech 
conf 
usenix boston ma pp 
june 
warmuth brandt ari adaptive caching nips 
ari amer miller brandt long acme adaptive caching multiple experts proceedings workshop distributed data structures carleton scientific 
wong wilkes cache 
making storage exclusive proc 
usenix annual tech 
conf 
usenix monterey ca pp 
june 
hsu smith young automatic improvement locality storage systems 
tech 
rep computer science division univ california berkeley nov 
hsu smith young characteristics traffic personal computer server workloads 
tech 
rep computer science division univ california berkeley july 
johnson standard test cache proc 
computer measurements group international conference 
johnson reich making standard benchmark open system storage comput 
resource management pp 
winter 
fractal structure data applications memory hierarchy 
boston ma kluwer academic publishers 
