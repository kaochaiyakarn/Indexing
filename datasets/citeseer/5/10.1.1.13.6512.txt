taming decentralized pomdps efficient policy computation multiagent settings nair tambe computer science dept university southern california los angeles ca nair tambe usc edu problem deriving joint policies group agents maximize joint reward function modeled decentralized partially observable markov decision process pomdp 
despite growing importance applications decentralized pomdp models arena algorithms developed efficiently deriving joint policies models 
presents new class locally optimal algorithms called joint search policies jesp 
describe exhaustive version jesp subsequently novel dynamic programming approach jesp 
complexity analysis reveals potential exponential speedups due dynamic programming approach 
theoretical results verified empirical comparisons jesp versions globally optimal brute force search algorithm 
prove piece wise linear convexity properties steps developing algorithms continuous belief states 
multiagent systems move research lab critical applications multi satellite control researchers need provide high performing robust multiagent designs nearly optimal feasible 
researchers increasingly resorted decision theoretic models framework formulate evaluate multiagent designs 
group agents problem deriving separate policies maximize joint reward modeled decentralized pomdp partially observable markov decision process 
particular dec pomdp decentralized pomdp bernstein mtdp markov team decision problem pynadath tambe generalizations pomdp case multiple distributed agents basing actions separate observations 
frameworks allow variety multiagent analysis 
particular interest allow formulate constitutes optimal policy multiagent system principle derive policy 
yokoo coop 
computing research grp 
ntt comm 
sc 
labs kyoto japan yokoo cslab ntt jp pynadath marsella information sciences institute university southern california marina del rey ca pynadath marsella isi edu exceptions effective algorithms deriving policies decentralized pomdps developed 
significant progress achieved efficient single agent pomdp policy generation algorithms monahan cassandra kaelbling 
research directly carried decentralized case 
finding optimal policies decentralized pomdps nexp complete bernstein 
contrast solving pomdp pspacecomplete papadimitriou tsitsiklis 
bernstein note suggests fundamental difference nature problems 
decentralized problem treated separate pomdps individual policies generated individual agents possible cross agent interactions reward transition observation functions 
action agent may different rewards possible actions agents may take 
domains possibility simplify nature policies considered agents 
example chad restrict agent policies memoryless reactive policies 
approximation define reward function transition function observations states simplifying problem solving multi agent mdp boutilier 
xuan describe derive decentralized mdp pomdp policies centralized mdp policy 
algorithm starts assumption full communication gradually relaxed relies instantaneous noise free communication 
simplifications reduce applicability approach essentially side step question solving decentralized pomdps 
peshkin take different approach gradient descent search find local optimum finite controllers bounded memory 
algorithm finds locally optimal policies limited subset policies infinite planning horizon algorithm finds locally optimal policies unrestricted set possible policies finite planning horizon 
remains critical need new efficient algorithms generating optimal policies distributed pomdps 
new class algorithms solving decentralized pomdps refer joint equilibrium search policies jesp 
jesp iterates agents finding optimal policy agent assuming policies agents fixed 
iteration continues improvements joint reward achieved 
jesp achieves local optimum similar nash equilibrium 
discuss exhaustive jesp uses exhaustive search find best policy agent 
exhaustive search single agent policy expensive dp jesp improves exhaustive jesp dynamic programming incrementally derive policy 
conclude empirical evaluation contrast jesp globally optimal algorithm derives globally optimal policy full search space policies 
prove piece wise linear convexity properties steps developing algorithms continuous initial belief states 
model describe markov team decision problem mtdp pynadath tambe framework detail provide concrete illustration decentralized pomdp model 
decentralized pomdp models potentially serve basis bernstein xuan 
team agents mtdp pynadath tambe defined tuple 
finite set world states 
sets action agents 
joint action represented 
transition function represents probability current state previous state previous joint action 
set observations agents 
observation function represents probability joint observation current state previous joint action 
agents receive single immediate joint reward shared equally 
practical analysis models mtdp assume observations agent independent observations 
observation function expressed agent chooses actions local policy mapping observation history actions 
time agent perform action refers joint policy team agents 
important thing note model execution distributed planning centralized 
agents don know observations actions run time know policies 
example scenario illustrative purposes useful consider familiar simple example capable bringing key difficulties creating optimal policies 
consider multiagent version classic tiger problem illustrating single agent pomdps kaelbling create mtdp example 
modified version agents corridor facing doors left right 
door lies hungry tiger lies agents know position 
indicating door tiger 
agents jointly individually open door 
addition agents independently listen presence tiger 

transition function specifies time agent opens doors state reset equal probability regardless action agent shown table 
agents listen state remains unchanged 
action agent receives observation new state 
observation function shown table return different probabilities depending joint action taken resulting world state 
example agents listen tiger left door state agent receives observation probability probability 
action transition sl sl sl sr sr sr sr sl listen listen table transition function action state hl hr listen listen sl listen listen sr table observation function agent opens door tiger attacked equally tiger see table 
injury sustained opened door tiger severe open door jointly open door 
similarly receive wealth share equally open door proportion number agents opened door 
agents incur small cost performing action 
clearly acting jointly beneficial agents receive sustain damage acting 
agents receive independent observations share observations need consider observation histories agent action perform 
action state sl sr listen listen listen listen listen listen table reward function consider consider case reward function vary penalty jointly opening door tiger see table 
action state sl sr listen listen listen listen listen listen table reward function optimal joint policy agents share observations coordinate selecting policies sensitive teammates possible beliefs agent entire history observations provides information 
problem facing team find optimal joint policy combination individual agent policies produces behavior maximizes team expected reward 
sure fire method finding optimal joint policy simply search entire space possible joint policies evaluate expected reward select policy highest value 
perform search able determine expected reward joint policy 
compute expectation projecting team execution possible branches different world states different observations 
agent version computation results easily generalize arbitrary team sizes 
time step compute expected value joint policy team starting state set past observations follows time step computation performs summation possible world states agent search performs observations time complexity algorithm computation possible joint policy 
policy specifies different actions possible histories observations number possible policies individual agent number possible joint policies agents correspond largest individual action observation spaces respectively agents 
time complexity find ing optimal joint policy searching space joint equilibrium search policies complexity exhaustively searching optimal joint policy clear methods successful amount time generate policy restricted 
section algorithms guaranteed find locally optimal joint policy 
refer category algorithms jesp joint equilibrium search policies 
just solution section solution obtained jesp nash equilibrium 
particular locally optimal solution partially observable identical payoff stochastic game peshkin 
key idea find policy maximizes joint expected reward agent time keeping policies agents fixed 
process repeated equilibrium reached local optimum 
problem optimum agents select multiple local optima encountered planning centralized 
exhaustive approach exhaustive jesp algorithm describes exhaustive approach jesp 
consider cooperative agents 
modify policy agent time keeping policies agents fixed 
function best policy returns joint policy maximizes expected joint reward obtained keeping agents policies fixed exhaustively searching entire policy space agent policy free 
iteration value modified joint policy increase remain unchanged 
repeated equilibrium reached policies agents remains unchanged 
policy guaranteed local maximum value new joint policy iteration nondecreasing 
algorithm exhaustive jesp prev random joint policy conv conv fix policy agents list policies new prev new value prev value conv conv prev new conv conv break return new best policy remain unchanged iterations convergence reached worst case joint policy best policy iteration 
algorithm worst case complexity exhaustive search globally optimal policy 
better practice illustrated section 
solution algorithm local optimum may adequate applications 
techniques random restarts simulated annealing applied perturb solution see settles different higher value 
exhaustive approach steps exhaustive jesp algorithm enumerates searches entire policy space single agent 
policies evaluating incurs time complexity ex approach incurs time complexity steps incur complexity cost pass jesp algorithm faster means performing function call step produce big payoff efficiency 
describe dynamic programming alternative exhaustive approach doing jesp 
dynamic programming dp jesp examine single agent pomdp literature inspiration find algorithms exploit dynamic programming incrementally construct best policy simply search entire policy space monahan cassandra kaelbling 
algorithms rely principle optimality states sub policy optimal policy optimal 
words step optimal policy history steps portion policy covers steps optimal remaining steps 
section show exploit analogous optimality property multiagent case perform efficient construction optimal policy jesp algorithm 
support dynamic programming algorithm define belief states summarize agent history past observations allow agents ignore actual history past observations supporting construction optimal policy possible 
single agent case belief state stores distri bution statistic agent compute optimal policy having consider actual observation sequence sondik 
multiagent case agent faces complex mal single agent pomdp policies agents fixed 
sufficient agent reason action selection agents observation histories agents 
time agent reasons tuple joint observation histories agents 
treating state agent time define transition function observation function single agent pomdp agent follows agents 
joint policy define novel multiagent belief state agent distribution initial state words reasoning agent policy context agents maintain distribution simply current state 
shows different belief states agent tiger domain 
instance shows probability distributions history agent observations sl current state 
section demonstrates multiagent belief state construct dynamic program incrementally constructs optimal policy agent 
dynamic programming algorithm model single agent value iteration algorithm dynamic program centers value function step finite horizon 
readability section presents derivation dynamic program listen hl sl hr sl hl sr hr sr hl sl sl listen hr sl hr sl hl sr hr sr hl trace tiger scenario agent case results easily generalize agent case 
having fixed policy agent value function represents expected reward team receive agent follows optimal policy th step onwards starting current belief state start time horizon way back 
way construct optimal policy maximizing value function possible action choices define action value function recursively term equation refers expected immediate reward second term refers expected reward 
belief state updated performing action observing base case reward leaving calculation expected immediate reward breaks follows compute immediate reward agent current belief state primitive elements mtdp model see section 
computation expected reward second term equation depends ability update agent light new observa belief state tion example belief state updated performing action receiving observation derive algorithm performing update computing remaining term equation 
initial belief state distribution initial state updated obtained equations bayes rule follows treat denominator equation normalizing constant bring sum numerator 
result enters computation expected reward second term equation 
compute agent new belief state expected reward value function turn agent current belief state primitive elements mtdp model 
having computed value function extract form optimal policy maps observation histories actions required equations 
algorithm presents pseudo code dynamic programming algorithm 
lines generate belief states reachable initial belief state possibly unique belief state sequence actions observations agent reachable belief states 
reachability analysis uses belief update procedure algorithm time complexity invoked belief state time 
reachability analysis phase time complexity lines perform heart dynamic programming algorithm time complexity lines translate resulting value function agent policy defined observation sequences required algorithm argument 
phase lower time space complexity phases considers optimal actions agent 
time complexity algorithm space complexity resulting value function policy essentially product number reachable belief states size belief state representation piecewise linearity convexity value function algorithm computes value function belief states reachable initial belief state subset possible probability distributions dynamic programming entire set show chosen value function piecewise linear convex 
agent faced solving single agent pomdp policies agents fixed shown section 
sondik showed value function single agent pomdp 
value function equation 
addition algorithm reachable reachable reachable reachable update downto reachable equation compute reward prob act prob act act prob update equation update arg return algorithm update act equation act act act normalize return supporting efficient dynamic programming algorithm novel choice belief state space value function potentially support dynamic programming algorithm entire continuous space possible belief states 
experimental results section perform empirical comparison algorithms described sections tiger scenario see section terms time performance 
shows results running globally optimal algorithm exhaustive jesp algorithm different reward functions tables 
finding globally optimal policy extremely slow doubly exponential finite horizon evaluate algorithms finite horizons 
ran jesp algorithm different randomly selected initial policy settings compared performance algorithms terms number policy evaluations axis log scale necessary 
seen jesp algorithm requires fewer evaluations arrive equilibrium 
difference run times globally optimal algorithm jesp algorithm apparent globally optimal algorithm performed policy evaluations jesp algorithm evaluations 
reward function jesp succeeded finding globally optimal policies expected reward expected reward 
case 
reward function jesp algorithm settles locally optimal policy expected reward thatis different globally optimal policy expected reward 
random restarts globally optimal reward obtained 
conclude exhaustive jesp algorithm performs better exhaustive search globally optimal policy times settle policy locally optimal 
sufficient problems difference locally optimal policy value globally optimal policy value small imperative policy quickly 
alternatively jesp algorithm altered doesn get stuck local optimum random restarts 
table compares presents experimental results comparison exhaustive jesp dynamic programming approach dp jesp 
results tiger domain show run time milliseconds ms algorithms increasing horizon 
dp jesp seen obtain significant speedups exhaustive jesp 
time horizon dp jesp run time essentially ms compared significant run times exhaustive jesp 
increased horizon run exhaustive jesp dp jesp easily run horizon 
summary growing importance decentralized pomdps arena design analysis critical develop efficient algorithms generating joint poli bernstein bernstein zilberstein immerman 
complexity decentralized control mdps 
uai 
boutilier boutilier 
planning learning coordination multiagent decision processes 
tark 
cassandra cassandra littman zhang 
incremental pruning simple fast exact method partially observable markov decision processes 
uai 
acknowledgments piotr gmytrasiewicz discussions related 
research supported nsf darpa award 

cies 
significant lack efficient algorithms 
novel contributions address shortcoming 
complexity exhaustive policy search algorithm doubly exponential number agents time describe class algorithms called joint equilibrium search policies jesp search local optimum global optimum 
particular provide detailed algorithms exhaustive jesp dynamic programming jesp dp jesp 
second provide complexity analysis dp jesp illustrates potential exponential speedups exhaustive jesp 
implemented algorithms empirically verified significant speedups provide 
third provide proof value function individual agents piece wise linear convex belief states 
key result pave way new family algorithms operate continuous belief states increasing range applications attacked decentralized pomdps major issue 
table run time ms various pentium ghz gb memory linux redhat common lisp method exhaustive jesp dp jesp chad chad scherrer charpillet 
heuristic approach solving decentralized pomdp assessment pursuit problem 
sac 
kaelbling kaelbling littman cassandra 
planning acting partially observable stochastic domains 
artificial intelligence 
monahan monahan 
survey partially observable markov decision processes theory models algorithms 
management science january 
papadimitriou tsitsiklis papadimitriou tsitsiklis 
complexity markov decision processes 
mathematics research 
peshkin peshkin meuleau kim kaelbling 
learning cooperate policy search 
uai 
pynadath tambe pynadath tambe 
communicative multiagent team decision problem analyzing teamwork theories models 
jair 
sondik edward sondik 
optimal control partially observable markov processes 
ph thesis stanford 
xuan xuan lesser zilberstein 
communication decisions multiagent cooperation 
agents 
evaluation results 
