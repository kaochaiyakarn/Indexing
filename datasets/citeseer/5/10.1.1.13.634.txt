long short term memory technical report sepp hochreiter fakultat fur informatik technische universitat munchen munchen germany informatik tu muenchen de www informatik tu muenchen de jurgen schmidhuber idsia corso lugano switzerland juergen idsia ch www idsia ch juergen revised august recurrent backprop learning store information extended time intervals takes long 
main reason insufficient decaying error back flow 
briefly review hochreiter analysis problem 
overcome introducing novel efficient method called long short term memory lstm 
lstm learn bridge minimal time lags excess time steps enforcing constant error flow internal states special units 
multiplicative gate units learn open close access constant error flow 
lstm update complexity time step number weights 
experimental comparisons real time recurrent learning back propagation time recurrent cascade correlation elman training procedure neural sequence chunking lstm leads successful runs learns faster 
lstm solves complex long time lag tasks solved previous recurrent net algorithms 
lstm works local distributed real valued noisy pattern representations 
principle recurrent nets feedback connections store representations input events form activations short term memory opposed long term memory embodied slowly changing weights 
potentially significant applications including speech processing non markovian control music composition mozer 
widely previous algorithms learning put short term memory take time don especially minimal time lags inputs corresponding teacher signals long 
instance conventional backprop time bptt williams zipser rtrl robinson fallside error signals flowing backwards time tend blow vanish temporal evolution backpropagated error exponentially depends size weights 
case may lead oscillating weights 
case learning bridge long time lags takes prohibitive amount time 
reasons briefly discussed section outlines detailed analysis error blow ups vanishing errors due hochreiter 
long short term memory lstm new approach overcomes problems 
schmidhuber chunking systems input abbreviation lstm refers novel architecture conjunction appropriate gradient learning algorithm 
combination designed overcome decaying error flow problems previous approaches 
expression lstm architecture architectural issues addressed 
expression lstm algorithm aspects algorithm addressed 
sequences contain local regularities partly predictable noisy highly unpredictable environments lstm learn bridge time intervals excess time steps loss short time lag capabilities 
major feature lstm architecture enforces constant non exploding non vanishing error flow internal states special units 
constant error backprop method fast 
outline 
didactic purposes overview error flow analysis section introduce naive approach constant error backprop highlight problems concerning information storage retrieval 
problems solved lstm architecture described section 
section numerous experiments comparisons competing methods 
lstm outperforms 
lstm learns solve complex tasks recurrent net algorithm solved 
section briefly review previous 
section discuss certain limitations advantages lstm 
appendix contains detailed description algorithm explicit formulae error flow 
constant error backprop subsection explain standard recurrent nets problems decaying error flow 
subsection naive approach overcome problems introducing single unit architecture enforces constant error flow 
certain problems architecture lead lstm architecture section 
exponentially decaying error conventional bptt williams zipser 
output unit target time denoted 
mean squared error error signal net gamma net activation non input unit activation function net ij gamma unit current net input ij weight connection unit non output unit backpropagated error signal net ij 
corresponding contribution jl total weight update ff gamma ff learning rate stands arbitrary unit connected unit problems conventional methods outline hochreiter analysis page 
suppose fully connected net non input unit indices range error occurring arbitrary unit time step propagated back time time steps arbitrary unit scale error factor gamma net gamma uv net gamma gammaq lv obtain gamma gamma net gamma gamma proof induction 
sum gamma terms net gamma gamma determines total error back flow note summation terms may different signs increasing number units necessarily increase error flow 
intuitive explanation equation 
jf net gamma gamma happen linear possible gamma exceed largest product increases exponentially error blows different error signals arriving unit lead oscillating weights unstable learning 
hand jf net gamma gamma case logistic sigmoid absolute weight initialization values see paragraph largest product decreases exponentially error vanishes learned acceptable time 
logistic sigmoid function maximal value 
gamma constant equal zero jf net gamma takes maximal values gamma gamma net goes zero jw gamma jw gamma absolute maximal weight value wmax smaller 
conventional logistic sigmoid activation functions weights absolute values especially training phase error flow tends vanish 
general larger initial weights won help seen relevant derivative goes zero faster absolute weight grow weights change signs crossing zero 
likewise increasing learning rate help won change ratio long range error flow short range error flow 
bptt sensitive distractions 
similar analysis bengio 
weak upper bound scaling factor 
slightly extended analysis vanishing error case takes number units account 
formula rewritten gamma gamma wf gamma net gamma weight matrix defined ij ij outgoing weight vector defined iv iv incoming weight vector defined ui ui gamma diagonal matrix order derivatives defined gamma ij gamma ij net gamma 
transposition operator ij element th column th row matrix th component vector matrix norm ka compatible vector norm define max fk gamma 
max fjx jg get jx yj jf net gamma gamma ka max obtain inequality gamma max gamma max ka inequality results ka ka ka ka unit vector components th component 
note weak extreme case upper bound reached gamma ka take maximal values contributions paths error flows back unit unit sign 
large ka typically result small values gamma ka confirmed experiments see hochreiter 
example norms ka max jw rs max jx max logistic sigmoid 
observe jw ij wmax ka result exponential decay setting gamma delta obtain gammaq refer hochreiter thesis additional details 
constant error flow naive approach single unit 
avoid vanishing error signals achieve constant error flow single unit single connection 
rules time local error back flow net jj enforce constant error flow require net jj 
note right hand side mozer fixed time constants time constant appropriate potentially infinite time lags comment expression time constant differential sense pearlmutter 
integrating differential equation obtain net net jj arbitrary net 
means linear unit activation remain constant net jj 
experiments ensured identity function setting jj 
course reality unit connected units 
invokes obvious related problems inherent gradient approaches 
input weight conflict simplicity focus single additional input weight ji assume total error reduced switching unit response certain input keeping active long time helps compute desired output 
provided nonzero incoming weight storing certain inputs ignoring ji receive conflicting weight update signals time recall linear signals attempt ji participate storing input switching protecting input preventing switched insignificant inputs 
conflict learning difficult calls context sensitive mechanism controlling write operations input weights 

output weight conflict assume switched currently stores previous input 
simplicity focus single additional outgoing weight kj kj retrieving content certain times preventing disturbing times long unit non zero kj attract conflicting weight update signals generated sequence processing signals attempt kj participate accessing information stored different times protecting unit perturbed instance tasks certain short time lag errors reduced early training stages 
training stages may suddenly start cause avoidable errors situations control attempting participate reducing difficult long time lag errors 
conflict learning difficult calls context sensitive mechanism controlling read operations output weights 
course input output weight conflicts specific long time lags occur short time lags 
effects particularly pronounced long time lag case time lag size increases stored information needs protection perturbation longer longer time intervals especially advanced stages learning correct outputs need protection perturbation 
due problems naive approach 
section shows right 
long short term memory memory cells gate units 
construct architecture allows constant error flow special self connected units disadvantages naive approach extend self connected linear unit section introducing additional features 
multiplicative input gate unit introduced allows protecting constant error flow perturbation irrelevant inputs 
likewise multiplicative output gate unit introduced allows protecting units perturbation currently irrelevant memory contents stored essentially gates designed learn open close access constant error flow resulting complex unit called memory cell 
see 
th memory cell denoted memory cell built central linear unit fixed self connection 
addition net gets input multiplicative unit output gate multiplicative unit input gate 
activation time denoted 
activation time denoted 
net net net gamma net gamma 
net gamma 
summation indices may stand input units gate units memory cells conventional hidden units see paragraph network topology 
different types units may convey useful information current state net 
instance input gate output gate may inputs memory cells decide store access certain information memory cell 
may recurrent self connections user define network topology 
see example 
time output computed sigma pi fashion internal state gamma gamma net delta differentiable function net differentiable function scales memory cell outputs computed internal state net net net architecture memory cell box corresponding gate units self recurrent connection weight indicates feedback delay time step 
see text appendix details 
gate units 
avoid input weight conflicts see section controls error flow memory cell input connections circumvent output weight conflicts see section controls error flow unit output connections 
symmetrically net decide keep override information memory cell see 
net decide access memory cell prevent units perturbed see 
error signals trapped memory cell change different error signals flowing cell different times output gate may get superimposed 
output gate learn errors trap memory cell appropriately scaling 
input gate learn release errors appropriately scaling 
essentially multiplicative gate units open close access constant error flow internal states memory cells 
conservative large negative initial gate bias turn useful preventing memory cells participating reducing short time lag errors training phase see paragraphs state drift abuse problem 
distributed output representations typically require output gates 
gate types necessary may sufficient 
instance experiments section possible input gates 
fact output gates indispensable case local output encoding preventing memory cells perturbing learned outputs done simply setting corresponding weights zero 
local output coding case output gates beneficial 
prevent net attempts storing long time lag memories usually hard learn perturbing activations representing easily learnable short time lag memories instance output gates quite useful experiment 
network topology 
network input layer hidden layer output layer 
fully self connected hidden layer contains memory cells corresponding gate units convenience refer memory cells gate units located hidden layer 
hidden layer may contain conventional hidden units providing inputs gate units memory cells 
units gate units layers directed connections serve inputs units layer higher layers experiments 
memory cell blocks 
memory cells sharing input gate output gate form structure called memory cell block size 
memory cell blocks facilitate information storage conventional neural nets easy code distributed input single cell 
memory cell block gate units single memory cell block architecture slightly efficient see paragraph computational complexity 
memory cell block size just simple memory cell 
experiments section memory cell blocks various sizes 
output hidden input cell block block cell block block cell cell example net input units output units memory cell blocks size 
marks input gate marks output gate cell block marks memory cell block 
cell block architecture identical gate units note rotating degrees anti clockwise match corresponding parts 
example assumes dense connectivity gate unit memory cell see non output units 
simplicity outgoing weights type unit shown layer 
efficient truncated update rule error flows connections output units fixed self connections cell blocks shown see 
error flow truncated wants leave memory cells gate units 
connection shown serves propagate error back unit connection originates connections output units connections modifiable 
truncated lstm algorithm efficient despite ability bridge long time lags 
see text appendix details 
shows architecture experiment bias non input units omitted 
learning 
variant rtrl robinson fallside properly takes account altered multiplicative dynamics caused input output gates 
ensure non decaying error backprop internal states memory cells truncated bptt williams peng errors arriving memory cell net inputs cell includes net net net get propagated back time serve change incoming weights 
memory cells errors propagated back previous internal states visualize error signal arrives memory cell output gets scaled ouput gate activation memory cell 
flow back back back scaled 
leaves memory cell input gate scaled input gate activation derivatives 
serves change incoming weights 
gets truncated 
scaled twice 
constant see appendix explicit formulae 
computational complexity 
mozer focused recurrent backprop algorithm mozer derivatives sc il need stored updated 
algorithm efficient lstm algorithm update complexity time step excellent comparison approaches rtrl complexity kw number output units number weights see details appendix 
memory cells fixed number output units lstm algorithm update complexity time step just bptt fully recurrent net units 
full bptt method local space schmidhuber need store activation values observed sequence processing stack potentially unlimited size 
comment backprop version don recommend just expensive outside memory units tends typical exponential decay 
abuse problem solutions 
learning phase error reduction may possible storing information time 
net tend abuse memory cells bias cells network activations constant outgoing connections adaptive thresholds units 
potential difficulty may take long time release abused memory cells available learning 
similar abuse problem appears memory cells store redundant information 
solutions abuse problem sequential network construction fahlman memory cell corresponding gate units added network error stops decreasing see experiment section 
output gate bias output gate gets negative initial bias push initial memory cell activations zero 
memory cells negative bias automatically get allocated see experiments section 
internal state drift remedies 
memory cell inputs positive negative internal state tend drift away time 
potentially dangerous reasons tend adopt small values gradient tend vanish 
certain tasks may require store precise values real numbers long durations time memory cell contents protected minor internal state drifts 
way circumvent drift problem choose appropriate function instance disadvantage unrestricted memory cell output range 
simple effective way solving drift problems learning initially bias input gate zero 
tradeoff magnitudes hand potential negative effect input gate bias negligible compared drifting effect 
appears need fine tuning initial bias logistic sigmoid activation functions precise initial bias hardly matters vastly different initial bias values produce near zero activations experiment section confirms precise initial bias important 
fact system learns generate appropriate input gate bias 
intra cellular backprop quite different context see doya 
experiments 
tasks appropriate demonstrate quality novel long time lag algorithm 
minimal time lags relevant input signals corresponding teacher signals long training sequences 
fact previous recurrent net algorithms manage generalize short training sequences long test sequences 
see pollack 
real long time lag problem short time lag exemplars training set 
elman training procedure bptt offline rtrl online rtrl fail miserably real long time lag problems 
see hochreiter mozer 
second important requirement tasks complex solved quickly simple minded strategies random weight guessing 
discussed paragraph 
guessing outperform long time lag algorithms 
discovered schmidhuber hochreiter tasks previous authors solved faster simple random weight guessing proposed algorithms 
instance easy guess solution bengio frasconi step parity problem 
requires classify sequences elements number odd 
target sequence odd 
ran experiment input unit hidden units output unit logistic activation functions sigmoid 
hidden unit receives connections input unit output unit output receives connections units units biased 
training set consists sequences class target class target 
correct sequence classification defined absolute error sequence 
guess solution repeatedly randomly initialize weights random weight matrix correctly classifies training sequences 
test test set 
compare results bengio results 
methods tested bengio 
sequences steps simulated annealing reported solve task trials method called discrete error bp took trials achieve final classification error 
bengio frasconi sequences steps em approach took trials achieve final classification error 
guessing solved problem trials mean trials final absolute test set errors 
similar examples described schmidhuber hochreiter 
course mean guessing algorithm 
just means previously problems inappropriate demonstrate quality previously proposed algorithms 
common experiments 
experiments experiment involve long minimal time lags short time lag training exemplars facilitating learning 
require free parameters inputs high weight precision random weight guessing infeasible 
line learning opposed batch learning logistic sigmoids activation functions 
experiments initial weights chosen range gamma experiments gamma 
training sequences generated randomly various task descriptions 
experiments sequence elements randomly generated line error signals generated sequence ends 
net activations reset processed input sequence 
comparisons recurrent nets taught gradient descent give results rtrl comparison includes bptt 
bptt see williams peng computes exactly gradient offline rtrl 
long time lag problems offline rtrl bptt online version rtrl activation resets online weight changes lead identical negative results confirmed additional simulations hochreiter see mozer 
offline rtrl online rtrl full bptt suffer badly exponential error decay 
lstm architectures selected quite arbitrarily 
course known complexity problem systematic approach start small net consisting memory cell 
try cells alternatively sequential network construction fahlman 
outline experiments 
ffl experiment focuses standard benchmark test recurrent nets embedded grammar 
allows training sequences short time lags long time lag problem 
include just provides nice example lstm output gates truly beneficial 
popular benchmark recurrent nets tried authors want include experiment conventional algorithms fail completely lstm clearly outperforms 
embedded grammar minimal time lags represent border case sense possible learn bridge conventional algorithms 
slightly longer minimal time lags impossible 
course interesting tasks remainder rtrl bptt solve reasonable time 
ffl experiment focuses noise free noisy sequences involving numerous input symbols distracting important ones 
difficult task task involves hundreds distractor symbols random positions minimal time lags steps 
lstm solves bptt rtrl fail case step minimal time lags see hochreiter mozer 
reason rtrl bptt omitted remaining complex experiments involve longer time lags 
ffl experiment addresses long time lag problems noise signal input line 
experiments focus bengio collaborators sequence problem 
problem solved quickly random weight guessing include difficult sequence problem requires learn real valued conditional expectations noisy targets inputs 
ffl experiment involves distributed continuous valued input representations requires learn store precise real values long time periods 
relevant input signals occur quite different positions input sequences 
minimal time lags involve hundreds time steps 
similar tasks solved recurrent net algorithms 
ffl experiment involves tasks different complex type solved recurrent net algorithms 
relevant input signals occur quite different positions input sequences 
experiment shows lstm extract information conveyed temporal order widely separated inputs 
ffl subsection summarize details lstm experiments tables 
experiment embedded grammar task 
task learn embedded grammar smith zipser cleeremans 
fahlman 
allows training sequences short time lags time steps long time lag problem 
include reasons popular recurrent net benchmark authors wanted experiment rtrl bptt fail completely course interesting tasks rtrl bptt solve 
provides nice example output gates truly beneficial 
starting leftmost node directed graph symbol strings generated sequentially empty string edges appending associated symbols current string rightmost node reached 
edges chosen randomly choice probability 
net task read strings symbol time transition diagram grammar 
grammar grammar transition diagram embedded grammar 
box represents copy grammar see 
permanently predict symbol error signals occur time step 
predict symbol net remember second symbol 
comparison 
compare lstm elman nets trained elman training procedure elm results taken cleeremans fahlman recurrent cascade correlation rcc results taken fahlman rtrl results taken smith zipser successful trials listed 
mentioned smith zipser task easier increasing probabilities short time lag exemplars 
didn lstm 
training testing 
local input output representation input units output units 
fahlman training strings separate test strings 
training set generated randomly 
training exemplars chosen randomly training set 
test sequences generated randomly sequences training set testing 
string presentation activations reinitialized zeros 
trial considered successful string symbols sequences test set training set predicted correctly output unit corresponding possible symbol active ones 
architectures 
architectures rtrl elm rcc reported listed 
lstm memory cell blocks 
block memory cells 
output layer incoming connections originate memory cells 
memory cell gate unit receives incoming connections memory cells gate units hidden layer fully connected connectivity may 
input layer forward connections units hidden layer 
gate units biased 
architecture parameters easy store input signals architectures employed obtain comparable numbers weights architectures 
parameters may appropriate 
sigmoid functions logistic output range range gamma range gamma 
allows pushing absolute memory cell outputs 
weights initialized gamma 
initial output gate biases gamma gamma gamma see abuse problem solution section 
tried different learning rates 
results 
different randomly generated pairs training sets test sets 
pair trials different weight initializations 
see table results mean trials 
methods lstm learns solve task 
ignore unsuccessful trials approaches lstm learns faster 
importance output gates 
experiment provides nice example output gate truly beneficial 
learning store perturb activations representing easily learnable transitions original grammar 
job output gates 
output gates achieve fast learning 
method hidden units weights learning rate success success rtrl fraction rtrl fraction elm rcc lstm blocks size lstm blocks size lstm blocks size lstm blocks size lstm blocks size table experiment embedded grammar percentage successful trials number sequence presentations success rtrl results taken smith zipser elman net trained elman procedure results taken cleeremans recurrent cascade correlation results taken fahlman new approach lstm 
weight numbers rows estimates corresponding papers don provide technical details 
lstm learns solve task failures trials 
ignore unsuccessful trials approaches lstm learns faster number required training examples final row varies 
experiment noise free noisy sequences task noise free sequences long time lags 
possible input symbols denoted gamma locally represented dimensional vector th component components 
net input units output units sequentially observes input symbol sequences time permanently trying predict symbol error signals occur single time step 
emphasize long time lag problem training set consisting similar sequences gamma gamma 
selected probability 
predict final element net learn store representation element time steps 
compare real time recurrent learning fully recurrent nets rtrl back propagation time bptt successful neural sequence chunker ch schmidhuber new method lstm 
cases weights initialized 
due limited computation time training stopped sequence presentations 
successful run fulfills criterion training successive randomly chosen input sequences maximal absolute error output units 
architectures 
rtrl self recurrent hidden unit non recurrent output units 
layer connections layers 
units logistic activation function sigmoid 
bptt architecture trained rtrl 
ch net architectures rtrl additional output predicting hidden unit see schmidhuber details 
lstm rtrl hidden unit replaced memory cell input gate output gate required 
logistic sigmoid identity function 
memory cell input gate added error stopped decreasing see abuse problem solution section 
results 
rtrl short time step delay trials successful 
trial successful 
long time lags neural sequence chunker new approach achieved successful trials bptt rtrl failed 
sequence chunker solved task trials 
lstm learned solve task 
comparing successful trials lstm learned faster 
see table details 
method delay learning rate weights successful trials success rtrl rtrl rtrl rtrl rtrl bptt ch lstm table task percentage successful trials number training sequences success real time recurrent learning rtrl back propagation time bptt neural sequence chunking ch new method lstm 
table entries refer means trials 
time step delays ch lstm achieve successful trials 
ignore unsuccessful trials approaches lstm learns faster 
task local regularities 
task chunker learns correctly predict final element predictable local regularities input stream allow compressing sequence 
additional difficult task involving different possible sequences remove compressibility replacing deterministic subsequence gamma random subsequence length gamma alphabet gamma obtain classes sets sequences gamma gamma gamma gamma gamma gamma 
sequence element predicted 
totally predictable targets occur sequence ends 
training exemplars chosen randomly classes 
architectures parameters success criteria experiment 
results 
expected chunker failed solve task bptt rtrl course 
new approach successful see success criterion 
average mean trials success achieved sequence presentations 
demonstrates new approach require sequence regularities 
task long time lags local regularities 
difficult task subsection 
possible input symbols denoted gamma called distractor symbols 
locally represented dimensional vector ith component components 
net input units output units sequentially observes input symbol sequences time 
training sequences randomly chosen union similar subsets sequences qg qg 
pick training sequence randomly generate sequence prefix length 
randomly generate sequence suffix additional elements probability alternatively probability 
conclude sequence depending nature second element 
leads uniform distribution possible sequences length 
minimal sequence length 
expected sequence length 
expected number occurrences element sequence goal predict symbol occurs trigger symbol error signals generated sequence ends 
predict final element net learn store representation second element time steps sees trigger symbol 
success defined prediction error final sequence element output units successive randomly chosen input sequences 
time lag gamma random inputs weights success table task lstm long minimal time lags lot noise 
number available distractor symbols number input units 
expected number occurrences distractor symbol sequence 
column lists number training sequences required lstm course bptt rtrl competitors chance solving non trivial tasks minimal time lags involving time steps 
number distractor symbols weights increase proportion time lag learning time increases slowly 
lower block illustrates expected slow due increased frequency distractor symbols 
architecture learning 
net input units output units 
weights initialized 
avoid learning time variance due different weight initializations hidden layer gets memory cells cell blocks size sufficient 
hidden units 
output layer receives connections memory cells 
memory cells gate units receive connections input units memory cells gate units hidden layer fully connected 
unit biased 
logistic sigmoid output range gamma logistic sigmoid output range gamma 
allows pushing absolute memory cell outputs 
learning rate 
note minimal time lag net sees short training sequences facilitating classification long test sequences 
results 
trials tested pairs 
table lists mean number training sequences required lstm achieve success course bptt rtrl chance solving non trivial tasks minimal time lags involving time steps 
scaling 
table shows number input symbols weights increase proportion time lag learning time increases slowly 
remarkable property lstm shared method aware 
rtrl bptt far scaling reasonably appear scale exponentially appear quite useless time lags exceed time steps 
distractor influence 
table column headed reflects expected frequency distractor symbols 
increasing frequency decreases learning speed 
effect due weight oscillations caused frequently observed input symbols 
experiment noise signal channel experiment addresses case noise signal input line 
focus bengio collaborators simple sequence problem 
experiment serves illustrate lstm encounter fundamental problems noise signal mixed input line 
experiment address interesting difficult sequence problem 
task sequence problem 
task observe classify input sequences 
classes 
class probability 
input line 
real valued sequence elements convey relevant information class 
sequence elements positions generated gaussian mean zero variance 
case sequence element class class 
case elements class class 
target sequence class class 
correct classification defined absolute output error sequence 
constant sequence length randomly selected difference bengio problem permit shorter sequences length 
sequence problem best method tested bengio 
multigrid random search sequence lengths precise stopping criterion mentioned solved problem sequence presentations final classification error 
bengio frasconi able improve results em approach reported solve problem trials 
guessing 
discovered sequence problem simple quickly solved random weight guessing 
ran experiment input unit hidden units output unit logistic activation functions sigmoid 
hidden unit sees input unit output unit output unit sees units units biased 
randomly guessing weights possible solve problem trials average 
bengio parameter architecture latch problem simple version sequence problem allows input tuning weight tuning problem solved trials average due tiny parameter space 
see schmidhuber hochreiter similar additional results 
lstm architecture 
layer net input unit output unit cell blocks size 
output layer receives connections memory cells 
memory cells gate units receive inputs input units memory cells gate units biased hidden layer fully connected 
sigmoid functions logistic 
gate units output unit sigmoid sigmoid gamma sigmoid gamma 
training testing 
weights bias weights gate units randomly initialized range gamma 
input gate bias initialized gamma second gamma third gamma 
output gate bias initialized gamma second gamma third gamma 
precise initialization values hardly matters confirmed additional experiments 
learning rate 
activations set zero new sequence 
training judge task solved criteria st sequences randomly chosen test set misclassified 
st st satisfied mean absolute test set error 
case st additional test set consisting randomly chosen sequences determine fraction misclassified sequences 
results 
see table 
results means trials different weight initializations range gamma 
lstm able solve problem 
far fast random weight guessing see paragraph guessing 
clearly trivial problem provide testbed compare performance various non trivial algorithms 
demonstrates lstm encounter fundamental problems faced signal noise channel 
task 
architecture parameters task gaussian noise mean variance added information conveying elements 
training judge task solved slightly redefined criteria st sequences randomly chosen test set misclassified 
st st satisfied mean absolute test set error 
case st additional test set consisting randomly chosen sequences determine fraction misclassified sequences 
results 
see table 
results represent means trials different weight initializations 
lstm easily solves problem 
interesting task 
architecture parameters task essential changes task non trivial targets class class respectively gaussian noise targets mean variance stdev st st weights st fraction misclassified table task bengio sequence problem 
minimal sequence length 
number information conveying elements sequence 
column headed st st provides number sequence presentations required achieve stopping criterion st st 
final column lists fraction misclassified post training sequences absolute error test set consisting sequences tested st achieved 
values means trials 
discovered problem simple random weight guessing solves faster lstm method know 
st st weights st fraction misclassified table task modified sequence problem 
table elements perturbed noise 

minimize mean squared error system learn conditional expectations targets inputs 
misclassification defined absolute difference output noise free target class class 
network output considered acceptable expected absolute difference noise free target output 
requires high weight precision 
tasks task quickly solved random guessing 
training testing 
learning rate 
training criterion sequences randomly chosen test set misclassified mean absolute difference noise free target output 
additional test set consisting randomly chosen sequences determine fraction misclassified sequences 
results 
see table 
results represent means trials different weight initializations 
despite noisy targets lstm solve problem learning real valued expected target values 
experiment adding problem difficult task section type solved recurrent net algorithms 
shows lstm solve long time lag problems involving distributed weights fraction misclassified av 
difference mean table task modified interesting sequence problem 
table noisy real valued targets 
system learn conditional expectations targets inputs 
final column provides average difference network output expected target 
tasks task quickly solved random weight guessing 
minimal lag weights wrong predictions success table experiment results adding problem 
minimal sequence length 
minimal time lag 
wrong predictions number incorrectly processed sequences error test set containing sequences 
row provides number training sequences required achieve stopping criterion 
values means trials 
number required training examples varies exceeding cases 
valued representations 
task 
element input sequence pair consisting components 
component real value randomly chosen interval gamma 
second component marker sequence task output sum components pairs marked second components equal 
value determine minimal sequence length randomly chosen integer sequence exactly pairs marked follows randomly select mark pairs component called 
randomly select mark gamma unmarked pairs component called 
second components remaining pairs zero final pair second components set zero rare case pair sequence got marked 
error signal generated sequence target sum scaled interval 
sequence processed correctly absolute error sequence 
architecture 
layer net input units output unit cell blocks size 
output layer receives connections memory cells 
memory cells gate units receive inputs memory cells gate units hidden layer fully connected connectivity may 
input layer forward connections units hidden layer 
non input units biased 
architecture parameters easy store input signals cell block size works 
sigmoidal functions logistic output range range gamma range gamma 
state drift versus initial bias 
note task requires store precise values real numbers long durations system learn protect memory cell contents minor internal state drifts see section 
study significance drift problem task difficult biasing non input units artificially inducing internal state drifts 
weights including bias weights randomly initialized range gamma 
section remedy state drifts input gate bias initialized gamma second gamma precise initialization values hardly matters confirmed additional experiments 
training testing 
learning rate 
training stopped average training error sequences processed correctly 
results 
test set consisting randomly chosen sequences average test set error incorrectly processed sequences 
table shows details 
experiment demonstrates lstm able distributed representations 
lstm able learn perform calculations involving continuous values 
system manages store continuous values deterioration minimal delays time steps significant harmful internal state drift 
experiment temporal order subsection lstm solves difficult tasks type solved recurrent net algorithms 
experiment shows lstm able extract information conveyed temporal order widely separated inputs 
task relevant widely separated symbols 
goal classify sequences 
elements represented locally binary input vectors non zero bit 
sequence starts ends trigger symbol consists randomly chosen symbols set fa dg elements positions sequence length randomly chosen randomly chosen randomly chosen 
sequence classes depend temporal order rules task relevant widely separated symbols 
goal classify sequences 
elements represented locally 
sequence starts ends trigger symbol consists randomly chosen symbols set fa dg elements positions sequence length randomly chosen randomly chosen randomly chosen randomly chosen 
sequence classes depend temporal order xs rules tasks error signals occur sequence 
sequence classified correctly final error output units 
architecture 
layer net input units cell blocks size task output units task 
non input units biased 
output layer receives connections memory cells 
memory cells gate units receive inputs input units memory cells gate units hidden layer fully connected connectivity may 
architecture parameters task easy store input signals 
sigmoid functions logistic output range range gamma range gamma 
training testing 
learning rate experiment 
training stopped average training error sequences classified correctly 
weights initialized range gamma 
input gate bias initialized gamma second gamma experiment third gamma precise initialization values hardly matter confirmed additional experiments 
results 
test set consisting randomly chosen sequences average test set error incorrectly classified sequences 
table shows details 
experiment shows lstm able extract information conveyed temporal order widely separated inputs 
instance task delays second relevant input second relevant input sequence time steps 
exemplary solutions 
experiment lstm distinguish temporal orders 
possible solutions store cell block second cell block 
occurs block see empty means recurrent connections 
block close input gate 
block filled closed fact visible block recall gate units memory cells receive connections non output units 
typical solutions require memory cell block 
block stores second occurs changes state depending stored symbol 
solution type exploits connection memory cell output input gate unit events cause different input gate activations occurs conjunction filled block occurs conjunction empty block 
solution type strong positive connection memory cell output memory cell input 
previous occurrence task weights wrong predictions success task task table experiment results temporal order problem 
wrong predictions number incorrectly classified sequences error output unit test set containing sequences 
row provides number training sequences required achieve stopping criterion 
results task means trials 
results task means trials 
represented positive negative internal state 
input gate opens second time output gate memory cell output fed back input 
causes represented positive internal state contributes new internal state twice current internal state cell output feedback 
similarly gets represented negative internal state 
summary experimental lstm details tables subsection provide overview important lstm parameters architecture details experiments 
task lag bias ff ga ga ga ga ga og id og id table summary experimental details lstm part st column task number 
nd column minimal sequence length rd column minimal number time steps relevant input information teacher signal 
th column number cell blocks th column block size th column number input units 
th column number output units 
th column number weights th column describes connectivity means output layer receives connections memory cells memory cells gate units receive connections input units memory cells gate units means layer receives connections layers 
th column initial output gate bias stands randomly chosen interval gamma og means output gate 
th column initial input gate bias see th column 
th column units biased 
stands unit biased stands non input units biased exception output units ga stands gate units biased stands non input units biased 
th column function id identity function 
logistic sigmoid gamma 
th column function logistic sigmoid logistic sigmoid gamma 
th column learning rate ff 
task select interval test set size stopping criterion success gamma training test correctly pred 
see text gamma test set exemplars ab gamma exemplars ab gamma exemplars ab gamma st st see text ab gamma st st see text ab gamma st st see text see text gamma st ab gamma st ab gamma st ab table summary experimental details lstm part ii 
st column task number 
nd column training exemplar selection stands randomly chosen training set stands randomly chosen classes stands randomly chosen line 
rd column weight initialization interval 
th column test set size 
th column stopping criterion training st fi stands average training error fi sequences processed correctly 
th column success criterion correct classification criterion ab fi stands absolute error output units sequence fi 
previous approaches elman fahlman williams pearlmutter schmidhuber related algorithms pearlmutter comprehensive overview suffer problems bptt see sections 
methods practicable short time gaps time delay neural networks lang plate method plate updates unit activations weighted sum old activations see de vries principe 
lin 
propose variants time delay networks called networks problems quickly solved simple weight guessing 
deal long time lags mozer uses time constants influencing activation changes 
long time gaps time constants need external fine tuning mozer 
sun alternative approach updates activation recurrent unit adding old activation scaled current net input 
net input tends perturb stored information long term storage impracticable 
bengio 
investigate methods simulated annealing multi grid random search time weighted pseudo newton optimization discrete error propagation 
latch sequence problems similar problem delay contain experimental details test set size precise success criteria 
report simulated annealing able solve problems perfectly random weight guessing solves simple problems faster see experiment 
bengio frasconi propose em approach propagating targets 
called state networks time system different states 
reported solve step parity problem requires different states quickly solved weight guessing see section 
solve adding problem section system need unacceptable number states state networks 
ring proposed method bridging long time lags 
unit network receives conflicting error signals adds higher order unit influencing appropriate connections 
approach extremely fast bridge time lag involving steps may require addition units 
ring net generalize unseen lag durations 
kalman filter techniques improve recurrent net performance 
reason believe kalman filter trained recurrent networks useful long minimal time lags 
fact derivative discount factor imposed decay exponentially effects past dynamic derivatives 
schmidhuber hierarchical chunker system capability bridge arbitrary time lags perfect predictability entire subsequence causing time lag see schmidhuber mozer 
instance postdoctoral thesis schmidhuber uses hierarchical recurrent nets rapidly solve certain grammar learning tasks involving minimal time lags excess time steps 
performance chunker systems deteriorates noise level increases 
course chunker systems augmented lstm combine advantages 
lstm method involves multiplicative units 
instance watrous kuhn multiplicative inputs second order nets 
differences lstm watrous kuhn architecture linear units enforce constant error flow 
designed solve long time lag problems 
fully connected second order sigma pi units lstm architecture multiplicative units gate units 
watrous kuhn algorithm costs operations time step 
costs 
see miller giles additional multiplicative inputs 
discovered simple weight guessing solves miller giles problems quickly algorithms investigate schmidhuber hochreiter 
discussion limitations lstm 
ffl particularly efficient truncated backprop version lstm algorithm won easily solve problems similar strongly delayed xor problems goal compute xor widely separated inputs previously occurred noisy sequence 
reason storing inputs won help reduce expected error task decomposable sense impossible incrementally reduce error solving easier subgoal corresponding modified weights building blocks complete solution 
theory limitation circumvented full gradient additional conventional hidden units receiving input memory cells 
increase computational complexity 
ffl memory cell block needs additional units input output gate 
comparison standard recurrent nets increase number weights factor 

conventional hidden unit replaced units lstm architecture 
fully connected case increases weight number factor note experiments quite comparable weight numbers architectures lstm competing approaches 
ffl generally speaking due constant error flow internal states memory cells lstm runs problems similar feedforward nets seeing entire input string 
instance tasks quickly solved random weight guessing truncated lstm algorithm small weight initializations step parity problem see section 
lstm problems similar ones feedforward net inputs trying solve bit parity 
lstm architecture trained lstm algorithm typically behaves feedforward net trained backprop sees entire input 
precisely clearly outperforms previous approaches non trivial tasks significant search spaces 
ffl lstm architecture algorithm problems notion recency go approaches 
gradient approaches suffer practical inability precisely count discrete time steps 
difference certain signal occurred steps ago additional counting mechanism necessary 
easier tasks requires difference say steps pose problems lstm 
instance generating appropriate negative connection memory cell output input lstm give weight inputs learn decays necessary 
advantages lstm 
ffl obvious advantage constant error backprop internal states memory cells resulting ability bridge long time lags 
ffl lstm works long time lag problems involving noise distributed representations continuous values 
lstm require priori choice finite number states 
ffl lstm generalizes cases positions widely separated relevant inputs input sequence matter 
previous approaches quickly learns distinguish widely separated occurrences particular element input sequence depending appropriate short time lag training exemplars 
ffl appears need parameter fine tuning 
lstm works broad range parameters learning rate input gate bias output gate bias 
instance readers learning rates experiments may large 
large learning rate pushes output gates zero automatically negative effects 
logistic sigmoid activation functions negative initial bias input output gates require fine tuning changing bias little effect activations derivatives range small derivatives logistic sigmoid 
ffl fully recurrent nets lstm algorithm update complexity time step essentially bptt excellent comparison approaches rtrl constant number output units complexity number units 
full bptt method local space 
ffl memory cells plugged feedforward recurrent net gradient algorithm plugged need chain rule lstm architecture algorithm seamlessly integrated feedforward recurrent nets trained rtrl bptt backprop gradient approaches 
authors hindsight lstm architecture simple obvious 
principle memory cell internal architecture guarantees constant error flow internal state 
provides basis bridging long time lags 
memory cell gate units designed learn open close access error flow memory cell 
multiplicative input gate allows protecting internal state perturbation irrelevant inputs 
likewise multiplicative output gate allows protecting units perturbation currently irrelevant memory contents 
long short term memory appears represent significant improvements previous neural long time lag methods 
tasks similar difficult artificial tasks discussed random weight guessing infeasible recommend lstm 

find practical limitations lstm intend apply real world data 
application areas include time series prediction music composition speech processing 
acknowledgments mike mozer brauer anonymous referees valuable comments suggestions helped improve previous version hochreiter schmidhuber 
supported dfg deutsche forschungsgemeinschaft 
appendix algorithm details follows index ranges output units ranges hidden units stands th memory cell block denotes th unit memory cell block stand arbitrary units 
gate unit logistic sigmoid range experiments exp gammax 
function range gamma experiments exp gammax gamma 
function range gamma experiments exp gammax gamma 
forward pass 
net input activation hidden unit net iu gamma net net input activation net gamma net net input activation net gamma net net input net internal state output activation th memory cell memory cell block net gamma gamma net net input activation output unit net gate ku gamma net backward pass 
describe backward pass particularly efficient truncated gradient version lstm algorithm see section 
truncated version approximates partial derivatives reflected tr signs notation 
truncates error flow wants leave memory cells gate units 
truncation ensures loops error left memory cell input input gate reenter cell output output gate 
turn ensures constant error flow memory cell internal state 
truncated backprop version derivatives replaced zero see section paragraph learning gamma tr gamma tr gamma tr 
get gamma net gamma tr gamma net gamma tr gamma gamma gamma gamma tr 
implies lm connections fc wlm gamma gamma wlm tr 
truncated derivatives output unit wlm net gate ku gamma lm ffi kl gamma tr net ffi kc gamma wlm gamma ffi ffi delta kc gamma lm hidden unit ki gamma wlm ffi kl gamma net gamma kc gamma wlm kc gamma wlm hidden unit ki gamma wlm ffi kronecker delta ffi ab size memory cell block truncated derivatives hidden unit part memory cell wlm net net wlm tr ffi li net gamma 
note possible full gradient affecting constant error flow internal states memory cells 
cell block truncated derivatives lm net net lm tr ffi net gamma 
wlm net net wlm tr ffi net gamma 
wlm gamma wlm lm net net net wlm tr ffi ffi gamma lm ffi wlm net ffi net net wlm ffi ffi gamma lm ffi wlm net ffi net gamma wlm wlm lm tr ffi wlm ffi ffi lm efficiently update system time truncated derivatives need stored time gamma gamma wlm computational complexity 
complexity cs sc kw number output units number memory cell blocks size memory cell blocks number hidden units maximal number units memory cells gate units hidden units cs cs number weights 
expression obtained considering computation derivatives output units respect weights sc number direct connections output unit csi number connections memory cells hi number connections hidden units ci number connections gate units 
single gate unit influences memory cells chain rule requires sum block size obtain complexity cis computing derivatives output unit respect connections leading gate units 
conclude memory cells fixed number output units lstm algorithm update complexity time step just bptt fully recurrent net units 
error flow compute error signal scaled flowing back memory cell time steps 
product analysis error flow memory cell internal state constant see section 
analysis highlights potential undesirable long term drifts see beneficial influence negatively biased input gates see 
truncated backprop learning rule obtain gamma gamma gamma gamma gamma gamma gamma net gamma delta gamma gamma net gamma delta net gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma net gamma delta gamma gamma net gamma delta net gamma gamma gamma gamma gamma gamma gamma tr tr sign indicates equality due fact truncated backprop replaces zero derivatives gammak gammak gamma gammak gammak gamma 
follows error starts flowing back output 
redefine ic 
definitions conventions section compute error flow truncated backprop learning rule 
error occuring output gate tr net 
error occuring internal state sc sc 
previous equation implies constant error flow internal states memory cells sc sc tr 
error occuring memory cell input net net net sc 
error occuring input gate tr net sc 
external error flow 
errors propagated back units unit outgoing connections weights lv external error note conventional units external error time net net 
get gamma gamma net gamma net gamma net gamma net gamma tr observe error arriving memory cell output backpropagated units external connections error flow memory cells 
focus error back flow memory cell 
type error flow allows bridging time steps 
suppose error arrives output time propagated back steps reaches memory cell input net 
scaled factor gammaq compute sc gamma tr sc sc gammaq sc gammaq sc gammaq 
expanding equation obtain gamma tr gamma sc gamma sc gamma tr gamma sc gamma gamma gamma tr ae net gamma gamma net gamma net gamma consider factors previous equation expression 
obviously error flow scaled times enters cell gamma leaves cell constant error flow internal state 
observe output gate effect scales errors reduced early training memory cell 
likewise scales errors resulting activating deactivating memory cell training stages instance output gate memory cell suddenly start causing avoidable errors situations control easy reduce corresponding errors memory cells 
see output weight conflict abuse problem sections 
large positive negative values drifted away time step gamma may small assuming logistic sigmoid 
see section 
drifts memory cell internal state easily negatively biasing input gate see section point 
recall section precise bias value hardly matters 
gamma net gamma small input gate negatively biased assume logistic sigmoid 
potential significance negligible compared potential significance drifts internal state factors may occasionally scale lstm error flow manner depends length time lag 
flow effective exponentially order decaying flow memory cells 
bengio frasconi 

credit assignment time alternatives backpropagation 
cowan tesauro alspector editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
bengio simard frasconi 

learning long term dependencies gradient descent difficult 
ieee transactions neural networks 
cleeremans servan schreiber mcclelland 

finite state automata simple recurrent networks 
neural computation 
de vries principe 

theory neural networks time delays 
lippmann moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
doya 

adaptive neural oscillator continuous time backpropagation learning 
neural networks 
elman 

finding structure time 
technical report crl technical report center research language university california san diego 
fahlman 

recurrent cascade correlation learning algorithm 
lippmann moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
hochreiter 

untersuchungen zu 
diploma thesis institut fur informatik lehrstuhl prof brauer technische universitat munchen 
hochreiter schmidhuber 

long short term memory 
technical report fakultat fur informatik technische universitat munchen 
lang waibel hinton 

time delay neural network architecture isolated word recognition 
neural networks 
lin horne tino giles 

learning long term dependencies difficult recurrent neural networks 
technical report umiacs tr cs tr institute advanced computer studies university maryland college park md 
miller giles 

experimental comparison effect order recurrent neural networks 
international journal pattern recognition artificial intelligence 
mozer 

focused back propagation algorithm temporal sequence recognition 
complex systems 
mozer 

induction multiscale temporal structure 
lippman moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
pearlmutter 

learning state space trajectories recurrent neural networks 
neural computation 
pearlmutter 

gradient calculations dynamic recurrent neural networks survey 
ieee transactions neural networks 
plate 

holographic recurrent networks 
hanson giles editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
pollack 

language induction phase transition dynamical recognizers 
lippmann moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 


nonlinear dynamical systems kalman filter trained recurrent networks 
ieee transactions neural networks 
ring 

learning sequential tasks incrementally adding higher orders 
hanson giles editors advances neural information processing systems pages 
morgan kaufmann 
robinson fallside 

utility driven dynamic error propagation network 
technical report cued infeng tr cambridge university engineering department 
schmidhuber 

neural bucket brigade local learning algorithm dynamic feedforward recurrent networks 
connection science 
schmidhuber 

fixed size storage time complexity learning algorithm fully recurrent continually running networks 
neural computation 
schmidhuber 

learning complex extended sequences principle history compression 
neural computation 
schmidhuber 

und 
institut fur informatik technische universitat munchen 
schmidhuber hochreiter 

guessing outperform long time lag algorithms 
technical report idsia idsia 
smith zipser 

learning sequential structures real time recurrent learning algorithm 
international journal neural systems 
sun chen lee 

time warping invariant neural networks 
hanson giles editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
watrous kuhn 

induction finite state languages second order recurrent networks 
neural computation 
williams 

complexity exact gradient computation algorithms recurrent neural networks 
technical report technical report nu ccs boston northeastern university college computer science 
williams peng 

efficient gradient algorithm line training recurrent network trajectories 
neural computation 
williams zipser 

gradient learning algorithms recurrent networks computational complexity 
back propagation theory architectures applications 
hillsdale nj erlbaum 

