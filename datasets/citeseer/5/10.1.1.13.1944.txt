modeling optimal probability prediction yong wang cs waikato ac nz department computer science university waikato new zealand ian witten cs waikato ac nz department computer science university waikato new zealand general modeling method optimal probability prediction observations model dimensionality determined natural product 
new method yields estimators establish theoretically optimal stated restrictions number free parameters infinite 
case study investigate problem fitting logistic models finite sample situations 
simulation results artificial practical datasets supportive 

proposed new approach fitting linear models called pace regression wang 
standard techniques problem include ordinary squares ols ols subset selection methods fpe aic akaike mallows bic mdl schwarz rissanen ric donoho johnstone foster george cic tibshirani knight shrinkage methods ridge regression nn breiman lasso tibshirani 
new approach adopts methodology resembles empirical bayes robbins shown rival generally outperform techniques terms predictive squared error 
determines dimensionality model natural product fitting process 
theoretically established new approach achieves predictive optimality number free parameters specified model approaches infinity 
minimizes squared error 
extends ideas problem predicting probabilities observations 
evaluate performance probability prediction kullback leibler distance distribution functions estimate probability density function pdf 
measure kl log log dx kullback leibler kullback 
appropriate surely kl 
xn iid 
goal modeling process intuitively appealing terms probability maximize left hand side infinite equivalent minimizing kullback leibler distance kl 
note involved expected probability expected log probability generally log density 
kl right hand side lies range 
maximum likelihood methodology closely related notion probability prediction wide range successful applications 
performs cases stuart 
example reduce dimension model datasets produces overfitted model poor predictive power 
new method maximum likelihood estimation 
show appropriate regularity conditions maximum likelihood estimator mle true pdf optimal sense minimizing kl potential estimators 
show constructively exhibiting better estimators including optimal estimator 
establish superiority new estimators asymptotic sense number free parameters number observations tends infinity 
ideas new methods empirical bayes methodology 
utilize mle known asymptotic normality property transform original parameters dummy ones 
form nonparametric mixture estimate observed values dummy parameters apply empirical bayes analysis minimize kl outline follows 
section presents main ideas core relationship optimal probability prediction empirical bayes estimation 
important issues arise including choice transformation finite sample considerations discussed section 
section investigates fitting logistic models case study gives actual results artificial practical datasets 
concluding remarks open issues 

optimal probability prediction section show build models predict probability optimally sense minimizing kullback leibler distance 
briefly review empirical bayes methodology extend estimate mean distribution apply idea general exhibit estimators optimal certain restrictions 
empirical bayes independent samples 
distributions may completely different known mle obtained joint distribution vector entry univariate mle example normal distribution mean 
adopt notation 
mle estimator inferior empirical bayes estimator eb dg dg denoting pdf corresponding inferior sense minimize expected squared error respect estimator 
iid 
mixing distribution mixture fg dg equation consistent estimator mixture sample robbins shows weak conditions eb minimizes bayes risk asymptotically optimal 
interpret result bayesian perspective 
estimator consistent surely 
just discrete function jumps distribution function resembles 
randomly sampled take fixed value situations 
estimate function 
interpretation leads result bayesian framework 
consistent estimators arbitrary available literature including mle laird lesperance minimum distance estimators choi kruse macdonald blum wang 
consistency means pr lim continuity point 
estimation distributions special case result occurs normal distribution function mean common known variance 
problem estimating single multivariate observation identity matrix 
generalize result problem estimating mean distribution purposes optimal probability prediction covariance matrix assumed known 
single observation 
problem transformed special case supposing qq matrix choice discussed section writing conforms previous formulation 
note transformation section distribution functions transformed parameters 
original ones 
upgraded equation formula section obtain upgraded estimate ultimately interested 
justify approach terms probability prediction 
denote pdf pdf estimate 
log log constant depending kl 
reduces problem optimal probability prediction minimizing expected squared errors terms dummy parameter expect 
empirical bayes estimator eb obtained asymptotically optimal sense minimizing kl 
suboptimal section 
note expectation entire sample space including observations sampled distribution 
expectation just training sample leads mle 
upgrading general results extend naturally case general mle upgraded specific distributions described previous subsections 
known regularity conditions mle distributed fisher information matrix 
likelihood function reduces exp sampling pdf mle 
course practice usually unknown accurately approximated consistent estimator estimated information observed information log needed follow derivation section 
terms exp respectively 
optimal certain restrictions section gave optimal probability prediction eb transformation criteria applied dummy parameter vector similar way example standard techniques mentioned section 
standard technique implies certain restrictions analogous estimators optimal restrictions outperform relevant standard technique 
course advantage provided restriction vanishes asymptotically eb optimum 
estimators originally derived fitting linear models wang readily carry case optimal probability prediction equivalent minimizing expected squared error see section 
consistent estimator define dg dt 
details see wang 
thresholding subject threshold retaining term discarding setting zero 
standard estimators belong category include ols fpe aic bic mdl log ric log cic log 
note equivalences asymptotic 
optimal threshold arg min wang 
thresh 
nested models brought model predefined sequence say 
loss generality 
optimal model type nested arg max 
subset models tested individually specifying threshold 
case sign criterion subset 
shrinkage shrunk constant 
shrinkage estimators include ridge regression nn lasso 
obtain shrink sgn eb min eb 

discussion important issues resolved yield practical modelling methods include choice transformation matrix defined section reduce dimensionality new estimators handle finite samples 
choice ideas section involve transforming original parameter vector dummy parameter vector empirical bayes method applied 
empirical bayes method applied directly transformation necessary ensure kl say minimized 
question left open qs satisfy qq qs include eigenvalue decomposition cholesky decomposition partial test orthogonal transformation 
note asymptotic optimality estimators section established condition applied estimators statistically independent response observed density probability 
kinds choice 
pick independent eigenvalue cholesky decomposition 
exploit information data partial test 
disadvantage choice may suit data 
disadvantage second fails independence condition hopefully slightly 
merits choice case dependent 
improvement depends diversity values diverse inform adjustment particular di ers radically little basis alter value 
chosen cluster possible 
practice course values unavailable consideration shed light choice special case dimensionality reduction shows 
dimensionality reduction estimators section explicitly reduce dimensionality model sense estimate entries zero 
include thresh nested subset discard values 
estimators eb shrink generally reduce dimensionality implicitly invariably turns entries eb shrink tiny truncated zero negligible impact kl reducing dimensionality paragraph certainly equivalent reducing dimensionality usually real interest practice 
general way reduce truncate values zero see truncation negligible ect 
way partial test obtain pivoting 
consecutive downwards zero close zero corresponding values zero close zero 
case illustrates advantage partial test choose ect zero close zero sifted corresponding transformed values clustered 
clustering ect upgrading mutually self supporting 
contrast arbitrarily chosen fails exploit information 
fact ect common practical datasets arbitrarily chosen perform cases 
improving probability prediction dimensionality reduction side benefits 
reducing number non zero parameters usually implies fewer measurement attributes faster prediction easily understood models 
decreases practical cost associated modelling operation cost di cult quantify di cult manipulate mathematically 
nested model selection likelihood ratio statistic asymptotic results finite samples 
serious problems arise 
fisher information matrix unknown estimate estimated information obtained full model inaccurate 
compromises foundations theoretical analysis 
second normality formulae poor approximations finite samples especially small ones 
blindly upgrading checking result data yield unexpected outcomes 
remedy likelihood ratio test statistic consecutively nested models denotes mle free parameters 
log likelihood 
selecting nested models 
fact distributed non central rests assumption asymptotic normality usually works finite samples 
asymptotically correspond transformation obtained likelihood ratio test 
mixing distribution function variable nested model selector carries readily terms 
wang describes handle mixtures non central 
applications techniques select nested models example pruning tree structured models breiman quinlan 

case study fitting logistic models time apply general results established special case logistic regression models simulation results 
logistic models logistic models class problems take form exp probability log likelihood function instances 
yn log log 
mle satisfies equation wx 
xn diag 
equation usually solved iteratively re weighted squares method newton raphson method fisher scoring method 
note fisher information matrix wx 
replaced estimated information wx obtain estimators defined section 
evaluate resulting models resort kullback leibler distance 
avoid numeric integration high dimensional spaces experiments monte carlo integration 
kullback leibler distance sample superscript kl log log 
easy evaluate true distribution function known artificial experiments 
unknown practice negative log likelihood test set log log 
equivalent kl constant 
logistic models natural application classification problems 
estimate attribute space split subspaces linear discriminant function 
instance classified 
class undetermined belongs class probability 
classification rate cr natural performance yardstick classification problems measures percentage correctly classified instances 
sampling space estimate cr indicator function denotes pdf 
versions classification rate sample kl depending known unknown cr cr 
simulation studies show results experiments fitting logistic models 
particular examine selection sequence nested models 
subset model mle deleting significant variable time 
process builds matrix implicitly 
investigate estimators ml maximum likelihood aic bic ric cic optimal nested model selector likelihood ratio statistic section denoted new 
optimal estimator derived sections included considerations explained section 
performance measures distance sample classification rate model dimension 
indicators probability prediction third interest dimensionality reduction 
second experiment true model unknown negative log likelihood substitute distance 
report results experiments 
artificial datasets accurate values performance measures obtained 
experiment shows strengths weaknesses modeling procedure di erent situations 
second experiment applies procedures artificial datasets cross validation results 
experiment artificial datasets 
experiment investigates ect non zero parameters true model artificial datasets 
dataset training test sets contains instances parameters intercept plus attribute associated parameters 
constant attribute 
parameter 
fraction set mean instances fall point 
shows cross section results situations 

data point graphs average runs conditions 
figures correspond kl cr fitted model dimension respectively 
figures show measures 
new method derived shown solid line 
apparent estimator new best best sense achieving minimal values kullback leibler distance kl methods performs consistently range ect observed classification rate cr maximized 
seen new reduces model dimensions appropriately range 
estimator achieves performance range works favorable circumstances 
example ml cic ric bic cic aic 
experiment practical datasets 
second experiment investigates performance methods practical datasets 
machine learning database repository uci blake wisconsin breast cancer original heart disease cleveland german statlog project german credit ionosphere pima pima indian diabetes breast cancer 
eighth crab dataset agresti 
datasets modified slightly attributes instances deleted eliminate missing values multi class problems transformed binary ones randomly chosen subset instances computational reasons 
table gives final number instances attributes dataset parentheses 
cases iteration maximum likelihood estimation converge replaced 
calculated slightly altered datasets 
true models unknown cross validation results calculated shown table terms performance measures negative log likelihood substitute kullback leibler distance classification rate cr model dimension 
value average runs fold cross validation 
negative log likelihood classification rate estimator new provides best nearly best results datasets 
results intermediate comparable estimators 
reduces model dimensionality mle 

summary new general approach probability prediction observations 
achieve optimal prediction determines appropriate dimensionality model natural byproduct optimality 
exhibited new estimators optimal optimal certain restrictions enforced 
cases optimality achieved number free parameters approaches infinity 
suggests performance tend improve number parameters increases new methods appropriate kullback leibler distance ml ric bic aic cic new classification rate ml ric bic aic cic new model dimension ml ric bic aic cic new kullback leibler distance ml ric bic cic aic new classification rate ml ric bic aic cic new model dimension ml ric bic aic cic new 
experiment 
upper row shows results lower row 
graphs row show kl cr model dimension respectively 
procedure crab german cr dim cr dim cr dim cr dim ml aic bic cic ric new ionosphere pima cr dim cr dim cr dim cr dim ml aic bic cic ric new table 
results practical datasets experiment 
high dimensional parameter spaces 
ideas resemble empirical bayes methodology adopt bayesian perspective assume existence random prior distribution 
conducted case study general approach fit logistic regression models 
theoretical asymptotic simulation results finite datasets artificial practical promising new method nearly best reduces model dimensions appropriately 
generality favourable theoretical properties new method shows great promise 
truly useful wide range application areas needed best choice transformation matrix improved approximations finite samples inclusion multi labeled enumerated variables handling missing values application model structures decision trees 
acknowledgments author alastair scott alan lee timely technical guidance phd research constant encouragement 
benefited greatly stimulating research environment provided weka project university waikato new zealand 
agresti 

categorical data analysis 
john wiley sons 
akaike 

fitting autoregressive models prediction 
ann 
inst 
statist 
math 
akaike 

information theory extension maximum likelihood principle 
nd international symposium information theory pp 

budapest 
blake keogh merz 

uci repository machine learning databases www ics uci edu mlearn mlrepository html 
department information computer science university california irvine 
blum 

estimation mixing distribution function 
ann 
probab 
lindsay 

man computer assisted analysis mixtures statistical algorithms 
biometrics 
breiman 

better subset regression nonnegative 
technometrics 
breiman friedman olshen stone 

classification regression trees 
wadsworth belmont ca 
choi 

estimation procedure mixtures distributions 
statist 
soc 

kruse 

construction sequences estimating mixing distribution 
ann 
math 
statist 
donoho johnstone 

ideal spatial adaptation wavelet shrinkage 
biometrika 
foster george 

risk inflation criterion multiple regression 
annals statistics 


ridge regression biased estimation nonorthogonal problems 
technometrics 
kullback 

information theory statistics nd ed 
new york dover 
reprinted ma peter smith 
kullback leibler 

information su ciency 
ann 
math 
statist 
laird 

nonparametric maximum likelihood estimation mixing distribution 
amer 
statist 
assoc 
lesperance 

algorithm computing nonparametric mle mixing distribution 
amer 
statist 
assoc 
macdonald 

comment choi 
statist 
soc 

mallows 

comments cp technometrics 
quinlan 

programs machine learning 
mateo calif morgan kaufmann publishers 
rissanen 

modeling shortest data description 
automatica 
robbins 

empirical bayes approach statistics 
proc 
third berkeley symposium math 
statist 
prob pp 

university california press 
robbins 

empirical bayes approach statistical decision problems 
ann 
math 
statist 
schwarz 

estimating dimension model 
annals statistics 
stuart ord arnold 

classical inference linear model volume kendall advanced theory statistics 
oxford university press 
tibshirani 

regression shrinkage selection lasso 
journal royal statistical society 
tibshirani knight 

covariance inflation criterion adaptive model selection 
journal royal statistical society series 
wang 

new approach fitting linear models high dimensional spaces 
phd thesis department computer science university waikato new zealand 
