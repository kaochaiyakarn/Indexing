information theoretic clustering dhillon cs utexas edu cs utexas edu modha ibm com ut cs technical report tr april dimensional contingency occurrence tables arise frequently important applications text web log market basket data analysis 
basic problem contingency table analysis clustering simultaneous clustering rows columns 
novel theoretical formulation views contingency table empirical joint probability distribution discrete random variables poses clustering problem optimization problem information theory optimal clustering maximizes mutual information clustered random variables subject constraints number row column clusters 
clustering algorithm monotonically increases preserved mutual information intertwining row column clusterings stages 
practical example simultaneous word document clustering demonstrate algorithm works practice especially presence sparsity 
clustering grouping similar objects practical importance wide variety applications text web log market basket data analysis 
typically data arises applications arranged contingency occurrence table occurrence table webpage user browsing data 
clustering algorithms focus way clustering cluster dimension table similarities second dimension 
example documents may clustered word distributions users may clustered visit similar web pages 
clearly applications desirable cluster simultaneously cluster dimensions contingency table 
clear duality clustering rows columns example just documents clustered word distributions words may clustered distribution occurrence documents 
natural approach problem treat normalized non negative contingency table joint probability distribution random variables 
information theory give theoretical formulation clustering problem optimal clustering leads minimum loss mutual information shown lemma loss non negative 
information theoretic formulation clustering quantifying loss mutual information novel algorithm directly optimizes loss function 
department computer sciences university texas austin tx 
department computer sciences university texas austin tx 
ibm almaden research center san jose ca 
resulting algorithm interesting row column clustering stages 
row clustering done assessing closeness row distribution relative entropy certain row cluster prototypes 
column clustering done similarly process iterated till converges local minimum 
clustering di ers ordinary sided clustering stages row cluster prototypes incorporate column clustering information vice versa 
empirically demonstrate clustering algorithm alleviates problems sparsity high dimensionality presenting results joint word document clustering 
resulting document clusters observed superior dimensional clustering results previously proposed information theoretic approaches cluster words clustering documents 
word notation upper case letters denote random variables 
elements sets denoted lower case letters quantities associated clusters example denotes random variable obtained clustering denotes cluster 
probability distributions denoted random variable obvious jx random variable explicit 
logarithms base 
motivation problem formulation discrete random variables take values sets fx xm fy yn respectively 
denote joint probability distribution think matrix 
practice known may estimated observations 
statistical estimate known dimensional contingency table way frequency table 
interested simultaneously clustering quantizing disjoint hard clusters disjoint hard clusters 
clusters written clusters written words interested nding maps cx cx fx xm fy yn brevity write cx random variables deterministic function respectively 
observe clustered separately function function 
partition functions cx allowed depend entire joint distribution 
de nition refer tuple cx clustering 
suppose clustering 
re order rows joint distribution rows mapping arranged rst followed rows mapping 
similarly re order columns joint distribution columns mapping arranged rst followed columns mapping 
row column reordering ect dividing distribution little dimensional blocks 
refer block cluster 
fundamental quantity measures amount information random variable contains vice versa mutual information 
judge quality clustering resulting loss mutual information non trivial lowers mutual information see lemma 
de nition optimal clustering minimizes subject constraints number row column clusters 
illustrate situation example 
consider matrix represents joint distribution looking row distributions natural group rows clusters fx fx fx similarly natural column clustering fy fy resulting joint distribution see veri ed mutual information lost due clustering clustering leads larger loss mutual information 
question eciently search clustering minimizes quantity 
lemma shows loss mutual information expressed distance approximation lemma facilitate search optimal clustering 
lemma xed clustering cx write loss mutual information jjq 
jj 
denotes kullback leibler divergence known relative entropy distribution form xj yj cx 
proof 
considering hard clustering de nition log log log log log log step follows xj cx similarly yj 
tu lemma shows loss mutual information non negative reveals nding optimal clustering equivalent nding approximating distribution form close kullback leibler divergence subject constraints number row column clusters 
section develop intuition approximation distribution preserves marginals cx xj yj similarly 
recall example natural row column clusterings led 
easy verify corresponding approximation de ned equals 
note marginals preserved section providing motivation theory source coding transmission 
set arti cial data compression problem want transmit source destination 
insist transmission done stages rst compute cx transmit cluster identi ers jointly separately transmit destination knows transmit destination knows rst step require average bits second step require average bits 
xed clustering average number bits transmitted source destination noting parallel easy show jjq nding optimal clustering equivalent nding code minimizes subject constraints number row column clusters 
observe contains captures interaction row column clusters 
underscores fact clustering rows columns interact clustering 
naive algorithm clusters rows paying attention columns vice versa critical interaction essence clustering 
related clustering literature focused sided clustering algorithms see comprehensive survey 
early clustering limited problems small sizes local greedy splitting procedure identify hierarchical row column clusters 
bipartite graph formulation spectral heuristic uses eigenvectors cluster documents words restriction word cluster associated document cluster 
impose restriction fact see section examples di erent types row column clusters 
information theoretic formulation preserving mutual information similar information bottleneck method :10.1.1.39.9882
information bottleneck method introduced sided clustering say tries minimize quantity order gain compression addition maximizing mutual information quantity considered parameter re ects tradeo compression preservation mutual information :10.1.1.39.9882
information bottleneck algorithm yields soft clustering data procedure similar deterministic annealing approach 
greedy agglomerative hard clustering version information bottleneck algorithm cluster words order reduce feature size supervised text classi cation :10.1.1.42.7488
task proposed divisive hard clustering algorithm directly minimizes loss mutual information result higher classi cation accuracies 
algorithms proposed sided clustering 
agglomerative hard clustering version information bottleneck algorithm cluster documents clustering words 
extended repetitively cluster documents words 
papers heuristic procedures cluster documents words independently agglomerative algorithm 
contrast rst quantify loss mutual information due clustering propose algorithm strong theoretical foundation monotonically reduces loss function converging local minimum 
considering general clustering framework bayesian belief networks proposed iterative optimization method amounts multivariate generalization uses deterministic annealing :10.1.1.39.9882
agglomerative algorithm problem advantages simpler fully deterministic non parametric 
need identify cluster splits tricky 
pointed agglomeration procedures scale linearly sample size top methods 
concerned principled top method scales 
results amount rst nding word clustering followed nding document clustering procedure word document clusterings continually improves suitable local minima true clustering procedure 
clustering algorithm describe novel algorithm monotonically decreases objective function 
describe algorithm related proofs convenient think joint distribution denote distribution 
observe loss generality write yj lemma purpose clustering seek approximation distribution form xj yj reader may want compare observe de ned combinations note cx zero 
think dimensional marginal dimensional marginal 
intuitively cluster denoted seek approximate distribution distribution form 
proposition state proof establishes harm adopting formulation 
proposition xed hard clustering jjq jjq rst establish simple useful equalities highlight properties desirable approximating proposition distribution form marginals conditionals preserved xj xj yj yj yj yj xj xj furthermore cx yj yj yj yj symmetrically xj xj xj xj proof 
equalities marginals simple show proved brevity 
equalities easily follow 
equation follows yj yj yj yj yj yj equation follows xj yj xj yj yj yj yj yj yj yj equality follows 
tu lemma quanti es loss mutual information clustering kullback leibler divergence 
proposition prove lemma expresses loss mutual information revealing ways 
lemma lead natural algorithm 
lemma loss mutual information expressed weighted sum relative entropies row distributions jx row lumped distributions ii weighted sum relative entropies column distributions jy column lumped distributions jjq cx jx jjq cy jy jjq proof 
show rst equality second follows similarly 
jjq cx cy log cx cy yjx log yjx yj cx yjx log yjx yj follows cx yjx 
tu signi cance lemma follows fact allows express objective function solely terms row clustering terms column clustering 
furthermore allows de ne distribution row cluster prototype similarly distribution column cluster prototype 
intuition clustering algorithm 
algorithm works follows 
starts initial clustering iteratively re nes obtain sequence clusterings 
associated generic clustering sequence compute distributions yj xj yj observe function variables depends iteration index marginal fact independent write xjy yjx respectively xjy yjx 
step algorithm starts initial clustering computes required marginals resulting approximation choice starting points important discussed detail section 
algorithm computes appropriate row cluster prototypes 
reader may wish think centroids observe centroid xj jx xj denotes number rows cluster write yj yj yj 
note gives formula dicult guess priori help analysis 
step algorithm re assigns row new row cluster row cluster prototype closest jx kullback leibler divergence 
essence step de nes new row clustering 
observe column clustering changed step algorithm clustering input joint probability distribution desired number row clusters desired number column clusters 
output partition functions 
initialization set 
start initial partition functions compute xj distributions 

compute row clusters row nd new cluster index argmin jx jjq resolving ties arbitrarily 

compute xj distributions xj 

compute column clusters column nd new cluster index argmin xjy jjq xj resolving ties arbitrarily 

compute xj distributions 

return change objective function value jjq jjq small say set go step 
information theoretic clustering algorithm simultaneously clusters rows columns 
step new row clustering old column clustering algorithm recomputes required marginals importantly algorithm recomputes column cluster prototypes 
ordinary centroids write xj xj xj cx 
step algorithm re assigns column new column cluster column cluster prototype closest jy kullback leibler divergence 
step de nes new column clustering holding row clustering xed 
step algorithm re computes marginals algorithm keeps iterating steps desired convergence condition met 
reassuring theorem main result guarantees convergence 
theorem algorithm clustering monotonically decreases objective function lemma 
proof 
jjq yjx log yjx yj yjx log yjx yjx log yjx yj yj yjx log yjx yj yjx log yj yjx log yj log yj yj log yj yj log yj yjx log yjx yj yj yjx log yjx yj yj jjq follows lemma follows step algorithm follows rearranging sum follows step algorithm follows non negativity kullback leibler divergence follows hold column clusters xed due lemma 
virtually identical argument omit brevity properties steps show jjq jjq combining follows iteration algorithm increases objective function 
tu corollary algorithm terminates nite number steps cluster assignment locally optimal loss mutual information decreased re assignment distribution jx jy di erent cluster distribution respectively de ning new distribution existing clusters 
proof 
result follows theorem number distinct clusterings nite 
tu closer examination proof shows established steps imply steps imply 
show generalize convergence guarantee class iterative algorithms 
particular algorithm uses arbitrary concatenations steps steps guaranteed monotonically decrease objective function 
example consider algorithm ips coin iteration performs steps coin turns head performs steps 
example consider algorithm keeps iterating steps improvement objective function noticed 
keep iterating steps improvement objective function noticed 
iterate steps forth 
algorithms algorithms spirit guaranteed monotonically decrease objective function 
algorithmic exibility allow computationally ecient exploration various local starting xed initial random partition step 
algorithm spirit means algorithm precise algorithm quite di erent usual means 
example algorithm distribution serves row cluster prototype 
quantity di erent naive centroid cluster similarly column cluster prototype di erent obvious centroid cluster fact detailed analysis evident proof theorem necessary identify key quantities 
simplicity restricted attention clustering joint distributions random variables 
algorithm main theorem easily extended cluster multi dimensional joint distributions 
illustrative example illustrate algorithm works showing discovers optimal clustering example distribution section 
table shows typical run clustering algorithm starts random partition rows columns 
iteration table shows steps algorithm clustering resulting approximation original distribution corresponding compressed distribution 
row column cluster numbers shown matrix indicate clustering stage 
notice intertwined row column clustering leads progressively better approximations original distribution 
iterations algorithm accurately reconstructs original distribution discovers natural row column partitions recovers ideal compressed distribution 
pleasing property observe iterations preserves marginals original 
experimental results section provides empirical evidence show bene ts clustering framework algorithm 
particular apply algorithm task document clustering occurrence data joint probability distribution 
show clustering steps steps steps table algorithm clustering gives progressively better clusterings approximations till optimal discovered example section 
dataset newsgroups included docs total group documents binary binary subject talk politics mideast talk politics misc multi multi subject comp graphics rec motorcycles rec sports baseball sci space talk politics mideast multi subject alt atheism comp sys mac hardware misc rec autos rec sport hockey sci crypt sci electronics sci med sci space talk politics gun table datasets data set consists documents randomly sampled respective news groups ng corpus 
approach overcomes sparsity yielding substantially better results approach clustering sparse data single dimension 
show better results compared previous algorithms 
algorithms greedy technique cluster documents words clustered greedy approach 
brevity notation denote various algorithms consideration 
call information bottleneck double clustering method ib double iterative double clustering algorithm idc 
addition clustering denote document clustering word clustering clustering single dimension 
data sets experimental results various subsets newsgroup data ng smart collection cornell ftp ftp cs cornell edu pub smart 
ng data set consists approximately newsgroup articles collected evenly di erent usenet news groups 
data set testing supervised text classi cation tasks un supervised document clustering tasks 
news groups share similar topics documents cross posted making boundaries news groups fuzzy 
comparison consistent previous algorithms reconstructed various subsets ng 
applied pre processing steps subsets removed words ignored le headers selected top words mutual information speci details subsets table 
smart collection consists medline cisi cranfield sub collections 
medline consists abstracts medical journals cisi consists abstracts information retrieval papers cranfield consists abstracts aerodynamic systems 
removing words numeric characters selected top words mutual information part pre processing 
refer data set classic 
implementation details bow library code useful writing text analysis language modeling information retrieval programs 
extended bow implement clustering document clustering matlab give spy plots matrices 
data sets di er pre processing steps 
includes subject lines 
prepared di erent data sets subject lines subject lines 
evaluation measures validating clustering results non trivial task 
relevance clustering varies domain domain 
presence true labels case data sets experiments form confusion matrix measure ectiveness algorithm 
entry confusion matrix represents number documents cluster belong true class objective evaluation measure micro averaged precision 
idea rst associate cluster dominant class label cluster 
confusion matrix suciently diagonal ectively calculates fraction documents diagonal total number documents 
class data set de ne number documents correctly assigned number documents incorrectly assigned number documents incorrectly assigned micro averaged precision de ned micro averaged recall de ned note uni labeled data 
results discussion demonstrate clustering signi cantly better clustering single dimension word document occurrence matrices 
experiments know number true document clusters give input algorithm 
example case binary data set ask document clusters 
vary number word clusters full range possibilities give plots showing clustering behaves varying number word clusters 
note initialization deterministic initialization word clusters choosing initial word cluster distributions maximally far apart random perturbation mean document initialize document clusters :10.1.1.157.392
initialization random component results averages trials stated 
shows confusion matrices obtained classic data set algorithms clustering clustering word clusters 
observe clustering extracted original clusters correctly resulting micro averaged precision clustering led micro averaged precision 
clustering clustering clustering accurately recovers original clusters classic data set 
shows confusion matrices obtained clustering document clustering binary binary subject data sets 
clustering achieves micro averaged precision data sets respectively clustering yielded 
show clustering discover structure sparse word document matrices 
shows original word document matrix reordered matrix obtained arranging rows columns cluster order reveal various clusters 
simplify gure binary binary subject clustering clustering clustering clustering clustering obtains better clustering results compared dimensional document clustering binary binary subject data sets nz nz sparsity structure binary subject word document occurrence matrix left right clustering reveals underlying structure various clusters 
shaded regions represent non zero entries 
nal row clusters clustering ordered ascending order cluster purity distribution entropy 
notice clustering reveals hidden sparsity structure various clusters data set 
word clusters highly indicative individual document clusters inducing block diagonal sub structure dense sub blocks bottom right panel show word clusters uniformly distributed document clusters 
observed similar sparsity structure data sets 
shows precision values vary number word clusters data set 
binary binary subject data sets maximum precision word clusters multi multi subject word clusters multi multi subject word clusters respectively 
di erent data sets achieve maximum di erent number word clusters 
general selecting number clusters start non trivial model selection task scope 
shows fraction mutual information lost clustering varying number word clusters data set 
note correlation figures lower loss mutual information better clustering 
example binary subject data set highest precision achieved word clusters loss mutual information lowest 
sake clarity omitted error bars figures variation values minimal plotted obscured original gures 
shows typical run clustering algorithm multi data set 
notice objective function value loss mutual information decreases monotonically 
observed clustering converges quickly iterations data sets 
shows micro averaged precision measures di erent data sets di erent algo number word clusters log scale binary binary subject multi multi subject multi multi subject micro averaged precision values varying number word clusters clustering di erent ng data sets 
number word clusters log scale binary binary subject multi multi subject multi multi subject fraction mutual information lost varying number word clusters di erent ng data sets 
rithms 
column clustering data set chosen peak precision value values di erent number word clusters 
report peak ib double idc precision values respectively 
data sets clustering performs better clustering clearly indicating utility clustering words documents simultaneously 
clustering signi cantly better ib double comparable idc supporting hypothesis word clustering alleviate problem clustering high dimensions 
observe including subject lines data improves precision values considerable amount 
document clustering main objective experiments clustering algorithm returns word clusters 
interesting application apply clustering occurrence matrices true labels available dimensions 
give example show word clusters obtained clustering meaningful representative document clusters 
shows word clusters obtained clustering multi subject data set total word clusters document clusters obtained 
clusters appear represent individual news groups word number iterations objective value loss mutual information multi objective function value loss mutual information monotonically decreases number iterations typical run clustering algorithm multi data set 
clustering clustering ib double idc binary binary subject multi multi subject multi multi subject clustering obtains better micro averaged precision values di erent news group data sets compared algorithms 
cluster containing words indicative single newsgroup 
correlates cluster block diagonal sub structure observed 
additionally clusters contained non di erentiating words clustering single cluster appears help clustering overcoming noisy dimensions 
provided information theoretic formulation clustering simple computationally ecient principled algorithm row column clusterings stages guaranteed reach local minima nite number steps 
examples motivate new concepts illustrate ecacy algorithm 
particular word document matrices arise information retrieval known highly sparse :10.1.1.38.4937
sparse data interested document clustering believe clustering ective plain clustering just documents 
reason clustering employed ectively word clusters underlying features individual words 
amounts implicit dimensionality reduction noise removal leading far better clusters 
side bene clustering leads better word annotation document clusters 
dod graphics space israel army ride season image nasa working rear players mac shuttle running riders scored ftp ight occupied museum color algorithm rights drive fans cd orbital palestinian visit teams package satellite post display budget cpu data srb civil plain starters format prototype mass word clusters obtained clustering multi subject data set 
clusters represent rec motorcycles rec sport baseball comp graphics sci space talk politics mideast news groups respectively 
cluster top words sorted mutual information shown 
clustering may directly useful sparse matrix re ordering non negative matrices widely scienti parallel computation 
simplicity restricted attention clustering joint distributions random variables algorithm main theorem easily extended cluster multi dimensional joint distributions computationally ecient manner 
assumed number row column clusters pre speci ed 
formulation information theoretic hope information theoretic regularization procedure mdl may allow select number clusters data driven fashion 
assumed dealing hard clusterings row column belong cluster 
seek extension formulation algorithm soft clustering allows weighted memberships multiple clusters 
interesting open research question seek generalization clustering multivariate clustering procedure applicable complex interactions variables clustered clusters 
acknowledgments 
part research supported nsf career award 
aci 
baker mccallum 
distributional clustering words text classi cation 
acm sigir pages 
acm august 
bradley fayyad reina 
scaling clustering algorithms large databases 
th international conference knowledge discovery data mining 
aaai press 
thomas cover joy thomas 
elements information theory 
john wiley sons new york usa 
dhillon 
clustering documents words bipartite spectral graph partitioning 
proceedings th acm sigkdd international conference knowledge discovery data mining kdd pages 
dhillon fan guan 
ecient clustering large document collections 
grossman kamath kegelmeyer kumar editors data mining scienti engineering applications pages 
kluwer academic publishers 
invited book chapter 
dhillon kumar 
divisive information theoretic feature clustering algorithm text classi cation 
journal machine learning research jmlr special issue variable feature selection march 
dhillon modha :10.1.1.38.4937
concept decompositions large sparse text data clustering 
machine learning january 
el yaniv 
iterative double clustering unsupervised semi supervised learning 
luc de raedt peter flach editors proceedings ecml th european conference machine learning pages freiburg de 
springer verlag heidelberg de 
fisher 
interpretation contingency tables calculation royal stat 
soc 
friedman slonim tishby 
multivariate information bottleneck 
uncertainty arti cial intelligence proceedings seventeenth conference uai pages san francisco ca 
morgan kaufmann publishers 
hartigan 
direct clustering data matrix 
journal american statistical association march 
jain dubes 
algorithms clustering data 
prentice hall englewood cli nj 
ken lang 
news learning lter netnews 
proc 
th int conf 
machine learning pages san francisco 
mccallum nigam 
comparison event models naive bayes text classi cation 
aaai workshop learning text categorization 
mccallum 
bow toolkit statistical language modeling text retrieval classi cation clustering 
www cs cmu edu mccallum bow 
rose 
deterministic annealing clustering compression classi cation regression related optimization problems 
proc 
ieee 
slonim friedman tishby 
agglomerative multivariate information bottleneck 
advances neural information processing systems nips 
slonim tishby 
document clustering word clusters information bottleneck method 
acm sigir pages 
slonim tishby 
power word clusters text classi cation 
rd european colloquium information retrieval research ecir 
tishby pereira bialek :10.1.1.39.9882
information bottleneck method 
proc 
th annual allerton conference communication control computing pages 

