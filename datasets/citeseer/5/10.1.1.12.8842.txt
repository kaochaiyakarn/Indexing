fast sparse gaussian process methods informative vector machine neil lawrence university eld portobello street eld dp neil dcs shef ac uk matthias seeger university edinburgh forrest hill edinburgh eh ql seeger dai ed ac uk ralf herbrich microsoft research thomson avenue cambridge cb fb microsoft com framework sparse gaussian process gp methods uses forward selection criteria informationtheoretic principles previously suggested active learning 
goal learn sparse predictors evaluated number training points perform training strong restrictions time memory requirements 
scaling method large real world classification experiments show match prediction performance popular support vector machine svm significantly faster training 
contrast svm approximation produces estimates predictive probabilities error bars allows bayesian model selection complex implementation 
gaussian process gp models powerful non parametric tools approximate bayesian inference learning 
comparison popular nonlinear architectures multi layer perceptrons behavior conceptually simpler understand model fitting achieved resorting non convex optimization routines 
training time scaling memory scaling number training points hindered widespread 
related non probabilistic support vector machine svm classifier renders results comparable gp classifiers prediction error fraction training cost 
possible tasks solved satisfactorily sparse representations data set 
svm triggered finding representations particular loss function encourages degree sparsity final predictor depends fraction training points crucial discrimination task 
call utilized points active set sparse predictor 
case svm classification active set contains support vectors points closest svm classifier trained minimizing regularized loss functional process interpreted approximation bayesian inference 
decision boundary misclassified ones 
active set size smaller svm classifier trained average case running time memory requirements significantly note restrictions data distribution rise ort overcome scaling problems range sparse gp approximations proposed :10.1.1.25.1089
fully achieved goals nontrivial approximation non sparse gp model matching svm prediction performance run time 
algorithm proposed accomplishes objectives experiments show significantly faster training svm 
furthermore time memory requirements may restricted priori 
potential benefits retaining probabilistic characteristics method numerous hard problems feature model selection dealt standard techniques bayesian learning 
approach builds earlier lawrence herbrich extend considering randomized greedy selections focusing alternative representation gp model facilitates generalizations settings regression multi class classification 
section introduce gp classification model method approximate inference 
section contains derivation fast greedy approximation description associated algorithm 
section large scale experiments mnist database comparing method directly svm 
close discussion section 
denote vectors matrices bold face sets row column indices respectively denote corresponding submatrix furthermore abbreviate density gaussian distribution mean covariance matrix denoted 
diag represent overloaded operator extracts diagonal elements matrix vector produces square matrix diagonal elements vector elements 
gaussian process classification assume sample 
xn yn drawn independently identically distributed unknown data distribution 
goal estimate typical learn predictor small error data 
model situation introduce latent variable separating classification noise model cumulative distribution function standard gaussian bias parameter 
bayesian viewpoint relationship random process gaussian process gp model gp prior mean function covariance kernel 
prior encodes belief observing data finite set 
corresponding latent outputs 
jointly gaussian mean covariance matrix gp models non parametric general finite dimensional bold symbol vector matrix denote components corresponding normal symbols focus binary classification framework applied straightforwardly regression estimation multi class classification 
parametric representation 
possible write linear function feature space associated sense gaussian prior induces gp distribution linear function 
feature map covariance function written 
linear function view predictors separating hyper planes frequently svm community 
general infinite dimensional uniquely determined kernel function denote sequence latent outputs training points 
xn covariance kernel matrix bayesian posterior process computed principle bayes formula 
noise model non gaussian case binary classification handled tractably usually approximated gaussian process ideally preserve mean covariance function 
easy show equivalent fitting moments finite dimensional marginal posterior training points gaussian approximation conditional posterior non training point identical conditional prior 
general computing infeasible authors proposed approximate global moment matching iterative schemes locally focus training pattern time 
schemes simplest forms result parametric form approximating gaussian exp 
may compared form true posterior shows obtained likelihood approximation 
borrowing graphical models vocabulary factors called sites 
initially 
order update parameters site replace corresponding true likelihood factor resulting non gaussian distribution mean covariance matrix computed 
allows approximate gaussian new moment matching 
site update called inclusion active set factorized form likelihood implies new old di er parameters site useful locality property scheme referred assumed density filtering adf 
special case adf gp models proposed 
sparse gaussian process classification simplest way obtain sparse gaussian process classification gpc approximation adf scheme leave site parameters 
active set succeed important choose decision boundary classes represented essentially accurately training set 
exhaustive search possible subsets course intractable 
follow greedy approach suggested including new patterns time selection pattern include computing score function generalization adf expectation propagation ep allows iterations data 
context sparse approximations allows remove points exchange outside consider moves 
algorithm informative vector machine algorithm require desired sparsity diag diag diag 

repeat compute 
argmax updates 
update matrices diag 

points 
subset thereof picking winner 
heuristic implement considered context active learning see chapter score example decrease entropy inclusion :10.1.1.31.4284
result locality property adf fact gaussian easy see entropy di erence new proportional log ratio variances marginals new 
heuristic referred di erential entropy score favors points inclusion leads large reduction predictive posterior variance corresponding site 
whilst selection heuristics argued utilized turns di erential entropy score simple likelihood approximation leads extremely cient competitive algorithm 
remainder section describe method give schematic algorithm 
detailed derivation discussions extensions 
diag 
current active set components zero algebra woodbury formula gives lower triangular cholesky factor order compute di erential entropy score point know including active set need update diag accordingly turn requires matrices kept date 
update equations 
update new appending row new appending row 
diag new diag new lp di erential entropy score computed variables log computed algorithm give algorithmic version scheme 
inclusion costs dominated computation apart computation kernel matrix column total time complexity nd 
storage requirement nd dominated bu er diag error expected log likelihood current predictor remaining points computed 
scores order decide points include final kernel functions constant diagonal selection heuristic constant patterns inclusion candidate chosen random 
training complete predict test points evaluating approximate predictive distribution du may compute back substitution factor approximate predictive distribution obtained averaging noise model gaussian 
optimal predictor approximation sgn independent variance 
simple scheme employs full greedy selection remaining points find inclusion candidate 
sensible early inclusions computationally wasteful ones important extension basic scheme allows randomized greedy selections 
maintain selection index 
times 
having included modify selection index means components diag updated requires columns exhibits inertia moving 
columns kept date 
implementation employ simple delayed updating scheme columns avoids double computations see details 
number initial inclusions done full greedy selection fixed size modification rule fraction retain best scoring points fill size drawing random 

experiments results experiments mnist handwritten digits database comparing method svm algorithm 
considered binary tasks form rest 

mapped 
sampled bitmaps size split mnist training set new training set size validation set size test set size 
run consisted model selection training testing results averaged runs 
employed rbf kernel exp hyper parameters process variance inverse squared length scale 
model selection done minimizing validation set error training random training set subsets size 
available online www research att com yann exdb mnist index html 
model selection training set run tested methods 
list kernel parameters considered selection size methods 
svm ivm gen time gen time table test error rates gen training times time binary mnist tasks 
svm support vector machine smo average number svs 
ivm sparse gpc randomized greedy selections final active set size 
figures means runs 
goal compare methods performance running time 
svm chose smo algorithm fast elaborate kernel matrix cache see details 
ivm employed randomized greedy selections fairly conservative settings 
binary digit classification task unbalanced bias parameter gpc model chosen non zero 
simply fixed ratio patterns training set added constant kernel account variance bias hyper parameter 
ideally chosen model selection initial experiments di erent values exhibited significant fluctuations validation errors 
ensure fair comparison initial svm runs initialized active set size average number runs svs independently re ran svm experiments allowing cache space 
table shows results 
note ivm shows comparable performance svm achieving significantly lower training times 
conservative settings randomized selection parameters speed ups realizable 
registered shown significant fluctuations training time svm runs stable priori predictable ivm 
ivm obtain estimates predictive probabilities test points quantifying prediction uncertainties 
produced hardest task reject fractions test set examples size 
svm size discriminant output quantify predictive uncertainty heuristically 
clearly inferior di erence pronounced simpler binary tasks 
svm community common combine rest classifiers obtain multi class discriminant follows test point decide class associated classifier highest real valued output 
ivm selections random full greedy selection index size retained fraction 
looked powerful combination schemes error correcting codes 
rejected fraction svm ivm plot test error rate increasing rejection rate svm dashed ivm solid task rest 
svm reject distance separating plane ivm estimates predictive probabilities 
ivm line runs svm line exhibiting lower classification errors identical rejection rates 
equivalent compare estimates log predictor pick maximizing suboptimal di erent predictors trained jointly 
estimates log depend predictive variances measure uncertainty predictive mean properly obtained svm framework 
combination scheme results test errors ivm svm 
comparing results literature recall experiments images sub sampled size usual 
discussion demonstrated sparse gaussian process classifiers constructed ciently greedy selection simple fast selection criterion 
focused change di erential entropy experiments simple likelihood approximation basis method allows equally cient criteria information gain :10.1.1.31.4284
method retains benefits probabilistic gp models error bars model combination interpretability faster memory cient training prediction 
comparison non probabilistic svm classification method enjoys advantages simpler implement having strictly predictable time requirements 
method significantly faster svm smo algorithm 
due fact smo active set typically fluctuates heavily training set large fraction full kernel matrix evaluated 
contrast ivm requires straightforward obtain ivm joint gp classification model training costs raise factor factor reduced sensible approximations open question 
expect svms catch tasks require fairly large active sets simple fast covariance functions appropriate sparse input patterns 
proposed sparse gp approximations method closely related :10.1.1.25.1089
sparse bayesian online scheme employ greedy selections uses accurate likelihood approximation expense slightly worse training time scaling especially compared randomized version 
requires specification rejection threshold dependent ordering training points 
incorporates steps remove points done straightforwardly scheme moves create numerical stability problems 
smola bartlett likelihood approximation di erent ivm scheme gp regression greedy selections contrast expensive selection heuristic score computation forced randomized greedy selection small selection indexes 
di erential entropy score previously suggested context active learning applies directly problem :10.1.1.31.4284
active learning label known time scored expected actual entropy changes considered 
furthermore mackay applies selection multi layer perceptron mlp models gaussian posterior approximations weights poor :10.1.1.31.4284
acknowledgments chris williams david mackay manfred opper helpful discussions 
ms gratefully acknowledges support research studentship microsoft research manfred opper 
sparse online gaussian processes 
comp 
neil lawrence ralf herbrich 
sparse bayesian compression scheme informative vector machine 
nips workshop kernel methods 
david mackay :10.1.1.31.4284
bayesian methods adaptive models 
phd thesis california institute technology 
thomas minka 
family algorithms approximate bayesian inference 
phd thesis mit january 
manfred opper ole winther 
gaussian processes classification mean field algorithms 
comp 
john platt 
fast training support vector machines sequential minimal optimization 
scholkopf editor advances kernel methods pages 

matthias seeger neil lawrence ralf herbrich 
sparse bayesian learning informative vector machine 
technical report department computer science eld uk 
see www dcs shef ac uk neil papers 
alex smola peter bartlett 
sparse greedy gaussian process regression 
advances nips pages 
michael tipping 
sparse bayesian learning relevance vector machine 
learn 
res 
volker tresp 
bayesian committee machine 
comp 
christopher williams matthias seeger 
nystrom method speed kernel machines 
advances nips pages 
