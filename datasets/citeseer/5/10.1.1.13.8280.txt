learning labeled unlabeled data label propagation zhu zoubin ghahramani june cmu school computer science carnegie mellon university pittsburgh pa investigate unlabeled data help labeled data classi cation 
propose simple iterative algorithm label propagation propagate labels dataset high density areas de ned unlabeled data 
give analysis algorithm show solution connection algorithms 
show learn parameters minimum spanning tree heuristic entropy minimization algorithm ability feature selection 
experiment results promising 
keywords arti cial intelligence learning pattern recognition models statistical pattern recognition design methodology classi er design evaluation general terms algorithms additional key words semisupervised learning label propagation 
labeled data essential supervised learning 
available small quantities unlabeled data may abundant 
unlabeled data labeled data theoretical practical interest 
proposed approaches see :10.1.1.28.850
promising family methods analogous nearest neighbor knn traditional supervised learning 
methods assume closer data points tend similar class labels 
result propagate labels dense unlabeled data regions 
propose new algorithm propagate labels 
formulate problem particular form label propagation node labels propagate neighboring nodes proximity 
clamp labels labeled data 
labeled data act sources push labels unlabeled data 
prove convergence algorithm analyze behavior datasets 
propose minimum spanning tree heuristic entropy minimization criterion learn parameters show algorithm learn detect irrelevant features 
label propagation problem setup labeled data yl fy class labels 
assume number classes known classes labeled data 
unlabeled data yu fy unobserved usually fx problem estimate yu yl transductive learning setting 
intuitively want data points close similar labels 
create fully connected graph nodes data points labeled unlabeled 
edge nodes weighted closer nodes local euclidean distance larger weight ij weights controlled parameter ij exp ij exp choices distance metric possible may appropriate example positive discrete 
chosen focus euclidean distance report allowing di erent dimension corresponding length scales processes 
nodes soft labels interpreted distributions labels 
labels node propagate nodes edges 
larger edge weights allow labels travel easier 
de ne probabilistic transition matrix ij ij kj ij probability jump node de ne label matrix ith row representing label probability distribution node initialization rows corresponding unlabeled data points important 
ready algorithm 
algorithm label propagation algorithm follows 
propagate ty 
row normalize 
clamp labeled data 
repeat step converges 
step nodes propagate labels including self loop step 
step row normalize maintain label probability interpretation 
step critical want persistent label sources labeled data 
letting initially labeled nodes fade away replenish clamping label distribution ic probability mass concentrated class 
constant push labeled nodes class boundaries pushed high density data settle low density gaps 
structure data ts classi cation goal algorithm unlabeled data help learning 
convergence show algorithm converges simple solution 
step combined ty row normalized matrix ij ij ik yl matrix formed top rows labeled data yu matrix remaining rows 
notice yl change clamping solely interested yu split th row th column sub matrices ll lu ul uu shown algorithm yu uu yu ul yl leads yu lim uu uu ul yl initial need show uu 
construction elements greater zero 
row normalized uu sub matrix follows uu ij uu ij uu ik uu ik uu ik row sums uu converges zero means uu 
initial point inconsequential 
obviously yu uu ul yl xed point 
unique xed point solution iterative algorithm 
gives way solve label propagation problem directly iterative propagation 
solution obtained solving sparse system linear equations truncate small elements uu ecient 
parameter setting set parameter heuristic 
nd minimum spanning tree data points euclidean distances ij kruskal algorithm kru 
node connected 
tree growth edges examined short long 
edge added tree connects separate components 
process repeats graph connected 
nd rst tree edge connects components di erent labeled points 
regard length edge heuristic minimum distance class regions 
arbitrarily set rule normal distribution weight edge close hope local propagation classes 
rebalancing class proportions classi cation purpose yu computed take ml class unlabeled point label 
way control nal proportion classes implicitly determined distribution data 
appropriate classes separated labeled data abound 
case incorporating constraints class proportions improve nal classi cation 
assume class proportions pc estimated labeled data known priori oracle 
propose post processing alternatives ml class assignment class mass normalization computing yu normalize class mass class proportion constraint 
class mass column sums yu denoted yu yu scale column yu yu pc label point class maximum element row scaling 
approach guarantee strict label proportion 
label bidding class labels sale computing yu view point having bids yu ic class bids processed high low 
assuming yu ic currently highest bid 
class labels remain label sold point point quits bidding 
bid yu ic ignored second highest bid processed 
label bidding guarantees strict label proportions met 
experimental results demonstrate properties algorithm investigate synthetic datasets real world classi cation problem 
shows synthetic dataset classes narrow horizontal band 
data points randomly drawn classes 
labeled points unlabeled points 
expected knn algorithm ignores band structure dataset algorithm takes advantage propagates labels bands 
example mst heuristic ml classi cation 
similarly shows synthetic dataset classes intertwined dimensional springs 
labeled points unlabeled points 
knn fails notice structure unlabeled data algorithm nds springs 
mst ml classi cation 
data knn label propagation bands dataset 
labeled data color symbols unlabeled data dots 
knn ignores unlabeled data structure label propagation uses 
data knn label propagation springs dataset 
real world example test label propagation handwritten digits dataset 
original dataset cedar bu alo binary digits database 
digits preprocessed reduce size digit image sampling gaussian smoothing 
interpolation created gray scale pixel values range cbd 
digits experiment classes 
class images total 
image represented dimensional vector 
shows random sample images dataset 
perform label propagation random labeled unlabeled splits dataset measure classi cation error rates unlabeled data 
vary labeled data size 
labeled data size perform trials 
trial randomly sample labeled data dataset rest images unlabeled data 
class absent sampled labeled set redo sampling 
labeled unlabeled data approximately iid 
run minimum spanning tree algorithm split nd 
trials close 
speed computation top neighbors considered image constructing transition matrix measure error rates 
ml labels see section 

class mass normalization post processing maximum likelihood estimate class proportions labeled data 

lbe label bidding post processing maximum likelihood estimate class proportions labeled data 

class mass normalization post processing knowledge oracle true class proportions 

label bidding post processing oracle class proportions 
alternative algorithms baselines 
rst standard knn 
report nn error rate best 
second baseline algorithm propagating nn nn unlabeled data nd point closest labeled point call 
label label add labeled set repeat 
nn crude version label propagation 
performs synthetic datasets results figures 
shows results 
small ml labeling worse nn ml better 
rebalance class proportions better 
estimate class proportions labeled data class frequency perform class mass normalization improve performance small 
note required classes training set 
biases class frequency uniform happens true proportion example 
especially true explains initial ramp 
priori knowledge true class proportion unlabeled data performance better label bidding slightly superior class mass normalization 
noted extra information employed baseline algorithms 
hand label bidding requires exact proportions performance bad class proportions estimated 
summarize label bidding best exact proportions known class mass normalization best 
nn consistently performs better nn 
table lists error rates nn nn ml 
entry averaged trials 
di erences statistically signi cant level test pairs bold face 
parameter learning label propagation ect label propagation minimum spanning trees heuristic set parameter 
see di erent values ect label propagation 
shows synthetic nn nn ml nn nn ml table average error rate trials di erent post processing methods sizes labeled data 
dataset bridge consists upper block lower block connected thin bridge 
upper block grid side lower looser grid side 
points bridge distance 
labeled points 
dataset simulates separated classes corrupted points bridge 
nn algorithm propagate label upper block rst block denser 
reaching bridge label travels competes label lower block 
result label takes majority territory 
shows nn susceptible small noise dataset 
standard nn algorithm nds decision boundary bisecting labeled points regardless structure data 
shows results label propagation di erent ml classi cation 
label propagation result approaches nn exponential weights uence nearest point dominates 
easy see unlabeled points similar class probabilities class frequency labeled data 
dataset essentially shrinks single point large unlabeled points receive uence labeled points 
appropriate 
learn 
data likelihood sense criterion setting clamped course labeled points labels determined nearby unlabeled points 
intuitively pay attention unlabeled data assigned labels 
minimize entropy result ij ij log ij sum entropy individual data points 
function 
intuition label points con dently minimize arbitrary labelings unlabeled data low entropy suggest criterion 
important point arbitrary low entropy labelings achieved propagating labels algorithm 
fact nd space low entropy labelings achievable label propagation small lends tuning parameters 
complication remains minimum notice approaches nn nn gives point hard label shows 
nn desirable xed smoothing transition matrix see 
smoothing transition matrix inspired analysis pagerank algorithm smooth interpolating uniform transition matrix ij place algorithm 
interpolation parameter 
smoothed transition matrix uniform component dominates outside self loop 
class probabilities close uniform unlabeled data resulting high entropy hand small original dominates results unsmoothed version 
shows curve entropy vs smoothing di erent values 
value 
derivative introduced nuisance parameter order learn advantage apparent introduce multiple parameters dimension 
weight ij exp analogous relevance length scales gaussian process 
gradient descent nd parameters minimizes nd derivatives chain rule ic ic ic log ic value ic read matrix yu yu uu ul yl uu ul yl uu ul uu uu uu ul yl uu ul uu uu uu ul fact dx dx uu ul sub matrices remember row normalized ij ij tmn tmn ik ij tmn ij ij ik smoothed transition matrix eq 
tmn tmn original transition matrix eq 
tmn tmn st st rn tmn st wmn tmn sn rn nally st st examples learning irrelevant dimensions nd optimal datasets figures assuming single 
bands dataset start minimum spanning tree heuristic nd optimal 
similarly springs dataset start heuristic nd optimal 
classi cation remains experiments section ml classi cation 
mst heuristic separated datasets close optimal 
multiple bridge dataset start third distance upper lower blocks 
substantial amount gradient descent iterations reach 
increasing asymptote 
classi cation result 
local minimum problem example start minimum spanning tree heuristic 
increasing expected 
bridge dataset horizontal dimension corresponding irrelevant classi cation 
large means labels propagated freely dimension 
algorithm detects dimension irrelevant classi cation 
illustrate create synthetic dataset ball data points uniformly sampled inside dimensional unit hypersphere gap dimensions 
gap dimensional projection dataset 
gap splits dataset classes dimensions irrelevant classi cation 
labeled points 
start reach 
dramatic bridge dataset large compared radius data signify irrelevance dimensions 
classi cation optimal obeys gap 
show method merely looking structures unlabeled data consider similar dataset ball ball gap dimensions 
gap dimensions 
unlabeled data point view dimensions interesting dimensions 
gap dimensions related classi cation 
information hinted labeled points 
start reach 
method thinks dimensions irrelevant data clustered dimensions 
classi cation follows gap dimensions 
related proposed label propagation algorithm closely related markov random walks algorithm sj 
utilize manifold structure de ned large amount unlabeled data assume structure correlated classi cation goal 
de ne probabilistic process labels transit nodes 
markov random walks algorithm approaches problem di erent perspective 
uses transition process compute step node random walk node probability node steps 
understand algorithm helpful imagine node separate labels hidden observable 
node observable label average nodes hidden labels weighted 
fact kernel regression kernel step 
hidden labels learned likelihood margin observed labels labeled data optimized 
algorithm sensitive time scale node looks equally ancestor observable labels 
algorithm labeled data constant sources push labels system achieves equilibrium 
resemblance label propagation mean eld approximation pa 
label propagation convergence equations unlabeled data ic ij jc ij jc consider labeled unlabeled data graph conditional markov random eld pairwise interaction ij nodes labeled nodes clamped 
unlabeled node states denoted vector called 
probability particular con guration pf exp ij ij see related mincut algorithm 
show label propagation approximately mean eld solution markov random eld approximates markov random eld structure pairwise interactions higher order interactions 
speci cally de ned pf exp log ij ij rst order pf exp ij ij exp ij ij pf nd mean eld approximation replace node state mean value obtain self consistent equations exp log ij ij leads mean eld solution ic ij jc ij jc approximation sense assume ik replace ij ij 
nd label propagation approximately mean eld approximation view easy compare label propagation graph mincut algorithm bc :10.1.1.19.3957
mincut algorithm nds minimum cut graph separate labeled data di erent classes 
corresponds state con guration minimum energy equivalently state con guration markov random eld label propagation seen nds state con guration approximate mean eld solution subtle di erence algorithms 
dataset perfectly symmetric labeled points 
mincut label middle band equally state con gurations blu 
label propagation spirit mean eld approximation split middle band classifying points upper half lower half low con dence 
addition mincut limited binary labels label propagation 
semi supervised learning algorithms label propagation labeled data xed change iterations 
argue reinforce mistakes labeled data noisy 
true label propagation assumes noise free labels 
assumption important able insist labels carried away unlabeled data distribution 
practice labeled dataset small relatively easy sure correctly labeled 
interesting open question con dent label clamp soft distribution yl express uncertainty 
related attempted boltzmann machine learning markov random eld learn labeled unlabeled data optimizing length scale parameters likelihood criterion labeled points zg 
summary discussion proposed label propagation algorithm learn labeled unlabeled data 
labels propagated combination random walk clamping 
showed solution process connection methods 
showed learn parameters 
various semi supervised learning algorithm kind label propagation works structure data distribution revealed abundant unlabeled data ts classi cation goal 
investigate better ways rebalance class proportions applications entropy minimization criterion learn propagation parameters real datasets possible connections di usion kernel kl 
sam roweis roni rosenfeld teddy guy lebanon jin rong jing liu helpful discussions 
sam roweis provided handwritten digits dataset 
rst author supported part microsoft research graduate fellowship 
bc blum chawla :10.1.1.19.3957
learning labeled unlabeled data graph mincut 
proc 
th international conf 
machine learning 
blu blum 
personal communication 
cbd le cun boser denker henderson 
howard howard jackel 
handwritten digit recognition back propagation network 
advances neural information processing systems ii denver 

jonathan hull 
database handwritten text recognition research 
ieee transactions pattern analysis machine intelligence 
michael jordan zoubin ghahramani tommi jaakkola lawrence saul 
variational methods graphical models 
machine learning 
kl kondor la erty 
di usion kernels graphs discrete input spaces 
proc 
th international conf 
machine learning 
kru kruskal 
shortest spanning subtree graph traveling salesman problem 
proceedings american mathematical society volume pages 
andrew ng alice zheng michael jordan 
link analysis eigenvectors stability 
international joint conference arti cial intelligence ijcai 
pa carsten peterson james anderson 
mean eld theory learning algorithm neural networks 
complex systems 
see matthias seeger :10.1.1.28.850
learning labeled unlabeled data 
technical report university edinburgh 
sj martin szummer tommi jaakkola 
partially labeled classi cation markov random walks 
nips 
zg zhu zoubin ghahramani 
semi supervised classi cation markov random elds 
technical report carnegie mellon university 
random sample nn ml ml labels nn labels err nn lbe lbe labels nn labels err nn labels digits dataset 
shows randomly sampled images 
show error rates di erent post processing methods 
point average random trials 
error bars standard deviation 
best oracle class proportion knowledge best 
data nn nn unsmoothed smoothing bridge dataset 
results label propagation di erent values smoothing 
shows entropy vs curve label propagation smoothing di erent values 
dim data dim dim dim data dim dim dim result dim dim dim result dim ball dataset 
dimensions deemed irrelevant lack structure unlabeled data 
result optimal learned gradient descent 
dim data dim dim dim data dim dim result dim dim dim result dim ball dataset 
structure dimensions 
labeled points indicate irrelevance dimensions 
data label propagation result label propagation symmetric dataset 
middle band splits label close uniform probabilities 
graph mincut algorithm generate labels middle band 

