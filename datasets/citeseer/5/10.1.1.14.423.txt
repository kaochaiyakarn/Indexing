appear proceedings ninth acm sigkdd international conference knowledge discovery data mining kdd washington dc august adaptive duplicate detection learnable string similarity measures mikhail bilenko raymond mooney department computer sciences university texas austin austin tx mooney cs utexas edu problem identifying approximately duplicate records databases essential step data cleaning data integration processes 
existing approaches relied generic manually tuned distance metrics estimating similarity potential duplicates 
framework improving duplicate detection trainable measures textual similarity 
propose employ learnable text distance functions database field show measures capable adapting specific notion similarity appropriate field domain 
learnable text similarity measures suitable task extended variant learnable string edit distance novel vector space measure employs support vector machine svm training 
experimental results range datasets show framework improve duplicate detection accuracy traditional techniques 
categories subject descriptors database management database applications data mining artificial intelligence learning keywords data cleaning record linkage distance metric learning trained similarity measures string edit distance svm applications 
databases frequently contain field values records refer entity syntactically identical 
variations representation arise typographical errors misspellings abbreviations integration multiple data sources 
variations particularly pronounced data automatically extracted unstructured semi structured documents web pages 
approximate duplicates deleterious effects including preventing data mining algorithms discovering important regularities 
problem typically handled tedious manual data cleaning de process 
previous addressed problem identifying duplicate records referred record linkage permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
sigkdd august washington dc usa copyright acm 
merge purge problem duplicate detection hardening soft databases matching clustering matching :10.1.1.28.8405:10.1.1.12.4018:10.1.1.34.4329:10.1.1.1.5288
typically standard string similarity metrics edit distance vector space cosine similarity determine values records alike duplicates :10.1.1.27.7690
investigated pairing functions combine multiple standard metrics 
estimate similarity strings vary significantly depending domain specific field consideration traditional similarity measures may fail estimate string similarity correctly 
token level certain words informative comparing strings equivalence 
example ignoring substring street may acceptable comparing addresses comparing names people nick street newspapers wall street journal 
character level certain characters consistently replaced omitted syntactic variations due systematic typographical ocr errors 
accurate similarity computations require adapting string similarity metrics field database respect particular data domain 
hand tuning distance metric field propose trainable similarity measures learned small corpora labeled examples adapt different domains 
string similarity measures 
utilizes expectation maximization em algorithm estimating parameters generative model string edit distance affine gaps 
string similarity measure employs support vector machine svm obtain similarity estimate vector space model text 
distance best suited shorter strings minor variations measure vector space representation appropriate fields contain longer strings global variations 
system marlin multiply adaptive record linkage induction employs level learning approach 
string similarity measures trained database field provide accurate estimates string distance values field 
final predicate detecting duplicate records learned similarity metrics applied individual fields 
utilize support vector machines task show outperform decision trees classifier prior :10.1.1.1.5288
evaluate approach real world data sets containing duplicate records show marlin lead improved duplicate detection accuracy traditional techniques 

learnable string distance background traditional methods calculating string similarity roughly separated groups character techniques vector space techniques 
rely character edit operations deletions insertions substitutions subsequence comparison transform strings vector representation similarity computations conducted 
best known character string similarity metric levenshtein distance defined minimum number insertions deletions substitutions necessary transform string 
needleman wunsch extended model allow contiguous sequences mismatched characters gaps alignment strings described general dynamic programming method computing edit distance 
commonly gap penalty calculated affine model cost cost opening gap cost extending gap length gap alignment strings assuming characters unit cost 
usually set value lower decreasing penalty contiguous mismatched substrings 
differences duplicate records arise abbreviations word insertions deletions model produces sensitive similarity estimate levenshtein distance 
string distance affine gaps strings lengths computed dynamic programming algorithm constructs matrices computational time recurrences construct matrices denotes cost edit operation aligns th character string th character string insertion deletion costs respective characters 
matrix represents minimum cost string alignment ends matched characters matrices represent alignments gap strings min min min min character metrics estimating distance strings differ due typographical errors abbreviations computationally expensive accurate larger strings 
differences equivalent strings expressed multiple words added deleted transposed character metrics may assign high cost non matching string segments producing low similarity scores strings word level differences 
vector space model text avoids problem viewing strings bags tokens disregarding order tokens occur strings :10.1.1.27.7690:10.1.1.27.7690
vocabulary possible tokens corpus strings represented sparse dimensional vectors real numbers nonzero component corresponds token string 
tf idf popular method computing weights vector representation takes account token frequencies particular string entire corpus 
similarity strings computed normalized dot product vector space representations cosine angle sim vectors representing strings highly sparse vectorspace cosine similarity efficient implemented properly provides reasonable shelf metric larger strings text documents 
learnable distance metrics learnable string edit distance affine gaps different edit operations varying significance different domains 
example digit substitution major difference street address effectively changes house number single letter substitution semantically insignificant caused typo abbreviation 
adapting string edit distance particular domain requires assigning different weights different edit operations 
prior yianilos developed generative model levenshtein distance expectationmaximization algorithm learns model parameters training set consisting matched strings 
propose similar stochastic model edit distance affine gaps 
distance strings corresponds particular alignment strings characters may include non matching regions 
alignment viewed sequence pairs corresponding strings characters indicates gap 
character pair represents edit operation substitution insertion deletion denoting gap matched characters thought special case substitution alignment represents sequence edit operations transform string associated cost 
affine gap alignment modeled simple stochastic transducer shown fig 
matrices original model correspond accumulated forward probabilities alignment ends states 
particular alignment strings generated model sequence traversals edges 
traversal accompanied emission character pair sampled probability distribution state reached traversal 
generative model string distance affine gaps string edit distance affine gaps obeys triangle inequality metric strict mathematical sense true normalized dot product 
process duplicate detection requires measure accurately ranks pairs strings respect relative similarities measures satisfy triangle inequality satisfactory 
restrict discussion measures obey triangle inequality term metric synonym measure 
alphabet symbols full set edit operations es ed es set substitution matching operations ed sets insertion deletion operations respectively 
production starts state terminates special state reached 
transitions matching state deletion state insertion state correspond gap alignment strings 
gap ended edge traversed back matching state 
remaining state edge corresponds sequence substitutions exact matches remaining states analogous extending gap second string 
sum transition probabilities normalized state model complete 
edit operations emitted state correspond aligned pairs characters substitutions exact matches state deletions string state insertions characters second string string state edit operation assigned probability 
edit operations higher probabilities produce character pairs aligned domain substitution phone numbers deletion addresses 
generative model similar amino acid sequences important differences transition probabilities distinct states transition probability parameter associated expressed transitions outgoing state 
strings length length probabilities generating pair prefixes suffixes computed dynamic programming standard forward backward algorithms tv time 
corpus matched strings corresponding pairs duplicates 
tn vn model trained variant baum welch algorithm shown fig expectation maximization procedure learning parameters generative models 
training procedure iterates steps step expected number occurrences state transition edit operation emission accumulated pair strings training corpus 
achieved accumulating posterior probabilities possible state transition accompanying character pair emission 
maximization procedure model parameters updated collected expectations 
pseudo code algorithms 
proved training procedure guaranteed converge local maximum likelihood observing training corpus trained model estimating distance strings computing probability generating aligned pair strings summed possible paths calculated forward backward algorithms log obtaining posterior probability 
numerical underflow may occur computation performed long strings problem resolved mapping computations logarithmic space periodic scaling values matrices 
practical considerations order strings aligned matter similarity database records estimated insertion deletion operations transitions states represented single set parameters input set equivalent strings output set parameters edit distance ne gaps minimizes distance method convergence expectation step forward backward algorithms accumulate expected number occurrences edit operation align 
maximization step update transition emission probabilities expected numbers occurrences re normalize 
training algorithm generative string distance affine gaps symbols generative model fig suffers drawbacks impede utility computing similarity strings database 
problem lies fact model assigns probability strings exact duplicates 
probability alignment monotonically decreases matching characters appended strings longer exact duplicates penalized severely shorter exact duplicates counter intuitive exacerbates problem 
second difficulty lies fact due large size edit operation set probabilities individual operations significantly smaller transition probabilities 
relatively small number training examples available probabilities edit operations may underestimated distances assigned strings vary significantly minor character variations 
steps taken address issues 
probability distribution set edit operations smoothed bounding edit operation probability reasonable minimum value 
second learned parameters generative distance model mapped operation costs additive model negative logarithm probability 
distance calculated analogously eq addition supplemental costs log gap log continuing substitute match characters 
equivalent calculating cost viterbi alignment strings generative model log space 
solve nonzero exact match problem decrease high variance distances due edit operation costs compared transition costs cost producing matching pairs characters set zero 
distance obtained model normalized sum string lengths correct increasing distance longer exact duplicates problem 
resulting metric viewed hybrid generative model original fixed cost model 
learnable vector space similarity tf idf weighting scheme useful similarity computations attempts give tokens weights proportional tokens relative importance 
true contribution token similarity domain specific 
example suppose duplicate detection conducted database contains street addresses 
records addresses street street name token example th lower weight due occurrences corpus resulting lower idf value 
time addresses may contain token square assigned approximately weight th may common 
addresses street similar addresses squares generic tfidf cosine similarity incapable making distinction 
training data form labeled pairs duplicate strings available learn similarity function give weight tokens indicators actual similarity 
rewrite eq form sim component sum corresponds th token vocabulary 
expression thought th component dimensional vector classified 
vector space similarity computation strings viewed consecutive steps dimensional pair vector created component corresponds product weights corresponding token vocabulary ii summation performed equivalent pair instance classified perceptron unit weights belonging equivalent string class output different string class output 
perceptron output value corresponds confidence prediction 
provided training data form equivalent string pairs different string pairs possible train classifier step ii produce outputs correspond desired categorization string pairs 
trained classifier able distinguish features informative similarity judgments adapting computation domain specific notion similarity contained training data 
classifier appropriate task able learn limited training sets high dimensional data 
produce meaningful confidence estimates correspond relative likelihood belonging equivalent string different string classes different string pairs 
key requirement hypothesis learned classifier independent relative sizes positive negative training sets proportion duplicate pairs training set higher actual database duplicates detected 
support vector machines satisfy requirements 
svms classify input vectors implicitly mapping kernel trick high dimensional space classes equivalent string pairs different string pairs separated hyperplane 
output classifier distance image pair vector highdimensional space separating hyperplane 
distance provides intuitive measure similarity pair vectors classified large margin belonging correspond pairs strings highly similar classified large margin belonging represent dissimilar pairs strings 
output svm form kernel function lagrangian coefficient corresponding th training pair vector obtained solution quadratic optimization problem 
svm training algorithms independent dimensionality feature space margin training data affects hypothesis choice guaranteed overfit solution training pairs 
methods obtaining calibrated posterior probabilities svm output obtain probabilistically meaningful similarity function point 
framework concerned obtaining correct ranking pairs respect true similarities distance hyperplane provides additional model fitting 
svm output function bounded specific range output values depends choice kernel function training set 
example radial basis function kernels exp loose bound upper lower bounds fmin fmax particular kernel function training set trivial obtain similarity value range svm output sim fmin fmax fmin approach obtaining similarity value vector space svms shown fig training algorithm fig 
similarity strings needs computed transformed vector space representations pair vector formed individual components individual vector representations 
pair vector classified trained svm final similarity value obtained svm prediction eq 
created pair instance 
obtained svm output similarity strings classified svm pair instance rd ne road rd ne road string converted vector space representation sim min fmax min string similarity calculation svm learnable vector space model input sets equivalent strings non equivalent strings output string similarity function sim method 
convert strings vector representation 
create positive negative training sets constructing pair instances 
train svm obtain classification function min max 
return sim fmin fmax fmin training algorithm svm vector space similarity 
record level similarity combining similarity multiple fields distance records composed multiple fields calculated necessary combine similarity estimates individual fields meaningful manner 
correspondence record similarity single field similarity vary greatly depending informative fields necessary weight fields contribution true distance records 
statistical aspects combining similarity scores individual fields addressed previous record linkage availability labeled duplicates allows direct approach uses binary classifier computes pairing function :10.1.1.12.4018
database contains records composed different fields set 
dm distance metrics represent pair records mk dimensional vector 
component vector represents similarity field values records calculated distance metrics 
matched pairs duplicate records construct training set feature vectors assigning positive class label 
pairs records labeled duplicates implicitly form complementary set negative examples 
transitive closure matched pairs contains disjoint sets duplicate records approach result noisy negative examples 
binary classifier trained training vectors discriminate pairs records corresponding duplicates non duplicates 
approach follows framework learnable vector space string similarity previous section 
reasoning svms classifier choice due resilience noise ability handle correlated features 
distance hyperplane provides measure confidence pair records duplicate transformed actual similarity value eq 
fig illustrates process computing record similarity multiple similarity measures field binary classifier categorize resulting feature vector belonging class duplicates non duplicates resulting distance estimate 
field database learnable distance measures trained compute similarity field 
values computed measures form feature vector classified support vector machine producing confidence value represents similarity database records 
duplicate detection framework view system marlin fig 
argyle sunset blvd hollywood french new sunset blvd west hollywood american name address city cuisine svm learned distance measures distance duplicate records non duplicate records feature vector cu cu cu cu computation record similarity individual field similarities training phase consists steps 
learnable distance metrics trained record field 
training corpus paired field level duplicates non duplicates obtained pairs values field set paired duplicate records 
duplicate records may contain individual fields equivalent training data noisy 
example record describing restaurant contains asian cuisine field duplicate record contains noisy training pair formed implies equivalence descriptors 
issue pose serious problem approach reasons 
particularly noisy fields unhelpful identifying record level duplicates considered irrelevant classifier combines similarities different fields 
second presence pairs database indicates degree similarity values training allows learnable metric capture likelihood 
individual similarity metrics learned compute distances field duplicate non duplicate record pairs obtain training data binary classifier form vectors composed distance features 
duplicate detection phase starts generation potential duplicate pairs 
large database producing possible pairs records computing similarity expensive require distance computations 
marlin utilizes canopies clustering method jaccard similarity computationally inexpensive metric inverted index separate records overlapping clusters canopies potential duplicates :10.1.1.34.4329
pairs records fall cluster candidates full similarity comparison shown fig 
learned distance metrics calculate distances field pair potential duplicate records creating distance feature vectors classifier 
confidence estimates belonging class duplicates produced binary classifier candidate pair pairs sorted increasing confidence 
problem finding similarity threshold separating duplicates non duplicates arises point 
trivial solution binary classification results label records duplicates non duplicates 
traditional approach problem requires assigning thresholds separates pairs records high confidence duplicates possible duplicates reviewed human expert 
relative costs labeling duplicate false positives overlooking true table sample duplicate records cora database authors title venue address year pages yoav freund sebastian seung eli shamir naftali tishby information prediction query committee advances neural information processing system san mateo ca pages freund seung shamir tishby information prediction query committee advances neural information processing systems san mateo ca 
pp 

table sample duplicate records restaurant database name address city phone cuisine sunset blvd west hollywood american argyle sunset blvd hollywood french new table sample duplicate records reasoning database kaelbling 
architecture intelligent reactive systems 
reasoning actions plans proceedings workshop 
morgan kaufmann kaelbling 
architecture intelligent reactive systems 
georgeff lansky eds reasoning actions plans morgan kaufmann los altos ca labeled duplicate pairs distance learner metric database records duplicates potential distance learned metrics binary classifier identified duplicates distance metrics learned binary classifier duplicate detection candidate pair extractor non duplicate duplicate distance features distance features learned parameters record training data extractor training field training data extractor field duplicates record duplicates non duplicates marlin overview cates false negatives vary database database silver bullet solution problem 
availability labeled data allows provide precision recall estimates threshold value offer way control trade false unidentified duplicates selecting threshold values appropriate particular database 
possible identified duplicate pairs contain record 
duplicate relation transitive necessary compute transitive closure equivalent pairs complete identification process 
marlin utilizes union find data structure store database records step allows updating transitive closure identified duplicates incrementally efficient manner 

experimental evaluation datasets experiments conducted datasets 
restaurant database restaurant names addresses containing duplicates obtained integrating records fodor guidebooks 
cora collection distinct citations computer science research papers cora computer science research search engine 
citations segmented multiple fields author title venue information extraction system resulting crossover noise fields 
reasoning face reinforcement constraint single field datasets containing unsegmented citations computer science papers corresponding areas citeseer scientific literature digital library 
reasoning contains citation records represent unique papers face contains citations papers reinforcement contains citations papers constraint contains citations papers 
tables contain sample duplicate records restaurant cora reasoning datasets 
experimental methodology dataset randomly split folds cross validation experimental run 
larger number folds impractical result duplicate records fold 
create folds duplicate records grouped resulting clusters randomly assigned folds resulted uneven folds trials 
results reported random splits split folds alternately training testing 
trial duplicate detection performed described section 
iteration pair records highest similarity labeled duplicate transitive closure groups duplicates updated 
precision recall measure defined pairs duplicates computed iteration precision fraction identified duplicate pairs correct recall fraction actual duplicate pairs identified measure harmonic mean precision recall citeseer nj nec com recision airs airs recall airs oft airs measure recision recall recision recall pairs lower similarity labeled duplicates recall increases precision begins decrease number non duplicate pairs erroneously labeled duplicates increases 
precision interpolated standard recall levels traditional procedure information retrieval :10.1.1.27.7690
results detecting duplicate field values evaluate usefulness adapting string similarity measures specific domain compared learned distance metrics fixed cost equivalents task identifying equivalent field values 
single field citeseer datasets chose meaningful fields restaurant dataset name address 
compared string similarity measures edit distance affine gaps substitution cost gap opening cost gap extension cost match cost commonly parameters learned edit distance affine gaps described section trained em algorithm shown fig edit operation probabilities smoothed converted additive cost model described section normalized dot product vector space cosine similarity computed tf idf weights stemming stopword removal learnable vector space svm similarity described section implemented tf idf representations stemming stopword removal 
svm implementation radial basis function kernel svm light package classifier 
results field level duplicate detection experiments summarized table 
entry table contains average maximum measure values evaluated folds 
results experiments difference learned corresponding unlearned metric significant level tailed paired test bold font 
figs 
contain recall precision curves performance marlin name address fields restaurant dataset respectively 
relatively low precision experiments due fact duplicates individual fields noisy example restaurants different cities may variations name trials variations considered non duplicate 
results section show combination individual field estimates provides accurate approximation record similarities 
comparison results learned metrics corresponding baselines shows despite noise learned edit distance able adjust notion similarity specific domain learned vector space similarity improved standard vectorspace similarity half domains 
peculiar learned vector space measure mistakes unlearned cosine similarity low recall values restaurant name recall precision affine learned affine vector space learned vector space field duplicate detection results name field restaurant dataset restaurant address recall precision affine learned affine vector space learned vector space field duplicate detection results address field restaurant dataset name address datasets outperformed higher recall 
conjecture cosine similarity able correctly identify duplicates transpositions exact matches hotel bel air bel air hotel failed give sufficiently high similarity difficult duplicates art deli art 
svm trained metric hand able generalize training examples resulting better similarity estimates range difficult duplicates penalizing obvious matches 
results table show learned affine edit distance outperforms non trained edit distance vector space cosine similarity individual field duplicate detection 
visual inspection learned parameter values reveals parameters obtained training algorithm capture certain domain properties allow accurate similarity computations 
example address field restaurant data lowest cost edit operations deleting space deleting deleting captures fact common cause street name duplicates abbreviations street str 
record level duplicate detection evaluated performance marlin multi field record level duplicate detection 
svm light table maximum measure detecting duplicate field values distance metric restaurant name restaurant address reasoning face reinforcement constraint edit distance learned edit distance vector space learned vector space table maximum measure duplicate detection multiple fields metric restaurant cora edit distance learned edit distance vector space learned vector space cora recall precision affine learned affine vector space learned vector space duplicate detection results cora dataset author title venue year pages fields implementation support vector machine binary classifier combines similarity estimates different fields produce measure record similarity shown fig 
compared performance learnable baseline text similarity metrics producing similarity estimates individual fields 
table summarizes results restaurant cora datasets 
results bold font correspond experiments differences learned unlearned string metrics significant level tailed paired test 
figures precision recall curves experiments 
results conclude learnable string edit distance affine gaps metrics compute similarity field values positive contribution similarities multiple fields combined 
better estimates individual field similarities result accurate calculation record similarity 
svm vector space learnable similarity lead improvements original vector space cosine similarity performance fact decreased 
results field duplicate detection learnable vector space metric mixed surprising 
conducted additional experiments folds created assigning equivalence classes duplicate records folds randomly assigning individual instances 
resulted duplicate pairs corresponding object training test sets experiments possible cora citeseer restaurant recall precision affine learned affine vector space learned vector space duplicate detection results restaurant dataset name address city cuisine fields datasets duplicates equivalence class restaurant dataset 
experiments learned vector space metrics outperformed unlearned baseline 
explained fact training algorithm svm light relies assumption training data comes distribution test data 
separating equivalence classes different folds results different token distributions sets learned classifier suitable producing accurate predictions test data result 
ran trials combined character metrics static learnable string edit distance affine gaps vector space cosine similarity 
experiments resulted near precision recall significant differences static adaptive field level metrics 
demonstrates combining character token distance metrics learned string distance affine gaps cosine similarity clearly advantage level learning approach implemented marlin 
current datasets allow show benefits adaptive metrics static prototypes scenario initial results suggest demonstrated challenging datasets 
comparison classifiers record level duplicate detection previous employed classifiers combine similarity estimates multiple fields utilized committees decision tree learners :10.1.1.1.5288
compared performance support vector machines boosted decision trees combining similarity estimates database fields produce similarity records 
conducted experiments settings limited training data negative positive duplicate pair examples large amounts training data randomly sampled negative pairs positive pairs fewer available restaurant dataset due limited number duplicates 
svm light implementation support vector machine radial basis function kernel compared weka package implementation alternating decision trees state art algorithm combines boosting decision tree learning 
unlearned vector space normalized dot product field level similarity measure 
figs illustrate results restaurant cora datasets 
recall precision restaurant adtree restaurant svm cora adtree cora svm duplicate detection results restaurant cora datasets different record level classifiers limited training data recall precision restaurant adtree restaurant svm cora adtree cora svm duplicate detection results restaurant cora datasets different record level classifiers large amounts training data results show support vector machines significantly outperform boosted decision trees training data limited scenario adaptive duplicate detection 
decision trees reliable classifiers obtaining calibrated confidence scores relies probability estimates training data statistics tree nodes 
little training data available frequency estimates unreliable 
result confidence decision tree classifier inaccurate measure relative record similarity leads poor accuracy duplicate detection process 

related problem identifying duplicate records databases originally identified newcombe record linkage context identifying medical records individual different time periods 
fellegi sunter developed formal theory record linkage offered statistical methods estimating matching parameters error rates 
statistics winkler proposed em methods obtaining optimal matching rules 
highly specialized domain census records hand tuned similarity measures 
hern andez stolfo developed sorted neighborhood method limiting number potential duplicate pairs require distance computation mccallum proposed canopies clustering algorithm task matching scientific citations :10.1.1.34.4329
monge elkan developed iterative merging algorithm union find data structure showed advantages string distance metric allows gaps 
cohen posed duplicate detection task optimization problem proved np hardness solving problem optimally proposed nearly linear algorithm finding local optimum union find data structure 
cohen richman proposed adaptive framework duplicate detection combines multiple similarity metrics :10.1.1.12.4018
sarawagi tejada developed systems employ committee active learning methods selecting record pairs informative training record level classifier combines similarity estimates multiple fields different metrics 
approaches fixed cost similarity metrics compare database records 
shown learnable similarity measures combined trainable record level similarity active learning techniques prior easily extended include distance measures proposed 

proposed general framework learnable string similarity measures duplicate detection provided algorithms character vector space text distances 
directions approach extended 
general classification framework computing vectorspace similarity improved modifying svm training algorithm avoid overfitting issues encountered 
algorithm iterative decomposition quadratic optimization problem svm light needs extended robust differences distributions test data training data 
task similar transduction require unlabeled test data learning process fundamental departure transduction unlabeled test data different distribution 
alternatively task learning vector space similarity pairs strings formalized parameter estimation optimization problem investigating statistical mathematical programming methods incorporate regularization deal distribution problem promising avenue improvement 
area lies generalizing edit distance include macro operators inserting deleting common substrings deleting street address fields 
string distance model gaps particularly useful task allow discovering useful deletion sequences developing stochastic model gaps created computing minimum cost alignments 
substructure discovery methods identify useful edit operation sequences include different edit operations 

duplicate detection important problem data cleaning adaptive approach learns identify duplicate records specific domain clear advantages static methods 
experimental results demonstrate trainable similarity measures capable learning specific notion similarity appropriate specific domain 
learnable distance measures improve character vector space metrics allow specializing specific datasets labeled examples 
shown support vector machines effectively utilized datasets string similarity record similarity computations outperforming traditional methods hope improve initial results 
framework duplicate detection integrates previous adaptive methods learnable similarity measures leading improved results 

acknowledgments wewould steve lawrence providing citeseer datasets sheila tejada restaurant dataset william cohen providing cora dataset 
research supported national science foundation iis faculty fellowship ibm 

baeza yates ribeiro neto :10.1.1.27.7690
modern information retrieval 
acm press new york 
bilenko mooney 
learning combine trained distance metrics duplicate detection databases 
technical report ai artificial intelligence laboratory university texas austin austin tx feb 
cohen kautz mcallester 
hardening soft information sources 
proceedings sixth international conference knowledge discovery data mining kdd boston ma aug 
cohen richman :10.1.1.12.4018
learning match cluster large high dimensional data sets data integration 
proceedings eighth acm sigkdd international conference knowledge discovery data mining kdd edmonton alberta 
cook holder 
substructure discovery minimum description length background knowledge 
journal artificial intelligence research 
durbin eddy krogh mitchison 
biological sequence analysis probabilistic models proteins nucleic acids 
cambridge university press 
fellegi sunter 
theory record linkage 
journal american statistical association 
freund mason 
alternating decision tree learning algorithm 
proceedings sixteenth international conference machine learning icml bled slovenia 
gusfield 
algorithms strings trees sequences 
cambridge university press new york 
hern andez stolfo 
merge purge problem large databases 
proceedings acm sigmod international conference management data sigmod pages san jose ca may 
joachims 
making large scale svm learning practical 
sch olkopf burges smola editors advances kernel methods support vector learning pages 
mit press 
joachims 
transductive inference text classification support vector machines 
proceedings sixteenth international conference machine learning icml bled slovenia june 
mccallum nigam ungar :10.1.1.34.4329
efficient clustering high dimensional data sets application matching 
proceedings sixth international conference knowledge discovery data mining kdd pages boston ma aug 
monge elkan 
field matching problem algorithms applications 
proceedings second international conference knowledge discovery data mining kdd pages portland aug 
monge elkan 
efficient domain independent algorithm detecting approximately duplicate database records 
proceedings sigmod workshop research issues data mining knowledge discovery pages az may 
nahm mooney 
information extraction aid discovery prediction rules texts 
proceedings sixth international conference knowledge discovery data mining kdd workshop text mining boston ma aug 
needleman wunsch 
general method applicable search similarities amino acid sequences proteins 
journal molecular biology 
newcombe kennedy james 
automatic linkage vital records 
science 
platt 
probabilistic outputs support vector machines comparisons regularized likelihood methods 
smola bartlett sch olkopf schuurmans editors advances large margin classifiers pages 
mit press 
rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
yianilos 
learning string edit distance 
ieee transactions pattern analysis machine intelligence 
sarawagi 
interactive deduplication active learning 
proceedings eighth acm sigkdd international conference knowledge discovery data mining kdd edmonton alberta 
tejada knoblock minton 
learning domain independent string transformation weights high accuracy object identification 
proceedings eighth acm sigkdd international conference knowledge discovery data mining kdd edmonton alberta 
vapnik 
statistical learning theory 
wiley 
winkler 
state record linkage current research problems 
technical report statistical research division bureau census dc 
witten frank 
data mining practical machine learning tools techniques java implementations 
morgan kaufmann san francisco 
zadrozny elkan 
obtaining calibrated probability estimates decision trees naive bayesian classifiers 
proceedings th international conference machine learning icml ma 
