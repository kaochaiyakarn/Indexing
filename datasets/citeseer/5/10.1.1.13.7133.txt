machine learning kluwer academic publishers 
manufactured netherlands 
mcmc machine learning christophe andrieu andrieu bristol ac uk department mathematics statistics group university bristol university walk bristol bs tw uk de freitas cs ubc ca department computer science university british columbia main mall vancouver bc canada arnaud doucet doucet ee mu oz au department electrical electronic engineering university melbourne victoria australia michael jordan jordan cs berkeley edu departments computer science statistics university california berkeley soda hall berkeley ca usa 
purpose introductory threefold 
introduces monte carlo method emphasis probabilistic machine learning 
second reviews main building blocks modern markov chain monte carlo simulation providing remaining papers special issue 
lastly discusses new interesting research horizons 
keywords markov chain monte carlo mcmc sampling stochastic algorithms 
survey places metropolis algorithm algorithms greatest influence development practice science engineering th century sullivan 
algorithm instance large class sampling algorithms known markov chain monte carlo mcmc 
algorithms played significant role statistics econometrics physics computing science decades 
high dimensional problems computing volume convex body dimensions mcmc simulation known general approach providing solution reasonable time polynomial dyer frieze kannan jerrum sinclair 
illness stan ulam playing solitaire 
occurred try compute chances particular solitaire laid cards come successfully 
attempting exhaustive combinatorial calculations decided go practical approach laying random observing counting number successful plays 
idea selecting statistical sample approximate hard combinatorial problem simpler problem heart modern monte carlo simulation 
andrieu stan ulam soon realised computers fashion answer questions neutron diffusion mathematical physics 
contacted john von neumann understood great potential idea 
years ulam von neumann developed monte carlo algorithms including importance sampling rejection sampling 
enrico fermi monte carlo calculation neutron diffusion designed monte carlo mechanical device performed calculations anderson 
nick metropolis young physicist designed new controls state art computer von neumann john wife 
monte carlo methods new computing device 
soon designed improved computer named hope computer scientists acronyms 
time spent working computing machines mathematicians physicists fermi von neumann ulam teller bethe feynman go problems 
eventually published public document monte carlo simulation stan ulam metropolis ulam 
introduces ideas monte carlo particle methods form basis modern sequential monte carlo methods bootstrap filters condensation survival fittest algorithms doucet de freitas gordon 
soon proposed metropolis algorithm tellers metropolis 
papers monte carlo simulation appeared physics literature 
inference perspective significant contribution generalisation metropolis algorithm hastings 
hastings student showed metropolis general metropolis hastings algorithms particular instances large family algorithms includes boltzmann algorithm hastings 
studied optimality algorithms introduced formulation metropolis hastings algorithm adopt 
important mcmc papers appeared fields computer vision artificial intelligence geman geman pearl 
despite existence mcmc publications statistics literature time generally accepted mcmc significant impact statistics gelfand smith 
neural networks literature publication neal particularly influential 
special issue focus describing algorithms feel main building blocks modern mcmc programs 
emphasize order obtain best results class algorithms important treat black boxes try incorporate domain specific knowledge possible design 
mcmc algorithms typically require design proposal mechanisms generate candidate hypotheses 
existing machine learning algorithms adapted proposal mechanisms de freitas 
essential obtain mcmc algorithms converge quickly 
addition believe machine learning community contribute significantly solution open problems mcmc field 
purpose outlined hot research directions 
readers encouraged consult excellent texts chen shao ibrahim gilks richardson spiegelhalter liu tweedie robert casella review papers besag 
brooks diaconis saloff coste jerrum sinclair neal tierney information mcmc 
remainder organised follows 
part outline general problems introduce simple monte carlo simulation rejection sampling importance sampling 
part deals mcmc presentation popular mcmc algorithms 
part describe important research frontiers 
accessible notational distinction distributions densities section reversible jump mcmc 

mcmc motivation mcmc techniques applied solve integration optimisation problems large dimensional spaces 
types problem play fundamental role machine learning physics statistics econometrics decision analysis 
just examples 

bayesian inference learning 
unknown variables data typically intractable integration problems central bayesian statistics normalisation 
obtain posterior prior likelihood normalising factor bayes theorem needs computed dx marginalisation 
joint posterior may interested marginal posterior dz 
expectation 
objective analysis obtain summary statistics form ep dx function interest rn integrable respect 
examples appropriate functions include conditional mean case conditional covariance xx ep 

statistical mechanics 
needs compute partition function system states hamiltonian exp kt boltzmann constant denotes temperature system 
summing large number possible configurations prohibitively expensive baxter 
note problems computing partition function normalising constant statistical inference analogous 
andrieu 
optimisation 
goal optimisation extract solution minimises objective function large set feasible solutions 
fact set continuous unbounded 
general computationally expensive compare solutions find optimal 

penalised likelihood model selection 
task typically involves steps 
finds maximum likelihood ml estimates model separately 
uses term example mdl bic aic select models 
problem approach initial set models large 
models interest computing resources wasted 
emphasized integration optimisation mcmc plays fundamental role simulation physical systems 
great relevance nuclear physics computer graphics forsyth guibas :10.1.1.40.2090

monte carlo principle idea monte carlo simulation draw set samples target density defined high dimensional space set possible configurations system space posterior defined combinatorial set feasible solutions 
samples approximate target density empirical point mass function pn denotes delta dirac mass located consequently approximate integrals large sums tractable sums converge follows dx 
estimate unbiased strong law large numbers surely converge 
variance univariate case simplicity satisfies ep variance estimator equal var central limit theorem yields convergence distribution error denotes convergence distribution robert casella section 
advantage monte carlo integration deterministic integration arises fact positions integration grid samples regions high probability 
samples obtain maximum objective function follows arg max show possible construct simulated annealing algorithms allow sample approximately distribution support set global maxima 
standard form gaussian straightforward sample easily available routines 
case need introduce sophisticated techniques rejection sampling importance sampling mcmc 

rejection sampling sample distribution known proportionality constant sampling easy sample proposal distribution satisfies mq accept reject procedure describe see 
accepted easily shown sampled probability robert 
rejection sampling algorithm 
denotes operation sampling uniform random variable interval 

rejection sampling sample candidate uniform variable accept candidate sample reject 
andrieu casella 
simple method suffers severe limitations 
possible bound reasonable constant space ifm large acceptance probability pr accepted pr mq small 
method impractical high dimensional scenarios 

importance sampling importance sampling alternative classical solution goes back see example geweke rubinstein 
introduce arbitrary importance proposal distribution support includes support 
rewrite follows dx known importance weight 
consequently simulate samples evaluate possible monte carlo estimate estimator unbiased weak assumptions strong law large num bers applies 
clear integration method interpreted sampling method posterior density approximated pn function integrated respect empirical measure pn 
proposal distributions obviously preferable 
important criterion choosing optimal proposal distribution find minimises variance estimator 
variance respect eq second term right hand side depend need minimise term jensen inequality lower bound eq eq dx lower bound attained adopt optimal importance distribution dx optimal proposal useful sense easy sample 
tells high sampling efficiency achieved focus sampling important regions relatively large name importance sampling 
result implies importance sampling estimates super efficient 
function possible find distribution yields estimate lower variance perfect monte carlo method 
property exploited evaluate probability rare events communication networks smith shafi gao 
quantity interest tail probability bit error rate ifx see 
estimate bit error rate efficiently sampling 
wasteful propose candidates regions utility 
applications aim usually different sense 
importance sampling place importance sampling state space regions matter 
particular example interested computing tail probability error detecting infrequent abnormalities 
andrieu wants approximation particular integral respect seek 
dimension increases harder obtain suitable draw samples 
sensible strategy adopt parameterised adapt simulation 
adaptive importance sampling appears originated structural safety literature extensively applied communications literature townsend 
technique exploited machine learning community de freitas cheng druzdzel ortiz kaelbling schuurmans 
popular adaptive strategy involves computing derivative term right hand side eq 
eq updating parameters follows learning rate 
optimisation approaches hessian possible 
normalising constant unknown possible apply importance sampling method rewriting follows dx dx known normalising constant 
monte carlo estimate normalised importance weight 
finite biased ratio estimates asymptotically weak assumptions strong law large numbers applies 
additional assumptions central limit theorem obtained geweke 
estimator shown perform better setups squared error loss robert casella 
interested obtaining samples pn asymptotically valid method consists resampling times discrete distribution pn 
procedure results samples possibility method known sampling importance resampling sir rubin 
resampling approximation target density pm resampling scheme introduces additional monte carlo variation 
clear sir procedure lead practical gains general 
sequential monte carlo setting described section essential carry resampling step 
conclude section stating adaptation impossible obtain proposal distributions easy sample approximations time 
reason need introduce sophisticated sampling algorithms markov chains 

mcmc algorithms mcmc strategy generating samples exploring state space markov chain mechanism 
mechanism constructed chain spends time important regions 
particular constructed samples mimic samples drawn target distribution 
reiterate mcmc draw samples directly evaluate normalising constant 
intuitive introduce markov chains finite state spaces take discrete values xs 
stochastic process called markov chain chain homogeneous remains invariant evolution chain space depends solely current state chain fixed transition matrix 
example consider markov chain states transition graph illustrated 
transition matrix example probability vector initial state follows iterations multiplications product converges 
matter initial distribution chain stabilise 
stability result plays fundamental role mcmc simulation 
starting point chain convergence andrieu 
transition graph markov chain example 
invariant distribution long stochastic transition matrix obeys properties 
irreducibility 
state markov chain positive probability visiting states 
matrix reduced separate smaller matrices stating transition graph connected 


chain get trapped cycles 
sufficient necessary condition ensure particular desired invariant distribution reversibility detailed balance condition summing sides gives mcmc samplers irreducible aperiodic markov chains target distribution invariant distribution 
way design samplers ensure detailed balance satisfied 
important design samplers converge quickly 
efforts devoted increasing convergence speed 
spectral theory gives useful insights problem 
notice left eigenvector matrix corresponding eigenvalue 
fact perron frobenius theorem linear algebra tells remaining eigenvalues absolute value 
second largest eigenvalue determines rate convergence chain small possible 
concepts irreducibility invariance better appreciated realise important role play lives 
search information world wide web typically follow set links berners lee 
interpret webpages links respectively nodes directed connections markov chain transition graph 
clearly say random walkers web want avoid getting trapped cycles want able access existing webpages irreducibility 
consider popular information retrieval algorithm search engine google pagerank page 
pagerank requires definition transition matrix components large link matrix rows columns corresponding web pages entry li represents normalised number links web page web page uniform random matrix small magnitude added ensure irreducibility 
addition noise prevents getting trapped loops ensures probability jumping web 
previous discussion case invariant distribution eigenvector represents rank webpage note possible design interesting transition matrices setting 
long satisfies irreducibility incorporate terms transition matrix favour particular webpages bias search useful ways 
continuous state spaces transition matrix integral kernel corresponding eigenfunction dx kernel conditional density value mathematical representation markov chain algorithm 
subsections describe various algorithms 

metropolis hastings algorithm metropolis hastings mh algorithm popular mcmc method hastings metropolis 
sections see practical mcmc algorithms interpreted special cases extensions algorithm 
mh step invariant distribution proposal distribution involves sampling candidate value current value 
markov chain moves acceptance probability min remains pseudo code shown shows results running mh algorithm gaussian proposal distribution bimodal target distribution exp exp iterations 
expected histogram samples approximates target distribution 
andrieu 
metropolis hastings algorithm 

target distribution histogram mcmc samples different iteration points 
mh algorithm simple requires careful design proposal distribution 
subsequent sections see mcmc algorithms arise considering specific choices distribution 
general possible suboptimal inference learning algorithms generate data driven proposal distributions 
transition kernel mh algorithm term associated rejection dx fairly easy prove samples generated mh algorithm mimic samples drawn target distribution asymptotically 
construction satisfies detailed balance condition consequently mh algorithm admits invariant distribution 
show mh algorithm converges need ensure cycles state positive probability reached finite number steps irreducibility 
algorithm allows rejection follows aperiodic 
ensure irreducibility simply need sure support includes support 
conditions obtain asymptotic convergence tierney theorem 
space small example bounded possible conditions prove uniform geometric ergodicity tweedie 
possible prove geometric ergodicity foster lyapunov drift conditions tweedie roberts tweedie 
independent sampler metropolis algorithm simple instances mh algorithm 
independent sampler proposal independent current state 
acceptance probability min min algorithm close importance sampling samples correlated result comparing sample 
metropolis algorithm assumes symmetric random walk proposal acceptance ratio simplifies min properties mh algorithm worth highlighting 
firstly normalising constant target distribution required 
need know target distribution constant proportionality 
secondly pseudo code single chain easy simulate independent chains parallel 
lastly success failure algorithm hinges choice proposal distribution 
illustrated 
different choices proposal standard deviation lead different results 
proposal narrow mode visited 
hand wide rejection rate high resulting high correlations 
modes visited acceptance probability high chain said mix 
andrieu 
approximations obtained mh algorithm gaussian proposal distributions different variances 

simulated annealing global optimization assume wanting approximate want find global maximum 
example likelihood posterior distribution want compute ml maximum posteriori map estimates 
mentioned earlier run markov chain invariant distribution estimate global mode arg max method inefficient random samples rarely come vicinity mode 
distribution large probability mass mode computing resources wasted exploring areas interest 
principled strategy adopt simulated annealing geman geman kirkpatrick gelatt vecchi van laarhoven arts 
technique involves simulating non homogeneous markov chain invariant distribution iteration longer equal pi ti 
general simulated annealing algorithm 

discovering modes target distribution simulated annealing algorithm 
ti decreasing cooling schedule ti 
reason doing weak regularity assumptions probability density concentrates set global maxima 
simulated annealing involves just minor modification standard mcmc algorithms shown 
results applying annealing example previous section shown 
obtain efficient annealed algorithms important choose suitable proposal distributions appropriate cooling schedule 
negative simulated annealing andrieu results reported literature stem poor proposal distribution design 
complex variable model selection scenarios arising machine learning propose complex reversible jump mcmc kernels section annealing algorithm andrieu de freitas doucet 
defines joint distribution parameter model spaces technique search best model mdl aic criteria ml parameter estimates simultaneously 
convergence results simulated annealing typically state ti homogeneous markov transition kernel mixes quickly convergence set global maxima ensured sequence ti ln problem dependent 
results obtained finite spaces geman geman van laarhoven arts compact continuous spaces 
results non compact spaces andrieu doucet 

mixtures cycles mcmc kernels powerful property mcmc possible combine samplers mixtures cycles individual samplers tierney 
transition kernels invariant distribution cycle hybrid kernel mixture hybrid kernel transition kernels invariant distribution 
mixtures kernels incorporate global proposals explore vast regions state space local proposals discover finer details target distribution andrieu de freitas doucet andrieu doucet robert casella 
useful example target distribution narrow peaks 
global proposal locks peaks local proposal allows explore space peak 
example require high precision frequency detector fast fourier transform fft global proposal random walk local proposal andrieu doucet 
similarly kernel regression classification want global proposal places bases kernels locations input data local random walk proposal perturbs order obtain better fits andrieu de freitas doucet 
mixtures kernels play big role samplers including reversible jump mcmc algorithm section 
pseudo code typical mixture kernels shown 
cycles allow split multivariate state vector components blocks updated separately 
typically samplers mix quickly blocking highly correlated variables 
block mcmc sampler indicate th block nb denote number blocks shown bnb 
transition kernel algorithm expression cycle nb denotes th mh algorithm cycle 

typical mixture mcmc kernels 

cycle mcmc kernels block mh algorithm 
obviously choosing size blocks poses trade offs 
samples components multi dimensional vector time chain may take long time explore target distribution 
problem gets worse correlation components increases 
alternatively samples components probability accepting large move tends low 
popular cycle mh kernels known gibbs sampling geman geman obtained adopt full conditional distributions xn proposal distributions notational simplicity replaced index notation 
section describes detail 

gibbs sampler suppose dimensional vector expressions full conditionals xn 
case advantageous andrieu proposal distribution 
corresponding acceptance probability min min min 
acceptance probability proposal deterministic scan gibbs sampler algorithm shown 
gibbs sampler viewed special case mh algorithm possible introduce mh steps gibbs sampler 
full conditionals available belong family standard distributions gamma gaussian draw new samples directly 
draw samples mh steps embedded gibbs algorithm 
gibbs sampler known data augmentation algorithm closely related expectation maximisation em algorithm dempster laird rubin tanner wong 
directed acyclic graphs dags best known application areas gibbs sampling pearl 
large dimensional joint distribution factored directed graph encodes conditional independencies model 
particular pa 
gibbs sampler 
denotes parent nodes node wehave pa follows full conditionals simplify follows pa ch xk pa ch denotes children nodes need take account parents children children parents 
set variables known markov blanket technique forms basis popular software package bayesian updating gibbs sampling bugs gilks thomas spiegelhalter 
sampling full conditionals gibbs sampler lends naturally construction general purpose mcmc software 
convenient block variables improve mixing jensen kong kj wilkinson yeung 

monte carlo em em algorithm baum dempster laird rubin standard algorithm ml map point estimation 
contains visible hidden variables xv xh local maximum likelihood xv parameters iterating steps 
step 
compute expected value complete log likelihood function respect distribution hidden variables log xh xv xh xv old xh old refers value parameters previous time step 

step 
perform maximisation new arg max 
practical situations expectation step sum exponentially large number summands intractable integral ghahramani ghahramani jordan mcculloch pasula see dellaert 
issue 
solution introduce mcmc sample xh xv old replace expectation step small sum samples shown 
convergence algorithm discussed sherman ho dalal levine casella review 
improve convergence behaviour em escape low local minima saddle points various authors proposed stochastic approaches rely sampling xh xv old step performing step samples 
andrieu 
mcmc em algorithm 
method known stochastic em sem draw sample celeux diebolt monte carlo em mcem samples drawn wei tanner 
annealed variants saem deterministic number iterations increases celeux diebolt 
efficient algorithms marginal map estimation doucet godsill robert 
wishes metropolis succeeded stopping proliferation acronyms 

auxiliary variable samplers easier sample augmented distribution auxiliary variable 
possible obtain marginal samples sampling subsequently ignoring samples useful idea proposed physics literature swendsen wang 
focus known examples auxiliary variable methods hybrid monte carlo slice sampling 

hybrid monte carlo 
hybrid monte carlo hmc mcmc algorithm incorporates information gradient target distribution improve mixing high dimensions 
describe leapfrog hmc algorithm outlined duane 
neal focusing algorithmic details statistical mechanics motivation 
assume differentiable strictly positive 
iteration hmc algorithm takes predetermined number deterministic steps information gradient 
explain detail need introduce set auxiliary momentum variables nx define 
hybrid monte carlo 
extended target density inx need introduce nx dimensional gradient vector log fixed step size parameter 
hmc algorithm draw new sample starting previous value generating gaussian random variable take frog leaps values leap proposal candidates mh algorithm target density 
marginal samples obtained simply ignoring algorithm proceeds illustrated 
deterministic step obtains langevin algorithm discrete time approximation langevin diffusion process 
langevin algorithm special case mh candidate satisfies inx 
choice parameters poses simulation tradeoffs 
large values result low acceptance rates small values require leapfrog steps expensive computations gradient move nearby states 
choosing equally problematic want large generate candidates far initial state result expensive computations 
hmc requires careful tuning proposal distribution 
efficient practice allow different step size coordinates 
andrieu 
slice sampler 
slice sampler damien wakefield walker higdon wakefield gelfand smith general version gibbs sampler 
basic idea slice sampler introduce auxiliary variable construct extended target distribution 
straightforward check du du 
sample sample ignore full conditionals augmented model ua 
ifa easy identify algorithm straightforward implement shown 
difficult identify worth introducing auxiliary variables damien wakefield walker higdon 
example assume fl fl positive functions necessarily densities 
introduce auxiliary variables define fl ul 

slice sampling previous sample sample uniform variable 
samples interval 

slice sampler 
check du du fl ul du fl 
slice sampler sample proceeds shown 
algorithmic improvements convergence results mira neal 

reversible jump mcmc section attack complex problem model selection 
typical examples include estimating number neurons neural network andrieu de freitas doucet holmes mallick rios ller number splines multivariate adaptive splines regression mars model holmes denison issue number sinusoids noisy signal andrieu doucet number lags autoregressive process godsill number components mixture richardson green number levels changepoint process green number components mixture factor analysers titterington issue appropriate structure graphical model friedman koller issue best set input variables lee issue :10.1.1.57.5225
family models mm focus constructing ergodic markov chains admitting xm invariant distribution 
simplicity avoid treatment nonparametric model averaging techniques see example escobar west green richardson 
section comparing densities acceptance ratio 
carrying model selection comparing densities objects different dimensions meaning 
trying compare spheres circles 
formal compare distributions dx pr dx common measure volume 
distribution dx assumed admit density respect measure interest lebesgue continuous case dx dx 
acceptance ratio include ratio densities ratio measures radon nikodym derivative 
gives rise jacobian term 
compare densities point wise need map models common dimension illustrated 
andrieu univariate density uniformly bivariate density uniformly expanded density compare densities point wise 
compare model model map model models common measure area case 
parameters xm xm xm model dependent 
find right model parameters sample model indicator product space xm carlin chib 
green introduced strategy avoids expensive search full product space green :10.1.1.57.5225
particular samples smaller union space xm 
full target distribution defined space dx xm 
probability equal belonging infinitesimal set centred xm 
marginalisation obtain probability subspace xm 
green method allows sampler jump different subspaces 
ensure common measure requires extension pair communicating spaces xm xn xm um xn xn un requires definition deterministic differentiable invertible dimension matching function fn xm xn xm um fn xn un xn un xn un define fm fm fn xn un xn un 
choice extended spaces deterministic transformation fm proposal distributions qn xn qm xm problem dependent needs addressed case case basis 
current state chain xn move xm generating un qn xn ensuring reversibility xm um fn xn un accepting move probability ratio min xn qm um qn un xn fn fn xn un fn jacobian transformation fn continuous variables involved transformation fm det fn xm um xm um illustrate assume concerned sampling locations number components mixture 
example want estimate locations number basis functions kernel regression classification number mixture components finite mixture model location number segments segmentation problem 
define merge move combines nearby components split move breaks component nearby ones 
merge move involves randomly selecting component combining closest neighbour single component new location corresponding split move guarantees reversibility involves splitting randomly chosen component follows un un simulation parameter example un 
note ensure reversibility perform merge move 
acceptance ratio split move min un denotes probability choosing uniformly random components 
jacobian un 
andrieu 
generic reversible jump mcmc 
similarly merge move min 
reversible jump mixture mcmc kernels moves 
addition split merge moves moves birth component death component simple update locations 
various moves carried mixture probabilities bk dk mk sk uk shown 
fact flexibility including possible moves reversible jump powerful model selection strategy schemes model selection mixture indicator diffusion processes birth death moves stephens 
problem reversible jump mcmc engineering reversible moves tricky time consuming task 

mcmc frontiers 
convergence perfect sampling determining length markov chain difficult task 
practice discards initial set samples burn avoid starting biases 
addition apply graphical statistical tests assess roughly chain stabilised robert casella ch 

general tests provide entirely satisfactory diagnostics 
theoreticians tried bound mixing time minimum number steps required distribution markov chain close target 
means exhaustive summary available results 
measure closeness total variation norm dy mixing time min 
state space finite reversibility holds true transition operator kf self adjoint 
kf kg real functions bra ket notation inner product 
implies real eigenvalues orthonormal basis real eigenfunctions fi kfi fi 
spectral decomposition cauchy schwartz inequality allow obtain bound total variation norm max diaconis saloff coste jerrum sinclair 
classical result give geometric convergence rate terms eigenvalues 
geometric bounds obtained general state spaces tools regeneration lyapunov foster conditions tweedie 
logical step bound second eigenvalue 
inequalities cheeger poincar nash differential geometry allows obtain bounds diaconis saloff coste 
example cheeger inequality obtain bound conductance markov chain min intuitively interpret quantity readiness chain escape small region state space rapid progress equilibrium jerrum sinclair 
andrieu mathematical tools applied show simple mcmc algorithms metropolis run time polynomial dimension state space escaping exponential curse dimensionality 
polynomial time sampling algorithms obtained important scenarios 
computing volume convex body dimensions large dyer frieze kannan 

sampling log concave distributions applegate kannan 

sampling truncated multivariate gaussians kannan li 

computing permanent matrix jerrum sinclair 
problem equivalent sampling matchings bipartite graph problem manifests ways machine learning stereo matching data association 
theoretical results far practice mcmc eventually provide better guidelines design choose algorithms 
results tell example wise independent metropolis sampler high dimensions mengersen tweedie 
remarkable breakthrough development algorithms perfect sampling 
algorithms guaranteed give independent sample certain restrictions 
major players coupling past propp wilson fill algorithm fill 
practical point view algorithms limited cases computationally inefficient 
steps taken obtaining general perfect samplers example perfect slice samplers casella 

adaptive mcmc look chain top right notice chain stays state long time 
tells reduce variance proposal distribution 
ideally automate process choosing proposal distribution possible 
information samples update parameters proposal distribution obtain distribution closer target distribution ensures suitable acceptance rate minimises variance estimator interest 
allow adaptation take place infinitely naive way disturb stationary distribution 
problem arises past information infinitely violate markov property transition kernel 
longer simplifies 
particular gelfand sahu pathological example stationary distribution disturbed despite fact participating kernel stationary distribution 
avoid problem carry adaptation initial fixed number steps standard mcmc simulation ensure convergence right distribution 
methods doing gelfand sahu 
idea running chains parallel sampling importance resampling rubin multiply kernels doing suppress 
approach uses approximation marginal density chain proposal 
second method simply involves monitoring transition kernel changing components example proposal distribution improve mixing 
similar method guarantees particular acceptance rate discussed browne draper 
adaptive mcmc methods allow perform adaptation continuously disturbing markov property including delayed rejection tierney mira parallel chains gilks roberts regeneration gilks roberts sahu mykland tierney yu 
methods unfortunately inefficient ways research required exciting area 

sequential monte carlo particle filters sequential monte carlo smc methods allow carry line approximation probability distributions samples particles 
useful scenarios involving real time signal processing data arrival inherently sequential 
furthermore wish adopt sequential processing strategy deal non stationarity signals information past greater weighting information distant past 
computational simplicity form having store data constitute additional motivating factor methods 
smc setting assume initial distribution dynamic model measurement model xt yt denote xt yt respectively states observations time note assume markov transitions conditional independence simplify model xt xt xt yt yt xt 
assumption necessary smc framework 
aim estimate recursively time posterior associated features including marginal distribution xt known filtering distribution expectations ft ep ft generic smc algorithm depicted 
particles time approximately distributed distribution smc methods allow compute particles approximately distributed posterior time sample posterior directly smc update accomplished introducing appropriate importance proposal distribution obtain samples 
samples appropriately weighted 
andrieu 
example bootstrap filter starts time unweighted measure provides approximation xt 
particle compute importance weights information time 
results weighted measure yields approximation xt 
subsequently resampling step selects fittest particles obtain unweighted measure approximation xt 
sampling prediction step introduces variety resulting measure approximation xt 

simple smc algorithm time purposes need storing resampling past trajectories 
generic smc simulation needs extend current paths obtain new paths proposal distribution dx 
integral tractable propose modify particles time leave past trajectories intact 
consequently xt samples weighted importance weights wt xt xt yt xt xt 
qt xt eq 
note optimal importance distribution xt xt 
proposal encounter difficulties ratio terms eq 
differs significantly andrieu doucet pitt shephard 
optimal importance distribution difficult evaluate 
adopt transition prior proposal distribution xt xt case importance weights likelihood function wt yt xt simplified version smc appeared names including condensation isard blake survival fittest kanazawa koller russell bootstrap filter gordon salmond smith 
importance sampling framework allows design principled clever proposal distributions 
instance adopt suboptimal filters approximation methods information available time generate proposal distribution doucet godsill andrieu de freitas pitt shephard van der merwe 
fact restricted situations may interpret likelihood distribution terms states sample directly 
doing importance weights equal transition prior fox 
importance sampling step selection scheme associates particle number children say ni ni selection step andrieu allows track moving target distributions efficiently choosing fittest particles 
various selection schemes literature performance varies terms var ni doucet de freitas gordon 
important feature selection routine interface depends particle indices weights 
treated black box routine require knowledge particle represents variables parameters models 
enables implement variable model selection schemes straightforwardly 
simplicity coding complex models major advantages algorithms 
possible introduce mcmc steps invariant distribution particle andrieu de freitas doucet gilks maceachern clyde liu 
basic idea particles distributed distribution applying markov chain transition kernel invariant distribution results set particles distributed posterior interest 
new particles moved interesting areas state space 
fact applying markov transition kernel total variation current distribution respect invariant distribution decrease 
note incorporate standard mcmc methods gibbs sampler mh algorithm reversible jump mcmc filtering framework longer require kernel ergodic 

machine learning frontier machine learning frontier characterised large dimensional models massive datasets varied applications 
massive datasets pose problem smc context 
batch mcmc simulation possible load entire dataset memory 
solutions importance sampling proposed great room innovation area 
despite polynomial bounds mixing time arduous task design efficient samplers high dimensions 
combination sampling algorithms gradient optimisation exact methods proved useful 
gradient optimisation inherent langevin algorithms hybrid monte carlo 
algorithms shown large dimensional models neural networks neal gaussian processes barber williams 
information derivatives target distribution forms integral part adaptive schemes discussed section 
argued combination mcmc variational optimisation techniques lead efficient sampling de freitas 
combination exact inference sampling methods framework rao blackwellisation casella robert result great improvements 
suppose divide hidden variables groups andv conditional conditional posterior distribution analytically tractable 
easily marginalise posterior need focus sampling lies space reduced dimension 
sample exact inference compute identifying troublesome variables sampling rest problem solved easily exact algorithms kalman filters hmms junction trees 
example apply technique sample variables eliminate loops graphical models compute remaining variables efficient analytical algorithms jensen kong kj wilkinson yeung 
application areas include dynamic bayesian networks doucet conditionally gaussian models carter kohn de jong shephard doucet model averaging graphical models friedman koller issue 
problem automatically identify variables sampled handled analytically open 
interesting development augmentation high dimensional models low dimensional artificial variables 
sampling artificial variables original model decouples simpler tractable submodels albert chib andrieu de freitas doucet wood kohn see holmes denison issue 
strategy allows map probabilistic classification problems simpler regression problems 
design efficient sampling methods times hinges awareness basic building blocks mcmc mixtures kernels augmentation strategies blocking careful design proposal mechanisms 
requires domain specific knowledge heuristics 
great opportunities combining existing sub optimal algorithms mcmc machine learning problems 
areas benefiting sampling methods include 
computer vision 
tracking isard blake ormoneit fleet stereo matching dellaert issue colour constancy forsyth restoration old movies morris fitzgerald segmentation clark quinn kam tu zhu 

web statistics 
estimating coverage search engines proportions belonging specific domains average size web pages bar yossef 

speech audio processing 
signal enhancement godsill rayner 

probabilistic graphical models 
example gilks thomas spiegelhalter wilkinson yeung papers issue 

regression classification 
neural networks kernel machines andrieu de freitas doucet holmes mallick neal ller rios gaussian processes barber williams cart denison mallick smith mars holmes denison issue 

computer graphics 
light transport guibas sampling plausible solutions multi body constraint problems forsyth :10.1.1.40.2090
andrieu 
data association 
vehicle matching highway systems pasula multitarget tracking bergman 

decision theory 
partially observable markov decision processes pomdps thrun salmond gordon markov policies bui venkatesh west influence diagrams ller rios 

order probabilistic logic 
pasula russell 

genetics molecular biology 
dna microarray data west cancer gene mapping newton lee protein alignment neuwald linkage analysis jensen kong kj 

robotics 
robot localisation map building fox 

classical mixture models 
mixtures independent factor analysers mixtures factor analysers titterington issue 
hope review useful resource people wishing carry research interface mcmc machine learning 
conciseness skipped interesting ideas including tempering coupling 
details advise readers consult 
acknowledgments robin morris kevin murphy mark mike titterington 
townsend 

stochastic gradient optimization importance sampling efficient simulation digital communication systems 
ieee transactions communications 
albert chib 

bayesian analysis binary response data 
journal american statistical association 
anderson 

metropolis monte carlo 
los alamos science 
andrieu doucet 

joint bayesian detection estimation noisy sinusoids reversible jump mcmc 
ieee transactions signal processing 
andrieu doucet 

convergence simulated annealing foster lyapunov criteria 
technical report cued infeng tr cambridge university engineering department 
andrieu de freitas doucet 

sequential mcmc bayesian model selection 
ieee higher order statistics workshop israel pp 

andrieu de freitas doucet 

reversible jump mcmc simulated annealing neural networks 
uncertainty artificial intelligence pp 

san mateo ca morgan kaufmann 
andrieu de freitas doucet 

robust full bayesian methods neural networks 
solla leen 
ller eds advances neural information processing systems pp 

mit press 
andrieu de freitas doucet 

robust full bayesian learning radial basis networks 
neural computation 
andrieu de freitas doucet 

rao blackwellised particle filtering data augmentation 
advances neural information processing systems nips 
andrieu doucet 

sequential monte carlo methods optimal filtering 
doucet de freitas gordon eds sequential monte carlo methods practice 
berlin springer verlag 
applegate kannan 

sampling integration near log concave functions 
proceedings third annual acm symposium theory computing pp 

bar yossef berg chien 

approximating aggregate queries web pages random walks 
international conference large databases pp 

barber williams 

gaussian processes bayesian classification hybrid monte carlo 
mozer jordan petsche eds advances neural information processing systems pp 

cambridge ma mit press 
baum petrie soules weiss 

maximization technique occurring statistical analysis probabilistic functions markov chains 
annals mathematical statistics 
baxter 

exactly solved models statistical mechanics 
san diego ca academic press 
sullivan 

metropolis algorithm 
computing science engineering 
bergman 

recursive bayesian estimation navigation tracking applications 
ph thesis department electrical engineering link ping university sweden 
berners lee cailliau luotonen nielsen secret 

world wide web 
communications acm 
besag green mengersen 

bayesian computation stochastic systems 
statistical science 
ller rios 

decision analysis augmented probability simulation management science 
brooks 

markov chain monte carlo method application 
statistician 
browne draper 

implementation performance issues bayesian likelihood fitting multilevel models 
computational statistics 


adaptive sampling iterative fast monte carlo procedure 
structural safety 
bui venkatesh west 

recognition markov policies 
national conference artificial intelligence aaai 
carlin chib 

bayesian model choice mcmc 
journal royal statistical society series 
carter kohn 

gibbs sampling state space models 
biometrika 
casella robert 

rao blackwellisation sampling schemes 
biometrika 
casella mengersen robert titterington 

perfect slice samplers mixtures distributions 
technical report bu department biometrics cornell university 
celeux diebolt 

sem algorithm probabilistic teacher algorithm derived em algorithm mixture problem 
computational statistics quarterly 
celeux diebolt 

stochastic approximation type em algorithm mixture problem 
stochastics stochastics reports 
chen shao ibrahim 
eds 

monte carlo methods bayesian computation 
berlin springer verlag 
cheng druzdzel 

ais bn adaptive importance sampling algorithm evidential reasoning large bayesian networks 
journal artificial intelligence research 
forsyth 

sampling plausible solutions multi body constraint problems 
siggraph pp 

clark quinn 

data driven bayesian sampling scheme unsupervised image segmentation 
ieee international conference acoustics speech signal processing arizona vol 
pp 

damien wakefield walker 

gibbs sampling bayesian non conjugate hierarchical models auxiliary variables 
journal royal statistical society 
de freitas jen rensen jordan russell 

variational mcmc 
breese koller eds uncertainty artificial intelligence pp 

san ca morgan kaufmann 
de freitas niranjan gee doucet 

sequential monte carlo methods train neural network models 
neural computation 
andrieu de jong shephard 

efficient sampling smoothing density time series models 
biometrika 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
denison mallick smith 

bayesian cart algorithm 
biometrika 
diaconis saloff coste 

know metropolis algorithm 
journal computer system sciences 
doucet 

sequential simulation methods bayesian filtering 
technical report cued infeng tr department engineering cambridge university 
doucet de freitas gordon 
eds 

sequential monte carlo methods practice 
berlin springer verlag 
doucet de freitas murphy russell 

rao blackwellised particle filtering dynamic bayesian networks 
boutilier eds uncertainty artificial intelligence pp 

morgan kaufmann publishers 
doucet godsill andrieu 

sequential monte carlo sampling methods bayesian filtering 
statistics computing 
doucet godsill robert 

marginal maximum posteriori estimation mcmc 
technical report cued infeng tr cambridge university engineering department 
duane kennedy 

hybrid monte carlo 
physics letters 
dyer frieze kannan 

random polynomial time algorithm approximating volume convex bodies 
journal acm 


stan ulam john von neumann monte carlo method 
los alamos science 
escobar west 

bayesian density estimation inference mixtures 
journal american statistical association 
fill 

interruptible algorithm perfect sampling markov chains 
annals applied probability 
forsyth 

sampling resampling colour constancy 
ieee conference computer vision pattern recognition pp 

fox thrun burgard dellaert 

particle filters mobile robot localization 
doucet de freitas gordon eds sequential monte carlo methods practice 
berlin springer verlag 
gelfand sahu 

markov chain monte carlo acceleration 
journal computational graphical statistics 
gelfand smith 

sampling approaches calculating marginal densities 
journal american statistical association 
geman geman 

stochastic relaxation gibbs distributions bayesian restoration images 
ieee transactions pattern analysis machine intelligence 
geweke 

bayesian inference econometric models monte carlo integration 
econometrica 
ghahramani 

factorial learning em algorithm 
tesauro touretzky alspector eds advances neural information processing systems pp 

ghahramani jordan 

factorial hidden markov models 
technical report mit artificial intelligence lab ma 
gilks 

monte carlo inference dynamic bayesian models 
unpublished 
medical research council cambridge uk 
gilks roberts 

strategies improving mcmc 
gilks richardson spiegelhalter eds markov chain monte carlo practice pp 

chapman hall 
gilks richardson spiegelhalter 
eds 

markov chain monte carlo practice 
chapman hall 
gilks roberts sahu 

adaptive markov chain monte carlo regeneration 
journal american statistical association 
gilks thomas spiegelhalter 

language program complex bayesian modelling 
statistician 
godsill rayner 
eds 

digital audio restoration statistical model approach 
berlin springer verlag 
gordon salmond smith 

novel approach nonlinear non gaussian bayesian state estimation 
iee proceedings 
green 

reversible jump markov chain monte carlo computation bayesian model determination 
biometrika 
green richardson 

modelling heterogeneity dirichlet process 
department statistics bristol university 


simulated annealing process general state space 
advances applied probability 
hastings 

monte carlo sampling methods markov chains applications 
biometrika 
higdon 

auxiliary variable methods markov chain monte carlo application 
journal american statistical association 
holmes mallick 

bayesian radial basis functions variable dimension 
neural computation 
isard blake 

contour tracking stochastic propagation conditional density 
european conference computer vision pp 

cambridge uk 


application hybrid monte carlo bayesian generalized linear models separation neural networks 
journal computational graphical statistics 
jensen kong kj 

blocking gibbs sampling large probabilistic expert systems 
international journal human computer studies 
jerrum sinclair 

markov chain monte carlo method approach approximate counting integration 
hochbaum ed approximation algorithms np hard problems pp 

pws publishing 
jerrum sinclair 

polynomial time approximation algorithm permanent matrix 
technical report tr electronic colloquium computational complexity 


monte carlo methods 
new york john wiley sons 
kam 

general multiscale scheme unsupervised image segmentation 
ph thesis department engineering cambridge university cambridge uk 
kanazawa koller russell 

stochastic simulation algorithms dynamic probabilistic networks 
proceedings eleventh conference uncertainty artificial intelligence pp 

morgan kaufmann 
kannan li 

sampling multivariate normal density 
th annual symposium foundations computer science pp 

ieee 
kirkpatrick gelatt vecchi 

optimization simulated annealing 
science 
levine casella 

implementations monte carlo em algorithm 
journal computational graphical statistics 
liu 
ed 

monte carlo strategies scientific computing 
berlin springer verlag 
maceachern clyde liu 

sequential importance sampling nonparametric bayes models generation 
canadian journal statistics 
mcculloch 

maximum likelihood variance components estimation binary data 
journal american statistical association 
mengersen tweedie 

rates convergence hastings metropolis algorithms 
annals statistics 
metropolis ulam 

monte carlo method 
journal american statistical association 
metropolis rosenbluth rosenbluth teller teller 

equations state calculations fast computing machines 
journal chemical physics 
tweedie 

markov chains stochastic stability 
new york springer verlag 
andrieu mira 

ordering slicing splitting monte carlo markov chains 
ph thesis school statistics university minnesota 
morris fitzgerald 

sampling approach line scratch removal motion picture frames 
ieee international conference image processing pp 

ller rios 

issues bayesian analysis neural network models 
neural computation 
mykland tierney yu 

regeneration markov chain samplers 
journal american statistical association 
neal 

probabilistic inference markov chain monte carlo methods 
technical report crg tr dept computer science university toronto 
neal 

bayesian learning neural networks 
lecture notes statistics 
new york springer verlag 
neal 

slice sampling 
technical report department statistics university toronto 
neuwald liu lipman lawrence 

extracting protein alignment models sequence database 
nucleic acids research 
newton lee 

inferring location effect tumor suppressor genes instability selection modeling loss data 
biometrics 
ormoneit fleet 

lattice particle filters 
uncertainty artificial intelligence 
san mateo ca morgan kaufmann 
ortiz kaelbling 

adaptive importance sampling estimation structured domains 
boutilier eds uncertainty artificial intelligence pp 

san mateo ca morgan kaufmann publishers 
page brin motwani winograd 

pagerank citation ranking bringing order web 
stanford digital libraries working 
pasula russell 

approximate inference order probabilistic languages 
international joint conference artificial intelligence seattle 
pasula russell ritov 

tracking objects sensors 
international joint conference artificial intelligence stockholm 
pearl 

evidential reasoning stochastic simulation 
artificial intelligence 


optimum monte carlo sampling markov chains 
biometrika 
pitt shephard 

filtering simulation auxiliary particle filters 
journal american statistical association 
propp wilson 

coupling past user guide 
aldous propp eds discrete probability 
dimacs series discrete mathematics theoretical computer science 
srinivasan nicola van 

adaptive importance sampling performance evaluation parameter optimization communications systems 
ieee transactions communications 
richardson green 

bayesian analysis mixtures unknown number components 
journal royal statistical society 


generalization boosting algorithms applications bayesian inference massive datasets 
ph thesis department statistics university washington 
rios ller 

feedforward neural networks nonparametric regression 
dey ller sinha eds practical nonparametric semiparametric bayesian statistics pp 

springer verlag 
robert casella 

monte carlo statistical methods 
new york springer verlag 
roberts tweedie 

geometric convergence central limit theorems multidimensional hastings metropolis algorithms 
biometrika 
rubin 

sir algorithm simulate posterior distributions 
bernardo degroot lindley smith eds bayesian statistics pp 

cambridge ma oxford university press 
rubinstein 
eds 

simulation monte carlo method 
new york john wiley sons 
salmond gordon 

particles mixtures tracking guidance 
doucet de freitas gordon eds sequential monte carlo methods practice 
berlin springer verlag 
schuurmans 

monte carlo inference greedy importance sampling 
boutilier eds uncertainty artificial intelligence pp 

morgan kaufmann publishers 
sherman ho dalal 

conditions convergence monte carlo em sequences application product diffusion modeling 
econometrics journal 
smith shafi gao 

quick simulation review importance sampling techniques communications systems 
ieee journal selected areas communications 
stephens 

bayesian methods mixtures normal distributions 
ph thesis department statistics oxford university england 
swendsen wang 

critical dynamics monte carlo simulations 
physical review letters 
tanner wong 

calculation posterior distributions data augmentation 
journal american statistical association 
thrun 

monte carlo pomdps 
solla leen 
ller eds advances neural information processing systems pp 

cambridge ma mit press 
tierney 

markov chains exploring posterior distributions 
annals statistics 
tierney mira 

adaptive monte carlo methods bayesian inference 
statistics medicine 
godsill 

reversible jump sampler autoregressive time series 
international conference acoustics speech signal processing vol 
iv pp 

tu zhu 

image segmentation data driven markov chain monte carlo 
international computer vision conference 


ensemble independent factor analyzers application natural image analysis 
neural processing letters 
van der merwe doucet de freitas wan 

unscented particle filter 
technical report cued infeng tr cambridge university engineering department 
van laarhoven arts 

simulated annealing theory applications 
amsterdam reidel publishers 
guibas 

metropolis light transport 
siggraph 
andrieu doucet godsill 

non stationary bayesian modelling enhancement speech signals 
technical report cued infeng tr cambridge university engineering department 
wakefield gelfand smith 

efficient generation random variates ratio methods 
statistics computing 
wei tanner 

monte carlo implementation em algorithm poor man data augmentation algorithms 
journal american statistical association 
west marks 

bayesian regression analysis large small paradigm application dna microarray studies 
department statistics duke university 
wilkinson yeung 

conditional simulation highly structured gaussian systems application blocking mcmc bayesian analysis large linear models 
statistics computing 
wood kohn 

bayesian approach robust binary nonparametric regression 
journal american statistical association 
