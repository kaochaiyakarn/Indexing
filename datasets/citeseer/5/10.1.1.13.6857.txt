generalization decision tree induction efficient classification data mining kamber lara wan gong shan cheng jiawei han database systems research laboratory school computing science simon fraser university canada kamber han cs sfu ca efficiency scalability fundamental issues concerning data mining large databases 
classification studied extensively known methods take serious consideration efficient induction large databases analysis data multiple abstraction levels 
addresses efficiency scalability issues proposing data classification method integrates attribute oriented induction relevance analysis induction decision trees 
integration leads efficient high quality multiple level classification large amounts data relaxation requirement perfect training sets elegant handling continuous noisy data 

computational efficiency scalability important challenging issues data mining 
data mining automated discovery non trivial potentially useful patterns embedded databases 
increasing computerization aspects life led storage massive amounts data 
large scale data mining applications involving complex decision making access bytes data 
efficiency applications paramount 
classification key data mining technique database tuples acting training samples analyzed order produce model data 
tuple assumed belong predefined class determined attributes called classifying attribute 
derived classification model categorize data samples provide better understanding database contents 
classification numerous applications including credit approval product marketing medical diagnosis 
number classification techniques statistics machine learning communities proposed 
accepted method classification induction decision trees 
decision tree flow chart structure consisting internal nodes leaf nodes branches 
internal node represents decision test data attribute outgoing branch corresponds possible outcome test 
leaf node represents class 
order classify unlabeled data sample classifier tests attribute values sample decision tree 
path traced root leaf node holds class predication sample 
decision trees easily converted rules decision making 
efficiency existing decision tree algorithms id cart established relatively small data sets 
efficiency scalability issues concern algorithms applied mining large real world databases 
decision tree algorithms restriction training tuples reside main memory 
data mining applications large training sets millions examples common 
restriction limits scalability algorithms decision tree construction inefficient due swapping training samples main cache memories 
induction decision trees large training sets previously addressed sliq sprint decision tree algorithms 
propose pre sorting techniques disk resident data sets large fit memory 
sliq scalability limited memory resident data structure sprint removes memory restrictions handle data sets large sliq 
sliq sprint operate raw low level data address efficiency scalability issues proposing different approach consisting steps attribute oriented induction concept hierarchies generalize low level data higher level concepts relevance analysis multi level mining decision trees induced different levels abstraction 
attribute oriented induction knowledge discovery tool allows generalization data offers major advantages mining large databases 
allows raw data handled higher conceptual levels 
generalization performed attribute concept hierarchies leaves attribute concept hierarchy correspond attribute values data referred primitive level data 
generalization training data achieved replacing primitive level data numerical values gpa grade point average attribute higher level concepts ranges categories excellent 
attribute oriented induction allows user view data meaningful abstractions 
decision tree induction examples data classified relatively high concept level mild temperature high humidity 
implies preprocessing may performed primitive data bring higher concept level 
attribute oriented induction useful allows generalization data multiple level generalized concepts 
particularly useful continuous valued attributes 
furthermore attribute oriented induction addresses scalability issue compressing training data 
generalized training data compact original training set involve fewer input output operations 
illustrate section efficiency resulting decision tree induction greatly improved 
second step approach aids scalability performing attribute relevance analysis generalized data prior decision tree induction 
process identifies attributes irrelevant redundant 
including attributes generalized data slow possibly confuse classification process 
step improves classification efficiency enhances scalability classification procedures eliminating useless information reducing amount data input classification stage 
third final step approach multi level mining 
propose algorithms allow induction decision trees different levels abstraction employing knowledge stored concept hierarchies 
furthermore decision tree derived concept hierarchies generalize specialize individual nodes tree allowing attribute rolling drilling reclassification data newly specified abstraction level 
interactive feature executes dbminer data mining system fast response 
organized follows 
section describes attribute oriented induction relevance analysis multi level mining components proposed approach 
component implemented dbminer 
section presents proposed multi level decision tree induction algorithms 
performance evaluation section 
section addresses related issues classification accuracy 
discussed section 
proposed approach generalization induction decision trees address efficiency scalability issues regarding data mining large databases proposing technique composed steps generalization attribute oriented induction compress training data 
includes storage generalized data multidimensional data cube allow fast accessing relevance analysis remove irrelevant data attributes compacting training data multi level mining combines induction decision trees knowledge concept hierarchies 
section describes step detail 
real world applications data mining tasks classification applied training data consisting thousands millions tuples 
applying attribute oriented induction prior classification substantially reduces computational complexity data intensive process 
data compressed concept tree ascension technique replaces attribute values generalized concepts corresponding attribute concept hierarchies 
generalized tuple count associated 
registers number tuples original training data generalized tuple represents 
count information represents statistical data classification process enables handling noisy exceptional data 
concept hierarchies may provided domain experts database administrators may defined database schema 
concept hierarchies numeric attributes generated automatically 
addition allowing substantial reduction size training set concept hierarchies allow representation data user vocabulary 
aside increasing efficiency attribute may result classification trees understandable smaller easier interpret trees obtained methods operating larger sets low level data 
degree generalization controlled 
number distinct values attribute equal threshold generalization attribute halted 
population super large medium small population pub state pub super ne ne nc state 
multi dimensional data cube 
attribute oriented induction performs generalization attribute removal 
technique attribute having large number distinct values removed higher level concept 
attribute removal compacts training data reduces resulting trees 
approach generalized data stored multi dimensional data cube similar data warehousing 
multi dimensional array structure 
dimension represents generalized attribute cell stores value aggregate attribute count information described earlier 
consider database consisting statistics describing incomes populations collected cities counties united states 
attributes database include state population city population pub percentage population receiving public assistance 
multi dimensional data cube depicted 
original values state generalized ne north east nc north central south west population generalized small city medium city large city super city pub generalized 
dimension includes value representing aggregate sum entire dimension 
advantage multi allows fast indexing cells slices cube 
instance may easily quickly access total count cities population received public assistance states population sizes combined cell 
illustrate ideas attribute oriented induction usa north east north central south west new england middle atlantic mountain pacific east pacific west alaska hawaii usa north east west pacific west small city medium city large city super city small city super city 
concept hierarchies attributes state population database 
example described database 
data mining task classify data database attribute median income median family income attributes city area square miles state population pub 
task specified data mining query language shown example 
example classification task 
classify median income relevance city area state population pub income population income place population place income state population state prior applying attribute oriented induction query retrieves task relevant data performing relational query 
tuples satisfying clause ignored data concerning relevant attributes specified relevance clause class attribute median income collected 
example task relevant training data shown table 
city state population pub median area income alabama wisconsin wyoming table 
task relevant data database 
attribute oriented induction performed set relevant data 
intermediate generalized relation achieved concept ascension concept hierarchies attributes state population displayed table 
owing attribute removal technique city area appear resulting table distinct values concept hierarchy generalized 
identical tuples table merged collecting count cnt information 
practice resultant table substantially smaller original database task relevant data 
state pub median cnt tion income north east large city ks north east small city ks ks west large city ks ks table 
task relevant data generalized intermediate concept level count cnt 
high level attribute generalized prior application relevance analysis multi level mining 
possibilities 
generalization may proceed minimally generalized concept level attribute generalization thresholds described 
relation minimally generalized attribute satisfies generalization threshold specialization attribute cause attribute exceed generalization threshold 
disadvantage approach database attribute discrete values decision tree induced quite bushy large 
extreme generalization may proceed high concept levels 
resulting relation small subsequent decision tree induction efficient obtained minimally generalized data 
classification process may lose ability classes subclasses may able construct meaningful decision trees 
trade adopt generalize intermediate concept level 
level specified domain expert threshold defines desired number distinct values attribute distinct attribute values 
example may generalize attribute values state north east north central south west possible intermediate level derived predefined concept hierarchy may best suit classification task 
cases level adjustment process introduced im prove classification quality 
section describe technique level adjustment integrated induction decision trees 
second step approach apply attribute relevance analysis generalized data obtained attribute oriented induction 
reduces size training data 
number statistical machine learning techniques relevance analysis proposed literature :10.1.1.48.2488
employ information theoretic asymmetric measure relevance known uncertainty coefficient 
coefficient information gain attribute selection measure building decision trees 
relevance analysis performed follows 
generalized data set data samples 
suppose classifying attribute distinct values defining distinct classes 
suppose contains samples arbitrary sample belongs class probability expected information needed classify sample attribute values partition contains samples value contain samples class expected information partitioning information gained branching uncertainty coefficient attribute obtained normalizing information gain ranges meaning statistical independence tween classifying attribute strongest degree relevance attributes user option retaining relevant attributes attributes uncertainty coefficient value greater pre specified uncertainty threshold threshold user defined 
note efficient apply relevance analysis generalized data original training data 
analysis large sets data computationally expensive 
applying induction step order reduce size training data reduce amount computation required step approach 
furthermore relevance analysis contributes step removing attributes slow possibly confuse classification process 
third final step method multi level mining 
combines decision tree induction generalized data obtained steps attribute oriented induction relevance analysis knowledge concept hierarchies 
section introduce algorithms multi level mining 
detailed algorithm follows section 
section degree attribute oriented induction applied order generalize data discussed 
generalization minimally generalized concept level result large bushy trees 
generalization high concept levels result decision trees little overgeneralization may cause loss interesting important subconcepts 
determined desirable generalize intermediate concept level set domain expert controlled user specified threshold 
noted generalization intermediate level derived predefined concept hierarchy may best suit classification task 
improve classification quality useful incorporate concept level adjustment procedure process inducing decision trees 
propose algorithms 
algorithm operates training data generalized intermediate level attribute oriented induction unimportant attributes removed relevance analysis 
med gen simply applies decision tree induction resulting data 
applies modified decision tree algorithm allows adjustment concept levels attribute effort enhance classification quality 
algorithms confine processing relatively small generalized relations repeatedly accessing initially large raw data algorithms reasonable processing cost 

proposed algorithms section describes algorithms classification large databases greater detail 
algorithms apply attribute oriented induction generalize data intermediate level followed relevance analysis means compressing original large training set 
induce decision tree classifiers generalized relevant data 
directly applies decision tree algorithm generalized data allows dynamic adjustment different levels abstraction tree building process 
decision tree induction algorithm med gen earlier version known id 
chosen generally accepted standard decision tree algorithms extensively tested 
application areas ranging medicine game theory basis commercial rule induction systems 
furthermore allows attribute selection measure known information gain 
comparative study selection measures information gain produce accurate small trees 
repeatedly shown perform relatively small data sets 
section outlines algorithm 
described sections respectively 
method greedy tree growing algorithm constructs decision trees top recursive divide conquer strategy 
tree starts single node containing training samples 
samples class node leaf labeled class 
algorithm uses equations select attribute highest information gain 
attribute decision test attribute node 
branch created value decision attribute samples partitioned accordingly 
algorithm uses process recursively form decision tree partition 
recursive partitioning stops samples node belong class remaining attributes samples may partitioned 
algorithm integrates attribute oriented induction relevance analysis slightly modified version decision tree algorithm 
introduces thresholds part 
exception threshold classification threshold 
criticism recursive partitioning resulting data subsets may small partitioning statistically significant basis 
maximum size insignificant data subsets statistically determined 
deal problem introduces exception threshold 
portion samples subset threshold partitioning subset halted 
leaf node created stores subset class distribution subset samples 
owing large amount wide diversity data large databases may reasonable assume leaf node contain samples belonging common class 
addresses problem employing classification threshold partitioning data subset node terminated percentage samples belonging class node exceeds classification threshold 
classification threshold similar precision threshold introduced 
thresholds dynamic pruning may efficient traditional tree pruning strategies fully grow tree prune 
algorithm outlined 
algorithm perform data classification prespecified classifying attribute relational database integration attribute oriented induction relevance analysis decision tree induction method 
input 
data classification task specifies set relevant data classifying attribute relational database set concept hierarchies attributes set attribute thresholds generalization attributes exception threshold classification threshold output 
classification set data set classification rules 
method 

data collection collect relevant set data relational query processing 

attribute oriented induction concept hierarchies perform attribute generalize intermediate level 
resultant 
generalization level set attribute thresholds associated attributes 
relevance analysis perform relevance analysis generalized data relation 
information theoretic uncertainty coefficient purpose methods suggested 
resulting data relation 

decision tree generation generalized relevant data relation compute information gain candidate attribute equations 
select candidate attribute gives maximum information gain decision test attribute current level partition current set objects accordingly 
alternatively selection measurements gini index 
subset created partitioning repeat step classify data substantial proportion classification threshold objects class attributes classification percentage objects subclass respect total number training samples exception threshold 

classification rule generation generate rules decision tree derived 
rationale algorithm 
step relational query processing 
step verified 
step eliminates irrelevant attributes generalized data 
step essentially algorithm correctness shown 
modification uses quantitative information collected step terminate classification frequency majority class subset greater classification threshold percentage training objects represented subset exception threshold 
classification performed recursively 
step generates rules decision tree reflect general regularity data database 
algorithm performs steps algorithm replaces decision tree generation step step level adjusted decision tree generation integration decision tree generation process abstraction level adjustment 
allows generalization specialization abstraction level individual decision nodes tree 
level adjustment process consists node merge node split split node merge operations 
say node largely belongs class number class samples contains exceeds classification threshold node merge child nodes selected concept level share parent largely belong north east ks ks 
state north central east south central pacific east south atlantic north west south pacific west mountain west south central ks ks pub ks ks population ks ks ks ks ks ks small city super city 
ks ks ks ks ks ks state pub ks ks pub ks ks ks nodes merged algorithm nodes split algorithm 
decision trees obtained median income left right 
class nodes merged 
set nodes representing numeric ranges merged merge result range extension 
node split node largely belong class sub nodes node split grouping sub nodes 
split node merge split nodes may share parent largely belong class nodes merged 
certain split nodes may merged neighboring nodes 
node merging reduce number disjunctions derived classification rules simplifying classification results 
example consider classification task example generalized table concept hierarchies 
classify data median income attribute state examined intermediate level consisting values north east north central south west north north central north east cities regions high median income north central north east merged single node designated north 
hand cities west show obvious median income class category may split 
recall west consists subregions mountain pacific east pacific west mountain subregion shows obvious class ks ks median income subregions show obvious class node west split sub nodes mountain pacific east pacific west 
south node may split similar fashion 
shows resulting tree 
level adjustment process performed candidate attribute 
level adjusted attribute largest information gain selected decision node partition training data 
algorithm recursively applied subset stopping condition 
level adjustment procedure step outlined 
step 
level adjusted decision tree generation 

attribute level adjustment procedure finds partitioning performing node merge generalization node split specialization node split merge generalization specialization performed relevant attributes parallel 

compute information gain candidate attribute adjusted partitioning obtained 
select candidate attribute decision attribute calculated information gains obtained 

recursively perform step subclass node created partitioning substantial proportion objects class attributes classification size node falls specified exception threshold result level adjustment may partition occurs multiple levels abstraction principal motivating factor improved classification dynamic adjustment 

performance evaluation section describes experiments conducted study efficiency scalability proposed approach classification large databases 
recall prior decision tree induction task relevant data generalized number different degrees 
generalization training data generalized 
case typical applications decision tree classifiers 
call algorithm 

minimal level generalization training data generalized minimally generalized concept level see section 
subsequently decision tree induction applied 
call algorithm 

intermediate level generalization training data generalized intermediate concept level decision tree induction 
case proposed algorithms 

high level generalization training data generalized high concept level decision tree induction 
call algorithm 
generalized data induced typically smaller algorithms listed expect efficient 
discussed earlier expect classifications meaningful due overgeneralization 
include study 
furthermore expect gen efficient methods 
approach involves working large set raw data entails heavy data accessing 
contrast classification generalized data faster working smaller generalized relation 
strategy storing data multi dimensional data cube results increased computation efficiency 
granted overhead required set cube 
subsequent advantage stored data data cube allows fast indexing cells slices cube 
recall requires calculation information gain attribute considering attribute value node 
results heavy computation calculation memory addresses subsequent accessing raw data 
fast accessing quick calculation addresses feature data cube storage indexing system 
experiments focus comparing efficiency 
classification accuracy discussed section 
experiments conducted pentium pc mb ram running windows nt 
algorithms implemented dbminer microsoft visual 
dbminer accessible web db cs sfu ca dbminer 
results obtained execution query database 
query executed times average cpu time milliseconds calculated 
database consists mb described section 
graphs execution time cpu milliseconds algorithm various classification threshold settings 
expected execution time algorithm generally increases classification threshold 
algorithms 
average execution time cpu milliseconds algorithm various classification threshold settings 
tested exception threshold set 
threshold values empirically determined 
tuples task relevant data relation tuples minimal concept level generalized relation tuples intermediate level generalized relation processed 
order generalize data intermediate concept level spend time attribute oriented induction 
generalized intermediate level data smaller generalized minimal level data algorithms require time induce decision trees 
efficient algorithms 
see generalizing training data intermediate level improve efficiency decision tree induction 
expected efficient due extra processing time incurred level adjustment 
balance struck user desired levels automatic refinement abstraction level ensure classification quality 
average trees induced leaves respectively 
generalization reduces average size induced decision trees contributing improved understandability classification results 
dynamic pruning empirically set classification threshold control growth branches construction tree compared post pruning employing classification threshold avoid pre pruning 
error cost complexity post pruning algorithm shown produce small accurate decision trees 
post pruning requires greater computational effort grow tree fully generate examine variations tree pruned different degrees order find accurate tree statistically significant difference accuracy decision trees obtained dynamic post pruning strategies 
user perform additional multi level mining displayed tree clicking nonleaf node expand showing node children shrink collapsing node descendants node 
storage generalized data multi dimensional data cube allows fast attribute rolling drilling reclassification data newly specified abstraction level 
interactive feature dbminer fast executing real time allows user gain additional insight classified data 

discussion classification accuracy 
classification problems commonly assumed objects uniquely classifiable training sample belong class 
addition efficiency classification algorithms compared accuracy 
accuracy measured test set objects class labels known 
accuracy estimated number correct class predictions divided total number test samples 
classify object path traced attribute values object root tree leaf node 
majority class node deemed class prediction object 
test object class prediction compared known class object 
match class prediction counted correct 
counted incorrect 
owing wide diversity data large databases reasonable assume objects uniquely classifiable 
probable assume object may belong class 
accuracy classifiers large databases measured 
accuracy measure appropriate take account possibility samples belonging class 
problem adequately solved literature 
second guess heuristic address situation classification prediction counted correct agrees majority class leaf object falls agrees second class majority leaf 
class predictions disagree classes majority leaf counted incorrect 
method take consideration degree non unique classification objects complete solution 
difficult compare accuracy decision trees classifying attribute different levels abstraction 
example population classifying attribute tree may values say low level attribute concept hierarchy class values may values higher level small city super city 
situation difficult compare decision trees obtained 
cases user plays important role judging classification quality 
seen difficult assess accuracy classifiers large databases 
presently investigating form weighted accuracy measure considers correctness objects belonging multiple classes 
additional issues 
inherent weakness information gain attribute selection criterion tendency favor valued attributes 
quinlan offers solution problem 
attribute oriented induction solution results generalization values small set distinct values 
creating branch decision attribute value encounters problem caused unnecessary partitioning data 
various solutions proposed problem 
addresses problem allows node merging discouraging data 

proposed simple promising method data classification addresses efficiency scalability issues regarding data mining large databases 
proposed algorithms 
algorithm generalizes data intermediate abstraction level attribute oriented induction 
compresses data allowing data meaningful levels abstraction 
relevance analysis applied remove irrelevant data attributes compacting generalized data 
resulting data smaller original training set 
applies modified version algorithm extract classification rules generalized relation inducing decision tree 
refinement allows level adjustment multiple abstraction levels decision tree construction 
balance struck user desired levels automatic refinement abstraction level promote quality classification 
generalization training data multidimensional data cube proposed algorithms scalable efficient algorithms operate smaller compressed relation take advantage fast data accessing due indexing structure cube 
involves refinement procedure 
example plan consider merging cousin nodes addition merging siblings node merge 
agrawal ghosh imielinski iyer swami 
interval classifier database mining applications 
proc 
th intl 
conf 
large data bases vldb pages vancouver canada 
almuallim dietterich 
learning irrelevant features 
proc 
th nat 
conf 
artificial intelligence volume pages menlo park ca july 
aaai press 
breiman friedman olshen stone 
classification regression trees 
wadsworth 
buntine niblett 
comparison splitting rules decision tree induction 
machine learning 
fayyad piatetsky shapiro smyth 
knowledge discovery data mining unifying framework 
proc 
nd intl 
conf 
knowledge discovery data mining kdd pages portland oregon 
fayyad 
branching attribute values decision tree generation 
proc 
aaai conf pages aaai press 
fayyad weir 
automating analysis cataloging sky surveys 
fayyad piatetsky shapiro smyth uthurusamy editors advances knowledge discovery data mining pages 
aaai mit press 
fayyad piatetsky shapiro smyth uthurusamy 
advances knowledge discovery data mining 
aaai mit press 
frawley piatetsky shapiro matheus 
knowledge discovery databases overview 
piatetsky shapiro frawley editors knowledge databases pages 
aaai mit press 
freeman jr applied categorical data analysis 
marcel dekker new york ny 
han cai cercone 
data driven discovery quantitative rules relational databases 
ieee trans 
knowledge data engineering 
han fu 
dynamic generation refinement concept hierarchies knowledge discovery databases 
proc 
aaai workshop knowledge discovery databases kdd pages seattle wa 
han fu 
exploration power induction data mining 
fayyad piatetsky shapiro smyth uthurusamy editors advances knowledge discovery data mining pages 
aaai mit press 
han fu wang chiang gong koperski li lu rajan xia zaiane 
dbminer system mining knowledge large relational databases 
proc 
intl 
conf 
data mining knowledge discovery kdd pages portland oregon august 
harinarayan rajaraman ullman 
implementing data 
proc 
acm sigmod int 
conf 
management data pages montreal canada june 
holder 
intermediate decision trees 
proc 
th intl 
joint conf 
artificial intelligence pages montreal canada aug 
john 
irrelevant features subset selection problem 
cohen hirsh editors proc 
th intl 
conf 
machine learning pages san fransisco ca july 
morgan kaufmann 
kamber collins francis evans 
model segmentation multiple sclerosis lesions magnetic resonance brain images 
ieee transactions medical imaging 

descriptive inferential statistics 
allyn bacon 
mehta agrawal rissanen 
sliq fast scalable classifier data mining 
proc 
intl 
conf 
extending database technology edbt avignon france march 
mingers 
empirical comparison pruning methods decision tree induction 
machine learning 
mooney shavlik towell grove 
experimental comparison symbolic connectionist learning algorithms 
proc 
th intl 
joint conf 
artificial intelligence pages detroit mi aug 
morgan kaufmann 
piatetsky shapiro frawley 
knowledge discovery databases 
aaai mit press 
quinlan 
learning efficient classification procedures application chess games 
editor machine learning artificial intelligence approach vol 
pages 
morgan kaufmann 
quinlan 
induction decision trees 
machine learning 
quinlan 
programs machine learning 
morgan kaufmann 
shafer agrawal mehta 
sprint scalable parallel classifier data mining 
proc 
nd intl 
conf 
large data bases vldb pages mumbai bombay india 
uthurusamy fayyad 
learning useful rules inconclusive data 
piatetsky shapiro frawley editors knowledge discovery databases pages menlo park ca 
aaai mit press 
weiss 
empirical comparison pattern recognition neural nets machine learning classification methods 
proc 
th intl 
joint conf 
artificial intelligence pages detroit mi aug 
morgan kaufmann 
weiss kulikowski 
computer systems learn classification prediction methods statistics neural nets machine learning expert systems 
morgan kaufman 
