appears th usenix symposium internet technologies systems usits 
jim gray published landmark study causes failures tandem systems techniques tandem prevent failures :10.1.1.59.6561
seventeen years internet services replaced fault tolerant servers new kid availability block 
data large scale internet services analyzed causes failures potential effectiveness various techniques preventing mitigating service failure 
find operator error largest cause failures services operator error largest contributor time repair services configuration errors largest category operator errors failures custom written front software significant extensive online testing thoroughly exposing detecting component failures reduce failure rates service 
qualitatively find improvement maintenance tools systems service operations staff decrease time diagnose repair problems 

internet services fail done 
david oppenheimer ganapathi david patterson university california berkeley eecs computer science division soda hall berkeley ca usa patterson cs berkeley edu number popularity large scale internet services google msn yahoo 
grown significantly years 
services poised increase importance repository data ubiquitous computing systems platform new global scale services applications built 
services large scale need operation led designers incorporate number techniques achieving high availability 
failures occur 
architects operators services see problems failures part system failures provide important lessons systems community large scale systems fail techniques prevent failures 
attempt answer question internet services fail done studied post mortem reports user visible failures large scale internet services 
identify service components failure prone highest time repair ttr service operators researchers know areas need improvement discuss detail instructive failure case studies examine applicability number failure mitigation techniques actual failures studied highlight need improved operator tools systems collection industry wide failure data creation service level benchmarks 
remainder organized follows 
section describe services analyzed study methodology 
section analyzes causes times repair component service failures examined 
section assesses applicability variety failure mitigation techniques actual failures observed services 
section case studies highlight interesting failure causes 
section discusses qualitative observations data section surveys related section conclude 

survey services methodology studied mature online service internet portal online bleeding edge global content hosting service content mature read internet service 
physically services housed geographically distributed colocation facilities commodity hardware networks 
architecturally site built load balancing tier stateless front tier back tier stores persistent data 
load balancing geographically distributed sites performance availability achieved dns redirection client cooperation online content 
front nodes initially contacted clients client proxy nodes content 
definition front nodes store appears th usenix symposium internet technologies systems usits 
service characteristic online content hits day machines sites sites sites front node architecture solaris sparc open source os open source os beck node architecture network appliance open source os open source os period data studied months months months component failures service failures table differentiating characteristics services described study 
sistent data may cache temporarily queue data 
back nodes store persistent data 
business logic traditional tier system terminology part definition front services integrate service logic code receives replies client requests 
front tier responsible primarily locating data back machine routing clients content providing online services email newsgroups web proxy online 
content front includes software running colocation sites client proxy software running hardware provided operated content physically located customer sites 
content geographically distributed colocation centers dozen customer sites 
front software sites content back software 
show service architectures content online respectively 
operationally services primarily custom written software administer service undergo frequent software upgrades configuration updates operate system operations centers operators monitor service respond problems 
table lists primary characteristics differentiate services 
details architecture operational practices services 
interested largescale internet services fail studied individual problem reports aggregate availability statistics 
operations staff services databases record information component service failures 
services online content gave access databases services gave access problem post mortem reports written major service failure 
online content defined user visible failure call service failure theoretically prevents user accessing service part service user reasonable error message significantly degrades user visible aspect system performance service failures caused component failures masked 
base dataset consisted reports component failures online component failures content 
component failures turned service failures online service failures content 
supplied service failures additional failures considered paired client service proxies internet load balancing switch total metadata servers total data storage servers paired backup site architecture site content 
stateless metadata servers provide file metadata route requests appropriate data storage servers 
persistent state stored commodity pc storage servers accessed custom protocol udp 
cluster connected twin site internet 
appears th usenix symposium internet technologies systems usits 
clients internet load balancing switch sparc solaris stateless workers stateful sparc solaris total services ail news favorites total users em ail new src 
total filesystem storage threshold deemed service failure 
problems corresponded months online months months content 
classifying problems considered operators component system fail failure may may result service failure 
attributed cause service failure component failed chain events leading service failure 
cause component failure categorized node hardware network hardware node software network software router switch firmware environment power failure operator error overload unknown 
location component categorized front node back node network unknown 
note significantly degrades user visible aspect system performance admittedly vaguely defined metric 
preferable correlate failure reports degradation aspect user observed quality service response time access archive metrics services 
note service measures archives response times data guaranteed detect user visible failures due periodicity placement network probes 
sum definition user visible problems potentially visible user tried access service failure 
news article storage solaris total web proxy cache database stateless workers stateless services content portals storage custom er records crypto keys billing info architecture site online 
depending particular feature user selects request routed web proxy cache servers servers stateless services servers user service group partition sixth users service backend data storage server 
persistent state stored network appliance servers accessed worker nodes nfs udp 
site connected second site collocation facility leased network connection 
underlying flaw may remained latent time cause component fail component particular way time 
due inconsistencies services security incidents break ins denial service attacks recorded problem tracking clients load balancing switch internet user user queries queries responses responses total web frontends total load balancing switch paired backup site storage back ends architecture site read 
small number web front ends direct requests appropriate back storage servers 
persistent state stored commodity pc storage servers accessed custom protocol tcp 
redundant pair network switches connects cluster internet twin site leased network connection 
appears th usenix symposium internet technologies systems usits 
databases ignored security incidents 
problems relatively easy map dimensional cause location space widearea network problems 
network problems affected links colocation facilities services content client sites colocation facilities 
root cause problems lay network internet service provider records access best problems label location network cause unknown 
analysis failure causes analyzed data component service failure respect properties component failures turn service failures section relative frequency component service failure root cause section mttr service failures section 

component failures service failures services studied redundancy attempt mask component failures 
try prevent component failures turning user visible failures 
indicated technique generally job preventing hardware software network component failures turning service failures effective masking operator failures 
qualitative analysis failure data suggests operator actions tend performed files affect operation entire service partition service configuration files content files 
difficulties masking network failures generally stemmed significantly smaller degree network redundancy compared node redundancy 
observed online non servers appeared reliable equivalent expensive servers 
apparently expensive hardware isn reliable 

service failure root cause examine source magnitude service failures categorized root cause location component type 
augmented data set previous section examining months data online yielding additional service failures bringing total service 
analyze component failures turn service failures extra months incidents component failure system failure content node operator node hardware number component failures resulting service failures content 
categories classified component failures operator error related node operation node hardware failure node software failure network failure unknown cause listed 
vast majority network failures content unknown cause network failures problems internet connections colocation facilities customer proxy sites colocation facilities 
node operator case fewer component failures service failures 
fully half operator errors resulted service failure suggesting operator errors significantly difficult mask service existing redundancy mechanisms 
exclusion section table shows contrary conventional wisdom front machines significant source failure fact responsible half service failures online content 
fact largely due operator configuration errors application operating system level 
problems network related attribute simpler better tested application software service fewer changes service day day basis higher degree node redundancy online content 
table shows operator error leading cause service failure services 
node software component failure service failure net unknown incidents appears th usenix symposium internet technologies systems usits 
component failure system failure online node operator node hardware node software net operator component failure service failure number component failures resulting service failures online 
categories classified component failures operator error related node operation node hardware failure node software failure various types network failure listed 
content operator error difficult mask service existing redundancy schemes 
content significant percentage network hardware failures service failures 
single explanation customer impacting network hardware problems affected various pieces equipment 
operator error services generally took form misconfiguration procedural errors moving user wrong fileserver 
services case nearly operator errors led service operator node operator net net hardware net net unknown node net failures configuration errors 
general operator errors arose operators making changes system scaling replacing hardware deploying upgrading software 
failures caused operator errors process fixing problem minority operator errors recorded problem tracking databases arose normal maintenance 
networking problems significant cause failure services caused surprising service failures 
mentioned section network failures masked node hardware software failures 
important reason fact networks single point failure services rarely redundant network paths equipment single site 
consolidation collocation network provider industries increased likelihood redundant network links collocation facility share physical link fairly close terms internet topology data center 
second reason networking problems difficult mask failure modes tend complex networking hardware software fail outright gradually overloaded start dropping packets 
combined inherent redundancy internet node net frontend backend unknown node network unknown net unknown online content read table service failure cause location 
contrary conventional wisdom failure root causes components service front 
environ ment online content read table service failure cause component type cause 
component described node network failure cause described operator error hardware software unknown environment 
excluded overload category small number failures caused 
appears th usenix symposium internet technologies systems usits 
failure modes generally lead increased latency decreased throughput experienced intermittently far fail behavior high reliability hardware software components aim achieve 
colocation facilities effective eliminating environmental problems environmental problems power failure led service failure power failure occur geographic redundancy saved day 
observed overload due non malicious causes insignificant 
comparing service failure data data component failures section note service failures component failures arise primarily front 
hardware software problems dominate operator error terms component failure causes 
case operator error frequent hardware software problems just frequently masked results service failure 
note able learn detailed causes software hardware failures able examine individual component system logs services software bug tracking databases 
example able break software failures operating system vs application theshelf vs custom written determined specific coding errors led software bugs 
cases operations problem tracking database entries provide sufficient detail classifications attempt 

service failure time repair analyze average time repair ttr service failures define time problem detection restoration service quality service problems repaired rebooting restarting component ttr time detection problem reboot complete 
problems repaired replacing failed component dead network switch disk drive time detection problem component replaced functioning 
problems break service functionally solved rebooting operator configuration error non transient software bug time error corrected definition service failure restoration service pre failure qos empirical measurement system qos inference system architecture component failed operator log repair process 
workaround put place whichever happens 
note ttr incorporates time needed diagnose problem time needed repair time needed detect problem definition problem go problem tracking database detected 
analyzed subset service failures section respect ttr 
categorized ttr problem root cause location type 
table inconclusive respect front failures take longer repair back failures 
table demonstrates operator errors take significantly longer repair types failures operator error contributed approximately time repair hours online content 
note unfortunately ttr values misleading ttr problem requires operator intervention partially depends priority operator places diagnosing repairing problem 
priority turn depends operator judgment impact problem service 
problems urgent cpu failure machine holding unreplicated database containing mapping service user ids passwords 
case repair initiated immediately 
problems problem occurs different context urgent cpu failure redundant front nodes addressed casually database cpu failure 
generally problem priority judged operator depends purely technical metrics performance degradation business oriented metrics importance customer affected problem importance part service experienced problem service email system may considered critical system generates advertisements vice versa 
front back network online content read table average ttr part service hours 
number parentheses number service failures compute average 
appears th usenix symposium internet technologies systems usits 
operator node operator net node 
techniques mitigating failures user visible failures inevitable despite services attempts prevent service failures observed avoided impact reduced 
answer question analyzed service failures online asking number techniques suggested improving availability potentially prevent original component design flaw fault prevent component fault turning component failure reduce severity degradation user perceived qos due component failure reduce degree service failure observed net node net unknown node unknown net online content read table average ttr failures component type cause hours 
component described node network failure cause described operator error hardware software unknown environment 
number parentheses number service failures compute average 
excluded overload category small number failures due cause 
component failure normal normal operation component fault operation component failure service qos significantly impacted service service failure failure failure detected service qos impacted negligibly failure detected problem queue diagnosis repair completed reduce time detection ttd time component failure detection failure reduce time repair ttr time component failure detection component repair 
interval corresponds time system qos degraded 
shows categories viewed state machine timeline component fault leading component failure possibly causing service failure component failure eventually detected diagnosed repaired returning system failure free qos 
techniques investigate potential effectiveness diagnosis initiated problem problem component queue diagnosis repair diagnosed repaired repair completed initiated timeline failure 
system starts normal operation 
component fault software bug alpha particle flipping memory bit operator misunderstanding configuration system modify may may eventually lead affected component fail 
component failure may may significantly impact service qos 
case simple component failure operating system bug leading kernel panic component failure may automatically detected diagnosed operating system notices attempt twice free block kernel memory repair initiating reboot automatically initiated 
complex component failure may require operator intervention detection diagnosis repair 
case system eventually returns normal operation 
study ttr denote time failure detected repair completed appears th usenix symposium internet technologies systems usits 
correctness testing testing system components correct behavior deployment production 
pre deployment testing prevents component faults deployed system online testing detects faulty components fail normal operation 
online testing catch failures created test situation example scale configuration dependent 
redundancy replicating data computational functionality networking functionality 
sufficient redundancy prevents component failures turning service failures 
fault injection load testing testing error handling code system response overload artificially introducing failure overload deployment production system 
aims prevent components faulty error handling load handling capabilities deployed online detects components faulty load handling capabilities fail properly handle anticipated faults loads 
configuration checking tools check low level component configuration files meet constraints expressed terms desired high level service behavior 
tools prevent faulty configurations deployed systems 
component isolation increasing isolation software components 
isolation prevent component failure turning service failure preventing cascading failures 
proactive restart periodic rebooting hardware restarting software 
prevent faulty components latent errors due resource leaks failing 
exposing monitoring failures better exposing software hardware component failures modules monitoring system better tools diagnose problems 
technique reduce time detect diagnose repair component failures especially important systems built redundancy masks component failures 
course implementing online testing online fault injection proactive restart care taken avoid interfering operational system 
service existing partitioning redundancy may exploited prevent operations interfering service delivered users additional isolation necessary 
table shows number problems online problem tracking database technique potentially prevented problem directly caused system enter corresponding failure state 
technique generally addresses system failure states listed failure states consider feasibly addressed corresponding technique 
analysis retrospect tried particularly careful assume reasonable application technique 
example trace past failed successful user requests input online regression testing mechanism considered reasonable software change creating bizarre combination inputs seemingly triggers failure 
note technique prevents problem causing system enter failure state necessarily prevents problem causing system enter subsequent failure state 
example technique online correctness testing expose monitor failures expose monitor failures system state transition avoided mitigated component failure component repaired problem diagnosed instances potentially avoided mitigated redundancy service failure config 
checking component fault online fault load injection component failure component isolation service failure pre deployment fault load injection component fault proactive restart component fail pre deployment correctness testing component fault table potential benefit online various proposed techniques avoiding mitigating failures 
service failures examined taken time period analyzed section 
techniques online indicated italics cases evaluate benefit technique extensively 
appears th usenix symposium internet technologies systems usits 
preventing component fault prevents fault turning failure degradation qos need detect diagnose repair failure 
note techniques reduce time detect diagnose repair component failure reduce service loss experienced amount qos lost failure multiplied length failure 
table observe online testing helped mitigating service failures 
second helpful technique thoroughly exposing monitoring software hardware failures decreased ttr ttd instances 
simply increasing redundancy mitigated failures 
automatic sanity checking configuration files online fault load injection appear offer significant potential benefit 
note techniques online uses redundancy monitoring isolation proactive restart pre deployment online testing table underestimates effectiveness adding techniques system 
naturally failure mitigation techniques described section benefits costs 
costs may financial technical 
technical costs may come form performance degradation increasing service response time reducing throughput reduced reliability complexity technique means bugs technique implementation 
table analyzes proposed failure mitigation techniques respect costs 
cost tradeoff mind observe techniques adding additional redundancy better exposing monitoring failures offer significant bang buck sense help mitigate relatively large number failure scenarios incurring relatively low cost 
clearly better online correctness testing mitigated large number system failures online exposing latent component faults turned failures 
kind online testing helped fairly high level self tests require application semantic information posting news article checking see showed newsgroup sending email checking see received correctly timely fashion 
unfortunately kinds tests hard write need changed time service functionality interface changes 
qualitatively say kind testing helped services examined useful technique 
online fault injection load testing likewise helped online services 
observation goes hand hand need better expos ing failures monitoring failures online fault injection load testing ways ensure component failure monitoring mechanisms correct sufficient 
choosing set representative faults error conditions instrumenting code inject monitoring response requires potentially online correctness testing 
online fault injection load testing require performance reliability isolated subset production service threat pose performance reliability production system 
despite best intentions offline test clusters tend set slightly differently production cluster online approach appears offer potential benefit offline version 

failure case studies section examine detail instructive service failures online failure content related service provided operations staff opposed users 
case study illustrates operator error affecting front machines 
problem operator online accidentally brought half front servers service group partition users administrative shutdown com technique expose monitor implementation cost medium high medium potential ity cost low moderate low false alarms perform ance impact low moderate low redundancy low low low online fault load high high moderate high config medium zero zero isolation moderate low moderate pre fault load high zero zero restart low low low pre correct medium high zero zero table costs implementing failure mitigation techniques described section 
appears th usenix symposium internet technologies systems usits 
mand issued separately servers 
technique redundancy mitigated failure service remote console remote power supply control servers operator physically travel colocation site reboot machines leading minutes users affected service group experienced performance degradation stateful services 
remote console remote power supply control redundant control path form redundancy 
lesson learned improving redundancy service accomplished replicating partitioning existing data service code 
redundancy come form orthogonal redundancy backup control path 
second interesting case study software error affecting service front provides example cascading failure 
problem software upgrade front daemon handles username alias lookups email delivery incorrectly changed format string daemon query back database stores usernames aliases 
daemon continually retried lookups looks failing eventually overloading back database bringing services database 
email servers overloaded perform necessary username alias lookups 
problem fixed rolling back software upgrade rebooting database front nodes relieving database overload problem preventing recurring 
online testing caught problem pre deployment component testing failure scenario dependent interaction new software module unchanged back database 
throttling back username alias lookups started failing repeatedly short period time mitigated failure 
isolation prevented database overloaded unusable providing services username alias lookups 
third interesting case study operator error affecting front machines 
situation users noticed news postings showing service newsgroups 
news postings local moderated newsgroups received users front news daemon converted email sent special email server 
delivery email server triggers execution script verifies validity user posting message 
sender valid online user verification fails server silently drops message 
service operator point configured email server run daemon looks usernames aliases server silently dropping converted email messages receiving 
operator accidentally configured email server run lookup daemon realize proper operation mail server depended running daemon 
lessons learned software silently drop messages data response error condition importantly operators need understand high level dependencies interactions software modules comprise service 
online testing detected problem better exposing failures improved techniques diagnosing failures decreased time needed detect localize problem 
online regression testing take place changes software components changes system configuration 
fourth failure studied arose problem interface online external service 
online uses external provider services 
external provider configuration change service restrict ip addresses users connect 
process accidentally blocked clients online 
problem difficult diagnose lack thorough error reporting online software poor communication online external service problem diagnosis external service change 
online testing security change detected problem 
problems interface providers increasingly common composed network services common 
techniques prevented failures described section orthogonal redundancy isolation understanding high level dependencies software modules difficult essential reliability world planetary scale ecologies networked services 
mentioned collect statistics problem reports pertaining systems failure directly affect user experience 
particular consider problem reports pertaining hardware software support system administration operational activities 
incident merits special mention provides excellent example multiple related non cascading component failures contributing single failure 
ironically problem led destruction online entire problem tracking database conducting research 
appears th usenix symposium internet technologies systems usits 
content problem tracking database stored commercial database 
data supposed backed regularly tape 
additionally data remotely mirrored night second machine just service data remotely mirrored night backup 
unfortunately database backup program running year half configuration problem related database host connects host tape drive 
considered low priority issue remote mirroring ensured existence backup copy data 
night disk holding primary copy problem tracking database failed leaving backup copy copy database 
unfortunate coincidence operator host holding backup copy database evening realized primary copy problem tracking database destroyed re imaging destroy remaining copy 
learn lessons failure 
lightning strike twice completely unrelated component failures happen simultaneously leading system level redundancy fail 
second categorizing failures particularly operator error tricky 
example backup machine primary failed really operator error operator unaware primary failed 
intentionally leaving tape backup broken operator error 
consider operator errors arguably understandable ones 
third vital operators understand current configuration state system architectural dependencies relationships components 
systems designed mask failures prevent operators knowing system margin safety reduced 
correct operator behavior previous problem replicate backup copy database machine holding 
order know operator needed know primary copy problem tracking database destroyed machine held backup copy database 
understanding system affected change particularly important embarking destructive operations impossible undo machine 

discussion section describe areas currently receiving little research attention believe help substantially improve availability internet services better operator tools systems creation industry wide failure repository adoption standardized service level benchmarks 
comment representativeness data 

operators class users despite huge contribution operator error service failure operator error completely overlooked designing high dependability systems tools monitor control 
oversight particularly problematic data shows operator error difficult component failure mask traditional techniques 
industry paid great deal attention user experience neglected tools systems operators configuration monitoring diagnosis repair 
previously mentioned majority operator errors leading service failure misconfigurations 
techniques improve situation 
improved operator interfaces 
mean simple gui wrapper existing component command line configuration mechanisms need fundamental advances tools help operators understand existing system configuration component dependencies changes component configuration affect service 
tools help visualize existing system configuration dependencies operator errors configuration related ensuring operator mental model existing system configuration matched true configuration 
approach build tools configuration files lint programs check configuration files known constraints 
tools built incrementally support additional types configuration files constraints added time 
idea extended ways 
support added user defined constraints form high level specification desired system configuration behavior viewed user extensible version lint 
second high level specification automatically generate component configuration files 
highlevel specification operator intent form semantic redundancy technique useful catching errors contexts types programming languages data structure invariants 
unfortunately widely generic tools allow operator specify high level way desired service architecture behavior specification checked existing configuration appears th usenix symposium internet technologies systems usits 
component configurations generated 
wide configuration interface remains error prone 
overarching difficulty related diagnosing repairing problems relates collaborative problem solving 
internet services require coordinated activity multiple administrative entities multiple individuals organizations diagnosing solving problems 
entities include operations staff service service software developers operators collocation facilities service uses network providers service collocation facilities customers 
today coordination handled entirely manually telephone calls contacts various points 
process greatly improved sharing unified problem tracking bug database entities deploying collaboration tools cooperative 
allowing collaboration useful properties problem tracking database gives operators history actions performed system indication actions performed 
unfortunately history human generated form operator annotations problem report walk steps diagnosing repairing problem 
tool structured way expresses history system including configuration system state change change change exactly changes help operators understand problem evolved aiding diagnosis repair 
tools determining root cause problems administrative domains traceroute rudimentary tools generally distinguish certain types problems host failures network problems network segment node located 
tools fixing problem source located controlled administrative entity owns broken hardware software site determines problem exists 
difficulties lead increased diagnosis repair times 
need tools techniques problem diagnosis repair effectively administrative boundaries correlate system observations multiple network vantage points greater age composed network services built top emerging platforms microsoft net sun 
systems tools operators administer services just primitive difficult brittle 
collect detailed statistics failures systems service operations observed reports failures components 
content reports problem tracking database related administrative machines services 
qualitative analysis problems services reveals organizations build operational side services redundancy pre deployment quality control parts service users 
philosophy operators problems administrative tools machines goes wrong users face service problems 
unfortunately result philosophy increased time detect diagnose repair problems due fragile administrative systems 

worldwide failure data repository analyzing failure data straightforward initial expectation turned far truth 
internet services studied recorded component failures database expected able simply write database queries collect quantitative data 
unfortunately operators frequently filled database forms incorrectly example fields problem starting time problem time root cause problem customer impacting service failure opposed just component failure contradicted timestamped operator narrative events accompanied problem reports 
data gathered reading operator narrative problem report accepting pre analyzed database data blind faith 
likewise insufficiently detailed problem reports led difficulty determining actual root cause problem time repair 
lack historical user perceived service qos measurements prevented rigorously defining service failure calculating user perceived service availability time period corresponding problem reports examined 
believe internet services follow lead fields aviation collecting publishing detailed industry wide failure cause data standardized format 
knowing real systems fail impact failures researchers practitioners know target efforts 
services ones studied databases store problem tracking information 
possible establish standard schema appears th usenix symposium internet technologies systems usits 
extensible internet services network providers colocation facilities related entities recording problems impact resolutions 
proposed possible failure cause location taxonomy schema 
standard schema benefit researchers services establishing sharing databases help address coordination problem described section 
note services release data want publicly available version database anonymized automating necessary anonymization nontrivial research question 

performability recovery benchmarks addition focusing dependability research real world problem spots failure data collected create fault model terminology component failure model service level performability benchmarks 
benchmarking efforts focused component level dependability observing single node application os response misbehaving disks system calls 
significant contribution service failure human error particularly multi node configuration problems network including wan problems suggest approach 
performability benchmarks small scale replica physically virtually isolated partition service created qos representative service workload mix measured representative component failures described injected 
simplify process measure qos impact individual component failures multiple simultaneous failures weight degraded qos response events relative frequency different classes component failure occur service benchmarked proportions survey 
important metrics measure addition qos impact injected failures time detect diagnose repair component failure automatically human 
suggested workload include standard service administrative tasks 
step type benchmark described 

representativeness data taken just services feel representative large scale internet services custom written software provide service 
giant scale services informally surveyed custom written software front scalability performance reasons 
information feel results apply internet services 
hand data somewhat skewed fact sites content read call content intensive meaning time spent transferring data back media front node large compared amount time front node spends processing request response 
sites content intensive custom written back software case online 
additionally services studied user requests require transactional semantics 
sites require transactional semantics commerce sites database back ends custom written back software 
theory factors tend decrease failure rate backend software online site shelf back software site lowest fraction back service failures 

related adds small body existing studies suggestions internet service architectures 
aware studies failure causes services 
number studies published causes failure various types computer systems commonly running internet sites operational environments internet services 
gray responsible widely cited studies computer system failure data 
operator error largest single cause failure deployed tandem systems accounting failures software runner 
strikingly similar percentages online content 
gray software major source outages swamping second largest contributor system operations 
note gray attributed failure component failure chain root cause making statistics directly comparable fact matters cascading failures 
quantitatively analyze length failure chains gray longer chains failure chains length gray study length length 
kuhn examined years data failures public switched telephone network reported appears th usenix symposium internet technologies systems usits 
fcc telephone companies 
concluded human error responsible failure incidents customer minutes number customers impacted multiplied number minutes failure lasted 
extended study examining year worth data examining blocked calls metric collected outage reports 
came similar human error responsible outages customer minutes blocked calls 
studies examined failures networks workstations 
thakur examined failures network sunos workstations divided problem root cause coarsely network non disk machine problems disk related machine problems 
studied months event logs lan windows nt workstations mail delivery determine causes machines rebooting 
problems software related average downtime hours 
closely related study xu examined network windows nt workstations enterprise infrastructure services studying windows nt event log entries related system reboots 
thakur studies allowed operators annotate event log indicate reason reboot authors able draw contribution operator failure system outages 
planned maintenance software installation configuration caused largest number outages system software planned maintenance caused largest amount total downtime 
unable classify large percentage problems 
note counted reboots installation patching software failure software installation configuration category comparable operator failure category despite named somewhat similarly 
number researchers examined causes failures enterprise class server environments 
sullivan chillarege examined software defects mvs db ims 
tang iyer conducted similar study vax vms operating system 
lee iyer categorized software faults tandem guardian operating system 
murphy gent examined causes system crashes vax systems hardware caused failures software system management bit 
study similar spirit studies network failure causes mahajan examined causes bgp misconfigurations labovitz conducted earlier study general causes failure ip backbones 

study component failures dozens user visible failures large scale internet services observe operator error leading cause failure services studied operator error largest contributor time repair services configuration errors largest category operator errors failures custom written front software significant extensive online testing thoroughly exposing detecting component failures reduce failure rates service 
availability certainly unattainable observations suggest internet service availability significantly enhanced proactive online testing thoroughly exposing monitoring component failures sanity checking configuration files logging operator actions improving operator tools distributed collaborative diagnosis problem tracking 
extent tools tasks exist generally ad hoc retrofitted existing systems 
believe important service software built ground concern testing monitoring diagnosis maintainability mind essence treating operators firstclass users 
accomplish apis emerging services allow easy online testing fault injection automatic propagation errors code modules operators handle distributed data flow tracing help detect diagnose debug performance reliability failures 
believe research system reliability benefit greatly industry wide publicly accessible failure database service level performability recovery benchmarks objectively evaluate designs improved system availability maintainability 
extend sincere gratitude operations staff management internet services provided data answered questions data 
possible help 
regret services individuals name due confidentiality agreements appears th usenix symposium internet technologies systems usits 

shepherd hank levy anonymous reviewers helpful feedback earlier versions members berkeley stanford roc group contributing ideas 
supported part defense advanced research projects agency department defense contract 
dabt national science foundation 
ccr nsf infrastructure 
eia hewlett packard microsoft california state micro program 
information necessarily reflect position policy government official endorsement inferred 
brewer 
lessons giant scale services 
ieee internet computing vol 

brown patterson 
err human 
proceedings workshop evaluating architecting system dependability easy 
engler chelf chou hallem 
checking system rules system specific programmer written compiler extensions 
th symposium operating systems design implementation osdi 
brown patterson 
lessons pstn dependable computing 
workshop self healing adaptive self managed systems 
gray 
census tandem system availability 
tandem computers technical report 
gray 
computers done 
symposium reliability distributed software database systems 
huang fulton 
software rejuvenation analysis models applications 
th symposium fault tolerant computing 
johnson 
lint program checker 
bell laboratories computer science technical report 
iyer 
failure data analysis lan windows nt computers 
th ieee symposium reliable distributed systems 
kuhn 
sources failure public switched telephone network 
ieee computer 
labovitz ahuja jahanian 
experimental study internet stability backbone failures 
fault tolerant computing symposium ftcs 
lee iyer 
software dependability tandem guardian system 
ieee transactions software engineering 
mahajan wetherall anderson 
understanding bgp misconfiguration 
sigcomm 
microsoft 
building scalable services 
www microsoft com default asp url ecommerce deploy bss asp 
murphy gent 
measuring system software reliability automated data collection process 
quality reliability engineering international vol 
li martin nguyen 
fault injection modeling evaluate performability cluster services 
th usenix symposium internet technologies systems usits 
oppenheimer patterson 
architecture operation dependability large scale internet services case studies 
ieee internet computing september october 
patterson brown candea chen cutler fox oppenheimer sastry tetzlaff 
recovery oriented computing roc motivation definition techniques case studies 
technical report ucb csd 
sullivan chillarege 
comparison software defects database management systems operating systems 
proceedings nd international symposium fault tolerant computing 
tang iyer 
analysis vax vms error logs multicomputer environments case study software dependability 
proceedings third international symposium software reliability engineering 
thakur iyer 
analyze environment collection adn analysis failures network workstations 
ieee transactions reliability 
xu iyer 
networked windows nt system field failure data analysis 
proceedings pacific rim international symposium dependable computing 
