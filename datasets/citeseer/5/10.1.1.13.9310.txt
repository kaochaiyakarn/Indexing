scheduler activations effective kernel support user level management parallelism thomas anderson brian bershad edward lazowska henry levy department computer science engineering university washington seattle wa threads vehicle concurrency approaches parallel programming 
threads separate notion sequential execution stream aspects traditional unix processes address spaces descriptors 
objective separation expression control parallelism sufficiently cheap programmer compiler exploit fine grained parallelism acceptable overhead 
threads supported operating system kernel user level library code application address space approach fully satisfactory 
addresses dilemma 
argue performance kernel threads inherently worse user level threads artifact existing implementations argue managing par user level essential high performance parallel computing 
argue lack system integration exhibited user level threads consequence lack kernel support user level threads provided contemporary multiprocessor operating systems argue kernel threads processes currently conceived wrong abstraction support user level management parallelism 
describe design implementation performance new kernel interface user level thread package provide functionality kernel threads ing performance flexibility advantages user level management parallelism 
supported part national science foundation 
ccr ccr ccr washington technology center digital equipment systems research center external research program 
anderson supported ibm graduate fellowship bershad ph scholarship 
anderson computer science division univer sity california berkeley bershad school computer science carnegie mellon university 
permission copy fee part material granted provided copies distributed direct commercial advantage acm copyright notice title publication date appear notice copying permission association computing machinery 
copy republish requires fee specific sion 
acm effectiveness parallel computing depends great extent performance primitives express control parallelism programs 
coarse grained parallel program exhibit poor perfor mance cost creating managing parallelism high 
fine grained program achieve formance cost creating managing parallelism way construct parallel program share memory collection traditional unix processes consisting single address space single sequential execution stream address space 
unfortunately processes designed multiprogramming uniprocessor environment simply inefficient general purpose parallel programming handle coarse grained parallelism 
shortcomings traditional processes general purpose parallel programming led threads 
threads separate notion sequential execution stream aspects traditional processes address spaces descriptors 
separation concerns yields significant performance advantage relative traditional processes 
problem threads supported user eve kernel 
approach fully satisfactory 
user level threads managed runtime library routines linked application thread management operations require kernel intervention 
result excellent performance systems pcr weiser fastthreads anderson cost user level thread operations order magnitude cost procedure call 
user level threads flexible customized needs language user kernel modification 
user level threads execute context traditional processes user level thread systems typi cally built modifications underlying operating system kernel 
thread package views process virtual processor treats physical pro cessor executing control 
reality virtual processors multiplexed real physical processors underlying kernel 
real world factors multiprogramming page faults distort equivalence virtual physical processors presence factors user level threads built top traditional processes exhibit poor performance incorrect behavior 
multiprocessor operating systems mach tevanian topaz thacker cheriton provide direct kernel support multiple threads address space 
programming kernel threads avoids system integration problems threads programmer compiler directly scheduled kernel 
unfortunately performance thread management primitives kernel threads typically order magnitude better traditional unix processes typically order magnitude worse user level threads 
consequently kernel threads just traditional processes heavyweight parallel programs 
suggested fact userlevel threads ultimately implemented top kernel threads mach cooper topaz vandevoorde roberts 
user level threads built top kernel threads exactly built top traditional processes exactly performance suffer exactly system integration problems 
parallel programmer faced difficult dilemma employ kernel threads right perform poorly employ user level threads implemented top kernel threads processes perform functionally deficient 
goals address dilemma 
describe ker nel interface user level thread package provide functionality system mimic behavior kernel thread management system presence multiprogramming page faults processor presence ready threads 
high priority thread waits processor low priority thread runs 
thread traps kernel block example page fault processor thread running run thread different address space 
performance system cost common thread management order magnitude procedure call essentially achieved best existing user level thread management systems suffer poor system integration flexibility user level part system structured simplify application specific customization 
simple change policy scheduling plication threads provide different concurrency model workers moeller nielsen actors agha futures ha stead 
difficulty achieving goals multiprogrammed multiprocessor necessary control scheduling information distributed kernel application address space 
kernel order effectively allocate processors applications needs access user level scheduling information parallelism address space 
user level support software order effectively manage parallelism needs aware certain state transitions hidden kernel processor re allocations events 
approach approach provides application virtual multiprocessor abstraction dedicated physical machine 
application knows exactly processors allocated complete control threads running processors 
operating system kernel complete control allocation processors address spaces including ability change number processors assigned application execution 
achieve kernel notifies address space thread scheduler event affecting address space 
kernel role vector events influence user level scheduling address space thread scheduler handle interpret events traditional operating system 
thread system address space notifies kernel subset user level events affect processor allocation decisions 
communicating upward kernel events functionality im proved application complete knowledge scheduling state 
communicating downward events affect processor allocation performance preserved events simple thread scheduling decisions need reflected kernel 
kernel mechanism realize ideas called scheduler activations 
scheduler activation execution context event vectored kernel address space address space thread scheduler uses context handle event modify user level thread data structures execute user level threads requests kernel 
implemented prototype design dec src firefly multiprocessor workstation thacker 
differences scheduler activations kernel threads crucial similarities great kernel portion implementation required straightforward modifications kernel threads topaz native operating system firefly 
similarly user level portion implementation involved relatively straightforward modifications fastthreads user level thread system originally designed run top topaz kernel threads 
goal demonstrate exact functionality kernel threads provided user level presentation assumes user level threads concurrency model programmer com piler 
emphasize concurrency models implemented user level top kernel threads processes suffer problems user level threads problems solved implementing top scheduler activations 
user level threads perfor mance advantages functionality limitations section motivate describing advantages user level threads offer relative kernel threads difficulties arise user level threads built top interface provided kernel threads processes 
argue performance user level threads inherently better kernel threads artifact existing implementations 
user level threads additional advantage flexibility respect programming models environments 
argue lack system integration exhibited user level threads inherent user level threads consequence inadequate ker nel support 
case user level thread management natural believe superior performance user level threads result particular attention paid aspect design performance enhancements applied kernel yielding kernel threads achieve performance user level threads compromises functionality 
unfortunately turns case reasons cost accessing thread management operations kernel threads program cross extra protection boundary thread operation processor switched threads address space 
involves extra kernel trap kernel copy check parameters order protect buggy malicious programs 
contrast invoking user level thread operations quite inexpensive particularly compiler techniques expand code inline perform sophisticated register allocation 
safety compromised address space boundaries isolate misuse user level thread system program occurs 
cost generality kernel thread management single underlying implementation topaz ultrix operation fastthreads threads processes null fork signal wait table thread operation latencies 
applications 
general purpose kernel thread system provide feature needed reasonable application imposes overhead appli cations particular feature 
contrast facilities provided user level thread system matched specific needs applications different applications linked different user level thread libraries 
example kernel thread systems implement preemptive priority scheduling parallel applications simpler policy vandevoorde roberts 
factors important thread management operations inherently expensive 
kernel trap overhead priority scheduling instance major contributors high cost unix processes 
cost thread operations order magnitude procedure call 
implies overhead added kernel implementation small significant written user level thread system significantly better performance written kernel level thread system 
illustrate quantitatively table shows performance example implementations user level threads kernel threads unix processes running similar hardware processor 
fastthreads topaz kernel threads measured firefly ultrix dec derivative unix measured uniprocessor workstation 
implementations optimal 
measurements illustrative definitive 
benchmarks null fork time cre ate schedule execute complete process thread invokes null procedure words overhead forking thread signal wait time process thread signal waiting process thread wait condition words overhead synchronizing threads 
benchmark executed single processor results averaged multiple repetitions 
comparison procedure call takes 
firefly kernel trap takes 
table shows order magnitude difference cost ultrix process management topaz kernel thread management order magnitude difference topaz threads fast threads 
despite fact topaz thread code highly tuned critical path written assembler 
commonly tradeoff arises performance flexibility choosing implement system set vices wulf 
user level threads avoid tradeoff simultaneously improve performance flexibility 
flexibility particularly important thread systems parallel programming models may require specialized support thread system 
kernel threads supporting multiple parallel programming models may require modifying kernel increases complexity overhead likelihood errors kernel 
sources poor integration user level threads built tradi tional kernel interface unfortunately proven difficult implement user level threads level integration system services available kernel threads 
managing parallelism user level consequence lack kernel support existing systems 
kernel threads wrong abstraction support ing user level thread management 
related characteristics kernel threads cause difficulty kernel events processor blocking resumption handled kernel invisibly user level 
kernel threads scheduled respect user level thread state 
cause problems system 
user level thread system create kernel threads serve virtual processors physical processors system run user level threads 
user level thread block ing request takes page fault kernel thread serving virtual processor blocks 
result physical processor lost address space pending kernel thread serve execution context running user level threads just processor 
plausible solution create kernel threads physical processors kernel thread blocks user level thread blocks kernel kernel thread available run user level threads processor 
difficulty occurs completes page fault returns runnable kernel threads processors kernel thread user level thread loaded context 
deciding kernel threads assigned processors operating system implicitly choose user level threads assigned processors 
traditional system runnable threads processors operating system employ kind time slicing ensure thread progress 
user level threads running top kernel threads time slicing lead problems 
example kernel thread preempted userlevel thread holding spin lock user level threads accessing lock spin wait lock holder rescheduled 
zahorjan shown presence spin locks result poor formance 
example kernel thread running user level thread time sliced run ker nel thread user level thread available idling user level scheduler 
kernel thread running high priority user level thread order run kernel thread happens running low priority user level thread 
exactly problems occur multiprogramming page faults 
job system receive machine processors job enters system operating system preempt job processors give new job tucker gupta 
kernel forced choose job kernel threads user level threads running context run remaining processors 
need preempt processors address space occurs due variations parallelism jobs zahorjan mccann show dynamic re allocation processors address spaces response variations parallelism important achieving high performance 
kernel interface designed allow user level influence kernel threads scheduled kernel choice black choice intimately tied user level thread state communication information kernel user level negates performance flexibility advantages user level threads place 
ensuring logical correctness user level thread system built kernel threads difficult 
applications particularly require coordination multiple address spaces free deadlock assumption runnable threads eventually receive processor time 
kernel threads directly applications kernel satisfies assumption processors runnable threads 
user level threads multiplexed fixed num ber kernel threads assumption may longer hold kernel thread blocks user level thread blocks application run kernel threads serve execution contexts runnable userlevel threads available processors 
effective kernel user level parallelism support management section described problems arise kernel threads programmer express parallelism poor performance poor flexibility user level threads built top kernel threads poor integration system services 
address problems designed new kernel interface thread system combine functionality kernel threads performance flexibility user level threads 
operating system kernel provides user level thread system virtual multiprocessor abstraction dedicated physical machine number processors machine may change execution program 
aspects abstraction kernel allocates processors address spaces kernel complete control processors give address space virtual multiprocessor 
address space user level thread system complete control threads run allocated processors application running bare physical machine 
kernel notifies address space kernel changes number processors assigned kernel notifies address space user level thread blocks wakes kernel 
kernel role vector events address space thread scheduler handle interpret events 
address space notifies kernel wants needs fewer processors 
allows kernel correctly allocate processors address spaces needs 
user level notifies kernel subset user level events affect processor allocation decisions 
result performance compromised vast majority address space events thread scheduling decisions suffer overhead communication kernel 
application programmer sees difference performance programming directly kernel threads 
user level thread system manages virtual multiprocessor transparently programmer providing programmers normal topaz thread interface birrell 
user level runtime system easily adapted provide different parallel programming model 
remainder section describe kernel events vectored user level thread system information provided application allow kernel appropriately allocate processors jobs handle user level spin locks 
explicit kernel events user level thread scheduler communication kernel processor allocator user level thread system structured terms scheduler activations 
term scheduler activation selected vectored event causes user level thread system reconsider scheduling decision threads run processors 
scheduler activation serves roles serves vessel execution context running user level threads exactly way kernel thread 
notifies user level thread system kernel event 
provides space kernel saving processor context activation current user level thread thread stopped kernel thread blocks kernel kernel preempts processor 
scheduler activation data structures quite similar traditional kernel thread 
scheduler acti vation execution stacks mapped kernel mapped application address space 
kernel stack user level thread running scheduler activation context executes kernel example system call kernel maintains control block activation akin thread control block record state scheduler activation thread blocks kernel preempted 
user level thread scheduler runs activation user level stack main tains record user level thread running scheduler activation 
user level thread allocated stack starts running anderson way thread blocks user level lock condition variable thread scheduler resume running kernel intervention 
program started kernel creates scheduler activation assigns processor upcalls application address space fixed entry point 
user level thread management system receives upcall uses activation context initialize run main application thread 
thread executes may create user threads request additional processors 
case kernel create additional scheduler activation processor upcall user level tell new processor avail able 
user level selects executes thread context activation 
similarly kernel needs notify user level event kernel creates scheduler activation assigns processor upcalls application address space 
upcall started activation similar traditional kernel thread process event run user level threads trap block kernel 
crucial distinction scheduler activations kernel threads activation user level thread stopped kernel thread directly resumed kernel 
new scheduler activation created notify user level thread system thread stopped 
user level thread system removes state thread old activation tells kernel old activation re decides thread run processor 
contrast traditional system kernel stops kernel thread running user level thread context kernel notifies user level event 
kernel directly kernel thread tion user level thread notification 
scheduler activations kernel able maintain invariant exactly running add processor processor execute runnable user level thread 
processor preempted preempted activation machine state return ready list user level thread executing context preempted scheduler activation 
scheduler activation blocked blocked activation blocked scheduler activation longer processor 
scheduler activation unblocked unblocked activation machine state return ready list user level thread executing context blocked scheduler activation 
table scheduler activation upcall points scheduler activations vessels running user level threads processors assigned address space 
table lists events kernel vectors user level scheduler activations parameters upcall parentheses action taken userlevel thread italicized 
note events vectored exactly points kernel forced scheduling decision 
practice events occur combinations occurs single upcall passes events need handled 
example scheduler activations consider happens user level thread blocks kernel 
kernel uses fresh scheduler activation notify user level thread system event allowing processor run user level threads 
user level thread unblocked kernel directly resume uses fresh activation notify user level thread scheduler event 
tion course requires processor 
address space processors kernel allocate upcall upcall notifies thread system new processor blocked thread resumed 
alternatively kernel may preempt processor address space upcall case upcall notifies user level thread system original thread resumed second thread running processor preempted 
user level thread system put threads back ready list deciding run processor 
user level thread blocks kernel preempted state needed eventually resume user level thread stack control block 
thread register state saved low level kernel routines interrupt page fault handlers kernel passes state user level part upcall notifying address space completion 
exactly mechanism processor preempted due multiprogramming 
example suppose kernel decides take processor away address space give 
kernel sending processor interrupt stopping old activation processor upcall new address space fresh activation 
kernel need obtain permission advance old address space steal processor violate semantics address space priorities new address space higher priority old address space 
old address space notified preemption occurred 
kernel doing preemption different processor running old address space 
second processor upcall old address space fresh scheduler activation notifying address space user level threads stopped 
user level thread scheduler full control threads bc run re processors 
processor preempted address space simply skip notifying address space preemption delay notification kernel eventually re allocates processor 
notification allows user level know processors assigned case explicitly managing cache locality 
description simplified minor respects 
threads priorities additional preemption may take place ones described 
instance completion processor running thread lower priority unblocked preempted thread 
case user level thread system ask kernel interrupt thread running processor start scheduler activation thread stopped 
user level know knows exactly thread running processors 
second described kernel stopping sav ing context user level threads kernel interaction application entirely terms scheduler tions 
application free build concurrency model top scheduler activations kernel behavior exactly case 
particular kernel needs knowledge data structures represent parallelism user level 
loo add processors additional processors needed allocate processors address space start running scheduler activations 
processor idle preempt processor address space needs 
table communication address space kernel third scheduler activations properly preemption page fault occurs user level thread manager user level thread running 
case thread manager state saved kernel 
subsequent upcall new activation stack allows reentrant thread manager recover way user level running different way 
example preempted processor idle loop action necessary handling event upcall user level context switch continue processing event 
added complication kernel upcall notify program page fault may turn page fault location kernel check occurs delay subsequent upcall page fault completes 
user level thread blocked kernel may need execute kernel mode completes 
kernel resumes thread temporarily blocks reaches point leave kernel 
occurs kernel notifies user level passing user level thread register state part 
notifying kernel user level events affecting processor alloca tion mechanism described sub section independent policy kernel allocating processors address spaces 
reasonable allocation policies available parallelism address space 
sub section show information efficiently communicated policies respect priorities processors idle runnable threads exist 
constraints met kernel thread systems far know met user level thread system built top kernel threads 
key observation user level thread system need tell kernel thread operation small subset affect kernel processor allocation decision 
contrast kernel threads directly parallelism processor traps ker nel best thread run thread respects priorities minimizing overhead preserving cache context address space 
system address space notifies kernel transition state runnable threads processors processors runnable threads 
provided application extra threads run processor allocator re assigned ditional processors processors busy changing amount parallelism address space violate constraints 
similarly application notified kernel idle processors kernel taken away system kernel need notified changes parallelism point application processors 
extension approach handles situation threads address spaces globally meaningful priorities 
table lists kernel calls address space state transitions 
example address space notifies kernel needs processors kernel searches address space registered processors 
axe happens address space may get processor address space decides extra 
notifications hints kernel gives address space processor longer needed time gets address space simply returns processor kernel updated information 
course user level thread system serialize notifications kernel ordering matters 
drawback approach assumption applications honest reporting parallelism operating system 
problem unique multiprocessors dishonest misbehaving program consume unfair proportion resources multiprogrammed uniprocessor 
user level thread systems multi level feedback encourage applications provide honest information processor allocation decisions 
processor locator favor address spaces fewer processors penalize 
encourages address spaces give processors needed priorities imply processors returned needed 
hand system fewer threads processors idle processors left address spaces create near avoid overhead processor re allocation created 
production uniprocessor operating systems similar 
average response time especially interactive performance improved favoring jobs remaining service approximated reducing priority jobs accumulate service time 
expect similar policy multiprogrammed mul achieve goal easily adapted encourage honest reporting idle processors 
critical sections issue addressed user level thread executing critical section instant blocked possible ill effects poor performance threads continue test application level spin lock held preempted thread zahorjan deadlock preempted thread holding lock user level thread ready list deadlock occur upcall tempted place preempted thread ready list 
problems occur critical sections protected lock 
example fastthreads uses unlocked processor really activation free lists thread control blocks improve latency anderson accesses free lists done atomically respect 
prevention recovery approaches dealing problem preemption 
prevention avoided scheduling locking protocol ker nel user level 
prevention number serious drawbacks particularly multiprogrammed environment 
prevention requires kernel yield control processor allocation temporarily user level vio semantics address space priorities 
prevention inconsistent efficient implementation critical sections describe section 
presence page faults prevention requires pinning physical memory virtual pages touched critical section identifying pages cumbersome 
adopt solution recovery 
upcall informs user level thread system thread preempted unblocked thread system checks thread executing section 
course check acquiring locks 
thread continued temporarily user level context switch 
continued thread exits critical section relinquishes control back original upcall user level context switch 
point safe place user level thread back ready list 
mechanism continue activation middle processing kernel event 
technique free deadlock 
continuing lock holder ensure lock acquired ways released presence processor preemption page faults 
technique supports arbitrary user level spin locks user level thread notified preemption occurs allow ithe need critical sections avoided wait free synchronization herlihy 
commercial architectures provide required hardware sup port assume atomic test set instruction addi tion overhead wait free synchronization prohibitive protecting small data structures 
ing continue spin lock holder 
correctness affected processor time may wasted spin waiting spin lock holder takes page fault solution relinquish processor spinning lo gligor 
implementation implemented design described section modifying topaz native operating system dec src firefly multiprocessor workstation fastthreads user level thread package 
modified topaz kernel thread management routines implement scheduler activations 
topaz blocked resumed preempted thread performs upcalls allow user level take actions see table 
addition modified topaz explicit allocation processors address spaces topaz scheduled threads address spaces belonged 
maintained object code ity existing topaz unix applications run 
fastthreads modified process upcalls resume interrupted critical sections provide topaz information needed processor allocation decisions see table 
added lines code past threads lines topaz 
comparison original topaz implementation kernel threads lines code 
majority new topaz code concerned implementing processor allocation policy discussed scheduler activations se 
scheduler activations mechanism policy wulf 
design neutral choice policies allocating processors address spaces scheduling threads processors 
course pair policies selected prototype implementation briefly describe performance enhancements debugging considerations subsections follow 
processor allocation policy processor allocation policy chose similar dynamic policy zahorjan mccann 
pol icy space shares processors respecting priorities guaranteeing processor 
processors divided evenly address spaces address spaces need processors share processors divided evenly remainder 
space sharing reduces number processor re allocations processors time sliced number available processors integer multiple num ber address spaces priority want 
implementation possible address space kernel threads requiring address space scheduler activations 
continuing support topaz kernel threads necessary preserve binary com existing possibly sequential topaz applica tions 
implementation address spaces kernel threads compete processors way applica tions scheduler activations 
kernel processor allocator needs know address space processors processors idle 
application state instance asked processor received asked processor 
interface described section provides information address spaces scheduler activations internal kernel data structures provide address spaces kernel threads directly 
processors assigned address spaces scheduler activations handed user level thread scheduler upcalls processors assigned address spaces kernel threads handed original topaz thread scheduler 
result need static partitioning processors 
thread scheduling policy important aspect design kernel knowledge application concurrency model schedul ing policy data structures manage parallelism user level 
application completely free choose appropriate tuned fit application needs 
default policy fastthreads uses processor ready lists accessed processor order improve cache locality processor scans ready list empty 
essentially policy multilisp 
addition implementation includes hysteresis avoid unnecessary processor re allocations idle processor spins short period notifying kernel available re allocation 
performance enhancements design just described sufficient provide user level functionality equivalent kernel threads additional considerations important performance 
significant relates critical sections described section 
order provide temporary continuation critical sections user level thread pre empted blocks kernel resumed user level thread system able check thread holding lock 
way thread set flag enters critical section dear flag leaves check see continued 
check needed thread temporarily continued relinquish processor original upcall reaches safe place 
unfortunately imposes overhead lock acquisition release preemption page fault occurs events infrequent 
latency particularly important critical sections building user level thread system 
adopt different solution imposes overhead common case related technique cessor trellis owl garbage collector moss kohler 
exact copy low level critical section 
delimiting special assembler labels critical section source code user level thread package post process compiler generated assembly code copy 
straightforward language compiler support 
copy original version critical section place code yield processor back 
normal execution uses original code 
preemption occurs kernel starts new scheduler activation notify user level thread system activation checks preempted thread program counter see critical sections continues thread corresponding place copy critical section 
copy relinquishes control back original upcall critical section 
normal execution uses original code code exactly concerned impact lock latency common case 
implementation occasionally procedure call critical section 
case bracket call straight line path setting clearing explicit flag 
second significant performance enhancement relates management scheduler activations 
logically new scheduler activation created upcall 
creating new scheduler activation free requires data structures allocated initialized 
discarded scheduler activations cached eventual re 
user level thread system recycle old scheduler activation returning kernel soon user level thread running removed context case preemption processing upcall notifies user level preemption case blocking kernel processing upcall notifies user level resumption possible 
similar optimization kernel thread implementations kernel threads created cached destroyed speed thread creations lampson redell 
discarded scheduler activations collected returned kernel bulk returning time 
ignoring occasional bulk deposit discards system number boundary crossings processor preemption traditional kernel thread system 
kernel thread blocks request example cross kernel system crosses back run ready thread 
completes system re enter kernel process interrupt cross back resume thread 
kernel boundary crossings occur system 
course kernel passes control back user level resume execution particular thread starts running context new scheduler activation scheduling choices user level 
fastthreads fastthreads operation topaz threads scheduler activations topaz threads ultrix processes null fork signal wait table thread operation latencies 
debugging considerations integrated scheduler activations firefly topaz debugger 
separate environments needs debugging user level thread system debugging application code running top thread system 
transparency crucial effective debugging de little effect possible sequence instructions debugged 
kernel support described informs user level thread system state physical processors inappropriate thread system debugged 
kernel assigns scheduler activation debugged logical processor debugger stops single steps scheduler activation events cause upcalls user level thread system 
assuming user level thread system working correctly debugger facilities thread sys tem examine state application code run ning context thread 
performance goal research combine functionality kernel threads performance flexibility advantages managing parallelism user application address space 
functionality flexibility issues addressed previous sections 
terms performance consider questions 
cost user level thread operations fork block system 
second cost communication kernel user level specifically upcalls 
third effect perfor mance applications 
thread performance cost user level thread operations system essentially fastthreads package running firefly prior running top topaz kernel threads associated poor system integration 
table adds performance system data original fastthreads topaz kernel threads ultrix processes contained table 
sys tem preserves order magnitude advantage userlevel threads offer kernel threads 
sec 
degradation null fork relative original fastthreads due incrementing number busy threads determining kernel bc notified 
eliminated program running machine running sufficient parallelism inform kernel wants processors available 

dation signal wait due factor plus cost checking preempted thread resumed case extra done restore condition codes 
order magnitude better kernel threads performance significantly worse zero overhead way marking lock held see section 
removing optimization fastthreads yielded null fork time sec 
signal walt time 
null fork benchmark critical sections execution path signal wait 
upcall performance thread performance section characterizes frequent case kernel involvement necessary 
upcall performance infrequent case important reasons 
helps determine break point ratio thread operations done user level require kernel intervention needed user level threads outperform kernel threads 
cost blocking preempting user level thread kernel scheduler activations similar cost blocking preempting thread scheduler actl practical uniprocessor 
latency thread preempted upcall determines long threads running application may wait critical resource held preempted thread 
began implementation expected call performance commensurate overhead topaz kernel thread operations 
implementation considerably slower 
measure performance time user level threads signal wait kernel analogous signal wait test table synchronization forced kernel 
approximates overhead scheduler machinery making ing request page fault 
signal wait time milliseconds factor worse topaz threads 
see inherent scheduler activations responsible difference attribute implementation issues 
built scheduler ac quick modification existing implementation topaz kernel thread system ma state overhead designed portion kernel scratch 
importantly topaz thread system written care fully tuned assembler kernel implementation entirely modula 
comparison schroeder burrows reduced src ppc processing costs factor recoding modula assembler 
expect tuned upcall performance commensurate topaz kernel thread performance 
result application performance measurements section somewhat worse achieved production scheduler activations implementation 
application performance illustrate effect system application performance measured parallel application topaz kernel threads original fastthreads built top topaz threads modified fastthreads running scheduler activations 
application measured nlog solution body problem barnes hut 
algorithm constructs tree representing center mass portion space traverses portions tree compute force body 
force exerted cluster distant masses approximated force exert center mass cluster 
depending relative ratio processor speed available memory application compute bound 
modified application explicitly manage part memory buffer cache application data 
allowed control amount memory application small problem size chosen buffer cache fit firefly physical memory 
simplification threads cache simply block kernel msec cache misses normally cause disk access 
measurements qualitatively similar took contention disk account firefly floating point performance physical memory size orders magnitude current generation systems measurements intended illustrative 
tests run processor firefly 
demonstrate application minimal kernel services runs quickly system original fastthreads faster topaz threads 
graphs application speedup versus number processors systems system memory negligible applications running 
speedup relative sequential implementation algorithm 
processor systems perform worse sequential implementation added overhead creating synchronizing threads parallelize application 
overhead greater topaz kernel threads user level thread system 
processors added top kernel threads initially improves flattens 
topaz thread acquire release application lock critical section trapping kernel provided contention lock 
thread tries acquire topaz threads orig number processors speedup body application vs number processors memory available busy lock thread block kernel re scheduled lock released 
topaz lock overhead greater presence contention 
speedup attained user level thread systems shows application parallelism overhead kernel threads prevents performance 
able improve performance application kernel threads re structuring critical sections bottleneck spinning short time user level lock busy trapping kernel karlin optimizations crucial application built user level threads 
performance original fastthreads system diverges slightly processors 
applications running tests topaz operating system daemon threads wake periodically execute short time go back sleep 
system explicitly allocates processors address spaces daemon threads cause idle processors available true native topaz scheduler controls kernel threads virtual processors original fastthreads 
application tries proces sors machine case processors number user level thread systems similar 
small impact perfor mance original fastthreads short tion 
show application requires kernel involvement system performs better original fastthreads topaz threads 
graphs application execution time processors function amount available memory 
systems performance degrades slowly sharply application working set fit memory 
application performance loo topaz threads orig available memory execution time body application vs amount available memory processors topaz original new threads fastthreads fastthreads table speedup body application multiprogramming level processors memory available original fastthreads degrades quickly systems 
user level thread blocks kernel kernel thread serving virtual processor blocks application loses physical processor duration curves modified fastthreads topaz threads parallel systems able exploit parallelism application overlap latency useful computation 
application performance better modified fastthreads topaz thread operations bc implemented kernel involvement 
shows effect performance application induced kernel events multiprogramming causes system induced kernel events result system having better performance original fastthreads topaz threads 
test ran copies application time processor firefly averaged execution times 
table lists note speedup maximum possible 
table shows application performance modified fastthreads multiprogrammed environment speedup obtained application ran processors 
small degradation expect bus contention need donate processor periodically run kernel daemon thread 
contrast multiprogrammed performance worse original fastthreads topaz threads different reasons 
plications original fastthreads multiprogrammed operating time kernel threads serving virtual processors result physical processors idling waiting lock released lock holder de scheduled 
performance worse topaz threads system common thread operations expensive 
addition topaz explicit processor allocation may scheduling kernel threads address space shows performance topaz threads processors assigned application 
firefly excellent vehicle constructing proof concept prototypes limited number proces ideal experimenting significantly parallel applications multiple multiprogrammed parallel applications 
reason implementing scheduler activations mach porting amber chase el 
programming system network multiprocessors firefly implementation 
related ideas systems goals closely related achieving properly integrated user level threads improved kernel support psyche scott el 

support numa multiprocessors primary goal high performance parallel unix implementation psyche context new operating system 
psyche provide virtual processors described sections augment virtual processors defining software interrupts notify user level kernel events 
software interrupts interrupts processor stack re entrant 
psyche explored notion multi model parallel programming user defined threads various kinds different address spaces synchronize sharing code data 
psyche share similar goals approaches taken achieve goals differ important ways 
psyche provides exact kernel threads respect page faults multiprogramming performance user level thread operations compromised 
discussed reasons section systems notify user level kernel events affect address space 
example psyche notify user level preempted virtual processor re scheduled 
result user level thread system know processors user threads running processors 
psyche provide shared writable memory kernel application system provides efficient mechanism user level thread system notify kernel processor allo cation needs re considered 
number processors needed application written shared memory give efficient way needs processors know application idle processors 
applications psyche share synchronization state kernel order avoid preemption moments spin locks held 
application sets variable shared kernel indicate critical section psyche application checks imminent preemption starting critical section 
setting checking bits adds lock latency constitutes large portion overhead doing high performance user level thread management anderson 
contrast system effect lock latency preemption occurs 
furthermore systems kernel notifies application intention preempt processor preemption occurs notification application choose place thread safe state voluntarily relinquish processor 
mechanism violates constraint higher priority threads run place lower priority threads 
systems provide asynchronous kernel mech anism solve problems user level thread management multiprocessors weiser 
flavor asynchronous system request processor returned application completes applica tion notified 
major differences traditional asynchronous systems 
important scheduler activations provide single uniform mechanism address problems processor preemption page faults 
relative asynchronous approach derives conceptual simplicity fact interaction kernel synchronous perspective single scheduler ac 
scheduler activation blocks kernel replaced new scheduler activation event occurs 
second asynchronous schemes may require significant changes application kernel code scheme leaves structure user level thread system kernel largely unchanged 
parts scheme related ways hydra wulf earliest multiprocessor operating systems scheduling policy moved kernel 
hydra separation came performance cost policy decisions required communication kernel scheduling policy server back kernel implement context switch 
system application set policy scheduling threads processors implement policy trapping kernel 
longer term processor allocation decisions system kernel hydra delegated distinguished application level server 
summary managing parallelism user level essential highperformance parallel computing kernel threads processes provided operating systems poor abstraction support 
described design implementation performance kernel interface user level thread package combine performance user threads common case thread operations implemented entirely user level functionality kernel threads correct behavior infrequent case kernel involved 
approach providing appli cation address space virtual multiprocessor application knows exactly processors exactly threads running processors 
responsibilities divided kernel application address space processor allocation allocation processors address spaces done kernel 
thread scheduling assignment address space threads processors done ad dress space 
kernel notifies address space thread scheduler event affecting address space 
address space notifies kernel subset user level events affect processor allocation de 
kernel mechanism implement ideas called scheduler activations 
scheduler activation execution context control kernel address space kernel event 
address space thread scheduler uses context handle event modify user level thread data structures execute userlevel threads requests kernel 
prototype implements threads concurrency abstraction supported user level scheduler activations axe linked particular model scheduler activations support user level concurrency model kernel knowledge user level data structures 
andrew black mike burrows jan mike jones butler lampson tom leblanc kai li brian marsh mullender dave redell michael scott garret john zahorjan comments 
dec systems research center providing firefly hardware software 
agha agha actors model concurrent com putation distributed systems 
mit press 
anderson anderson lazowska levy 
performance implications thread management alternatives shared memory multiprocessors 
ieee transactions december 
appeared proceedings acm sigmetrics performance conference measurement modeling computer systems may 
barnes hut barnes hut hierarchical log force calculation algorithm 
na ture 
guttag homing levin synchronization primitives multiprocessor formal specification 
proceedings th symposium operating systems principles pages november 
black black scheduling support concurrency parallelism mach operating system 
ieee computer magazine may 
chase chase levy amber system parallel programming network multiprocessors 
proceedings lth cm symposium operating systems principles pages december 
cheriton distributed system 
communications acm march 
cooper cooper threads 
technical report cmu cs school computer science carnegie mellon university june 
schonberg process management highly parallel unix systems 
proceedings workshop unix supercomputers pages 
halstead halstead multilisp language con current symbolic computation 
cm transactions programming languages systems october 
herlihy herlihy methodology implementing highly concurrent data structures 
proceedings nd acm sigplan symposium principles practice parallel programming pages march 
karlin karlin li manasse ow empirical studies competitive spinning shared memory multiprocessor 
proceedings th um symposium operating systems principles october 
lampson lampson redell processes monitors mesa 
communications february 
lo gligor lo 
gligor comparative analysis multiprocessor scheduling algorithms 
proceedings th international conference distributed computing systems pages september 
marsh marsh scott leblanc markatos class user level threads 
proceedings th cm symposium operating systems principles october 
moeller moeller nielsen problem heap paradigm multiprocessor algorithms 
parallel computing february 
moss kohler moss kohler concurrency features trellis owl language 
proceedings european conference object oriented programming ecoop pages june 
redell redell topaz tele debugger 
pro ceedings um sigplan workshop parallel distributed debugging may 
schroeder burrows schroeder burrows performance firefly rpc 
acm transactions computer systems february 
scott scott leblanc marsh multi model parallel programming psyche 
proceedings nd cm symposium principles practice parallel programming pages march 
tevanian tevanian rashid golub black cooper young mach threads unix kernel battle control 
proceedings usenix summer conference pages 
thacker thacker stewart waite jr 
firefly multiprocessor workstation 
ieee transactions computers august 
tucker gupta tucker gupta process control scheduling issues shared memory multiprocessors 
proceedings th cm symposium erating systems principles pages de 
vandevoorde roberts vandevoorde roberts abstraction controlling parallelism 
international journal parallel programing august 
weiser weiser demers hauser portable common runtime approach interoperability 
proceedings lth acm symposium operating systems principles pages december 
wulf levin dra mmp experimental computer sys tem 
mcgraw 
zahorjan mccann zahorjan mccann processor scheduling shared memory multiprocessors 
proceedings agm conference measurement modeling computer systems pages may 
zahorjan eager effect scheduling discipline spin overhead shared memory multiprocessors 
ieee transactions parallel distributed systems april 

