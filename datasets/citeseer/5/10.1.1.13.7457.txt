transforming classifier scores accurate multiclass probability estimates class membership probability estimates important applications data mining classification outputs combined sources information decision making example dependent misclassification costs outputs classifiers domain knowledge 
previous calibration methods apply class problems 
show obtain accurate probability estimates multiclass problems combining calibrated binary probability estimates 
propose new method obtaining calibrated class probability estimates applied classifier produces ranking examples 
naive bayes support vector machine classifiers give experimental results variety class multiclass domains including direct marketing text categorization digit recognition 

supervised learning methods produce classifiers output scores rank examples test set probable member probable member class examples applications ranking examples class membership probability 
needed accurate estimate probability test example member class interest 
probability estimates important classification outputs isolation combined sources information decision making example dependent misclassification costs outputs classifier 
example handwritten character recognition outputs classifier input high level system incorporates domain information language model 
current methods transforming ranking scores accurate probability estimates apply class problems 
propose method obtaining accurate multiclass probability estimates ranking scores decompose multiclass problem series binary problems learn classifier permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
sigkdd edmonton alberta canada copyright acm 
zadrozny charles elkan department computer science engineering university california san diego la jolla california zadrozny elkan cs ucsd edu calibrate scores classifier combine obtain multiclass probabilities 
new method obtaining accurate class probability estimates applied classifier produces ranking examples 
fast simple understand implement method produces probability estimates comparable better ones produced methods 
section review notion calibration probability estimates show scores produced naive bayes support vector machine svm classifiers tend rank examples calibrated 
section review previous methods mapping class scores probability estimates explain shortcomings new method 
section discuss combine calibrated class probability estimates calibrated multiclass probability estimates 
section experimental evaluation methods applied naive bayes svm scores variety domains 
section summarize contributions suggest directions 

calibration naive bayes svm scores assume classifier example outputs score 
classifier said empirical class membership probability converges score value number examples classified goes infinity 
intuitively consider examples classifier assigns score examples members class question 
calibration important want scores directly interpretable chances membership class 
naive bayesian classifiers assign test example score interpreted principle class membership probability estimate 
known scores calibrated 
naive bayes assumption attributes examples independent class examples 
attributes tend correlated real data scores produced naive bayes typically extreme near near naive bayesian classifiers tend rank examples calibration classifier visualized reliability diagram 
case small number possible score values score value compute empirical probability number examples score belong class divided total number examples empirical class membership probability adult dataset nb score empirical class membership probability insurance dataset nb score reliability diagrams nb 
numbers indicate examples fall bin test set 
empirical class membership probability adult dataset svm score re scaled empirical class membership probability svm score re scaled insurance dataset reliability diagrams svm 
numbers indicate examples fall bin test set 
score plot versus classifier calibrated points fall line indicating scores equal empirical probability 
practical situations number possible scores large compared number available test examples calculate reliable empirical probabilities possible score value 
case resort discretizing score space 
scores uniformly distributed carefully choose bin sizes examples calculate reliable empirical probability estimates bin 
show reliability diagrams known datasets adult tic see section information datasets score space discretized bins size respectively 
see graphs tending vary monotonically empirical probability naive bayes scores calibrated points fall line 
test example svm classifier outputs score distance hyperplane learned separating positive examples negative examples 
sign score indicates example classified positive negative 
magnitude score taken measure confidence prediction examples far separating hyperplane presumably classified correctly 
range svm scores depends problem map scores interval re scaling 
original score re scaled score 
scores tend calibrated distance separating hyperplane exactly proportional chances membership class 
show reliability diagrams re scaled svm scores adult tic datasets score spaces dis bins size respectively 
see svm scores vary monotonically empirical probability calibrated 

mapping scores probability estimates suppose set examples know true labels 
case assume positive examples negative examples 
apply classifier examples obtain scores learn function mapping scores probability estimates learning method overfit training data data learn function 
need break training data sets learning classifier learning mapping function 
case need regularization criterion avoid learning mapping function generalize new data 
possible regularization criteria impose particular parametric shape function available data learn parameters function fits data measure 
parametric approach proposed platt svm scores consists finding parameters sigmoid function form mapping scores probability estimates negative log likelihood data minimized 
method motivated fact relationship svm scores empirical probabilities appears sigmoidal datasets 
case adult dataset seen show learned sigmoid training data empirical probabilities test data adult tic 
platt shown empirically method yields probability estimates accurate ones obtained training svm specifically producing accu empirical class membership probability test data learned sigmoid adult dataset svm score empirical class membership probability test data learned sigmoid insurance dataset svm score mapping svm scores probability estimates sigmoid function 
empirical class membership probability test data learned sigmoid adult dataset nb score empirical class membership probability test data learned sigmoid insurance dataset nb score mapping nb scores probability estimates sigmoid function 
rate class membership probability estimates faster 
method applied naive bayes 
proposed bennett reuters dataset 
show sigmoidal fit naive bayes scores adult tic datasets 
sigmoidal shape appear fit naive bayes scores fits svm scores datasets 
shape mapping function unknown resort non parametric method binning 
binning training examples sorted scores sorted set divided subsets equal size called bins 
bin compute lower upper boundary scores 
test example place bin score estimate corrected probability belongs class fraction training examples bin belong difficulty binning method choose number bins cross validation 
dataset small highly unbalanced cross validation indicate optimal number bins 
size bins fixed position boundaries chosen arbitrarily 
boundaries average labels examples clearly different probability estimates binning method fail produce accurate probability estimates 
propose intermediary approach sigmoid fitting binning isotonic regression 
isotonic regression non parametric form regression assume function chosen class isotonic non decreasing functions 
assume classifier ranks examples correctly mapping scores probabilities non decreasing isotonic regression learn mapping 
commonly algorithm computing isotonic regression pair adjacent pav 
algorithm finds stepwise constant isotonic function best fits data mean squared error criterion 
pav works follows 
xi training examples xi value function learned training example xi isotonic regression 
isotonic return subscript xi xi xi examples xi called empirical class membership probability test data isotonic regression adult dataset nb score empirical class membership probability test data isotonic regression adult dataset svm score pav algorithm map naive bayes svm scores probability estimates 
pair adjacent violate isotonic assumption 
values xi xi replaced average examples xi xi comply isotonic assumption 
new set values isotonic xi xi xi xi 
process repeated new values isotonic set values obtained 
computational complexity algorithm implementation pav matlab available lutz 
apply algorithm problem mapping scores probability estimates sort examples scores xi xi negative xi positive 
scores rank examples perfectly negative xi come positive xi values changed 
new probability estimate negative examples positive examples 
hand scores give information ordering examples constant function value average values xi base rate positive examples 
general case pav average examples parts score space classifier ranks examples incorrectly examples parts space classifier ranks correctly 
view pav binning algorithm position boundaries size bins chosen classifier ranks examples 
pav returns set intervals estimate interval obtain estimate test example find interval lowest highest scores interval assign probability estimate show result applying pav adult dataset naive bayes svm 
line shows function learned training data stars show empirical probabilities test data 

estimates notion calibration introduced section readily applied multiclass probability estimates 
suppose multiclass classifier output scores ci class ci example classifier calibrated class ci empirical probability ci ci converges score value ci number examples classified goes infinity 
calibration methods discussed section designed exclusively class problems 
mapping scores probability estimates works class case mapping dimensional spaces 
setting easy impose sensible restrictions shape function learned done sigmoidal shape monotonicity requirements 
general multiclass case mapping dimensional space dimensional space 
case clear function shape imposed mapping function 
furthermore curse dimensionality non parametric methods yield accurate probabilities number classes grows 
reasons attempt directly calibrate multiclass probability estimates 
reduce multiclass problem number binary classification problems 
learn classifier binary problems calibrate scores classifier 
combine binary probability estimates obtain multiclass probabilities 
known approaches reducing multiclass problem set binary problems known allpairs 
train classifier class positives examples belong class negatives examples 
pairs train classifier possible pair classes ignoring examples belong classes question 
allwein represent possible decomposition multiclass problem binary problems code matrix number classes number binary problems :10.1.1.10.5265
examples belonging class considered positive examples binary classification problem similarly examples belonging considered negative examples examples belonging training classifier example class case pairs code matrix code matrices generalization error correcting output coding ecoc scheme 
difference ecoc allow zeros code matrix meaning examples binary classification problem 
arbitrary code matrix estimate rb column rb set classes respectively 
obtain set probabilities example compatible rb subject ci 
free parameters constraints generally consider matrices constrained problem exact solution 
approaches proposed finding approximate solution problem 
squares method non negativity constraints proposed kong dietterich 
proposed method original ecoc matrices easily applied arbitrary matrices 
test binary probability estimates decision trees classifiers learned known calibrated 
synthetic data show method produces better estimates multiclass 
alternative method called coupling iterative algorithm finds best approximate solution minimizing log loss squared error 
method proposed extension pairwise coupling method applies pairs mse profit method training test training test nb sigmoid nb pav nb table mse profit kdd dataset matrices 
algorithm tested boosted naive bayes binary learner scores tend calibrated naive bayes scores extreme 
open question existing methods combining binary probability estimates yields accurate multiclass probability estimates 
desirable property method better calibrated binary estimates better calibrated multiclass estimates 
section compare methods experimentally multiclass datasets 

experimental evaluation results application methods discussed previous sections variety datasets 
methods learning classifiers overfit training data datasets experiments data learning classifier calibration functions 
primary metric assessing accuracy probability estimates mean squared error mse known brier score 
example squared error se defined probability estimated example class defined actual label 
calculate se example training test sets obtain mse set 
degroot fienberg show mse separated components measuring calibration measuring refinement 
classifier calibrated component zero 
classifiers calibrated probability estimates closer said refined predictions confident 
classifiers calibrated lowest mse refined preferable 
mse applied general sensible evaluate quality probability estimates practical situations domain specific metric 
example direct mailing evaluate probability estimates profit obtained mail people policy uses estimates 
mse tends correlated profit domain specific information calculate profit mse evaluate methods 
class problems dataset kdd dataset available uci kdd repository 
dataset contains information persons donations certain charity 
decision making task choose donors request new donation 
data divided standard way training test set 
training set consists records known person donation person donated donation 
test set consists records donation campaign 
choice attributes fixed informally kdd winning submission 
optimal mailing policy solicit people expected return donation greater cost mailing solicitation estimated donation amount 
concerned donation amount estimation method training test nb sigmoid nb pav naive bayes svm sigmoid svm pav svm table mse tic dataset fixed values obtained linear regression 
naive bayes estimate donation apply calibration methods discussed section 
table shows mse profits raw naive bayes scores calibrated scores obtained sigmoid fitting pav 
expected mse profit significantly improved calibration 
pav overfits slightly training data performs better sigmoid fitting 
compared pav binning bin sizes varying 
set parameters pav method performed comparably best parameter setting binning 
dataset insurance benchmark tic known coil dataset available uci kdd repository 
decision making task analogous kdd task deciding customers offer insurance policy 
dataset divided standard way training set examples test set examples 
attributes winning entry coil challenge 
training set learn model probability customer acquired insurance policy 
cost mailing offer benefit selling policy depends customer probability customer buy policy choose customers mail offer 
cost benefit information available dataset decisions report profits 
just report mse different methods 
applied naive bayes linear kernel svm dataset package 
show mse results raw naive bayes svm scores calibration method table 
order obtain mse svm scores re scale explained section 
correction methods able greatly improve mse naive bayes pav performs slightly better sigmoid fitting 
mse svm scores reduced calibration methods 
case sigmoid fitting best 
compared pav binning bin sizes varying pav slightly worse binning optimal number bins 
applied method adult dataset available uci ml repository 
prediction task determine person year demographic information person 
dataset divided standard way training set examples test set examples 
apply naive bayes svm dataset feature selection 
learning svm classifier package done platt linear kernel svm discretized features 
table shows mse error rates dataset 
error rate calculated classifying positive belonging indicates income greater 
note calibrating naive bayes scores reduce error rate 
happens threshold raw scores calibrated scores 
svm mse error rate method training test training test nb sigmoid nb pav nb svm sigmoid svm pav svm table mse error rate adult dataset 
error rate slightly increased apply correction methods 
indicates svm scores uncalibrated threshold classification optimal 
calibration methods error rate increased refinement classifier slightly reduced 
surprisingly shape function mapping svm scores empirical probability estimates distinctive sigmoidal shape pav method performs slightly better sigmoid fitting method 
compared pav binning bin sizes varying naive bayes svm binning worse pav 
indicates dataset fixed number examples bin accurately model mapping svm naive bayes scores calibrated probability estimates 
multiclass problems multiclass dataset consider pendigits available uci ml repository 
consists training examples test examples pen written digits classes 
digits represented vectors attributes integers ranging 
experiments code matrix 
naive bayes boosted naive bayes binary learners apply pav calibrate scores 
mentioned section methods combining binary probability estimates multiclass probability estimates arbitrary code matrices squares coupling 
possible method normalization 
case binary classifier outputs estimate ci simply normalize estimates sum 
table shows mse error rate apply methods naive bayes pav naive bayes boosted naive bayes pav boosted naive bayes 
calibrate probability estimates combining methods mse error rate lower raw scores 
clear methods combining binary estimates preferred 
calibrated estimates difference method 
reason recommend simple normalization simplest method 
second multiclass dataset newsgroups collected originally lang 
contains text documents evenly distributed classes 
standard training test split dataset randomly select documents class training testing 
conduct experiments training test splits report mean standard deviation 
previous research performed code matrices dataset terms error rate restrict experiments 
calibrate naive bayes scores pav apply methods obtaining multiclass probability estimates raw naive method mse error rate nb normalization nb squares nb coupling pav nb normalization pav nb squares pav nb coupling bnb normalization bnb squares bnb coupling pav bnb normalization pav bnb squares pav bnb coupling table mse error rate pendigits test set bayes scores pav scores 
table shows mse error rates method 
see applying pav binary naive bayes scores significantly reduce mse slightly improve error rate 
lowest mse achieved calibrate scores pav normalization obtain multiclass probability estimates 

simple general methods obtaining accurate class membership probability estimates class multiclass problems binary classifiers output ranking scores 
demonstrated experimentally methods variety data mining domains different classifier learning methods 
class problems recommend pav algorithm learn mapping ranking scores calibrated probability estimates 
multiclass problems separate problem number binary problems calibrate scores binary classifier pav combine obtain multiclass probabilities 
show experimentally calibrating binary scores improve substantially calibration multiclass probabilities obtained simplest way breaking multiclass problem binary problems 
domains sophisticated code matrices yield better results terms error rate 
conducted experiments method applicable arbitrary code matrices 
experiments necessary determine best method combining binary probability estimates general case 
open question research design optimal code matrix obtaining accurate class membership probability estimates 

allwein schapire singer 
reducing multiclass binary unifying approach margin classifiers 
journal machine learning research 
ayer brunk ewing reid silverman 
empirical distribution function sampling incomplete information 
annals mathematical statistics 
bay 
uci kdd archive 
department information computer sciences university california irvine 
kdd ics uci edu 
bennett 
assessing calibration naive bayes posterior estimates 
technical report cmu cs carnegie mellon university 
blake merz 
uci repository machine learning databases 
department information computer sciences university california irvine 
www ics uci edu mlearn mlrepository html 
method mse error rate nb norm nb ls nb coup pav nb norm pav nb ls pav nb coup table mse error rate newsgroups 
brier 
verification forecasts expressed terms probability 
monthly weather review 
degroot fienberg 
comparison evaluation forecasters 
statistician 
dietterich bakiri 
solving multiclass learning problems error correcting output codes 
journal artificial intelligence research 
domingos pazzani 
independence conditions optimality simple bayesian classifier 
proceedings thirteenth international conference machine learning pages 
morgan kaufmann publishers 

statistical software matlab 
available www math mu de workers software software html 
elkan 
boosting naive bayesian learning 
technical report cs university california san diego 
elkan 
magical thinking data mining lessons coil challenge 
proceedings seventh international conference knowledge discovery data mining pages 
acm press 
georges 
kdd competition knowledge discovery contest report 
available www cse ucsd edu users elkan html 
hastie tibshirani 
classification pairwise coupling 
advances neural information processing systems volume 
mit press 
kong dietterich 
probability estimation error correcting output coding 
int 
conf artificial intelligence soft computing 
lang 
newsweeder learning filter netnews 
proceedings twelfth international conference machine learning pages 
murphy winkler 
reliability subjective probability forecasts precipitation temperature 
applied statistics 
platt 
probabilistic outputs support vector machines comparison regularized likelihood methods 
advances large margin classifiers 
mit press 
provost domingos 
trained pets improving probability estimation trees 
working stern school business new york university ny ny 
rennie rifkin 
improving multiclass text classification support vector machine 
technical report aim mit 
rifkin 

available percent nation mit edu 
robertson wright 
order restricted statistical inference chapter 
john wiley sons 
zadrozny 
reducing multiclass binary coupling probability estimates 
advances neural information processing systems nips 
appear 
zadrozny elkan 
learning making decisions costs probabilities unknown 
proceedings seventh international conference knowledge discovery data mining pages 
acm press 
zadrozny elkan 
obtaining calibrated probability estimates decision trees naive bayesian classifiers 
proceedings eighteenth international conference machine learning pages 
morgan kaufmann publishers 
