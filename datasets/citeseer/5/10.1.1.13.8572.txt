tackling poor assumptions naive bayes text classi ers jason rennie mit edu lawrence shih kai mit edu jaime mit edu david karger karger mit edu arti cial intelligence laboratory massachusetts institute technology cambridge ma naive bayes baseline text classi cation fast easy implement 
severe assumptions eciency possible adversely affect quality results 
propose simple heuristic solutions problems naive bayes classi ers addressing systemic issues problems arise text generated multinomial model 
nd simple corrections result fast algorithm competitive stateof art text classi cation algorithms support vector machine 

naive bayes punching bag classi ers lewis earned dubious distinction placing near numerous head head classi cation papers yang liu joachims zhang oles :10.1.1.11.6124
frequently text classi cation fast easy implement 
erroneous algorithms tend slower complex 
investigate reasons naive bayes poor performance 
problem propose simple heuristic solution 
example look naive bayes linear classi er nd ways improve learned decision boundary weights 
better match distribution text distribution assumed naive bayes 
doing classi er problems making slower signi cantly dicult implement 
rst review multinomial naive bayes model classi cation discuss systemic problems 
systemic problem class training examples naive bayes selects poor weights decision boundary 
due studied bias ect shrinks weights classes training examples 
balance amount training examples estimate introduce complement class formulation naive bayes 
systemic problem naive bayes features assumed independent 
result words dependent word contributes evidence individually 
magnitude weights classes strong word dependencies larger classes weak word dependencies 
keep classes dependencies dominating normalize classi cation weights 
addition systemic problems multinomial naive bayes model text 
simple transform enables naive bayes emulate power law distribution matches real term frequency distributions closely 
discuss pre processing steps common information retrieval naive bayes classi cation incorporate real world knowledge text documents 
signi cantly boost classi cation accuracy 
naive bayes modi cations summarized table produces classi er longer generative interpretation 
common model techniques uncover latent classes incorporate unlabeled data em applicable 
nd improved classi cation accuracy worthwhile 
new classi er approaches state theart accuracy support vector machine svm text corpora faster easier implement svm modern day classi ers 

multinomial naive bayes naive bayes classi er studied 
early description duda hart 
reasons classi er common fast easy implement relatively ective 
domingos pazzani discuss feature independence assumption explain naive bayes performs classi cation gross proceedings twentieth international conference machine learning icml washington dc 
simpli cation 
mccallum nigam posit multinomial naive bayes model text classi cation show improved performance compared multi variate bernoulli model due incorporation frequency information 
multinomial version call multinomial naive bayes mnb discuss analyze improve 

modeling classi cation multinomial naive bayes models distribution words document multinomial 
document treated sequence words assumed word position generated independently 
classi cation assume xed number classes mg xed set multinomial parameters 
parameter vector class cn size vocabulary ci ci probability word occurs class 
likelihood document product parameters words appear document dj 
ci frequency count word document assigning prior distribution set classes arrive minimum error classi cation rule duda hart selects class largest posterior probability argmax log log ci argmax ci threshold term ci class weight word values natural parameters decision boundary 
especially easy see binary classi cation boundary de ned setting di erences positive negative class parameters equal zero form equation identical decision boundary learned linear support vector machine logistic regression linear squares perceptron 
naive bayes relatively poor performance results chooses ci 
parameter estimation problem classi cation number classes labeled training data class parameters class 
parameters estimated training data 
selecting dirichlet prior expectation parameter respect posterior 
details refer reader section heckerman 
gives simple form estimate multinomial parameter involves number times word appears documents class ci divided total number word occurrences class 
word prior adds imagined occurrences estimate smoothed version maximum likelihood estimate ci ci denotes sum set di erently word follow common practice setting words 
substituting true parameters equation estimates get mnb classi er mnb argmax log log ci class prior estimate 
prior class probabilities estimated word estimates 
class probabilities tend combination word probabilities uniform prior estimate simplicity 
weights decision boundary de ned mnb classi er log parameter estimates ci log ci 
correcting systemic errors naive bayes systemic errors 
systemic errors algorithm cause inappropriate favoring class 
section discuss studied systemic errors cause naive bayes perform poorly 
highlight cause misclassi cations propose solutions mitigate eliminate 

skewed data bias section show skewed data training examples class cause decision boundary weights biased 
causes classi er unwittingly prefer class 
show reason bias propose alleviate problem learning weights class training data class 
table 
shown simple classi cation example classes 
class binomial distribution probability heads respectively 
training sample class training samples class want label heads occurrence 
nd maximum likelihood parameter settings possible sets training data estimates label test example class predicts higher rate occurrence heads 
class higher rate heads test example classi ed class 
class class data label tt fht class hh class tt class fht class hh table gives simple example bias 
example class higher rate heads class 
classi er labels heads occurrence class class 
class default 
classi er labels tails example class despite class lower rate tails 
effect call skewed data bias directly results imbalanced training data 
number training examples class get expected result heads example labeled class higher rate heads 
consider complex example weights decision boundary text classi cation shown equation learned 
log concave function expected value weight estimate log expected value parameter estimate ci log ci 
training data skewed di erence approximately classes 
training data skewed weights lower class training data 
classi cation erroneously biased class case example table 
deal skewed training data introduce complement class version naive bayes called complement naive bayes cnb 
estimating weights regular mnb equation training data single class contrast cnb estimates parameters data classes think cnb estimates ective uses amount training data class lessen bias weight estimates 
nd get stable weight estimates improved classi cation accuracy 
improvements due data estimate amount data just way susceptible skewed data bias 
cnb estimate ci ci ci number times word occurred documents classes total number word occurrences classes smoothing parameters equation 
weight estimate ci log ci classi cation rule cnb argmax log log ci negative sign represents fact want assign class documents poorly match complement parameter estimates 
shows di erent amounts training data ect regular weight estimates complement weight estimates 
regular weight estimates shift change ordering examples training data examples 
particular word smallest weight examples moves th largest weight estimated examples 
complement weights show ects smoothing show severe upward bias retain relative ordering 
complement estimates mitigate problem skewed data bias 
cnb related versus commonly versus technique frequently multi label classi cation example may label 
berger zhang oles vs mnb works better regular mnb 
classi cation rule ova argmax log log ci log ci combination regular complement classi cation rules 
attribute improvement classification weight training examples class training examples class 
average classi cation weights highly discriminative features newsgroups 
amount training data varied axis 
plot shows weights mnb plot shows weights cnb 
cnb stable varying amount training data 
vs complement weights 
nd cnb performs better regular mnb eliminates biased regular mnb weights 

weight magnitude errors section discussed uneven training sizes cause naive bayes bias weight vectors 
section discuss independence assumption erroneously cause naive bayes produce di erent magnitude classi cation weights 
magnitude naive bayes weight vector larger class class may preferred 
naive bayes di erences weight magnitudes deliberate attempt create greater uence class 
weight di erences partially artifact applying independence assumption dependent data 
naive bayes gives uence classes violate independence assumption 
example illustrates ect 
consider problem distinguishing documents discuss boston ones discuss san francisco 
assume boston appears boston documents san francisco appears san francisco documents expect 
assume rare see words san francisco apart 
time test document occurrence san francisco multinomial naive bayes double count add weight san weight francisco 
san francisco boston occur equally respective classes single occurrence san francisco contribute twice weight occurrence boston 
summed contributions classi cation weights may larger class cause mnb prefer class incorrectly 
example document occurrences boston san francisco mnb label document san francisco boston 
practice case weights tend lean class 
problem identifying barley documents reuters corpus advantageous choose threshold term negative chosen counting documents 
testing di erent smoothing values gave extreme example 
threshold term classi er achieved low error rate smoothing value 
threshold term calculated prior estimate counting training documents log 
threshold yielded somewhat higher rate error 
naive bayes independence assumption lead strong preference barley documents 
correct fact classes greater dependencies normalizing weight vectors 
assigning ci log ci assign ci log ci log ck call combined cnb weight normalized complement naive bayes 
experiments indicate ective 
alternately address problem optimizing threshold terms webb pazzani give method doing calculating class weights identi ed violations naive bayes classi er webb pazzani 
manipulating weight vector directly table 
experiments comparing multinomial naive bayes mnb weight normalized complement naive bayes data sets 
industry sector news reported terms accuracy reuters terms precision recall breakeven 
outperforms mnb 
mnb industry sector newsgroups reuters micro reuters macro longer model aspects naive bayes 
common model techniques incorporate unlabeled data uncover latent classes em applicable 
trade improved classi cation performance 

bias correction experiments ran classi cation experiments validate techniques suggested 
table gives classi cation performance text data sets reporting accuracy newsgroups industry sector precision recall breakeven reuters 
see appendix description data sets experimental setup 
compared weight normalized complement naive bayes standard multinomial naive bayes mnb resulted marked improvement data sets 
improvement greatest data sets training data quantity varied classes reuters industry sector 
greatly improved reuters macro breakeven score suggests improvement attributed better performance classes training examples 
shows improvement small signi cant newsgroups distribution training examples classes 
comparing note baseline mnb similar mnb results 
newsgroups result closely matches reported mccallum nigam vs 
di erence ghani industry sector result vs due feature selection 
zhang oles result industry sector signi cantly higher optimize smoothing parameter 
optimized smoothing parameter mnb cross validation experiments reported mnb results similar 
smoothing parameter optimization improved 
micro macro scores reuters reasonably similar yang liu vs vs di erences due feature selection di erent scoring metric di erent pre processing system smart 

modeling text better far discussed systemic issues arise naive bayes classi er 
mnb uses multinomial model text accurate 
section look transforms better align model data 
transform ects frequencies term frequency distributions heavier tail multinomial model expects 
transform document frequency keep common terms dominating classi cation length keep long documents dominating training 
transforming data better suited multinomial model nd signi cant improvement performance mnb transforms 

transforming term frequency order understand mnb job classifying text looked empirical term distributions text 
term distributions heavier tails predicted multinomial model appearing power law distribution 
simple transform power law term distributions look multinomial 
measure multinomial model ts term distribution text compared empirical distribution maximum likelihood multinomial 
visualization purposes took set words approximately occurrence rate created histogram term frequencies set documents similar length 
term frequency rates predicted best multinomial model plotted log scale 
gure shows empirical term distribution di erent multinomial model predict 
empirical distribution heavier tail meaning multiple occurrences term expected best multinomial 
example multinomial model predicts chance seeing average word occur times document low event unexpected collection news stories written 
reality chance rare single document unexpected collection documents 
behavior called burstiness ob term frequency data power law multinomial term frequency data power law 
shown example term frequency probability distribution compared best analytic distributions 
data heavier tail multinomial model predicts 
power law distribution log matches closely particularly optimal chosen 
served church gale katz 
developed sophisticated models deal term burstiness simple heavy tailed distribution power law distribution better model text motivate simple transform features mnb model 
shows example empirical distribution alongside power law distribution log chosen closely match text distribution 
probability proportional log similar multinomial model probability proportional multinomial model generate probabilities proportional class power law distributions simple transform log 
transform log advantages identity transform zero counts pushing larger counts 
transform allows realistically handle text giving advantages mnb 
setting match data optimized term frequency doc length doc length doc length 
plotted average term frequencies words classes reuters documents short documents medium length documents long documents 
terms longer documents heavier tails 
produce distribution closer empirical distribution best multinomial shown power law line 

transforming document frequency useful transform discounts terms occur documents 
common words related class document random variations create apparent ctitious correlations 
adds noise parameter estimates classi cation weights 
common words appear hold classi cation decision weight di erences classes small 
reason advantageous words 
heuristic transform information retrieval ir community known inverse document frequency discount terms document frequency jones 
common way log ij ij word occurs document sum document indices salton buckley 
rare words increased term frequencies common words weight 
improve performance 

transforming length documents strong word inter dependencies 
word rst appears document appear 
mnb assumes occurrence independence long documents negatively ect parameter estimates 
normalize word counts avoid table 
experiments comparing multinomial naive bayes mnb transformed normalized complement naive bayes support vector machine svm data sets 
performance substantially better mnb approaches svm performance 
industry sector news reported terms accuracy reuters results precision recall breakeven 
mnb svm industry sector newsgroups reuters micro reuters macro problem 
shows empirical term frequency distributions documents di erent lengths 
surprising longer documents larger probabilities larger term frequencies jump larger term frequencies large 
documents group average twice long group chance word occurring times group larger word occurring twice group 
case text multinomial 
deal common ir transform seen naive bayes 
discount uence long documents transforming term frequencies pp yielding length term frequency vector document 
transform common ir community probability generating document model compared documents case want short documents dominating merely fewer words 
classi cation comparisons classes documents bene normalization subtle especially multinomial model accounts length naturally lewis 
transform keeps single document dominating parameter estimates 

experiments described set transforms term frequencies 
tries resolve di erent problem modeling assumptions naive bayes 
set modi cations procedure applying shown table 
apply modi cations nd signi cant improvement text classi cation performance mnb 
ta table 
new naive bayes procedure 
assignments possible index values 
steps distinguish 
dn set documents ij count word document yn labels 

ij log ij tf transform 
ij ij log ik idf transform 
ij ij pp length norm 

ci ij complement 
ci log ci 
ci ci ci weight normalization 
test document count word 
label document arg min ci ble shows classi cation accuracy industry sector newsgroups precision recall breakeven reuters 
tests length normalization transform useful followed log transform 
document frequency transform import 
show results support vector machine svm comparison 
transforms described section svm improved classi cation performance 
discussed similarities multinomial naive bayes results section 
support vector machine results similar 
industry sector result matches reported zhang oles vs 
di erence 
result vs newsgroups due di erent multiclass schema 
micro macro scores reuters di er yang liu vs vs due feature selection di erent scoring metric di erent pre processing system smart 
larger di erence macro results due sensitivity macro calculations heavily weighs small classes 

described techniques shown table correct de ciencies application naive bayes classi er text data 
series transforms information retrieval community steps table improves performance naive bayes text classi cation 
example transform described step converts text closely modeled power law look multinomial 
training complement class step solves problem uneven training data 
normalizing classi cation weights step improves naive bayes handling word occurrence dependencies 
modi cations better align naive bayes realities bag words textual data shown empirically signi cantly improve performance number data sets 
modi ed naive bayes fast easy implement near stateof art text classi cation algorithm 
grateful yu han chang tommi jaakkola input 
anonymous reviewers valuable comments 
supported mit oxygen partnership national science foundation itr graduate research fellowships nsf 
berger 

error correcting output coding text classi cation 
proceedings ijcai 
church gale 

poisson mixtures 
natural language engineering 
domingos pazzani 

independence conditions optimality simple bayesian classi er 
proceedings icml 
duda hart 

pattern classi cation scene analysis 
wiley sons ghani 

error correcting codes text classi cation 
proceedings icml 
sarawagi chakrabarti 

scaling multi class support vector machines confusion 
proceedings sigkdd 
heckerman 

tutorial learning bayesian networks technical report msr tr 
microsoft research 
joachims 

text categorization support vector machines learning relevant features 
proceedings ecml 
jones 

statistical interpretation term speci city application retrieval 
journal documentation 
katz 

distribution content words phrases text language modelling 
natural language engineering 
lewis 

naive bayes independence assumption information retrieval 
proceedings ecml 
mccallum nigam 

comparison event models naive bayes text classi cation 
proceedings aaai 
salton buckley 

term weighting approaches automatic text retrieval 
information processing management 
webb pazzani 

adjusted probability naive bayesian induction 
proceedings ai 
yang liu 

re examination text categorization methods 
proceedings sigir 
zhang oles 

text categorization regularized linear classi cation methods 
information retrieval 
appendix experiments known data sets newsgroups industry sector reuters 
industry sector news single label data sets document assigned single class 
reuters multi label data set document may labels 
reuters multi label handled di erently described 
mnb standard vs usually vs binary problem 
cnb vs making amount data estimate 
industry sector reuters widely varying numbers documents class single class dominates 
distribution documents class newsgroups examples class 
newsgroups ran random splits training data testing data class 
industry sector documents classes largest category documents smallest 
industry sector ran random splits training data testing data class 
reuters modapte split topics training document testing document 
gives training documents classes largest category training documents 
svm experiments set 
vs produce multi class labels svm 
linear kernel performs non linear kernels text classi cation yang liu 
available mit edu 
