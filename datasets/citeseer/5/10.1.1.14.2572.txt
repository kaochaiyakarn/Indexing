comparative study maximum entropy discriminative training acoustic modeling automatic speech recognition wolfgang hermann ney lehrstuhl fur informatik vi computer science department rwth aachen university technology aachen germany fw informatik rwth aachen de maximum entropy learning procedures successfully applied text natural language processing little investigations acoustic modeling automatic speech recognition 
show known generalized iterative scaling gis algorithm alternative method discriminatively train parameters speech recognizer gaussian densities 
approach compared conventional maximum likelihood training discriminative training extended baum algorithm 
experimental results reported connected digit string recognition task 

past years maximum entropy training methods successfully applied field natural language processing language modeling part speech tagging language understanding statistical machine translation 
natural language processing methods hardly investigated automatic speech recognition 
employed estimate parameters direct model phoneme recognizer 
approach generalizes markov models memm proposed sequential processes complex contextual information processed 
reason hardly acoustic modeling general optimization problem difficult manage hidden variables occur 
hidden variables may lead non convex objective functions common training algorithms generalized iterative scaling gis faster version improved iterative scaling iis applied 
automatic speech recognition typically types events observable described hidden variables alignment sequence acoustic observations states markov chain component weights mixture densities 
tries optimize class posterior probabilities objective function corresponds maximum mutual information mmi criterion asr effective optimization method extended baum eb algorithm allows efficiently training free model parameters 
show discriminatively train acoustic parameters speech recognizer gaussian densities relaxing constraints gis algorithm 
contrast features rank lists gaussian model indices cepstral coefficients provided signal analysis part common speech recognizers 
framework takes competing classes account compare achieved performance word error rate obtained maximum likelihood ml trained system discriminatively trained system mmi criterion 
experimental results reported speech corpus recognition telephone line recorded german connected digit strings 
remainder organized follows 
section briefly describe basics gis algorithm show acoustic models embedded log linear framework 
section derive update rules gis algorithm compare resulting re estimation formulae obtained conventional discriminative approach eb algorithm 
experimental results discussion section 
concludes summary outlook section 
maximum entropy modeling set training samples principle choose distribution consistent constraints derived training data making little assumptions possible 
shown resulting distribution defined leads log linear model 
usually constraints formulated called feature functions 
presentation convenient definitions features 
feature function shall return value iff predicted class corresponds actual class observation satisfies condition true feature said activated returned value larger called inactive 
solution approach log linear exponential form cjx exp denoting set free parameters normalization term exp optimal parameter setting estimated maximum likelihood framework 
sequence labeled training samples xn cn objective function criterion log cn log cjx number occurrences pair training corpus 
parameter setting maximizes eq 
obtained wrt 

cjx 
note counts necessarily integer values eq 
positive real number 
sum convex functions convex exists exactly global maximum effectively determined gis algorithm 
iteration gis algorithm requires constant number active features fulfilled adding correction feature max determined training corpus 
parameter update 
feature results solving equation 
exp 
leads gis algorithm shown table 

automatic speech recognition denote sequence training utterances 
utterance shall sequence acoustic observations xr xr transcription wr wr speech recognizers employ cepstral features define feature functions set mappings project dimensional observation vector ir dth component iff predicted markov state corresponds state observation aligned table implementation gis algorithm 
parameter small positive value controls terminating condition outer loop 
compute init 
loop sample log cn class index active feature log 
ensure features positive sum affinely transformed diagonal scaling matrix bias 
td tr holds training sample define feature functions yd note affine transformations applied arbitrary finite sets training samples force positivity normalization data transformations change optimum gis algorithm 
emission probability markov state acoustic observation vector modeled gaussian distribution xj xj denoting state dependent mean globally pooled covariance matrix 
covariance matrix advantage terms quadratic exponent neglected cancel due explicit renormalization 
obtain required log linear model emission probabilities wr viterbi wr 
max tr tr js 

max tr jw tr js 
wr 
max tr tr js pd 
bx 
max tr jw tr js pd 
bx denotes set competing word sequences log det ir augmented observation vector ir ideally set contain possible word sequences 
practice obtained recognition pass represented word lattices best lists 
objective function corresponds maximum mutual information mmi criterion denote respective function fme order emphasize log linear model fme log wr 
xr 
xr jw xr jw max tr jw tr js pd 
bx due omitted normalization xr jw contrast wr real distribution 
decode spoken utterance sufficient compare scores recognition phase 
order maximize objective function determine partial derivative fme wrt 
fme tr 
forward backward fb probability hypothesizing state time frame transcription wr generalized fb probability hypothesizing state time frame independently word sequence 
fb probabilities estimated efficiently word lattices state graphs 

discriminative training far addressed problem aligning acoustic observations states markov chain 
mentioned alignment observed directly modeled hidden variable 
general alignment fixed may change training passes 
assumption alignment spoken word sequence statistics remains unaltered gis iterations 
assumption risky holds certain update parameter set change optimal alignment spoken sentence 
compensate training utterances re aligned times order ensure statistics reliable 
consequence counts depend parameter set redefine statistics eq 
tr 
tr 
due necessary re alignments optimization problem convex longer expect gis algorithm converge global optimum 
gradients resulting update rules train parameters log linear model emission probabilities log 
comparison discriminative training contrast eq 
re estimation formulae standard discriminative training lead update rules depend logarithm difference ratio 
demonstrate consider standard optimization technique discriminative training extended baum eb algorithm 
objective functions identical mmi criterion introduce mmi objective function separate definition order distinguish dependency model log linear vs gauss optimization method gis vs eb log wr 
xr wr 
xr derivation wrt 
yields log denotes discriminative average state defined tr wr 
xr eb method leads re estimation equations 

ds ds state specific iteration constant controls convergence training process 
choice ds ensure positive achieved setting ds 
max min min constant guarantees positive definite variances parameter chosen prevent overflows caused low valued denominators scaling factor controls step size gradient 
decomposition discriminative averages correct model competitive model tr wr 
xr tr 
xr rewrite eq 
obtain re estimation formulae gis algorithm eb method gis 
log eb 

comparing gradients may expect convergence gis algorithm turn slow update rules depend logarithm ratio correct model competing model gradient eb algorithm difference 
integration linear feature space transformations log linear form emission probabilities allows initializing parameter set linear feature transformation linear discriminant analysis lda 
denote lda transformation matrix 
full rank mean variance scaled features computed parameters gaussian distribution xj unscaled lda transformed feature space ah ah zh obtain expression order initialize free parameters log linear model ms log det ms iteration male male lda female female lda wer re alignment male male lda female female lda evolution objective function course iteration process left word error rates realignment points right sietill training corpus 

experimental results experiments performed sietill corpus telephone line recorded german continuous digit strings 
corpus consists approximately spoken digits sentences training test set 
table information corpus statistics summarized 
recognition system gender dependent word hmms continuous emission densities 
gender distinct states plus silence 
observation vectors consist cepstral features derivatives second derivative component 
baseline recognizer applies ml training viterbi approximation achieves word error rate wer cf 
table 
training eb training initialized ml trained parameters 
time objective function increases relative gis algorithm terminated training utterances re aligned 
re alignments system achieves wer relative improvement compared ml system 
number re alignments eb algorithm standard discriminative approach achieves wer 
gis algorithm requires iterations re alignments order obtain reported performance 
linear discriminant analysis lda relative performance gain decreases significant 
relative improvement baseline result approach 
contrast ml eb trained systems parameters trained system initialized lda transformation matrix eq 

consequence gis algorithm operates untransformed high dimensional features possibility extract information data ml eb systems lda transformed features 
due small number re alignments approach reached optimum training iterations necessary 
table corpus statistics sietill corpus 
corpus female male sent 
digits sent 
digits test train table word error rates sietill test corpus different optimization criteria 
method lda re align wer ser ml eb ml eb 
generalized iterative scaling gis algorithm applied discriminatively train parameters speech recognizer 
purpose constraints gis algorithm modified 
maximum entropy approach analytically experimentally compared standard approach discriminative training extended baum eb algorithm 
experiments conducted connected digit string recognition task achieved relative improvement compared maximum likelihood trained system 
combination linear discriminant analysis eb algorithm achieved performance gain relative compared approach 
acknowledgments funded european commission human language technologies project ist 

rosenfeld maximum entropy approach adaptive statistical language modeling computer speech language vol 
pp 
july 
bender och ney comparison alignment templates maximum entropy models natural language understanding proc 
th conf 
europ 
chapter assoc 
computational linguistics budapest hungary apr 
och ney discriminative training maximum entropy models statistical machine translation proc 
annual meeting association computational linguistics philadelphia pa july pp 

gao direct models phoneme recognition int 
conf 
acoustics speech signal processing orlando fl may vol 
pp 

mccallum freitag pereira maximum entropy markov models information extraction segmentation proc 
th international conf 
machine learning 
pp 
morgan kaufmann san francisco ca 
darroch ratcliff generalized iterative scaling log linear models annals mathematical statistics vol 
pp 

della pietra della pietra lafferty inducing features random fields ieee trans 
pattern analysis machine intelligence vol 
pp 
april 
implementation comparison discriminative training methods automatic speech recognition diploma thesis lehrstuhl fur informatik vi rwth aachen aachen nov 
