class gram models natural language peter brown peter desouza robert mercer ibm watson research center vincent della pietra lai address problem predicting word previous words sample text 
particular discuss gram models classes words 
discuss statistical algorithms assigning words classes frequency occurrence words 
find able extract classes flavor syntactically groupings semantically groupings depending nature underlying statistics 

number natural language processing tasks face problem recovering string english words garbled passage noisy channel 
tackle problem successfully able estimate probability particular string english words input noisy channel 
discuss method making estimates 
discuss related topic assigning words classes statistical behavior large body text 
section review concept language model give defini tion gram models 
section look subset gram models words divided classes 
show maximum likelihood assignment words classes equivalent assignment average mutual information adjacent classes greatest 
finding optimal assignment words classes computationally hard describe algorithms finding suboptimal assignment 
section apply mutual information forms word clustering 
find pairs words function single lexical entity 
examining probability words appear reasonable distance find classes loose semantic coherence 
describing draw freely terminology notation mathematical theory communication 
reader unfamiliar field allowed facility concepts fall may profit brief perusal feller gallagher 
reader focus conditional probabilities markov chains second entropy mutual information 
ibm watson research center yorktown heights new york 
association computational linguistics computational linguistics volume number source channel setup 

language models source language model channel model pr pr pr shows model long automatic speech recognition bahl jelinek mercer proposed machine translation brown automatic spelling correction mercer 
automatic speech recognition acoustic signal machine translation sequence words language spelling correction sequence characters produced possibly imperfect typist 
applications signal seek determine string english words gave rise 
general different word strings give rise signal hope recover successfully cases 
minimize probability error choosing estimate string posteriori probability greatest 
fixed choice probability proportional joint probability shown product terms priori probability probability appear output channel placed input 
priori probability pr probability string arise english 
attempt formal definition english concept arising english 
assume production english text characterized set conditional probabilities pr wk terms probability string words expressed product pr wkl pr wl pr pr wk iw 
represents string wk 
conditional probability pr wk call wl history wk prediction 
refer computational mechanism obtaining conditional probabilities language model 
choose different language models better 
performance language model complete system depends delicate interplay language model components system 
language model may surpass part speech recognition system perform translation system 
expensive evaluate language model context complete system led seek intrinsic measure quality language model 
example lan peter brown vincent della pietra class gram models natural language guage model compute joint probability collection strings judge better language model yields greater probability 
perplexity language model respect sample text reciprocal geometric average probabilities predictions words perplexity pr 
language model smaller perplexity assigns larger probability perplexity depends language model text respect measured important text representative language model intended 
perplexity subject sampling error making fine distinctions language models may require perplexity measured respect large sample 
gram language model treat histories equivalent words assume pr wk wl equal pr wk wk vocabulary size gram model independent parameters word minus constraint probabilities add 
gram model independent parameters form pr wl form pr total independent parameters 
general gram model independent parameters form pr wn call order parameters plus parameters gram model 
estimate parameters gram model examining sample text call training text process called training 
number times string occurs string gram language model maximum likelihood estimate parameter pr estimate parameters gram model estimate parameters gram model contains choose order parameters maximize pr tn 
order parameters wn pr wn ew wl call method parameter estimation sequential maximum likelihood estimation 
think order parameters gram model constituting transition matrix markov model states sequences words 
probability transition state 
wn state pr wn wn steady state distribution transition matrix assigns probability gram denote 
say gram language model consistent string ln probability model assigns 
sequential maximum likelihood estimation general lead consistent model large values model nearly consistent 
maximum likelihood estimation parameters consistent gram language model interesting topic scope 
vocabulary english large small values number parameters gram model enormous 
ibm speech recognition system vocabulary words employs gram language model parameters 
illustrate problems attendant parameter estimation gram language model data table 
show number grams appearing various frequencies sample words english text variety sources 
vocabulary consists different words plus special computational linguistics volume number count grams grams grams table number grams various frequencies words running text 
unknown word words mapped 
grams occurred data occur occurred 
similarly grams occurred occur occurred 
data turing formula expect maximum likelihood estimates percent grams percent grams new sample english text 
confident gram appear sample fact rare aggregate probability substantial 
increases accuracy gram model increases reliability parameter estimates drawn limited training text decreases 
jelinek mercer describe technique called interpolated estimation combines estimates language models estimates accurate models reliable unreliable fall back reliable estimates accurate models 
prq wi conditional probability determined jth language model interpolated estimate pr wi pr iwi pr wi 
values prq chosen help em algorithm maximize probability additional sample text called held data baum dempster laird rubin jelinek mercer 
interpolated estimation combine estimates gram models choose depend history count gram wi wi 
expect count gram high gram estimates reliable count low estimates unreliable 
constructed interpolated gram model divided different sets gram counts 
estimated held sample words 
measure performance model brown corpus contains variety english text included training held data ku era francis 
brown corpus contains words perplexity respect interpolated model 

word classes clearly words similar words meaning syntactic function 
surprised learn probability distribution words vicinity thursday words vicinity friday 
peter brown vincent della pietra class gram models natural language course identical rarely hear say god thursday 
worry thursday th 
successfully assign words classes may possible reasonable predictions histories previously seen assuming similar histories seen 
suppose partition vocabulary words classes function maps word wi class ci 
say language model gram class model gram language model addition pr wk pr wk ck pr ck 
gram class model independent parameters form pr wi ci plus independent parameters gram language model vocabulary size trivial cases gram class language model fewer independent parameters general gram language model 
training text maximum likelihood estimates parameters gram class model pr pr mean number words tl class equations see pr pr pr gram class model choice mapping rr effect 
gram class model sequential maximum likelihood estimates order parameters maximize pr tl equivalently log pr clc pr cl cc clc definition pr clc pr cl pr cl sequential maximum likelihood estimation pr clc clc cl clc cl clc numbers words class cl strings respectively final term equation tends tends infinity 
pr clc tends relative frequency clc consecutive classes training text 
rr 
tr icl pr wl pr cl pr 
clc pr ww tends relative frequency training text pr limit tr log pr icl pr cic cl computational linguistics volume number entropy gram word distribution cl average mutual information adjacent classes 
depends average mutual information partition maximizes limit partition maximizes average mutual information adjacent classes 
know practical method finding partitions maximize average mutual information 
partition know practical method demonstrating fact maximize average mutual informa tion 
obtained interesting results greedy algorithm 
initially assign word distinct class compute average mutual information adjacent classes 
merge pair classes loss average mutual information 
merges classes remain 
find classes obtained way average mutual information larger moving words class 
having derived set classes successive merges cycle moving word class resulting partition greatest average mutual information 
eventually potential reassignment word leads partition greater average mutual information 
point 
may possible find partition higher average mutual information ously reassigning words regard search costly feasible 
suboptimal algorithm practical exercise certain care implementation 
approximately merges investigate carry th step 
average mutual information remaining sum terms involves logarithm 
altogether merges straightforward approach computation order seriously contemplate calculation small values frugal organization computation take advantage redundancy straightforward calculation 
shall see computation average mutual information remaining merge constant time independent suppose merges resulting classes ck ck ck wish investigate merge ck ck pk pr ck ck probability word class ck follows word class ck 
plk pk prk pk qk pk pk log pl average mutual information remaining merges ik qk 
notation represent cluster obtained merging ck ck 
peter brown vincent della pietra class gram models natural language example pk pk pk pk qk pk log plk prk average mutual information remaining merge ck ck ik ik sk sk qk qk qk qk qk sk qk qk qk 
know ik sk sk majority time involved computing ik devoted computing sums second line equation 
sums approximately terms reduced problem evaluating ik order order improve keeping track pairs pk different 
recall table example grams occurred training data occur 
case sums required equation average non zero terms expect size vocabulary 
examining pairs find pair loss average mutual information lk ik ik 
complete step merging ck ck form new cluster ck 
rename ck ck set ck ck 
obviously lk ik 
values pk plk prk qk obtained easily pk plk prk qk 
denote indices equal easy establish sk sk lk sk qk qk qk qk qk qk sk qk qk qk qk qk qk lk qk qk qk qk qk qk lk lk qk qk qk qk qk qk lk lk evaluate sk lk equations 
entire update process requires order computations course determine pair clusters merge 
algorithm order 
described algorithm finding clusters determine 
continue algorithm merges single cluster course entire vocabulary 
order clusters merged determines binary tree root corresponds computational linguistics volume number plan letter request memo case question charge statement draft day year week month quarter half ii representatives reps representative rep sample subtrees word mutual information tree 
evaluation assessment analysis understanding opinion conversation discussion accounts people customers individuals employees students il single cluster leaves correspond words vocabulary 
intermediate nodes tree correspond groupings words intermediate single words entire vocabulary 
words statistically similar respect immediate neighbors running text close tree 
applied tree building algorithm vocabularies words 
shows substructures tree constructed manner frequent words collection office correspondence 
words algorithm fails practicality 
obtain clusters larger vocabularies proceed follows 
arrange words vocabulary order frequency frequent words assign words distinct class 
step algorithm assign jr st probable word new class merge pair resulting classes loss average mutual information 
th step algorithm assign th probable word new class 
restores number classes merge pair loss average mutual information 
steps words vocabulary assigned classes 
algorithm divide word vocabulary table classes 
table contains examples classes find particularly interesting 
table contains examples selected random 
lines tables contains members different class 
average class words table manageable include words occur times peter brown vincent della pietra class gram models natural language friday monday thursday wednesday tuesday saturday sunday weekends june march july april january december october november september august people guys folks fellows backwards sideways downwards water gas coal liquid acid sand carbon steam iron great big vast sudden mere sheer gigantic lifelong scant man woman boy girl lawyer doctor guy farmer teacher citizen american indian european japanese german african catholic israeli italian pressure temperature permeability density porosity stress velocity viscosity gravity tension mother wife father son husband brother daughter sister boss uncle machine device controller processor cpu printer spindle subsystem compiler john george james bob robert paul william jim david mike anybody somebody feet miles pounds degrees inches barrels tons acres meters bytes director chief professor commissioner commander founder dean cus liberal conservative parliamentary royal progressive tory provisional federalist pq hadn asking telling wondering instructing informing reminding bc tha head body hands eyes voice arm seat eye hair mouth table classes word vocabulary 
include frequent words class months appear class months extended limit twelve 
degree classes capture syntactic semantic aspects english quite surprising constructed counts bigrams 
class tha interesting tha english words computer discovered data 
table shows number class grams occurring text various frequencies 
expect data maximum likelihood estimates assign probability percent class grams percent class grams new sample english text 
substantial improvement corresponding numbers gram language model percent word grams percent word grams achieved expense precision model 
class model distin different words class relative frequencies text 
looking classes tables feel computational linguistics volume number little prima moment tad minute tinker teammate ask remind instruct urge interrupt invite warn object apologize cost expense risk profitability deferral cardinality dept aa cl pi pa mgr 
rel rel 
nai ow sumit research training education science advertising arts medicine machinery art aids rise focus depend rely concentrate dwell capitalize embark intrude minister mover running moving playing setting holding carrying passing cutting driving fighting court judge jury slam marshal annual regular monthly daily weekly quarterly periodic yearly convertible aware unaware unsure cognizant mindful force force conditioner forwarder systems loggers products econ centre correctors industry producers makers arabia addiction addict brought moved opened picked caught tied gathered cleared hung lifted table randomly selected word classes 
count gram grams grams table number class grams various frequencies words running text 
reasonable pairs john george liberal conservative pairs little prima minister mover 
classes construct interpolated gram class model training text held data word language model discussed 
measured perplexity brown corpus respect model 
interpolated class estimators word estimators perplexity test data small improvement perplexity obtained word model 
peter brown vincent della pietra class gram models natural language 
sticky pairs semantic classes previous section discussed methods grouping words statistical similarity surroundings 
discuss ad ditional types relations words discovered examining various occurrence statistics 
mutual information pair wl adjacent words pr wl log pr wl pr 
follows wl expect basis independent frequencies mutual information negative 
follows wl expect mutual information positive 
say pair sticky mutual information pair substantially greater 
table list pairs words word sample text canadian parliament 
mutual information pair bits corresponds base logarithm equation 
pairs proper names foreign phrases adopted english mutatis mutandis avant 
mutual information bits means pair occurs roughly times expect individual frequencies 
notice property sticky pair symmetric forms sticky pair 
table sticky word pairs 
word pair mutual information ku lao bao tse tung avant bobby orr mutatis mutandis abu aldo computational linguistics volume number table semantic clusters 
question questions asking answer answers answering performance performed perform performs performing tie jacket suit write writes writing written wrote pen morning noon evening night nights midnight bed attorney counsel trial court judge problems problem solution solve analyzed solved solving letter addressed enclosed letters correspondence large size small larger smaller operations operations operating operate operated school classroom teaching grade math street block avenue corner blocks table tables dining chairs plate published publication author publish writer titled wall ceiling walls enclosure roof sell buy selling buying sold seeking pairs words occur expect seek pairs words simply occur near expect 
avoid finding sticky pairs considering pairs words occur close 
precise proba bility word chosen random text wl second word chosen random window words centered wl excluding words window centered wl 
say wl semantically sticky larger pr wo pr 
stickiness semantic stickiness symmetric wl sticks semantically sticks semantically wl 
table show interesting classes constructed manner similar described preceding section 
classes group gether words having morphological stem performance performed perform performs performing 
classes contain words semantically related different stems attorney counsel trial court judge 

discussion described methods feel clearly demonstrate value simple statistical techniques allies struggle tease words linguistic secrets 
demonstrated full value secrets gleaned 
expense slightly greater perplexity gram model word classes requires third storage gram language model word treated unique individual see tables 
combine models able achieve improvement perplexity 
confident eventually able significant improvements gram language models help classes kind described 
peter brown vincent della pietra class gram models natural language acknowledgment authors john lafferty assistance constructing word classes described 
bahl brown cole das davies gennaro de de souza epstein jelinek lewis mercer nadas nahamoo van 

experiments word speech recognizer 
proceedings ieee international conference acoustics speech signal processing 
dallas texas 
bahl jelinek mercer 

maximum likelihood approach continuous speech recognition 
ieee transactions pattern analysis machine intelligence pami 
baum 

inequality associated maximization technique statistical estimation probabilistic functions markov process 
inequalities 
brown cocke dellapietra dellapietra jelinek lafferty mercer 

statistical approach machine translation 
computational linguistics 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
feller 

probability theory applications volume john wiley sons gallagher 

information theory reliable communication 
john wiley sons 

population frequencies species estimation population parameters 
biometrika 
jelinek mercer 

interpolated estimation markov source parameters sparse data 
proceedings workshop pattern recognition practice amsterdam netherlands 
ku era francis 

computational analysis day american english 
brown university press 
damerau mercer 

context spelling correction 
proceedings ibm natural language itl 
paris france 

