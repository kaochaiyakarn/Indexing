submitted th international conference machine learning icml 
employing em pool active learning text classification andrew mccallum zy mccallum com kamal nigam cs cmu edu just research henry street pittsburgh pa school computer science carnegie mellon university pittsburgh pa shows text classifier need labeled training data reduced combination active learning expectation maximization em pool unlabeled data 
query committee actively select documents labeling em naive bayes model improves classification accuracy concurrently estimating probabilistic labels remaining unlabeled documents improve model 
metric better measuring disagreement committee members accounts strength disagreement distribution documents 
experimental results show method combining em active learning requires half labeled training examples achieve accuracy em active learning 
keywords text classification active learning unsupervised learning information retrieval settings learning text classifiers obtaining labeled training examples expensive obtaining large quantities unlabeled examples cheap 
example consider task learning web pages user finds interesting user may patience hand label training pages interesting unlabeled pages readily available internet 
presents integration active learning expectation maximization efficient learning naive bayes text classifiers 
resulting method works limited amounts training data advantage large pool unlabeled data 
previous nigam show clustering unlabeled labeled documents expectation maximization em reduces text classification error third traditional supervised learning realworld data sets 
setting em classifier trained limited labeled examples available classifier probabilistically fill missing labels unlabeled data 
new classifier trained combination labeled probabilistically labeled data process iterates 
em uses unlabeled data better model underlying distribution data finding classifier parameters locally maximize probability labeled unlabeled data 
method demonstrates way learning labeled documents 
active learning approaches problem different way 
setting em active learner request true class label unlabeled examples selects 
request considered expensive operation point perform queries possible 
specific subsetting appropriate text learning selective sampling learner choose examples stream fixed pool creating unlabeled synthetic examples 
consider pool sampling 
active learning aims select informative examples labeling pool 
informative examples class label known reduce classification error variance distribution examples 
methods measure expected classification variance reduction directly cohn method query committee qbc easier apply closed form calculation variance prohibitively complex traditional approaches explicitly model distribution examples freund qbc measures variance indirectly examining disagreement class labels assigned set classifier variants variants sampled probability distribution classifiers resulting labeled training examples 
presents results showing combination em qbc learns fewer labeled examples individually 
experimental results real world text data set em applied labeled examples chosen active learning remaining unlabeled examples requires half labeled examples achieve accuracy active learning em 
advocate paradigm call pool leveraged sampling uses estimated statistics pool data improve active learning document selection 
context describe selection metric qbc explicitly models distribution data 
experimental results show application paradigm improves performance 
discuss stronger method pool leveraged sampling interleaves qbc em unsupervised structure pool unlabeled examples informs selection active learning queries 
having qbc committee members perform em hope avoid selecting examples labels reliably filled em encourage selection examples help em find local maximum higher classification accuracy 
probabilistic framework naive bayes section presents probabilistic framework text classification em active learning 
sections add em active learning building framework 
approach task text classification bayesian learning perspective 
commonly assumptions domingos pazzani joachims nature generative parametric model training data calculate bayes optimal estimates model parameters 
armed estimates classify new test documents bayes rule turn generative model calculate probability class generated test document question 
assume text documents generated mixture model parameterized 
secondly assume mixture model contains mixture components component corresponds single class documents 
dictates document created selecting class class priors having corresponding mixture component generate document parameters distribution jc 
probability generating document independent class sum total probability mixture components jcj jc document comprised ordered sequence word events drawn vocabulary assume lengths documents evenly distributed independent class 
final assumption naive bayes assumption probability word event document independent word context position document 
document drawn multinomial distribution words independent trials number words write ik word position document subscript indicates index vocabulary 
probability document class jc jd jd ik jc mixture model parameterization composed disjoint sets parameters class parameters define multinomial distribution class 
class composed probabilities word jt jc jt jt 
parameters model class prior probabilities written 
set labeled training documents calculate bayes optimal estimates parameters model generated documents 
maximum likelihood estimates straightforward counting events supplemented simple smoothing primes word count count avoid probabilities zero vapnik define count number times word occurs document define jd document class label 
estimate probability word class jt jc jdj jd jv jv jdj jd class prior parameters set maximum likelihood estimate jdj jd jdj estimates parameters calculated training documents possible calculate probabilistic labels new unlabeled test document probability class component generated 
formulate applying bayes rule substituting jc equations 
jd jc jd ik jc jcj jd ik jc classify document simply select class largest jd predicted class document 
despite fact mixture model word independence assumptions strongly violated real world data naive bayes performs classification 
domingos pazzani discuss violation word independence assumption little damage classification accuracy domingos pazzani em incorporate unlabeled data naive bayes just small set labeled training data classification accuracy suffer variance parameter estimates generative model high 
augmenting small set large set unlabeled data combining pools em improve parameter estimates 
em concurrently generates probabilistic labels unlabeled documents probable model smaller parameter variance predicts probabilistic labels 
section describes em combine pools better parameter estimation probabilistic framework previous section 
em class iterative algorithms maximum likelihood estimation problems incomplete data dempster model data generation data missing values em converge set generative parameters locally maximizes likelihood labeled unlabeled data 
treating class labels unlabeled data missing values running em entire data set resulting parameter estimates give higher classification accuracy new documents pool labeled examples small nigam special case general missing values formulation ghahramani jordan practice em iterative step process 
step calculates probabilistic class labels jd unlabeled document current estimate equation 
step calculates new maximum likelihood estimate labeled data original probabilistically labeled equations 
initialize process parameter estimates just labeled training data iterate reaches fixed point 
resulting smaller variance parameter estimates documents forming estimates 
see nigam details 
active learning em estimating class labels unlabeled documents em active learning requests true class labels selected unlabeled documents 
optimally learner selects labels documents minimize classification error document distribution 
statistical learners equivalent minimizing classification variance 
section presents mechanics applying query committee active learning explains method selecting documents labeled describes integration active learning em 
query committee measures expected reductions classification variance indirectly creating set classifier variants committee members measuring disagreement committee members potential query 
theoretical empirical qbc freund dagan engelson committee members created sampling classifiers distribution classifier parameters specified training data 
parameters jt sampled normal distributions mean equation variance jc gamma jc total number word occurrences training data class jd 
sample times create committee members 
individual committee members denoted number documents selected class label requests 
dagan engelson sampling unlabeled documents produce stream document stream measure classification disagreement committee members vote entropy heuristically convert probability selecting document labeling 
vote entropy entropy class label distribution resulting having committee member deterministically vote winning class 
heuristic tuned 
obtaining labels documents active learning process repeats creating new committee members incorporating newly labeled documents requesting labels batches documents 
active learning completed final single classifier created variance perturbations testing new documents 
approach qbc differs dagan engelson ways select documents entire pool unlabeled documents 
tuning heuristic picking documents stream choose documents entire pool highest disagreement 
vote entropy measure committee disagreement document kullback leibler divergence mean pereira unlabeled documents probabilistically classified committee member resulting class distributions pm cjd committee member document vote entropy compares committee members top ranked class kl divergence measure strength certainty disagreement committee members calculating differences committee members class distributions pm cjd 
kl divergence mean average kl divergence distribution mean distributions pm cjd jjp avg cjd selecting document time computational convenience 
avg cjd class distribution mean committee members avg cjd pm cjd kl divergence delta information theoretic measure inefficiency resulting sending messages sampled distribution code optimal second 
kl divergence class distributions jjp gamma jcj log addition preferring documents differing committee classifications incorporate disagreement metric preference documents reduce classification variance documents prescribed theory cohn 
stream approach approximates implicitly stream produced sampling underlying distribution 
accomplish accurately especially labeling small number documents modeling document density explicitly 
approximate density estimation measuring document distance class centroid similarity metric proposed word occurrence probabilities dagan appropriate approximation assumption data generated mixture model density highest near centroids mixture components 
distance document class defined gammad jd jjp jc random variable words vocabulary jd maximum likelihood estimate words sampled uniformly document jd jd jc individual words equation 
explore density estimation independent class 
importance selecting unlabeled document labeling committee classification disagreement weighted distance centroid 
gamma jcj pm log avg pm ffl loop adding documents build initial estimate labeled documents 
equations loop times committee member create committee member sampling distributions mean variance indicated training data 
page starting sampled classifier apply em unlabeled data 
loop classifier parameters change delta current classifier probabilistically label unlabeled documents equation 
delta recalculate classifier parameters probabilistically assigned labels equations 
current classifier probabilistically label unlabeled documents equation calculate disagreement unlabeled documents equation request class labels documents important 
ffl build classifier equations 
ffl starting classifier apply em 
table active learning algorithm 
traditional query committee omits em steps indicated italics uses different scoring metric 
density weighted kl metric tend select document committee member thinks strongly prototypical different class 
experience vote entropy pool sampling document classification tends select outliers documents high committee disagreement simply short unusual 
combining active learning em active learning straightforwardly combined em running em convergence actively selecting training data labeled 
understood active learning select better starting point em hill climbing randomly selecting documents label starting point 
interesting approach pool leveraged sampling interleave em active learning em builds results active learning active learning informed em 
run em convergence committee member performing disagreement calculations 
intended effect avoid requesting labels examples label reliably filled em encourage selection examples help em find local maximum higher classification accuracy 
accurate committee members qbc pick informative documents label 
complete active learning algorithm em summarized table 
previous queries generated cohn previous unlabeled data available stream dagan engelson freund assumption availability pool unlabeled data leverage possible 
pool real world tasks efficient labels important especially text learning 
related approach qbc active learning em follows dagan engelson dagan engelson stream sampling vote entropy pool sampling density weighted kl 
studies investigated active learning text categorization 
lewis gale examine uncertainty sampling relevance sampling lewis gale lewis techniques select queries single classifier committee approximate classification variance reduction 
liere tadepalli committees winnow learners active text learning liere tadepalli select documents randomly selected committee members disagree class label 
text studies learn binary classifiers classes low frequency reuters results titles full documents 
previous show em unlabeled data reduces text classification error third nigam studies em combine labeled unlabeled data active learning classification non text tasks miller uyar shahshahani landgrebe ghahramani jordan em mixture models fill missing values ghahramani jordan experimental results section provides evidence combination active learning em better individually 
results data sets usenet newswires 
newsgroups data set collected ken lang contains articles evenly divided usenet discussion groups joachims confusable comp classes data set 
tokenizing data skip usenet headers discarding subject line tokens formed contiguous alphabetic characters 
best performance obtained feature selection stemming normalizing word counts document length 
resulting vocabulary removing words occur words 
trial documents randomly selected placement test set 
modapte train test split reuters distribution data set consists reuters newswire articles overlapping topic categories 
studies joachims liere tadepalli build binary classifiers populous classes 
ignore words stoplist stemming 
resulting vocabulary words 
results reported complete test set breakeven points standard information retrieval measure binary classification 
data sets initial classifier trained random document class 
active learning proceeds described table 
active learning iteration selects documents labeling original pool unlabeled training examples reuters documents pool newsgroups 
experiments run active learning iterations 
qbc committee size initial experiments show committee size little effect 
em runs perform em iterations classification accuracy improve seventh iteration 
results averages runs condition 
left hand graph shows comparison different selection metrics qbc em newsgroups data set 
best performer density weighted kl metric requiring example labelings data sets available internet 
see www cs cmu edu www research att com lewis 
number training documents density weighted kl kl divergence mean random vote entropy number training docs qbc em interleaved qbc em random em qbc random active learning newsgroups data 
left comparison disagreement metrics qbc shows considering density density weighted kl better metrics 
right combinations qbc em outperform stand qbc em 
faster learning rates random example selection 
cases qbc uses density weighted kl selection metric 
note resolution vertical axes range 
achieve accuracy 
best kl divergence mean weighting requiring labelings accuracy 
baseline comparison random selection equivalent traditional supervised learning needs labelings 
vote entropy performs slightly worse baseline needing labelings 
expected density weighted kl captures density information statistically significantly best performer 
vote entropy performs worst attempt capture density information strengths disagreement 
note previous uses vote entropy done stream sampling setting implicitly captures density information 
consider addition em learning scheme 
variants applied newsgroups data set 
em baseline post processes random selection runs em random em 
straightforward method combining em active learning run em active learning completes qbc em 
interleave em active learning running em committee member qbc em 
includes post processing run em 
qbc disagreement measured density weighted kl previous experiment indicated appropriate 
random selection random qbc em qbc repeated previous experiment 
right hand graph includes results combining em ac tive learning newsgroups data set 
expected random selection straight qbc give slowest learning rates labelings reach accuracy respectively 
random em improves especially random performance starts plateau needs labelings reach 
results consistent earlier results domain nigam qbc em impressive needing labelings 
interleaved qbc em marginally improves qbc em accuracy needing documents training data random labeled examples qbc labeled examples em 
qbc em qbc em statistically significantly better methods accuracies 
accuracy qbc em requires labeled examples random em qbc 
results indicate combination em active learning provides large benefit 
qbc interleaved em perform better qbc followed em expecting 
hypothesize discrepancy caused extreme classification probabilities assigned naive bayes 
due independence assumption naive bayes typically assigns probability near winning class probabilities near zero classes 
having small amounts training data tends curb effect em brings effect back incorporates lot unlabeled training data 
outlier documents selected higher disagreement weights qbc em qbc em 
try improved score probability mappings believe cause qbc em select documents fall close different centroids different runs em exactly documents labeled help em form correct clusters 
comparison previous active learning studies text classification domains lewis gale liere tadepalli magnitude classification accuracy increase relatively modest 
previous studies consider binary classifiers positive class small priors 
infrequent positive class random selection perform extremely poorly nearly documents selected labeling negative class 
tasks class priors random selection perform better making improvement active learning dramatic 
eye testing hypothesis perform subset previous experiments reuters data set characteristics number training documents qbc random number training documents qbc random number training documents qbc random active learning results categories reuters data corn trade acq respectively clockwise top left increasing order frequency 
note active learning committees outperforms random selection magnitude improvement larger infrequent classes 
skewed priors 
compare random qbc density weighted kl disagreement metric 
presents results binary classification tasks 
frequencies positive classes respectively left corn category right trade bottom acq graphs 
results representative spectrum active learning results class frequencies 
cases active learning results accurate classifiers random 
labelings improvements accuracy random corn trade acq 
distinct trend categories frequently occurring positive classes bigger improvements active learning 
conclude earlier results reasonable class priors random provides relatively strong baseline performance 
shows combination active learning em learning presence unlabeled data provides powerful improvement method 
paradigm pool leveraged selective sampling allows collection unlabeled examples explicitly incurring cost requesting labels 
em uses pool unlabeled examples create accurate classifiers 
new metric approximates density pool selecting documents better reduce classification variance 
results show combination em active learning pool sampling results factor reduction labelings 
ongoing explore methods converting extreme naive bayes probabilities values better match empirical probabilities correct 
plan comparison vote entropy kl divergence selection metrics stream sampling 
considering different density approximators class dependent parametric 
acknowledgments doug baker help formatting reuters data set 
research supported part darpa hpkb program contract 
cohn cohn ghahramani jordan 
active learning statistical models 
journal artificial intelligence research 
cohn david cohn 
neural network exploration optimal experiment design 
nips 
dagan engelson ido dagan sean engelson 
committee sampling training probabilistic classifiers 
icml 
dagan ido dagan fernando pereira lillian lee 
similarity estimation word cooccurrence probabilities 
nd annual meeting association computational linguistics 
dempster dempster laird rubin 
maximum likelihood incomplete data em 
algorithm 
journal royal statistical society series 
domingos pazzani domingos pazzani 
independence conditions optimality simple bayesian classifier 
machine learning 
freund freund seung shamir tishby 
selective sampling query committee algorithm 
machine learning 
ghahramani jordan zoubin ghahramani michael jordan 
supervised learning incomplete data em approach 
nips 
joachims thorsten joachims 
probabilistic analysis rocchio algorithm tfidf text categorization 
icml 
joachims thorsten joachims 
text categorization support vector machines learning relevant features 
ecml 
lewis gale lewis gale 
sequential algorithm training text classifiers 
proceedings acm sigir conference 
lewis david lewis 
sequential algorithm training text classifiers corrigendum additional data 
sigir forum 
liere tadepalli ray liere prasad tadepalli 
active learning committees text categorization 
aaai 
miller uyar david miller hasan uyar 
mixture experts classifier learning labelled unlabelled data 
advances neural information processing systems nips 
nigam kamal nigam andrew mccallum sebastian thrun tom mitchell 
learning classify text labeled unlabeled documents 
submitted aaai 
www cs cmu edu mccallum 
pereira fernando pereira naftali tishby lillian lee 
distributional clustering english words 
proceedings st annual meeting association computational linguistics 
shahshahani landgrebe shahshahani landgrebe 
effect unlabeled samples reducing small sample size problem mitigating hughes phenomenon 
ieee trans 
geoscience remote sensing sept 
vapnik vapnik 
estimations dependences statistical data 
springer publisher 

