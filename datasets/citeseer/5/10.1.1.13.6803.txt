exploiting unlabeled data ensemble methods kristin bennett department mathematical sciences rensselaer polytechnic institute troy ny rpi edu adaptive semi supervised ensemble method ble proposed constructs classification ensembles labeled unlabeled data 
assemble alternates assigning pseudo classes unlabeled data existing ensemble constructing base classifier labeled data 
mathematically intuitive algorithm corresponds maximizing classification margin hypothesis space measured labeled unlabeled data 
alternative approaches assemble require semi supervised learning method base classifier 
assemble conjunction cost sensitive classification algorithm class multi class problems 
assemble decision trees won nips unlabeled data competition 
addition strong results benchmark datasets decision trees neural networks support proposed method 
keywords boosting semi supervised learning ensemble learning classification 
practical classification applications areas image analysis drug discovery web pages analysis labeled data known class labels short supply unlabeled data unknown class labels readily available 
semi supervised learning deals methods exploiting unlabeled data addition labeled data improve performance classification task 
semi supervised learning topic different neural information processing workshops www rpi edu permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
sigkdd edmonton alberta ca copyright acm 
demiriz richard maclin business department department computer science hidden ridge university minnesota irving tx mn demiriz com umn edu 
existing approaches include semi supervised svm training mixture models 
examine problem ensemble perspective 
ensemble methods adaboost iteratively base learning mechanism construct classifier improve ensemble classifier adding classifier current ensemble appropriate scalar multiplier step size 
known algorithms performing gradient descent error function function space 
depending measure quality classifier different criteria produced choosing base classifier assigning step size 
buc showed error measures extended semi supervised learning resulting algorithm limited utility requires base learner semi supervised algorithm base classifiers may required acceptable performance uses small fixed step size 
goal produce adaptive semi supervised ensemble method cost sensitive base learner simple adaptive step size rule 
intuitive approach choose ensemble works consistently unlabeled data classification functions tended vote class unlabeled data 
form regularization prevent overfitting training data 
show intuitive idea equivalent maximizing margin function space labeled unlabeled data 
known classification boosting regarded maximizing measure margin functions space margin measures adopted semi supervised learning 
adopt marginboost notation strategy adapted margin measured labeled unlabeled data 
assemble analysis resulting algorithms different 
key difference assign unlabeled data 
pseudo classes assemble far practical powerful approach 
advantages assemble weight sensitive classification algorithm boosted labeled unlabeled data 
unlabeled data assimilated margin cost ensemble algorithms class multiclass problems 
assemble efficiently exploit adaptive stepsizes weight base learner existing supervised ensemble methods 
practically limited fixed step sizes 
assemble exploit unlabeled data reduced number classifiers needed ensemble speeding learning 
assemble works practice 
provides account winning assemble algorithm nips unlabeled data competition 
computational results show approach effective number test problems producing accurate ensembles adaboost number base learners 
remainder organized follows section examines margins labeled unlabeled data semi supervised version anyboost algorithm 
section presents winning assemble adaboost algorithm nips unlabeled data competition provides comparative results supervised adaboost contest datasets 
section semi supervised supervised ensembles neural networks decision trees studied benchmark datasets 
conclude discussion section 
margins unlabeled data semi supervised boosting order apply boosting datasets labeled unlabeled data come mechanism defining margin associated unlabeled data points 
strategy general applicable different margin cost functions focus adaboost 
base classifiers fj fj jth classifier ensemble 
labeled training data dimensional points 
know labels 

assume problem classes yi 
multi class extension discussed sections 
ensemble classifier formed linear combination base classifiers wj weighting term jth classifier 
labeled data points margin yif xi 
adaboost performs gradient descent function space order minimize exponential margin cost function labeled yif xi suppose set unlabeled data incorporate unlabeled data define margin unlabeled data point 
know class yi unlabeled data points 
note labeled data points margin yif xi positive point correctly classified negative point wrongly classified 
unlabeled point right wrong sense think correct 
define margin unlabeled data point xi xi allow margin supervised unsupervised data introduce concept pseudo class 
pseudo class unlabeled data point xi defined yi sign xi 
margin yif xi yi known class label xi labeled pseudo class xi unlabeled 
pseudo class critical difference approach independently developed 
introducing pseudo classes show adaptive semi supervised ensemble method assemble corresponds intuitive semi supervised ensemble algorithm maximizes margins labeled unlabeled points function space 
noted margin labeled points yif xi define margin unlabeled data points xi 
values define margin cost function incorporates labeled unlabeled data 
assemble cost function adaboost xi je xj labeled unlabeled general supervised case cost function cost function im yif xi typically number points allow different weights 
general assemble cost function margin cost function im yif xi xj labeled unlabeled terms weight labeled unlabeled data example choose weight margins associated unlabeled data points counting margins labeled data points 
create practical descent algorithm build anyboost approach 
recall anyboost algorithm algorithm 
anyboost algorithm 


ft ft ft 
ft ft 
return ft 

choose wt 
ft ft wt ft 

return ft ft represents ensemble classifier tth component classifier added 
maximum number classifiers propose include ensemble 
ft ft weak learning function applied existing ensemble classifier produces new possible classifier ft maximizes ft ft inner product new base classifier gradient cost function 
inner product negative adding ft decrease cost function algorithm terminates adding new component classifier 
add classifier ensemble weighting factor wt selected appropriate linesearch guarantees decrease cost function 
adding unlabeled points cost function im yif xi jm xi labeled unlabeled note function differentiable absolute value function differentiable 
pseudo class yi unlabeled data subgradient ifx xi 
yif xi xi number labeled unlabeled data points respectively yi class true labeled unlabeled derivative margin cost function respect get xi yif xi safely assume margin cost function monotonically decreasing positive 
maximizing equivalent finding maximizing im yif xi im yif xi unlabeled yj sign xj 
equivalently define vector misclassification costs allow base learner maximize constructing minimum cost classifier 
im yif xi define im yif xi 
base learner construct base classifier minimizing approximately minimizing weighted error xi note pseudo classes unlabeled data cost sensitive base learning algorithm 
resulting adaptive semi supervised ensemble algorithm algorithm 
assemble simplify presentation represent subgradient 
strictly speaking subgradient method gradient descent method 

select 
yi unlabeled 



ft dt 
dt xi 
return ft 

choose wt 
ft ft wt ft 
yi sign ft xi unlabeled 
dt im ift xi yj ft xj 

return ft dt base learning algorithm applied distribution data points current labels data points weighted current distribution dt 
assume base learning algorithm optimize misclassification costs ignoring points yi dt 
exact algorithm results depends choice cost function 
example assume choose cost function adaboost step algorithm dt je choice step size problematic determine step size wt step 
original adaboost algorithm stepsize performed exact linesearch adaboost cost functional exact step size closed form solution 
add unsupervised data wt chosen minimize labeled im yi ft xi wt ft xi unlabeled im ft xi wt ft xi 
unfortunately closed form solution exists wt 
possibility pick small fixed step size wt 
step size sufficiently small algorithm converge may converge slowly 
algorithm inefficient terms training time storage classification function prediction time base classifiers 
approach 
intuitive approach just step sizes supervised learning ensembles simply pseudo classes unlabeled data 
example choose wt minimize im yi ft xi wt ft xi easy show mathematically leads decrease decrease possible 
unlabeled point xi sign pseudo class yi sign ft xi correctly predicted new base classifier ensemble sign ft xi yj pseudo class cost function accurate ft xi wt ft xi yi ft xi wt ft xi unlabeled point incorrectly classified yi sign ft xi yi ft xi wt ft xi ft xi yi wt ft xi ft xi wt ft xi ft xi wt ft xi monotonically decreasing function yi ft xi wt ft xi ft xi wt ft xi pseudo class cost function provides upper bound true cost function 
choose wt strictly decrease pseudo cost function strictly decrease true cost function 
furthermore possible strictly decrease pseudo cost function problem optimal 
constant data point just step sizes developed supervised ensemble methods various costs see example 
variable minor modifications may required 
assemble variations possible construct variations basic assemble algorithm 
example employ ble multi class problems extend mechanism assigning pseudo classes unlabeled points weighted vote previous classifiers ensemble 
approach works letter recognition results see section 
example employ loss functions associated step sizes defined gradient classification algorithms exponential loss logistic regression 
add step estimate pseudo costs iteration 
sections examine adaboost adapted semi supervised learning 

assemble nips competition variation assemble algorithm semi supervised method competition nips workshop competition unlabeled data supervised learning organized stefan kremer deborah 
assemble previously known semi supervised boosting best algorithms participants utilizing unlabeled data 
assemble assimilate unlabeled data multiclass version adaboost 
adaboost adopted multiclass similar approach 
specifically ft xi instance xi correctly classified ft xi 
adaboost increase weight misclassified point decrease 
predicted class achieves majority vote weighted ensemble weights 
class case pseudo classes unlabeled data predicted classes 
keep training times similar adaboost assemble adaboost unlabeled labeled data sampled iteration size training set base learner equaled size labeled data 
usual adaptive step size adaboost class unlabeled points pseudo classes 
contest version assemble algorithm 
assemble adaboost 


yi class nearest neighbor point 


yi ft xi 

dt yi yi 


wt log 
ft ft 
yi ft xi 
dt adaboost equation 
sample dt 
ft dt 

return ft assemble adaboost algorithm practically inherits properties adaboost 
adaboost prone overfitting 
overfitting adaboost prevented maximizing margin function space labeled unlabeled data 
sample points available data assemble similar computational complexity adaboost 
nearest neighbor assignment assign initial pseudo class labels 
algorithm potentially applicable supervised learning method limited depth decision trees competition 
depth decision trees set level minimizes adaboost training error 
rationale decision trees show semisupervised approach simple classification method 
initial misclassification costs set skewed emphasize labeled data misclassification costs unlabeled labeled data assumed equal importance 
contest assemble adaboost implemented splus 
results competition summarized table 
originally datasets competition classification regression problems 
considered classification problems 
limitations splus run assemble boosting datasets 
separate labeled unlabeled table results assemble adaboost nips semi supervised competition 
acc test set accuracy impr percentage improvement assemble adaboost adaboost 
data dim 
points acc impr classes cp cp test data provided competition 
accuracy results test data improvements compared adaboost reported table 
number iterations adaboost ble adaboost general competition 
number iterations determined training error convergence supervised adaboost runs 
parameter set runs 
semi supervised algorithm produced double digit improvements relative adaboost 

benchmark experiments order assess performance algorithms outlined performed additional experiments number datasets drawn literature 
test effectiveness assemble different classifier methodologies performed experiments decision trees neural networks methods proven effective previous experiments ensembles :10.1.1.33.353:10.1.1.105.506
results dt experiments assemble adaboost decision trees tested benchmark datasets obtained boosting literature 
benchmark datasets data transformation 
dataset consists random instances training test dataset pairs 
detailed information datasets 
conform previous approaches empirical studies semisupervised learning methods left training data unlabeled data evaluated algorithm test data 
training data sampled times different levels form labeled unlabeled data 
specifically size combined labeled unlabeled data held constant proportion data treated unlabeled varied percent 
predictions test point 
assemble adaboost code comparison identical contest described section 
code implemented splus allow decision tree algorithm base learner 
set maximum depth decision trees generated 
information gain splitting criterion 
regression problems examined classification case 
number boosting iterations limited maximum 
set parameters experiments 
results experiments reported table 
report mean error rates adaboost adaboost different runs benchmark dataset 
unlabeled data sampled total data training 
results table demonstrate semi supervised boosting better comparable adaboost despite fact trial assemble adaboost received labeled data 
results nn experiments set experiments examined performance assemble neural networks 
ble variant neural network experiments slightly different nips contest 
algorithm differs step assemble adaboost dropped available data steps unlabeled points initially set class incorporated classifier 
changes largely significant cost nearest neighbor labeling larger datasets employed multiple tests 
employ parameter different sets data term weights margins unlabeled points fractions employed prevent networks overly focusing unlabeled points 
neural network experiments simple multilayer perceptrons single layer hidden units 
networks trained backpropagation learning rate momentum value 
datasets experiments breast cancer wisconsin pima indians diabetes letter recognition drawn uci machine learning repository 
number units hidden layer datasets breast cancer diabetes datasets letter recognition dataset 
number training epochs set diabetes letter recognition done previous ensemble experiments see 
explored number values ranging parameter weight unlabeled data points little difference values 
results show value parameter 
obtain results performed fold cross validation experiments result 
unlabeled data obtained randomly marking percentage data case percent data unlabeled points 
semi supervised regular adaboost experiment run set points unlabeled points dataset left applying adaboost 
method allowed produce members ensemble breast cancer table experiments decision trees dataset unlabeled adaboost assemble rate test set error std dev test set error std dev cancer cancer cancer banana banana banana diabetes diabetes diabetes table experiments assemble adaboost neural networks dataset unlabeled adaboost assemble rate test set error std dev test set error std dev wisconsin breast cancer wisconsin breast cancer wisconsin breast cancer pima indians diabetes pima indians diabetes pima indians diabetes letter recognition letter recognition letter recognition diabetes converged quickly stopped fewer classifiers 
table shows results neural networks described component classifiers variation assemble algorithm 
note case produced small measurable gain performance 
test effectiveness algorithm performed experiments largest datasets letter recognition assess useful unlabeled data overcoming limitations imposed bias classifier 
case greatly limited capabilities classifier decreasing number units hidden layer performed experiments hidden units 
problem output classes resulting neural networks perform generalization encoding decoding problem 
expect additional data help overcome problems 
shows resulting error rate graphed function number classifiers 
note assemble outperform standard adaboost resulting gains generally happen earlier large number classifiers ensemble adaboost begins catch 

introduce novel semi supervised learning technique assemble adaptive semi supervised ensemble able solve semi supervised learning problems problems data labeled class 
assemble cost sensitive classification method semi supervised incorporating semi supervised boosting algorithm 
key making assemble pseudo classes unlabeled data 
demonstrate appropriate margin derived attaching yi unlabeled data point simply reflects majority class picked point current ensemble 
choice pseudo class allows derive class boosting algorithms applied semi supervised learning situations 
incorporating pseudo classes existing ensemble algorithms readily adapted semisupervised learning 
focus ble adaboost semi supervised version popular adaboost algorithm exponential margin cost functions 
assemble performs extremely empirical tests class multi class problems 
decision trees component classifier placed algorithms nips competition semi supervised datasets 
empirical tests neural networks decision trees datasets data points artificially marked unlabeled assemble consistently performs better adaboost 
assemble algorithm appears robust method combining unlabeled data labeled data 
plan apply assemble larger datasets order determine algorithm scales larger problems 
assemble readily adaptable scalable boosting algorithms 
interesting open problem exploit unlabeled data regression problems 
classification assemble favors ensembles vote consistently unlabeled data 
analogous strategy regression favor ensembles exhibit low variance unlabeled data 
leave issues 
error error letter recognition number classifiers letter recognition error number classifiers letter recognition number classifiers semi sup semi sup semi sup semi sup semi sup semi sup semi sup semi sup semi sup neural network results letter recognition dataset networks hidden units 
results shown percent data marked unlabeled adaboost assemble semi sup 

acknowledgments partially supported nsf iri nsf iis nsf dms 
stefan kremer deborah organizing nips workshop 

bauer kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
bennett demiriz 
semi supervised support vector machines 
kearns solla editor advances neural information processing systems pages cambridge ma 
mit press 
blake merz 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
blum mitchell 
combining labeled unlabeled data training 
colt proceedings workshop computational learning theory morgan kaufmann publishers 
caruana de sa kearns mccallum 
integrating supervised unsupervised learning 
www cs cmu edu mccallum 
buc 
semi supervised marginboost 
dietterich becker ghahramani editors advances neural information processing systems 
mit press 
demiriz bennett embrechts 
genetic algorithm approach semi supervised clustering 
journal smart engineering system design 
taylor francis 
freund schapire 
experiments new boosting algorithm 
international conference machine learning pages 
fung mangasarian 
semi supervised support vector machines unlabeled data classification 
optimization methods software 
graepel herbrich obermayer 
unlabeled data supervised learning 
stat cs tu berlin de nips 
buc 
boosting mixture models semi supervised learning 
dorffner bischof hornik editors icann pages 
lncs springer verlag 
grove schuurmans 
boosting limit maximizing margin learned ensembles 
aaai iaai pages 
hastie tibshirani friedman 
elements statistical learning 
springer verlag new york 
kremer 
competition unlabeled data supervised learning 
cis ca nips 
kremer bennett 
unlabeled data supervised learning competition 
cis ca nips 
mason bartlett baxter frean 
functional gradient techniques combining hypotheses 
sch lkopf smola bartlett schuurmans editors advances large margin classifiers 
mit press 
nigam mccallum mitchell 
em classify test labeled unlabeled documents 
machine learning 
opitz maclin 
popular ensemble methods empirical study 
journal artificial intelligence research 
tsch 
benchmark datasets 
ida gmd de data benchmarks htm 
street kim 
ensemble method large scale classification 
seventh acm sigkdd international conference knowledge discovery data mining kdd 
atkinson 
recursive partitioning software february 
available www mayo edu hsr html 
vapnik 
statistical learning theory 
wiley new york 
