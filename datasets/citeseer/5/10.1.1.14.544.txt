transductive learning spectral graph partitioning thorsten joachims tj cs cornell edu cornell university department computer science upson hall ithaca ny usa new method transductive learning seen transductive version nearest neighbor classifier 
transductive learning methods training problem meaningful relaxation solved globally optimally spectral methods 
propose algorithm robustly achieves generalization performance trained ciently 
key advantage algorithm require additional heuristics avoid unbalanced splits 
furthermore show connection transductive support vector machines ective training algorithm arises special case 

applications examples prediction needed known training classifier 
kind prediction called transductive learning vapnik 
example task relevance feedback information retrieval 
relevance feedback users give positive negative examples kinds documents interested 
documents training examples rest collection test set 
goal generalize training examples find remaining documents collection match users information need 
transductive setting di erent regular inductive setting 
transductive setting learner observe examples test set potentially exploit structure distribution 
methods designed goal mind 
vapnik introduced transductive svms vapnik refined bennett joachims 
methods blum chawla multi way cuts kleinberg tardos 
related idea training blum mitchell exploits structure resulting redundant representations 
study approaches common problems 
particular focus training show undesirable biases require additional di cult control heuristics 
overcome problem propose motivate set design principles transductive learners 
principles introduce new transductive learning method viewed transductive version nearest neighbor knn rule 
key advantage require greedy search leads optimization problem solved ciently globally optimally spectral methods 
evaluate algorithm empirically benchmarks showing improved robust performance transductive svms 
furthermore show training emerges special case new algorithm performs substantially better original training algorithm 

transductive learning model setting transductive inference introduced vapnik see vapnik 
learning task defined fixed array points 
data point desired classification yn 
simplicity assume labels binary data points vectors training learner receives labels random subset training data points 
goal learner predict labels remaining test points accurately possible 
vapnik gives bounds deviation error rates observed training sample test sample vapnik 
bounds depend capacity hypothesis space measured example number di erent labelings learner potentially produce smaller smaller bound 
proceedings twentieth international conference machine learning icml washington dc 
idea structural risk minimization srm vapnik consider sequence learners nested hypothesis spaces capacity increases structure aligned learning task bound limits generalization error 
information exploit built structure 

principles designing transductive learner contrast inductive learning transductive learner analyze location data points particular test set 
transductive learner structure hypothesis space location test points help design hypothesis space 
imagine knew labels examples including training test examples 
call perfect classification 
gave examples inductive learner ind learning large training set reasonable assume ind learns accurate classifier 
example experience tell text classification svms typically achieve low prediction error training examples 
contains data points expect svm labels achieves low leave cross validation error err sv loo err loo unbiased estimate prediction error typically low variability 
help transductive learner access small subset labels 
assuming inductive learner ind achieves low prediction error trained full implies perfect classification test set highly terms leave error 
transductive learner trans uses labelings test set corresponding inductive learner trans low leave error transductive learner exclude perfect classification potentially excluding bad labelings 
suggests srm structure transductive learner trans yn err ind loo leading general principle defining transductive learner inductive learner 
transductive learner label test set postulate achieves low training error postulate corresponding inductive learner highly self consistent low leave 
initially phrased terms transductive svm vapnik joachims follows postulates joachims 
transductive svm labels test examples margin maximized 
large margin svm shown low leave error vapnik 
transductive learning algorithms transductive ridge regression chapelle blum chawla minimize leave error 
leave measure self consistency 
training algorithm blum mitchell maximizes consistency classifiers 
discussed detail section 
shows postulates define ective transductive learner 
consider case nearest neighbor knn odd 
knn rule leave error example majority nearest neighbors class 
knn define margin quantity knn ij knn wim ij reflects similarity similarity weighted knn rule leave oneout error 
upper bound leave error err knn loo 
computationally di cult find labeling test examples minimizes leave oneout error knn having low training error cient algorithms minimizing upper bound 
write optimization problem max knn ij knn wim positive negative matrix notation objective written equivalently ij ij knn wik nearest neighbors zero 
problem typically convex cient methods solution 
particular thought adjacency matrix graph vertices represent examples edges represent similarities see 

mincut example 
solution denote set examples vertices 
define cut bi partitioning graph 
undirected graph cut value cut ij sum edge weights cut 
maximum determined matrix entries ij minimizing ij cut maximizes 
undirected graphs maximizing subject equivalent finding mincut positive examples form source negative examples form sink 
mincut cut separates positive negative examples minimizing cut value 
connects transduction method blum chawla blum chawla 
mincut maxflow algorithms starting intuition separation positive negative examples put strongly connected examples class 
blum chawla discuss connection leave error case nn 
mincut algorithm intuitively appealing easily leads degenerate cuts 
consider graph line thickness indicates edge weight ij graph contains training examples labeled indicated 
nodes represent test examples 
split graph dominant clusters mincut leads degenerate solution just cuts positive example 
behavior due fact mincut minimizes sum weight balanced cuts potential edges cut 
sparser graph help example degenerate behavior occurs knn graphs correct partitioning wrong neighbors edges connecting training examples unlabeled examples 
practically case su ciently large numbers unlabeled examples 
consider undirected nn graph example neighbors correct class incorrect class 
positive training example example average edges 
unlabeled examples mincut return degenerate cut strongly clustered graph 
degenerate cuts fulfill postulate zero training error postulate high self consistency terms leave postulates specify transductive learner su ciently 
reasonable constraint put transductive solution postulate 
postulate averages examples average margin pos neg ratio expected value training test set 
postulate motivated perfect classification 
example average margin knn fulfill perfect classification distribution drawing training set uniform subsets 
mincut violates postulate pos neg ratio average margin 
particular training examples negative margins test examples large margin 
functions conceivable joachims 
blum chawla experiment di erent heuristics pre processing graph avoid degenerate cuts 
appears problems study 
transductive learning algorithms similar degeneracies 
example transductive svms joachims cotraining blum mitchell fraction positive examples test set fixed priori 
constraints problematic small training sets estimated pos neg ratio unreliable 
problem degenerate cuts avoided principled way 

normalized graph cuts constraints problem mincut traced objective function aims minimize sum edge weights cut 
number elements sum depends directly size cut sets 
particular number edges cut vertices side vertices side potentially cut 
st mincut objective inappropriate account dependency cut size 
natural way normalize cut size dividing objective 
minimizing sum weights optimization problem minimizes average weight cut 
max cut positive negative problem related hagen kahng 
traditional problem unsupervised constraints 
solving unconstrained known np hard shi malik 
cient methods spectrum graph exist give approximations solution hagen kahng 
generalize methods case constrained transduction 
denote laplacian graph adjacency matrix diagonal degree matrix ii ij require graph undirected symmetric positive semi definite 
dhillon ignoring constraints unsupervised optimization problem equivalently written min straightforward verify feasible point 
problem np hard minimum real relaxation min equal second eigenvalue corresponding eigenvector solution 
solution relaxed problem approximation solution known ective practice 
moving supervised problem propose include constraints adding quadratic penalty objective function 
min labeled example corresponding element equal positive negative examples zero test examples 
estimates number observed positive negative examples training data 
see estimates need precise 
parameter trades training error versus cut value diagonal cost matrix allows di erent misclassification costs example 
eigendecomposition laplacian introduce new parameter vector substitute eigenvector smallest eigenvalue laplacian constraint equivalent setting 
matrix eigenvectors eigenvalues smallest get equivalent optimization problem 
min defining cv cv cv objective function written term dropped constant 
argument problem minimized smallest eigenvalue 
identity matrix 
compute optimal value producing predicted value example 
value rank test examples threshold hard class assignment 
obvious choice threshold midpoint refined methods probably appropriate 

spectral graph transducer basic method computing supervised suggests algorithm transductive learning call spectral graph transducer sgt 
implementation available sgt joachims org 
input algorithm training labels weighted undirected graph adjacency matrix similarity weighted nearest neighbor graph ij sim knn sim knn step preprocesses graph done compute diagonal degree matrix ii ij compute laplacian compute normalized laplacian corresponding normalized cut criterion shi malik 
compute smallest eigenvalues eigenvectors store normalize spectrum graph replace eigenvalues monotonically increasing function 
ii see chapelle 
fixing spectrum graph way abstracts example di erent magnitudes edge weights focuses ranking smallest cuts 
steps done new training set estimate number positive negative training examples 
set positive negative examples 
give equal weight positive negative examples set cost positive negative training examples ii ii 
ii test examples 
compute cv cv cv find smallest eigenvalue equation 
compute predictions ranking 
threshold wrt 
get hard class assignments sign 

connection transductive svms argument shows relationship sgt tsvm 
consider tsvm described joachims 
hyperplanes passing origin tsvm optimizes min max diag diag positive negative 
analysis simplify problem adding constraint 
objective written scalar maximum achieved substituting solution objective shows value maximum ya shows simplified tsvm problem equivalent mincut graph balance cut fixed 
sgt removes need fixing exact cut size priori 

connection training training applied redundant representations training examples blum mitchell 
goal train classifiers representation predictions maximally consistent examples goal blum mitchell propose greedy algorithm iteratively labels examples current classifiers confident 
algorithm ratio predicted positive negative examples test set fixed priori avoid degenerate solutions 
training emerges special case sgt consider knn classifiers representations note see eq 
upper bound number inconsistent predictions 
maximize consistency apply sgt graph contains links knn representation links example knn representation 
connections approaches unlabeled data supervised learning exist 
related approach image segmentation described yu 
aim segment images higher level constraints 
di erence arrive constrained cut problems constraints homogeneous leading di erent technique solution 
spectrum laplacian considered chapelle belkin niyogi 
leading eigenvectors feature extraction design kernels 
addition chapelle normalization spectrum 
szummer jaakkola apply short random walks knn graph labeling test examples exploiting random walk cross cluster boundaries stay clusters szummer jaakkola 
interesting connection sgt normalized cut minimizes transition probability random walk cut meila shi 
lead connection generative modeling approach nigam label test example latent variable nigam 

experiments evaluate sgt performed experiments datasets report results 
datasets frequent categories reuters text classification collection setup joachims uci table 
macro averages datasets training sets size total number examples number features 
task sgt knn tsvm svm optdigits reuters isolet adult ionosphere tory datasets optdigits digit recognition isolet speech recognition ionosphere adult data representation produced john platt 
evaluate training connection webkb data blum mitchell tfidf weighting 
goal empirical evaluation threefold 
evaluate sgt transductive setting comparing inductive learning methods particular knn linear svm 
second compare existing transduction methods particular tsvm 
third evaluate robustly sgt performs di erent data sets parameter settings 
learning tasks knn sgt normalized laplacian cosine similarity measure 
probably suboptimal tasks results indicate reasonable choice 
furthermore equally ects knn sgt relative comparisons remain valid 
example zero cosine examples randomly connected nodes uniform weight 
sure performance di erences learning methods due bad choices parameters give conventional learning methods knn svm tsvm unfair advantage 
methods report results parameter setting best average performance test set 
sgt hand chose constant datasets 
choice building nearest neighbor graph discussed 
results reported averages stratified transductive samples substantial di erences significant respect distribution 
samples chosen contain positive example 
report error rate appropriate unstable exception tsvm adult isolet samples prohibitively expensive 
table 
error rate webkb course data averaged training sets size 
features sgt knn tsvm svm page link page link fair comparison unbalanced class ratios 
rank measures comparisons 
popular measure information retrieval precision recall curve summarize break point see 
joachims 
tasks multiple classes reuters optdigits isolet summarize performance reporting average classes macro averaging 
unlabeled data help improve prediction performance 
results summarized table 
tasks ionosphere sgt gives substantially improved prediction performance compared inductive methods 
sgt performs better knn inductive variant individual binary task reuters optdigits 
isolet sgt performs better knn binary tasks 
improvements tsvm typically smaller 
adult tsvm ine cient applied full dataset give results subsample size 
tsvm failed produce reasonable results isolet 
tsvm improve performance reuters improvement reported joachims 
assumption ratio positive negative examples test set known accurately 
typically case estimate training set 
true fraction tsvm achieves performance 
joachims proposes measures detect wrong fraction done running tsvm 
repeatedly trying di erent fractions prohibitively expensive 
ective sgt training 
table shows results training webkb 
built graph nn page nn links 
table compares cotraining setting just page links combined representation feature sets concatenated 
sgt training setting achieves highest performance 
tsvm gives large improvements compared inductive methods outperforming sgt tsvm take advantage training set number training examples adult ionosphere isolet optdigits reuters 
amount sgt exceeds optimal knn 
number eigenvectors adult ionosphere isolet optdigits reuters 
amount sgt lower particular number eigenvectors compared best tables 
ting 
results blum mitchell added column 
training set sizes transductive learning ective 
shows difference average sgt knn di erent training set sizes 
learning tasks performance improvement largest small training sets 
larger sets performance sgt approaches knn 
negative values largely due bias selecting parameters knn test set 
allowed sgt di erences vanish substantially smaller 
sensitive sgt choice number eigenvectors 
plots loss compared achieved test set optimal number eigenvectors 
tasks sgt achieves close optimal performance eigenvectors included 
conclude su cient tasks 
value adult ionosphere isolet optdigits reuters 
amount sgt lower particular value compared best number nearest neighbors adult ionosphere isolet optdigits reuters 
average average normalized value objective function vertically flipped positioned fit graph sgt depending number nearest neighbors tables 
sensitive sgt choice error parameter 
analogous plots loss compared best value due normalization spectrum laplacian optimum values comparable datasets 
tasks performance points away optimum 
exception isolet requires larger values 
conclude give reasonable performance tasks 
sensitive sgt choice graph 
choice building nearest neighbor graph strong influence performance 
top part shows average depending select 
small training set sizes positive example cross validation feasible 
value objective function interpreted measure capacity suitable model selection 
bottom half shows average value objective function normalization 
particular objective value ik training set choice normalized norm ik average normalized objective tracks performance curve suggesting interesting connection value capacity sgt experiments reported previous section value minimizes average normalized objective 
adult reuters optdigits isolet ionosphere training 
kind model selection particularly useful tasks relevance feedback information retrieval learning tasks examples collection objects 
ciently sgt trained 
due naive implementation time spent computing nn graph 
sped appropriate data structures inverted indices kd trees 
computing smallest eigenvalues takes approximately minutes task examples neighbors ghz cpu matlab 
preprocessing steps performed 
training particular training set predicting test examples takes second 

studied existing transductive learning methods abstracted principles problems 
introduced new transductive learning method seen transductive version knn classifier 
new method trained ciently spectral methods 
evaluated classifier variety test problems showing substantial improvements inductive methods small training sets 
algorithms unlabeled data need additional heuristics avoid unbalanced splits 
furthermore require greedy search robust existing methods outperforming tsvm tasks 
modeling learning problem graph ers large degree flexibility encoding prior knowledge relationship individual examples 
particular showed training arises special case new algorithm outperforms original training algorithm 
algorithm opens interesting areas research 
particular possible derive tight sample dependent capacity bounds cut value 
furthermore interesting consider settings cotraining modeled graph temporal drifts distribution training views 
research supported part nsf projects iis iis gift google 
lillian lee filip bo pang eric breck insightful comments 
belkin niyogi 

semi supervised learning manifolds 
nips 
bennett 

combining support vector mathematical programming methods classification 
scholkopf 
eds advances kernel methods support vector learning 
mit press 
blum chawla 

learning labeled unlabeled data graph mincut 
icml 
blum mitchell 

combining labeled unlabeled data training 
colt 
chapelle vapnik weston 

transductive inference estimating values functions 
nips 
chapelle weston 

cluster kernels semi supervised learning 
nips 
dhillon 

clustering documents words bipartite spectral graph partitioning 
kdd conference 
golub von matt 

constrained eigenvalue problem 
linear algebra applications 
hagen kahng 

new spectral methods ratio cut partitioning clustering 
ieee transactions cad 
joachims 

transductive inference text classification support vector machines 
icml 
joachims 

learning classify text support vector machines methods theory algorithms 
kluwer 
kleinberg tardos 

approximation algorithms classification problems pairwise relationships metric labeling markov random fields 
focs 
meila shi 

random walks view spectral segmentation 

nigam mccallum thrun mitchell 

text classification labeled unlabeled documents em 
machine learning 
shi malik 

normalized cuts image segmentation 
pami 
szummer jaakkola 

partially labeled classification markov random walks 
nips 
vapnik 

statistical learning theory 
wiley 
yu gross shi 

concurrent object recognition segmentation graph partitioning 
nips 
