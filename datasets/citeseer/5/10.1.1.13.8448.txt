approximate policy iteration policy language bias draft alan fern yoon robert givan electrical computer engineering purdue university lafayette explore approximate policy iteration api replacing usual learning step learning step policy space 
give policy language biases enable solution large relational markov decision processes mdps previous technique solve 
particular induce high quality domain specific planners classical planning domains deterministic stochastic variants solving domains extremely large mdps 
dynamic programming approaches finding optimal control policies markov decision processes mdps explicit flat state space representations break state space extremely large 
extends algorithms propositional relational state space representations :10.1.1.111.9112:10.1.1.158.8490
extensions shown capacity solve large classical planning problems benchmark problems planning competitions 
methods typically calculate sequence cost functions 
familiar strips adl planning domains useful cost functions difficult impossible represent compactly 
techniques guarantee certain accuracy stage 
focus inductive techniques guarantees 
existing inductive forms approximate policy iteration api select compactly represented approximate cost functions iteration dynamic programming suffering representation difficult 
know previous applies form api benchmark problems classical planning 
reason complexity typical cost functions problems natural specify policy space 
inductive policy selection relational planning domains shown useful policies learned policy space bias described generic knowledge representation language 
incorporate practical approach api strips adl planning domains 
replace cost function approximations policy representations api direct compact state action mappings standard relational learner learn mappings 
inherit familiar api methods sampled policy evaluation phase simulation current policy inductive policy selection relational reinforcement learning applied strips problems simpler goals typical benchmark planning domains discussed section 
learned lagoudakis parr pursuing similar policy bias approach api attribute value domains 
phase inducing approximate policy sampled current policy values 
evaluate api approach strips adl planning domains showing iterative policy improvement 
technique solves entire planning domains finding policy applied problem domain solving just single problem instance domain 
view planning domain single large mdp state specifies current world goal 
api method learns control knowledge policy planning domain 
api technique naturally leverages heuristic functions cost function estimates available allows benefit advances domain independent heuristics classical planning discussed 
greedy heuristic search solves essentially domain instances api technique successfully bootstraps heuristic guidance 
demonstrate technique able iteratively improve policies correspond previously published hand coded control knowledge tl plan policies learned yoon 
technique gives new way heuristics planning domains complementing traditional heuristic search strategies 
approximate policy iteration review api general action simulator mdp representation section detail particular representation planning domains relational mdps corresponding policy space learning bias 
problem setup 
follow adapt 
represent mdp generative model hs ii finite set states finite set actions randomized action simulation algorithm state action returns state component action cost function maps real numbers randomized initial state algorithm inputs returns state treat random variables 
mdp hs ii policy possibly stochastic mapping cost function cost function unique solutions representing expected cumulative discounted cost policy starting state discount factor 
seek heuristically minimize due complexity problems consider 
current policy define new improved policy pi argmin 
cost function pi guaranteed worse state improve state non optimal 
exact policy iteration iterates policy improvement pi initial policy reach optimal fixed point 
policy improvement divided steps computing policy evaluation computing selecting minimizing action policy selection 
approximate policy iteration 
api heuristically approximates policy iteration large state spaces approximate policy improvement operator trained simulation 
approximate operator performs policy evaluation simulation evaluating policy state drawing number sample trajectories starting performs policy selection constructing training set samples cost functions small representative set states training set induce new approximately improved policy 
api assumes states actions represented factored form typically feature vector facilitates generalizing properties training data entire state action spaces 
due api inductive nature typically policy improvement api converges usefully 
start api providing initial policy real valued heuristic function interpreted estimate cost state presumably respect optimal policy 
note may trivial returning constant random action respectively 
api effective important combine provide guidance improvement 
example goal planning domains occasionally reach goal provide non trivial information experiments consider scenarios different types initial policies heuristics bootstrap api 
mdp hs fa am ii api produces policy sequence iterating steps approximate policy improvement note initial iteration heuristic 
approximate policy improvement computes approximate improvement policy attempting approximate output exact policy improvement argmin 
steps estimating costs actions representative set states resulting data set learn approximation step cost estimation 
see construct training set describing improved policy consisting tuples hs am sampled state action term refers estimated drawing sampling width trajectories length horizon computing average discounted trajectory cost sampled trajectories cost trajectory includes value heuristic function horizon state 
get representative set states include state visited indicated estimates horizon steps training set size states drawn initial distribution 
step learn policy 
select goal minimizing cumulative cost approximating minimization exact policy iteration 
traditional api uses cost function space learning bias selection section detail policy space learning bias technique 
labeling training state associated costs action simply best action enable learner informed trade offs note inclusion training example enables learner normalize data desired learner see section uses bias focuses states large improvement appears possible 
show pseudo code variant api 
api relational planning order api framework represent classical planning domains just single instances relationally factored mdps 
describe compact relational policy language associated learner step api framework 
planning domains mdps 
say mdp hs ii relational defined giving finite sets objects predicates action types fact predicate applied appropriate number objects 
state set facts taken true state states 
action action type applied appropriate number objects action space set actions 
classical planning domain specified providing set world predicates action similar general easier representing cost functions encountered pi 
important knowledge novel states sampled match training distribution implied test set distribution 
concurrent takes approach 
api training set size sampling width horizon initial policy cost estimator heuristic function loop draw training set learn decision list satisfied change small return draw training set training set size sampling width horizon cost estimator current policy set states sampled state draw trajectory sample states policy action maximizing state sampled hs return policy computes estimate policy state sampling width horizon cost estimator initialize vector indexed actions zeroes action sample step state sampled return pseudo code api algorithm 
mdp hs ii assumed globally known 
general approach inherited restated clarity 
key differences learn decision list discussed section choice action draw training set see footnote 
types action simulator 
simultaneously solve problem instances size bound planning domain constructing relational mdp described 
fixed set objects set action types planning domain 
define mdp action space 
mdp state single problem instance initial state goal planning domain specifying current world goal 
achieve letting set world predicates classical domain new set goal predicates world predicate 
goal predicates named prepending corresponding world predicate 
mdp states sets world goal facts involving objects objective reach mdp states goal facts subset world facts goal states 
state fon table clear goal state blocks world mdp goal state clear 
represent objective defining assign zero cost actions taken goal states positive cost actions states 
addition take action simulator planning domain modified treat goal states terminal preserve change goal predicates 
cost function low cost policy arrive goal states quickly possible 
initial state distribution program example blocks world classical planning domain problem instance initial block configuration set goal conditions 
classical planners attempt find solutions specific problem instances domain 
generates legal problem instances mdp states classical planning domain problem generator planning competition 
taxonomic decision list policies 
adapt api method section step policy space language bias learning method previous learning policies relational domains small problem solutions briefly reviewed 
relational domains useful rules take form apply action type object set unload object destination 
introduced decision lists rules language bias learning policies 
lists represent sets objects needed class expressions written taxonomic syntax defined argument relation binary relation predicates argument relations denote set objects true denotes image objects class binary relation natural semantics constructs shown please refer 
state concept expressed taxonomic syntax straightforward compute set domain objects represented order execute policy 
restricting attention argument action types write policy hc cn taxonomic syntax concepts action types 
see yoon examples details 
training example hs am define advantage action state 

take heuristic value concept action rule number training examples rule fires plus cumulative advantage rule achieves training examples yoon describes straightforward beam search method incrementally building decision list size bounded rules attempt maximize heuristic value 
note advantage cost focuses learner instances large improvement previous policy possible 
relational planning experiments experiments support number claims 
guidance weak domain independent heuristic api learns effective policies entire classical planning domains 
learned policy domain specific planner fast empirically compares state art domain plan :10.1.1.26.673
api improve previously published control knowledge learned previous systems 
domains 
consider deterministic domains standard definitions stochastic domains yoon bw block blocks world lw location truck package logistics world stochastic variant bw stochastic logistics world cars trucks spw version paint action 
draw problem instances domain generating pairs random initial goal states 
domain independent ff plan heuristic experiment domains involving multiple argument actions converted argument action domains adding actions :10.1.1.26.673
conversion may problem difficult solve 
extending language multiple argument actions practical motivation 
coverage term included covering example zero advantage covering example contributing zero 
zero advantage thing previous policy optimal state 
domain definitions www ece purdue edu givan nips domains html 
space precludes description complex studied planning heuristic 
iteration bw sr iteration bw sr iteration lw sr bootstrapping api domain independent heuristic 
iteration tl bw bw sr iteration tl bw bw sr iteration tl lw lw sr tl plan control knowledge initial policies 
iteration spw policy sr policy sr iteration policy sr policy sr iteration policy sr policy sr previously learned initial policies 
specifies planning domain initial policy iterates api progress 
evaluate policy random problem instances recording success ratio sr fraction problems solved horizon normalized average solution length average plan length successful trials divided horizon omitting low sr initial policy performance plotted iteration zero 
bootstrapping heuristic 
consider domain independent initial policy ff greedy acts ff heuristic step look ahead 
figures show sr api iteration bw bw 
ff greedy poor domains 
initial period apparent progress followed rapid improvement nearly perfect sr examination learned bw policies shows early iterations find important concepts iterations find policy achieves small sr point rapid improvement ensues 
shows sr lw 
ff greedy performs api yields compact declarative policies quality ff greedy 
replicated experiments stochastic variants domains similar results shown space reasons 
initial hand coded policies 
tl plan uses human coded domain specific control knowledge solve classical planning problems 
initial policies api correspond domain specific control knowledge appearing blocks discount factor select sufficiently large horizons rank policies accurately 
training set size trajectories sampling width 
considered domain independent means constructing policy 
able exactly capture tl plan knowledge policy language 
write policies capture knowledge prune away bad actions tl plan world tl plan provides sets control knowledge increasing quality best second best sets get policies tl bw tl bw respectively 
logistics set knowledge yielding policy tl lw 
figures show sr api starting tl bw tl bw tl lw lw 
case api improves human coded policies 
starting tl bw tl lw perfect sr api uncovers policies maintain sr improve approximately respectively 
starting tl bw sr api quickly uncovers policies perfect sr dramatic difference quality ff greedy iteration tl bw bw initial policy api finds policies roughly identical quality requiring iterations lower quality initial policies 
initial machine learned policies 
yoon policies learned solutions randomly drawn small problems stochastic domains test 
significant range policy qualities results due random draw 
api starting average policies figures show results spw 
domain api shown improve sr arbitrarily selected average learned starting policies nearly 
api successfully exploits previous noisy learning robustly obtain policy 
table ff plan vs learned policies 
ff plan api scheme domains sr time sr time bw bw bw bw lw lw comparing learned policies ff plan 
selected blocks world policy logistics world policy corresponding learned policies iteration figures best sr breaking ties applied ff plan appropriate selected policy new test problems domains shown table 
planning cutoff times set seconds bw bw domains respectively 
table records percent problems solved time cutoff sr average length successful trials average time successful trials time ff plan selected policies 
blocks worlds blocks api policy improves ff plan category scaling better blocks 
heuristic information different way api uncovers policies significantly outperform ff plan 
ff plan heuristic suited logistics worlds eliminating search problems 
method performs equivalently slow prototype scheme implementation 
related typically previous learning planning systems learn small problem solutions improve efficiency quality planning 
primary approaches learn control knowledge search planners closely related learn stand control policies 
severely limited utility problem see swamped low utility rules 
critically policy language bias confronts issue preferring simpler policies 
regarding novel api iteratively improve policies leads robust learner shown 
addition leverage consider tl plan policies better corresponding tl plan policy 
stochastic domains provide heuristic designed deterministic domains deterministic strips domain approximation outcome action 
domain independent planning heuristic avoid need access small problems 
learning approach tied having base planner 
closely related relational reinforcement learning rrl form online api learns relational cost function approximations 
cost functions learned form relational decision trees trees learn corresponding policies trees 
rrl results clearly demonstrate difficulty learning cost function approximations relational domains 
compared trees trees tend generalize poorly larger 
rrl demonstrated scalability problems complex considered previous rrl blocks world experiments include relatively simple goals lead cost functions complex ones 
rrl api assumes unconstrained simulator ff plan heuristic world model provided learned additional techniques 
ricardo daniel borrajo pedro 
genetic programming learn improve control knowledge 
aij 
fahiem bacchus 
aips planning competition 
ai magazine 
fahiem bacchus kabanza 
temporal logics express search control knowledge planning 
aij 
bellman 
dynamic programming 
princeton university press 
bertsekas tsitsiklis 
neuro dynamic programming 
athena scientific 
craig boutilier richard dearden 
approximating value trees structured dynamic programming 
saitta editor icml 
craig boutilier richard dearden moises goldszmidt 
stochastic dynamic programming factored representations 
aij 
craig boutilier raymond reiter bob price :10.1.1.111.9112
symbolic dynamic programming firstorder mdps 
ijcai 
dzeroski driessens 
relational reinforcement learning 
mlj 
tara estlin raymond mooney 
multi strategy learning search control planning 
aaai 
robert givan thomas dean matt greig 
equivalence notions model minimization markov decision processes 
aij 
carlos guestrin daphne koller ronald parr 
max norm projections factored mdps 
ijcai pages 
jorg hoffmann bernhard nebel :10.1.1.26.673
ff planning system fast plan generation heuristic search 
jair 
howard 
dynamic programming markov decision processes 
mit press 
yi cheng huang bart selman henry kautz 
learning declarative control rules constraint planning 
icml pages 
michael kearns mansour andrew ng 
sparse sampling algorithm nearoptimal planning large markov decision processes 
mlj 
roni khardon 
learning action strategies planning domains 
aij 
lagoudakis parr 
reinforcement learning classification leveraging large margin classifiers 
icml submitted 
mario martin hector geffner 
learning generalized policies planning domains concept languages 
krr 
mcallester givan 
taxonomic syntax st order inference 
jacm 
minton 
quantitative results utility explanation learning 
aaai 
minton editor 
machine learning methods planning 
morgan kaufmann 
minton carbonell knoblock kuokka etzioni gil 
learning problem solving perspective 
aij 
tesauro 
practical issues temporal difference learning 
mlj 
tesauro 
online policy improvement monte carlo search 
nips 
tsitsiklis van roy 
feature methods large scale dp 
mlj 
veloso carbonell perez borrajo fink blythe 
integrating planning learning prodigy architecture 
journal experimental theoretical ai 
yoon fern givan 
inductive policy selection order mdps 
uai 
complex blocks world goal rrl achieve block environment 
consider blocks world goals involve blocks 
