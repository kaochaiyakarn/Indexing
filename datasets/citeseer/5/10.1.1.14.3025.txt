pairwise preference learning ranking johannes rnkranz austrian research institute artificial intelligence wien austria informatics institute marburg university hans str marburg germany mathematik uni marburg de technical report tr consider supervised learning ranking function mapping instances total orders set labels options 
training information consists examples partial possibly inconsistent information associated rankings 
induce ranking function reducing original problem number binary classification problems pair labels 
main objective investigate trade quality induced ranking function computational complexity algorithm depending amount preference information example 
theoretical results complexity pairwise preference learning 
carry controlled experiments investigating predictive performance method different types preference information top ranked labels complete rankings 
domain study prediction rational agent ranking actions uncertain environment 
increasing trend treat consumers computer users patients individuals produced things user adapted software operating systems horvitz commerce personalization products services systems patient centered medical care couch 
key prerequisite applications ability discovering capturing individual preferences problem referred preference elicitation 
consider acquisition preferences context supervised learning 
roughly speaking means generalize examples preference structure valued function function assigns preference structures instances computer users customers patients 
problem obviously seen extension learning classification function referred preference learning 
distinguished preference elicitation narrow sense goal learn preferences single individual specific questions asked individual 
problem learning preferences received lot attention machine learning literature 
problem particularly challenging involves prediction complex structures weak partial order relations single values 
training input usually case offered form complete examples may comprise general types information relative preferences different kinds indirect feedback 
specifically learning scenario consider consists collection training examples associated finite set decision alternatives 
common notation supervised learning shall refer labels 
contrary standard classification training example assigned single label set pairwise preferences labels expressing label preferred 
goal pairwise preferences predicting total order ranking possible labels new training example 
generally seek induce ranking function maps instances examples rankings fixed set decision alternatives labels analogy classification function maps instances single labels 
investigate round robin learning pairwise classification 
seen round robin appears particularly appealing context extended classification preference learning quite natural manner 
organized follows section introduce learning problem formal way 
extension pairwise classification pairwise preference learning application ranking discussed section 
section provides results computational complexity pairwise preference learning 
results experimental studies investigating predictive major problem ask questions clever way find approximation individual preference structure small possible number questions 
performance approach various training conditions section 
conclude overview related section complementary final remarks section 
learning problem consider learning problem set labels 
set examples ek 
training example ek set preferences pk pk indicates label preferred label example ek 
find function orders labels 
example 
abbreviate pk particular example ek doesn matter clear context 
setting previously introduced constraint classification har peled 

pointed framework generalization common learning settings particular see formal derivation results ranking training example associated total order labels pair labels holds 
classification single class label assigned example 
implicitly defines set preferences 
multi label classification training example ek associated subset sk possible labels 
implicitly defines set preferences 
pointed interested predicting ranking total order labels 
assume instance exists total order labels form transitive asymmetric relation 
practical applications assumption appears acceptable true preferences 
observed revealed preferences incomplete inconsistent 
require data consistent sense transitivity asymmetry applies pk 
fact property compulsory learning algorithm 
reasonable assumption pk irreflexive anti symmetric 
note pk consequence properties 
pairwise preference ranking key idea approach learn separate theory pairwise preferences labels 
formally possible pair labels learn model mij decides example holds 
model trained examples ek known 
examples known preference ignored 
classification time example submitted theories prediction interpreted vote label 
classifier mij predicts count vote conversely prediction considered vote labels ranked number votes receive models mij 
ties broken frequency labels top rank class distribution classification setting randomly 
refer technique pairwise preference ranking round robin ranking 
straight forward generalization pairwise classification aka round robin learning solves multi class problems learning separate theory pair classes 
previous rnkranz showed rule learning algorithms technique preferable commonly classification method learns theory class examples class positive examples negative examples 
round robin successfully fields particular area support vector machines hsu lin 
refer section rnkranz brief survey related pairwise classification 
importantly rnkranz showed despite complexity quadratic number classes algorithm slower conventional technique 
generalize results section 
complexity consider learning problem training examples labels 
theorem total number training examples binary preference learning problems pk max pk proof training examples added pk binary training sets correspond preferences 
total number training examples pk 
number preferences example bounded maxk pk number larger maxk pk turn bounded size complete set preferences nc 
immediately follows result rnkranz corollary classification problem total number training examples linear number classes 
proof class label expands preferences pk note considered number training examples complexity learner runs examples 
algorithm linear run time complexity follows immediately total run time dn maximum average number preferences training example 
learner super linear complexity total run time lower dn training effort spent large training set small training sets 
particular complete preference set total complexity complexity round robin classification cn rnkranz 
comparison technique learning setting know har peled constructs twice training examples positive negative preference example examples projected space times attributes original space 
examples put single training set separating hyper plane learned 
reasonable assumption increase number features approximately effect corresponding increase number examples total complexity cdn algorithm finding separating hyper plane complexity class training set size summary complexity pairwise constraint classification depends maximum average number preferences training example 
quadratic number labels complete ranking linear classification setting 
case efficient technique proposed har peled 

noted price pay large number classifiers stored tested classification time 
empirical results previous sections shown extended version round robin learning induce ranking function set preferences single label 
turned computational complexity issue 
especially ranking induces quadratic number pairwise preferences complexity round robin ranking quadratic number labels 
context ask possible improve efficiency cost tolerable decrease performance learning process ignore preferences decreasing predictive accuracy 
apart incomplete training data clearly point practical relevance complete rankings rarely observable 
experimental evaluation section meant investigate issues related incomplete training data detail especially increase understanding trade number pairwise preferences available training data quality learned ranking function 
systematic investigation questions kind need data principle complete ranking known example 
information allows systematic variation amount preference information training data precise evaluation predicted rankings test data 
aware suitable real world datasets decided conduct experiments synthetic data 
synthetic data consider problem learning ranking function expected utility maximizing agent 
specifically proceed standard setting ex pected utility theory 
ac set actions agent choose 
set world states 
agent faces problem decision risk decision consequences lotteries choosing act ai state yields utility uij probability state pj 
expected utility act ai ai pj uij 
expected utility theory justifies criterion ranking actions gives rise preference relation ai aj ai aj 
suppose probability vector 
pm parameter decision problem utility matrix matrix uij fixed 
denote ranking actions induced vector 
decision theoretic setting generating synthetic data preference learning 
set instances corresponds set probability vectors generated random uniform distribution rm 
pm 
ranking function associated example ek ranking ek defined 
experiment characterized parameters number actions labels number world states number examples utility matrix generated random independent uniformly distributed entries uij 
experimental setup report results experiments different states various numbers labels 
configurations generated different data sets originating different randomly chosen utility matrix data sets consisted training test examples 
example data sets provided probability vector complete ranking possible actions 
training examples labeled subset complete set pairwise preferences imposed ranking data set 
subsets selected experiments described experiments 
occurrence actions equal expected utility probability 
decision tree learner quinlan default settings learn model pairwise preference 
instances test set obtained final ranking simple voting tie breaking described section 
predicted ranks compared actual ranks test set evaluation measures computed follows denote 
true top ranked label action 
likewise ranking test example ek denote 
predicted ranking label assigned top rank 
rk denote true rank label example ek 
evaluation metrics computed error percentage examples top rank incorrect average deviation average average absolute deviation predicted rank true rank cn rk maximum deviation average maximum absolute deviations predicted rank true rank example max rk correlation average spearman rank correlation coefficient rk note coefficient assumes values reversed rankings identical rankings 
choice learner solely versatility wide availability 
aimed maximizing performance particular problem resort algorithms directly represent separating hyperplanes binary preference 
table comparison ranking complete set preferences vs classification preferences top rank 
shown results complementary setting preferences top rank omitted 
error avg dev 
max dev 
rank corr 
ranking classification complement ranking classification complement ranking classification complement ranking vs classification shows experimental results cases pairwise preferences selected follows full set pairwise preferences 
second classification setting uses preferences involve top label 
third complementary setting uses preferences involve top label 
interesting things note results 
difference error rates classification ranking setting comparably small 
interested top rank may suffice pairwise preferences involve top label 
advantage case course reduced complexity linear number labels 
hand results show complete ranking information improve classification accuracy information available training example willing pay price quadratic complexity 
results complementary setting show information top rank preferences crucial dropping information pairwise preferences involve top label error rate top rank increases considerably higher error rate classification noted special top rank 
expect type results observed focus arbitrary rank bottom rank median rank 
setting 
bit surprising consider classification setting average number training examples learning model mij smaller complementary setting 
interestingly effective number training examples top labels decrease 
fact learning scenario dominating actions utility degrees systematically larger actions 
worst case action optimal probability vectors complementary set contain information 
situation course extreme class distribution unbalanced scenario 
example determined experimentally probability having optimal action half examples expected gini index class distribution 
respect prediction complete rankings performance learning complementary set preferences performance learning complete set preferences performance ranking induced classification setting considerably worse 
time result hardly surprising easily explained amount information provided cases 
fact complementary set determines ranking label top label hardly provide information complete ranking 
interesting finding note classification accuracy decreases increasing number labels rank correlation increases revealed curves 
words quality predicted rankings increases quality predictions individual ranks decreases 
effect explained fact classification error affected increase number labels 
illustration consider random guessing chances guessing top label correctly expected value rank correlation regardless speculate importance correct vote individual learner mij decreases increasing number labels 
roughly speaking incorrect classifications individual learners better compensated average 
conjecture supported independent experiment simulated set homogeneous learners mij biased coin flipping prespecified error rate 
turned quality measures predicted rankings tend increase number labels large dependence measures number labels necessarily monotone see fig 

gives intuitive support interpretation round robin learning ensemble learning technique rnkranz 
expected spearman rank correlation function number labels learners mij error rate curves shown 
missing preferences previous results shed light trade utility costs special types preference information top ranked labels complete rankings give satisfactory answer general case 
selected set preferences classification setting strongly focused particular label example resulting biased distribution 
look quality predicted rankings selecting subsets pairwise preferences full sets equal right 
shows curves classification error top rank average spearman rank correlation predicted true ranking number preferences 
generate curves started full set preferences ignored increasingly larger numbers 
implemented parameter pi caused preference training data ignored probability pi pi plotted axis 
similar shape curves labels suggests decrease ranking quality attributed solely missing preferences independent number labels 
particular average error rate left spearman rank correlation right various percentages ignored preferences 
error bars indicate standard deviations 
vertical dotted lines right indicate number preferences classification problems classes left complementary sizes 
inclined conclude contrary case focused top rank general possible reduce number training preferences order magnitude quadratic linear number labels severely decreasing ranking quality 
seen dotted vertical lines right 
lines indicate percentage preferences classification setting labels inner outer 
comparison error rates intersection line corresponding curve respective error rates shows extreme difference coincidental selection pairwise preferences systematic selection focused top rank 
see half preferences ignored maintaining reasonable performance level 
quite common learning curves concave functions size training set descent accuracy appears remarkably flat case 
tempted attribute redundancy pairwise preferences induced ranking principle ranking reconstructed preferences 
means small fraction pairwise preferences needed 
careful explanation 
trying reconstruct single ranking solve slightly different problem learn ranking function 
second learning algorithm reconstruct ranking suggested 
fact simple voting procedure take dependencies individual learners mij account means learners really cooperate 
contrary voting procedure exploits just redundancy preference information top rank winner preferred pairwise comparisons 
note shape curves probably depends number training examples 
investigated issue mainly interested possibility reducing complexity constant factor losing predictive accuracy 
interesting example compare training examples full preferences training examples pairwise preferences 
mislabeled preferences recall learning scenario assumes preference structures complete rankings labels transitive asymmetric relations 
pointed assumption observed preferences may access complete sets preferences case studied previous section 
second process generating preferences reproduce underly average spearman rank correlation various percentages random preferences 
error bars indicate standard deviations 
solid thin lines curves ignored preferences 
ing total order incorrectly produce inconsistent preferences 
problem quite common example case human judgments 
simulate behavior adopted model proceeding pairwise preferences induced ranking preference kept probability ps probability ps preferences selected coin flip 
approximately ps cases preference point wrong direction 
ps data remain unchanged preferences training data completely random ps 
shows average spearman rank correlations observed experiment 
note shape curve shape curves ignored preferences 
possible directly compare curves graphs level means preferences intact 
main difference remaining preferences ignored re fact implemented procedure selecting ps preferences reversing sign 
assigned random 
facilitate comparison plotted curves ignored preferences ones graph solid thin lines 
interesting see cases performance degrades slowly albeit somewhat steeper examples completely ignored 
roughly speaking completely omitting pairwise preference appears better including random preference 
reasonably explained learning behavior classifier mij mij perform additional correct example probably classified correctly improve mij slightly decision tree induction example mij remain completely unchanged new example classified correctly 
opposed incorrect example probably classified incorrectly produce far reaching modification mij decision tree induction erroneous example produce completely different tree 
expected benefit mij caused random preference negative preference simply ignored 
consideration may conclude pairwise preference better ignored confident coin flip 
grasped intuitively preference provide information case 
confident clearly carries information better include best way action depend number reliability preferences available 
note experiments suggest strategy deciding include individual preference information uncertainty preference 
case preference equally uncertain 
reasonable strategies include ignore complete sample 
course strategy better soon probability correctness exceeds confirmed experimental results 
example correlation coefficient remains visibly preferences assigned chance probability particular preference correct 
may conjecture pairwise preference ranking particularly robust noise erroneous example affects single classifier mij turn limited influence eventually predicted ranking 
related pointed especially relevant framework constraint classification introduced extension standard classification har peled 

learning method proposed constructs training examples preference original dimensional training examples mapped cd dimensional space 
positive example copies original training vector components 
di negation components 
dj vector new space 
remaining entries filled negative example elements reversed signs 
cd dimensional space learner tries find separating hyperplane 
classifying new example labels ordered response resulting multiplying th element section hyperplane 
technique compares favorably approach 
ranking algorithms 
example crammer singer consider variety line learning algorithms problem ranking possible labels multi label text categorization task 
aware uses complete ranking available labels example training evaluation brazdil 
investigate meta learning task ranking learning algorithms suitability new dataset characteristics dataset 
authors investigated problem preference elicitation narrow sense learning single preference function 
example cohen 
propose step approach ranking set objects set labels associated objects approach feedback form preference judgments 
similarly haddawy 
assume training data available form pairwise comparisons objects 
data train artificial neural network takes input objects outputs depending object preferred second 
somewhat similar approach proposed wang 
joachims analyzes click data order rank documents retrieved search engine relevance 
nice example kind indirect preference information 
information learning retrieval function accomplished training support vector machine 
problem learning preference relation set labels approached somewhat indirect way learning value utility function assigns utility degree label 
note preference relation induced utility function necessarily complete linear sense tuples labels assumed comparable 
note learning utility function considered difficult problem learning linear preference relation subsumes vice versa 
depending underlying utility scale distinguish learning numeric function learning function maps ordinal ordered categorical scale 
cases involve respectively problem standard regression ordinal regression called ordinal classification 
ordinal regression investigated thoroughly statistics econometrics mccullagh nelder received attention machine learning 
example method ordinal regression modification regression tree learning proposed kramer 

frank hall suggest method translating ordinal regression problem set ordinary binary classification problems 
ordinal regression approached context support vector machines special type loss function suitable comparing predictions ordered categorical scale 
problem learning eliciting real valued utility functions investigated fields decision theory economics long time topic research ai machine learning 
particularly elegant approach due tesauro proposes symmetric network architecture trained representations states training signal indicates states preferable 
elegance comparison training approach comes property replace symmetric parts network single network subsequently provide real valued evaluation single states 

simplify elicitation utility functions clustering exemplary utility functions deriving prototypes clusters inducing decision tree inner nodes associated properties utility functions questions asked person leaf nodes identified prototypes 
idea 
simplify elicitation exploiting additive independence variables 
database exemplary utility functions statistical learning model selection methods order induce factorization utility functions additive functions 

accomplish learning utility function treating utility random variable 
starting prior distribution derived analyzing database available utility functions model incrementally updated information elicited user 
order decide questions asked user authors fall back principle underlying value information 

study problem learning utility function determines behavior agent rational sense expected utility theory 
approach proposed authors proceeds prior probability distribution class utility functions having certain linear structure 
agent decisions defining constraints true utility function see ng russell quite similar approach 
constraints employed order turn prior distribution class utility functions posterior distribution 
learning preferences key topic recommender systems collaborative filtering goldberg resnick varian kautz 
methods proposed field closer learning utility functions specifically adjusted commercial applications set alternatives labels recommended usually large 
method choice quite case memory approach basic idea estimate user preferences preferences users appear similar see ha haddawy nakamura abe billsus pazzani 
concluding remarks introduced pairwise preference learning extension pairwise classification constraint classification learning scenario training examples labeled preference relation possible labels single class label conventional classification setting 
information learn model pair classes focus learning complete ranking labels predicting label 
main interest investigate trade ranking quality amount training information terms number preferences available example 
experimentally investigated trade varying parameters synthetic domain simulates decision theoretic agent ranks possible actions unknown utility function 
roughly speaking results show large parts information pairwise preferences ignored round robin ranking losing predictive performance 
classification setting interested predicting top label turned full ranking information restricting pairwise preferences involving top label improve classification accuracy suggesting lower ranks contain valuable information 
reasons efficiency advisable concentrate smaller set preferences reducing size training set raises order magnitude 
main limitation technique probably assumption having training examples learning pairwise preference 
data large number labels small set preferences example technique hardly applicable 
particular successful collaborative filtering problems goldberg resnick varian breese mapped constraint classification framework straightforward way 
limitation quadratic number theories stored memory evaluated classification time 
increase memory requirements balanced increase computational efficiency comparison technique har peled 

addition pairwise preference learning inherits advantages pairwise classification particular implementation easily parallelized reduction independent subproblems 
directions 
prediction rankings improved combining individual learners votes sophisticated way 
authors looked sophisticated ways combining predictions pairwise theories final ranking available options 
proposals include weighting predicted preferences classifiers confidences rnkranz iterative algorithm combining pairwise probability estimates hastie tibshirani 
previous works evaluated techniques ranking context elaborate proposals error correcting output decoding allwein organizing pairwise classifiers tree structure platt stacked classifier rnkranz specifically tailored classification setting 
account fact explicitly seeking ranking lead promising alternatives 
example thinking selecting ranking minimizes number predicted preferences need reversed order predicted relation transitive 
departing counting votes offer possibilities extending method prediction preference structures general rankings total orders weak preference relations labels comparable 
apart theoretical considerations important aspect concerns practical application method evaluation real world problems 
unfortunately real world data sets fit framework quite rare 
fact currently aware data set significant size provides instances attribute value representation plus associated complete ranking limited number labels 
acknowledgments johannes rnkranz supported apart austrian academy sciences 
austrian research institute artificial intelligence supported austrian federal ministry education science culture 
allwein schapire singer 
reducing multiclass binary unifying approach margin classifiers 
journal machine learning research 
billsus pazzani 
learning collaborative information filters 
proceedings th international conference machine learning icml pages 
morgan kaufmann 
brazdil soares da costa 
ranking learning algorithms ibl meta learning accuracy time results 
machine learning march 
breese heckerman kadie 
empirical analysis predictive algorithms collaborative filtering 
cooper moral editors proceedings th conference uncertainty artificial intelligence uai pages madison wi 
morgan kaufmann 
getoor norman shahar 
utility elicitation classification problem 
cooper moral editors proceedings th conference uncertainty ai uai pages 
koller ormoneit 
learning agent utility function observing behavior 
proceedings th international conference machine learning icml pages 
koller parr 
making rational decisions adaptive utility elicitation 
proceedings th national conference artificial intelligence aaai pages 
koller 
discovering structure utility functions additive conditionally additive independence properties utility attributes 
proceedings st annual meeting society medical decision making mdm 
cohen schapire singer 
learning order things 
journal artificial intelligence research 
couch 
disease management overview 
couch editor health care professional guide disease management patient centered care st century 
aspen publishers 
crammer singer 
family additive online algorithms category ranking 
journal machine learning research 
frank hall 
simple approach ordinal classification 
raedt flach editors proceedings th european conference machine learning ecml pages freiburg germany 
springer verlag 
rnkranz 
round robin classification 
journal machine learning research 
rnkranz 
round robin ensembles 
intelligent data analysis 
appear 
goldberg nichols oki terry 
collaborative filtering weave information tapestry 
communications acm december 
ha haddawy 
similarity personal preferences theoretical foundations empirical analysis 
artificial intelligence 
appear 
haddawy ha geisler miyamoto 
preference elicitation theory refinement 
journal machine learning research 
appear 
har peled roth 
constraint classification new approach multiclass classification 
cesa bianchi reischuk editors proceedings th international conference algorithmic learning theory alt pages beck germany 
springer 
hastie tibshirani 
classification pairwise coupling 
jordan kearns solla editors advances neural information processing systems nips pages 
mit press 
graepel obermayer 
large margin rank boundaries ordinal regression 
smola sch lkopf schuurmans editors advances large margin classifiers pages 
mit press 
horvitz breese heckerman 
lumiere project bayesian user modeling inferring goals needs software users 
cooper moral editors proceedings th conference uncertainty ai uai pages 

hsu 
lin 
comparison methods multi class support vector machines 
ieee transactions neural networks march 
joachims 
optimizing search engines clickthrough data 
kdd proceedings acm conference knowledge discovery data mining 
kautz editor 
recommender systems papers aaai workshop menlo park ca 
aaai press 
technical report ws 
kramer widmer pfahringer 
prediction ordinal classes regression trees 
fundamenta informaticae xxi 
mccullagh nelder 
generalized linear models 
chapman hall london 
nakamura abe 
collaborative filtering weighted majority prediction algorithms 
proceedings th international conference machine learning icml pages 
morgan kaufmann 
ng russell 
algorithms inverse reinforcement learning 
proceedings th international conference machine learning icml 
platt cristianini shawe taylor 
large margin dags multiclass classification 
solla leen 
ller editors advances neural information processing systems nips pages 
mit press 
quinlan 
programs machine learning 
morgan kaufmann san mateo ca 
resnick varian 
special issue recommender systems 
communications acm 

views personalization 
communications acm 
rnkranz 
combining pairwise classifiers stacking 
submitted publication 
tesauro 
connectionist learning expert preferences comparison training 
touretzky editor advances neural information processing systems nips pages 
morgan kaufmann 
wang 
artificial neural networks versus natural neural networks connectionist paradigm preference assessment 
decision support systems 

