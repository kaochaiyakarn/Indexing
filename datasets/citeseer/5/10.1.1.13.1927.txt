boosting metaphor algorithm design kevin leyton brown eugene galen andrew jim mcfadden yoav shoham shoham cs stanford edu stanford university stanford ca 
hard computational problems solvable multiple algorithms perform different problem instances 
describe techniques building algorithm portfolio outperform constituent algorithms just aggregate classifiers learned boosting outperform classifiers composed 
provide method generating test distributions focus algorithm design problems hard existing portfolio 
demonstrate effectiveness techniques combinatorial auction winner determination problem showing portfolio outperforms state art algorithm factor 
algorithms better average rarely best algorithm problem 
case different algorithms perform different problem instances 
surprisingly phenomenon pronounced algorithms solving np hard problems runtimes algorithms highly variable instance instance 
algorithms exhibit high runtime variance faced problem deciding algorithm rice dubbed algorithm selection problem 
nearly decades followed issue algorithm selection failed receive widespread study course excellent exist 
far common approach algorithm selection measure different algorithms performance problem distribution algorithm having lowest average runtime 
approach refer winner driven advances algorithm design refinement resulted neglect algorithms average offer excellent performance particular problem instances 
consideration algorithm selection literature dissatisfaction winner take approach led ask questions 
general techniques perform instance distribution algorithm selection 
second rejected notion winner take algorithm evaluation ought novel algorithms evaluated 
idea boosting machine learning guiding metaphor strive answer questions 
previously published page extended 
boosting metaphor boosting machine learning paradigm due schapire widely studied 
technical results boosting literature takes inspiration boosting philosophy 
stated simply boosting insights 
poor classifiers combined form accurate ensemble classifiers areas effectiveness sufficiently uncorrelated 

new classifiers trained problems current aggregate classifier performs poorly 
argue algorithm design informed analogous ideas 
algorithms high average running times combined form algorithm portfolio low average running time algorithms easy inputs sufficiently uncorrelated 

new algorithm design focus problems current algorithm portfolio performs poorly 
course analogy boosting imperfect discuss differences section 
case study combinatorial auctions weighted set packing discuss effectiveness algorithm design methodology necessary perform case study 
chose consider combinatorial auction winner determination problem wdp runtime prediction techniques runtime data previous 
emphasized techniques propose particular problem domain 
full version consider domains particular positive initial results building portfolios sat 
combinatorial auctions provide general framework allocation problems self interested agents allowing bids bundles goods 
wdp weighted set packing problem spp goal choose non conflicting subset bids maximizing seller revenue 
spp np complete constant factor cf 

number goods number bids 
bid pair 
set goods requested bid bid price offer 
wdp formulated integer program maximize subject consider algorithms solving wdp ilog cplex package gl simple branch bound algorithm cplex lp solver heuristic cass complex branch bound algorithm non lp heuristic 
unfortunately unable get access cabob wdp algorithm :10.1.1.21.1090
overview sections give general methods boosting analogy algorithm design 
section methodology constructing algorithm portfolios show results case study 
go section offer practical extensions methodology including techniques avoiding computation costly features trading accuracy hard easy instances building models runtime data capped maximum running time 
section consider empirical evaluation portfolios describe method learned model runtime generate test distribution hard portfolio 
similar techniques generate instances score highly realism metric 
section discusses design choices compares choices related 
algorithm portfolios previous demonstrated statistical regression learn surprisingly accurate algorithm specific models empirical hardness distributions problem instances 
short method proposed 
domain knowledge select features problem instances indicative runtime 

generate set problem instances distribution collect runtime data algorithm instance 

regression learn real valued function features predicts runtime 
existing technique predicting runtime propose building portfolios multiple algorithms follows 
train model algorithm described 

instance compute feature values predict algorithm running time runtime models run algorithm predicted fastest technique powerful deceptively simple 
discussion comparison approaches literature please see section 
demonstrate case study portfolios dramatically outperform algorithms composed 
matching scheduling regions arbitrary percentage class runtime order magnitude fig 

gross hardness cplex fig 

mean absolute error case study experimental setup performed case study data collected past recap briefly section 
results focused problems fixed size numbers goods non dominated bids held constant respectively 
instance distribution involved making uniform choice distributions combinatorial auction test suite cats randomly choosing parameters instance 
complete dataset composed instances 
instance collected runtime data cplex computed features fall roughly categories 
norms linear programming slack vector integrality lp relaxation ip 
deviations prices 
node statistics bid bipartite graph 
various statistics bid graph effectively problem constraint graph data collected mhz pentium xeon machines running linux years cpu time spent gathering data 
fig 
shows histogram distribution hard instances dataset 
observe cplex runtime varied orders magnitude number goods bids held constant 
considerable variation distributions 
quadratic regression able build accurate models logarithm runtime 
fig 
shows histogram mean absolute error predicting log cplex runtime observed test set instances 
methodology relies separate research effort process extending models variable problem size models available possible extend techniques modification 
machine learning split data training validation test sets 
report portfolio runtimes test set train evaluate models 
error predicting log means runtime mispredicted factor roughly instance misclassified bins fig 
observe nearly prediction errors 
cplex portfolio optimal cplex portfolio optimal gl cass cplex time gl cass cplex time fig 

algorithm portfolio runtimes cplex fig 

optimal cplex fig 

selected case study results turn new results 
described section build regression models new algorithms gl cass 
fig 
compares average runtimes algorithms cplex cass gl portfolio note cplex chosen winner take algorithm selection 
optimal bar shows performance ideal portfolio algorithm selection performed perfectly overhead 
portfolio bar shows time taken compute features light portion time taken run selected algorithm dark portion 
despite fact cass gl slower cplex average portfolio outperforms cplex roughly factor 
neglecting note change scale graph repeated cplex bar cost computing features portfolio selections take longer run optimal selections 
figs 
show frequency algorithm selected ideal portfolio portfolio 
illustrate quality algorithm selection relative value algorithms 
observe portfolio right choice particular selects gl 
mistakes models occur algorithms similar running times mistakes costly explaining portfolio choices running time close optimal 
results show portfolio methodology small number algorithms algorithm average performance considerably better 
suspect techniques better settings 
extending portfolio methodology demonstrated algorithm portfolios offer significant speedups winner take algorithm selection worthwhile consider modifications methodology useful practice 
specifically describe methods reducing amount time spent computing features transforming response variable capping runs algorithms 
smart feature computation feature values computed portfolio choose algorithm run 
expect portfolios useful combine algorithms having high runtime variance fast polynomial time features sufficient models 
instances computation individual features may take substantially longer algorithms take run 
cases desirable perform algorithm selection spending time computing features expense accuracy choosing fastest algorithm 
order achieve partition features sets ordered time complexity 
implying feature takes significantly longer compute feature portfolio start computing easiest features iteratively compute set expected benefit selection exceeds cost computation 
precisely 
set learn provide model estimates time required compute 
simple average time scaled input size 

divide training examples sets 
set train models 
predicting algorithm runtime features note model algorithm basic portfolio methodology 
portfolio selects argmin assume features low runtime variance 
assumption hold case study 
feature runtime variance difficult partition features time complexity sets smart feature computation difficult 

second training set learn models 
predicting difference runtime algorithms selected second set avoid training difference models data runtime models fit 
instance portfolio works follows 
compute features continue 
return algorithm predicted fastest transforming response variable average runtime obvious measure portfolio performance goal minimize computation time large number instances 
models minimize root mean squared error appropriately penalize seconds error equally instances take second hours run 
reasonable goal may perform instance regardless hardness case relative error appropriate 
portfolio runtime optimal runtime respectively instance number instances 
measure gives insight portfolio relative error percent optimal measure relative error average percent suboptimal logarithm runtime simple way equalize importance relative error easy hard instances 
models predict log runtime help improve average percent suboptimal albeit expense terms portfolio average runtime 
show different functions linear identity log extreme values clearly functions fall 
functions normalized maximum value affect regression allows better visualize effect 
case study section cube root function particularly effective 
capping runs methodology section requires gathering runtime data algorithm problem instance training set 
time cost step fundamentally unavoidable approach gathering perfect data instance take unreasonably long time 
example algorithm usually slower cases dramatically outperforms perfect model runtime hard instances may needed discriminate algorithms 
process gathering data easier capping runtime algorithm maximum recording runs having terminated 
approach safe chosen significantly greater minimum algorithms runtimes preferable sacrifice predictive accuracy dramatically reduced model building time 
note algorithm capped dangerous particularly log transformation gather data algorithm capping time portfolio inappropriately select algorithm smaller 
case study results fig 
shows performance smart feature computation discussed section upper part bar indicating time spent computing features 
compared computing features reduce overhead half nearly cost running time 
runtime max transformation linear cube root log fig 

transformation ns normalized fig 

smart features average suboptimal table 
portfolio results table shows effect response variable transformations average runtime percent optimal average percent suboptimal 
row results obtained perfect portfolio 
discussed section linear identity transformation yields best average runtime log function leads better algorithm selection 
tried transformation functions linear log 
show best cube root nearly average runtime performance linear choices nearly accurately log 
focused algorithm design decided select existing algorithms portfolio approach necessary reexamine way design evaluate algorithms 
purpose designing new algorithms reduce time take solve problems designers aim produce new algorithms complement existing portfolio 
essential choose distribution reflects problems encountered practice 
portfolio greatest opportunity improvement instances hard portfolio common 
precisely importance region problem space proportional amount time current portfolio spends working instances region 
analogous principle boosting new classifiers trained instances hard existing ensemble proportion occur original training set 
inducing hard distributions model portfolio runtime instance features constructed minimum models constitute portfolio 
normalizing reinterpret model density function argument generate instances product distribution original distribution problematic sample may non analytic instance generator depends features evaluated instance created 
way sample rejection sampling generate problems keep probability proportional method works best distribution available guide sampling process hard instances 
test distributions usually tunable parameters hardness instances generated parameter values vary widely somewhat predictive hardness 
generate instances way 
create hardness model features normalize create pdf 
generate large number instances 
construct distribution instances assigning instance probability proportional hp select instance sampling distribution 
observe turns helpful hard instances encountered quickly 
worst case directs search away hard true rejection sampling step generate single instance accepted rejected step 
technique approximates process doesn require normalize hf allows output instance generating constant number samples 
instances observe sample correct distribution weights divided 
practice may factored distribution unrelated instance generators different parameter spaces distribution parameters chosen instance generator case difficult learn single solution factor hardness model choice instance generator feature hardness model instance generator parameter space 
likewise single feature space hardness model train separate model generator normalize pdf goal generate instances distribution done follows 
instance generator create hardness model features normalize create pdf 
construct distribution instance generators probability generator proportional average hardness instances generated 
generate large number instances select generator sampling select parameters generator sampling run generator chosen parameters generate instance 

construct distribution instances assigning instance generator probability proportional hg hp select instance sampling distribution 
inducing realistic distributions important portfolio methodology realistic distribution accurately reflecting sorts problems expected occur practice 
care taken construct generator set generators producing instances representative problems target domain 
possible construct function scores realism generated instance features instance function encode additional information nature realistic data easily expressed generator 
function provided construct parameterized set instance generators place learning way learned allow informed choices setting parameters instance generators discard realistic data generated 
note inducing hard distributions hardness model infeasible score sample actual portfolio runtime 
case inducing realistic distributions longer problem realism function evaluated sample 
rejection sampling technique guaranteed generate instances increased average realism scores 
parameter space models improve performance reducing number samples needed obtaining results 
case study results figs 
hardness models hf trained dataset models trained individual distributions 
learning new models probably yield better results 
fig 

inducing harder distributions fig 

matching fig 

scheduling case study results due wide spread runtimes composite distribution orders magnitude high accuracy model quite easy technique generate harder instances 
results fig 

runtime data capped way know hardest instances new distribution harder hardest instances original distribution note easy instances generated 
instances induced distribution came predominantly cats arbitrary distribution rest 
demonstrate technique works challenging settings sought different distribution small runtime variance 
happens ongoing discussion wdp literature cats distributions relatively easy configured harder see 
consider easy distributions low variance cats matching scheduling show harder originally proposed 
figures show histograms runtimes ideal portfolio technique applied 
fact distributions generated instances respectively times harder previously seen 
average runtime new distributions greater observed maximum running time original distribution 
discussion related helpful analogy boosting clearly perfect 
key difference lies way components aggregated classifiers combined majority voting point algorithm selection run single algorithm 
advocate learned models runtime basis algorithm selection leads important difference 
easy problems multiple algorithms uncorrelated models accurate reliably recommend slower algorithms uncorrelated instances 
impossible improve correctly classifying instance possible solve problem instance quickly 
improvement possible easy instances hard instances analogy boosting holds sense focusing hard regions problem space increases potential gain terms reduced average portfolio runtimes 
algorithm selection long understood algorithm performance vary substantially different classes problems 
rice formalize algorithm selection computational problem framing terms function approximation 
broadly identified goal selecting mapping space instances space algorithms maximize performance measure perf 
rice offered concrete techniques subsequent algorithm selection seen falling framework 
explain choice methodology relating approaches algorithm selection proposed literature 
parallel execution tempting alternative portfolios select single algorithm parallel execution available algorithms 
true additional processors readily available case processors put uses running different algorithms parallel parallelizing single search algorithm solving multiple problem instances time 
meaningful comparisons running time parallel non parallel portfolios require computational resources fixed parallel execution modelled ideal overhead task swapping single processor 
time takes run algorithm fastest instance number algorithms 
portfolio executes algorithms parallel instance take time nt 
data case study parallel execution roughly average runtime winner take algorithm selection algorithms cplex times slower optimal portfolio techniques better achieving running times roughly 
domains parallel execution effective technique 
gomes selman proposed approach incomplete sat algorithms term portfolio describe set algorithms run parallel 
domain runtime depends heavily variables random seed making runtime difficult predict parallel execution outperform portfolio chooses single algorithm 
cases possible extend methodology allow parallel execution 
add new algorithms portfolio algorithm standing placeholder parallel execution original algorithms training data running time times minimum constituents 
approach allow portfolios choose task swap sets algorithms parts feature space minimums individual algorithms runtimes smaller means choose single algorithms parts feature space 
term portfolio may seen extension term coined gomes selman referring set algorithms strategy selecting subset parallel execution 
classification algorithm selection fundamentally discriminative entails choosing algorithm exhibit minimal runtime classification obvious approach consider 
standard classification algorithm decision tree learn algorithm choose features instance labelled training examples 
problem classification algorithms wrong error metric penalize misclassifications equally regardless cost 
want minimize portfolio average runtime accuracy choosing optimal algorithm 
penalize misclassifications difference runtimes chosen fastest algorithms large small 
just happens decision criterion select smallest prediction set regression models fit minimize root mean squared error 
second classification approach entails dividing running times bins predicting bin contains algorithm runtime choosing best algorithm 
example horvitz classification predict runtime csp sat solvers inherently high runtime variance heavy tails 
despite similarity portfolio methodology approach suffers classification algorithm predict runtime 
learning algorithm error function penalizes large misclassifications bin heavily small misclassifications bin 
second approach unable discriminate algorithms multiple predictions fall bin 
runtime continuous variable class boundaries artificial 
instances runtimes lying close boundary misclassified accurate model making accurate models harder learn 
markov decision processes related lagoudakis littman 
worked mdp framework concentrated recursive algorithms sorting sat sequentially solving algorithm selection problem subproblem 
demonstrates encouraging results generality limited factors 
algorithm selection stage recursive algorithm require extensive recoding may simply impossible black box commercial proprietary algorithms competitive 
second solving algorithm selection problem recursively requires value functions inexpensive compute case study computationally expensive features required accurate predictions run time 
techniques undermined non markovian algorithms clause learning taboo lists forms dynamic programming 
course approach described mdp framework action choice algorithm leading terminal state reward equal negative runtime 
optimal policy selection trivial value function key success value estimation 
approach emphasizes making value functions models runtime explicit provides best defense fragile policies 
describe models mdps framework redundant absence sequential decision making 
different regression approaches tre select simple branch bound algorithms prediction running time 
similar spirit prediction single feature works particular class branch bound algorithms 
goal discriminate algorithms appropriate learn models pairwise differences algorithm runtimes models absolute runtimes 
linear regression forms nonlinear regression easy show approaches mathematically equivalent 
inducing hard distributions widely recognized choice test distribution important algorithm development 
absence general techniques generating instances realistic hard development new distributions usually performed manually 
excellent example selman 
describes method generating sat instances near phase transition threshold known hard sat solvers 
just boosting allows weak classifiers effectively algorithms combined portfolios build greater sum parts 
described build portfolios 
techniques elaborated reduce cost computing features reduce time spent gathering training data capping runs strike right balance penalties easy hard instances 
second argued algorithm design focus problem instances portfolio existing algorithms spends time 
provided techniques inducing distributions refining distributions emphasize instances high scores realism function 
performed case study combinatorial auctions showed portfolio composed cplex older generally slower algorithms outperformed cplex factor 
aim perform case studies methodology hard problems effort direction portfolio algorithms entered sat competition 
acknowledgments ryan porter carla gomes bart selman helpful discussions 
supported darpa intelligent information systems institute cornell stanford graduate fellowship 

doucet de freitas gordon ed 
sequential monte carlo methods practice 
springer verlag 

leyton brown shoham 
taming computational complexity combinatorial auctions optimal approximate approaches 
ijcai 

gomes selman 
algorithm portfolios 
artificial intelligence 

lehmann 
optimal solutions multi unit combinatorial auctions branch bound heuristics 
acm conference electronic commerce 

lehmann 
linear programming helps solving large multi unit combinatorial auctions 
technical report tr leibniz center research computer science april 

horvitz ruan gomes kautz selman chickering 
bayesian approach tackling hard computational problems 
uai 

lagoudakis littman 
algorithm selection reinforcement learning 
icml 

lagoudakis littman 
learning select branching rules dpll procedure satisfiability 
lics sat 

leyton brown andrew mcfadden shoham 
portfolio approach algorithm selection 
ijcai 

leyton brown shoham 
learning empirical hardness optimization problems case combinatorial auctions 
cp 

leyton brown pearson shoham 
universal test suite combinatorial auction algorithms 
acm ec 

tre 
branch bound algorithm selection performance prediction 
aaai 

rice 
algorithm selection problem 
advances computers 

ruan horvitz kautz 
restart policies dependence runs dynamic programming approach 
cp 

sandholm 
algorithm optimal winner determination combinatorial auctions 
ijcai 

sandholm suri levine 
cabob fast optimal algorithm combinatorial auctions 
ijcai 

schapire 
strength weak learnability 
machine learning 

selman mitchell levesque 
generating hard satisfiability problems 
artificial intelligence 
