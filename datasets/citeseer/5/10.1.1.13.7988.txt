discriminative clustering text documents jaakko sinkkonen samuel kaski helsinki university technology neural networks research centre box fin hut finland vector space distributional methods text document clustering discussed 
discriminative clustering proposed method uses external data find characteristics documents clustering defined external data 
introduce distributional version discriminative clustering represents text documents probability distributions 
methods tested task clustering scientific document abstracts ability methods predict independent topical classification abstracts compared 
discriminative methods topically meaningful clusters vector space distributional clustering models 

clustering texts smaller number homogeneous groups useful mining exploration summarization text document collections preprocessing information retrieval 
word order disregarded computational reasons texts considered bags words finite length multinomial samples 
topical content documents identified underlying multinomial distributions 
goal clustering find homogeneous data subsets homogeneity measured crucial 
texts measure differences relevant topical content 
traditional solution compile lists irrelevant words weight remaining words estimated importance 
question addressed latent semantic indexing lsi 
probabilistic version lsi included comparisons 
vector spaces choosing measure homogeneity equivalent choosing feature selection distance measure metric 
method allows clusters constructed terms primary data cluster homogeneity measured data clusters :10.1.1.28.7256
suitable task relevant auxiliary data available indirectly define homogeneity degree circumvent feature selection problem 
apply discriminative clustering method scientific texts 
auxiliary data keywords document authors 
assuming chosen signify relevant full text 
method introduced vectorial data 
extend distributions arguably accurate representations textual documents 
results experimentally compared standard vector space distributional probabilistic clustering methods 
test ability various methods discover topically homogeneous clusters predict known independent topical classification documents 

document clustering exist numerous clustering algorithms focus partitional clustering divides data space number partitions 
text may assigned cluster generally membership function may give degree document belongs cluster membership functions satisfy 
review widely applied partitional text clustering methods promising newer ones discriminative clustering methods section 

mixture model vector space salton introduced vector space model vsm information retrieval field 
text documents represented points vector space 
word corresponds dimension space coordinate document dimension determined number occurrences word document 
document similarity measured angle inner product document vectors 
mixture density model applicable vsm 
assume document produced generators soft clusters 
data density modeled mixture xjm probability cluster parameters optimized maximizing model likelihood cluster memberships jx computed bayes rule 
vsm normalized documents lie hypersphere appropriate density estimator mixture von mises fisher kernels hypersphere analogs gaussians xj exp governs spread kernel normalizes proper density 
apply term weighting simply normalized unit length 

distributional clustering occurrence data information bottleneck information bottleneck method clustering documents documents converted distributional form lk lk lr soft partitioning documents set clusters sought minimizing cost function motivated information theory dis kl represent cluster memberships form probabilities summing unity kl kullback leibler divergence document prototype denotes mutual information generated document clusters regarded random variable documents :10.1.1.39.9882
variational optimization leads clusters general form quite similar leaving prototypes prior probabilities clusters fitted data 
parameter chooses compromise cluster smoothness minimization average distortion 
implement related method 

mixture models occurrence data generative probabilistic model called asymmetric clustering model acm closely related distributional clustering method :10.1.1.34.97:10.1.1.34.97
shown obtaining maximum likelihood solution acm equivalent minimizing acm kl empirical frequency document acm document probabilistically assigned clusters cluster memberships priori unknown :10.1.1.34.97
alternative directly model occurrence patterns words documents 
separable mixture model smm called probabilistic lsi occurrence probability word document modeled ju ju denotes cluster probability cluster probabilities parameters optimized maximizing likelihood em algorithm :10.1.1.34.97
smm designed clustering method decomposes occurrences probabilistically factors 
clustering regarding factors clusters 
cluster probabilities document computed bayes rule 

discriminative clustering clustering principle aims implicitly find optimal way measure data similarity 
call principle discriminative clustering incorporates discriminative elements clustering task 
general clustering aims maximize cluster similarity homogeneity 
discriminative clustering applicable primary samples paired discrete auxiliary labels 
auxiliary data supposed canonical indicator important variation primary data 
inhomogeneities primary data noted associated variation conditional auxiliary distributions 
homogeneity measure cluster similarity auxiliary data distributions 
clusters defined terms primary data 
auxiliary data guides optimization 
clustering new samples clustered auxiliary data 
previously vector space clustering algorithm denote vector space discriminative clustering :10.1.1.28.7256
general purpose clustering method applied texts vector form works practice 
simplifying assumptions account distributional nature texts improve results 
derive distributional version arguably compatible text documents bag words 

discriminative clustering texts assume documents generated multinomial distributions parameter vectors denote auxiliary samples construct parameterized partitioning distribution space 
partitioning due computational reasons allowing nonzero memberships discriminative clustering generalizes vector quantization vq represents data prototypes minimizes caused distortion 
measure distortion conditional distributions distributional prototypes average distortion kl dq unknown sampling distribution data 
distributional prototypes re parameterized ji exp ji exp jm keep summed unity 
membership functions distributional space parameterized normalized gaussians distance measured kullback leibler divergence kl natural choice distributional document space 
yields kl ensures denotes collection parameters membership functions 
easy see resemblance information bottleneck method section cluster memberships similar functional shape 
view differences distributional clustering performs constrained soft vector quantization space multinomial distributions discriminative distributional clustering finds partitions relevant auxiliary variable 
noise model practice unknown documents finite length 
postulate noise model takes fact account propose tractable approximation essentially equivalent 
bag words assumption parameters multinomial model posterior parameter distribution prior 
take uncertainty account minimize distortion averaged parameters zz dq dn kl 
distributional clustering model applied documents keywords results readily applicable new full text documents keywords 
approximate 
conjugate dirichlet prior symmetric words posterior proportional nk mode posterior approximated mode average distortion distortion simplifies kl dn equal word frequencies documents normalized approximate distributions 
call model discriminative distributional clustering ddc 

ddc algorithm partitioning optimized minimizing cost function respect prototype sets shown done stochastic approximation algorithm iterates steps 
iteration sample labeled text document distribution practice randomly data 
denotes index auxiliary value 
denote 

sample clusters distribution fy 
adapt parameters log ji li lm lm lm mi re parameterized setting ji exp ji exp jk 
due symmetry possible apparently advantageous adapt parameters twice swapping second adaptation 
positive gradually decreasing learning coefficient principle fulfill conditions stochastic approximation theory 
empirical comparison section compare models empirically vector space mixture model called vmf generative occurrence models acm smm discriminative clustering models vector distribution space ddc 
hypothesize discriminative clustering models discover essential structure data outperform methods topical clustering 
measured ability clusterings predict independent topical categories produced 
note distributional models outperform heuristic vector space models interesting pairs acm smm vs vmf ddc vs 

data feature selection data scientific abstracts inspec database 
documents collected partially overlapping inspec topic categories 
topic categories final phase compare methods 
algorithms clustered textual documents consisting free text title fields abstracts 
words converted base form occurrences counted 
discriminative methods keywords field abstracts auxiliary data keywords descriptive words documents original authors 
keywords set frequent ones accepted 
ddc cases multiple keywords document assimilated minimizing average cost keywords 
ran sets experiments different preprocessing 
random features prior information word relevance 
words picked randomly words occurrences corpus 
second experiment idf picked features prior information 
words list words discarded words largest idf weights chosen 
idf inverse documents word occurs 
final data contained documents words yielding documents random features documents idf picked features 

optimization methods number clusters set models 
discriminative models trained line iterations stochastic approximation decreased piecewise linearly zero 
updated higher parameters 
precise values chosen preliminary experiments 
acm smm trained convergence em algorithm deterministic annealing iterations convergence recommended :10.1.1.33.1187:10.1.1.33.1187
vmf optimized convergence em algorithm 
dispersion parameter vmf ddc annealing parameters acm smm chosen validation models optimized validation set equal size training set 
keep optimization annealing parameter comparable cross validation parameters models varying run suggested kept constant em algorithm run convergence :10.1.1.33.1187:10.1.1.33.1187
data set model mean std random ddc features vmf acm smm idf picked ddc features vmf acm smm table results fold cross validation models sets 
mean standard deviation std empirical mutual information bits clusters categories 
best results shown bold 
models shown discriminative distributional clustering ddc vector space discriminative clustering vector space mixture model vmf asymmetric clustering model acm separable mixture model smm 

evaluation models compared able extract independent topically meaningful clusters 
criterion ability predict inspec categories abstracts 
category information training 
performance models measured empirical mutual information extracted clusters topic categories 
empirical mutual information estimated test data training 
empirical mutual information positively biased small samples 
reduced bias measuring soft mutual information conditional probabilities jn clusters document assigning document single cluster 
technically computed ij log ij il kj experimental relative frequencies ij ij nk jn nk jn denotes ith topic category topic category sample indexes clusters 

results demonstration performance models data sets shown table 
discriminative methods ddc cluster titles sample documents genetic algorithm approach chinese handwriting normalization size normalization line unconstrained handwriting recognition security mceliece public key cryptosystem implementation elliptic curve cryptosystem movement memory function biological neural networks analog retina edge detection table sample documents mapped clusters algorithm idf picked features outperform unsupervised models ddc attains highest mutual information random features idf picked features 
differences best models significant mcnemar test acm consistently outperforms unsupervised results roughly equal supervised model significantly different ddc 
surprisingly smm worst data sets comparison discriminative algorithms interesting better idf picked features distributional version ddc worked better randomly chosen features 
detailed investigation needed possible reason data sparseness case 
demonstrate discriminative algorithms computed clusters subset documents idf chosen features set 
dispersion parameter model selected remaining documents validation set 
sample document titles clusters shown table 
cluster articles handwriting recognition cryptosystems biological artificial neural networks 
articles clusters homogeneous 
typically articles topics cluster 

discussion shown discriminative clustering improves text clustering results 
clusters closely related relevant categories human experts categories training 
full text clustering guided auxiliary data keywords document authors 
clusters homogeneous keywords discriminate 
primary full text space clustered clusters defined documents keywords 
alternative estimate joint density documents words classes keywords define clusters marginalizing estimated density 
fixed resources result probably suboptimal purposes joint density estimation 
sought fixed number clusters 
criteria choosing number clusters need developed 
alternative build large cluster set summarize agglomeration 

acknowledgments supported part academy finland contract reach 

deerwester dumais furnas landauer indexing latent semantic analysis journal american society information science pp 

hofmann probabilistic latent semantic analysis proceedings fifteenth conference uncertainty articial intelligence morgan kaufmann publishers san francisco ca pp :10.1.1.33.1187:10.1.1.33.1187

hofmann puzicha statistical models cooccurrence data memo mit :10.1.1.34.97
kaski sinkkonen metrics learn relevance proceedings international joint conference neural networks volume ieee service center piscataway nj pp 

mclachlan basford mixture models 
inference applications clustering marcel dekker new york ny 
pereira tishby lee distributional clustering english words proceedings th annual meeting association computational linguistics acl columbus oh pp 

salton mcgill modern information retrieval mcgraw hill new york 
sinkkonen kaski clustering conditional distributions auxiliary space neural computation pp :10.1.1.28.7256

tishby pereira bialek information bottleneck method th annual allerton conference communication control computing urbana illinois 
