adaptive version boost majority algorithm yoav freund labs park avenue florham park nj usa october propose new boosting algorithm 
boosting algorithm adaptive version boost majority algorithm combines bounded goals boost majority algorithm adaptivity adaboost 
method making boost majority adaptive consider limit boosting iterations infinitesimally small contribution process 
limit modeled differential equations govern brownian motion 
new boosting algorithm named brownboost finding solutions differential equations 
describes methods finding approximate solutions differential equations 
method results provably polynomial time algorithm 
second method newton raphson minimization procedure efficient practice known polynomial 
adaboost boosting algorithm years popular algorithm practice 
main reasons popularity simplicity adaptivity 
say adaboost adaptive amount update chosen function weighted error hypotheses generated weak learner 
contrast previous boosting algorithms designed assumption uniform upper bound strictly smaller exists weighted error weak hypotheses :10.1.1.153.7626
practice common behavior learning algorithms error gradually increases number boosting iterations result number boosting iterations required adaboost far smaller number iterations required previous boosting algorithms 
boost majority algorithm bbm suggested freund appealing optimality properties rarely practice adaptive 
analyze adaptive version bbm call brownboost reason name clear shortly 
believe brownboost useful algorithm real world learning problems 
success adaboost increasing evidence algorithm quite susceptible noise 
convincing experimental studies establish phenomenon reported dietterich 
experiments compares performance adaboost bagging standard learning benchmarks studies dependence performance addition classification noise training data 
expected error adaboost bagging increases noise level increases 
increase error significant adaboost 
gives convincing explanation reason behavior 
shows adaboost boost majority margin weight schematic comparison examples weights function margin adaboost bbm boosting tends assign examples noise added higher weight examples 
result hypotheses generated iterations cause combined hypothesis fit noise 
consider binary classification problems 
denote example instance gamma label 
weights adaboost assigns example gammar gamma label training set combined strong hypothesis weighted sum weak hypotheses corresponds instance ln gamma ffl ffl ffl denotes weighted error respect weighting generating 
call margin example 
easy observe example prediction hypotheses incorrect gamma large negative number weight example increases rapidly bound 
reduce problem authors suggested weighting schemes functions margin increase slowly example algorithm gentle boost friedman suggestions formal boosting property defined pac framework 
experimental problems adaboost observed dietterich various attempts overcome problems motivated look bbm 
weights assigned examples algorithm functions margin 
form function relates margin weight different adaboost depicted schematically 
weight non monotone function margin 
small values weight increases decreases way similar adaboost point onwards weight decreases decreases 
reason large difference behavior bbm algorithm optimized minimize training error pre assigned number boosting iterations 
algorithm approaches predetermined examples large negative margins eventually correctly labeled 
optimal algorithms give examples concentrate effort examples margin small negative number 
bbm needs pre specify upper bound gamma fl error weak learner target error ffl 
show get rid parameter fl 
parameter ffl specified 
specification akin making bet accurate hope final hypothesis words inherent noise data 
shall show setting ffl zero transforms new algorithm back adaboost 
said adaboost special case bbm generates majority rules hypotheses weight margin linear combination number weak hypotheses correct number iterations far 
new algorithm initial bet error reduced zero 
intuition agrees fact adaboost performs poorly noisy datasets 
derive brownboost analyzed behavior bbm limit boosting iteration small change distribution number iterations increases infinity 
limit show bbm behavior closely related brownian motion noise 
relation leads design new algorithm proof main property 
organized follows 
section show relationship bbm brownian motion derive main ingredients brownboost 
section describe brownboost prove main theorem regarding performance 
section show relationship brownboost adaboost suggest heuristic choosing value brownboost parameter 
section prove variant brownboost boosting algorithm pac sense 
section solutions numerical problem brownboost needs solve order calculate weights hypotheses 
solutions approximate solution guaranteed polynomial time second probably faster practice don proof efficient cases 
section remarks generalization error expect brownboost 
section describe open problems 
derivation section describe intuitive derivation algorithm brownboost 
derivation thought experiment consider behavior bbm bound error weak learner increasingly close 
thought experiment shows close relation boost majority brownian motion drift 
relation gives intuition brownboost 
claims section fully rigorous proofs hope help reader understand subsequent formal sections 
order bbm parameters specified ahead time desired accuracy ffl non negative parameter fl weak learning algorithm guaranteed generate hypothesis error smaller gamma fl 
weighting examples boosting iteration depends directly pre specified value fl 
goal get rid parameter fl create version bbm adapts error hypotheses encounters runs way similar adaboost 
start fixing ffi small positive value small expect weak hypotheses error smaller gamma ffi 
hypothesis error gamma fl fl ffi define probability ffi fl prob 
gamma ffi fl prob 
gamma ffi fl easy check error exactly gamma ffi 
assume hypothesis proceed iteration 
note ffi small change weighting examples occurs boosting iteration small 
hypothesis error gamma ffi consecutive iterations 
words calling weak learner boosting iteration reuse hypothesis error larger gamma ffi words close 
bbm fashion result combined hypothesis weighted majority weak hypotheses hypothesis integer coefficient error larger ffi error gammah smaller gamma ffi 
corresponds number boosting iterations survived 
arrived algorithm behavior similar variant adaboost suggested schapire singer :10.1.1.156.2440
choosing weights weak hypotheses error choose error weak hypothesis altered distribution close 
note really strange altered hypotheses combining contain large amount artificially added noise 
iteration add new independent noise fact ffi small behavior dominated random noise 
contribution actual useful hypothesis proportional ffi fl 
problem principle modification bbm long get ffi gamma ln ffl boosting iterations guaranteed expected error final hypothesis smaller ffl 
sense just described adaptive version bbm 
algorithm adapts performance weak hypotheses generates weighted majority vote final hypothesis 
accurate weak hypothesis iterations boosting participates larger weight receive combined final hypothesis 
exactly looking 
need set ffi small fact number iterations increases ffi gamma running time algorithm prohibitive 
overcome problem push thought experiment little ffi approach characterize resulting limit algorithm 
consider fixed example fixed real weak hypothesis characterize behavior sum randomly altered versions randomized alterations ffi 
turns limit characterized proper scaling known stochastic process called brownian motion drift 
precisely define real valued variables think time position 
set time ffi time boosting process ffi gamma iterations constant independent ffi define location ffi ffi dt ffi ffi stochastic process ffi approaches defined continuous time stochastic process follows brownian motion characterized mean variance oe equal fl ds oe gamma fl weighted error hypothesis time derivation meant proof attempt formally define notion limit merely bridge get continuous time notion boosting 
note limit consider distribution prediction example normal distribution results brownian motion sum 
far described behavior altered weak hypothesis 
complete picture describe corresponding limit weighting function defined bbm 
weighting function binomial distribution equation ff ffi gamma gamma ffi gamma ffi ffi gammar gamma ffi ffi gammai gamma excellent brownian motion see breiman especially relevant section describes limit 
note sum contains ffi gamma terms constant average magnitude multiplied ffi ffi maximal value sum diverges sigma ffi variance ffi converges limit 
ffi total number boosting iterations index current iteration number correct predictions far 
definitions ffi letting ffi get ff approaches limit irrelevant constant factor ff exp gamma gamma gamma lim ffi ffi ffi similarly find limit potential function equation fi ffi gammar ffi gamma ffi gamma ffi ffi gammai gammaj fi gamma erf gamma gamma erf delta called error function erf gammax dx simplify notation shall slightly different potential function 
potential function essentially identical fi 
definitions ff fi translate bbm algorithm continuous time domain 
domain running bbm huge number small steps solve differential equation defines value corresponds distribution respect error weak hypothesis exactly half 
route abandon intuitive trail lead salvage definitions variables ff fi describe algorithm brownboost functions directly referring underlying intuitions 
preliminaries assume label predict gamma 
schapire singer allow confidence rated predictions real numbers range gamma :10.1.1.156.2440
error prediction defined error jy gamma yj gamma yy interpret randomized prediction predicting probability gamma probability gamma 
case error probability mistaken prediction 
hypothesis mapping instances gamma 
shall interested average error hypothesis respect training set 
perfect hypothesis instances completely random instances 
call hypothesis weak hypothesis error slightly better random guessing notation correspond average error slightly smaller 
convenient measure strength weak hypothesis correlation label gamma error 
boosting algorithms algorithms define different distributions training examples weak learner called base learner generate hypotheses slightly better random guessing respect generated distributions 
combining number weak hypotheses boosting algorithm creates combined hypothesis accurate 
denote correlation hypothesis fl assume fl significantly different zero 
inputs training set set labeled examples gamma 
weaklearn weak learning algorithm 
positive real valued parameter 
small constant avoid degenerate cases 
data structures prediction value example associate real valued margin 
margin example iteration denoted initial prediction values examples zero 
initialize remaining time 
associate example positive weight gamma 
call weaklearn distribution defined normalizing receive hypothesis advantage random guessing fl 
fl ff real valued variables obey differential equation dt dff fl exp gamma ffh gamma exp gamma ffh gamma constants context 
boundary conditions ff solve set equations find ff ff fl 
update prediction value example ff 
update remaining time gamma output final hypothesis gamma erf ff gamma sign ff algorithm brownboost algorithm algorithm brownboost described 
receives input parameter positive real number number determines target error rate larger smaller target error 
parameter initialize variable seen count clock 
algorithm stops clock reaches zero 
algorithm maintains example training set margin 
structure algorithm similar adaboost 
iteration weight associated example weak learner called generate hypothesis delta correlation fl hypothesis algorithm chooses weight ff addition positive number represents amount time subtract count clock calculate ff algorithm calculates solution differential equation 
lemma show solution exists sections discuss detail amount computation required calculate solution 
ff update margins count clock repeat 
adaboost algorithm arbitrary point wait count clock reaches value zero 
point algorithm output hypothesis 
interestingly natural hypothesis output stochastic rule 
thresholded truncation stochastic rule get deterministic rule error bound twice large stochastic rule 
order simplify notation rest shall shorter notation referring specific iteration 
drop iteration index index refer specific examples training set 
example parameters ff define quantities old margin step ffu gamma new margin exp gammad exp gammad weight notation rewrite definition fl fl ff exp gammad exp gammad shall write fl ff ew short hand notation describes average value respect distribution defined main properties differential equation analysis equations partial derivatives fl ew du gamma ew ew stands covariance respect distribution defined similarly find clean expressions derivatives fl reminiscent derivatives partition function statistical mechanics 
don point clear physical interpretation quantities 
ff fl ew ew du gamma ew du ij gamma du lemma shows differential equation guaranteed solution 
lemma set real valued constants function dt ff dff ff exp gamma ffb gammat exp gamma ffb gammat function continuous continuously differentiable 
proof appendix lemma guarantees exists function solves differential equation passes ff 
fl know derivative function zero positive 
solution guaranteed continuous derivative possibilities 
reach boundary condition fl derivative remains larger case reach boundary condition clear range relationship ff 
ff index solution differential equation 
prove main theorem states main property brownboost 
note inequalities statement proof strict equalities 
theorem algorithm brownboost exits main loop exists finite final hypothesis obeys jy gamma gamma erf proof define potential example time iteration fi erf ffh gamma weight example exp gamma ffh gamma depend ff depends time central quantity analysis brownboost average potential training data 
show quantity invariant algorithm 
words average potential remains constant execution brownboost 
algorithm starts examples ff potential example erf average potential 
equating average potential average potential get erf fi erf erf ff erf ff plugging definition final prediction dividing sides equation gammam adding side get gamma erf gamma yp jy gamma statement theorem 
show average potential change function time 
follows directly definitions iteration example potential example change boosting iterations fi fi 
average potential change boundary boosting iterations 
remains show average potential change iteration 
simplify equations fixed iteration notation fi replace fi respectively 
single example get dt fi dff dt gamma solution differential equation requires dff dt fl definition fl averaging examples get dt fi cm fl gamma cm gamma shows average potential change time completes proof 
choosing value running brownboost requires choosing parameter ahead time 
cause think improved situation bbm 
parameters choose ahead time fl ffl 
brownboost choose parameter quite adaboost 
parameters whatsoever 

section show fact hidden parameter setting adaboost 
adaboost equivalent setting target error ffl brownboost zero 
observe functional relationship ffl give theorem ffl gamma erf 
second note ffl get 
interesting characterize behavior algorithm 
solution equation implies algorithm reaches normal solution fl solution ff satisfies exp gamma ff gammat exp gamma ff gammat assume jh ff bounded constant iteration easy see conditions lim terms remain bounded mn 
lim exp gamma ff gammat exp gamma ff gammat lim exp gamma ff gammat exp gamma ff gammat exp gamma ff exp gamma ff note limit dependencies cancel denominator 
plugging definitions get condition choice ff exp gamma gamma ff ff jj exp gamma gamma ff ff jj stare equation sufficiently long realize condition defines choice weight th hypothesis ff ff identical defined schapire singer generalized version adaboost theorem 
note effect letting increase bound algorithm reach condition exit loop apply theorem bound error combined hypothesis 
hand bounds proven adaboost 
set get trivially algorithm exits loop immediately 
devise reasonable heuristic choose start running adaboost corresponds setting large algorithm measure error resulting combined hypothesis held test set 
error small done 
hand error large set observed error equal gamma erf run algorithm see reach loop exit condition 
decrease increase repeating binary search identify locally optimal value value brownboost exits loop theoretical bound holds slightly larger setting cause brownboost achieve exit loop 
remains open global maximum legitimate values form open closed segment max 
parameter 
solve equation find ff 
ln 
fl ff ff 

fl find corresponding ff ff ff fl 
variant step algorithm brownboost 
variant provably boosting algorithm pac sense 
brownboost boosting algorithm far shown brownboost interesting properties relate bbm adaboost 
shown boosting algorithm pac sense 
words provides polynomial time transformation weak pac learning algorithm strong pac learning algorithm 
parts showing algorithm pac learning algorithm 
show errors weak hypotheses smaller gamma fl fl algorithm reach desired error level ffl number boosting iterations poly logarithmic ffl 
secondly need show solving differential equation done efficiently polynomial time 
section show part issue efficiency addressed section 
order show poly logarithmic number iterations suffices need show remaining time parameter decreases constant errors uniformly bounded away 
turns case brownboost 
decrease arbitrarily small error constant 
shall see case simple choice ff sufficiently large 
choice exact solution differential equation shall see influence average potential sufficiently small 
describe variant brownboost utilizes observation 
desired property variant stated theorem 
theorem assume variant brownboost described 
ffl desired accuracy set erf gamma gamma ffl ffl advantages weak hypotheses satisfy fl gamma ac ln ln ffl algorithm terminates training error final hypothesis ffl 
proof fact follows similar route proof theorem case potential function weight function essentially gammax dx gammae gammax case gammax erf 
result interest adaboost authors started referring algorithms boosting algorithms 
hand best knowledge algorithms algorithm proven boosting algorithms 
authors feeling term boosting context concept learning reserved algorithms proven boosting algorithms pac sense 
corollary fl fl number boosting iterations required brownboost generate hypothesis error ffl fl gamma gamma ln ffl ignoring factors order ln ln ffl 
proof gamma erf gammac 
sufficient set ln ffl guarantee initial potential smaller ffl 
plugging choice statement theorem proves corollary 
solving differential equation order show brownboost efficient pac boosting algorithm remains shown efficiently solve differential equation 
shall show methods doing theoretical interest prove requires polynomial time 
second method practice efficient prove requires polynomial number steps 
polynomial time solution solution described section calculating finite step solution differential equation 
words start ff 
ff calculate fl calculate small update ff arrive ff repeat process fl point go iteration brownboost 
check point total weight smaller follow prescription set fl ff ff small updates solve differential equation exactly show total decrease average potential cause arbitrarily small 
solution method corresponds solving differential equation small step approximation 
clearly crude approximation constants far optimal 
point show required calculation done polynomial time 
section describe solution method efficient practice don proof efficiency 
theorem ffl choose min ln ffl step brownboost settings ff min ffl fl ff total decrease average potential ffl proof appendix approximation get brownboost poly time boosting algorithm pac sense 
sketch analysis algorithm 
ffl small constant desired accuracy 
suppose weak learning algorithm distribution examples generate hypothesis correlation larger ffl 
set large max ln ffl ffl gamma erf 
conditions satisfied log ffl 
brownboost approximate solution described theorem 
guaranteed ffl ln ffl ffl iterations 
training error generated hypothesis final potential ffl 
practical solution section describe alternative method solving differential equation believe initial experiments indicate efficient accurate previous method 
know proof theorem exact solution equation guaranteed leave average potential examples training set unchanged 
words solution ff satisfy erf ffb gamma erf hand boundary condition fl corresponds equation exp gamma ffb gamma nonlinear equations unknowns ff wish find simultaneous solution 
suggest newton raphson method finding solution 
order simplify notation index examples sample 
recall boosting iteration keep fixed derivation 
define ff gamma 
notation write non linear equations components function exp gamma delta erf delta gamma erf goal find 
newton raphson method generates sequence approximate solutions recursion gamma df gamma df jacobian function notation delta gammad write jacobian follows df gammav gammaw order calculate inverse df calculate determinant df det df gamma ub adjoint df express inverse df gamma gamma det df combining equations subscript denote value constants th iteration newton raphson denoting erf gamma erf find newton raphson update step ff ff cw cu gamma cb cv gamma divide enumerator denominator get expression update step function expected values respect distribution defined normalizing weights ff ff gamma gamma efficient solution method 
newton raphson methods guaranteed asymptotically quadratic rate convergence twice differentiable conditions 
means error decreases rate gammai starting point sufficiently close correct solution 
currently trying show error solution decreases similar rate start easy calculate starting point suggested theorem 
generalization error brownboost schapire prove theorems theorems bound generalization error convex combination classifiers function margin distribution combination :10.1.1.31.2869
clearly theorem applied output brownboost 
claim brownboost appropriate adaboost minimizing bounds 
bounds consist terms equal fraction training examples margin smaller second proportional 
cases data noisy clearly get better bounds giving noisy training examples allocating term doing increasing decreasing second term 
adaboost brownboost tuned parameter achieve effect 
issue important controlling norm coefficients weak hypotheses 
theorem assume jff 
stated brownboost control norm coefficients 
simple trick sure norm bounded 
suppose weak learner generates hypothesis finding coefficient ff altered version gamma gamma ff suppose gamma jff long ff coefficients remain positive sum remains 
case ff degenerate case effectively eliminates previous hypotheses new combination new hypothesis remains 
case remove previous hypotheses combination starting algorithm combined hypothesis 
shown brownboost boosting algorithm possesses interesting properties 
planning experiment algorithm extensively near see performs practice 
technical issues resolve regarding brownboost 
show newton raphson method similar guaranteed converge quickly solution differential equation 
know local maximum formalize noise resistant properties algorithm characterize types noise overcome 
brownboost optimizing function margin closely related bound proven adaboost :10.1.1.31.2869
regard method direct optimization margins suggested mason 
experiments needed order see theoretical advantage pans practice 
relationship boosting brownian motion studied schapire freund opper 
acknowledgments special eli shamir pointing similarities boost majority brownian motion drift 
roland freund help problems numerical analysis 
rob schapire helpful discussions insights 
leo breiman 
probability 
siam classics edition 
original edition published 
leo breiman 
bagging predictors 
machine learning 
thomas dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
machine learning appear 
yoav freund 
boosting weak learning algorithm majority 
information computation 
yoav freund manfred opper 
continuous drifting games 
proceedings thirteenth annual conference computational learning theory pages 
morgan kaufman 
yoav freund robert schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences august 
jerome friedman trevor hastie robert tibshirani 
additive logistic regression statistical view boosting 
technical report 
mason peter bartlett jonathan baxter 
direct optimization margins improves generalization combined classifiers 
technical report systems engineering australian national university 
robert schapire 
strength weak learnability 
machine learning 
robert schapire 
drifting games 
proceedings twelfth annual conference computational learning theory 
robert schapire yoav freund peter bartlett wee sun lee :10.1.1.31.2869
boosting margin new explanation effectiveness voting methods 
annals statistics october 
robert schapire yoram singer 
improved boosting algorithms confidence rated predictions 
proceedings eleventh annual conference computational learning theory pages 
appear machine learning 

numerical analysis 
springer verlag 
proof lemma proof prove lemma standard lipschitz condition ordinary differential equations state slightly simplified form theorem theorem defined continuous strip ja rg finite 
constant jf gamma gamma exists exactly function 
continuous continuously differentiable 

infinitely differentiable suffices prove bound partial derivative respect case ff fl 
equation know fl sufficient prove strip gammaa ff value uniformly bounded 
finite number examples max ju max jv finite 
remains show upper bound ffu gamma unfortunately bounded strip need little harder 
overcome problem potentially unbounded fix real number define fl clipped version function fl fl ff fl ff max gammab min note fl equal fl jtj continuous 
partial derivative tfl equal fl zero undefined magnitude bounded jd au equation conclude fl satisfies conditions theorem strip ff gammaa conclude function ff satisfies dt ff dff fl ff strip 
note jfl ff implies jfl ff derivative dt ff dff range ff gammaa function bounded jt ff au setting au conclude solution differential equation defined fl ff solution differential equation defined fl ff 
solution exists setting solutions conform 
solution differential equation real line 
proof theorem proof proof consists parts corresponding cases algorithm follow iteration 
case show properties 
show difference remaining time gamma fl second show total decrease average potential time ffl 
parameter chosen initial potential gamma ffl 
combining claims get final potential gamma ffl 
lower bound final potential argument proof theorem find error final hypothesis ffl 
case average weight iteration idea part proof 
initial boundary differential equation fl fl ff final boundary fl shall give lower bound denoted dfl ff dff integral ff dt dff dff ff fl ff dff ff fl gamma bff dff fl fl gamma fl gamma fact assume algorithm 
general proof order note bounded range assumption requirement solution differential equation 
compute lower bound equations get dfl ff dff ff fl dt dff fl ff fl fl fl ew ew du gamma ew du ij fl ew du gamma ew ew fle du gamma ew du gamma fl ew bound sum bound absolute value term 
definition fl 
expectations bounded lemma 
lemma am real valued numbers ja jb exists gammab fi fi fi fi fi gammab gammab fi fi fi fi fi ln proof lemma 
continue proof case assumption case gammad setting noting ju apply lemma find je du ln 
similarly setting find je ln je theta du ln 
combining bounds get fi fi fi fi dff fl ff fi fi fi fi ln combining equations find iterations total weight remains gamma fl gamma fl gamma conservation average potential know proof theorem iterations exact solution differential equation total potential change 
completes proof case case ii average weight smaller point iteration case claim gamma fl follows directly construction algorithm 
remains shown case decrease average potential sufficiently small 
show speed decrease potential function time smaller ffl total time gives maximal total decrease potential ffl 
derivative average potential average weight dt fi ff gammaw ff remains shown average weight point smaller remain smaller ffl ff kept unchanged increased quantity smaller equal recall fl 
lemma 
lemma am real numbers gammaa gamma gammax gamma gammap ln gammax delta recall definitions ff 
set ffl ff gamma lemma get ff gamma gammat exp gamma ln gamma exp gamma ln gamma ln gamma ffl inequality follows constraint ffl implies 
completes proof case ii 
proof lemma easy see prove lemma sufficient show case gammab ln gammab separate sum lhs equation parts gammab gammab gammab gammab ln 
note 
show assumption lemma number terms large 
denote number terms ffm assumption lemma implies gammab gammab ffm gamma ff gammaa ffm gamma ff ff gamma implies ff 
show third sum equation small relative expression related sum 
observe function xe gammax monotonically decreasing 
gammab ae gamma ln gammaa gammaa gammab combining equations get gammab gammab gammab gammab gammab gammab gammab gammab gammab proves equation completes proof lemma 
proof lemma fix maximize lhs equation constraint defined equation 
note replacing gammaa change constrained equation increases lhs equation 
assume 
furthermore setting reduces sum equation increases sum equation 
assume lagrange method constrained maximization find gamma gammax gammaa gamma gamma gammax gammaa implies gamma gammax assume expression positive monotonically increasing extremum maximum occurs equal equal ln 
plugging value equation completes proof 
proof theorem proof proof follows line proof theorem 
difference showing average potential stays completely constant show decrease iteration small 
follows fix boosting iteration denote potential example iteration fi erf viewed constant part ffh gamma variable part 
start focusing change potential single example single iteration bound change average potential examples specific iteration 
sum change iterations 
part fix drop indices 
concentrate change fi function fi infinite number derivatives respect taylor expansion third order estimate fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi fi number range 
computing derivatives get fi gamma fi exp gamma gamma gamma exp gamma considering variability term get bound fi gamma fi exp gamma gamma gamma choice ff get bound jz examples jz jff gamma jff yj jt jff order get upper bound decrease average potential iteration sum inequality examples training set 
estimate sum power separately 
bound sum term definition fl fact ff fl exp gamma ff gamma exp gamma exp gamma ff fl gamma ff exp gamma ff upper bound sum second term bound jz separate sum parts value 
exp gamma exp gamma exp gamma ff case technical lemma proof 
lemma ffl non negative real numbers min ln ffl exp gamma ffl combining lemma condition setting statement theorem get exp gamma ffl ff ffl inequality follows fact terms sum 
bound term assumption ff ffl bound equation ff ffl combining bounds equations get bound decrease average potential iteration fi gamma fi exp gamma ff gamma ff gamma ffl gamma ffl summing bound iterations assumption get total decrease initial potential final potential ffl ffl argument proof theorem get statement theorem 
proof lemma consider possible ranges ffl exp gamma ffl exp gamma ffl ln ffl 
ffl exp gamma exp gamma ffl ffl ffl exp gamma ffl ffl ffl xe gammax 
