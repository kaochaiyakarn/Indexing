extended message passing algorithm inference loopy gaussian graphical models kumar department electrical computer engineering coordinated science laboratory university illinois urbana champaign consider message passing probabilistic inference undirected gaussian graphical models 
show singly connected graphs message passing yields algorithm equivalent application gaussian elimination solution particular system equations 
relation provides natural way extending message passing arbitrary graphs loops rst studying operations required gaussian elimination 
obtain nite time convergent algorithm solves inference problem exactly complexity grows gradually distance graph tree 
algorithm implemented distributed fashion nodes message passing example sensor networks 
key words graphical models message passing loopy graphs probabilistic inference gaussian inference material partially supported contract nos 
daad daad darpa contract nos 
afosr contract 
darpa afosr contract 
nsf contract 
nsf ani 
opinions ndings recommendations expressed publication authors necessarily re ect views agencies 
university illinois urbana champaign csl west main st urbana illinois 
email addresses control csl uiuc edu control csl uiuc edu kumar 
url black csl uiuc edu kumar 
preprint submitted elsevier science october set random variables problem probabilistic inference cast computing posterior probability subset variables values subset see example :10.1.1.114.4996
number variables large inference requires integration high dimensional spaces easily intractable 
cases conditional independence relationships sets random variables 
collection conditional independence relations gives rise factorization joint probability distribution product functions depends subset variables 
factorization signi cantly reduce complexity inference 
focus ecient probabilistic inference collections random variables 
graphical models conditional independence relations set random variables encoded graph 
node graph represents random variable independence relations encoded edges 
graph directed bayesian networks undirected markov random elds 
focus undirected graphs 
ord theorem provides connection independence factorization strictly positive probability distribution satis es conditional independence relations implied graph factors maximal cliques underlying graph singly connected path pair nodes tree forest ecient algorithms exist solve inference problem :10.1.1.114.4996
particular message passing algorithms associate nodes individual processors perform local computations communicate messages 
messages converge nite number steps node correctly computed posterior distribution 
graph loops message passing scheme applied algorithm guaranteed converge general converge correct posterior distributions see 
despite message passing algorithms applied enormous success loopy graphical models way approximating posterior distributions see example 
gaussian graphical models models nodal variables jointly gaussian problem probabilistic inference reduces computing posterior mean covariance 
problem linear algorithm takes simpli ed form 
fact posterior mean obtained solution system linear equations posterior covariance obtained inverse matrix 
message passing algorithms studied gaussian graphical models loops turbo decoding graph gaussian nodes arbitrary graphs gaussian nodes 
cases shown message passing converges computed posterior means correct covariances wrong 
papers provide sucient conditions convergence 
iterative embedded trees algorithm computing posterior means covariances 
step modi ed inference problem solved spanning subtree graph computed mean covariance subsequent iteration 
shown algorithm converges geometrically exact means covariances 
algorithm studied 
relation algorithm di erent iterative methods solution systems linear equations 
particular shown algorithm derive nite time algorithm computes posterior means time covariances time 
number nodes graph minimum number edges need removed reveal embedded tree dimension nodal variables 
important mention algorithm applied arbitrary graphical models gaussian providing recursive algorithm inference 
consider message passing general gaussian graphical models loops loops tree 
focus scalar random variables results easily generalized random vectors 
show graph tree resulting algorithm equivalent application gaussian elimination solution particular system equations 
known rooted tree tree particular node chosen root inference performed steps rst ne coarse kalman ltering step equivalent gaussian elimination message passing context step corresponds messages sent starting leaves root rauch smoothing step see :10.1.1.19.5207
show message passing solves inference problem sending messages directly tree equivalent gaussian elimination rooted trees 
fact inference problem solved considering rooted trees rooted di erent node applying kalman lter step 
message passing performs computations simultaneously tree 
observation facilitates understanding dynamics algorithm allows generalize message passing natural way handle graphs loops 
obtain nite time algorithm converging nl steps correct posterior means algorithm computes posterior means variances nl steps number nodes isolated subgraph remaining edges deleted spanning subtree obtained di erent approach complexity algorithms equivalent algorithms 
sections details graphical models consider assumptions introduce notation 
section presents expressions messages gaussian graphical models 
section show tree system equations solved simple structure fact mentioned section exploit structure solve system gaussian elimination show algorithm obtain equivalent message passing 
include brief review gaussian elimination section 
sections equivalence extend message passing algorithm compute posterior means variances respectively graphs loops 
section presents proposed algorithms pseudo code 
include centralized version implemented single processor distributed implemented example sensor network node fact separate physical unit 
section study complexity proposed algorithms conclude remarks section 
setting follow closely setting notation 
consider gaussian stochastic process unobserved ir valued state vector probability density function exp 
associate undirected graph contains nodes indexed edge connecting th entry nonzero 
manner de ned markov respect subsets fx fx fx kn removing vertices completely isolates variables mutually conditionally independent variables simplicity suppose node corresponds component subvector general case 
fig 

example gaussian stochastic process de ned loopy graph 
variables represented circles 
observed variables colored black 
corresponds noisy observation observation vector satis es cx 
append nodes indexed edges call graph 
connected implies random variables fy conditionally independent turn means diagonal matrices 
shows example graphical model observed nodes nodes corresponding colored black 
call gaussian graphical model 
goal inference problem determine conditional marginals jy posterior probability observations 
standard consider joint probability distribution coecients 
joint distribution gaussian posterior distribution gaussian suces determine posterior mean covariance known satisfy pc note conditional error variances diagonal elements solving fact general solving inference problem stated complete posterior covariance computed just diagonal elements 
interested computing diagonal elements expression see conditioned gaussian random vector mean cx covariance matrix conditional density xjy yjx exp cx cx completing square expression nd conditioned gaussian random vector covariance mean pc list notation conventions follows nodes correspond state variables observations simplify notation indices indicate state nodes node means node edge refers edge joining name observations 
denotes set indices state variables neighbors set indices state variables neighbors fjg 
matrix denote th component th row respectively 
likewise denotes th component vector 
simplify presentation de ne matrices cr equation de nes posterior mean notation consider application message passing trees 
distinguish particular node root node relabel variables breadth rst order starting root see 
note general labeling unique 
particular breadth rst order labeling chosen ect 
permutation matrix maps original indices new ones function index breadth rst order labeling root node 
example 
applying message passing rooted tree denote parent node 
usual graphical models notation denote proportional pair compatible quantities vectors functions 
gures observed nodes shown explicitly 
clarity consider graphical models variables fx fy studying message passing algorithm tree nd relation gaussian elimination 
relation extend algorithm handle graphs loops 
root node fig 

example relabeling process 
tree shows original labeling 
node chosen root variables relabeled breadth rst order tree obtained 
parametrization order message passing solve inference problem necessary factorization joint distribution product local functions kernels functions maximal cliques graph 
general factorization unique 
choice local functions provides parametrization joint distribution 
valid parametrization needs satisfy certain conditions study section 
expression joint density function exp goal construct message passing algorithm loopy graphs modi cation message passing algorithm trees 
assuming corresponds tree noting tree pairwise graph graph largest clique size know joint distribution factors product local kernels depends variables say matrix corresponds graph structure respects connectivity connected gaussian graphical models kernels general form exp exp constitute correct parametrization joint distribution conditions met diag xn xn diag xn note pair indices likewise easy task nd valid parametrization satis es 
messages message passing node sends messages neighbors 
message sends contains parameters function gaussian graphical model messages parametrizations gaussian distribution functions represented mean inverse covariance precision 
denote function corresponding message mean inverse covariance respectively 
tree order messages sent irrelevant nal result 
purposes convenient schedule algorithm hy hj hy hj fig 

complete set messages transmitted application message passing solve inference problem node graph 
rule node allowed send message node received messages neighbors 
note rule implemented loops 
trees procedure initiated leaves tree progress inwards tree deduced 
inference problem want solve computation posterior distributions state nodes observations 
problem solved sending messages pair neighboring nodes 
example shows tree complete set messages necessary solve inference problem 
solving inference problem state variable requires transmission subset messages message computed exactly general case 
consider simpler problem rst 
relation gaussian elimination clear 
clarity consider tree rooted relabel variables breadth rst order starting root depicted 
messages necessary nd posterior distribution root node ones owing upwards root 
example message passing rooted tree shown 
general message passing algorithm 
mean precision message node sends neighbor message passing nished node computes posterior mean precision equations note rooted tree root node new labeling computes rest section suppose message passing performed rooted tree 
avoid confusing notation assume parametrization joint distribution respect new labeling 
section compare message passing gaussian elimination 
need detailed expressions messages 
expanding expression ak ak step message passing second step message passing fig 

example message passing process rooted tree chosen schedule 
note tree shown equivalent tree rooted relabeled breadth rst order 
note subset messages shown transmitted 
nd parent de ning nd initial condition recursion leaf node 
fashion see letting write initial condition root node detailed expression precision root node fig 

shows structure corresponding tree 
nonzero elements shown black squares 
likewise value expressions 
structure section apply gaussian elimination solve inference problem root node relabeled rooted tree 
rst need nd structure section simplify problem assuming respect relabeled variables 
recall gaussian graphical model nonzero connected edge see 
words structure adjacency matrix diagonal terms 
breadth rst order labeling gives simple structure 
see consider example relabeled rooted tree shown 
consider xed row entry nonzero neighbor see neighbor parent rooted tree 
likewise neighbors index higher children means row contains nonzero entries structure mean positions nonzero entries 
positions corresponding parent diagonal term children important note row unique nonzero element left diagonal corresponding unique parent symmetry follows column unique nonzero element diagonal corresponding parent structure tree shown 
review gaussian elimination section review brie gaussian elimination introduce terminology helpful describing gaussian elimination applied context graphical models 
consider system equations ax suppose want transform system equivalent solution xed say element eliminated 
adding scaled version row row case say row eliminate element element eliminate element formalize matrix de ned way call elimination factor 
wa notation simpler mention matrix explicitly 
fact consider th row matrix th element updated directly update operation known row operation 
values variables known substituted equations reducing order system solved 
procedure called 
see fuller detailed explanation gaussian elimination 
gaussian elimination trees section assume inference particular node relabeling variables problem stated nding section assume parametrization joint distribution terms breadth rst order labeling 
allows directly drop nd gaussian elimination solve problem 
order eliminate elements upper triangle row operations 
process system transformed lower triangular matrix 
denote th diagonal entry th component fact correct posterior mean variance root node 
determine order elements upper triangle eliminated 
considering tree shown structure easy infer rules leaf node 
means rows corresponding leaf nodes need changed 
element row eliminated diagonal term column 
note elimination term done diagonal term row child rooted tree 
column contains nonzero element upper triangle row elimination process fact corresponds message sent node parent 
elements row contained upper triangle exactly child terms eliminated rows corresponding children corresponds children sending message parent 
terms row upper triangle eliminated row eliminate unique nonzero element contained upper triangle column element parent observations imply elimination process done exactly order message passing start leaf nodes parents leaves root reached 
complete elimination process tree shown 
order elimination done gives recursive expression diagonal terms fd parent rooted tree 
initial conditions rows elements eliminate step rows elements eliminate step step elements eliminate rows rows elements eliminate final matrix step fig 

complete sequence row operations nd posterior mean root node tree 
step shows row operations 
action element shown arrow 
nonzero elements shown black squares 
recursion leaf node comparing see nd expression note factor elimination term order elimination done gives recursion initial conditions leaf comparing nd shown computations performed message passing equivalent application gaussian elimination solve associated system equations 
note previous analysis xed root node performed message passing nd posterior distribution root node 
priori solving inference problem nodes mean having run message passing rooted trees equivalently gaussian elimination nd systems equations critical observation node send message rooted tree parent 
messages recomputed choice particular right hand side gaussian elimination 
rooted tree parent 
observation message passing applied directly non rooted tree parent child relationships 
node sends message neighbors see 
sends message acts parent rooted tree nodes act children 
example interpreted right hand side position gaussian elimination rooted tree parent 
scheme allows message passing solve inference problems simultaneously 
messages node receives neighbors exactly receive relabeled tree rooted extended message passing algorithm computing means loopy graphs fact gaussian elimination matrix suggests extending message passing loopy graphs graphs cycles rst studying operations performed gaussian elimination 
section extend message passing algorithm compute posterior means loops 
see algorithm divided stages message passing solution smaller system equations partial solution 
stage modi ed message passing correspond loopy graph 
nding decomposition way loopy graph spanning subtree 
set extra edges 
denote set endpoints vertices edges call nodes indexed special nodes 
note number special nodes recall number non isolated nodes 
ir tree corresponds tree 
note case set neighbors 
root node tree kq task nd systems matrices gaussian elimination 
know message passing provide node information rst row rst element need create new messages allow compute nonzero elements rst row considering observations section new message tree values nonzero elements row parent rooted tree 
formalize node record vector ir edge vector ir vectors xed algorithm 
contains nonzero elements row rooted tree parent 
de nitions see vectors satisfy recursion message passing nished node compute likewise compute perturbation terms rst row interpretation know rst equation system rewritten original labeling true equation fact de nes new system equations consider equations corresponding nodes indexed nodes equation corresponds subsystem equations solved independently 
message passing case message passing special nodes structure new system matrix special nodes special nodes fig 

structure new system equations de ned equation 
process solving new system done steps solution subsystem 
reduce system equations order order stage solution subsystem equations ir ir structure shown 
system equations de ned rewritten system equations de ned equation solved standard gaussian elimination write message passing special nodes 
ir ir implement gaussian elimination solve way node sends node 
node updates parameters way process nished node computes note algorithm corresponds message passing version standard gaussian elimination 
stage values variables computed system de ned 
value extended message passing algorithm computing posterior variances loopy graphs section construct nite time convergent message passing algorithm compute variance estimator diagonal terms covariance matrix rooted tree solving really nding matrix lower triangular reading see section 
new messages introduced previous section allow compute introduce new messages compute rst row ect row operations identity matrix 
computing node allow compute complete posterior covariance doing increase excessively complexity algorithm 
interested diagonal terms need compute rows special node xed 
compute observe symmetry view introduce new messages provide node information necessary compute vector ir nd expressions desired elements translate messages 
rooted tree unique path joining root node expression non rooted tree de ne ir interpret th diagonal element rooted tree parent 
path joining order compute required products de ne vectors ir new message satisfying recursion structure new system matrix special nodes special nodes structure matrix fig 

structure matrices computing posterior variances 
irrelevant entries marked question marks 
message passing nished node computes vector way interpretation information collected node construct new system equations just computing mean case perform row operations nd solution system nd diagonal elements de ne matrix ir note elements de ned zero really unknown unimportant purposes 
structure shown 
solving subsystem equations ect row operations rows recorded 
information matrix distributed nodes need transmit information row operations performed special nodes node special node records elimination factor operation step algorithm 
speci special node record vector ir message passing contains elimination factors node solution subsystem equations 
doing initialize add update operations de ned step algorithm rule message special nodes nished special node sends node node de nes vector node reproduces row operations performed solution subsystem equations nds ect give details algorithm section 
node compute posterior variance section provide algorithms 
complete algorithms computing means variances loopy graphs summarize complete procedure provide algorithm toto 
algorithm computes posterior means variances 
means desired part code computes variances omitted 
versions algorithm centralized version implemented single processor distributed implemented distributed fashion example sensor network node measurement applying algorithm necessary nd spanning subtree set special nodes centralized version algorithm done specifying root node applying dijkstra algorithm nd shortest path node see example 
distributed implementation algorithm node run dijkstra algorithm nd node executes deterministic algorithm nds spanning subtree approach execute distributed bellman ford algorithm starting message passing 
reduces amount information node requires 
detailed presentation dijkstra bellman ford algorithms solve shortest path problem see 
order computations distributed algorithm require node unique identifying number 
specially important solving subsystem equations 
presentation algorithms follows 
algorithms pseudo code 
centralized algorithm centralized algorithm implemented single processor 
centralized algorithm input graph nodes fx prespeci ed root node gaussian kernel equation 
gaussian kernel equation 
vector observations ir output posterior mean variance find spanning subtree example running dijkstra algorithm root de ne non isolated vertices node ng de ne fjg repeat node ng node messages nodes received compute node received messages neighbors 
node ng compute node lg de ne ir ir lg flg update compute node compute initialize update update compute compute algorithm 
distributed algorithm distributed algorithm implemented nodes passing messages sensor network 
algorithm executed node distributed algorithm input identity number identity prespeci ed root node graph identity node observation ir 
gaussian kernel 
gaussian kernel equation 
output posterior mean variance equation 
find spanning subtree example dijkstra algorithm root node de ne non isolated vertices fjg repeat messages nodes received compute send quantities messages nodes received 
compute special de ne ir ir repeat wait sent special nodes update special node index sent information 
send special node 
repeat wait sent special nodes update messages nodes received 
compute send node send node number operations tree means means computation variances 



nl 
nl 
nl nl nl nl solution subsystem nl total 

nl nl table comparison complexities message passing extended message passing means extended message passing means variances 
wait sent special node compute wait receive special node de ne update update compute compute algorithm 
complexity algorithms section consider complexity complete algorithms compare complexity message passing trees 
table shows order number operations required relevant 
xn xn xn xn 
xn xn xn xn 
xn xn xn xn fig 

example judicious choice spanning subtree 
tree produces roughly twice number special nodes compared tree 
operations algorithm 
table 
represents maximum degree nodes spanning subtree note constants neglected 
example vectors computed edge 
tree edges 
computations requires order 
operations 
number operations necessary compute vectors 
nl 
table see total complexity message passing tree 
complexities extended algorithms 
nl means nl means variances 
see complexity computing means grows linearly number special nodes complexity computing variances grows quadratically 
fact number operations required solve inference problem grows gradually complexity graph increased intuitively satisfying justi es extended algorithms graphical models underlying graph close tree 
useful note judicious choice spanning subtree reduce signi cantly 
example 
example consider grid 
figures show di erent spanning subtrees graph 
number special edges tree number special edges tree 
see choice ect complexity algorithm 
concluding remarks standard message passing tree directly graph cycles yields recursive algorithm convergence guaranteed 
fact converges converges rate convergence depend topology graph parametrization joint probability distribution 
opposed extended message passing algorithms guaranteed terminate nite time parametrization graph 
complexity algorithms increases gradually number special nodes judicious choice spanning subtree signi cantly reduce complexity extended message passing algorithm computing posterior means loopy graphs 
nl complexity algorithm computing posterior means variances nl 
drawback new algorithms node needs information structure graph 
apply traditional message passing node needs know neighbors 
extended algorithms node needs know incident edges spanning subtree nodes special 
solving subsystem equations assumed xed ordering special nodes 
issues represent diculties algorithm applied single computer proper scheduling messages required algorithm applied distributed fashion network 
intriguing question ideas applied solve problem inference general loopy graphical models gaussianity assumptions 
aji mceliece generalized distributive law ieee transactions information theory 
bertsekas tsitsiklis parallel distributed computation englewood cli prentice hall 
besag spatial interaction statistical analysis lattice systems journal royal statistical society series 
chou willsky multiscale recursive estimation data fusion regularization ieee trans 
automat 
contr 

chou willsky multiscale systems kalman lters equations ieee trans 
automat 
contr 

frey revolution belief propagation graphs cycles advances neural processing systems 
frey graphical models machine learning digital communication cambridge ma mit press 
golub loan matrix computations baltimore johns hopkins university press 
jordan bishop graphical models preprint :10.1.1.114.4996
kumar varaiya stochastic systems estimation identi cation adaptive control englewood cli prentice hall 
willsky levy boundary value descriptor systems posedness reachability observability international journal control 
pearl probabilistic reasoning intelligent systems 
networks plausible inference morgan kaufmann 
van roy analysis belief propagation turbo decoding graph gaussian densities ieee transactions information theory 
sudderth embedded trees estimation gaussian processes graphs cycles 
ms thesis massachussets institute technology february 
wainwright sudderth willsky tree modeling estimation gaussian processes graphs cycles advances neural information processing systems 
weiss freeman correctness belief propagation gaussian graphical models arbitrary topology neural computation 
willsky multiresolution markov models signal image processing proceedings ieee 

