algorithms large scale markov blanket discovery ioannis aliferis alexander department biomedical informatics vanderbilt university garland ave nashville tn ioannis aliferis alexander vanderbilt edu presents number new algorithms discovering markov blanket target variable training data 
markov blanket variable selection classification causal discovery bayesian network learning 
introduce low order polynomial algorithm variants soundly induce markov blanket certain broad conditions datasets thousands variables compare state art local global methods excellent results 
markov blanket variable interest denoted mb minimal set variables conditioned variables probabilistically independent target property knowledge values mb determine probability distribution values variables superfluous 
variables mb adequate optimal classification 
strong connection mb optimal principled variable selection explored aliferis 
addition certain conditions faithfulness bayesian network see section mb identical direct causes direct effects direct effects direct causes causal discovery reduce number variables consider order discover direct causes markov blanket discovery algorithms guide bayesian network learning algorithms mb identified step guide construction bayesian network domain approach taken thrun 
potential uses significance concept markov blanket surprising little attention attracted context bayesian net structure learning fundamental property bayesian net thrun 
novel algorithms soundly induce mb data scale thousands copyright american association artificial intelligence www aaai org 
rights reserved 
variables 
compare new algorithms state art methods mb excellent results 
novel algorithms particularly suited cases available sample size perform conditional independence tests condition full mb 
background bayesian networks bn neapolitan mathematical objects compactly represent joint probability distribution graph annotated conditional probabilities connected markov condition property node conditionally independent non descendants parents 
mb probabilistically shields rest variables graphically corresponds neighborhood bn graph 
denote conditional independence 
definitions markov blanket variable mb minimal set mb mb thrun 
bn faithful joint probability distribution variable set dependence entailed graph spirtes :10.1.1.32.5739
bn faithful faithful corresponding distribution markov condition ensures conditional independence entailed graph probability distribution faithfulness markov condition establish close relationship graph empirical theoretical probability distribution practical terms faithfulness implies associate statistical properties probability distribution properties graph corresponding bn 
turns faithful bns set parents children spouses parents children unique mb 
example markov blanket concept displayed mb set gray filled nodes 
iamb algorithm variants section novel algorithms discovering mb sound assumptions data generated processes faithfully represented bns ii exist reliable statistical tests conditional independence measures associations variable distribution sample size sampling data 
discuss rationale justification assumptions discussion section 
iamb description incremental association markov blanket iamb consists phases forward backward 
estimate mb kept set cmb 
forward phase variables belong mb possibly false positives enter cmb backward phase false positives identified removed cmb mb 
heuristic iamb identify potential markov blanket members phase start empty candidate set cmb admit iteration variable maximizes heuristic function cmb 
function return non zero value variable member markov blanket algorithm sound typically measure association cmb 
experiments mutual information similar suggested thrun cheng cmb mutual information cmb 
important informative effective heuristic set candidate variables phase small possible reasons time efficiency spend time considering irrelevant variables sample efficiency require sample larger absolutely necessary perform conditional tests independence 
backward conditioning phase ii remove features belong mb testing feature cmb independent remaining cmb 
iamb proof correctness sketch feature belongs mb admitted step dependent subset feature set faithfulness mb minimal set property 
feature member mb conditioned mb superset mb independent removed cmb second phase 
argument inductively see unique mb 
description smaller conditioning test finite sample fixed size accurate statistical tests independence measure associations 
algorithm uses methods reduce size conditioning sets interleaves admission phase iamb phase backward conditioning phase ii attempting keep size mb small possible steps algorithm execution 
substitutes backward conditioning phase implemented iamb pc algorithm spirtes bayesian network learning algorithm determines direct edges variables sample efficient manner sound stated assumptions see section expected iamb :10.1.1.32.5739
addition practical pc running small sets variables full set variables 
proof correctness sketch parents children enter cmb property mentioned 
pc sound remove variables 
effects enter cmb conditioned spouses parents children dependent cmb enter cmb point 
pc sound permanently remove may removed temporarily enter cmb subsequent iteration elaborate due space limitations included final output 
iamb variants experimented similar employ interleaving phases pc backward phase respectively 
iamb provides theoretical guarantees sample limit quality output approximation true mb degrades gracefully practical settings finite sample see experimental section 
iamb variants expected perform best problems mb small relatively available data samples domain may contain hundreds thousands variables 
time complexity typically performance algorithms tests conditional independence measured number association calculations conditional independence tests executed operations take similar computation effort distinguish spirtes cheng thrun :10.1.1.32.5739
phase ii performs cmb conditional independence tests 
phase performs association computations variable enters cmb number variables algorithm performs cmb tests 
worst case cmb giving order 
experiments iamb observed cmb mb giving average case order mb tests 
mutual information exists algorithm linear size data thrun 
iamb variants higher worst case time complexity example pc exponential number variables computation higher performance 
experiments observed size cmb relative small total number variables additional time overhead variants versus vanilla iamb minimal 
markov blanket algorithms knowledge algorithm developed explicitly discovering mb scales grow shrink gs algorithm thrun 
theoretically sound uses static potentially inefficient heuristic phase 
iamb enhances gs employing dynamic heuristic 
koller sahami algorithm ks koller sahami algorithm feature selection employ concept markov blanket 
ks heuristic algorithm provides theoretical guarantees 
gs algorithm structurally similar iamb follows phase structure 
important difference gs statically orders variables considered inclusion phase strength association empty set 
admits cmb variable ordering conditionally independent cmb 
problem heuristic mb contains spouses case spouses typically associated weakly empty set considered inclusion mb late phase associations spouses confounding common ancestors variables weaker ancestors associations 
turn implies false positives enter cmb phase conditional tests independence unreliable sooner iamb heuristic 
contrast conditioned common children spouses may strong association iamb heuristic enter cmb early 
analogous situation constraint satisfaction dynamic heuristics typically outperform static ones 
provide evidence support hypothesis experiment section 
note proof correctness gs correct assumes faithfulness just existence unique mb stated non faithful counter example exclusive gs fail discover mb unique 
ks algorithm koller sahami employed concept markov blanket feature selection 
algorithm accepts parameters number variables retain ii parameter maximum number variables algorithm allowed condition 
ks equivalent ordering variables selecting koller sahami explicitly claim identify mb guess size mb set parameter number ideally algorithm output mb 
viewed way treated ks algorithm algorithm approximating mb variables 
iamb iamb variants gs ks algorithm provide theoretical guarantees discovering mb 
pc spirtes prototypical bn learning algorithm sound stated set assumptions :10.1.1.32.5739
pc learns network scale mb easily extracted set parents children spouses pc algorithm starts fully connected unoriented bayesian network graph phases 
phase algorithm finds undirected edges criterion variable edge variable iff subsets features subset 
phases ii iii algorithm orients edges performing global constraint propagation 
thought improving gs employing efficient sound way pc backward phase dynamic heuristic forward phase improving pc providing admissible phase heuristic focuses pc local neighborhood 
provide hypothetical trace iamb bn 
assume reader familiarity separation criterion graph theory criterion implies probabilistic conditional independence 
cmb empty variable associated empty set enter cmb general expect variables closer exhibit highest univariate association 
conditioned associations variables calculated 
possible variable enter conditioned dependent 
cmb independent enter cmb 
suppose enters false positive 
guaranteed enter cmb dependent subset variables 
backwards phase removed independent notice gs variables considered inclusion association empty set 
increases probability number false positives entered cmb considered making conditional independence tests unreliable 
experimental results order measure performance algorithm need know real mb gold standard practice possible simulated data 
experiment set bns real diagnostic systems table 
tested algorithm alarm network beinlich captures structure medical domain having variables bn modeling predicting weather published abramson variables 
randomly sampled training instances joint probability network specifies 
task find markov blanket certain target variables 
alarm target variables variables report average performance natural target nodes corresponding weather forecasting report performance individually 
performance measure area roc curve metz 
created examining various different thresholds statistical tests independence 
pc algorithm thresholds correspond significance levels statistical test employed algorithm gs iamb variants consider cmb iff mutual info cmb threshold 
ks tried possible values parameter variables retain create detailed roc curve values suggested original 
example graph bayesian network 
gray filled nodes mb 
phase forward cmb cmb changed find feature cmb maximizes cmb cmb add cmb phase ii backwards remove cmb variables cmb return cmb iamb algorithm 
experiment set random bns table 
generated random bns nodes number parents node randomly uniformly chosen free parameters conditional probability tables drawn uniformly 
markov blanket arbitrarily chosen target variable contained variables parents children spouse held fixed networks consistent comparisons achieved different sized networks 
network adds variables previous altering mb 
ran algorithms sample sizes set report average areas curve table 
remind reader released version pc algorithm accept variables missing cells 
see iamb variants scale large number variables performance computation time iamb variants took minutes largest datasets took hours methods took hours experiments intel xeon alarm target target target target aver age iamb gs ks ks ks pc table experiments bayesian networks real decision support systems 
mb spouse mb spouses parents children parent children average aver vars vars vars vars vars vars age iamb gs ks ks ks pc table experiments randomly generated bayesian networks ghz pentium 
generated bns approach time mb contained spouse nodes parent children nodes total nodes 
interpretation results shown tables 
best performance column shown bold pc excluded scale 
test faithfulness assumption holds networks results indicative performance algorithms arbitrary bns 
applicable see pc best algorithms 
experiment set best algorithms average 
iamb variants better gs implying dynamic heuristic selecting variables important 
ks equivalent ordering variables univariate association target standard common technique statistical analysis 
algorithm performs set behavior ks quite unstable non monotonic different values consistent results original koller sahami 
experiment set expect simple static heuristic gs ks perform cases members mb strong univariate association typically case spouses mb 
random bn spouse algorithms perform 
second random bn spouses seriously degrades performance ks gs 
ks unpredictable behavior performs worse iamb variants 
iamb variants pc algorithm perform trickier case 
results due space limitations impossible report experiments 
experiments ran provide evidence support important hypothesis iamb dynamic heuristic expensive data sample possible small sample sizes simplest heuristics ks gs perform better especially spouses mb 
experiments suggest performance pc significantly degrades small instances data samples 
explained fact pc bias sensitivity removes edge prove removed retains 
certain sample size pc able remove edges reports unnecessarily large markov 
empirical results suggest practitioners apply algorithms appropriate available sample variable size pc algorithm sizes training instances variable size gs ks univariate association ordering sizes iamb variants 
discussion discussion mb discovery algorithms causal discovery 
exists faithful bn captures data generating process mb bn contain direct causes significantly prunes search space wants identify direct causes 
fact algorithms post process mb direct edges identify direct causes experiments pc fci algorithm assumes causal sufficiency second spirtes :10.1.1.32.5739
spirtes specific conditions discussed faithfulness gets violated :10.1.1.32.5739
situations relatively rare sample limit supported meek 
bn learning mb identification algorithms explicitly implicitly assume faithfulness pc gs implicitly bn score search scoring metrics see heckerman 
took step developing comparing markov blanket identification algorithms 
concept markov blanket strong connections principled optimal variable selection aliferis part bayesian network learning thrun causal discovery 
novel algorithms sound broad conditions scale thousands variables compare favorably rest state art algorithms tried 
followed principled approach allowed interpret empirical results identify appropriate cases usage algorithm 
room improvement algorithms hopefully inspire researchers address important class algorithms 
abramson brown edwards murphy winkler bayesian system forecasting severe weather international journal forecasting beinlich alarm monitoring system case study probabilistic inference techniques belief networks 
proc 
second european conference artificial intelligence medicine london england 

cheng bell liu learning bayesian networks data efficient approach information theory technical report url www cs ualberta ca doc report pdf heckerman meek cooper bayesian approach causal discovery technical report microsoft research msr tr 
koller sahami 
optimal feature selection proc 
thirteenth international conference machine learning 

thrun bayesian network induction local neighborhoods carnegie mellon university technical report cmu cs august 
meek strong completeness bayesian networks proc 
uncertainty artificial intelligence uai 
metz 
basic principles roc analysis seminars nuclear medicine 
neapolitan probabilistic reasoning expert systems theory algorithms 
john wiley sons 
spirtes glymour scheines 
constructing bayesian network models gene expression networks microarray data 
proc 
atlantic symposium computational biology genome information systems technology 
aliferis principled feature selection relevancy filters wrappers proc 
ninth international workshop artificial intelligence statistics 
