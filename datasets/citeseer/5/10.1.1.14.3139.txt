approaches combining local evolutionary search training neural networks review new results kim ku mak siu department computer science city university hong kong edu hk center multimedia signal processing department electronic information engineering hong kong polytechnic university 
training neural networks local search gradient algorithms difficult 
calls development alternative training algorithms evolutionary search 
training evolutionary search requires long computation time 
chapter investigate possibilities reducing time taken combining efforts local search evolutionary search 
number attempts combine search strategies successful 
chapter provides critical review attempts 
different approaches combining evolutionary search local search compared 
experimental results indicate baldwinian phase approaches inefficient improving evolution process difficult problems lamarckian approach able speed training process improve solution quality 
chapter strength weakness approaches illustrated factors affecting efficiency applicability discussed 
past decades development neural networks focused particular types neural network called multilayer feedforward networks 
static networks network outputs depend current inputs past inputs outputs 
feedforward networks applications pattern classification functional interpolations subjected constraint temporal information stored naturally encoded explicitly tapped delay inputs 
circumvent drawback recurrent neural networks rnns introduced number researchers elman jordan pineda williams zipser name 
networks feedback connections preserve past activities computation 
result current network state depends previous ones potentially unbounded period time time network started operate 
contrast outputs ku mak siu feedforward network tapped delay inputs dependent inputs limited period time 
capability handling temporal information rnns model temporal processes occurring nature science engineering 
training local search school thought determine network weights local search methods 
typical examples backpropagation algorithm developed rumelhart training feedforward neural networks realtime recurrent learning rtrl algorithm developed williams zipser training rnns 
local search methods gradient information network error function 
commonly believed gradient information train neural networks difficulties escaping local optima search surface rugged finding better solutions surface plateaus gradient zero example deciding search direction gradient information readily available lack target signals example 
difficulties non gradient searching approaches evolutionary search proposed 
training evolutionary search school thought train neural networks evolutionary search 
genetic algorithms evolutionary programming evolution strategies typical examples evolutionary search 
attempts training feedforward neural networks evolutionary search include fogel yao liu montana davis 
attempts evolve recurrent networks angeline mcdonnell 
applying evolutionary search complex types neural networks high order networks example review evolving neural networks provided 
back fogel provided various evolutionary search algorithms 
generally population candidate solutions ranked performance maintained updated iteratively evolutionary search 
candidate solution represents neural network weights encoded string binary floatingpoint numbers 
performance solution determined network error function optimized evolutionary search 
local search evolutionary search maintains population potential solutions single solution 
risk getting stuck local optima smaller 
gradient information required evolutionary search applicable problems gradient information unavailable cases search surface contains plateaus 
approaches combining local evolutionary search iterative process evolutionary search requires evaluation large number candidate solutions consequently evolutionary search usually slower local search 
lack fine tuning operations evolutionary search limits accuracy final solution 
combining local evolutionary search training neural networks local search higher risk getting stuck local optima applications limited cases gradient information readily available 
limitations overcome training performed evolutionary search 
training evolutionary search usually slow process 
obviously search strategies strengths weaknesses 
possible way constructing efficient hybrid algorithm allow search strategies complement 
chapter possibilities creating efficient hybrid training algorithms combining efforts local search evolutionary search investigated 
attempts combining local evolutionary search belief better results achieved combining local search evolutionary search various attempts adopt synergetic approach construct train neural networks 
achieved results resulting hybrid algorithms efficient 
attempts differ local search applied differences summarized section 
nature local search local search aims searching better solutions neighborhood current solution 
different local search methods typical examples 
stochastic methods maniezzo proposed hybrid algorithm evolving feedforward networks 
maniezzo networks enhanced local search method similar simplex procedure linear programming 
specifically local search embedded evolutionary search kind evolutionary operators 
evolutionary operator crossover mutation local search operator selected fixed probability 
operator optimizes new binary encoded offspring binary strings parent solutions corresponding fitness values works follows 
suppose fitness values ku mak siu parents sorted best parent worst 
ith bit offspring denoted set set negation local search method simple inclusion operator able improve evolution process 
gruau whitley proposed local search method compared different approaches combining local search evolutionary search 
hybrid algorithms boolean neural network represented grammar tree string floating point numbers binary string specifies number nodes connectivity network weights 
activation node value weight restricted gamma 
local search applied new offspring weights connecting network outputs changed local search 
certain extent local search method similar hebbian learning 
weight connecting output node associated variable initialized zero applying local search 
training patterns fed network time application local search increased activations nodes output node clamped target output weight decreased 
training patterns fed final values deciding weights flipped 
subset weights signs opposite variable selected consideration flipping 
weight largest absolute value subset flipped flipped small probabilities 
gruau whitley showed simple local search speed evolution process 
mcdonnell proposed hybrid algorithm combines method wets evolutionary search evolving rnns 
iteration evolutionary search set offspring generated applying wets method set generated perturbing parent solutions normal distribution 
wets method directs search comparing fitness ffix gamma ffix parent solution ffix normally distributed offset 
precisely ffix better ffix chosen offspring solution gamma ffix offspring better worse new solutions different ffix tried systematically 
solutions produced frequently ffix increased enlarge search step ffix decreased refine search 
promising results obtained search method efficient 
finding solution may require large number iterations iteration requires evaluation fitness ffix gamma ffix computationally intensive 
approaches combining local evolutionary search gradient methods backpropagation algorithm wellknown gradient algorithm training feedforward neural networks 
common combine backpropagation evolutionary search construct hybrid algorithms 
example miller backpropagation applied iteratively network generated evolutionary search 
iteration backpropagation gradient search surface calculated network weights changed direction opposite gradient 
computationally expensive large number iterations required find acceptable network 
promising results obtained combining backpropagation evolutionary search fast variants backpropagation required speed hybrid algorithms 
considering computational trade offs local evolutionary search braun adopted fast backpropagation algorithm rprop local search method 
hybrid algorithm evolutionary search interleaved gradient local search 
experimental results show hybrid algorithm able produce high quality networks 
conjugate gradient widely gradient local search methods 
methods conjugate gradient different backpropagation series conjugate search directions generated optimization current direction affect optimization previous directions 
skinner proposed hybrid algorithm conjugate gradient train networks evolutionary search 
experimental results demonstrate hybrid algorithm shorten training time 
recipients local search criteria select candidate solutions local search 
applying local search final fine tuning montana davis attempted local search fine tune feedforward networks evolutionary search 
best network encoded string floatingpoint numbers obtained fixed amount evolutionary search backpropagation algorithm 
algorithm different standard backpropagation weights updated adaptive step size proportional magnitude error gradient 
experimental results show network performance improved short period followed period improvement 
montana davis concluded combination local evolutionary search provide significant improvement 
ku mak siu applying local search preferred individuals performed experiments train feedforward networks hybrid algorithm evolutionary search interleaved hillclimber 
iteration evolution process offspring solutions produced evolutionary operators taken hillclimbing fitness 
weight networks encoded binary string hillclimbing achieved bit flipping 
specifically significant bits string flipped round robin manner 
bit inversion change kept improves fitness 
hillclimbing terminated improvement obtained 
experimental results showed hillclimber achieve small fitness improvement iteration 
local search method computationally expensive fitness evaluated flipped bit 
benefit gained local search little 
unbiased application local search belew investigated efficiency combining local evolutionary search proposed hybrid algorithms apply local search offspring 
offspring encoded form binary strings specifies initial weight vector searching better networks backpropagation begins 
range initial weights sigma explored evolutionary search smaller range weights local search 
rationale evolutionary search find initial weight vector ability gradient algorithms finding satisfactory solutions heavily influenced initial weight vector 
experimental results demonstrate evolutionary search able find initial weights backpropagation solutions hybrid algorithms better ones evolutionary search backpropagation multiple random restarts 
evolutionary search find initial weights hybrid algorithm spends time applying backpropagation 
bit symmetry problem studied belew backpropagation applied offspring epochs 
complex problems require far epochs 
affect hybrid algorithm efficiency considerably 
combination approaches attempts combining local search evolutionary search categorized synergetic approaches 
include phase approach lamarckian evolution approaches baldwin effect 
phase approach kitano adopted phase approach train feedforward networks classification problems 
kitano evolution approaches combining local evolutionary search ary search find regions contain global optimum local search final fine tuning operator 
evolutionary search terminated network performance reaches pre defined threshold value indicates proximity global optimum 
best network taken training local search 
backpropagation applied iteratively acceptable solution 
kitano training process improved phase approach 
similarly belew genetic algorithms gas find initial weights feedforward networks trained backpropagation second phase 
hybrid algorithms phase approach proposed skinner algorithms effective training feedforward networks solve function interpolation problems 
investigations approach 
studies mentioned illustrated phase approach capability training tasks generally difficult local search find solutions successfully 
difficult training tasks benefit approach unclear 
necessary evaluate phase approach capability applying test problem solved local search 
lamarckian evolution lamarckian evolution inheritance acquired characteristics obtained learning 
approach adopted hybrid algorithm proposed yao liu evolve feedforward networks 
iteration evolution process hybrid algorithm selects network population trains backpropagation fixed number epochs 
training improves network performance trained network associated fitness put back population evolution 
mechanism preserves acquired characteristics obtained learning similar lamarckian evolution 
promising results lamarckian evolution reported braun constructing feedforward networks solve classification problems 
braun argued fine tuning network fast backpropagation algorithm reduce search space set local optimal points saddle points finding global optimum efficient 
noteworthy previous studies lamarckian evolution mentioned typically employ local search methods high computational complexity 
introduce serious burden hybrid algorithms 
studies demonstrated capability lamarckian evolution show actual time improvement making real benefit combining local evolutionary search ku mak siu difficult observe 
chapter evaluates capability lamarckian learning comparing actual time taken experiments 
baldwin effect biological systems school thought biologically plausible mechanism baldwin effect 
contrast lamarckian learning baldwinian learning allow parent pass learned characteristics offspring fitness learning retained 
hinton nowlan baldwinian learning evolving neural networks 
experiments random search applied neural network generated evolutionary search 
random search change network fitness updated reflect distance global optimum 
experimental results show hybrid algorithm able find global optimum unachievable evolutionary search 
ackley littman baldwinian learning assist evolution adaptive agents struggle survival artificial ecosystem 
behavior agent specified evaluation neural network action neural network 
ackley littman evolution learning produces ill behaved agents unfit survival ecosystem causing extinction adaptive agents short time 
hand baldwinian learning behaved agents long lasting populations produced 
ackley littman argued baldwin effect beneficial evolution allows agents stay longer ecosystem 
capability baldwinian learning demonstrated suggested baldwin effect circumstances lead inefficient hybrid algorithms :10.1.1.18.2428
prompt investigate efficiency baldwinian learning determine situations degrade hybrid algorithms performance 
attempts apart investigations attempts examining relationship local evolutionary search neural networks 
works provide bases modeling learning evolution biological organisms order understand complex behavior 
studies evolutionary search find optimal learning parameter set local search methods learning rate momentum term backpropagation algorithm 
ambitious works include investigation evolution local search methods 
example delta rule feedforward neural networks successfully evolved 
approaches combining local evolutionary search long term dependency problem previous combining gradient algorithms evolutionary search simple task parity symmetry tasks task readily solved local search classification tasks function interpolation tasks 
gradient algorithms successfully train neural networks training tasks little incentive evolutionary search methods 
situations gradient algorithms difficulties finding appropriate neural network 
long term dependency problem typical example 
sequence recognition tasks speech recognition handwriting recognition grammatical inference involve long term dependencies output depends inputs occurred long time ago 
tasks depend mainly long term dependencies accurately represented extracting dependencies data easy task 
rnns provide promising solution problem researchers shown commonly gradient algorithms difficulty learning long term dependencies 
long term dependency problem chapter defined follows 
required learn temporal relationship output time depends inputs time gamma gamma 
assume input sequence contains symbols drawn symbol set symbol represented binary number bits 
possible input sequences ae fa symbols symbol set 
symbol input sequence input symbols fixed 
corresponding output sequences ae words input symbol time output time input symbol time output time time intervals output predicts input 
training sequence formed concatenating randomly chosen input output sequences 
problem increasingly difficult temporal length increases experiments chapter length time steps sufficiently difficult algorithms 
shorter temporal length time steps example problem easily solved gradient algorithms consequently training evolutionary search unnecessary 
ku mak siu output node km fm ng ym ym xm mg fig 

fully connected recurrent neural network inputs processing nodes output fully connected rnn solve long term dependency problem 
fig 
shows typical rnn inputs processing nodes output 
parameters defined follows signal applied input node time step actual output processing node time step set indices representing input nodes including bias 
set indices representing processing nodes 
set indices representing output nodes 
target output processing node time step activation processing node time step ij weight connecting node node activation processing node network weighted sum current inputs feedback signals kp kq output processing node time step network initialized nonlinear activation function defined gammas approaches combining local evolutionary search performance neural networks measured network error function typically defined sum squared error actual network outputs target values fixed period 
fd gamma denote instantaneous squared error network time step network error function period total better network performs period smaller value network error function 
typically provided time step remaining unknown parameters estimated minimizing network error function 
assuming network size fixed nonlinear activation functions adjustable parameters parameters required optimized weights ij different types training algorithms developed determine weights 
experiments rnns see fig 
input nodes twelve processing nodes dedicated output nodes learn long term dependency problem temporal length time steps 
total theta theta weights required optimized 
cellular genetic algorithms gas described fig 
optimize weights rnns 
weights encoded strings floating point numbers 
population size random walk steps cellular ga able find acceptable solutions long term dependency problem 
average performance simulations running sun sparc workstation cellular ga shown fig 
forms baseline performance comparing various hybrid approaches described sections 
local search methods order improve evolution process long term dependency problem different local search methods incorporated cellular ga local search methods performance long term dependency problem described evaluated section 
local search methods backpropagation time investigated previous studies :10.1.1.14.3139
performance hybrid algorithms produced methods satisfactory 
ku mak siu procedure ck chromosome xk yk xk yk coordinate grid xk yk particular relationship cnew newly produced chromosome length random walk total number chromosomes population ij weights ij network corresponding ck ck fitness ck set indexes representing input nodes including bias set indexes representing processing nodes initialize population chromosomes ck evaluate corresponding fitness ck generate new chromosome reproduction cycle repeat randomly select grid choose parent ca random walk originating create random walk set fc jxk gamma xk jy gamma yk gamma select ca ca best random walk choose parent random walk originating create random walk set fc jx gamma jy gamma gamma jx gamma jy gamma select best random walk apply crossover ca produce cnew cnew ij ae ca ij probability ij probability endloop apply mutation cnew randomly selecting processing node network weight connected input part node changed exponentially distributed mutation randomly select cnew ij ae cnew ij ffi probability cnew ij gamma ffi probability ffi positive number randomly generated exponential source gammax endloop replace cnew better fitness evaluate cnew cnew cnew termination condition reached endproc fig 

procedure cellular ga approaches combining local evolutionary search real time recurrent learning real time recurrent learning rtrl algorithm calculates instantaneous error gradient ij gamma gamma ij defined instantaneous squared error time step sensitivity yk ij obtained recursion ij ffi ki kq ij yk ij ffi ki kronecker delta 
rtrl algorithm gradient algorithm weights changed time step direction opposite instantaneous error gradient 
computationally intensive computational complexity time step number processing nodes 
delta rule running time rtrl algorithm scales poorly network size 
order reduce computational complexity propose update weights connect output nodes 
specifically compute gradient ij node output node 
simplified ij ae output node 
equivalent delta rule feedforward networks 
dynamics network remain unchanged updates weights feedforward architecture 
philosophy approach lower computational complexity eliminating term kq yq ij 
applying local search long term dependency problem set control experiments performed train rnns local search 
limitation gradient algorithms rtrl delta rule clearly demonstrated fig 

simulation runs mean square errors mses quickly reduced value improvement obtained training 
ku mak siu achievable mse long term dependency problem minutes rtrl delta rule cellular ga fig 

mse average simulations best network various gradient local search methods 
performance cellular ga illustrated 
hand fig 
shows cellular ga capable solving problem 
average mse attained minutes simulations generations lower local search methods 
phase approach cellular gas viable training algorithms neural networks training cellular gas may require long computation time typical problem evolutionary search 
order shorten training time improve solution quality different combinations local search cellular gas investigated chapter 
intuitive approach combining efforts local search cellular gas phase approach 
phase approach cellular ga phase roughly locate global optimum 
aim avoid local optima local search may get stuck 
phase terminates mse best network population reaches pre defined threshold 
local search applied fine tune best network second phase order accelerate search process 
section points difficult regions mse search space threshold ensures cellular approaches combining local evolutionary search ga moved solutions difficult regions 
overcome barriers hinders gradient local search increase chance finding satisfactory solution second phase 
achievable mse long term dependency problem minutes pure cga phase rtrl fig 

mse average simulations achieved phase approach cellular ga applied phase rtrl applied second phase 
pre defined threshold switching phases set 
performance rtrl algorithm pure cellular ga cga shown 
fig 
shows pure cellular ga learning outperforms ga rtrl hybrid algorithm 
minutes simulations cellular ga achieves significantly lower significance calculated student tests average mse hybrid algorithm 
twophase approach experiment improve evolution process produce better solution quality compared applying rtrl 
different hybrid algorithms phase approach constructed different threshold values example replacing rtrl algorithm second phase delta rule 
hybrid algorithms outperform cellular ga ku mak siu lamarckian evolution lamarckian evolution approach combining evolutionary search local search 
inheritance acquired characteristics individual pass characteristics observed phenotype acquired learning offspring genetically encoded genotype lamarckian hybrid algorithms local search rtrl delta rule applied newly born offspring generation 
application local search offspring fitness changed offspring corresponding weights weights result genetic operations replaced weights obtained learning genetic operations 
fig 
shows embedding rtrl cellular ga appropriate performance cga rtrl poor 
rtrl algorithm computationally intensive fitness improvement obtained learning compensate loss computation time 
hand performance significantly better cga average mse attained minutes attained pure cellular ga suggests embedding delta rule cellular ga merits 
advantage embedding delta rule considerably saves computation time 
example pure cellular ga takes minutes attain mse 
evolve network accuracy cga requires minutes suggesting computation time saved 
computational complexity delta rule low rnn considered feedforward network error gradient computed 
delta rule simple error gradient obtained algorithm may inaccurate 
result fitness chromosome deteriorate application delta rule 
despite deficiency low computational complexity delta rule shorten training time delta rule embedded cellular ga baldwin effect learning takes place phenotype space lamarckian evolution requires inverse mapping phenotype space neural networks genotype space strings floating point numbers impossible biological systems impractical complex genotypes phenotypes relations neural networks represented grammar trees 
time involved learning taken account hybrid algorithm achieve lower mse compared pure cellular ga approaches combining local evolutionary search achievable mse long term dependency problem minutes cga rtrl mse pure cga mse cga mse fig 

mse average simulations best network lamarckian hybrid algorithms 
average mses minutes simulation shown parentheses 
approach baldwin effect biologically plausible applicable different situations 
lamarckian evolution learning approach modify genotypes directly 
fitness replaced learned fitness fitness learning 
learning chromosome associated learned fitness fitness fitness learning 
characteristics learned phenotype space genetically specified evidence baldwin effect able direct genotypic changes 
order investigate efficiency baldwinian learning experiments similar section performed 
local search rtrl delta rule applied newly born offspring generation 
learning baldwinian mechanism lamarckian mechanism 
fig 
illustrates performance lamarckian baldwinian hybrid algorithms rtrl learning method 
evidently lamarckian hybrid algorithms outperform baldwinian counterparts 
table shows time involved learning taken consideration baldwinian hybrid algorithms rtrl perform poorly compared pure cellular ga conjecture suggested explaining phenomenon 
ku mak siu achievable mse long term dependency problem minutes pure cga mse lamarckian learning mse learning mse fig 

mse average simulations best network lamarckian baldwinian hybrid algorithms rtrl applied generations 
average mses minutes simulation shown parentheses 
table 
mses attained generations different baldwinian hybrid algorithms 
results average simulation runs cga rtrl mses average simulation runs long computation time required 
baldwinian hybrid algorithms average mses pure cga cga rtrl cga difficult genetic operations crossover mutation produce changes genotypes corresponding fitness learned fitness poorer performance baldwinian learning 
baldwinian learning learned fitness chromosome fitness obtained learning 
learned fitness equal fitness corresponding genotype 
genetic operations required produce change genotype change correspond difference fitness learned fitness 
approaches combining local evolutionary search genotypic changes produced randomly crossover mutation may match phenotypic changes caused learning 
gene weight allowed changed learning difficult genetic operations produce change 
rtrl algorithm weights changed consequently difficult genetic operations produce corresponding changes weights 
conjecture baldwinian hybrid algorithms perform poorly time spent learning considered 
achievable mse long term dependency problem minutes pure cga mse lamarckian learning mse baldwinian learning mse fig 

mse average simulations best network lamarckian baldwinian hybrid algorithms delta rule applied offspring generated generation 
average mses minutes simulation shown parentheses 
inefficiency baldwinian learning illustrated fig 
delta rule embedded cellular ga hybrid algorithm achieves significantly lower significance mse generations shown table 
indicates computation time concern hybrid algorithm merits 
particular interest situation occurs rtrl embedded cellular ga baldwinian mechanism 
recall main difference rtrl delta rule smaller number changeable weights 
learned fitness obtained changing gene keeping genes fixed 
ku mak siu consequently relatively easy genetic operations produce changes weights caused simplified learning methods 
conjecture baldwinian hybrid algorithms delta rule outperform rtrl 
evidence support conjecture 
generalization performance desirable trained networks generalization performance 
words networks capability processing unseen patterns 
long term dependency problem training sequence formed concatenation randomly chosen input output sequences 
compare generalization performance test sequence comprising randomly chosen input output sequences determine misclassification rate chance misclassifying input sequence trained rnns 
results tabulated table 
table 
comparisons generalization performance average simulations 
training algorithms average mses training misclassification pure cga cga lamarckian cga rtrl lamarckian minutes simulations rnns trained pure cellular ga average misclassification rate 
rnns trained cga lamarckian solution quality improved corresponding misclassification rate reduced 
trained network able solve long term dependency problem 
interesting explore capability cga lamarckian difficult long term dependency problem temporal length increased time steps 
fig 
demonstrates despite substantial increase complexity evolution process improved embedding delta rule cellular ga discussion training neural networks local search gradient algorithms difficult 
instance algorithms may difficulties escaping local optima search surface rugged finding better solutions surface plateaus deciding search approaches combining local evolutionary search mse minutes pure cga cga lamarckian fig 

mse average simulations best network achieved long term dependency problem temporal length time steps 
complexity problem rnn trained input nodes processing nodes dedicated output nodes population size increased 
direction gradient information readily available 
calls development alternative training algorithms evolutionary search 
training evolutionary search requires long computation time 
possible reduce computation time combining efforts local search evolutionary search 
chapter reviewed number previous attempts combine local evolutionary search 
compared different approaches combining search strategies 
phase approach evolutionary search locate roughly region global optimum phase local search accelerate local convergence second phase 
experimental results indicate evolutionary search able find network local search start inefficient local search method second phase degrade performance 
chapter suggests success phase approach depends factors efficiency locating promising regions phase efficiency finding acceptable solution second phase 
factor particularly hard fulfill difficult problems long term dependency problem 
threshold switching algorithm second phase important 
finding optimal value problem dependent ku mak siu thresholds difficult 
factors limit applicability phase approach 
phase approach investigated baldwinian approach combining local evolutionary search 
baldwinian hybrid algorithms satisfactory performance 
particular baldwinian hybrid algorithm rtrl inferior evolutionary search time involved learning taken account 
observations suggest baldwinian learning may inefficient evolving neural networks especially local search methods change network weights 
conjecture proposed explain inefficiency baldwinian learning level difficulties genetic operations produce genotypic changes match phenotypic changes due learning significantly affect baldwin effect 
conjecture suggests weights changed baldwinian learning changes large resulting hybrid algorithms better evolutionary search 
possible phenotypic changes obtaining genotypic phenotypic matches difficult 
evaluated lamarckian approach combining local evolutionary search 
able speed training process improve solution quality 
findings observing simulation runs long term dependency problem minutes 
experiments required see findings applicable situations 
factors example lamarckian learning detrimental adaption neural networks changing environment affecting efficiency lamarckian approach 
investigations required clarify benefit lamarckian evolution 
lamarckian hybrid algorithms investigated delta rule achieves best performance 
noteworthy delta rule simple able find solution 
low computational complexity suitable embedded cellular ga suggests local search methods need sophisticated order obtain benefit combining evolutionary search local search 
experimental results simple hypothetical problem similar phenomenon observed difficult benchmark problem inverted pendulum problem 
suggests idea combining simple local search evolutionary search viable 
possible extension apply hybrid algo acceptable solution long term dependency problem obtained minutes simulation 
longer simulation time expected problems longer dependency larger values 
approaches combining local evolutionary search rithms difficult real world problems known unsolvable conventional methods 
acknowledgment part supported hong kong polytechnic university gv 

ackley littman 
interactions learning evolution 
langton taylor farmer rasmussen editors artificial life pages 
redwood city ca addison wesley 

ackley littman 
case lamarckian evolution 
langton editor artificial life pages 
reading ma addison wesley 

angeline saunders pollack 
evolutionary algorithm constructs recurrent neural networks 
ieee transactions neural networks 

back hammel 
schwefel 
evolutionary computation comments history current state 
ieee transactions evolutionary computation 

baldwin 
new factor evolution 
american naturalist 

belew mcinerney schraudolph 
evolving networks genetic algorithm connectionist learning 
langton taylor farmer rasmussen editors artificial life pages 
redwood city ca addison wesley 

bengio simard frasconi 
learning long term dependencies gradient descent difficult 
ieee transactions neural networks 

braun 
hybrid approach optimizing neural networks evolution learning 
davidor 
schwefel manner editors parallel problem solving nature ppsn iii pages 
berlin springer verlag 

chalmers 
evolution learning experiment genetic connectionism 
touretzky editor proceedings connectionist models summer school pages 
san mateo ca kaufmann 

collins jefferson 
selection massively parallel genetic algorithms 
proceedings fourth international conference genetic algorithms pages 


artificial evolution generalized class adaptive processes 
ai workshop evolutionary computation pages 


dantzig 
linear programming extensions 
princeton nj princeton university press 

davidor 
naturally occurring niche species phenomenon model results 
proceedings fourth international conference genetic algorithms pages 
ku mak siu 
de garis 
gennets genetically programmed neural networks genetic algorithm train neural nets inputs outputs vary time 
proceedings ieee international joint conference neural networks pages 

elman 
finding structure time 
technical report crl center research language university california san diego 


short term load forecasting genetically optimized neural network cascaded modified kohonen clustering process 
proceedings ieee international symposium intelligent control pages 

fogel 
simulated evolutionary optimization 
ieee transactions neural networks 

fogel 
evolutionary computation new philosophy machine intelligence 
piscataway nj ieee press 

fogel fogel porto 
evolving neural networks 
biological cybernetics 

fogel owens walsh 
artificial intelligence simulated evolution 
new york wiley 

meir 
evolving learning algorithm binary perceptron 
network 

french 
genes baldwin effect learning evolution simulated population 
rodney pattie editors artificial life pages 
cambridge ma mit press 

goldberg 
genetic algorithms search optimization machine learning 
reading ma addison wesley 

greenwood 
training partially recurrent neural networks evolutionary strategies 
ieee transactions speech audio processing 

gruau whitley 
adding learning cellular development neural networks evolution baldwin effect 
evolutionary computation 

krishnamurthy 
acoustic phonetic mapping recurrent neural networks 
ieee transactions neural networks 

harp samad guha 
genetic synthesis neural networks 
schaffer editor proceedings third international conference genetic algorithms pages 
kaufmann 

harvey 
new factor evolution 
evolutionary computation 

hebb 
organization behavior 
new york wiley 

hinton nowlan 
learning guide evolution 
complex systems 

hornik 
approximation capabilities multilayer feedforward neural networks 
neural networks 

huang lippmann 
neural net traditional classifiers 
anderson editor neural information processing systems pages 
new york american institute physics 

takano 
reasoning learning method fuzzy rules neural networks adaptive structured genetic algorithm 
approaches combining local evolutionary search proceedings ieee international conference systems man cybernetics pages 

janson 
application genetic algorithms training higher order neural networks 
journal systems engineering 

janson 
training product unit neural networks genetic algorithms 
ieee expert 

jordan 
attractor dynamics parallelism connectionist sequential machine 
proceedings eighth annual conference cognitive science society pages 

stork 
evolution learning neural networks number distribution learning trial affect rate evolution 
lippmann moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 

kim jung kim park 
fast learning method back propagation neural network evolutionary adaptation learning rates 


kitano 
empirical studies speed convergence neural network training genetic algorithms 
proceedings eighth national conference artificial intelligence pages 

kolen pollack 
back propagation sensitive initial conditions 
complex systems 


training neural networks means genetic algorithms working long chromosomes 
international journal neural systems 

ku 
combination local evolutionary search training recurrent neural networks 
phd thesis hong kong polytechnic university hong kong 

ku mak 
exploring effects lamarckian baldwinian learning evolving recurrent neural networks 
proceedings ieee international conference evolutionary computation pages 

ku mak 
empirical analysis factors affect baldwin effect 
eiben back schoenauer 
schwefel editors parallel problem solving nature ppsn pages 
berlin springer verlag 

ku mak siu 
study lamarckian evolution recurrent neural networks 
ieee transactions evolutionary computation 

ku mak siu 
adding learning cellular genetic algorithms training recurrent neural networks 
ieee transactions neural networks 

lee 
line recognition totally unconstrained handwritten numerals multilayer cluster neural network 
ieee transactions pattern analysis machine intelligence 

lippmann 
computing neural nets 
ieee acoustics speech signal processing magazine pages 

maniezzo 
genetic evolution topology weight distribution neural networks 
ieee transactions neural networks 
ku mak siu 

landscapes learning costs genetic assimilation 
evolutionary computation 

mcdonnell 
evolving recurrent perceptrons time series modelling 
ieee transactions neural networks 

menczer parisi 
evidence hyperplanes genetic learning neural networks 
biological cybernetics 

merelo pat prieto mor 
optimization competitive learning neural network genetic algorithms 
proceedings international workshop artificial neural networks pages 

michalewicz 
genetic algorithms data structures evolution programs 
berlin springer verlag 

miller todd hegde designing neural networks genetic algorithms 
schaffer editor proceedings third international conference genetic algorithms pages 
san ca morgan kaufmann 

mitchell 
genetic algorithms 
cambridge ma mit press 

montana davis 
training feedforward neural network genetic algorithms 
proceedings eleventh international joint conference artificial intelligence pages 

mozer 
induction multiscale temporal structure 
moody hanson lippmann editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 

nolfi elman parisi 
learning evolution neural networks 
adaptive behavior 


self tuning neuro pid control applications 
proceedings ieee international conference systems man cybernetics pages 

giles 
rule revision recurrent neural networks 
ieee transactions knowledge data engineering 

paredis 
coevolutionary life time learning 

voigt ebeling rechenberg 
schwefel editors parallel problem solving nature ppsn iv pages 
berlin springer verlag 

parisi nolfi 
influence learning evolution 
belew mitchell editors adaptive individuals evolving populations models algorithms pages 
reading mass addison wesley 

pineda 
generalization backpropagation recurrent neural networks 
physical review letters 

port 
representation recognition temporal patterns 
connection science 

porto fogel fogel 
alternative neural networks training methods 
ieee expert 

rechenberg 
evolution strategy nature way optimization 
optimization methods applications possibilities limitations volume lecture notes engineering 
berlin springer verlag 

riedmiller braun 
direct adaptive method faster backpropagation learning rprop algorithm 
proceedings international conference neural networks pages 
approaches combining local evolutionary search 
rudolph 
global optimization means distributed evolution strategies 
schwefel manner editors parallel problem solving nature ppsn pages 
berlin springer verlag 

rumelhart hinton williams 
learning internal representations error propagation 
rumelhart mcclelland pdp research group editors parallel distribution processing explorations microstructure cognition 
vol 
foundation 
cambridge ma mit press 

saravanan fogel 
evolving neural control systems 
ieee expert 

sasaki tokoro 
adaptation changing environments various rates inheritance acquired characters comparison darwinian lamarckian evolution 
proceedings second asia pacific conference simulated evolution learning pages 


schwefel 
evolution optimum seeking 
new york wiley 

sejnowski rosenberg 
parallel networks learn pronounce english text 
complex systems 

skinner 
neural networks computational materials science training algorithms 
modelling simulation materials science engineering 


wets 
minimization random search techniques 
mathematics operations research 

turney 
myths legends baldwin effect 
proceedings workshop evolutionary computing machine learning th international conference machine learning pages 

waibel 
modular construction time delay neural networks speech recognition 
neural computation 

whitley 
genetic algorithm tutorial 
statistics computing 

whitley gordon mathias 
lamarckian evolution baldwin effect function optimization 
davidor 
schwefel manner editors parallel problem solving nature ppsn iii pages 
berlin springer verlag 

whitley starkweather 
genetic algorithms neural networks optimizing connections connectivity 
parallel computing 

wieland 
evolving neural network controllers unstable systems 
proceedings international joint conference neural networks pages 

williams zipser 
experimental analysis real time recurrent learning algorithm 
connection science 

williams zipser 
learning algorithm continually running fully recurrent neural networks 
neural computation 

wu chen lee 
cache genetic modular fuzzy neural networks robot path planning 
proceedings ieee international conference systems man cybernetics pages 

yao 
evolving artificial neural networks 
proceedings ieee 

yao liu 
new evolutionary system evolving artificial neural networks 
ieee transactions neural networks 
