similarity estimation word cooccurrence probabilities ido dagan fernando pereira bell laboratories mountain ave murray hill nj usa dagan research att 
com pereira research att 
com applications natural language processing necessary determine likelihood word combination 
example speech recognizer may need determine word combinations eat eat beach 
statis tical nlp methods determine likelihood word combination frequency training cor pus 
nature language word combinations infrequent occur corpus 
propose method es probability previously unseen word combinations available information sim ilar words 
describe probabilistic word association model distributional word similarity apply improving probability estimates unseen word bi grams variant katz back model 
similarity method yields perplexity im provement prediction unseen bigrams sta significant reductions speech recognition er ror 
data sparseness inherent problem statistical methods natural language processing 
meth ods statistics relative frequencies config elements training corpus evaluate alternative analyses interpretations new samples text speech 
analysis taken contains frequent config 
problem data sparseness arises analyses contain configurations occurred training corpus 
possible estimate probabilities observed frequencies estimation scheme 
focus particular kind configuration word cooccurrence 
examples cooccurrences include relationships head words syntactic constructions verb object adjective noun exam ple word sequences grams 
commonly models probability estimate previously un seen cooccurrence function probability esti lillian lee division applied sciences harvard university oxford st cambridge ma usa das harvard edu mates words cooccurrence 
example bigram models study probabil ity wl conditioned word occurred training conditioning word wl calculated probability estimated frequency corpus jelinek mercer roukos katz 
method depends independence assumption cooccurrence wl frequent higher estimate wl regardless wl 
class similarity models provide alternative independence assumption 
models relationship words mod analogy words sense similar ones 
brown 
suggest class gram model words similar cooccurrence distri butions clustered word classes 
cooccurrence probability pair words estimated ac cording averaged cooccurrence probability corresponding classes 
pereira tishby lee propose soft clustering scheme certain grammatical cooccurrences membership word class probabilistic 
cooccurrence probabil ities words modeled averaged cooccur rence probabilities word clusters 
dagan markus markovitch argue reduction relatively small number predetermined word classes clusters may cause substantial loss information 
similarity model avoids clus tering altogether 
word modeled specific class set words simi lar nearest neighbor approaches pattern recognition 
scheme predict unobserved cooccurrences 
model probabilistic provide probability estimate unobserved cooccurrences 
com plete probabilistic framework gram language models probabilistic lexicalized grammars schabes lafferty sleator temperley 
give similarity method estimating probabilities cooccurrences unseen training 
similarity estimation language modeling cooccurrence smoothing method es sen steinbiss derived tic model smoothing sugawara 

different method takes starting point back scheme katz 
allocate appropriate probability mass unseen cooccurrences back method 
redistribute mass unseen cooccurrences av cooccurrence distribution set similar conditioning words relative entropy sim ilarity measure 
second step replaces independence assumption original back model 
applied method estimate unseen bigram probabilities wall street journal text compared standard back model 
testing held sample similarity model achieved reduction perplexity unseen bigrams 
constituted just test sample leading re duction test set perplexity 
exper application language modeling speech recognition yielded statistically signifi cant reduction recognition error 
remainder discussion terms bigrams valid types word 
discounting redistribution low probability bigrams missing finite sample 
aggregate probability unseen bigrams fairly high new sample contain 
data sparseness reliably maximum likelihood estimator mle bigram prob abilities 
mle probability bigram wi simply pml wi wi frequency wi train ing corpus total number bigrams 
estimates probability unseen hi gram zero clearly undesirable 
previous proposals circumvent problem jelinek mercer roukos katz church gale take mle ini tial estimate adjust total probability seen bigrams leaving probabil ity mass unseen bigrams 
typically adjustment involves interpolation new estimator weighted combination mle estimator guaranteed nonzero unseen bigrams discounting mle decreased model unreliability small frequency counts leaving probability mass unseen bigrams 
back model katz provides clear separation frequent events observed frequencies reliable probability estimators low frequency events prediction involve addi tional information sources 
addition back model require complex estimations inter parameters 
hack model requires methods discounting estimates previously observed events leave positive probability mass unseen events redistributing unseen events probabil ity mass freed discounting 
bigrams resulting estimator general form wl wi wt wl pr wt pd represents discounted estimate seen bigrams model probability redistribution unseen bigrams normalization factor 
mass left unseen bigrams starting wi wi normalization ew wl wi factor required ensure wl pr wi second formulation normalization compu preferable total number pos sible bigram types far exceeds number observed types 
equation modifies slightly katz tion include placeholder pr alternative models distribution unseen bigrams 
katz uses turing formula replace actual frequency wi bigram event general discounted frequency wi de fined wi wl nc wl nc wl nc number different bigrams cor pus frequency uses discounted frequency conditional probability calculation bigram wi pa wt wl original turing method free probability mass redistributed uniformly unseen events 
katz back scheme redistributes free probability mass non uniformly proportion frequency set ting pr katz assumes conditioning word wl probability unseen word proportional unconditional probability 
form model depend assumption investigate esti mate wl derived averaging estimates conditional probabilities follows words distributionally similar wl 
similarity model scheme assumption words similar wl provide predictions distribution wl unseen bigrams 
wl denote set words similar wl determined similarity metric 
define psim wl similarity model condi tional distribution wl weighted average conditional distributions words wl psim wl wl fll wl unnormalized weight determined degree similarity wl 
ac cording scheme follow wl tends follow words similar wl 
complete scheme necessary define simi larity metric accordingly wl wl 
pereira tishby lee measure word similarity relative entropy kullback leibler kl distance corre sponding conditional distributions ii wl log lw kl distance wl increases distribution similar 
compute nonzero esti mates wl necessary de fined 
estimates standard back model satisfy requirement 
application similarity model averages standard back estimates set similar condi words 
define wl set nearest words wl excluding wl satisfy wl ii parameters control contents wl tuned experimentally see 
wl defined wl exp wl ii weight larger words similar closer wl 
parameter fl controls relative contribution words different distances wl value fl increases nearest words wl get rel weight 
fl decreases remote words get larger effect 
tuned experimentally 
having definition psim wl directly pr wl back scheme 
better smooth psim wl inter unigram probability recall katz pr wl 
linear get wl psim lwl experimentally determined interpolation parameter 
smoothing appears compensate inaccuracies wl mainly infrequent conditioning words 
evaluation low shows values small similarity model plays stronger role independence assumption 
summarize construct similarity model wl interpolate 
interpolated model back scheme pr wl obtain better estimates unseen bi grams 
parameters tuned experimentally relevant process determine set similar words considered deter mines relative effect words de importance similarity model 
evaluation evaluated method comparing perplexity effect speech recognition accuracy base line bigram back model developed mit lincoln laboratories wall journal wsj text dictation corpora provided arpa hlt pro grain paul 
baseline back model follows closely katz design compactness frequency bigrams ignored 
counts ill model obtained words wsj text years 
perplexity evaluation tuned similarity model parameters minimizing perplexity ad ditional sample words wsj text drawn arpa hlt development test set 
best parameter values 
values improvement perplexity unseen bigrams held thou sand word sample bigrams unseen just 
improvement unseen perplexity conditional bigram probability model respect true bigram distribution information theoretic measure model quality jelinek mercer roukos empirically esti mated exp log tu test set length intuitively lower perplexity model model assign high probability bigrams occur 
task lower perplexity indicate better prediction unseen bigrams 
arpa wsj development corpora come ver sions verbalized punctuation 
experiments 
training reduction test reduction table perplexity reduction unseen bigrams different model parameters bigrams corresponds test set perplexity improvement 
table shows reductions training test perplexity sorted training reduction different choices num ber closest neighbors 
values best ones equation clear computational cost applying similarity model unseen bi gram 
lower values computationally preferable 
table see reducing incurs penalty perplexity improvement relatively low values appear sufficient achieve benefit similarity model 
table shows best value increases decreases lower greater weight condi tioned word frequency 
suggests predic tive power neighbors closest modeled fairly frequency conditioned word 
bigram similarity model tested lan guage model speech recognition 
test data experiment pruned word lattices wsj closed vocabulary test sentences 
arc scores lattices sums acoustic score negative log language model score case negative log probability provided baseline bi gram model 
lattices constructed new lattices arc scores modified similar ity model baseline model 
compared best sentence hypothesis original lattice modified counted word disagree ments hypotheses correct 
total disagreements 
similarity model correct cases back model 
advantage similarity model cally significant level 
reduction error rate small number disagreements small compared values fl refer base logarithms expo calculations 
number errors current recognition setup 
table shows examples speech recognition disagreements models 
hypotheses labeled back similarity bold face words errors 
similarity model able model better regularities semantic parallelism lists avoiding past tense form 
hand similarity model mistakes function word inserted place punctuation written text 
related cooccurrence technique essen steinbiss earlier stochastic speech modeling sugawara 
main previous attempt similarity estimate prob ability unseen events language modeling 
addi tion original language modeling speech recognition grishman sterling applied cooccurrence smoothing technique estimate selectional patterns 
outline main parallels differences method cooccurrence smoothing 
detailed analy sis require empirical comparison methods corpus task 
cooccurrence smoothing method base line model combined similarity model refines probability estimates 
sim ilarity model cooccurrence smoothing intuition similarity words measured confusion probability pc lw substituted arbi context training corpus 
baseline probability model taken mle confusion probability pc lwl conditioning words wl defined pc wl probability wl followed context words 
bigram estimate derived commitments leaders felt point dollars commitments leaders fell point dollars followed bv france agreed italy followed france greece italy aide cooccurrence smoothing necessity change exist necessity change exists additional reserves reported additional reserves reported darkness past church darkness passed church ps wl lw pc notice formula form sim ilarity model uses confusion proba bilities normalized weights 
addition restrict summation sufficiently similar words cooccurrence smoothing method sums words lexicon 
similarity measure symmetric sense pc lw pc identical fre pc quency normalization pc contrast asymmetric weighs context proportion probability occur rence wq way comparable frequencies sharper context distribution greater 
similarity model play stronger role estimating vice versa 
properties motivated choice relative entropy similarity measure intuition words sharper distributions infor words words flat distri butions 
presentation corresponds model essen steinbiss 
presentation follows equiv model averages similar conditioned words similarity defined preceding word context 
fact equivalent models symmetric treatment conditioning conditioned word rewritten ps lwl table speech recognition disagreements models wl wl iw wl consider definitions confusion probabil ity smoothed probability estimate yielded best experimental results 
similarity model missing bigrams back scheme essen steinbiss linear interpolation bi grams combine cooccurrence smoothing model mle models bigrams unigrams 
notice choice back interpolation independent similarity model 
research model provides basic scheme probabilistic similarity estimation developed directions 
variations may tried different similarity metrics different weight ing schemes 
simplification current model parameters may possible especially re spect parameters select near est neighbors word 
substantial variation base model similarity con words similarity con words 
evidence may combined similarity estimate 
instance may advantageous weigh estimates measure re liability similarity metric neighbor distributions 
second possibility take ac count negative evidence wl frequent followed may statistical evidence put upper bound estimate wl 
may require adjustment similarity estimate possibly lines rosenfeld huang 
third similarity estimate smooth likelihood estimate small nonzero frequencies 
similarity estimate relatively high bigram receive higher estimate predicted uniform discount ing method 
similarity model may applied configurations bigrams 
trigrams necessary measure similarity differ ent conditioning bigrams 
done directly measuring distance distributions form wl corresponding different bigrams wl 
alternatively practically possible define similarity measure bi grams function similarities correspond ing words 
types conditional cooccur rence probabilities probabilistic pars ing black 
configuration question includes words possible model bigrams 
configuration includes elements nec essary adjust method lines discussed trigrams 
similarity models suggest appealing approach dealing data sparseness 
corpus statistics provide analogies words agree linguistic domain intuitions 
new model implements similarity approach provide estimates conditional probabilities unseen word cooccur fences 
method combines similarity estimates katz back scheme widely language modeling speech recognition 
scheme originally proposed preferred way implementing independence assumption suggest appropriate implementing similarity models class models 
enables rely direct maximum likelihood estimates reliable statistics available re sort estimates indirect model 
improvement achieved bigram model statistically significant modest ef fect small proportion unseen events 
bigrams easily accessible plat form develop test model substantial improvements obtainable informa tive configurations 
obvious case tri grams sparse data problem severe 
longer term goal apply similarity techniques linguistically motivated word cooccurrence configurations suggested lexical ized approaches parsing schabes lafferty sleator temperley 
configurations verb object adjective noun evidence pereira tishby lee sharper word cooccurrence distributions obtainable leading improved predictions similarity techniques 
acknowledgments slava katz discussions topic doug mcilroy detailed comments doug paul wsj trigrams test set trigrams occur words training doug paul personal communication 
help baseline back model andre michael riley providing word lattices experiments 
black fred jelinek john lafferty david magerman david mercer salim roukos 

history grammars richer mod els probabilistic parsing 
th annual meet ing association computational linguistics pages columbus ohio 
ohio state university association computational linguistics morris town new jersey 
brown peter vincent della pietra peter desouza lai robert mercer 

class gram models natural lan guage 
computational linguistics 
church kenneth william gale 

comparison enhanced turing deleted estimation methods estimating probabilities english bigrams 
computer speech language 
dagan ido shaul markus shaul markovitch 

contextual word similarity estimation sparse data 
th annual meeting computational linguistics pages columbus ohio 
ohio state university asso ciation computational linguistics morristown new jersey 
essen ute volker steinbiss 

smoothing stochastic language modeling 
pro ceedings icassp volume pages 
ieee 

population frequencies species estimation population parameters 
biometrika 
grishman ralph john sterling 

smoothing automatically generated selectional constraints 
human language technology pages san francisco california 
advanced research projects agency software intelligent systems technology office morgan kaufmann 
jelinek frederick robert mercer salim roukos 

principles lexical language mod eling speech recognition 
furui mohan sondhi editors advances speech sig nal processing 
mercer dekker pages 
katz slava 
estimation probabilities sparse data language model component speech recognizer 
ieee transactions acoustics signal processing 
lafferty john daniel sleator davey temperley 

grammatical trigrams aa probabilistic model link grammar 
robert goldman editor aaai fall symposium probabilistic approaches natu ral language processing cambridge massachusetts 
american association artificial intelligence 
paul douglas 
experience stack decoder hmm csr back gram lan guage models 
proceedings speech nat ural language workshop pages palo alto california february 
defense advanced research projects agency information science technol ogy office morgan kaufmann 
pereira fernando naftali tishby lil lee 

distributional clustering english words 
oth annual meeting association computational linguistics pages urn bus ohio 
ohio state university association computational linguistics morristown new jersey 
rosenfeld ronald huang 

im stochastic language modeling 
darpa speech natural language workshop pages new york february 
mor gan kaufmann san mateo california 
yves 

stochastic tree adjoining grammars 
proceeedings th international conference computational linguis tics nantes france 
sugawara nishimura kaneko 

isolated word recognition hidden markov models 
proceedings icassp pages tampa florida 
ieee 

