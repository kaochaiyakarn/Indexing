refinement approach handling model misfit text categorization wu tong heng bing liu li school computing national university singapore science drive singapore comp nus edu sg text categorization classification automated assigning text documents pre defined classes contents 
problem studied information retrieval machine learning data mining 
far effective techniques proposed 
techniques underlying models assumptions 
data fits model classification accuracy high 
data fit model classification accuracy low 
propose refinement approach dealing problem model misfit 
show need change classification technique underlying model flexible 
propose successive refinements classification training data correct model misfit 
apply proposed technique improve classification performance simple efficient text classifiers rocchio classifier na bayesian classifier 
techniques suitable large text collections allow data reside disk need scan data build text classifier 
extensive experiments benchmark document corpora show proposed technique able improve text categorization accuracy techniques dramatically 
particular refined model able improve na bayesian rocchio classifier prediction performance average 
keywords text categorization na bayesian classifier rocchio algorithm 
increasing volume text data various online sources important task categorize classify text documents manageable easy understand categories 
text categorization classification aims automatically assign categories classes unseen text documents 
task commonly described follows set labeled training permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
sigkdd july edmonton alberta canada 
documents classes system uses training set build classifier employed classify new documents classes 
problem studied extensively information retrieval machine learning natural language processing 
past research produced text classification techniques na bayesian classifier rocchio algorithm support vector machines 
existing techniques automatically catalog news articles classify web pages learn reading interests users 
automatic text classifier save considerable time human effort particularly aiding human indexers produced large database categorized document collection 
classification techniques underlying models assumptions 
data fits model classification accuracy high 
underlying model fit data performance resulting classifiers quite poor 
instance na bayesian classifier assumes text documents generated mixture model oneto correspondence mixture components classes 
assumption seriously violated real world applications prevention junk mails junk mails generally contain multiple sub topics adult content various unrelated business letters 
oneto correspondence assumption hold 
cases na bayesian classifier suffers problem model misfit 
hand may explain model misfit due feature space heterogeneity 
feature space heterogeneity occurs best features base classification different different regions feature space 
widely classification techniques na bayesian algorithm rely measures computed features entire training data build classifiers performances inevitably affected averaging effect entire training space 
resulting classifiers sub optimal due complex nature training data 
serious model misfit leads poor classification performance 
unstable classifiers decision trees neural networks boosting algorithms enhance basic weak learners 
generally believed boosting effective stable classifiers linear classifiers rocchio algorithm 
na bayesian classifier relatively stable respect small changes training data 
techniques meta learning copyright acm 
bing liu department computer science university illinois chicago cs uic edu 
complement improve individual classifiers combining multiple classification techniques relatively complicated mechanisms 
novel simple method deal problem model misfit 
proposed technique training errors successively refine classification model training data 
learning model misfit generally leads high training error classifiers na bayesian rocchio algorithms 
prediction errors training data retrain sub classifier training examples predicted class learning method 
way force classifiers learn refined regions training data making model stronger fit training data better 
technique flexible needs classification method change method way 
original classification model improved successively splitting training data re learning sub models 
applying technique enhance probabilistic classifiers na bayesian classifier linear classifiers rocchio overfitting generally problem 
training data basically consistent test data training classification model better corresponding classification performance test instances improves 
apply proposed technique improve classification performance simple efficient text classifiers rocchio classifier na bayesian classifier 
extensive experiments benchmark document corpora show proposed technique able improve text categorization accuracy techniques dramatically 
techniques suitable large text collections allow data reside disk need scan data build classifier 
resulting classifiers technique efficient faster state art approaches svm adaboost gaining prediction performance 
results show refined na bayesian classifier outperforms proven superior classifier svm high memory requirements needs data memory 
furthermore svm converges slowly large data sets 
rest organized follows 
section discusses related section reviews existing text categorization techniques related 
section describes proposed technique detail 
experimental results section followed concluding remarks section 

related improving prediction accuracy text classifiers important issue 
studies conducted area 
popular frameworks meta learning classifier committees integration multiple learning models achieve higher accuracy 
combination different learning approaches represent integration different learning biases complement inefficient characteristics 
pioneer applying meta learning data mining done littlestone warmuth 
proposed weighted majority algorithms combining different classifiers 
chan stolfo adapted methods learn weights validation set 
techniques learn arbiter arbitrate predictions generated different classifiers combiner merge predictions classifiers 
similar direction includes stacked generation combining multiple rule sets bayesian utility theory 
technique different meta learning approach 
simply refine classifier training data 
metalearning voting involved process 
require multiple classification techniques 
popular framework adaptive resampling adaptively selects instances labelled training set improve classification accuracy 
selection process biases favour misclassified data 
particular boosting algorithm adaptively resamples data biasing misclassified examples training set combine predictions set classifiers 
course execution assigns different importance weights different training tuples 
weak learning algorithm takes weights consideration algorithm progresses training documents hard classified correctly get incrementally higher weights documents easy classify get lower weights 
effect forces weak learning algorithm concentrate documents misclassified previously 
freund schapire adaptive boosting adaboost algorithm reported superior voting method 
studies implementing adaboost various classifiers 
elkan provided framework apply boosting na bayesian classifier 
kim uses na bayes classifier weak learner boosting allowing boosting algorithm utilize term frequency information maintaining probabilistic accurate confidence ratio 
clear evidence improve na bayes classifier 
hand ting reported boosting na bayesian classifier 
show proposed technique remarkably improve performance na bayesian classifier 
refined nb classifier superior adaboost decision stumps 
terms tree strategy handle data heterogeneity problem apte introduced importance profile angle ipa split feature space 
compute ipa value feature 
value exceeds suitable threshold indication heterogeneity 
training data recursively split feature gives largest ipa value 
friedman introduced hybrid approach classification combining aspects nearest neighbor treestructured recursive partitioning techniques 
consists strategies 
successive splitting procedure 
begins entire input measurement space divides regions input variables attributes 
splitting criterion maximizes estimated relevance 
employs alternative splitting strategy respective variables influence split proportion estimated relevance winner takes approach 
method different need sophisticated extra mechanisms select variables split data 
kohavi reported hybrid system na bayesian nb decision tree 
algorithm similar decision algorithm leaf nodes created nb classifiers nodes predicting single class 
uses validation set determine nb form leaf 
approach differs aspect require complex combination mechanism integration different classification techniques algorithm level 
classification results partition training data apply single classification technique 
need validation set 

text categorization techniques review commonly text classification methods 
apply technique baseline algorithms comparison experiments 
rocchio algorithm early text classification technique information retrieval rocchio algorithm originally designed relevance feedback 
widely document classification 
algorithm documents represented popular vector space representation 
building classification achieved constructing document vectors prototype vector cr class cj 
normalized document vectors relevant examples class irrelevant examples class summed 
prototype vector computed weighted difference summation 
parameters adjust relative impact relevant irrelevant training examples 
buckley recommended 
classification test document simply cosine measure compute similarity prototype class vector particular class assigned class vector similar naive bayesian classifier naive bayesian nb method effective technique text classification 
shown perform extremely practice researchers 
set training documents document considered ordered list words 
di denote word position document word vocabulary 
vocabulary set words consider classification 
set predefined classes consider class classification 
order perform classification need compute posterior probability class document 
bayesian probability multinomial model laplacian smoothing wt wt wt di count number times word wt occurs document di cj di depending class label document 
assuming probabilities words independent class obtain na bayesian classifier naive bayesian classifier class highest assigned class document 
support vector machine years support vector machine svm shown accurate classification method text documents 
svm relatively new approach introduced vapnik solving class pattern recognition problems 
structural risk minimization principle error bound analysis theoretically motivated 
method defined vector space problem find decision surface best separates data vectors classes 
joachims provides theoretical empirical evidences svm suitable text categorization 
compared svm classification methods showed svm outperformed methods tested experiments 
adaboost adaboost proposed schapire learning algorithm generates multiple classifiers uses build ultimate classifier 
suited text categorization problem 
instance set classifiers built weak learner adaboost produces final classifier combining hypotheses weak classifiers number iterations weight hypothesis calculated equation original adaboost 
schapire singer introduced boostexter system text classification employs adaboost level decision tree decision stumps weak learner reported excellent results 

proposed technique section describe framework proposed technique analyse approach solve model misfit problem na bayesian classifier rocchio classifier 
algorithm consider binary text classification assigns document dj positive class complement negative class 
theoretically binary text classification general multi class multiclass classification problem transformed set independent binary ones 
text classification task set pre labelled training examples choose classification technique cl na bayesian classifier base classifier proposed technique 
entire training set learning initial classifier cl classify positive negative classes respectively 
split subsets dp dn consisting predicted positive documents negative documents respectively 
resulting predicted examples generally contain errors predicted positive class true positive false positive documents 
learn sub classifier clp dp training examples predicted positive class 
sub classifier clp refiner original classifier cl 
combine clp cl apply clp classifier predicted positive documents produced cl 
denote combined classifier cl contribution clp evaluated comparing classification performances cl cl training data 
cl outperforms cl keep sub classifier clp refiner original classifier cl discard clp 
similar refining process done predicted negative documents produced cl output refined classifier cln 
recursively carry learning process build refinement tree illustrated 
tree structure proposed framework effectiveness text classifiers commonly evaluated measuring recall precision score computed 
technique score employed text classification 
score training data evaluate resulting sub classifiers clp cln process building refinement tree 
number true positive predictions number false positive predictions number false negative predictions number true negative predictions note interested positive class 
recall precision defined positive class cl cl cl 
score positive class computed follows gives equal weight recall precision pr proposed technique recursively build refinement tree root node 
root node comprises example documents training set node build base classifier cl selected classification technique training data node 
predictions cl split node positive negative children nodes child child compute score training data node class assignments cl build sub classifiers clp cln training data positive negative children nodes respectively compute new scores adding contributions clp cln accordingly fp ap ap bp cp fn bn cn fp output child prune branch fn output child prune branch algorithm heuristic tries refine classification model improving score training data 
resulting refinement tree score base classifier particular node smaller combined classifier produced merging node base classifier child node classifier 
node splitting process measure increase approach may reach local maximum global maximum 
experimental results show proposed technique extremely effective 
algorithm 
refiner 
build base classifier cl training data 

split cl 
build classifier cl 
build classifier cl 


refiner prune branch 
refiner prune branch constructing refinement tree technique 
sub section show proposed approach able deal problem model misfit contexts na bayesian classifier rocchio classifier 
naive bayesian nb classifier devising bayesian method text classification assumptions text documents generated mixture model mapping mixture components classes document features independent class 
researchers shown bayesian classifier performs surprisingly obvious violation 
causes difficulty hold 
real life situations correspondence mixture components classes hold 
class category may cover number sub topics 
proposed technique building original classifier entire training data build sub classifier predicted positive training examples contain false positive examples misclassified negative ones true positive ones 
predicted positive training examples similar probabilities original model 
retrain predicted positive training examples gotten rid mixture components training data classified negative class 
perform process recursively refine classification closer correspondence mixture components classes 
rocchio classifier illustrates problem model misfit rocchio algorithm represent positive negative prototype vectors respectively 
rocchio classification model assumption document assigned particular class similarity document vector prototype vector class largest 
data fit model rocchio classifier suffers see data points regions 
training instances region misclassified positive documents instances region misclassified negative documents 
proposed technique splitting entire training data initial classifier obtain situation predicted positive class 
see fits rocchio model 
original region classified correctly rocchio sub classifier 
model misfit rocchio classifier refined model predicted positive class examples training data 
experiments empirical evaluation done data corpora 
give dataset descriptions experimental settings evaluation measures 
empirical results experiments datasets 
experimental setup data corpus reuters dataset collected reuters newswire 
modapte split leading corpus training documents test documents 
potential topic categories populous positive classes 
binary text classification performed experiments 
second data collection usenet articles collected lang different newsgroups 
articles collected newsgroup 
main categories newsgroups computing comp recreation rec science sci talk talk 
perform classification sub categories 
detailed data compositions shown table appendix 
example take graphics category positive class rest comp categories os ms ibm pc mac win negative class 
posted articles entire dataset testing 
classification techniques na bayesian classifier nb rocchio classifier tested proposed approach base classifier 
attempted apply algorithm svm 
svm previously proven superior classifier yang fits training data general 
categories svm generate substantial training error technique fails refine improvement minimal 
svm baseline comparison 
technique works different feature settings 
particular performance refined classifiers outstanding fewer features 
select words highest information gains features classifiers refinement 
experiments conducted adaboost decision stumps boostexter package 
experiments conducted publicly available rainbow text classification package score positive class error evaluation measures system 
score takes account recall precision reliable suitable measure 
error classification fraction documents classified wrongly respect classes 
ratio sum numbers false positives false negatives total number documents 
metrics distributed categories kind averaging needed get global performance measures 
essentially methods perform averaging microaveraging macro averaging 
micro averaging www research att com lewis reuters html www cs cmu edu afs cs project theo www naivebayes newsgroups tar gz www research att com schapire boostexter www cs cmu edu mccallum bow individual numbers true false positives true false negatives summed evaluation measures computed 
macro averaging individual measures score error category computed average calculated total number categories 
consequently macro averaging treats categories equally microaveraging treats documents equally 
denominator error rate constant categories resulting value macro averaging micro averaging 
reuters dataset table shows experimental results reuters data set 
table columns contain error score nb classifier top categories reuters collection respectively 
columns show results refined nb classifier technique 
column provides classification improvement score column column improvement percentage refined nb classifier initial nb classifier 
similarly table shows classification performance refined rocchio classifier 
experiments top features words 
results algorithm works applied nb rocchio classifiers 
example datasets nb performs badly ship corn system successfully enhances nb overwhelming improvement 
refined classifiers average better original nb classifiers better original rocchio classifiers terms macro averaged score 
terms micro averaged score refined classifiers better original nb classifiers original rocchio classifiers respectively 
addition technique reduces error rates substantially 
observe refined classifiers able improve dataset 
refined classifiers show outstanding performances refined nb classifier gives better results refined rocchio classifier 
representative technique performance comparison 
illustrates comparison techniques benchmarks terms score 
include adaboost decision stumps boostexter nb linear svm 
yang reported linear svm provides slightly better results nonlinear models reuters dataset 
linear version representative svm comparison 
comparison features nb svm 
classifiers perform better features 
diagram clear refined nb classifiers outperform adaboost greatly categories 
general performs better svm 
rocchio refiner slightly worse svm 
terms running time system faster svm adaboost requiring linear scans incurring significantly computational cost 
categories reuters dataset optimal results algorithm obtained reaching levels refined classification tree 
training error decreases test error decreases apparently overfitting 
behavior observed usenet dataset experiments demonstrates flexibility technique 
usenet dataset tables show refinement results obtained nb classifier main newsgroups 
tables show corresponding results rocchio base classifier 
results top features words 
figures give comparison various benchmark techniques features nb svm 
experiments observe technique clearly outperforms methods significant margin 
boostexter perform usenet data set accordance results reported 
svm perform usenet data set 
possible reason svm overfits training data training errors low datasets 
refined nb classifier refined rocchio classifier suffer overfitting 
regarding running efficiency svm slower system quadratic optimization 
adaboost large number rounds boosting time consuming 
nb rocchio base classifier refined system requires linear scans data average times faster svm rainbow package 
table results refining nb classifier reuters nb refined nb error error earn acq money fx grain crude trade interest ship wheat corn table results refining rocchio classifier reuters rocchio refined rocchio error error earn acq money fx grain crude trade interest ship wheat corn earn acq money grain crude trade interest ship wheat corn comparison score reuters dataset table results refining nb classifier usenet comp comp nb error refined nb error graphics os ms ibm pc mac win table results refining rocchio classifier usenet comp comp rocchio error refined rocchio error graphics os ms ibm pc mac win table results refining nb classifier usenet rec rec nb error refined nb error autos motorcycle baseball hockey boostexter table results refining rocchio classifier usenet rec rec rocchio error refined rocchio error autos motorcycle baseball hockey table results refining nb classifier usenet sci sci nb error refined nb error crypt electronics medical space table results refining rocchio classifier usenet sci sci rocchio error refined rocchio error crypt electronics medical space nb svm nb refiner table results refining nb classifier usenet talk talk nb error refined nb error guns mideast misc religion graphics os ms ibm pc mac win autos motorcycle baseball hockey boostexter 
proposed refinement approach handle problem model misfit evident existing text categorization techniques 
approach successively refines classification model prediction errors training data 
extensive experiments conducted reuters usenet newsgroups corpora confirm technique inflexible classification model versatile outperform state art techniques svm adaboost significantly 
results reported necessarily best achieved 
room improvement finding better ways identify performance characteristics base classifier initial training phase 
approach described able achieve outstanding prediction performance complex strategies incurring significant computational costs 
nb svm nb refiner comparison score usenet rec boostexter nb svm nb refiner comparison score usenet comp table results refining rocchio classifier usenet talk talk rocchio refined rocchio error error guns mideast misc religion crypt electronics medical space boostexter 
gratefully acknowledge support star national university singapore academic research fund 
wee sun lee useful discussions 


ali pazzani error reduction learning multiple descriptions 
machine learning 

apte hong hosking pednault rosen decomposition heterogeneous classification problems intelligent data analysis 

breiman bagging predictors 
machine learning 

buckley salton allan effect adding relevance information relevance feedback environment 
proceedings seventeenth annual nb svm comparison score usenet sci guns mideast misc religion nb refiner boostexter nb svm comparison score usenet talk nb refiner acm sigir conference 

chan stolfo comparative evaluation voting meta learning partitioned data 
proceedings twelfth international conference machine learning 

chan stolfo learning arbiter combiner trees partitioned data scaling machine learning 
proceedings international conference knowledge discovery data mining 

cortes vapnik support vector networks 
machine learning 

craven dipasquo freitag mitchell nigam slattery learning extract symbolic knowledge world wide web 
proceedings aaai 

duda hart pattern classification scene analysis 

elkan boosting naive bayesian learning 
proceedings international conference knowledge discovery data mining 

freitag multistrategy learning information extraction 
proceedings fifteenth international conference machine learning 

freund schapire experiments new boosting algorithm proceedings thirteenth international conference machine learning 

friedman flexible metric nearest neighbor classification 
technical report 

guo knowledge probing distributed data mining 
advances distributed parallel knowledge discovery 

hand yu idiot bayes stupid 


hull pedersen schutze method combination document filtering 
proceedings nineteenth international conference research development information retrieval 

iyengar apte zhang active learning adaptive resampling proceedings seventh international 
conference knowledge discovery data mining 

joachims text categorization support vector machines learning relevant features 
proceedings tenth european conference machine learning 

joachims probabilistic analysis algorithm tfidf text categorization 
proceedings fourteenth international conference machine learning 

kim hahn zhang text filtering boosting naive bayes classifiers 
proceedings sigir 

kohavi scaling accuracy na bayes classifiers decision tree hybrid 
proceedings second international 
conference knowledge discovery data mining 

kumar hierarchical multi classifier system hyperspectral data analysis 
proceedings international workshop multiple classifier systems 

lam lai meta learning approach text categorization 
proceedings sigir 

lang newsweeder learning filter netnews 
proceedings international conference machine learning 

larkey croft combining classifiers text categorization 
proceedings nineteenth international acm sigir conference research development information retrieval 

lewis gale sequential algorithm training text classifiers 
proceedings sigir 

lewis ringuette comparison learning algorithms text categorization 
third annual symposium document analysis information retrieval pp 


littlestone warmuth weighted majority algorithm 
tech 
report ucsc crl uc 
santa cruz 

mccallum nigam comparison event models na bayes text classification 
aaai workshop learning text categorization 
tech 
rep ws aaai press 
pavlov mao scaling support vector machines boosting algorithm 
international conference pattern recognition 

quinlan bagging boosting 
proceedings aaai 

schapire freund bartlett lee boosting margin new explanation effectiveness voting methods 
proceedings fourteenth international conference machine learning 

schapire singer singhal boosting rocchio applied text filtering 
proceedings sigir 

schapire singer improved boosting algorithms confidence rated predictions 
machine learning 

schapire singer boostexter boosting system text categorization machine learning 

ting witten stacked generalization 
proceedings fifteenth international joint conference artificial intelligence 

ting zhen improving performance boosting naive bayesian classification 
proceedings pakdd 

vapnik nature statistical learning theory 

wolpert stacked generalization 
neural networks 

yang evaluation statistical approaches text categorization 
journal information retrieval 

yang liu re examination text categorization methods 
proceedings acm sigir conference research development information retrieval 

yang pierce combining multiple learning strategies effective cross validation 
proceedings international conference machine learning 
appendix table usenet newsgroups positive categories index group name category graphics comp comp graphics os ms comp comp os ms windows misc ibm pc comp comp sys ibm pc hardware mac comp comp sys mac hardware win comp comp windows autos rec rec autos motorcycle rec rec motorcycles baseball rec rec sport baseball hockey rec rec sport hockey crypt sci sci crypt electronics sci sci electronics medical sci sci med space sci sci space guns talk talk politics guns mideast talk talk politics mideast misc talk talk politics misc religion talk talk religion misc 
