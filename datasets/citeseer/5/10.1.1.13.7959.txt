stochastic neighbor embedding geoffrey hinton sam roweis department computer science university toronto king college road toronto canada cs toronto edu describe probabilistic approach task placing objects described high dimensional vectors pairwise dissimilarities low dimensional space way preserves neighbor identities 
gaussian centered object high dimensional space densities gaussian dissimilarities define probability distribution potential neighbors object 
aim embedding approximate distribution possible operation performed low dimensional images objects 
natural cost function sum kullback leibler divergences object leads simple gradient adjusting positions low dimensional images 
dimensionality reduction methods probabilistic framework easy represent object mixture widely separated low dimensional images 
allows ambiguous objects document count vector word bank versions close images river finance forcing images outdoor concepts located close corporate concepts 
automatic dimensionality reduction important toolkit operation machine learning preprocessing step algorithms reduce classifier input size goal visualization interpolation compression ways embed objects described high dimensional vectors pairwise dissimilarities lower dimensional space 
multidimensional scaling methods preserve dissimilarities items measured euclidean distance nonlinear squashing distances shortest graph paths isomap 
principal components analysis pca finds linear projection original data captures variance possible 
methods attempt preserve local geometry lle associate high dimensional points fixed grid points low dimensional space self organizing maps probabilistic extension gtm :10.1.1.111.3313
methods require high dimensional object associated single location low dimensional space 
difficult unfold mappings single ambiguous object really belongs disparate locations low dimensional space 
define new notion embedding probable neighbors 
algorithm stochastic neighbor embedding sne tries place objects low dimensional space optimally preserve neighborhood identity naturally extended allow multiple different low images object 
basic sne algorithm object potential neighbor start computing asymmetric probability ij pick neighbor ij exp ij exp ik dissimilarities ij may part problem definition need symmetric may computed scaled squared euclidean distance affinity high dimensional points ij jjx jj set hand experiments binary search value entropy distribution neighbors equal log effective number local neighbors perplexity chosen hand 
low dimensional space gaussian neighborhoods fixed variance set loss generality induced probability ij point picks point neighbor function low dimensional images objects expression ij exp jjy jj exp jjy jj aim embedding match distributions possible 
achieved minimizing cost function sum kullback leibler divergences original ij induced ij distributions neighbors object ij log ij ij kl jjq dimensionality space chosen hand number objects 
notice making ij large ij small wastes probability mass distribution cost modeling big distance high dimensional space small distance low dimensional space cost modeling small distance big 
respect sne improvement methods lle som widely separated data points collapsed near neighbors low dimensional space :10.1.1.111.3313
intuition sne emphasizes local distances cost function cleanly enforces keeping images nearby objects nearby keeping images widely separated objects relatively far apart 
differentiating tedious affects ij normalization term eq 
result simple ij ij ji ji nice interpretation sum forces pulling pushing away depending observed neighbor desired 
gradient possible ways minimize just begun search best method 
steepest descent points adjusted parallel inefficient get stuck poor local optima 
adding random jitter decreases time finds better local optima method examples quite slow 
initialize embedding putting low dimensional images random locations close origin 
minimization methods including annealing perplexity discussed sections 
application sne image document collections graphic illustration ability sne model high dimensional near neighbor relationships dimensions ran algorithm collection bitmaps handwritten digits set word author counts taken scanned proceedings nips conference papers 
datasets intrinsic structure fewer dimensions raw dimensionalities handwritten digits author word counts 
set digit bitmaps ups database examples classes 
variance gaussian point dimensional raw pixel image space set achieve perplexity distribution high dimensional neighbors 
sne initialized putting random locations close origin trained gradient descent annealed noise 
sne information class labels quite cleanly separates digit groups shown 
furthermore region low dimensional space sne arranged data properties orientation skew stroke thickness tend vary smoothly 
embedding shown sne cost function eq 
value nats uniform distribution lowdimensional neighbors cost log nats 
applied principal component analysis pca data projection principal components separate classes nearly cleanly sne pca interested getting large separations right causes boundaries similar classes 
experiment digit classes similar pairs 
classes available dimensions sne cleanly separate similar pairs 
applied sne word document word author matrices calculated text nips volume papers 
shows map locating nips authors dimensions 
authors published nips vols 
shown dot position sne larger red dots corresponding names authors published papers period 
distances ij computed norm difference log aggregate author word counts summed nips papers 
authored papers gave fractional counts evenly authors 
words occurring documents included stopwords giving vocabulary size 
bow toolkit part pre processing data 
set achieve local perplexity neighbors 
sne grouped authors broad nips field generative models support vector machines neuroscience reinforcement learning vlsi distinguishable localized regions 
full mixture version sne clean probabilistic formulation sne easy modify cost function single image high dimensional object different versions low dimensional image 
alternative versions mixing proportions sum 
image version object location mixing proportion low dimensional neighborhood distribution mixture distributions induced image versions image versions potential neighbor ij jc exp jjy jc jj kd exp jjy kd jj multiple image model derivatives respect image locations straightforward derivatives mixing proportions easily expressed result running sne algorithm dimensional grayscale images handwritten digits 
pictures original data vectors scans handwritten digit shown location corresponding low dimensional images sne 
classes quite separated sne information class labels 
furthermore class properties orientation skew tend vary smoothly space 
points shown produce display digits chosen random order displayed region display centered location digit embedding overlap regions digits displayed 
sne initialized putting random locations close origin trained batch gradient descent see eq 
annealed noise 
learning rate 
iterations point jittered adding gaussian noise standard deviation position update 
jitter reduced iterations 
dayan sejnowski jordan hinton williams ghahramani bengio lecun graf simard denker guyon vapnik smola muller scholkopf solla bishop jaakkola tishby zemel mozer pouget sun giles lee chen meir alspector mjolsness rangarajan gold jabri seung lee harris murray ruppin mead horn bialek li brown ruderman mel cowan baird saad platt bartlett shawe taylor williamson singh barto kearns saul singer tresp leen moody wolpert opper barber morgan viola nowlan movellan cottrell waibel bourlard lippmann doya bell spence maass moore thrun principe obermayer sutton kawato warmuth atkeson cohn kowalczyk amari abu mostafa yang kailath stork baldi smyth mackay pomerleau touretzky tenenbaum bower koch barber moore thrun warmuth abu mostafa cohn atkeson goodman tesauro ahmad pentland lippmann schraudolph touretzky pomerleau maass baluja chauvin munro sanger shavlik lewicki schmidhuber baldi omohundro mackay smyth robinson krogh buhmann hertz pearlmutter tenenbaum cottrell movellan kailath yang wiles embedding nips authors dimensions 
authors published nips vols 
show dot location sne algorithm 
larger red dots corresponding names authors published papers period 
inset upper left shows blowup crowded boxed central portion space 
dissimilarities authors computed squared euclidean distance vectors log aggregate author word counts 
authored papers gave fractional counts evenly authors 
words occurring documents included stopwords giving vocabulary size 
nips text data available www cs toronto edu roweis data html 
terms jc probability version picks version jc jc exp jjy jc jj kd exp jjy kd jj effect ij changing mixing proportion version object ij mg mi mg jc mg mg mj jc mi 
effect changing mg cost mg ij ij ij mg optimizing mixing proportions directly easier perform unconstrained optimization softmax weights defined exp exp 
proof concept implemented simplified mixture version object represented low dimensional space exactly components constrained mixing proportions 
components pulled force increases linearly threshold separation 
threshold force remains constant 
ran experiments simplified mixture version sne 
took dataset containing pictures digits added hybrid digit pictures constructed picking new examples classes pixel random parents 
minimization hybrids non hybrids significantly different locations mixture components 
mixture components hybrid lay regions space devoted classes parents region devoted third class 
example perplexity defining local neighborhoods step size position update times gradient constant jitter 
simple mixture version sne possible map circle line losing near neighbor relationships introducing new ones 
points near cut point circle mapped mixture points near line near 
obviously location cut dimensional circle gets decided pairs mixture components split stochastic optimization 
certain optimization parameters control ease mixture components pulled apart single cut circle 
parameter settings circle may fragment smaller line segments topologically correct may linked 
example hybrid digits demonstrates primitive mixture version sne deal ambiguous high dimensional objects need mapped widely separated regions low dimensional space 
needs done sne efficient cope large matrices document word counts dimensionality reduction method know promises treat homonyms sensibly going back original documents disambiguate occurrence 
threshold 
threshold force nats unit length 
low space natural scale variance gaussian determine ij fixed 
practical optimization strategies current method reducing sne cost steepest descent added jitter slowly reduced 
produces quite embeddings demonstrates sne cost function worth minimizing takes hours find embedding just datapoints clearly need better search algorithm 
time iteration reduced considerably ignoring pairs points ij ji ij ji small 
matrix ij fixed learning natural replacing entries certain threshold zero renormalizing 
pairs ij ji zero ignored gradient calculations ij ji small 
turn determined logarithmic time size training set sophisticated geometric data structures trees ball trees ad trees ij depend ky computational physics attacked exactly complexity performing multibody gravitational electrostatic simulations example fast multipole method 
mixture version sne appears interesting way avoiding local optima involve annealing jitter 
consider components mixture object far apart low dimensional space 
raising mixing proportion lowering mixing proportion move probability mass part space appearing intermediate locations 
type probability wormhole way avoid local optima arise cluster low dimensional points move bad region space order reach better 
search method success toy problems provide extra dimensions low dimensional space penalize non zero values dimensions 
search sne extra dimensions go lower dimensional barriers penalty dimensions increased cease effectively constraining embedding original dimensionality 
discussion preliminary experiments show find optima annealing perplexities high jitter reducing jitter final perplexity reached 
raises question sne doing variance gaussian centered high dimensional point big distribution neighbors uniform 
clear high variance limit contribution ij log ij ij sne cost function just important distant neighbors close ones 
large shown sne equivalent minimizing mismatch squared distances spaces provided squared distances object normalized subtracting mean mismatch ij ij ij ij jjx jj log exp ik ij jjy jj log exp ik number objects 
mismatch similar stress functions nonmetric versions mds enables understand large variance limit sne particular variant procedures 
investigating relationship metric mds pca 
sne seen interesting special case linear relational embedding lre 
lre data consists triples colin mother victoria task predict third term 
lre learns dimensional vector object nxn dimensional matrix relation 
predict third term triple lre multiplies vector representing term matrix representing relationship uses resulting vector mean gaussian 
predictive distribution third term determined relative densities known objects gaussian 
sne just degenerate version lre relationship near matrix representing relationship identity 
summary new criterion stochastic neighbor embedding mapping high dimensional points low dimensional space stochastic selection similar neighbors 
self organizing maps low dimensional coordinates fixed grid high dimensional ends free move sne high dimensional coordinates fixed data low dimensional points move 
method applied arbitrary pairwise dissimilarities objects available addition high dimensional observations 
gradient sne cost function appealing push pull property forces acting bring closer points selecting points selecting neighbor 
shown results applying algorithm image document collections sensibly placed similar objects nearby low dimensional space keeping dissimilar objects separated 
importantly probabilistic formulation sne ability extended mixtures ambiguous high dimensional objects word bank widely separated images low dimensional space 
acknowledgments anonymous referees visitors poster helpful suggestions 
yann lecun provided digit nips text data 
research funded nserc 
cox cox 
multidimensional scaling 
chapman hall london 
tenenbaum 
mapping manifold perceptual observations 
advances neural information processing systems volume pages 
mit press 
tenenbaum de silva langford 
global geometric framework nonlinear dimensionality reduction 
science 
roweis saul :10.1.1.111.3313
nonlinear dimensionality reduction locally linear embedding 
science 
kohonen 
self organization associative memory 
springer verlag berlin 
bishop williams 
gtm generative topographic mapping 
neural computation 
hull 
database handwritten text recognition research 
ieee transaction pattern analysis machine intelligence may 
jolliffe 
principal component analysis 
springer verlag new york 
yann lecun 
nips online web site 
nips org 
andrew mccallum 
bow toolkit statistical language modeling text retrieval classification clustering 
www cs cmu edu mccallum bow 
hinton 
learning distributed representations concepts relational data linear relational embedding 
ieee transactions knowledge data engineering 
