fast recovery trump high reliability 
armando fox stanford university david patterson university california berkeley stanford berkeley recovery oriented computing project roc widely accepted equation availability availability mttf mttf mttr mttf mean time failure system subsystem reciprocal reliability mttr mean time recovery number 
equation suggests improve availability decrease mttr example just valuable increase mttf founding observation recovery oriented computing roc project 
argue interactive internet applications decrease mttr valuable corresponding increase mttf improve availability amount case adopting mttr primary metric reasoning system availability focusing designs fast recovery 
availability equation characterizes percent availability system nines system expect meaning system available time 
common interpretation availability represents probability request system successfully serviced 
today data centers internet sites achieve availabilities meaning mttf mttr 
observations summarize claims supporting argument lowering mttr valuable increasing mttf 
case hardware today component mttf high directly measuring requires system years operation customers afford largely rely vendor claims assess impact mttf availability 
hand mttr directly measured hardware software making mttr claims independently verifiable 

user interactive services web sites lowering mttr directly affect user experience outage 
particular reducing mttr shown reduce impact cost specific outage especially redundancy failover mask failure 
contrast increasing mttf may reduce frequency failures probability user experience failure session capture impact particular outage user experience cost service provider 

low mttr completely mask transient failure may possible structure service level service gracefully degrades partial failure recovery 
give simple performability inspired argument degree degradation small duration degraded service recovery time short frequency degraded service intervals low experience suggests users willing tolerate graceful degradation 
observation translated directly techniques fall back degradation recovering failure avoid unscheduled downtime inducing failure recovering rejuvenation 
direct measurability relevance mttr today hardware mttf large largest customers measure 
example disks quoted mttf years verifying claims requires system years operation 
verification complicated hardware manufacturers exclude operator error environmental failures calculations account unplanned downtime cluster mainframe installations half unplanned downtime selection contemporary internet services 
case software mttf vary days months mttr varies minutes hours 
study randomly selected internet servers observed skewed mttf distribution median days mean days 
studies took steps isolate near destination failures opposed near middle failures network infrastructure server updates requiring manual operator intervention relatively rare defensible order estimates internet host software mttf 
contrast finds typical internet host mttr minutes longest mttr complex commercial database products typically mid size internet service installations order hours making software mttr claims verifiable 
hardware software mttf mttr support recovery benchmarking starting appear 
related complication resulting high mttf duration observation window 
example year microsoft blamed operator error hour outage affecting major web sites 
wanted achieve nines availability despite outage web sites need operate outage free years 
fact claim stretch credibility meaningful businesses wait years assess customer satisfaction need finer grained ongoing metrics things system availability perceived customers 
summarize arguments follows high mttf guarantee interval mttf infinite mttf best measures frequency happen chance happen observation window chance outage occur month 
measurable cost service provider increased likelihood outage impact specific outage show 
mttf capture outage cost increasing mttf directly lead lowering outage costs lowering mttr mitigate impact failure measured specific incident 
support claim identifying thresholds mttr 
user benefit lower mttr april ebay hour outage affecting services 
ebay suffered unplanned outages year mttf half year availability formula considering unplanned outages hours hours 
long outages impact fact ebay suffered hour outage april affected certain auction categories hour outage june hour outage may hour outage may hour outage december 
may outage caused software related failures 
user loyalty investor confidence june outage investors dropped ebay stock price market capitalization fell incurred direct costs credits affected ebay users auctions reported substantial short term rise number users 
contrast site suffer unplanned minute outage day availability 
minute outage week availability hours hours 
availability achieved shorter outages affect smaller percentage site subscribers simultaneously subscribers large service site time 
example america online registered email users typical hour outage affects users 
outages affect small percentage users affecting users valuable reputation 
especially true cost customer switching competitor relatively low online book retailers switching cost slightly higher ebay users accommodated credibility points ebay profiles higher email services 
claim user interactive services exist specific mttr thresholds user experience appreciably disrupted transient failure improvement threshold yields marginal benefit intolerable users give click competitor 
human computer interaction literature find thresholds regarding user perception latency interacting computing system 
miller user wait sec system reacts command perceives system sluggish wait sec gets distracted move task 
small study user perception www site latency industry analyst research confirmed sec page view approximate threshold users complain site excessively slow presumably give 
users considered site performance acceptable page view latency sec 
write sec tok sec initial approximation 
relevant sites contain machinery automatic fast failover event front node failure existing commercial load balancing products automatically 
steady state response latency site operating partial failure conditions long mttr tok mttr tok user perceive site excessively slow failure completely masked reason invest significant effort lowering mttr impact failure negligible 
tok mttr user perceive click give move site 
may incremental benefit improving mttr impact failure depends individual user perceptions 
ebay outage policy free day extension affected auction listings outage exceeds hours site typically receives bids minute items auction 
variance associated sec number high authors unable conclude exists uniform specific latency users tolerate 
threshold acceptable higher users patient forgiving incremental page loads page views 
mttr user give 
impact failure temporary permanent loss customer competitor 
tok technology independent assess impact achieving particular absolute levels mttr mttr 
note transient failures handled automatically outage requires human intervention effectively sets lower bound resolution time measured tens minutes 
exploiting observation goal design service goal managed operations illustrate example subsequent sections 
low mttr degradation vs outages transient failures resolved simple techniques way takes time longer example rebooting typical cluster node takes minute 
failures masked simply added latency take advantage key property large scale internet services users may willing tolerate forms service degradation latency long amount degradation bounded duration degraded service bounded incidence degraded service rare 
particular claim users willing tolerate temporary service degradation long degraded quality quality greater threshold quality min duration degradation exceed threshold probability user experience degradation visit site exceed frequency degraded service intervals low point view user 
performability analysis provides formal way constructing probability measure captures varying quality level system performs observation period 
originally designed capture behaviors long running systems proposed performability applied internet services capturing example incremental latency experienced user requests high server load probability user request simply rejected admission control high server load 
generalize notion degradation cases response delivered successfully acceptable latency quality answer perfect answer imprecise stale requires service software structured way partial failures map directly specific user visible deviations service behavior 
fortunately existence proofs design possible systematic set guidelines achieve design particular application give example 
duration degradation greater declare outage occurred mean transient failure perceived users real outage user tolerance temporary degradation runs user point abandon site temporarily permanently 
service service user user may trend time users move broadband connections baseline tolerance long latency may decrease 
claim useful design guideline answer questions 
determine user centric thresholds quality min service 

service quality probability degradation user measured 
specifically quantitative effect metrics partial failures occur service 
estimation quality min definition estimating thresholds requires observing user user behaviors 
direct observation interrogation may costly impractical large sites attempt indirectly infer user intentions monitoring behaviors large user populations 
example way estimate value abandonment detection user abandons incomplete task visiting site possible interpretation user patience run unwilling service temporary degradation repaired 
large sites generally forthcoming way arrive specific numbers example america online email server reportedly operates minute 
similarly thresholds quality min service specific admittedly extreme anecdotal example involves physical move inktomi search engine 
move took hours service remained online delivered search results half database quality 
lower bound quality searches touch part database frequently hit pages replicated database partitions quality typical search higher presumably quality min example harvest measure search results quality 
similarly survey jupiter media metrix reports certain classes users decrease site performance leads increase site abandonment users method accessing site dialup vs broadband result different expectations site performance 
sites anonymizer com hotmail yahoo offer free service premium pay service 
case hotmail yahoo main benefit pay service user storage space email case anonymizer faster performance 
anonymizer prioritizes premium users deliberately degrades service free users unclear introduces possibility users may willing tolerate higher values exchange service usage fee 
example quality rolling reboot specific example mapping partial failures effects quality consider inktomi search engine described 
user query hits node farm workstations 
average node contributes result search search yield hits node frequently requested pages replicated multiple nodes 
avoid reducing throughput transient node failures search engine returns results nodes responds search query prespecified timeout refer fraction search harvest search 
internet sites search engine nodes rebooted rolling basis 
assume worst case search hits nodes effect single node rebooted reduction harvest implying degraded quality 
typically large clusters 
fact lower bound quality searches may affected 
general degradation experienced users service rolling reboot period 
node takes time reboot nodes take time nr rebooted sequentially 
decrease quality perceptible practice node rebooting better case nr user service rolling reboot abandon outage long resolve 
possible remedy selectively rolling reboots chunks chunk rolling reboot degrade quality longer 
remedy determined quality min reboot nodes resulting quality harvest reduction 
long quality min nr stay user tolerance thresholds amount degradation duration incident 
rolling reboots preventive maintenance induced control setting frequency rolling reboots 
particular rolling reboot initiated period simplifying assumptions load uniform times user visits site uniformly randomly distributed user probability nr hitting site rolling reboot experiencing degraded service 
concrete day nodes seconds probability smaller scheduling reboots predictable troughs workload troughs exist 
example better case 
method extended model transient node failures node mttf approximate frequently node service user request arrives 
rolling reboots node failures independent practice reboots effect preventing failures happen probabilities hitting site rolling reboot hitting site node suffered transient failure simply added get upper bound probability user perceived degradation quality 
rolling reboot admittedly simple example controlled partial failure 
example serves show cases specific user visible effects partial failures quantified measurement tools available internet community goal capture changes user satisfaction result effects 
related begun explore fast recovery techniques recursive systems designed operate degraded fashion independent partial failures 
continuing focuses mapping abstractions needed applications directly design techniques allow partial failures map specific types service degradation 
nature internet service workloads presents reasons optimizing mttr 
requests independent short lived idempotent retrying failed request common user strategy 
patterson find common services search directory failed request retried hour retry succeeded time 
addition xie find enduser behavior tendency retry approximate retry interval considered sites lower mttr lower mttf perceived users available sites having availability mttf mtbf higher mttr 
currently sophisticated model user retry behavior direct observation tantalizing quite different approaches point similar 
anticipate synergy approaches 
tact presents general framework measuring degradation service particular case degradation measured user perceived inconsistency 
defining application specific units consistency system uses replication improve availability tradeoff consistency availability quantifiable 
similarly online aggregation treats special case compute time explicitly traded precision numerical query computation statistic extremely large dataset 
approaches assume threshold acceptable consistency precision externally determined assumed quality min 
summary raising mttf guarantee failure free interval mttf infinite lowering mttr mitigate impact failure interval 
cases build services fail retry multiple levels long recovery fast perceptible degradation occurs infrequently 
done levels front failures may recoverable time user frustrated abandon site back failures may recoverable time temporary degradation service quality appears user long lived causes user abandon site 
cases mttr directly measurable estimate impact specific outage provided estimates quality min arrived 
suggested specific methods estimating thresholds possibility inference thresholds observations large group behaviors tracking events abandonment tracking widely offered measurement services keynote 
contrast manufacturer mttf claims may difficult verify directly capture user impact outages 
systems low mttr may appear behave acceptably component mttf low long mttf sufficiently long user perceive service degradation probability greater 
focus ongoing research agenda clearly derive guidelines architecting applications partial failures fact map directly measurable effects service quality probability receiving degraded service capture degradation simple tractable analytical models similar performability 
focusing discussion lowering mttr advocating effort spent debugging operations hard failures masked redundancy achieve lower mttr 
observations measurability relevance mttr availability metric ability exploit low mttr mitigate effects partial failures suggest community aggressively pursue opportunities improvement design fast recovery direct correlations user satisfaction boot 
acknowledgments 
recovery oriented computing research group advisors anonymous reviewers amr yahoo 
peter chen univ michigan steve gribble univ washington sam bea systems lisa ibm contributions discussion draft 
nina bhatti anna alan 
integrating user perceived quality web server design 
proc 
www 
eric brewer 
lessons giant scale services 
ieee internet computing july aug 
george candea james cutler armando fox minimizing time recover recursively restartable system 
proc 
intl 
conf 
dependable systems networks dsn bethesda md june 
tim clark 
ebay recovers outage 
cnet news com may 
news com com html tim clark scott ard 
ebay blacks 
cnet news com june 
www idg net crd sun html chet 
yahoo ebay outage 
commerce times june 
www com perl story html brown patterson 
lessons pstn dependable computing 
workshop self healing adaptive self managed systems new york june 
melanie austria farmer robert 
microsoft blames technicians massive outage 
cnet news com january 
news com com html legacy cnet armando fox 
recovery oriented computing 
proc 
th intl 
conf 
large data bases vldb hong kong china august 
armando fox eric brewer 
harvest yield scalable tolerant systems 
proc 
hotos vii tucson az 
garg trivedi analysis software rejuvenation policies 
proc 
th annual conf 
computer assurance june pp 

auction news posting 
ebay outage april 
www com generic html 
ebay outage leaves users luck 
cnn com technology july 
www cnn com tech computing ebay outage idg clare 
ap summit sun altered policy ebay outage 
idg news service november 
www idg net crd sun html joseph hellerstein peter haas helen wang 
online aggregation 
proc 
intl 
conf 
management data acm sigmod phoenix az 
carl workshop dependability internet business systems aol email presentation commerce workshop intl 
conf 
dependable systems networks dsn bethesda md june 
iyer patel reliability internet hosts case study user perspective computer networks vol 
pp 

darrell long andrew muir richard golding 
longitudinal survey internet host reliability 
proc 
symposium reliable distributed systems 
jupiter research 
tying performance profit track abandonment improve loyalty 
jupiter media metrix research report stp june 
steve levin 
exploring realities internet uptime 
strategic technology consulting report july 
www com knowledge management uptime realities mcnaughton 
ebay suffers prolonged outage 
cnet news com may 
news com com html matthew dan patterson 
measuring user availability web practical experience 
proc 
intl 
performance dependability symposium held conjunction intl 
conf 
dependable systems networks dsn washington dc june 
meyer 
performability retrospective pointers 
perf 
eval 
pp 
meyer lisa 
performability utility imperative 
proc 

miller 
response time man computer conversational transactions 
proc 
afips fall joint computer conference 
david oppenheimer david patterson 
studying failure data largescale internet services 
proc 
sigops european workshop sept lisa 
systems fail 
review studies presentation ifip wg dependable computing fault tolerance st meeting st john virgin islands jan brian sullivan 
ebay suffers outages days 
cnn com sci tech april 
www cnn com tech internet ebay outages idg index html troy 
ebay hit site slowdown 
cnet news com april 
news com com html tag cd mh xie sun cao trivedi 
modeling online service availability perceived web users 
technical report center advanced computing communication duke university 
yu amin vahdat 
design evaluation continuous consistency model replicated services 
proc 
osdi 
ji zhu james mauro ira 
system recovery benchmarking 
proc dsn workshop dependability benchmarking bethesda md june 
research group 
need speed 
research report july 
