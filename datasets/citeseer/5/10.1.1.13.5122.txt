combining clustering training enhance text classification unlabelled data raskutti blackburn road clayton victoria australia raskutti team com new training strategy unlabelled data 
trains predictors parallel predictor labelling unlabelled data training predictor round 
predictors support vector machines trained data original feature space trained new features derived clustering labelled unlabelled data 
standard training methods method require priori existence redundant views classification dependent availability different supervised learning algorithms complement 
evaluated method classifiers text benchmarks webkb reuters newswire articles newsgroups 
evaluation shows training technique improves text classification accuracy especially number labelled examples 

text classification problem automatically assigning electronic text documents pre specified categories 
typically text classification systems learn models categories large training corpus labelled data order classify new examples 
due tedious subjective nature manual labelling labelled examples difficult expensive obtain whilst unlabelled documents plentiful easy obtain 
common method utilising unlabelled data classification training 
method input data create predictors complementary 
predictor classify unlabelled data train new corresponding complementary predictors 
typically complementarity achieved redundant views data different supervised learning algorithms :10.1.1.32.4821:10.1.1.114.9164
training methods applicable limited class problems 
permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
sigkdd edmonton alberta canada copyright acm 
herman blackburn road clayton victoria australia herman team com adam kowalczyk blackburn road clayton victoria australia adam kowalczyk team com new training strategy unlabelled data 
method creates new features derived clustering labelled unlabelled data 
new features include membership information similarity cluster centroids populous clusters 
new input feature space creates alternate view data training supervised learning algorithm original feature space 
evaluation strategy classifiers text benchmarks indicates extremely useful improving accuracy especially number labelled examples 
organised follows 
section describe related research 
section describes method cotraining features derived clustering 
section describe experimental setup corpora evaluation metrics 
results experiments section discuss implications section 
section summarises contributions 

related research various researchers shown unlabelled data useful reducing error rates text classification :10.1.1.1.5684:10.1.1.20.9305
main approaches exploiting unlabelled data transform input feature spaces information unlabelled data iteratively label part unlabelled data 
hirsh follow approach unlabelled corpus create model domain incorporates words occur corpus 
new model created latent semantic indexing represent test training examples comparisons performed transformed space 
second approach labelling part unlabelled data common number different techniques proposed achieve 
nigam perform iterative labelling expectation maximisation em naive bayes classifier :10.1.1.1.5684
popular technique transductive inference uses unlabelled training set determine decision function iteratively guessing labels unlabelled examples regularisation risk minimised :10.1.1.20.9305
study technique labelling part unlabelled data training algorithm uses complementary predictors iteratively label unlabelled data 
blum mitchell apply problems target concept described redundant views classification :10.1.1.114.9164
goldman zhou different training strategy predictors consist supervised learning algorithms complement different representations hypotheses provided labelled data different ways :10.1.1.32.4821
method procedure assume priori redundant views data :10.1.1.114.9164
requirement different supervised learning algorithms complement :10.1.1.32.4821
create new features clustering labelled unlabelled data creating alternate view labelled data 
original view new view create predictors training single learning algorithm 
approach applied larger class problems domains necessarily natural redundancies 

training clustering consider class classification problem labelled sample xy xm ym patterns xi target values yi qg unlabelled sample xm xm patterns xj algorithm creates alternate view input space clustering labelled samples unlabelled samples order create set new features 
view training 
creating cluster features view non hierarchical single pass clustering algorithm partitions patterns xi non overlapping clusters 
computational complexity algorithm average range thresholds explored number samples clustered 
total number labelled unlabelled patterns large worthwhile splitting patterns sets creating partitions set 
reduces computational effort required generating clusters 
typically clustering process creates partition pattern set consisting large number clusters clusters contain small number patterns 
order derive useful information clusters clusters sorted sizes largest clusters chosen representative major concepts labelled unlabelled data 
chosen clusters contributes features patterns labelled test set binary feature indicating closest clusters 
similarity pattern cluster centroid similarity pattern cluster unlabelled centroid unlabelled centroid average unlabelled patterns belong cluster 
class labelled set similarity pattern cluster class centroid defined average patterns class belong cluster 
similarities derived full class membership information contained data set implications training 
number features created class problem partitions data set sn 
classifiers linear kernel svm experiments separate machine trained class 
output class ranked list documents scores allocated formula vector features document denotes dot product features space 
vector bias determined minimising functional jjwjj max yi xi xi yi labelled training instances feature vector document di corresponding label indicates membership class indicates non membership number training instances 
constant controls trade complexity solution error penalty 
method different svms quadratic penalty machine svm linear penalty machine svm 
training training approach distinguish predictors data view 
predictors trained word presence features referred word presence wp classifiers trained features derived clustering known cluster feature cf classifiers 
training proceeds follows form training set data view labelled examples train cf classifier 
cf classifier assign labels unlabelled set 
select newly labelled cases form training data set word presence features 
data set consists selected newly labelled cases original labelled examples 
similarly wp classifier form training data set cluster features 
train trained word presence classifier training set step trained cluster features classifier training set step 
iterate steps replacing classifier cf classifier classifier predicted labels unlabelled data set stable 

experimental setup text classification experiments distinct corpora documents 
case pre process data chosen corpus create word presence matrix collection 
presence matrix provides features word presence view 
create partitions labelled unlabelled data sets apply clustering algorithm described section turn 
partitions purely order reduce computational effort clustering 
partition select populous clusters compute features cluster view 
corpus evaluate model independent test set 
data collections corpus webkb data set containing web pages gathered university computer science departments :10.1.1.1.5684
populous categories excluding category course student faculty project remove pages specify browser relocation 
creates data set containing pages 
experiments web pages universities cornell texas washington wisconsin randomly split training test sets training test entries 
web pages misc unlabelled data set 
clustering performed training unlabelled data set 
micro averaged breakeven point impact number iterations accuracy reuters labelled examples webkb labelled examples wp svm cf svm wp svm cf svm newsgroups labelled examples number iterations bp data sets function number training iterations 
iteration shows baseline results cf classifiers 
second data corpus modapte split reuters news wires collection documents :10.1.1.1.5684:10.1.1.20.9305
split training documents test documents 
studies populous classes build binary classifiers class 
clustering performed training set split labelled unlabelled set different proportions needed complete test set calculate performance measures case 
third data set newsgroups data set collected ken lang 
consists articles spread evenly different usenet discussion groups 
version data set duplicates headers removed 
articles randomly split training articles unlabelled articles articles clustering remaining articles testing 
standard text processing consistent previous research yields word presence features respectively webkb reuters newsgroups number cluster features respectively 
performance measures standard evaluation criterion reuters benchmark breakeven point point precision equals recall 
different different categories normal report micro averaged breakeven point bp 
micro averaging gives category document assignment decision equal weight 
concerns single merit particularly micro averaging step obscure subtle details 
addition bp report breakeven points error rates categories 

experiments results evaluate impact training strategy microaveraged breakeven point bp precision recall breakeven point classification errors different data sets 
classifiers experiments svm quadratic penalty svm svm linear penalty svm 
machines set number instances training set regularisation constant svm equation section 
results averages random selections labelled set 
setting training parameters key issues resolved training 
firstly guarantee predicted labels unlabelled data stable finite number iterations training 
determine sensible stopping criterion training 
secondly unlabelled cases chosen form training sets 
number training iterations order resolve issue label examples score 
apply training scheme iterations average squared change score 
shows results experiment datasets classifiers 
vertical axis indicates bp categories webkb top categories reuters categories newsgroups 
horizontal axis indicates number training training labelling iterations 
iteration represents case original labelled examples training 
shown iteration training significantly improves bp classifiers wp cf 
accuracy classifiers starts deteriorating iterations performed indicating labelling trained models error prone iteration cotraining sufficient obtain significant improvements 
observation holds training set sizes 
effect classification error hand 
generally attains best lowest error iteration true datasets 
cf classifier behaviour varied small categories improvement error rate training bigger categories obtain improvement iteration reach lowest error iterations 
strong experimental evidence iteration sufficient improve breakeven point classification error 
efficiency considerations single iteration training subsequent experiments 
choosing examples training sets svm classifier training returns score example interpret likelihood example belongs target class 
general positive scores indicate membership negative scores non membership greater absolute value score greater likelihood event 
choose threshold score confidence value label examples score members target class high confidence score non members high confidence 
larger value higher confidence labels 
examples scores range left unlabelled 
large values result small training sets smaller values result larger training sets 
experiments different values show considerable variability maximum accuracy reached 
strong theoretical evidence recall performance svm models training set provides unbiased estimate recall unseen data svm score outside range 
fixed training cases 
impact micro averaged breakeven point shows effect unlabelled data training webkb data set 
vertical axis indicates bp micro average breakeven point svm classifier wp cf wp cf number labelled examples impact training webkb svm classifier wp cf wp cf number labelled examples bp webkb data set function number initial labelled examples 
webkb categories 
horizontal axis indicates amount labelled data training 
curves plotted models created word information wp ii cluster features information cf iii word presence information labels generated cf model iv cluster information trained labels generated wp model 
curves plotted svm svm 
seen svms achieve best performance trained labels generated cf classifier especially number examples small svm 
training classifier class labels generated binary instance belongs class 
wp classifier fully utilise binary labels presence information encode class information 
cf classifier hand utilises centroid features encode full class membership information training set section 
newly labelled cases binary labels full information content new labels missing corresponding centroid features suffer making general useful prediction 
sufficient information newly labelled instances similarity unlabelled data clusters similarity clusters improves accuracy interestingly svm models classifier shows improvement number initial labelled examples increased 
may due lower accuracy wp classifier consequent inaccurate labelling classifier 
order determine significance observed improvements computed relative gain bp new bp old com bp old observed improvement ultimate improvement possible ideal classifier 
shows average relative gain function number labelled samples standard deviation envelope 
curves plotted improvement wp classifier trained labels generated cf classifier 
standard deviation envelope svm svm classifiers lies parts expect reasonable confidence improvement wp classifier trained cf classifier labelling cases 
importantly im micro averaged breakeven relative gain significance improvement cotraining webkb svm classifier average standard deviation number labelled examples svm classifier average standard deviation number labelled examples relative gain bp webkb data set function number initial labelled examples 
provement larger needed number labelled samples small improvements svm labelled samples 
results bp newsgroups reuters data sets similar rows table labelled training examples consisting data respectively 
reuters improvement bp populous classes classifier svm svm translates gain respectively 
improvement bp newsgroups classifier svm svm translates gain respectively 
impact breakeven point micro averaging breakeven point gives equal weight category document assignment decision 
averaging step obscure subtle details 
order determine impact cotraining individual categories table lists individual breakeven points different classifiers support vector machines learners reuters newsgroups data sets 
results shown populous categories decreasing order training set sizes reuters models trained just initial labelled examples table 
newsgroups results shown categories alphabetical order number initial labelled examples table 
average number labelled examples class shown second column entitled 
results svm svm classifiers 
seen results table increase breakeven point labelling classifier sufficiently accurate 
instance category crude cf classifier accurate wp classifier svms 
result improvement decrease accuracy results newsgroups show similar story 
cf classifier performs high breakeven point accompanying increase performance classifier 
notice reuters case classifier outperforms matches cf classifier categories 
difference data sets due smaller number labelled initial data cases available smaller categories reuters collection column inthe svm breakeven svm breakeven wp cf wp cf reuters initial labelled examples bp newsgroups initial labelled examples bp table breakeven points individual categories reuters newsgroups 
denotes average number labelled target class examples 
table 
trend seen webkb data set 
plots breakeven points individual categories function number initial labelled examples svm classifier learning 
shown training usually increases breakeven point classifier improvement significant classifier 
results individual categories differ substantially 
instance smallest category project cf classifier outperforms classifiers consistently 
cases training generally positive impact breakeven point categories especially number initial labelled examples category small 
results svm classifier similar lower breakeven points cf classifiers greater improvement training classifier 
impact classification error section determine classification error reflects tendencies breakeven point different categories 
plot classification error webkb categories breakeven point breakeven point wp cf class course size data class faculty size data number labelled examples wp cf class student size data class project size data number labelled examples precision recall breakeven points webkb svm function number initial labelled examples svm classifier 
shown training improves classification error cases notable exception smallest category project cf classifier outperforms 
line findings improvement breakeven points 
comparing seen improvements breakeven points accompanied corresponding decreases classification errors 
instance category course cf classifier behaves differently metric 
error rate starts lower classifiers decreases slower rate eventually higher wp fig 
contrast breakeven point consistently worst classifiers 
inconsistencies classification error breakeven point observed reuters newsgroups 
observations highlight fact evaluation criteria measure different properties classification model suited different goals 

discussion shown previous section training features derived clustering labelled unlabelled data generally improves categorisation performance classifiers corpora 
analyse significance observed improvements contributing factors 
significance test tailed matched pairs test reject hypothesis mean error reduced training significance level data sets 
similar test breakeven precision indicates increase breakeven point significant corpora 
value cluster features features derived clustering create pattern aggregated information document set 
information quite different avail classification error classification error wp cf class course size data class faculty size data number labelled examples wp cf class student size data class project size data number labelled examples classification error webkb svm classifier webkb reuters news examples examples examples svm svm svm svm table summary gain bp able word presence matrix 
clusters thought higher level concepts feature space features derived clusters indicate similarity document concepts 
cluster features high quality performance cf classifier uses features vs uses features webkb case 
shown results category project webkb data set figures number examples category small features provide significant information available word presence features 
suggests cluster features way addition presence information 
early results investigation indicate augmentation improves classification performance 
creating final classifier experiments shown major improvements obtained wp classifier cf classifier 
ultimate trained classifier classifier obtains micro averaged breakeven point gain data sets table 
marginal gain classifier due fact generated labels binary quality cluster features classifier lower derived original labelled set 
full class membership information generated scores binary classifiers may improve accuracy classifier case final classifier may combination classifiers 

new training strategy automatically creates alternate view data clustering labelled unlabelled data 
strategy require complementary predictors widely applicable 
experiments support vector machines classifiers text benchmarks show strategy improves classification performance especially number labelled examples 
permission managing director research laboratories publish gratefully acknowledged 

blum mitchell :10.1.1.114.9164
combining labeled unlabeled data training 
colt proceedings workshop computational learning theory morgan kaufmann publishers 
cristianini shawe taylor 
support vector machines kernel methods 
cambridge university press 
goldman zhou :10.1.1.32.4821
enhancing supervised learning unlabeled data 
proceedings seventeenth international conf 
machine learning pages 
morgan kaufmann san francisco ca 
joachims 
transductive inference text classification support vector machines 
proceedings sixteenth international conf 
machine learning pages 
morgan kaufmann san francisco ca 
kowalczyk 
maximal margin perceptron 
smola bartlett scholkopf schuurmans editors 
advances large margin classifiers 
mit press cambridge ma 
kowalczyk raskutti 
learner self assessment case study svm information retrieval 
proceedings fourteenth australian joint conference artificial intelligence 
lang 
newsweeder learning filter netnews 
international conference machine learning pages 
nigam mccallum thrun mitchell :10.1.1.1.5684
text classification labeled unlabeled documents em 
machine learning 
raskutti kowalczyk 
second order features maximising text classification performance 
proceedings twelfth european conference machine learning ecml 
raskutti kowalczyk 
unlabelled data text classification addition cluster parameters 
proceedings nineteenth international conference machine learning icml 
scholkopf smola 
learning kernels support vector machines regularization optimization 
mit press 
vapnik 
statistical learning theory 
wiley 
hirsh 
lsi text classification presence background text 
proceedings tenth international conf 
information knowledge management 
