design implementation high performance distributed web crawler shkapenyuk torsten suel cis department polytechnic university brooklyn ny research att com suel poly edu broad web search engines specialized search tools rely web crawlers acquire large collections pages indexing analysis 
web crawler may interact millions hosts period weeks months issues robustness flexibility manageability major importance 
addition performance network resources os limits taken account order achieve high performance reasonable cost 
describe design implementation distributed web crawler runs network workstations 
crawler scales pages second resilient system crashes events adapted various crawling applications 
software architecture system discuss performance bottlenecks describe efficient techniques achieving high performance 
report preliminary experimental results crawl pages hosts 
world wide web grown pages pages 
due explosion size web search engines increasingly important primary means locating relevant information 
search engines rely massive collections web pages acquired help web crawlers traverse web hyperlinks storing downloaded pages large database indexed efficient execution user queries 
researchers looked web search technology years including crawling strategies storage indexing ranking techniques significant amount structural analysis web web graph see overviews supported nsf career award nsf ccr intel new york state center advanced technology telecommunications polytechnic university equipment intel sun microsystems :10.1.1.110.9023
basic techniques 
highly efficient crawling systems needed order download hundreds millions web pages indexed major search engines 
fact search engines compete primarily size currency underlying database addition quality response time ranking function 
largest search engines google altavista currently cover limited parts web data months date 
note crawling speed obstacle increased search engine size scaling query throughput response time larger collections major issue 
crawler large search engine address issues 
crawling strategy strategy deciding pages download 
second needs highly optimized system architecture download large number pages second robust crashes manageable resources web servers 
academic interest issue including strategies crawling important pages crawling pages particular topic particular type refreshing pages order optimize freshness collection pages scheduling crawling activity time 
contrast second issue 
clearly major search engines highly optimized crawling systems details systems usually proprietary 
system described detail literature appears mercator system heydon najork dec compaq altavista 
details known version google crawler system internet archive :10.1.1.109.4049
fairly easy build slow crawler downloads pages second short period time building high performance system download hundreds millions pages weeks presents number challenges system design network efficiency robustness manageability 
crawling strategies address performance issues attempts minimize number pages need downloaded maximize benefit obtained downloaded page 
exception considers system performance focused crawler built top generalpurpose database system throughput system significantly high performance bulk crawler 
case applications limited bandwidth acceptable 
case larger search engine need combine crawling strategy optimized system design 
describe design implementation optimized system network workstations 
choice crawling strategy largely orthogonal 
describe system example simple breadth crawl system adapted strategies 
primarily interested network efficiency aspects system scalability issues terms crawling speed number participating nodes 
currently crawler acquire large data sets aspects web search technology indexing query processing link analysis 
note high performance crawlers currently widely academic researchers groups run experiments scale similar major commercial search engines exception webbase project related stanford 
interesting questions realm massive data sets deserve attention academic researchers 
crawling applications number different scenarios crawlers data acquisition 
describe examples differ crawling strategies 
breadth crawler order build major search engine large repository internet archive high performance crawlers start small set pages explore pages links breadth fashion 
reality web pages traversed strict breadth fashion variety policies pruning crawls inside web site crawling important pages pages updates pages initially acquired may periodically checked updates 
simplest case done starting broad breadth crawl simply requesting urls collection 
vari see heuristics attempt crawl important pages experimental results showing strict breadth quickly find pages high pagerank 
heuristics employed important pages sites domains frequently 
strategies crucial maintaining date search index limited crawling bandwidth cho garcia molina studied techniques optimizing freshness collections observations page update history 
focused crawling specialized search engines may crawling policies attempt focus certain types pages pages particular topic particular language images mp files computer science research papers 
addition heuristics general approaches proposed link structure analysis machine learning techniques 
goal focused crawler find pages interest lot bandwidth 
previous high performance crawler doing support large specialized collections significantly date broad search engine 
random walking sampling techniques studied random walks web graph slightly modified graph sample pages estimate size quality search engines 
crawling hidden web lot data accessible web resides databases retrieved posting appropriate queries filling forms web pages 
lot interest focused automatic access data called hidden web deep web federated facts figures 
looked techniques crawling data 
crawler described extended efficient front system 
note challenges associated access hidden web efficiency front probably important issue 
basic crawler structure scenarios design flexible system adapted different applications strategies reasonable amount 
note significant differences scenarios 
example broad breadth crawler keep track pages crawled commonly done url seen data structure may reside disk large crawls 
link analysis focused crawler hand may additional data structure represent graph structure crawled part web classifier judge relevance page size structures may smaller 
hand number common tasks need done scenarios enforcement robot exclusion crawl speed control dns resolution 
simplicity separate crawler design main components referred crawling application crawling system 
crawling application decides page request current state previously crawled pages issues stream requests urls crawling system 
crawling system eventually downloads requested pages supplies crawling application analysis storage 
crawling system charge tasks robot exclusion speed control dns resolution common scenarios application implements crawling strategies breadth focused 
implement focused crawler breadth crawler crawling system different parameter settings significantly different application component written library functions common tasks parsing maintenance url seen structure communication crawling system storage 
system storage crawling application system crawling world wide web basic components crawler url requests downloaded pages components glance implementation crawling system may appear trivial 
true highperformance case pages downloaded second 
fact crawling system consists components replicated higher performance 
crawling system application replicated independently different applications issue requests crawling system showing motivation design details architecture section 
note partition application system components design choice system systems obvious component particular task handled 
focus case broad breadth crawler crawling application 
requirements crawler discuss requirements crawler approaches achieving 
details solutions course case application replication designer component decide partition data structures workload 
mercator partition achieves flexibility pluggable java components 
subsequent sections 
flexibility mentioned able system variety scenarios modifications possible 
low cost high performance system scale pages second hundreds millions pages run run lowcost hardware 
note efficient disk access crucial maintain high speed main data structures url seen structure crawl frontier large main memory 
happen downloading pages 
robustness aspects 
system interact millions servers tolerate bad html strange server behavior configurations odd issues 
goal err side caution necessary ignore pages entire servers odd behavior applications download subset pages anyway 
secondly crawl may take weeks months system needs able tolerate crashes network interruptions losing data 
state system needs kept disk 
note really require strict acid properties 
decided periodically synchronize main structures disk limited number pages crash 
speed control extremely important follow standard conventions robot exclusion robots txt robots meta tags supply contact url crawler supervise crawl 
addition need able control access speed different ways 
avoid putting load single server contacting site seconds specified 
desirable throttle speed domain level order overload small domains reasons explained 
campus environment connection shared users need control total download rate crawler 
particular crawl low speed peek usage hours day higher speed late night early morning limited mainly load tolerated main campus router 
manageability reconfigurability appropriate interface needed monitor crawl including speed crawler statistics hosts pages sizes main data sets 
administrator able adjust speed add remove components shut system force checkpoint add hosts domains blacklist places crawler avoid 
crash shutdown software system may modified fix problems may want continue crawl different machine configuration 
fact software huge crawl significantly different start due need numerous fixes extensions apparent tens millions pages downloaded 
content describe design implementation distributed web crawler runs network workstations 
crawler scales pages second resilient system crashes events adapted various crawling applications 
software architecture system discuss performance bottlenecks describe efficient techniques achieving high performance 
report preliminary experimental results crawl pages hosts 
remainder organized follows 
section describes architecture system major components section describes data structures algorithmic techniques detail 
section presents preliminary experimental results 
section compares design systems know 
section offers concluding remarks 
system architecture give detailed description architecture distributed crawler 
mentioned partition system major components crawling system crawling application 
crawling system consists specialized components particular crawl manager downloaders dns resolvers 
components plus crawling application run different machines operating systems replicated increase system performance 
crawl manager responsible receiving url input stream applications forwarding available downloaders dns resolvers enforcing rules robot exclusion crawl speed 
downloader high performance asynchronous client capable downloading hundreds web pages parallel dns resolver optimized stub dns resolver forwards queries local dns servers 
example small configuration downloaders shows main data flows system 
configuration similar crawls time downloaders 
configuration require workstations achieve estimated peek rate html pages second discuss scaling peek rate depends machine configuration network con downloader downloader downloader crawling application robots txt files resolver dns manager crawl urls small crawler configuration url requests downloaded files detail 
communication system done ways sockets small messages file system nfs larger data streams 
nfs particular design flexible allows tune system performance redirecting partitioning disks 
nfs performance limitations believe basic approach scale networks low cost workstations necessary easy switch optimized file transfer mechanism 
shown interactions crawling application storage system downloaders web server crawl manager separate web interface system management 
entire system contains lines python code 
implementation started july part author senior project 
significant crawls performed january development continuing 
give details components 
crawl manager crawl manager central component system component started 
components started register manager offer request services 
manager component visible components exception case parallelized application described different parts application interact 
manager receives requests urls application request priority level pointer file containing urls lo 
due limited network capacity able run pages second campus connection 
cated disk accessible nfs 
manager enqueue request eventually load corresponding file order prepare download done late possible order limit size internal data structures 
general goal manager download pages approximately order specified application reordering requests needed maintain high performance putting load particular web server 
policy formalized subsection 
loading urls request files manager queries dns resolvers ip addresses servers address cached 
manager requests file robots txt web server root directory copy file 
part initially implemented separate component acted application issuing requests robot files high priority back manager 
incorporated manager process robot files written separate directory data accessed parsed manager see 
parsing robots files removing excluded urls requested urls sent batches downloaders making sure certain interval requests server observed 
manager notifies application pages downloaded available processing 
manager charge limiting speed crawl balancing load downloaders dns resolvers monitoring adjusting dns resolver load downloader speed needed 
manager performs periodic snapshots data structures crash limited number pages may 
application detect duplicate pages 
crawl manager implemented uses berkeley db stl major data structures described detail subsection 
downloaders dns resolvers downloader component implemented python fetches files web opening connections different servers polling connections arriving data 
data marshaled files located directory determined application accessible nfs 
downloader receives pages second large number pages written disk operation 
note way pages assigned data files unrelated structure request files sent application manager 
application keep track url requests completed 
manager ad available www com just speed downloader changing number concurrent connections 
dns resolver implemented fairly simple 
uses gnu asynchronous dns client library access dns server usually collocated machine 
dns resolution significant bottleneck crawler design due synchronous nature dns interfaces observe significant performance impacts system library 
dns lookups generate significant number additional frames network traffic may restrict crawling speeds due limited router capacity 
crawling application mentioned crawling application consider breadth crawl starting set seed urls case urls main pages universities initially sent crawl manager 
application parses downloaded page hyperlinks checks urls encountered sends manager batches 
downloaded files forwarded storage manager compression storage repository 
crawling application implemented stl red black tree implementation library 
application consists threads red black tree data structure required different implementations current implementation stl thread safe 
data structure performance aspects application discussed detail subsection 
note important points page contains average hyperlinks set encountered necessarily downloaded urls grow quickly size main memory eliminating duplicates 
downloading pages size set encountered urls 
second point hyperlink parsed newly downloaded page sent manager downloaded days weeks 
reason manager immediately insert new requests dynamic data structures 
scaling system main objectives design system performance scaled adding additional low cost workstations run additional components 
starting configuration simply add additional downloaders resolvers improve performance 
estimate single manager fast downloaders turn www org uk ian users footprints net kaz html require dns resolvers 
point create second crawl manager essentially second crawling system application split requests managers 
bottleneck system arise crawl application currently able parse process pages second typical workstation 
number improved somewhat aggressive optimizations eventually necessary partition application machines 
urls robots txt files downloaded files downloaded files url requests crawl manager crawling application crawling application downloader downloader downloader resolver dns resolver dns resolver dns urls robots txt files downloaded files downloaded files url requests crawl manager crawling application crawling application resolver dns resolver dns resolver dns downloader downloader downloader exchange parsed urls large configuration shows possible scaled version system uses crawl managers downloaders dns resolvers application components 
point able test performance configuration involve machines result download rates probably pages second way capacity dedicated oc link 
partitioning breadth crawler components quite simple technique similar employed internet archive crawler 
partition space possible urls subsets hash function application component responsible processing requesting subset 
recall manager sure pages downloaded different applications stored separate directories determined applications 
parsing component encounters hyperlink belonging different subset url simply forwarded appropriate application component determined hash value 
crawl manager application components manager appear completely unrelated applications 
undue load particular server avoided multiplying second interval number crawl managers making sure host mapped subset second option avoid increase cost robot exclusion dns resolution 
crawling strategies focused crawling may harder parallelize unavoidable particular design 
note communication involving significant amounts data transmission downloaded files 
principle system wide area distributed environment assuming tolerate fact downloaded files may remote locations application component downloader location 
discussion system configuration resulting performance subsection 
implementation details algorithmic techniques describe specific problems performance bottlenecks encountered data structures techniques deal 
start application follow path urls system 
recall breadth crawl application parses newly downloaded documents hyperlinks checks data structure see urls hyperlinks encountered sends new urls manager downloading 
application parsing network performance parsing implemented perl compatible regular expression library note parse hyperlinks indexing terms significantly slow application 
parsing indexing terms done separate application scans documents repository outside scope 
current parsing implementation process pages second kb page 
tuning parser nfs network performance considerations important application side 
usually downloader store pages nfs disk machine running crawling application 
application reads files parsing storage manager copies separate permanent repository nfs 
data item enter leave machine running application network 
machines connected full duplex switched fast ethernet 
kb page pages available www org second enter leave application machine realistic assumptions network nfs performance 
possible solutions 
scale system partitioning application components soon bottleneck arises described subsection 
partition downloaded files machines possibly downloaders application read nfs buy 
alternatively upgrade gigabit ethernet switch rcp largely remove bottleneck 
url handling hyperlinks parsed files normalization relative links checked url seen structure contains urls downloaded encountered hyperlinks far parsing speed pages second results urls second need checked possibly inserted 
url average length bytes naive representation urls quickly grow memory size 
solutions proposed problem 
crawler internet archive uses bloom filter stored memory results compact representation gives false positives pages downloaded collide pages bloom filter 
lossless compression reduce url size bytes high large crawls 
cases main memory eventually bottleneck partitioning application partition data structures machines 
scalable solution uses disk resident structure example done mercator 
challenge avoid separate disk access lookup insertion 
done mercator caching seen frequently encountered urls resulting cache hit rate 
system fast disks average crawl speed pages second 
goal design completely avoid random disk accesses linear growth accesses urls 
achieve perform lookups insertion bulk offline operation 
take approach known techniques initially keep urls main memory red black tree 
structure grows certain size write sorted list urls disk switch bulk mode 
red black tree main memory buffer newly encountered urls periodically merge memory resident data disk resident data simple scan copy operation necessary implemented fingerprint technique mercator filter downloaded pages identical content different urls 
lookups insertions performed 
merge performed spawning separate thread application continue parsing new files merge started hour previous completed 
system gracefully adapt merge operations start longer structure grows 
efficiency optimized storing urls disk simple compressed form removing common prefixes subsequent urls sorted order 
method lookups insertions bottleneck 
recall average page contains hyperlinks removing duplicates number encountered downloaded pages grow quickly 
downloading pages pages queue waiting downloaded 
number consequences 
url parsed page downloaded days weeks 
second justifies decision perform lookup insertion operations bulk fashion urls immediately needed crawling system 
goal download pages switch application point crawling system requests queue 
raises question exactly means application keep system 
design breadth application parse files speed downloaded permanently transferred storage 
alternatively store pages retrieve parse long generate urls keep crawling system busy 
domain throttling important put load single web server observing time period requests 
desirable domain throttling sure requests balanced different second higher level domains 
motivations 
domains may fairly slow network connection large number web servers impacted crawler 
problem encountered larger organizations universities intrusion detection systems may raise alarm servers campus contacted short period time timeouts observed accesses server 
crawler timeouts accesses hostname ip address detect web servers collocated machine 
cases hosts domain collocated machine decided address domain throttling crawl suspect done try influence search engines link ranking cases sites forming large cliques similar content structure 
application easiest way 
note fetching urls order parsed pages bad idea lot domain locality links 
consider case large cliques attached fairly webmasters 
scramble urls random order urls domain spread evenly 
fact simple deterministic way provable load balancing properties put hostname url reverse order com amazon www inserting url data structures subsection 
checking duplicates take sorted list new urls perform way permutation say sending manager 
way easily implemented scanning urls sorted order dividing files round robin fashion concatenating files 
fact permutations random permutations parallel computation balancing properties case guarantee adjacent urls sorted order spread far apart reversing hostname pages domain spread set requests 
note large crawl reverse hostnames 
encounter problems approach provide provable bounds 
crawl manager data structures crawl manager maintains number data structures scheduling requests downloaders observing robot exclusion request interval policies 
main structures shown 
particular fifo request queue containing list request files sent manager 
request file typically contains urls reasons efficiency located disk accessible nfs 
note files immediately loaded managers stay disk long possible 
number fifo host queues containing urls organized hostname 
structures implemented berkeley db single tree hostname key reader think separate queues host 
hosts organized host data structures host selected download take entry corresponding host queue send downloader 
different host structures host dictionary containing entry host currently manager pointer corresponding host queue ii priority queue pointers note bit reversal permutation similar effect 
request files urls hosts host queues downloader request queue queue ready wait queue manager data structures host dictionary pointers pointers pointers hosts ready download ready queue iii priority queue pointers hosts accessed waiting seconds contacted waiting queue 
url sent manager implicit request number corresponding ordering request stream 
goal manager preserve ordering possible observing interval requests achieving high performance 
done follows 
host pointer ready queue key value request number url corresponding host queue 
extracting host minimum key value select url lowest request number urls ready downloaded send downloader 
page downloaded pointer host inserted waiting queue key value equal time host accessed 
checking minimum element priority queue extracting waiting time passed transfer hosts back ready queue 
new host encountered create host structure put host dictionary 
dns resolution robot exclusion finished insert pointer host ready queue 
urls queue downloaded host deleted structures 
certain information robots files kept host url comes host structure reinitialized 
host responding put host waiting queue time longer time second try 
applications decide give longer shorter timeouts certain hosts crawl powerful server faster priority queues dictionary implemented stl 
suggested choose timeout time previous download complete multiplied 
easily incorporated system 
scheduling policy manager performance discuss scheduling policy performance manager data structures 
mentioned number requested urls waiting downloaded may hundreds millions number hosts may millions 
immediately insert urls berkeley db tree structure quickly grow main memory size result bad behavior expect disk access urls especially url sets application 
delay inserting urls structures long possible 
recall pointers url files organized queue 
follow simple policy hosts ready queue read batch urls manager data structures 
goal significantly decrease size structures get better behavior 
choose value 
number pages crawled second timeout interval host seconds 
long hosts queue hosts keep crawl running speed 
note count hosts waiting queue contains hosts timed longer periods due servers 
total number host structures corresponding url queues time 
number hosts ready queue 
estimate number hosts currently waiting accessed number hosts waiting number hosts dictionary queue small happens robot exclusion dns lookup actual process downloading page 
ignoring hard estimate number host structures usually 
example typically speed pages second resulted hosts manager 
look consequences performance 
turns limited number hosts tree url queues berkeley db eventually grow main memory size due number hosts urls waiting downloaded queues continue grow 
look caching effects find queue data pages really active head urls removed tail new ones inserted 
fact head queue page tail queue structures hosts waiting longer times rarely accessed 
sure main memory cache hold active data pages expect reasonable caching policies 
various sizes mb berkeley db cache size observed behavior 
result manager uses moderate cpu memory disk resources small number downloaders 
ask policy affects order pages sent downloaders 
answer ordering fact immediately insert request urls manager 
assume number hosts ready queue drops manager able increase number downloaders run reasonable 
host obliviousness assumption essentially saying changes exact timing host access result widely different host response time 
result requests pages host downloaded order issued application 
requests pages different hosts downloaded order issued host queue backed requests due timeout rule host delay occurred robot exclusion dns resolution actual process downloading host 
case delay matter requests issued close 
experimental results experiences preliminary experimental results experiences 
detailed analysis performance bottlenecks scaling behavior scope require fast simulation testbed possible appropriate study current internet connection real web sites 
results large crawl ran crawl web pages hosts 
crawl performed period days crawler continuously operation time 
longer network outages due outside attacks campus network 
number crawler crashes crawl size 
crawler offline hours changes software implemented 
days crawler running low speed download urls large host queues remained application switched earlier urls discovered 
crash crawler limited number pages previous checkpoint 
operation speed crawler limited certain rate depending time day sure users campus 
table shows approximate statistics crawl percentages estimated subset logs 
requests network errors read timeout exceeded errors robots txt requests successful non robots requests average size page bytes total data size tb table 
basic crawl statistics total non robot ok moved temporarily moved permanently forbidden unauthorized internal server error table 
errors network errors include server exist behaves incorrectly extremely slow read timeout exceeded 
robots files downloaded times expire cache hours 
estimate successful requests unique pages 
note numbers include errors shown table network limits speed control mentioned control speed crawler impact campus users minimized 
agreed network administrator reasonable limits different times day 
usually limited rates pages second mb peak times pages second late night early morning 
limits changed displayed web java interface 
campus connected internet link cisco main campus router 
observed router gets clogged requests approach capacity 
briefly ran rate pages second doing longer probably bring router 
achieve speed control crawl manager varies number connections downloader opens simultaneously 
measured download rate target number increased decreased 
turns controlling speed automatically quite challenging factors influence performance 
adding connections campus network busy idea decreasing number connections cases increase speed bottleneck machines running downloaders 
ended completely satisfactory solution modifies number connections having absolute upper bound number connections speed setting 
number incoming bytes outgoing bytes outgoing frames campus connection courtesy 
data includes traffic going poly edu domain hours may 
day crawler run high speed relatively little traffic originating campus 
crawler performed checkpoint hours crawling speed close zero minutes 
clearly shown graph number incoming bytes bottoms curve estimate amount bandwidth taken crawler versus campus traffic 
pattern exist outgoing bytes crawler sends small requests clearly visible number outgoing frames partly due requests dns system system performance configuration crawl sun ultra workstations dual processor sun 
try minimize amount cost hardware configuration powerful higher download rates fast network 
try give rough estimate amount hardware really needed 
requires understanding cpu memory disk requirements different components collocated machine 
downloader connections take cpu little memory 
manager takes little cpu time depending downloaders attached needs reasonable amount mb buffer space berkeley db 
application needs cpu memory collocated downloader 
example low configuration sun ultra gb eide disks gb mb memory respectively 
running downloader manager machine components observed sustained crawling speed pages second similar performance achievable comparable linux workstations 
attempts increase speed result sharply lower speed due components starving 
note number outgoing bytes unusually high probably due mentioned break ins brought campus network times 
machine running manager downloader mb memory suffice 
note translates crawling speed pages day academic research projects 
larger configurations believe pages second node achievable distributing components appropriately remains studied 
experiences observed brin page running high speed crawler generates fair amount email phone calls :10.1.1.109.4049:10.1.1.109.4049:10.1.1.109.4049
case crawler supervision important 
small host blacklist data structure manager changed quickly prevent crawler re annoying annoyed people highly useful 
issue ran repeatedly came security software raises alarm believes trying break scanning ports machines university networks network administrator suspected attacks 
tools unaware dealing requests 
tries protect customers ports scanning issued stern alarm accesses problematic port protected way house web server linked pages web altavista 
events bugs robot exclusion helpful bug reports various curious questions 
clearly numerous social legal issues arise 
comparison systems compare architecture performance systems know 
major search engines number navigation services copyright surveillance companies high performance crawling systems details systems public 
detailed description high performance crawler academic literature know mercator crawler heydon najork altavista search engine 
give surprisingly difficult write parser robot files works correctly millions servers number available libraries buggy 
detailed comparison system successor system called concurrently developed system 
compare certain aspects crawlers know familiar aspects systems 
crawling strategies see section 
mercator written completely java gives flexibility pluggable components posed number performance problems addressed 
mercator centralized crawler designed fairly powerful server cpus fast disk 
major difference system try completely avoid random mercator uses caching catch random fast disk system handle significant remaining accesses 
advantage centralized system data directly parsed memory written disk system 
scheduling urls download mercator obtains performance hashing hostnames set queues queue host 
mercator uses thread queue opens connection synchronous fairly powerful downloaders open hundreds asynchronous connections 
distributed version mercator called essentially ties mercator systems technique partitioning application described subsection earlier internet archive crawler 
uses disk efficient merge similar described subsection 
familiar details unpublished 
say employs similar approach scaling uses powerful centralized system mercator basic unit replication replicate small distributed system network workstations get larger shown 
way looking difference say scales replicating vertical slices system layers scalable horizontal slices services application url queuing manager downloader 
limited details early version google crawler :10.1.1.109.4049
system uses asynchronous python downloaders 
difference parsing indexing terms integrated crawling system download speed limited indexing speed 
mentioned internet archive crawler uses bloom filter identifying seen pages allows structure held memory results pages falsely omitted 
described architecture implementation details crawling system preliminary experiments 
obviously improvements system 
major open issue detailed study scalability system behavior components 
probably best done setting simulation testbed consisting workstations simulates web artificially generated pages stored partial snapshot web currently considering looking testbeds high performance networked systems large proxy caches 
main interest crawler research group look challenges web search technology students system acquired data different ways 
acknowledgments campus network administrator tom schmidt patience jeff lim duan great systems support 
acknowledge equipment donations intel sun microsystems 
arasu cho garcia molina raghavan 
searching web 
acm transactions internet technologies june 
baeza yates neto 
modern information retrieval 
wesley 
bar yossef berg chien 
approximating aggregate queries web pages random walks 
proc 
th int 
conf 
large data bases september 
bharat broder henzinger kumar venkatasubramanian 
connectivity server fast access linkage information web 
th int 
world wide web conference may 
brin page :10.1.1.109.4049
anatomy large scale hypertextual web search engine 
proc 
seventh world wide web conference 
burner 
crawling eternity building archive world wide web 

chakrabarti dom kumar raghavan rajagopalan tomkins gibson kleinberg 
mining web link structure 
ieee computer 
note basically scalable crawler trap 
chakrabarti van den berg dom 
distributed hypertext resource discovery examples 
proc 
th int 
conf 
large data bases pages september 
chakrabarti van den berg dom 
focused crawling new approach topic specific web resource discovery 
proc 
th int 
world wide web conference www may 
cho garcia molina 
evolution web implications incremental crawler 
proc 
th int 
conf 
large data bases pages sept 
cho garcia molina 
synchronizing database improve freshness 
proc 
acm sigmod int 
conf 
management data pages may 
cho garcia molina page 
efficient crawling url ordering 
th int 
world wide web conference may 
diligenti coetzee lawrence giles gori 
focused crawling context graphs 
proc 
th int 
conf 
large data bases september 
henzinger heydon mitzenmacher najork 
measuring index quality random walks web 
proc 
th int 
world wide web conference www pages 
henzinger heydon mitzenmacher najork 
near uniform url sampling 
proc 
th int 
world wide web conference may 
heydon najork 
mercator scalable extensible web crawler 
world wide web 
hirai raghavan garcia molina paepcke 
webbase repository web pages 
proc 
th int 
world wide web conference may 
kahle 
archiving internet 
scientific american march 
kaufmann suel 
algorithms routing sorting meshes 
proc 
th annual acm siam symp 
discrete algorithms pages arlington va jan 
najork 
distributed web crawler 
presentation research march 
najork wiener 
breadth search crawling yields high quality pages 
th int 
world wide web conference 
raghavan garcia molina 
crawling hidden web 
proc 
th int 
conf 
large data bases sept 
appear 
rennie mccallum 
reinforcement learning spider web efficiently 
proc 
int 
conf 
machine learning icml 
suel yuan 
compressing graph structure web 
proc 
ieee data compression conference dcc march 
appear 
liu nain coffman 
controlling robots web search engines 
sigmetrics conference june 
witten moffat bell 
managing gigabytes compressing indexing documents images 
morgan kaufmann second edition 
