learning kernel matrix semi definite programming gert lanckriet gert cs berkeley edu department electrical engineering computer science university california berkeley ca usa nello cristianini nello wald ucdavis edu department statistics university california davis ca usa laurent el ghaoui eecs berkeley edu department electrical engineering computer science university california berkeley ca usa peter bartlett michael jordan bartlett jordan stat berkeley edu computer science division department statistics university california berkeley ca usa report 
ucb csd october computer science division eecs university california berkeley california learning kernel matrix semi definite programming gert lanckriet gert cs berkeley edu department electrical engineering computer science university california berkeley ca usa nello cristianini nello wald ucdavis edu department statistics university california davis ca usa laurent el ghaoui eecs berkeley edu department electrical engineering computer science university california berkeley ca usa peter bartlett michael jordan bartlett jordan stat berkeley edu computer science division department statistics university california berkeley ca usa october kernel learning algorithms embedding data euclidean space searching linear relations embedded data points 
embedding performed implicitly specifying inner products pair points embedding space 
information contained called kernel matrix symmetric positive definite matrix encodes relative positions points 
specifying matrix amounts specifying geometry embedding space inducing notion similarity input space classical model selection problems machine learning 
show kernel matrix learned data semi definite programming sdp techniques 
applied kernel matrix associated training test data gives powerful transductive algorithm labelled part data learn embedding unlabelled part 
similarity test points inferred training points labels 
importantly learning problems convex obtain method learning model class function local minima 
furthermore approach leads directly convex method learn norm soft margin parameter support vector machines solving important open problem 
novel approach supported positive empirical results 
keywords kernel methods learning kernels transduction model selection support vector machines convex optimization semi definite programming advances kernel learning algorithms brought field machine learning closer desirable goal autonomy goal providing learning systems require little intervention possible part human user 
particular kernel algorithms generally formulated terms convex optimization problems single global optimum require heuristic choices learning rates starting configurations free parameters 
course statistical model selection problems faced kernel approach particular choice kernel corresponding feature space central choices generally human user 
provides opportunities prior knowledge brought bear di cult practice find prior justification kernel 
desirable explore model selection methods allow kernels chosen automatic way data 
important observe necessarily need choose kernel function representation finite training data set kernel function entirely specified kernel matrix known gram matrix contains entries inner products appropriate feature space pairs data points 
note possible show symmetric positive definite matrix valid gram matrix sense specifies values inner product 
suggests viewing model selection problem terms gram matrices kernel functions 
focus transduction problem completing labelling partially labelled dataset 
words required predictions finite set points specified priori 
learning function need learn set labels 
practical problems formulation natural example prediction gene function genes interest specified priori function genes unknown 
address problem learning kernel matrix corresponding entire dataset matrix optimizes certain cost function depends available labels 
words available labels learn embedding apply labelled unlabelled data 
resulting kernel matrix combination number existing learning algorithms kernels 
example discuss detail support vector machine svm methods yield new transduction method svms scales polynomially number test points 
furthermore approach er method optimize norm soft margin parameter svm learning algorithms solving important open problem 
done full generality techniques semi definite programming sdp branch convex optimization deals optimization convex functions convex cone positive semi definite matrices convex subsets thereof 
convex set kernel matrices set kind 
furthermore turns natural cost functions motivated error bounds convex kernel matrix 
section recall main ideas kernel learning algorithms introduce variety criteria assess suitability kernel matrix hard margin norm norm soft margin alignment 
section reviews basic definitions results semi definite programming 
section considers optimization various criteria set kernel matrices 
set linear combinations fixed kernel matrices optimization problems reduce sdp 
linear coe cients constrained positive simplified 
linear combination contains unity matrix proven provide convex method optimize norm soft margin parameter support vector machines 
section presents error bounds motivate cost functions 
empirical results reported section 
notation square symmetric matrix means positive semi definite means positive definite 
vector notations understood componentwise 
kernel methods kernel learning algorithms see example cristianini shawe taylor scholkopf smola embedding data hilbert space searching linear relations space :10.1.1.11.2062
embedding performed implicitly specifying inner product pair points giving coordinates explicitly 
approach advantages important deriving fact inner product embedding space computed easily coordinates points 
input set embedding space consider map points function returns inner product images space known kernel function 
definition kernel function mapping inner product feature space kernel matrix square matrix nn ij 
kernel function kernel matrix known gram matrix 
symmetric positive semi definite matrix specifies inner products pairs points completely determines relative positions points embedding space 
consider finite input set characterize kernel functions matrices simple way 
proposition positive semi definite symmetric matrix kernel matrix 
conversely kernel matrix symmetric positive semi definite 
notice kernel matrix need know kernel function implicitly defined map coordinates points 
need vector space fact generic finite set 
guaranteed data implicitly mapped hilbert space simply checking kernel matrix symmetric positive semi definite 
solutions sought kernel algorithms support vector machine svm linear functions feature space weight vector kernel exploited weight vector expressed linear combination training points implying express 
important issue applications choosing kernel learning task intuitively wish choose kernel induces right metric space 
criteria kernel methods kernel methods choose function linear feature space optimizing criterion sample 
section describes criteria 
see example cristianini shawe taylor scholkopf smola :10.1.1.11.2062
criteria considered measures separation labelled data 
consider hard margin optimization problem 
definition hard margin labelled sample 
hyperplane solves optimization problem min subject 
exists realizes maximal margin classifier geometric margin optimizes 
geometrically corresponds distance convex hulls smallest convex sets contain data class classes bennett bredensteiner 
transforming corresponding dual problem solution max vector ones defined ij ij means 
hard margin solution exists labelled sample linearly separable feature space 
non linearly separable labelled sample define soft margin 
consider norm norm soft margins 
definition norm soft margin labelled sample 
hyperplane solves optimization problem min subject 

realizes norm soft margin classifier geometric margin optimizes 
margin called norm soft margin 
hard margin considering corresponding dual problem express solution follows max 
definition norm soft margin labelled sample 
hyperplane solves optimization problem min subject 

realizes norm soft margin classifier geometric margin optimizes 
margin called norm soft margin 
considering corresponding dual problem solution expressed max 
fixed kernel criteria give upper bounds misclassification probability see example chapter cristianini shawe taylor 
solving optimization problems single kernel matrix way optimizing upper bound error probability 
allow kernel matrix chosen class kernel matrices 
previous error bounds applicable case 
see section margin bound performance support vector machines transduction linearly parameterized class kernels 
discuss merit di erent cost functions deferring current literature classification cost functions widely fixed kernels 
goal show cost functions optimized respect kernel matrix sdp setting 
define alignment kernel matrices cristianini :10.1.1.23.6757
unlabelled sample 
frobenius inner product gram matrices trace 
definition alignment empirical alignment kernel kernel respect sample quantity kernel matrix sample kernel viewed cosine angle bi dimensional vectors representing gram matrices 
consider yy vector labels sample yy yy yy yy yy yy yy semi definite programming sdp section review basic definition semi definite programming important concepts key results 
details proofs boyd vandenberghe 
semi definite programming nesterov nemirovsky vandenberghe boyd boyd vandenberghe deals optimization convex functions convex cone symmetric positive semi definite matrices pp ne subsets cone 
proposition viewed search space possible kernel matrices 
consideration leads key problem addressed wish specify convex cost function enable learn optimal kernel matrix semi definite programming 
definition semi definite programming linear matrix inequality abbreviated lmi constraint form 

vector decision variables 
symmetric matrices 
notation means symmetric matrix negative semi definite 
note convex cone constraint general nonlinear constraint term linear name lmi merely emphasizes ne important feature lmi constraint convexity set satisfy lmi convex set 
lmi constraint seen infinite set scalar ne constraints 
constraint indexed ne inequality ordinary sense 
alternatively standard result linear algebra may state constraint trace 
semi definite program sdp optimization problem linear objective linear matrix inequality ne equality constraints 
definition semi definite program problem form min subject 

ax vector decision variables objective vector matrices pp 
convexity lmi constraints sdps convex optimization problems 
usefulness sdp formalism stems important facts 
despite seemingly specialized form sdps arise host applications second exist interior point algorithms globally solve sdps extremely theoretical practical computational ciency vandenberghe boyd 
useful tool reduce problem sdp called schur complement lemma invoked 
lemma schur complement lemma consider partitioned symmetric matrix square symmetric 
det define schur complement matrix schur complement lemma states 
illustrate lemma cast nonlinear convex optimization problem sdp consider result lemma quadratically constrained quadratic program qcqp min subject 
equivalent semi definite programming problem min subject 
seen rewriting qcqp min subject 
convex quadratic inequality equivalent lmi schur complement lemma 
similar steps quadratic inequality constraints yields sdp standard form equivalent 
shows qcqp cast sdp 
course practice qcqp solved general purpose sdp solvers particular structure problem hand ciently exploited 
show particular linear programming problems belong sdp family 
duality important principle optimization important principle duality 
illustrate duality case sdp review basic concepts duality theory show extended semi definite programming 
particular duality give insights optimality conditions semi definite program 
consider optimization problem variables scalar constraints min subject 
context duality problem called primal problem denote optimal value assume convexity 
definition lagrangian lagrangian corresponding minimization problem defined 


called lagrange multipliers dual variables 
notice max 
function coincides objective regions constraints 
satisfied infeasible regions 
words acts barrier feasible set primal problem 
objective function rewrite original primal problem unconstrained optimization problem min max 
notion weak duality amounts exchanging min max operators formulation resulting lower bound optimal value primal problem 
strong duality refers case exchange done altering value result lower bound equal optimal value weak duality hold primal problem convex strong duality may hold 
large class generic convex problems strong duality holds 
lemma weak duality original problem convex exchange max min get lower bound max min min max objective function maximization problem called lagrange dual function 
definition lagrange dual function lagrange dual function defined min min 

furthermore concave convex 
concavity easily seen considering ne function concave function 
pointwise minimum concave functions concave 
definition lagrange dual problem lagrange dual problem defined max 
concave convex optimization problem primal 
weak duality non convex problems 
value called duality gap 
convex problems usually strong duality optimum referred zero duality gap 
convex problems su cient condition zero duality gap provided slater condition lemma slater condition primal problem convex strictly feasible 
sdp duality optimality conditions consider simplicity case sdp single lmi constraint ne equalities min subject 

general case multiple lmi constraints ne equalities handled elimination block diagonal matrices represent single lmi 
classical lagrange duality theory outlined previous section directly apply dealing finitely constraints scalar form noted earlier lmi constraint involves infinite number constraints form 
way handle constraints introduce lagrangian form trace zf dual variable symmetric matrix size 
check lagrange function fulfills role assigned function defined definition case scalar constraints 
define max 
barrier primal sdp coincides objective feasible set infinite 
notice lmi constraint associate multiplier matrix constrained positive semi definite cone 
fact symmetric matrix sup trace zf positive eigenvalue zero negative semi definite 
property obvious diagonal matrices case variable constrained diagonal loss generality 
general case follows fact eigenvalue decomposition diagonal matrix containing eigenvalues orthogonal trace zf trace zu spans positive semi definite cone 
lagrangian cast original problem unconstrained optimization problem min max 
weak duality obtain lower bound exchanging min max max min min max inner minimization problem easily solved analytically due special structure sdp 
obtain closed form lagrange dual function min min trace zf trace zf trace zf trace zf 

dual problem explicitly stated follows max min max trace zf subject trace zf 

observe problem sdp single lmi constraint ne equalities matrix dual variable weak duality holds strong duality may sdps 
surprisingly slater type condition ensures strong duality 
precisely primal sdp strictly feasible exist addition dual problem strictly feasible meaning exist trace zf 
primal dual optimal values attained optimal pair 
case characterize optimal pairs follows 
view equality constraints dual problem duality gap expressed trace trace 
zero duality gap equivalent trace turn equivalent denotes zero matrix product positive semi definite negative semidefinite matrix zero trace zero 
summarize consider sdp lagrange dual 
problem strictly feasible share optimal value 
problems strictly feasible optimal values problems attained coincide 
case primal dual pair optimal trace 
conditions represent generalization sdp case karush kuhn tucker kkt conditions linear programming 
sets conditions express feasible respective problems condition expresses condition generalizes sdp case complementarity condition linear programming 
pair strictly feasible primal dual sdps solving primal minimization problem equivalent maximizing dual problem considered simultaneously 
algorithms relationship duality gap stopping criterion 
general purpose program sturm handles problems ciently 
code uses interior point methods sdp nesterov nemirovsky methods worst case complexity general problem 
practice problem structure exploited great computational savings pp consists diagonal blocks size 
methods worst case complexity vandenberghe boyd 
algorithms learning kernels transduction setting data training set labelled remainder test set unlabelled aim predict labels test data 
setting optimizing kernel corresponds choosing kernel matrix 
matrix form tr tr tr ij 
tr tr 
tr tr number labelled training unlabelled test data points respectively 
optimizing cost function training data block tr want learn optimal mixed block tr optimal test data block implies training test data blocks entangled tuning entries optimize embedding imply test data entries automatically tuned way 
achieved constraining search space possible kernel matrices control capacity search space possible kernel matrices order prevent overfitting achieve generalization test data 
consider general optimization problem kernel matrix restricted convex subset positive semi definite cone 
consider specific examples 
set positive semi definite matrices expressed linear combination kernel matrices set 
km 
case set intersection low dimensional linear subspace positive semi definite cone geometrically viewed computing embeddings disjoint feature spaces weighting 
set 
km set initial guesses kernel matrix linear gaussian polynomial kernels di erent kernel parameter values 
fine tuning kernel parameter kernel cross validation evaluate kernel range kernel parameters optimize weights linear combination obtained kernel matrices 
alternatively chosen rank matrices subset eigenvectors initial kernel matrix set orthogonal vectors 
practically important form case diverse set possibly gram matrices similarity measures representations constructed heterogenous data sources 
challenge combine measures optimal similarity measure embedding learning 
second example restricted set kernels set positive semi definite matrices expressed linear combination kernel matrices set 
km parameters constrained nonnegative 
subset set defined constrains class functions represented 
advantages shall see corresponding optimization problem significantly reduced computational complexity convenient studying statistical properties class kernel matrices 
see section estimate performance support vector machines transduction properties class thresholded version proportion errors test data probability bounded log norm margin cost function margin training set certain measure complexity kernel class grows linearly trace kernel matrices hard margin section derive main optimization result maximizing margin hard margin svm respect kernel matrix realized semi definite programming framework 
soft margin case extension basic result discussed section 
inspired try find kernel matrix convex subset positive semidefinite matrices corresponding embedding shows maximal margin training data keeping trace constant min tr trace 
notice fundamental property margin property crucial remainder 
proposition quantity max convex easily seen considering ne function convex function 
secondly notice pointwise maximum convex functions convex 
problem convex optimization problem 
theorem shows suitable choice set problem cast sdp 
theorem linearly separable labelled sample 
corresponding set labels kernel matrix optimizes solving problem min subject trace tr 
proof substituting tr squared inverse margin defined yields min max tr trace constant 
assume tr tr extended general semi definite case 
proposition know tr convex convex constraints optimization problem certainly convex write min max tr trace express constraint max tr lmi duality 
particular duality allow drop minimization schur complement lemma yields lmi 
define lagrangian maximization problem tr duality tr max min min max 
tr optimum tr form dual problem tr min tr 
implies constraint tr holds exist tr equivalently schur complement lemma tr holds 
account expressed min subject trace tr yields 
notice diag lmi 
notice optimization problem sdp standard form 
course case constraint ensure large margin training data give large margin test data 
easy see criterion optimized test matrix 
consider constraint span 
km 
obtain convex optimization problem min tr subject trace written standard form semi definite program manner analogous min subject trace tr 
notice sdp approach consistent bound 
margin optimized labelled data tr positive semi definiteness trace constraint imposed entire kernel matrix 
leads general method learning kernel matrix semi definite programming margin criterion hard margin svms 
applying complexity results mentioned section leads worst case complexity interior point methods solve particular sdp 
furthermore gives new transduction method hard margin svms 
vapnik original method transduction scales exponentially number test samples new sdp method polynomial time complexity 

specific case rank matrices orthonormal normalized eigenvectors initial kernel matrix semi definite program reduces quadratically constrained quadratic program qcqp see appendix max ct subject 
diag tr 
qcqp problem special form sdp boyd vandenberghe solved ciently programs sturm andersen andersen 
codes interior point methods qcqp nesterov nemirovsky yield worst case complexity mn tr tr 
implies major improvement compared worst case complexity general sdp 
furthermore codes simultaneously solve problem dual form 
return optimal values dual variables allows obtain optimal weights 
hard margin kernel matrices positive linear combinations learn kernel matrix linear class solve convex optimization problem precisely semi definite programming problem 
general purpose programs sturm interior point methods solve sdp problems nesterov nemirovsky polynomial time worst case complexity particular case 
consider restriction set kernel matrices matrices restricted positive linear combinations kernel matrices 
km 
assuming positive weights yields smaller set kernel matrices weights need positive positive semi definite components positive semi definite 
restriction beneficial computational ects general sdp reduces qcqp significantly lower complexity mn tr tr constraint result improved numerical stability prevents algorithm large weights opposite sign cancel 
shall see section constraint yields better estimates generalization performance algorithms 
solving original learning problem subject extra constraint yields min max tr subject trace tr expressed 
omit second constraint implied constraints 
trace problem reduces min max tr subject tr tr tr 
write min max diag tr diag min max diag tr diag min max tr max min tr tr diag tr diag 
interchange order minimization maximization justified standard results convex optimization see boyd vandenberghe objective convex linear concave minimization problem strictly feasible maximization problem strictly feasible skip case elements having sign consider margin case 
obtain max min tr max max tr max max tr reformulated follows max ct subject tr 

problem convex optimization problem precisely qcqp boyd vandenberghe 
note problems solved worst case complexity mn tr tr 
note optimal weights 
recovered primal dual solution standard software sturm 
norm soft margin case non linearly separable data consider norm soft margin cost function 
training svm kernel involves minimizing quantity respect yields optimal value obviously minimum function particular choice expressed explicitly dual problem 
optimize quantity respect kernel matrix try find kernel matrix corresponding embedding shows minimal tr keeping trace constant min tr trace 
convex optimization problem 
theorem labelled sample 
corresponding set labels kernel matrix optimizes solving convex optimization problem min subject trace tr 
norm soft margin case follows easy extension main result previous section 
detailed proof appendix sdp 
adding additional constraint linear combination fixed kernel matrices leads sdp min subject trace tr 

specific case rank matrices orthonormal normalized eigenvectors initial kernel matrix sdp reduces qcqp manner analogous hard margin case max ct subject 
diag tr 
solving original learning problem subject extra constraint yields similar derivation max ct subject tr 

norm soft margin case non linearly separable data consider norm soft margin cost function 
training kernel minimize quantity respect minimum function particular choice expressed dual form 
optimize quantity respect kernel matrix min tr trace 
convex optimization problem restated follows 
theorem labelled sample 
corresponding set labels kernel matrix optimizes solving optimization problem 
min subject trace tr tr 
proof substitution tr defined min max tr tr trace constant 
note tr convex pointwise maximum ne functions 
convex constraints optimization problem certainly convex theorem direct extension main result section 
seen comparing 
replacing matrix tr tr tr directly yields 
sdp 
constraining linear combination fixed kernel matrices obtain min subject trace tr tr 
rank matrices orthonormal obtain qcqp see appendix max ct subject 
imposing constraint yields max ct subject tr 
similar derivation 
learning norm soft margin parameter section shows norm soft margin parameter svms learned sdp qcqp 
details de bie 

previous section tried find kernel matrix corresponding embedding yields minimal tr keeping trace constant 
similarly simultaneously automatically tune parameter quantity tr minimized proposed de bie 

consider dual formulation notice tr convex pointwise maximum ne convex functions 
secondly leads tr impose constraint trace results convex optimization problem min tr trace 
theorem restated follows min subject trace tr tr 
sdp 
imposing additional constraint linear function fixed kernel matrices obtain sdp min subject trace tr tr imposing additional constraint rank matrices obtain qcqp max ct subject 
diag diag tr 
imposing constraint yields max ct subject tr 
qcqp 
solving corresponds learning kernel matrix positive linear combination kernel matrices norm soft margin criterion simultaneously learning norm soft margin parameter comparing see reduces learning augmented kernel matrix positive linear combination kernel matrices identity matrix hard margin criterion 
important di erence evaluating resulting classifier actual kernel matrix augmented see example shawe taylor cristianini 
notice directly reduces 
corresponds automatically tuning parameter norm soft margin svm kernel matrix learning kernel matrix approach learn norm soft margin parameter automatically 
alignment section consider problem optimizing alignment set labels kernel matrix class positive semi definite kernel matrices 
show class linear combinations fixed kernel matrices problem cast sdp 
result generalizes approach cristianini 

theorem kernel matrix maximally aligned set labels tr solving optimization problem max tr yy subject trace identity matrix dimension proof want find kernel matrix maximally aligned set labels max tr yy subject trace 
equivalent optimization problem max tr yy subject trace 
express standard form semi definite program need express quadratic equality constraint lmi 
notice equivalent max tr yy subject maximizing objective linear entries optimum constraint trace achieved 
quadratic inequality constraint equivalent trace 
implies trace trace trace linearity trace 
schur complement lemma express lmi 
rewrite optimization problem max tr yy subject trace corresponds 
notice set positive semi definite matrices sdp inequality constraint corresponds dimensional lmi consider entries matrices unknowns 
case solution simply selecting tr yy alignment equal maximized 
adding additional constraint linear combination fixed kernel matrices leads max tr yy subject written standard form semi definite program similar way max tr yy subject trace 

specific case rank matrices orthonormal normalized eigenvectors initial kernel matrix semi definite program reduces qcqp see appendix max subject 
tr 
corresponds exactly qcqp obtained illustration cristianini 
entirely captured general sdp result obtained section 
solving original learning problem subject extra constraint yields max tr yy subject 
omit second constraint implied constraints 
reduces max tr yy subject tr tr tr 
expanding yields tr yy tr yy tr yy trace tr yy trace tr tr ij mm fact trace abc trace bca products defined 
obtain learning problem max subject qcqp 
induction previous sections considered transduction setting assumed covariate vectors training labelled test unlabelled data known 
setting captures realistic learning problems interest consider possible extensions approach fuller setting induction covariates known training data 
consider situation 
learn kernel matrix positive linear combination normalized kernel matrices obtained evaluation kernel function known procedure string matching kernel granting 

normalization done replacing 
case extension induction setting elegant simple 
tr number training data points labelled 
consider transduction problem tr data points unknown test point hard margin svm 
optimal weights 
learned solving max ct subject tr tr 

knowing test point entries related column row tr know tr tr normalization 
trace tr 
allows solving optimal weights 
optimal svm parameters 
tr knowing test point 
test point available complete computing tr th column row evaluate kernel function follow procedure normalize 
combining weights yields final kernel matrix label test point sign tr 
optimal weights independent number unknown test points considered setting 
consider transduction problem unknown test points unknown test point max subject tr tr 

see solving equivalent solving optimal values relate tr tr tr tr optimal weights 

tackling induction problem full generality remains challenge 
obviously consider transduction case zero test points yielding induction case 
weights constrained nonnegative furthermore matrices guaranteed positive semi definite weights reused new test points 
deal induction general sdp setting solve transduction problem new test point 
test point leads solving sdp dimension tr computationally expensive 
clearly need explore recursive solutions sdp problem allow solution sdp dimension tr solution sdp dimension tr 
solutions course immediate applications line learning problems 
error bounds transduction problem transduction access unlabelled test data labelled training data aim optimize accuracy predicting test data 
assume data fixed order chosen randomly yielding random partition training test sets 
convenience suppose training test sets size 
fix sequence pairs 


random permutation chosen uniformly 
half randomly ordered sequence training data second half test data 
function write proportion errors test data thresholded version er 
theorem shows error kernel classifier test data bounded terms average certain cost function training data margins properties kernel matrix 
define margin cost function 
notice norm soft margin cost function convex upper bound 
consider kernel classifiers obtained thresholding kernel expansions form chosen bounded norm kernel matrix ij 
constraint value margin yf distance feature space decision boundary 
notice corresponds normalized version optimal assuming simplicity loss generality see 
defined consistent defined section 
fk denote class functions form satisfying fk ij set positive semi definite matrices 
interested class kernel expansions obtained certain linear combinations fixed set 
km kernel matrices 
consider class fkb kb trace class trace theorem satisfy probability data chosen function fk er log emax expectation chosen uniformly furthermore kb trace bn min nmax trace largest eigenvalue notice test error bounded sum average training data margin cost function plus complexity penalty term depends ratio trace kernel matrix squared margin parameter kernel matrix full matrix combining test training data 
bound complexity kernel class easier check bound kb 
term minimum shows set positive linear combinations small set kernel matrices complex 
second term shows set large long largest eigenvalue dominate sum eigenvalues trace set positive linear combinations complex 
proof theorem appendix empirical results results hard margin soft margin support vector machines 
kernel matrix initial guesses kernel matrix 
polynomial kernel function gaussian kernel function exp linear kernel function normalized 
evaluating initial kernel matrices weights optimized hard margin norm soft margin norm soft margin criterion respectively semi definite programs solved general purpose optimization software sturm leading optimal weights weights constrained non negative optimized criteria second order cone programs solved general purpose optimization software andersen andersen leading optimal weights positive weights report results norm soft margin hyperparameter automatically learned 
empirical results standard benchmark datasets summarized tables 
wisconsin breast cancer dataset contained incomplete examples 
breast cancer ionosphere sonar data obtained uci repository 
heart data obtained statlog normalized 
data twonorm problem data generated specified breiman 
dataset randomly partitioned training test sets 
reported results averages random partitions 
kernel parameters tables respectively 
kernel matrices svm trained training block tr tested mixed block tr defined 
margin hard margin criterion respectively optimal soft margin cost functions soft margin criteria reported initial kernel matrices optimal furthermore average test set accuracy tsa average value average weights partitions listed 
comparison performance best soft margin svm gaussian kernel reported soft margin hyperparameter kernel parameter gaussian kernel tuned cross validation random partitions training set 
note gives rise linearly separable embedding training data case hard margin classifier indicated dash 
matrices allow training hard margin svm margin larger margin di erent components consistent sdp qcqp optimization 
soft margin criteria optimal value cost function smaller value individual consistent sdp qcqp optimizations 
notice constraining weights positive results slightly smaller margins larger cost functions expected 
furthermore number test set errors usually smaller di erent components supports error bound criteria proposed section 
notice better improve computational complexity substantially significant loss performance 
performance comparable best soft margin svm rbf kernel 
rbf svm needs additional tuning kernel parameter cross validation kernel learning approach doesn 
norm soft margin svm auto tuned hyperparameter longer need cross validation leads smaller optimal cost function compared case sm performs test set ers advantage automatically adjusting wonder di erence sdp qcqp approach twonorm data find positive weights shouldn forget values table averages sdp negative weights averages positive 
example illustrating flexibility sdp framework consider setup 
gaussian kernels respectively 
combining optimally norm soft margin svm auto tuning yields results table averages training test sets 
test set accuracies obtained competitive best soft margin svm rbf kernel tuned cross validation 
average weights show kernels selected 
ectively obtain data choice smoothing parameter recourse cross validation 
cristianini 
empirical results optimization alignment kernel matrix results show optimizing alignment improves generalization power parzen window classifiers 
explained section turns particular case sdp reduces exactly quadratic program obtained cristianini 
results provide support general framework current 
discussion new method learning kernel matrix data 
approach semi definite programming sdp ideas 
motivated fact symmetric positive definite matrix viewed kernel matrix corresponding certain embedding finite set data fact sdp deals optimization convex cost functions convex cone positive semi definite matrices convex subsets cone 
convex optimization machine learning concerns merge provide powerful methodology learning kernel matrix sdp 
focused transductive setting labelled data learn embedding applied unlabelled part data 
new generalization bound transduction shown impose convex constraints ectively control capacity search space possible kernels yield cient learning procedure implemented sdp 
furthermore approach leads directly convex method learn norm soft margin parameter support vector machines solving important open problem 
promising empirical results standard benchmark datasets reported results underline fact new approach provides principled way combine multiple kernels yield classifier may perform better individual kernel 
challenges need met research sdp learning algorithms 
clearly interest explore convex quality measures kernel matrix may appropriate learning algorithms 
example setting gaussian processes relative entropy zero mean gaussian process prior covariance kernel corresponding gaussian process approximation true intractable posterior process depends log det trace ky constant independent verify convex respect see vandenberghe 
minimizing measure respect motivated pac bayesian generalization error bounds gaussian processes see seeger achieved solving called maximum determinant problem vandenberghe general framework contains semi definite programming special case 
secondly investigation parameterizations kernel matrix important topic study 
linear combination kernels studied useful practical problems capturing notion combining gram matrix experts worth considering parameterizations 
parameterizations respect constraint quality measure kernel matrix convex respect parameters proposed parameterization 
class examples arises positive definite matrix completion problem vandenberghe 
symmetric kernel matrix entries fixed 
remaining entries parameters case chosen resulting matrix positive definite simultaneously certain cost function optimized trace sk log det matrix 
specific case reduces solving maximum determinant problem convex unknown entries parameters proposed parametrization 
third important area research consists finding faster implementations semidefinite programming 
case quadratic programming platt special purpose methods developed exploit exchangeable nature learning problem classification result cient algorithms 
proof result case orthonormal original learning problem min tr subject trace min max tr subject trace recall tr diag tr diag tr tr tr tr tr tr tr tr 
furthermore orthonormal eigenvalues implies trace 
write min max diag diag min max diag diag min max min max max min diag interchanged order minimization maximization 
standard results convex optimization see boyd vandenberghe imply allowed obtain optimal value objective convex linear concave minimization problem strictly feasible maximization problem skip case elements having sign consider margin case 
obtain max min max max max max reformulated follows max ct subject 
gives result 
proof theorem substitution tr defined min max tr trace constant 
assume tr tr extended general case 
note tr convex pointwise maximum ne functions 
convex constraints optimization problem certainly convex write min max tr trace 
express max tr lmi 
done exactly way hard margin express constraint dual minimization problem 
allow drop minimization schur complement lemma obtain lmi 
define lagrangian maximization problem tr ce duality tr max min min max 
similarly 
tr optimum tr form dual problem tr min tr 
obtain constraint tr true exist tr equivalently schur complement lemma tr holds 
account expressed min subject trace tr yields 
notice diag lmi similarly 
proof result case orthonormal learning problem min tr subject trace min max tr tr subject trace equivalent min max tr subject trace comparing see formulations identical terms objective function depend kernel matrix weights 
terms conserved entire derivation hard margin case easily checked appendix reason result norm soft margin case obtained replacing result hard margin case 
yields max ct subject 
diag diag tr 
exactly 
proof result case orthonormal original learning problem max tr yy subject working gives tr yy trace tr tr yy trace tr tr yy trace yy trace trace kk trace trace trace trace trace tr 
fact trace abc trace bca products welldefined vectors 
orthonormal ij furthermore orthogonal eigenvalues implies 

obtain optimization problem max subject 
yields result 
proof theorem function define 
proof part involves steps step 
class real functions defined sup er sup 
see notice er average test set indicator function bounds function 
step 
class valued functions pr sup sup exp expectation random permutation 
follows mcdiarmid inequality 
see need define random permutation set independent random variables 
choose 
uniformly random interval 
surely distinct 

define position random variables ordered size 
easy see changes changes 
mcdiarmid inequality implies result 
step 
class valued functions sup sup expectation independent uniform valued random variables 
result essentially lemma bartlett mendelson lemma contained similar bound argument holds fixed randomly permuted 
step 
class real valued functions defined closed negations defines yf 
bound contraction lemma ledoux talagrand 
step 
class fk kernel expansions notice proof lemma bartlett mendelson max fk emax max emax emax 
vector rademacher random variables 
combining gives part theorem 
second part consider kb max kb emax max 
matrix satisfies conditions trace trace trace trace sum positive supremum achieved trace write kb max kb trace 
notice maximum eigenvalue trace shows kb bn 
max emax emax trace 
term maximum non negative replace sum show trace bm 
alternatively write maximum eigenvalue shows trace acknowledge support onr muri nsf iis 
sincere de bie helpful conversations suggestions 
andersen andersen 

interior point optimizer linear programming implementation homogeneous algorithm 
roos terlaky zhang editors high performance optimization pages 
kluwer academic publishers 
bartlett mendelson 

rademacher gaussian complexities risk bounds structural results 
technical report australian national university 
bennett bredensteiner 

duality geometry svm classifiers 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 
boyd vandenberghe 

convex optimization 
course notes ee stanford university 
available www stanford edu class ee 
breiman 

arcing classifiers 
annals statistics 
cristianini shawe taylor 

support vector machines 
cambridge university press cambridge cristianini shawe taylor kandola 

kernel target alignment 
dietterich becker ghahramani editors advances neural information processing systems cambridge ma 
mit press 
cristianini shawe taylor kandola 

kernel target alignment 
advances neural information processing systems 
mit press cambridge ma 
de bie lanckriet cristianini 

convex optimization norm soft margin parameter support vector machines 
technical report progress university california berkeley dept eecs 
ledoux talagrand 

probability banach spaces processes 
springer verlag new york ny 
nesterov nemirovsky 

interior point polynomial methods convex programming theory applications 
siam philadelphia pa platt 

sparseness analytic qp speed training support vector machines 
kearns solla editor advances neural information processing systems cambridge ma 
mit press 
scholkopf smola 

learning kernels 
mit press cambridge ma 
seeger 

pac bayesian generalization error bounds gaussian process classification 
technical report edi inf rr university edinburgh division informatics 
shawe taylor cristianini 

soft margin margin distribution 
smola scholkopf bartlett schuurmans editors advances large margin classifiers 
mit press 
sturm 

matlab toolbox optimization symmetric cones 
optimization methods software 
special issue interior point methods cd supplement software 
vandenberghe boyd 

semidefinite programming 
siam review 
vandenberghe boyd wu 

determinant maximization linear matrix inequality constraints 
siam journal matrix analysis applications 
rbf breast cancer hm tsa sm tsa sm tsa sm tsa ionosphere hm tsa sm tsa sm tsa sm tsa table svms trained tested initial kernel matrices optimal kernel matrices hard margin svms hm resulting margin dash meaning hard margin classifier soft margin svms sm norm soft margin sm norm soft margin sm norm soft margin auto tuning optimal value cost function 
furthermore test set accuracy tsa average weights average values 
trace hm sm sm 
initial kernel matrices evaluated multiplied 
assures compare di erent hm sm sm resulting kernel matrix constant trace scale 
sm trace trace 
allows comparing di erent sm allows comparing sm sm choose sm trace constant cases scale 
column rbf reports performance best soft margin svm rbf kernel tuned cross validation 
rbf heart hm tsa sm tsa sm tsa sm tsa sonar hm tsa sm tsa sm tsa sm tsa table see table explanation 
rbf twonorm hm tsa sm tsa sm tsa sm tsa table see table explanation 
tsa sm tsa rbf breast cancer ionosphere heart sonar twonorm table initial kernel matrices gaussian kernels respectively 
trace trace 
average weights optimal kernel matrix norm soft margin svm auto tuning average value 
test set accuracies tsa optimal norm soft margin svm auto tuning sm best soft margin svm rbf kernel rbf reported 

