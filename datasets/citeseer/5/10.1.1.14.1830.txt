crawling web gautam pant srinivasan menczer department management sciences school library information science university iowa iowa city ia usa email gautam pant srinivasan menczer uiowa edu summary 
large size dynamic nature web highlight need continuous support updating web information retrieval systems 
crawlers facilitate process hyperlinks web pages automatically download partial snapshot web 
systems rely crawlers exhaustively crawl web incorporate focus crawlers harvest application topic specific collections 
discuss basic issues related developing crawling infrastructure 
followed review topical crawling algorithms evaluation metrics may judge performance 
innovative applications web crawling invented take brief look developed past 
web crawlers programs exploit graph structure web move page page 
infancy programs called robots spiders fish worms words quite evocative web imagery 
may observed noun crawler indicative speed programs considerably fast 
experience able crawl tens thousands pages minutes 
key motivation designing web crawlers retrieve web pages add representations local repository 
repository may serve particular application needs web search engine 
simplest form crawler starts seed page uses external links attend pages 
process repeats new pages offering external links follow sufficient number pages identified higher level objective pentium workstation internet connection 
gautam pant reached 
simple description lies host issues related network connections spider traps urls parsing html pages ethics dealing remote web servers 
fact current generation web crawler sophisticated fragile parts application embedded 
web static collection pages little long term crawling 
pages fetched repository search engine database need crawling 
web dynamic entity subspaces evolving differing rapid rates 
continual need crawlers help applications stay current new pages added old ones deleted moved modified 
general purpose search engines serving entry points web pages strive coverage broad possible 
web crawlers maintain index databases amortizing cost crawling indexing millions queries received 
crawlers blind exhaustive approach comprehensiveness major goal 
contrast crawlers selective pages fetch referred preferential heuristic crawlers 
may building focused repositories automating resource discovery facilitating software agents 
vast literature preferential crawling applications including :10.1.1.43.1111:10.1.1.1.7474
preferential crawlers built retrieve pages certain topic called topical focused crawlers 
search engines topical crawlers certainly possible specialized responsibility identifying subspaces relevant particular communities users 
techniques preferential crawling focus improving freshness search engine suggested 
significant portion chapter devoted description crawlers general slant particularly sections topical crawlers 
dimensions topical crawlers exciting object study 
key question motivated research crawler selectivity achieved 
rich contextual aspects goals parent application lexical signals web pages features graph built pages seen reasonable kinds evidence exploit 
additionally crawlers differ mechanisms evidence available 
second major aspect important consider studying crawlers especially topical crawlers nature crawl task 
crawl characteristics queries keywords provided input criteria crawler user profiles desired properties pages fetched similar pages popular pages authoritative pages lead significant differences crawler design implementation 
task constrained parameters maximum number pages fetched long crawls vs short crawls available memory 
crawling task crawling web viewed constrained multi objective search problem 
wide variety objective functions coupled lack appropriate knowledge search space problem hard 
furthermore crawler may deal optimization issues local vs global optima :10.1.1.1.9512
key dimension regarding crawler evaluation strategies necessary comparisons determine circumstances crawlers best 
comparisons fair eye drawing statistically significant differences 
require sufficient number crawl runs sound methodologies consider temporal nature crawler outputs 
significant challenges evaluation include general unavailability relevant sets particular topics queries 
evaluation typically relies defining measures estimating page importance 
part chapter presents crawling infrastructure describes basic concepts web crawling 
review number crawling algorithms suggested literature 
discuss current methods evaluate compare performance different crawlers 
outline web crawlers applications 
building crawling infrastructure shows flow basic sequential crawler section consider multi threaded crawlers 
crawler maintains list unvisited urls called frontier 
list initialized seed urls may provided user program 
crawling loop involves picking url crawl frontier fetching page corresponding url parsing retrieved page extract urls application specific information adding unvisited urls frontier 
urls added frontier may assigned score represents estimated benefit visiting page corresponding url 
crawling process may terminated certain number pages crawled 
crawler ready crawl page frontier empty situation signals dead crawler 
crawler new page fetch stops 
crawling viewed graph search problem 
web seen large graph pages nodes hyperlinks edges 
crawler starts nodes seeds follows edges reach nodes 
process fetching page extracting links analogous expanding node graph search 
topical crawler tries follow edges expected lead portions graph relevant topic 
frontier frontier list crawler contains urls unvisited pages 
graph search terminology frontier open list unexpanded gautam pant crawling loop initialize frontier seed urls check termination done done start pick url frontier url fetch page parse page add urls frontier url fig 

flow basic sequential crawler unvisited nodes 
may necessary store frontier disk large scale crawlers represent frontier memory data structure simplicity 
available memory decide maximum size frontier 
due large amount memory available pcs today frontier size urls exceptional 
maximum frontier size need mechanism decide urls ignore limit reached 
note frontier fill quickly pages crawled 
expect urls frontier crawl pages assuming average links page 
frontier may implemented fifo queue case breadth crawler blindly crawl web 
url crawl comes head queue new urls added tail queue 
due limited size frontier need sure add duplicate urls frontier 
linear search find newly extracted url frontier costly 
solution allocate amount available memory maintain separate hash table url key store frontier urls fast lookup 
hash table kept synchronized actual frontier 
time consuming alternative maintain frontier hash table url key 
provide fast lookup avoiding duplicate urls 
time crawler needs url crawl need search pick url earliest time stamp time url added frontier 
memory issue speed solution may preferred 
frontier reaches maximum size breadth crawler add unvisited url new page crawled 
crawling web frontier implemented priority queue preferential crawler known best crawler 
priority queue may dynamic array kept sorted estimated score unvisited urls 
step best url picked head queue 
corresponding page fetched urls extracted scored heuristic 
added frontier manner order priority queue maintained 
avoid duplicate urls frontier keeping separate hash table lookup 
frontier maximum size max exceeded best max urls kept frontier 
crawler finds frontier empty needs url crawl crawling process comes halt 
large value max seed urls frontier rarely reach empty state 
times crawler may encounter spider trap leads large number different urls refer page 
way alleviate problem limiting number pages crawler accesses domain 
code associated frontier sure consecutive sequence say urls picked crawler contains url fully qualified host name www cnn com 
side effects crawler polite accessing web site crawled pages tend diverse 
history page repository crawl history time stamped list urls fetched crawler 
effect shows path crawler web starting seed pages 
url entry history fetching corresponding page 
history may post crawl analysis evaluations 
example associate value page crawl path identify significant events discovery excellent resource 
history may stored occasionally disk maintained memory data structure 
provides fast lookup check page crawled 
check important avoid revisiting pages avoid adding urls crawled pages limited size frontier 
reasons important urls section adding history 
page fetched may stored indexed master application search engine 
simplest form page repository may store crawled pages separate files 
case page map unique file name 
way map page url compact string form hashing function low probability collisions uniqueness file names 
resulting hash value file name 
md way hashing function provides bit hash code url 
implementations md hashing algorithms readily available different programming languages gautam pant 
refer java security framework 
bit hash value converted character hexadecimal equivalent get file name 
example content www uiowa edu stored file named fcb ec 
way fixed length file names urls arbitrary size 
course application needs cache pages may simpler hashing mechanism 
page repository check url crawled converting character file name checking existence file repository 
cases may render unnecessary memory history data structure 
fetching order fetch web page need client sends request page reads response 
client needs timeouts sure unnecessary amount time spent slow servers reading large pages 
fact may typically restrict client download kb page 
client needs parse response headers status codes redirections 
may parse store modified header determine age document 
exception handling important page fetching process need deal millions remote servers code 
addition may beneficial collect statistics timeouts status codes identifying problems automatically changing timeout values 
modern programming languages java perl provide simple multiple programmatic interfaces fetching pages web 
careful high level interfaces may harder find lower level problems 
example java may want java net socket class send requests ready java net class 
discussion crawling pages web complete talking robot exclusion protocol 
protocol provides mechanism web server administrators communicate file access policies specifically identify files may accessed crawler 
done keeping file named robots txt root directory web server www biz uiowa edu robots txt 
file provides access policy different user agents robots crawlers 
user agent value denotes default policy crawler match user agent values file 
number disallow entries may provided user agent 
url starts value disallow field retrieved crawler matching user agent 
crawler wants retrieve page web server fetch appropriate robots txt file sure url fetched java sun com crawling web disallowed 
details exclusion protocol www org wc html 
efficient cache access policies number servers visited crawler 
avoid accessing robots txt file time need fetch url 
sure cache entries remain sufficiently fresh 
parsing page fetched need parse content extract information feed possibly guide path crawler 
parsing may imply simple hyperlink url extraction may involve complex process tidying html content order analyze html tag tree see section 
parsing involve steps convert extracted url canonical form remove stopwords page content stem remaining words 
components parsing described 
url extraction canonicalization html parsers freely available different languages 
provide functionality easily identify html tags associated attribute value pairs html document 
order extract hyperlink urls web page parsers find anchor tags grab values associated href attributes 
need convert relative urls absolute urls base url page retrieved 
different urls correspond web page mapped single canonical form 
important order avoid fetching page times 
steps typical url canonicalization procedures convert protocol hostname lowercase 
example www uiowa edu converted www uiowa edu 
remove anchor part url 
biz uiowa edu faq html reduced biz uiowa edu faq html 
perform url encoding commonly characters 
prevent crawler treating dollar biz uiowa 
edu pant different url dollar biz uiowa edu 
urls add trailing dollar biz uiowa edu dollar biz uiowa edu map canonical form 
decision add trailing require heuristics cases 
gautam pant heuristics recognize default web pages 
file names index html index htm may removed url assumption default files 
true retrieved simply base url 
remove parent directory url path 
url path seeds dat reduced dat 
leave port numbers url port 
alternative leave port numbers url add port port number specified 
important consistent applying canonicalization rules 
possible seemingly opposite rules equally port numbers long apply consistently urls 
canonicalization rules may applied application prior knowledge sites known mirrors 
noted earlier spider traps pose serious problem crawler 
dummy urls created spider traps increasingly larger size 
way tackle traps limiting url sizes say characters 
stemming parsing web page extract content information order score new urls suggested page helpful remove commonly words stopwords 
process removing stopwords text called 
note systems dialog recognize words stopwords 
addition may stem words page 
stemming process normalizes words number morphologically similar words single root form stem 
example connect connected connection reduced connect implementations commonly porter stemming algorithm easily available programming languages :10.1.1.1.9512
authors experienced cases bio medical domain stemming reduced precision crawling results 
html tag tree crawlers may assess value url content word examining html tag context resides 
crawler may need utilize tag tree dom structure html page :10.1.1.6.1212
shows example list stopwords refer www dcs gla ac uk idom ir resources linguistic utils words www dialog com crawling web tag tree corresponding html source 
html tag forms root tree various tags texts form nodes tree 
unfortunately web pages contain badly written html 
example start tag may tag may required html specification tags may properly nested 
cases html tag body tag missing html page 
structure criteria require prior step converting dirty html document formed process called tidying html page 
includes insertion missing tags reordering tags page 
tidying html page necessary mapping content page tree structure integrity node single parent 
essential precursor analyzing html page tag tree 
note analyzing dom structure necessary topical crawler intends html document structure non trivial manner 
example crawler needs links page text portions text page simpler html parsers 
parsers readily available languages 
html head title projects title head body projects ul li href blink html lamp linkage analysis multiple processors li li href nice html nice network infrastructure combinatorial exploration li li href html dna sequence assembly algorithm li li href dali html dali distributed adaptive order logic theorem prover li ul body html title text head html text body multi threaded crawlers ul li text fig 

html page corresponding tag tree sequential crawling loop spends large amount time cpu idle network disk access network interface idle cpu operations 
multi threading thread follows crawling www org people tidy gautam pant loop provide reasonable speed efficient available bandwidth 
shows multi threaded version basic crawler 
note thread starts locking frontier pick url crawl 
picking url unlocks frontier allowing threads access 
frontier locked new urls added 
locking steps necessary order synchronize frontier shared crawling loops threads 
model multi threaded crawler follows standard parallel computing model 
note typical crawler maintain shared history data structure fast lookup urls crawled 
addition frontier need synchronize access history 
multi threaded crawler model needs deal empty frontier just sequential crawler 
issue simple 
thread finds frontier empty automatically mean crawler reached dead 
possible threads fetching pages may add new urls near 
way deal situation sending thread sleep state sees empty frontier 
thread wakes checks urls 
global monitor keeps track number threads currently sleeping 
threads sleep state crawling process 
optimizations performed multi threaded model described instance decrease contentions threads streamline network access 
section described general components crawler 
common infrastructure supports extreme simple breadth crawler crawler algorithms may involve complex url selection mechanisms 
factors frontier size page parsing strategy crawler history page repository identified interesting important dimensions crawler definitions 
crawling algorithms discuss number crawling algorithms suggested literature 
note algorithms variations best scheme 
difference heuristics score unvisited urls algorithms adapting tuning parameters crawl 
naive best crawler naive best crawlers detailed evaluated authors extensive study crawler evaluation 
crawler represents fetched web page vector words weighted occurrence frequency 
crawler computes cosine similarity page query description provided user scores unvisited urls thread frontier get url add urls done check termination done lock frontier pick url frontier unlock frontier fetch page parse page lock frontier add urls frontier unlock frontier thread get url add urls crawling web done check termination done lock frontier pick url frontier unlock frontier fetch page parse page lock frontier add urls frontier unlock frontier fig 

multi threaded crawler model page similarity value 
urls added frontier maintained priority queue scores 
iteration crawler thread picks best url frontier crawl returns new unvisited urls inserted priority queue scored cosine similarity parent page 
cosine similarity page query computed sim vq vp vq vp vq vp term frequency tf vector representations query page respectively vq vp dot inner product vectors euclidean norm vector sophisticated vector representation pages tf idf weighting scheme information retrieval problematic crawling applications priori knowledge distribution terms crawled pages 
multiple thread implementation crawler acts best crawler function number simultaneously running threads 
best generalized version best crawler picks best urls crawl time 
research best crawler strong competitor showing clear superiority retrieval relevant pages :10.1.1.1.9512
note best crawler keeps gautam pant frontier size upper bound retaining best urls assigned similarity scores 
version improvements 
uses similarity measure naive best crawler estimating relevance unvisited url 
refined notion potential scores links crawl frontier 
anchor text text surrounding links link context inherited score ancestors influence potential scores links 
ancestors url pages appeared crawl path url 
predecessor maintains depth bound 
crawler finds unimportant pages crawl path stops crawling path 
able track information url frontier associated depth potential score 
depth bound provided user potential score unvisited url computed score url inherited url neighborhood url parameter neighborhood score signifies contextual evidence page contains hyperlink url inherited score obtained scores ancestors url 
precisely inherited score computed inherited url sim sim inherited parameter query page url extracted 
neighborhood score uses anchor text text vicinity anchor attempt refine score url allowing differentiation links page 
purpose crawler assigns anchor score context score url 
anchor score simply similarity anchor text hyperlink containing url query sim anchor text 
context score hand broadens context link include nearby words 
resulting augmented context aug context computing context score follows context url anchor url sim aug context derive neighborhood score anchor score context score neighborhood url anchor url context url crawling web parameter 
note implementation need preset different parameters 
values suggested 
focused crawler focused crawler hypertext classifier developed chakrabarti :10.1.1.43.1111
basic idea crawler classify crawled pages categories topic taxonomy 
crawler requires topic taxonomy yahoo odp addition user provides example urls interest bookmark file 
example urls get automatically classified various categories taxonomy 
interactive process user correct automatic classification add new categories taxonomy mark categories interest user 
crawler uses example urls build bayesian classifier find probability pr crawled page belongs category taxonomy 
note definition pr root category taxonomy 
relevance score associated crawled page computed pr crawler soft focused mode uses relevance score crawled page score unvisited urls extracted 
scored urls added frontier 
manner similar naive best crawler picks best url crawl 
hard focused mode crawled page classifier finds leaf node taxonomy maximum probability including parents taxonomy marked user urls crawled page extracted added frontier 
interesting element focused crawler distiller 
distiller applies modified version kleinberg algorithm find topical hubs 
hubs provide links authoritative sources topic 
distiller activated various times crawl top hubs added frontier 
context focused crawler context focused crawlers bayesian classifiers guide crawl 
focused crawler described classifiers trained estimate link distance crawled page relevant pages 
appreciate value estimation dmoz org gautam pant browsing experiences 
looking papers numerical analysis may go home pages math computer science departments move faculty pages may lead relevant papers 
math department web site may words numerical analysis home page 
crawler naive best crawler put page low priority may visit 
crawler estimate relevant numerical analysis probably links away way giving home page math department higher priority home page law school 
context focused crawler trained context graph layers corresponding seed page 
seed page forms layer graph 
pages corresponding links seed page layer 
inlinks layer pages layer pages 
obtain links pages layer search engine 
depicts context graph www biz uiowa edu programs seed 
context graphs seeds obtained pages layer number graph combined single layer 
gives new set layers called merged context graph 
followed feature selection stage seed pages possibly layer pages concatenated single large document 
tf idf scoring scheme top terms identified document represent vocabulary feature space classification 
set naive bayes classifiers built layer merged context graph 
pages layer compute pr cl probability occurrence term class cl corresponding layer prior probability pr cl assigned class number layers 
probability page belonging class cl computed pr cl 
probabilities computed class 
class highest probability treated winning class layer 
probability winning class threshold crawled page classified class 
class represents pages fit classes context graph 
probability winning class exceed threshold page classified winning class 
set classifiers corresponding context graph provides mechanism estimate link distance crawled page relevant pages 
mechanism works math department home page get classified layer law school home page get classified crawler maintains queue class containing pages crawled classified class 
queue sorted probability scores pr cl 
crawler needs url crawl picks top page non empty queue smallest tend pick pages closer relevant pages 
links pages get explored links pages far away relevant portions web 
layer layer layer www biz uiowa edu programs www biz uiowa edu www com list business html crawling web fig 

context graph adaptive population agents search pages relevant topic 
agent essentially crawling loop section adaptive query list neural net decide links follow 
algorithm provides exclusive frontier agent 
multi threaded implementation see section agent corresponds thread execution 
thread non contentious access frontier 
note algorithms described chapter may implemented similarly frontier thread 
original algorithm see agent kept frontier limited links page fetched agent 
due limited memory approach crawler limited links current page outperformed naive best crawler number evaluation criterion 
number improvements inspired naive best original algorithm designed retaining capability learn link estimates neural nets focus search promising areas selective reproduction 
fact redesigned version algorithm outperform various versions naive crawlers specific crawling tasks crawls longer pages 
adaptive representation agent consists list keywords initialized query description neural net evaluate new links 
input unit neural net receives count frequency keyword occurs vicinity link traversed weighted give importance keywords occurring near link maximum anchor text 
single output unit 
output neural net numerical quality estimate link considered input 
estimates combined estimates cosine similarity equation agent keyword vector page containing links 
parameter regulates relative gautam pant importance estimates neural net versus parent page 
combined score agent uses stochastic selector pick links frontier probability pr url local frontier combined score 
parameter regulates greediness link selector 
new page fetched agent receives energy proportion similarity keyword vector new page 
agent neural net trained improve link estimates predicting similarity new page inputs page contained link leading 
back propagation algorithm learning 
learning technique provides unique capability adapt link behavior course crawl associating relevance estimates particular patterns keyword frequencies links 
agent energy level determine agent reproduce visiting page 
agent reproduces energy level passes constant threshold 
reproduction meant bias search areas agents lead pages 
reproduction offspring new agent thread receives half parent link frontier 
offspring keyword vector mutated expanded adding term frequent parent current document 
term addition strategy limited way comparable classifiers section try identify lexical cues appear pages leading relevant pages 
section variety crawling algorithms variations best scheme 
readers may pursue menczer details algorithmic issues related crawlers 
evaluation crawlers general sense crawler especially topical crawler may evaluated ability retrieve pages 
major hurdle problem recognizing pages 
operational environment real users may judge relevance pages crawled allowing determine crawl successful 
unfortunately meaningful experiments involving real users assessing web crawls extremely problematic 
instance scale web suggests order obtain reasonable notion crawl effectiveness conduct large number crawls involve large number users 
secondly crawls live web pose serious time constraints 
crawls short lived ones overly burdensome crawling web user 
may choose avoid time loads showing user results full crawl limits extent crawl 
distant majority direct consumers information web agents working behalf humans web agents humans 
quite reasonable explore crawlers context parameters crawl time crawl distance may limits human acceptance imposed user experimentation 
general important compare topical crawlers large number topics tasks 
allow ascertain statistical significance particular benefits may observe crawlers 
crawler evaluation research requires appropriate set metrics 
research reveals innovative performance measures 
observe basic dimensions assessment process 
need measure crawled page importance secondly need method summarize performance set crawled pages 
page importance enumerate methods measure page importance 

keywords document page considered relevant contains keywords query 
frequency keywords appear page may considered 

similarity query user specifies information need short query 
cases longer description need may available 
similarity short long description crawled page may judge page relevance 

similarity seed pages pages corresponding seed urls measure relevance page crawled 
seed pages combined single document cosine similarity document crawled page page relevance score 

classifier score classifier may trained identify pages relevant information need task 
training done seed pre specified relevant pages positive examples 
trained classifier provide boolean continuous relevance scores crawled pages :10.1.1.43.1111

retrieval system rank different crawlers started seeds allowed run till crawler gathers pages 
pages collected crawlers ranked initiating query description retrieval system smart 
rank provided retrieval system page relevance score 
gautam pant 
link popularity may algorithms pagerank hits provide popularity estimates crawled pages 
simpler method just number links crawled page derive similar information 
variations link methods topical weights choices measuring topical popularity pages 
summary analysis particular measure page importance summarize performance crawler metrics analogous information retrieval ir measures precision recall 
precision fraction retrieved crawled pages relevant recall fraction relevant pages retrieved crawled 
usual ir task notion relevant set recall restricted collection database 
considering web large collection relevant set generally unknown web ir tasks 
explicit recall hard measure 
authors provide precision measures easier compute order evaluate crawlers 
discuss precision measures 
acquisition rate case boolean relevance scores measure explicit rate pages 
relevant pages pages crawled acquisition rate harvest rate pages 

average relevance relevance scores continuous averaged crawled pages 
general form harvest rate :10.1.1.43.1111
scores may provided simple cosine similarity trained classifier 
averages see may computed progress crawl pages pages 
running averages calculated window pages pages current crawl point 
measures analogous recall hard compute web authors resort indirect indicators estimating recall 
indicators 
target recall set known relevant urls split disjoint sets targets seeds 
crawler started seeds pages recall targets measured 
target recall computed target recall pt pc pt pt set target pages pc set crawled pages 
recall target set estimate recall relevant pages 
gives schematic justification measure 
note underlying assumption targets random subset relevant pages 
pt targets pr relevant crawled pc crawling web fig 

performance metric pt pc pt estimate pr pc pr 
robustness seed urls split disjoint sets sa sb 
set initialize instance crawler 
overlap pages crawled starting disjoint sets measured 
large overlap interpreted robustness crawler covering relevant portions web 
metrics measure crawler performance manner combines precision recall 
example search length measures number pages crawled certain percentage relevant pages retrieved 
shows example performance plots different crawlers 
crawler performance depicted trajectory time approximated crawled pages 
naive best crawler outperform breadth crawler evaluations topics pages crawled crawler topic evaluation involves millions pages 
average precision breadth naive best pages crawled average target recall breadth naive best pages crawled fig 

performance plots average precision similarity topic description average target recall 
averages calculated topics error bars show standard error 
tailed test alternative hypothesis naive best crawler outperforms breadth crawler pages generates values performance metrics 
gautam pant section outlined methods assessing page importance measures summarize crawler performance 
conducting fresh crawl experiment important select evaluation approach provides reasonably complete sufficiently detailed picture crawlers compared 
applications briefly review applications crawlers 
intent comprehensive simply highlight utility 
query time crawlers java applet implements naive best algorithms 
applet available online 
multi threaded crawlers started user submits query 
results displayed dynamically crawler finds pages 
user may browse results crawling continues background 
multi threaded implementation applet deviates general model specified 
line autonomous multi agent nature algorithm section thread separate frontier 
applies naive best algorithm 
thread independent non contentious access frontier 
applet allows user specify crawling algorithm maximum number pages fetch 
order initiate crawl system uses google web api obtain seeds pages 
crawler threads started seeds crawling continues required number pages fetched frontier empty 
shows working user query algorithm 
cora building topic specific portals topical crawler may build topic specific portals sites index research papers 
application developed mccallum collected maintained research papers computer science cora 
crawler application reinforcement learning rl allows finding crawling policies lead immediate long term benefits 
benefits discounted far away current page 
hyperlink expected immediately lead relevant page preferred bear fruit links 
need consider benefit crawl path motivated biz uiowa edu www google com apis crawling web fig 

user interface crawl algorithm 
search process spider reproduced visible expandable tree right 
spider details revealed clicking tree left crawl top hits spider seeds 
hit viewed clicking url results frame 
fact lexical similarity pages falls rapidly increasing link distance 
noted earlier math department home page leads numerical analysis may provide little lexical signal naive best crawler searching 
motivation rl crawling algorithm similar context focused crawler 
rl crawler trained known paths relevant pages 
trained crawler estimate benefit hyperlink 
building topical site maps approach building site maps start seed url crawl breadth manner certain number pages retrieved certain depth reached 
site map may displayed graph connected pages 
interested building site map focusses certain topic mentioned approach lead large number unrelated pages crawl greater depths fetch pages 
corrects shark search see gautam pant section guide crawler build visual graph highlights relevant pages 
letizia browsing agent letizia agent assists user browsing 
user web letizia tries understand user interests pages browsed 
agent follows hyperlinks starting current page browsed find pages interest user 
hyperlinks crawled automatically breadth manner 
user interrupted suggested pages possible interest needs recommendations 
agents topical locality web provide context sensitive results 
applications crawling general topical crawling particular applied various applications appear technical papers 
example business intelligence gain topical crawling 
large number companies web sites describe current objectives plans product lines 
areas business large number start companies rapidly changing web sites 
factors important various business entities sources general purpose search engines keep track relevant publicly available information potential competitors collaborators 
crawlers biomedical applications finding relevant literature gene 
different note controversial applications crawlers extracting email addresses web sites spamming 
due dynamism web crawling forms back bone applications facilitate web information retrieval 
typical crawlers creating maintaining indexes general purpose diverse usage topical crawlers emerging client server applications 
topical crawlers important tools support applications specialized web portals online searching competitive intelligence 
number topical crawling algorithms proposed literature 
evaluation crawlers done comparing crawlers limited number queries tasks considerations statistical significance 
anecdotal results important suffice crawling web thorough performance comparisons 
web crawling field matures disparate crawling strategies evaluated compared common tasks defined performance measures 
see sophisticated usage hypertext structure link analysis crawlers 
current example chakrabarti suggested pages html tag tree dom structure focusing crawler 
shown benefit dom structure thorough study merits structure different ways crawling warranted :10.1.1.6.1212
topical crawlers depend various cues crawled pages prioritize fetching unvisited urls 
thorough understanding relative importance cues extended anchortext link analysis ancestor pages needed 
potential area research stronger collaboration search engines crawlers crawlers :10.1.1.2.700
scalability benefits distributed topical crawling fully realized 
crawlers help search engine focus user interests 
search engine help crawler focus topic 
crawler machine help crawler 
questions motivate research crawler applications 

aggarwal yu 
intelligent crawling world wide web arbitrary predicates 
www hong kong may 

amento terveen hill 
authority mean quality 
predicting expert quality ratings web documents 
proc 
th annual intl 
acm sigir conf 
research development information retrieval 

arasu cho garcia molina paepcke raghavan 
searching web 
acm transactions internet technology 

bharat henzinger 
improved algorithms topic distillation hyperlinked environment 
proceedings st annual international acm sigir conference research development information retrieval 

sergey brin lawrence page 
anatomy large scale hypertextual web search engine 
computer networks isdn systems 

chakrabarti 
mining web 
morgan kaufmann 

chakrabarti dom gibson kleinberg raghavan rajagopalan 
automatic resource list compilation analyzing hyperlink structure associated text 
proceedings th international world wide web conference 

chakrabarti 
accelerated focused crawling online relevance feedback 
www hawaii may 

chakrabarti van den berg dom 
focused crawling new approach topic specific web resource discovery 
computer networks 

cho garcia molina page 
efficient crawling url ordering 
computer networks 
gautam pant 
davison 
topical locality web 
proc 
rd annual intl 
acm sigir conf 
research development information retrieval 

de bra post 
information retrieval world wide web making client searching feasible 
proc 
st international world wide web conference 

diligenti coetzee lawrence giles gori 
focused crawling context graphs 
proc 
th international conference large databases vldb pages cairo egypt 

eichmann 
ethical web agents 
second international world wide web conference pages 

maarek pelleg ur 
shark search algorithm application tailored web site mapping 
www 

kleinberg 
authoritative sources hyperlinked environment 
journal acm 

kumar gupta karypis 
parallel computing design analysis algorithms 
benjamin cummings 

lieberman christopher 
exploring web reconnaissance agents 
communications acm august 

mccallum nigam rennie seymore 
automating construction internet portals machine learning 
information retrieval 

menczer belew 
adaptive retrieval agents internalizing local context scaling web 
machine learning 

menczer pant ruiz srinivasan 
evaluating topic driven web crawlers 
proc 
th annual intl 
acm sigir conf 
research development information retrieval 

menczer pant srinivasan 
topical web crawlers evaluating adaptive algorithms 
appear acm trans 
internet technologies 
dollar biz uiowa edu fil papers toit pdf 

pant 
deriving link context html tag tree 
th acm sigmod workshop research issues data mining knowledge discovery 

pant menczer 
evolve intelligent web crawlers 
autonomous agents multi agent systems 

pant menczer 
topical crawling business intelligence 
proc 
th european conference digital libraries 

pant srinivasan menczer 
exploration versus exploitation topic driven crawlers 
www workshop web dynamics 

pant bradshaw menczer 
search engine crawler symbiosis adapting community interests 
proc 
th european conference digital libraries 

porter 
algorithm suffix stripping 
program 

raghavan rajagopalan sivakumar tomkins upfal 
stochastic models web graph 
focs pages nov 

rennie mccallum 
reinforcement learning spider web efficiently 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 
crawling web 
salton mcgill 
modern information retrieval 
mcgraw hill 

srinivasan mitchell pant menczer 
web crawling agents retrieving biomedical information 
agents bioinformatics bologna italy 
