hierarchical approach pomdp planning execution pineau cs cmu edu nicholas roy ri cmu edu sebastian thrun thrun cs cmu edu robotics institute school computer science forbes ave pittsburgh pa usa presents hierarchical approach pomdps takes advantage structure problem domain nd modular policies complex tasks 
decomposition partitioning action space specialized groups related actions 
illustrate appropriateness approach providing empirical results contrasting domains 

vast majority ai planning focused situations state environment fully observable russell norvig 
realworld applications far true 
partially observable markov decision processes pomdps sondik provide general planning decision making framework acting optimally partially observable domains gained attention aaai 
computational cost nding optimal policy agent signi cantly limits approach preventing successful application pomdps complex problems 
real world domains structure exploited nd policies complex problems 
idea leveraging structure address large problems explored markov decision processes mdps solve complex problems singh dayan hinton dietterich :10.1.1.32.8206
unfortunately solutions directly applicable pomdps assume state environment observable transitioning di erent sub modules conditioned state environment 
structure pomdps preliminary attempts wiering schmidhuber typically strict assumptions prior knowledge low level tasks ordering substantially restrictive 
memory approaches hierarchical pomdps proposed hernandez mahadevan amount training data required exceeds available problems especially dialogue management domains interested 
single idea underlying approach decompose domain actions 
task domains space actions naturally decomposes hierarchy actions characterizes applicability groups actions di erent situations 
consider example mobile robotic assistant 
actions involved navigation move turn 
fundamentally di erent actions concerned people interaction speak horn display 
approach di ers adopt state decomposition problems 

review pomdps section establishes basic terminology providing brief overview essential concepts pomdps see kaelbling detailed discussion 
pomdp consists set states fs set actions fa am agent execute set observations fo perceived agent 
dynamics model described state transition probability distribution ja observation probability distribution ojs reward function numerical rewards 
probability state time assuming state time agent executed action probability agent observes world state executing point time system assumed state general possible determine current state complete certainty 
belief distribution maintained succinctly represent history agent interaction applied perceived domain jo exist interesting problems pomdps state tracking policy optimization 
state tracking 
operate domain apply belief conditioned policy agent constantly update belief vector jo oja denominator simply normalizing factor 
domains problem trivial compared computing useful action selection policy 
computing policy 
goal pomdp problem solving select actions maximize reward collection 
set selected actions commonly referred policy 
policy function belief state computed value iteration sondik assigns value combination belief state 
convergence value sum possibly discounted payo agent expects receive time current belief literature provides collection algorithms computing exact value function optimal policy nite horizon pomdps cassandra kaelbling 
exact algorithms bounded double exponential computational growth planning horizon practice exponential 
points need ecient algorithms 

hierarchical pomdps fundamental idea approach decomposition model pomdp problem action hierarchy 
assuming pomdp problem model designer hierarchically partitions probability time agent state history fo 
original action set spans collection hierarchically related smaller pomdps refer subtasks notation pomdp subtask 
action assigned subtasks subtask independently learns policy subset actions existing pomdp solving algorithms 
high level subtasks generally learn policies selection lower level subtasks low level subtasks responsible selection primitive actions 
action hierarchy de ning element approach action hierarchy 
illustrates basic concept action hierarchy 
formally action hierarchy tree leaf labeled action action henceforth called primitive actions attached leaf 
internal leaves called actions bar indicate action 
fact abstraction actions nodes directly hierarchy abstraction 
general form action hierarchy task decomposition key step hierarchical problem solving translate action hierarchy collection pomdps individually smaller original pomdp collectively de ne complete policy 
approach internal node action hierarchy immediate children de nes subtask shown triangle 
subtask constitutes separate pomdp de ned full state space observation space set applicable actions limited immediate children action hierarchy 
policy optimization subtask limited action subset 
shows problem divided subtasks respective action sets fa fa computing local policies approach independently optimizes local action policy subtask 
note subtasks exclusively primitive actions contain de ned pomdps subtasks containing actions far ill de ned original pomdp provide meaningful parameters conditioned actions 
hierarchical approach recursively policies lower level subtasks parameterize actions proceed bottomup manner nd local policy subtask leaves hierarchy root 
subtasks primitive actions solved rst existing algorithms currently algorithm described cassandra 
solve subtasks actions need infer model parameters js ojs actions 
action 
traversing hierarchy bottom fashion calculated pomdp solution subtask spanned 
policy calculated subtask de ne js js ojs ojs right hand side terms de ned original model left hand side terms needed model action 
words model state state basis action chosen policy subtask definitions lead fully parameterized subtask solved pomdp algorithm incremental pruning 
clearly de nition parameters constitutes approximation 
consider example subtask assumes parameters inferred actions having access full parameterization actions fa important assumption approach subtask contains local reward information local policies meaningfully optimized 
inconsistent single goal problems partial progress rewarded variety problems experimental sections suggests complex pomdp problems meet assumption 
local action policy policy de ned action subset 
de ned pomdp parameters js ojs de ned calculating policy left task constructing global policy set local policies produced subtasks 
rst notice policies hierarchy de ned entire belief space assume abstraction applied belief tracking 
agent possession full belief space 
practice global policy generated execution time 
generate action agent traverses tree top leaf 
level agent queries local policy current belief action proposed policy primitive 
action primitive action directly executed agent 
action action agent queries policy subtask spanned action 
trivial show recursive algorithm generates primitive action 
recursive action selection hierarchy traversal repeated time step 
di ers hierarchical mdp algorithms agent remains subtask called terminal state reached 
di erence consequence partial state observability pomdps suggests guarantee detectability terminal states 
state observation abstractions general number linear pieces representing exact pomdp value function recursively joj jaj enumerated time jsj joj 
hierarchical algorithm described far reduces computational complexity computing pomdp policies speci cally large planning horizons 
subtask possesses reduced action set factors exponential factor complexity 
savings partially set fact compute policies just 
fortunately applications including ones discussed opportunity reduce computational costs 
consider domains di erence certain states relevant speci action available robot location may irrelevant subtasks involve navigation 
case state features may safely ignore certain subtasks robot location dialogue subtasks 
consequently state set reduced include relevant state features related observations done subtask basis ignore state features currently done hand soon automatic 
irrelevant small action subset 
subtasks de ned smaller state observation spaces uencing policy optimization 
resulting computational savings tremendous orders magnitude 

experimental evaluation demonstrate approach practice evaluated algorithm di erent domains 
generated policies problems approaches conventional pomdp algorithm hierarchical pomdp algorithm referred pomdp mdp solution solves problem mdp execution uses state heuristic map belief states states 
policy computations performed incremental pruning algorithm pomdps value iteration mdps computations performed mhz pentium ii 
tasks evaluated runs show performance time 
rst task considered parts manufacturing problem introduced sondik thesis sondik 
selected problem speci cally constructed exhibit structure illustrate generality approach 
furthermore considered di erent hierarchical decompositions problem better study algorithm highly susceptible design action hierarchy 
shows action hierarchy considered domain 
manufacture replace inspect examine root navigate read open right left 
action hierarchies manufacturing task taxi navigation task illustrates average reward decompositions decreasing order performance clear grouping rst decompositions yield near optimal policies weaker mdp heuristic currently intuitive results reported obtained simulated user due large number experiments necessary gain signi cance 
experiments currently underway verify performance robot policy real users 
theoretical guarantee pomdp policy necessarily better mdp policy explanation performance di erence object ongoing 

manufacturing task results second task modi ed version dietterich taxi task dietterich noisy perception :10.1.1.32.8206
problem simple solvable conventional pomdp techniques 
shows action hierarchy domain 
third task challenging 
arises robot interface domain robot perform diverse tasks involving motion dialogue exchanges exhibit signi cant uncertainty large part due poor speech recognition 
pomdps currently best solution high level robot control problem 
shows action hierarchy decomposes action space natural divisions various conversational goals 
move act phone greet initiate 
dialogue problem action hierarchy table presents results complex problems showing policy computation times average reward action 
taxi task performance hierarchical pomdp pomdp approaches conventional pomdp goal reached occasionally requires extra action clearly exceeds greedy mdp heuristic unable take advantage uncertainty reducing actions 
results robot problem show exact pomdp solver unable nd policy completed iteration hrs hierarchical approach pomdp able obtain policy reasonable time 
execution performance hpomdp policy superior obtained mdp heuristic terms average reward action goal success rate 
performance difference mdp pomdp policies smaller domain time steps state fully observable 
problem solution cpu time average goal secs reward success taxi problem jsj jaj joj mdp pomdp pomdp robot problem jsj jaj joj mdp pomdp pomdp hrs table 
performance results table provides sample interaction robot domain pomdp policy 
shows policy generated approach able additional information gathering actions allowing system recover speech recognition errors third observation go erroneous speech recognition extracted go big user go room pomdp decides cost accidentally moving wrong location outweighs cost asking clari cation 
mdp heuristic policy exhibit similar adaptability poor recognition performance 
actor speech observation action person hello hello robot help 
initiate person time time request robot 
person go big go robot go 
person go go room robot go room 
person room go room robot want go room 
person robot robot goes room table 
example dialogue interface domain 
worth mentioning domains examined exhibit structure di erently 
rst case structure speak 
second case nal goal satis ed sequence intermediate subgoals 
third case dialogue manager satisfy alternate goals uni ed domain 
experiments address di erent problem setups algorithm shedding light approach di erent circumstances 

hierarchical pomdp algorithm optimize policies complex pomdp 
bottom algorithm introduced computes sequence pomdp policies task hierarchy 
run time resulting hierarchy policies traversed top bottom primitive action 
mild computational savings achieved reduced action space 
tasks action hierarchy gives rise state observation abstractions drastically reduce computational complexity 
experimental results obtained di erent tasks illustrate reduction computational complexity orders magnitude minimal performance loss compared computationally hard pomdp model 
experiments suggest hierarchical approach provides viable approach solving complex pomdps intractable assuming domain possesses structure expressed action hierarchy 
aaai 
aaai symposium pomdps 
www 
cs duke edu talks pomdp symposium html 
cassandra littman zhang 

incremental pruning simple fast exact method partially observable markov decision processes 
uai 


approximate dynamic programming sensor management 
conf 
decision control 
dayan hinton 

feudal reinforcement learning 
nips 
dietterich 

hierarchical reinforcement learning maxq value function decomposition 
journal arti cial intelligence research 
hernandez mahadevan 

hierarchical memory reinforcement learning 
nips 
kaelbling littman cassandra 

planning acting partially observable stochastic domains 
arti cial intelligence 
russell norvig 

arti cial intelligence modern approach 
prentice hall 
singh 

transfer learning composing solutions elemental sequential tasks 
machine learning 
sondik 

optimal control partially observable markov processes 
doctoral dissertation stanford 
wiering schmidhuber 

hq learning 
adaptive behavior 
