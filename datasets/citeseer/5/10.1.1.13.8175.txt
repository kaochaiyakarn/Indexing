getting mileage web text sources conversational speech language modeling class dependent mixtures ivan mari ostendorf department electrical engineering university washington seattle wa 
mo ee washington edu sources training data suitable language modeling conversational speech limited 
show training data supplemented text web filtered match style topic target recognition task possible get bigger performance gains data class dependent interpolation grams 
language models constitute key components modern speech recognition systems 
training ngram language model commonly type model requires large quantities text matched target recognition task terms style topic 
tasks involving conversational speech ideal training material transcripts conversational speech costly produce limits amount training data currently available 
methods developed purpose language model adaptation adaptation existing model new topics domains tasks little training material may available 
domain data contain relevant irrelevant information various methods identify relevant portions domain data prior combination 
past pre selection word frequency counts rudnicky probability perplexity word part speech sequences iyer ostendorf latent semantic analysis bellegarda information retrieval techniques mahajan iyer ostendorf 
clustering defining subsets domain data clarkson robinson martin test set perplexity prune documents training corpus 
common method additional text sources train separate language models small amount domain large amounts domain data combine interpolation referred mixtures language models 
andreas stolcke sri international menlo park ca 
stolcke speech sri com technique reported ibm liu sites 
alternative approach involves decomposition language model class gram interpolation iyer ostendorf ries allowing content words interpolated different weights filled pauses example gives improvement standard mixture modeling conversational speech 
researchers turned world wide web additional source training data language modeling 
just time language modeling berger miller adaptation data obtained submitting words initial hypotheses user utterances queries web search engine 
queries treated words individual tokens ignored function words 
search strategy typically generates text non conversational style ideally suited asr 
zhu rosenfeld downloading actual web pages authors retrieved ngram counts provided search engine 
approach generates valuable statistics limits set grams ones occurring baseline model 
approach extracting additional training data web searching text better matched conversational speaking style 
show better new data applying class dependent interpolation 
collecting text web amount text available web enormous web pages indexed google continues grow 
text web non conversational fair amount chat material similar conversational speech omitting disfluencies 
primary target extracting data web 
queries submitted google composed grams occur frequently switchboard training corpus thought think searching exact match grams text web pages 
web pages returned google part consisted conversational style phrases friends don relationship really haven seen years slightly different search strategy collecting topic specific data 
extended baseline vocabulary words small domain training corpus ostendorf grams new words web queries wireless know recognizer meeting transcription task morgan 
web pages returned google contained technical material related topics similar discussed meetings inspired weighted count scheme experiments bellman ford algorithm retrieved web pages filtered content language modeling 
stripped html tags ignored pages high oov rate 
piped text maximum entropy sentence boundary detector ratnaparkhi performed text normalization nsw tools sproat 
class dependent mixture lms linear interpolation standard approach combining language models probability word wi history computed linear combination corresponding gram probabilities dif ferent models wi sps wi 
depending adaptation data available may beneficial estimate larger number mixture weights data source order handle source mismatch specifically letting mixture weight depend context approach mixture weight corresponding source posterior probability weintraub 
choose weight vary function previous word class wi wi ps wi classes wi part speech tags frequent words form individual classes 
scheme generalize domains tapping syntactic structure pos tags shown useful cross domain language modeling iyer ostendorf time target conversational speech top words cover tokens switchboard training corpus 
combining grams produce model large number parameters costly decoding 
cases grams typically pruned 
entropy pruning stolcke mixing unpruned models reduce model aggressively original size 
pruning parameters applied models experiments 
experiments evaluated tasks switchboard godfrey specifically hub eval set having total words spoken speakers icsi meeting recorder morgan eval set having total words spoken speakers 
sets featured spontaneous conversational speech 
words held data task 
text corpora conversational telephone speech cts available training language models consisted switchboard callhome english total words 
addition words broadcast news bn transcripts collected words conversational text web 
meetings task words meeting transcripts available training collected words text web 
experiments conducted sri large vocabulary speech recognizer stolcke best rescoring mode 
baseline bigram language model generate best lists various trigram models 
table shows word error rates wer hub test set comparing performance class mixture standard class independent interpolation 
class mixture gave better results cases cts sources probably sources similar class mixture mainly useful data sources diverse 
obtained lower wer web data bn indicates web data better matched task conversational 
training data completely arbitrary benefits recognition task minimal shown example word corpus collected random web pages 
baseline switchboard model gave test set perplexity reduced standard mixture cts bn data reduced adding web data best case class dependent interpolation added web data 
increasing amount web training data gave relatively small performance gains 
trimmed word web corpus words choosing documents lowest perplexity combined cts model yielding web data source 
model web gave wer trained original web corpus 
web text obtained google filtering fairly homogeneous little gained perplexity filtering 
choosing better matched data exclude new grams may occur testing 
table hub eval wer results standard class mixtures 
lm data sources std 
mix class mix baseline cts bn web random web web bn web bn web bn web table meetings results wer 
lm data sources std 
mix class mix baseline meetings web topic meetings web topic results meeting test set shown table baseline model trained cts bn sources 
hub experiments classbased mixture outperformed standard interpolation 
achieved lower wer web data meeting transcripts best results obtained data sources 
language model perplexity reduced baseline best case 
tried different class assignments classbased mixture hub set automatically derived classes part speech tags lead performance degradation long allocate individual classes top words 
automatic class mapping class mixtures feasible languages part speech tags difficult derive 
summary shown filtered web text successfully training language models conversational speech outperforming outof domain bn small domain specific meetings sources data 
combining lms different domains class dependent interpolation particularly top words forms class achieve lower wer standard approach mixture weights depend data source 
recognition experiments show significant reduction wer absolute due additional training data class interpolation 
bellegarda 

exploiting local global constraints statistical language modeling 
proc 
icassp pages ii 
berger miller 

just time language modeling 
proc 
icassp pages ii 
clarkson robinson 

language model adaptation mixtures exponentially decaying cache 
proc 
icassp pages ii 
godfrey mcdaniel 

switchboard telephone speech corpus research development 
proc 
icassp pages 
iyer ostendorf 

transforming domain estimates improve domain language models 
proc 
eurospeech volume pages 
iyer ostendorf 

relevance weighting combining multi domain data gram language modeling 
computer speech language 


selecting articles language model training corpus 
proc 
icassp pages iii 
liu 
ibm switchboard progress evaluation site report 
lvcsr workshop gaithersburg md national institute standards technology 
mahajan beeferman huang 

improved topic dependent language modeling information retrieval techniques 
proc 
icassp pages 
martin 
adaptive topic dependent language modeling word 
proc 
eurospeech pages 
morgan 
meeting project icsi 
proc 
conf 
human language technology pages 
ratnaparkhi 

maximum entropy part speech tagger 
proc 
empirical methods natural language processing conference pages 
ries 

class approach domain adaptation constraint integration empirical gram models 
proc 
eurospeech pages 
rudnicky 

language modeling limited domain data 
proc 
arpa spoken language technology workshop pages 
ostendorf 

text normalization varied data sources conversational speech language modeling 
proc 
icassp pages 
sproat 
normalization non standard words 
computer speech language 
stolcke 
sri march hub conversational speech transcription system 
proc 
nist speech transcription workshop 
stolcke 

entropy pruning backoff language models 
proc 
darpa broadcast news transcription understanding workshop pages 
weintraub 
lm project report fast training portability 
technical report center language speech processing johns hopkins university baltimore 
zhu rosenfeld 

improving trigram language modeling world wide web 
proc 
icassp pages 
