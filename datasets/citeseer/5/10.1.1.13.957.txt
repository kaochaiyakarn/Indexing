evolutionary artificial neural networks xin yao department computer science university college university new south wales australian defence force academy canberra act australia published international journal neural systems vol 
pp 
evolutionary artificial neural networks eanns considered combination artificial neural networks anns evolutionary search procedures genetic algorithms gas 
distinguishes levels evolution eanns evolution connection weights architectures learning rules 
reviews kind evolution detail analyses major issues related kind evolution 
shown lot evolution connection weights architectures research evolution learning rules early stages 
interactions different levels evolution far understood 
argued evolution learning rules interactions levels evolution play vital role eanns 
keywords evolutionary artificial neural networks learning evolution genetic algorithms 
yao evolutionary artificial neural networks interest combinations anns evolutionary search procedures grown rapidly years 
covers wide range topics weight training architecture design learning learning rule input feature selection genetic reinforcement learning initial weight selection ann analysis ann conferences papers combinations 
workshop solely devoted area :10.1.1.13.957
emphasis different review papers :10.1.1.13.957
concerned understanding development eanns regarded general framework adaptive systems systems change architectures learning rules different environments human intervention 
combinations ann analysis gas covered 
interested exploring possible benefits arising interactions anns evolutionary search procedures anns evolutionary search procedures shall concentrate popular models anns evolutionary search procedures study feedforward anns gas trying cover kinds models 
discussion applicable models 
major advantages eanns adaptability dynamic environment 
adaptive process achieved levels evolution evolution connection weights architectures learning rules progress different time scales play different roles adaptation 
slowest time scale evolution highest level explores eann space searches promising regions eanns dealing environment 
fastest time scale evolution lowest level exploits promising region order find near optimal eann 
evolution connection weights discussed section 
aim find near optimal set connection weights eann thresholds biases viewed connection weights fixed input gamma 
yao evolutionary artificial neural networks fixed architecture evolution 
gas evolutionary weight training process 
various methods encoding connection weights advantages disadvantages discussed section 
comparisons evolutionary approach conventional training algorithms back propagation 
comparisons suggest fast gradient training algorithms efficient evolutionary training algorithms gradient information cheaply available evolutionary training algorithms better choice 
general single algorithm winner kinds networks 
section devoted evolution architectures evolution lead near optimal architecture tasks hand 
known architecture eann decides maximum functionality eann 
evolution architectures provides powerful adaptive system decide architecture tasks hand 
automatic way designing eann architectures human intervention 
motivation constructive destructive learning algorithms :10.1.1.125.6421:10.1.1.32.7223:10.1.1.13.957
important issues encode architectures evolutionary search procedure addressed section 
problems overlooked current research indicated 
imagining eann connection weights architectures hardware easier understand importance evolution eann software learning rules 
section discusses evolution learning rules eanns examines relationship learning evolution learning guides evolution learning evolves 
issue learning learn evolution learn learning rule 
demonstrated eann learning ability developed improved evolution 
research topic early stages studies doubt benefit research eanns machine learning 
kinds evolution mentioned studied years attempts understand relationships 
clear interact adaptation eann dynamic yao evolutionary artificial neural networks environment efficient 
section describes general framework eanns interactions kinds evolution shown 
framework provides common ground comparing different ann models algorithms broader view evolutionary search procedures taken 
section 
evolution connection weights learning anns roughly divided supervised unsupervised reinforcement learning 
supervised learning usually formulated minimisation error function total mean square error target outputs actual outputs 
supervised learning algorithms back propagation bp conjugate gradient algorithms gradient descent search 
successful applications bp algorithms various areas 
drawbacks bp algorithm exist due gradient descent nature 
gets trapped local minimum error function inefficient finding global minimum error function multimodal nondifferentiable 
detailed review current state bp algorithm learning algorithms 
way overcome bp gradient descent search training algorithms shortcomings formulate training process evolution connection weights environment determined architecture learning task 
global search procedures gas effectively evolution find near optimal set connection weights eann 
fitness eann defined different needs 
fitness function differentiable continuous gas depend gradient information search 
important factors appear fitness function error target outputs actual outputs complexity eann 
gas dealing large complex nondifferentiable multimodal spaces typical space defined error function yao evolutionary artificial neural networks 
decode individual genotype current generation set connection weights construct corresponding eann weights 

evaluate eann computing total mean square error actual outputs target outputs 
error functions 
fitness individual determined error 
higher error lower fitness 
optimal mapping error fitness usually problem dependent 

reproduce number children individual current generation fitness 

apply genetic operators crossover mutation child individual generated obtain generation 
typical cycle evolution connection weights 
fitness function lot done evolution connection weights 
evolution connection weights provides alternative approach training eanns 
evolutionary approach consists major stages 
stage decide genotype representation connection weights form binary strings 
second evolution driven gas evolutionary search procedures genetic operators crossover mutation decided conjunction representation scheme 
different representation schemes genetic operators lead different training performance 
typical cycle evolution connection weights shown 
yao evolutionary artificial neural networks binary representation binary representation shown beneficial ga search way represent connection weights encode binary strings 
representation scheme connection weight represented number binary bits certain length 
example whitley bits represent connection weight ranges gamma experiments xor adder problems 
eann represented concatenation connection weights network 
heuristic concerning order concatenation put connection weights hidden node 
hidden nodes eanns essence feature extractors detectors 
separating inputs hidden node far apart binary representation increase difficulty constructing useful feature detectors may destroyed recombination operators easily 
probability obtaining better feature detector combining part feature detector part feature detector low 
open issue lessen destructive effect crossover maintain feature detectors far evolution connection weights 
problem eanns general including evolution connection weights course permutation problem 
caused mapping representation actual eann eanns label hidden nodes differently different representations functionally equivalent 
problem recombination operators difficult produce highly fit children 
shows equivalent eanns different labels hidden nodes 
advantages binary representation lie simplicity generality 
easy apply standard crossover mutation binary strings 
little need design complex tailored genetic operators 
binary representation facilitates digital hardware implementation eanns 
disadvantage binary representation poor scalability 
example eanns yao evolutionary artificial neural networks gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma equivalent networks order hidden nodes differently 
connections uncommon nowadays gas operating bits inefficient 
encoding methods uniform gray exponential binary representation 
encode real values different ranges precisions number bits 
issue precision especially important 
bits represent connection weight training fail combinations real connection weight unable hard approximated discrete values 
hand bits binary strings representing large eanns extremely long evolution inefficient 
real number representation debates cardinality genotype alphabet 
researchers argued minimal cardinality binary representation best 
real numbers proposed represent connection weights real number connection weight 
eann represented set real numbers binary strings 
recombination operator exchange real numbers sets change numbers 
real numbers changed random mutations 
usually gaussian random number added real number 
connection weights represented real numbers standard genetic operators dealing binary strings longer applied directly 
circumstances important task carefully design set genetic operators tailored yao evolutionary artificial neural networks deal real numbers order improve speed accuracy evolutionary training 
montana davis defined large number tailored genetic operators incorporated heuristics training eanns 
major aim retain useful feature detectors formed hidden nodes evolution 
results showed evolutionary training approach faster bp training algorithms problems considered 
bartlett downs demonstrated evolutionary approach faster better scalability bp training method 
tailored genetic operators conjunction real number representation connection weights 
mean set complex genetic operators indispensable 
simpler genetic operators 
example fogel adopted genetic operator excluding selection gaussian random mutation evolution connection weights 
possible improvement method cauchy random mutation :10.1.1.51.6553
worth pointing permutation problem destructive effect crossover feature detectors eanns exist representing connection weights real numbers problem concerning representation precision weights gone 
comparison evolutionary training gradient training indicated section evolutionary training approach attractive handle global search problem better vast complex multimodal nondifferentiable space 
depend gradient information error fitness function particularly appealing gradient information unavailable costly get 
example evolutionary approach reinforcement learning recurrent network learning higher order network learning :10.1.1.13.957
noted evolutionary algorithm training different networks yao evolutionary artificial neural networks feedforward networks recurrent networks higher order networks generality save lot human efforts developing different training algorithms different type network 
evolutionary training approach easier generate eanns special characteristics 
method decrease eann complexity improve generalisation ability include penalty term fitness function 
gradient training worry term differentiable 
requirements weight sharing weight decaying incorporated fitness function easily 
evolutionary training generally slow comparison fast variants bp algorithm conjugate gradient algorithms gradient information cheaply available deal wide range networks gradient information unavailable 
gradient information available evolutionary training faster cases 
described ga training algorithm significantly faster methods generalized delta rule gdr 
tests reported ga training algorithm took total hours minutes gdr took total hours minutes 
bartlett downs gave modified ga order magnitude faster bp algorithm bit parity problem 
modified ga better scalability bp algorithm twice slow bp algorithm xor problem 
interestingly quite different results reported kitano 
ga bp method method runs ga standard bp best equally efficient faster variants back propagation small scale networks far efficient larger networks 
test problems include xor problem various size encoder decoder problems spiral problem 
discrepancy seemingly contradictory results attributed partly different gas bp algorithms compared 
yao evolutionary artificial neural networks comparison standard ga fast bp algorithm fast ga standard bp algorithm 
known gas bp algorithms sensitive parameters 
bp algorithms sensitive initial conditions 
hybrid training major problem gas inefficiency fine tuned local search global search 
efficiency evolutionary training improved significantly incorporating local search procedure evolution combining ga global search ability local search fine tuning ability 
gas locate region space local search procedure find near optimal solution region 
local search algorithm bp algorithm random search algorithms 
belew gas search near optimal set initial connection weights bp algorithm perform local search initial weights 
results showed hybrid ga bp approach efficient ga bp algorithm 
taken account fact bp algorithm run times practice order find connection weights bp sensitivity initial conditions hybrid training algorithm quite competitive comparison gradient training algorithms 
similar evolution initial weights done competitive learning neural networks 
interesting consider finding initial weights locating region space 
defining basin attraction local minimum composed points sets weights case converge local minimum local search algorithm global minimum easily local search algorithm ga locate point set initial weights basin attraction global minimum 
yao evolutionary artificial neural networks evolution architectures section assumes architecture eann predefined fixed evolution connection weights 
section concerns design eann architectures 
architecture eann includes topological structure connectivity transfer function node eann 
indicated architecture design crucial successful application eanns architecture significant impact eann information processing abilities 
example learning task eann connections hidden nodes linear node transfer function may able perform task due limited information processing ability eann large number connections hidden nodes nonlinear node transfer function may powerful task eann poor generalisation ability 
unfortunately architecture design human expert job 
depends heavily expert experience tedious trial error process 
systematic way design near optimal architecture automatically task 
research constructive destructive algorithms effort automatic design architectures :10.1.1.125.6421:10.1.1.32.7223:10.1.1.13.957
roughly speaking constructive algorithm starts minimal network network minimal number hidden layers nodes connections adds new layers nodes connections necessary training destructive algorithm opposite starts maximal network deletes unnecessary layers nodes connections training 
design optimal architecture eann formulated search problem architecture space point represents architecture 
performance optimality criteria fastest learning lowest complexity architectures performance level architectures forms surface space 
optimal architecture design equivalent finding highest point surface 
characteristics surface indicated yao evolutionary artificial neural networks miller ga evolutionary algorithms better candidate searching surface constructive destructive algorithms mentioned 
characteristics ffl surface infinite large number possible nodes connections unbounded 
ffl surface nondifferentiable changes number nodes connections discrete discontinuous effect eann performance 
ffl surface complex noisy mapping architecture performance indirect strongly epistatic dependent evaluation method 
ffl surface deceptive similar architectures may quite different performance 
ffl surface multimodal different architectures may similar performance 
similar evolution connection weights major stages involved evolution architectures genotype representation scheme architectures evolution 
key issue decide information architecture encoded genotype representation 
extreme detail connection node architecture specified genotype representation binary bits 
kind representation schemes called direct encoding scheme extreme important parameters architecture number hidden layers hidden nodes layer encoded 
detail architecture left training process decide 
kind representation schemes called indirect encoding scheme 
representation scheme worked evolution architectures progress cycle shown 
different names scheme miller call strong specification scheme indirect encoding scheme weak specification scheme 
yao evolutionary artificial neural networks 
decode individual current generation architecture 
indirect encoding scheme detail architecture specified developmental rules training process 

train eann decoded architecture pre defined learning rule parameters learning rule learned training starting different sets random initial connection weights learning rule parameters 

define fitness individual encoded architecture training result performance criteria complexity architecture 

reproduce number children individual current generation fitness 
apply genetic operators children generated obtain generation 
typical cycle evolution architectures 
yao evolutionary artificial neural networks advantages evolutionary design architectures lot research carried years 
research deals topological structure eanns little done evolution node transfer functions evolution topological structures node transfer functions 
concentrate evolution topological structures analyse genotype representation scheme topological structures section 
convenience sake term architecture interchangeably term topological structure sections 
section discusses evolution node transfer functions briefly 
direct encoding scheme direct encoding scheme connection architecture directly specified binary representation 
example theta matrix ij thetan represent architecture nodes ij indicates presence absence connection node node ij indicate connection ij connection 
fact ij connection weights node node topological structure connection weights eann evolved time 
matrix direct mapping corresponding architecture 
binary string representing architecture just concatenation rows columns matrix 
constraints architectures explored easily incorporated representation scheme setting constraints matrix feedforward eann non zero entries upper triangle matrix 
direct encoding scheme relatively simple straightforward implement 
suitable precise deterministic handling small architectures architectures small number nodes connections 
may facilitate rapid generation optimisation tightly pruned interesting designs hit far 
scale large architectures require yao evolutionary artificial neural networks large matrices represent 
way cut size matrices domain knowledge possible reduce search space 
example complete connection input output hidden layer assumed feedforward eann information number hidden nodes needs encoding 
length genotype representation reduced greatly case 
evolution architectures adaptive process prior knowledge architectures assumed availability knowledge increase efficiency evolution 
easy incorporate knowledge available evolution 
flexibility provided evolution architectures stems fitness definition 
virtually limitation differentiable continuous fitness function defined step 
training result pertaining architecture error training time fitness function 
complexity measurement number nodes connections fitness function 
matter fact lot criteria information theory statistics readily introduced fitness function difficulty 
improvement eann generalisation ability expected criteria adopted 
schaffer experiment shows eann designed evolutionary approach better generalisation ability training bp algorithm human designed architecture :10.1.1.13.957
major problem evolutionary design architectures permutation problem illustrated section 
functionally equivalent eanns label hidden nodes differently different genotype representations probability producing highly fit child crossover low 
researchers abandoned crossover adopted mutation evolution architectures shown crossover useful important increasing efficiency evolution 
hancock suggested permutation problem severe supposed population size selection mechanism yao evolutionary artificial neural networks increased number ways solving problem outweigh difficulties bring building blocks 
worth indicating studies permutation problem concentrate genetic algorithm genetic operators population sizes selection mechanisms 
necessary investigate algorithm important study genotype representation scheme performance surface defined section decided representation genetic operators 
research needed understand impact permutation problem evolution architectures 
indirect encoding scheme order reduce length genotype representation architectures indirect encoding scheme researchers important features architecture encoded genotype representation 
detail individual connection specified developmental rules 
indirect encoding scheme compact biologically plausible direct encoding scheme 
discoveries neuroscience impossible genetic information encoded chromosomes specify independently nervous system 
parametric representation architecture specified parameters number hidden layers number hidden nodes layer number connections layers parameters encoded various forms genotype representation 
harp blueprint represent architecture consists segments representing area layer efferent connectivity projections 
area constrained input output area respectively 
segment includes parts information area number nodes area spatial organisation area efferent connectivity 
noted yao evolutionary artificial neural networks connectivity pattern connection specified 
detailed node node connection specified implicit developmental rules network instantiation software harp 
similar parametric representation methods different sets parameters proposed 
interesting aspect harp combination learning parameters architectures genotype representation learning parameters evolve architecture parameters 
near optimal combination learning parameters architectures evolved 
parametric representation method reduce length binary strings specifying eann architectures lacks scalability needed real world applications length grows quickly number nodes eanns increases 
developmental rules play limited role 
tend fixed assumptions architectures prior knowledge 
developmental rule representation quite different indirect encoding method encode developmental rules construct architectures genotype representation 
shift direct optimisation architectures optimisation developmental rules results advantages compact representation better scalability method better modularity generalisation ability resultant architecture 
destructive effect crossover lessened developmental rule representation capable preserving promising building blocks far 
developmental rule usually described recursive equation generation rule similar production rule knowledge system left hand side right hand side 
connectivity pattern architecture form matrix constructed basis single element matrix repetitively applying suitable developmental rules non terminal elements current yao evolutionary artificial neural networks matrix matrix contains terminal elements indicate presence absence connection connectivity pattern fully specified 
kitano modified version graph generation system includes set graph generation rules construct connection matrices connection matrix corresponds directed graph 
developmental rule graph generation rule consists left hand side lhs non terminal element right hand side rhs theta matrix terminal non terminal elements 
typical step constructing connection matrix find rules left hand sides appear current matrix replace appearance respective right hand sides 
rule represented allele positions corresponding elements rule genotype 
length genotypes variable position fixed initial element basis 
position genotype take value element range 
rules left hand side theta matrices right hand side pre defined participate evolution 
developmental rules obtained evolution 
consistently better results various size encoder decoder problems reported kitano comparison direct encoding scheme 
developmental rule representation method fine tuning detailed connections single nodes concentrates connections groups nodes 
addition fine tuning process dynamic node adjustment algorithm similar constructive destructive algorithms evolution way improve performance architecture 
mjolsness described similar rule encoding method rules represented recursive equations specify growth connection matrices 
coefficients recursive equations represented decomposition matrices terminal element existence connection non existence connection non terminal element symbol 
definitions slightly different kitano 
yao evolutionary artificial neural networks encoded genotypes optimised simulated annealing gas 
connection weights optimised connectivity simulated annealing entry connection matrix real number weight just 
advantage simulated annealing gas evolution avoidance destructive effect crossover 
wilson simulated annealing eann architecture design 
research developmental rule representation demonstrated effectiveness efficiency evolution architectures 
gruau reported number successful experiments cellular encoding method including experiments input parity problem input symmetry problem 
similar kitano cellular encoding method grammar rules eanns implement boolean functions considered 
restriction weights gamma evolution architectures connection weights easier 
unclear method perform real valued connection weights 
fractal representation merrill port proposed method encoding architectures fractal subsets plane 
argued fractal representation architectures biologically plausible developmental rule representation 
method better scalability developmental rule representation real number parameters edge code input coefficient output coefficient specify node architecture 
fast simulated annealing evolution 
representations different approach evolution architectures proposed andersen 
representation unique individual population represents hidden node architecture architecture 
ar yao evolutionary artificial neural networks chitecture built layer layer hidden layers added current architecture reduce training error certain threshold 
hidden layer constructed automatically evolutionary process employs ga sharing 
sharing help evolution form different feature detectors population hidden node different feature detector 
problem andersen method usually hidden nodes similar functionality basically feature detector population 
redundancy removed additional clean algorithm 
restriction andersen method layer layer approach deal multilayer perceptrons 
evolution node transfer functions discussion evolution architectures far covers topological structure architecture transfer function node architecture assumed fixed pre defined human experts transfer function shown important part architecture significant impact architecture performance :10.1.1.51.6553
transfer function assumed nodes architecture nodes layer 
stork best knowledge apply gas evolution topological structures node transfer functions neural networks consisting nodes investigated 
transfer function specified structural genes genotype representation 
complex usual sigmoid function tried model biological neuron circuitry 
white adopted simpler approach evolution topological structures node transfer functions 
individual eann initial population nodes eann sigmoid transfer function nodes gaussian transfer function 
evolution decide yao evolutionary artificial neural networks optimal mixture transfer functions automatically 
sigmoid gaussian transfer function evolvable 
parameters function evolved 
evolution architectures connection problem evolution architectures touched noisy evaluation genotypes 
evaluation genotype noisy actual architecture phenotype created genotype developmental rules evaluate genotype fitness 
practice popular way evaluate genotypes transform genotype encoded architecture phenotype fully specified architecture developmental rules train phenotype different initial connection weights generated random 
training result part fitness function evaluate genotype 
due stochastic nature random initial weights fitness genotype evaluated way noisy 
evaluation noisier development rules deterministic indirect encoding scheme 
way alleviate problem employ evolution architectures connection weights time noise introduced stochastic development rules hard removed 
evolution natural choice eanns binary threshold nodes considered 
general combination levels evolution increase search space 
suppose size architecture space js size connection weight space js size level search space js sa size level search space js theta sa trade efficiency accuracy practice 
yao evolutionary artificial neural networks evolution input features evolution input features reduce combine possible inputs eann compact effective set 
thought approach dimension reduction evolution 
cases number inputs outputs eann decided problem hand 
example hand printed digit recognition network may naturally theta inputs outputs 
unfortunately number inputs eann usually large interesting real world problems 
may redundancy different inputs 
large number inputs eann increase size eann require training data training order obtain reasonable generalisation ability eann 
preprocessing needed reduce number inputs eann 
various dimension reduction techniques including principle component analysis purpose 
problem reducing number inputs eann formulated search problem 
set inputs want find optimal subset fewest number inputs performance eann subset worse eann set 
evolutionary approach identified way search vast space 
results better performance fewer inputs reported studies 
evolution input features individual current population represents subset inputs 
evaluation individual carried training eann inputs result part fitness function 
evaluation noisy due reason explained section 
evolution input features provide way discover important features inputs adaptively discover new training examples 
zhang described active learning paradigm training algorithm gas select training examples 
yao evolutionary artificial neural networks evolution learning rules known training algorithm may different performance applied different architectures 
design training algorithms fundamentally learning rules adjust connection weights depends type architectures investigation 
different variants hebbian learning rule proposed deal different architectures 
design hard little prior knowledge eann architecture case practice 
desirable develop automatic systematic way adapt learning rule architecture tasks hand 
designing learning rule human experts implies assumptions necessarily true eann architectures tasks performed eann 
example widely accepted hebbian learning rule shown outperformed new rule proposed cases 
new rule learn patterns optimal hebbian rule learn exceptions regularities 
hard say rule optimal architectures 
fact needed eann ability adjust learning rule adaptively tasks performed internal environment architecture 
words eann learn learning rule designed human experts 
evolution fundamental forms adaptation surprising evolution learning rules introduced eanns order learn learning rules 
relationship evolution learning extremely complex 
various models proposed deal issue learning guide evolution relationship evolution architectures connection weights 
issue evolution learning rules just started attracting attention 
research evolution learning rules important providing automatic way optimising learning rules modelling relationship learning yao evolutionary artificial neural networks 
decode individual current generation learning rule 

construct set eanns randomly generated architectures pre defined architecture cases initial connection weights train decoded learning rule 

calculate fitness individual encoded learning rule training result criteria 

reproduce number children individual current generation fitness 

apply genetic operators child individuals produced obtain new generation 
typical cycle evolution learning rules 
evolution modelling creative process newly evolved learning rules deal complex dynamic environment 
research help better understand creativity emerge artificial systems eanns model creative process biological systems 
typical cycle evolution learning rules described 
similar reason indicated section fitness evaluation individual encoded learning rule noisy techniques alleviate problem weighted average training results eanns different initial connection weights fitness function 
architecture pre defined individual evolution evaluated different architectures 
additional noise introduced evaluation case 
yao evolutionary artificial neural networks evolution algorithmic parameters adaptive adjustment bp algorithm parameters learning rate momentum evolution considered attempt evolution learning rules 
harp encoded bp algorithm parameters genotypes eann architecture 
evolutionary approach different non evolutionary evolution algorithmic parameters architectures facilitates exploration interactions learning algorithm architectures near optimal combination bp algorithm architecture evolved 
cost benefit mentioned section larger search space longer computation time 
researchers evolutionary process find parameters bp algorithm eann architecture pre defined 
parameters evolved case tend optimised architecture general applicable ones 
number bp algorithms adaptive learning rate momentum non evolutionary approach 
comparison approaches quite useful 
evolution learning rules evolution algorithmic parameters certainly interesting hardly touches fundamental part training algorithm learning rule weight adjusting rule 
adapting learning rule evolution expected enhance eann adaptivity greatly dynamic environment 
evolution connection weights architectures deal static objects eann weights architectures evolution learning rules dynamic behaviour eann 
biggest problem encode dynamic behaviour learning rule static genotypes 
trying develop universal representation scheme specify kind dynamic behaviours clearly impractical prohibitive long computation yao evolutionary artificial neural networks time required search learning rule space 
constraints set type dynamic behaviours basic form learning rules evolved reduce representation complexity search space 
basic assumptions learning rules weight updating connection depends local information activation input node activation output node current connection weight learning rule connections eann 
learning rule assumed linear function local variables products 
learning rule described function deltaw delta delta delta delta delta deltai gamma time deltaw weight change delta delta delta local variables real number coefficients decided evolution learning rules 
due large number terms eq evolution extremely slow impractical constraints set biological heuristic knowledge 
chalmers defined form learning rules linear function local variables pairwise products 
third fourth order terms 
coefficients scale parameter encoded binary string exponential encoding 
architecture fitness evaluation fixed single layer eanns considered number inputs outputs fixed learning task hand architecture generated random evolved learning rules 
generations starting population randomly generated learning rules evolution discovered wellknown delta rule variants 
experiments simple preliminary demonstrated potentiality evolution learning rules discovering novel learning rules merely known ones 
constraints set learning rules prevent learning rules evolved include third fourth order terms 
order defined number variables product 
yao evolutionary artificial neural networks similar experiments evolution learning rules carried 
meir chalmers approach evolve learning rules binary perceptrons 
considered local variables terms adopted weight updating function included order second order third order terms eq 
baxter took step just evolution learning rules 
tried evolve complete eanns single level evolution 
clear search space possible eanns enormous strict constraints set connection weights architectures learning rules 
experiments eanns binary threshold nodes considered weights gamma 
number nodes eanns fixed 
learning rule concerned boolean variables 
baxter experiments simple confirmed complex behaviours learned eann learning ability improved evolution 
bengio approach slightly different chalmers sense gradient descent algorithms simulated annealing employed means evolution 
local variables zeroth order order second order terms eq weight updating function 
research related evolution learning rules includes parisi evolve learning rules explicitly 
put emphasis crucial role environment evolution happens simple neural networks study 
issue environmental diversity related noisy evaluation genotypes indicated section section 
possible sources noise decoding process morphogenesis genotypes second introduced decoded learning rule evaluated train eanns 
environmental diversity essential getting approximation fitness decoded learning rule reducing noise second source 
general learning rule applicable wide range architectures learning tasks needed environmental diversity high different yao evolutionary artificial neural networks architectures learning tasks fitness evaluation genotype 
concluding remarks evolution introduced eanns various levels roughly divided evolution connection weights architectures learning rules 
section describes general framework eanns gives 
general framework eanns general framework eanns described 
evolution connection weights proceeds lowest level fastest time scale environment decided architecture learning rule learning tasks 
alternatives decide level evolution architectures learning rules evolution architectures highest level learning rules lower vice versa 
lower level evolution faster time scale 
point view engineering decision level evolution depends kind prior knowledge available 
prior knowledge eann architectures learning rules particular class architectures pursued better put evolution architectures highest level knowledge encoded architecture genotype representation reduce architecture search space lower level evolution learning rules biased kind architectures 
hand evolution learning rules highest level prior knowledge available special interest certain type learning rules 
unfortunately usually little prior knowledge available architectures learning rules practice vague statements 
case appropriate put evolution architectures yao evolutionary artificial neural networks highest level optimality learning rule sense evaluated environment including architecture learning rule applied 
evolution architectures evolution learning rules evolution connection weights evaluation architectures reproduction architectures evaluation learning rules reproduction learning rules evaluation reproduction tasks weights fitness oe oe oe architecture fitness learning rule fitness general framework eanns 
viewed general framework adaptive systems restrict gas levels 
simulated annealing gradient descent search blind random search considered types evolutionary search procedures 
example traditional bp network considered special case general framework random trial error search evolution architectures shot candidate search yao evolutionary artificial neural networks evolution learning rules bp algorithm evolution connection weights 
fact general framework provides basis comparing various specific eann models search procedures different levels defines dimensional space represents shot search represents exhaustive search axis 
eann model corresponds point space 
evolution introduced eanns levels 
evolution connection weights adopted adaptive approach connection weight training especially reinforcement learning recurrent network learning gradient training algorithms experience great difficulties 
due simplicity generality evolution fact gradient training algorithms run multiple times order avoid trapped bad local optimum evolutionary approach quite competitive comparison approaches 
evolution architectures generate near optimal architecture dynamically tasks hand 
advantages knowledge heuristic methods characteristics architectures section 
indirect encoding scheme architectures shown scalability 
facilitate generation eanns generalisation ability incorporating various factors fitness function 
direct encoding scheme fine tuning architecture poor scalability 
evolution learning rules allows eann adapt learning rule environment efficient learning rule obtained 
sense evolution provides eanns ability learning learn 
helps model relationship learning evolution 
preliminary experiments shown efficient learning rules evolved randomly generated rules 
current research evolution learning rules normally assumes learning rules specified eq 
constraints learning rules necessary reduce search space evolution prevent interesting learning rules yao evolutionary artificial neural networks discovered 
evolution learning rules closely related artificial life research evolution complex behaviours studied explicit representation learning rule 
global search procedures gas usually computationally expensive run 
better gas levels evolution practice 
beneficial introduce global search levels evolution especially little prior knowledge available level performance eann required high trial error heuristic methods inefficient circumstances 
experiments demonstrate advantages hybrid global local search different levels issue optimal combination different search procedures needs investigation 
increasing power parallel computers simulation large eanns feasible 
simulation discover possible new eann architectures learning rules offer way model creative process result eann adaptation dynamic environment 
author anonymous referees comments suggestions help improve greatly 
whitley schaffer ed :10.1.1.13.957
proc 
int workshop combinations genetic algorithms neural networks 
ieee computer society press los alamitos ca 
schaffer whitley eshelman 
combinations genetic algorithms neural networks survey state art 
yao evolutionary artificial neural networks whitley schaffer editors proc 
int workshop combinations genetic algorithms neural networks pages 
ieee computer society press los alamitos ca 
weiss 
combining neural evolutionary learning aspects approaches 
technical report institut fur informatik technische universitat munchen may 
rudnick 
bibliography intersection genetic search artificial neural networks 
technical report cs department computer science engineering oregon graduate institute science technology january 
yao 
review evolutionary artificial neural networks 
international journal intelligent systems 
eberhart 
role genetic algorithms neural network query learning explanation facilities 
whitley schaffer editors proc 
int workshop combinations genetic algorithms neural networks pages 
ieee computer society press los alamitos ca 

genetic algorithms tool analysis adaptive resonance theory network training sets 
whitley schaffer editors proc 
int workshop combinations genetic algorithms neural networks pages 
ieee computer society press los alamitos ca 
eberhart 
designing neural network explanation facilities genetic algorithms 
yao evolutionary artificial neural networks proc 
ieee international joint conference neural networks ijcnn singapore volume pages 
ieee press new york ny 
hertz krogh palmer 
theory neural computation 
addison wesley reading ma 
horne :10.1.1.13.957:10.1.1.42.3375
progress supervised neural networks 
ieee signal processing magazine january 
holland 
adaptation natural artificial systems st mit press edn 
mit press cambridge ma 
goldberg 
genetic algorithms search optimization machine learning 
addison wesley reading ma 
fahlman lebiere 
cascade correlation learning architecture 
touretzky editor advances neural information processing systems pages 
morgan kaufmann san mateo ca 
frean 
algorithm method constructing training feedforward neural networks 
neural computation 
mozer smolensky 
skeletonization technique trimming fat network relevance assessment 
connection science 
dow 
yao evolutionary artificial neural networks creating artificial neural networks generalize 
neural networks 
hirose yamashita 
back propagation algorithm varies number hidden units 
neural networks 
lecun denker solla 
optimal brain damage 
touretzky editor advances neural information processing systems pages 
morgan kaufmann san mateo ca 
roy kim mukhopadhyay 
polynomial time algorithm construction training class multilayer perceptrons 
neural networks 

hwang 

lay 
jou 
wrong cascaded correlation learning network projection pursuit learning perspective 
technical report department electrical engineering ft university washington seattle wa 
rumelhart hinton williams 
learning internal representations error propagation 
rumelhart mcclelland editors parallel distributed processing explorations cognition vol 
pages 
mit press cambridge ma 
mller 
scaled conjugate gradient algorithm fast supervised learning 
neural networks 
lang waibel hinton 
time delay neural network architecture isolated word recognition 
neural networks 
yao evolutionary artificial neural networks fels hinton 
glove talk neural network interface data glove speech synthesizer 
ieee trans 
neural networks 
personnaz dreyfus 
handwritten digit recognition neural networks single layer training 
ieee trans 
neural networks 
sutton 
problems backpropagation steepest descent learning procedures networks 
proc 
th annual conf 
cognitive science society pages 
lawrence erlbaum associates hillsdale nj 
whitley starkweather 
genetic algorithms neural networks optimizing connections connectivity 
parallel computing 
hinton 
connectionist learning procedures 
artificial intelligence 
whitley 
genitor algorithm selective pressure rank allocation reproductive trials best 
schaffer editor proc 
third int conf 
genetic algorithms applications pages 
morgan kaufmann san mateo ca 
montana davis 
training feedforward neural networks genetic algorithms 
proc 
eleventh int joint conf 
artificial intelligence pages 
morgan kaufmann san mateo ca 
yao evolutionary artificial neural networks dolan 
parametric connectivity training constrained networks genetic algorithms 
schaffer editor proc 
third int conf 
genetic algorithms applications pages 
morgan kaufmann san mateo ca 
fogel fogel porto 
evolving neural networks 
biological cybernetics 
bartlett downs 
training neural network genetic algorithm 
technical report dept elec 
eng univ queensland january 

parallel algorithms learning neural networks evolution strategy 
evans peters editors proc 
parallel computing pages 
elsevier science publishers amsterdam 
belew mcinerney schraudolph 
evolving networks genetic algorithm connectionist learning 
technical report cs revised computer science engr 
dept 
univ california san diego la jolla ca usa february 
radcliffe 
genetic neural networks mimd computers compressed edition 
phd thesis dept theoretical phys university edinburgh scotland uk 

training multilayered neural networks replacing fit hidden neurons 
yao evolutionary artificial neural networks proc 
ieee volume pages 
ieee press new york ny 
de garis 
steerable gennets genetic programming steerable behaviors gennets 
varela bourgine editors practice autonomous systems proc 
european conference artificial life pages 
mit press cambridge ma usa 
de garis 
genetic algorithm train time dependent behaviors neural networks 
michalski tecuci editors proc 
international workshop multistrategy learning msl pages 
center artificial intelligence fairfax va usa 
srinivas 
learning neural network weights genetic algorithms improving performance search space reduction 
proc 
ieee international joint conference neural networks ijcnn singapore volume pages 
ieee press new york ny 
de garis 
gennets genetically programmed neural nets genetic algorithm train neural nets inputs outputs vary time 
proc 
ieee international joint conference neural networks ijcnn singapore volume pages 
ieee press new york ny 
guan 
training weights neural networks genetic algorithms messy genetic algorithms 
yao evolutionary artificial neural networks editor proc 
second iasted international symposium expert systems neural networks pages 
acta press anaheim ca usa 

new learning algorithm training multilayered neural networks uses genetic algorithm techniques 
electronics letters july 
wieland 
evolving neural network controllers unstable systems 
proc 
ieee international joint conference neural networks ijcnn seattle volume pages 
ieee press new york ny 
koza rice 
genetic generation weights architecture neural network 
proc 
ieee international joint conference neural networks ijcnn seattle volume pages 
ieee press new york ny 
dominic das whitley anderson 
genetic reinforcement learning neural networks 
proc 
ieee international joint conference neural networks ijcnn seattle volume pages 
ieee press new york ny 
dill deer 
exploration genetic algorithms selection connection weights dynamical neural networks 
proc 
ieee national aerospace electronics conference volume pages 
ieee press new york ny 
bornholdt 
general asymmetric neural networks structure design genetic algorithms 
yao evolutionary artificial neural networks neural networks 

genetically programmed neural network solving pole balancing problem 
kohonen simula kangas editors artificial neural networks proc 
int conf 
artificial neural networks icann vol 
pages 
north holland amsterdam 

evolving sequential machines amorphous neural networks 
kohonen simula kangas editors artificial neural networks proc 
int conf 
artificial neural networks icann vol 
pages 
north holland amsterdam 
menczer parisi 
evidence hyperplanes genetic learning neural networks 
biological cybernetics 
ichikawa 
neural network application direct feedback controllers 
ieee transactions neural networks 
neil 
genetic training layer neural network 
electronics letters jan 

mixed genetic approach optimization neural controllers 
editors proc 
pages 
ieee computer soc 
press los alamitos ca 
janson 
application genetic algorithms training higher order neural networks 
journal systems engineering 
yao evolutionary artificial neural networks 
training neural networks genetic algorithms target detection 
proceedings spie conf 
science artificial neural networks orlando fl usa pt 
brown 
alternative learning methods training neural network classifiers 
proceedings spie conf 
science artificial neural networks orlando fl usa pt 
lewis fagg 
genetic programming approach construction neural network control walking robot 
proc 
ieee international conference robotics automation volume pages 
ieee computer soc 
press los alamitos ca 

genetic breeding algorithm exhibits self organizing neural networks 
editor proc 
iasted international symposium artificial intelligence application neural networks pages 
acta press anaheim ca usa 
von 
application artificial neural networks genetic algorithms personnel selection financial industry 
proc 
international conference artificial intelligence wall street pages 
ieee computer soc 
press los alamitos ca 
elias 
genetic generation connection patterns dynamic artificial neural network 
whitley schaffer editors proc 
int workshop combinations genetic algorithms neural networks pages yao evolutionary artificial neural networks 
ieee computer society press los alamitos ca 
beer gallagher 
evolving dynamical neural networks adaptive behavior 
adaptive behavior 
hancock 
genetic algorithms permutation problems comparison recombination operators neural net structure specification 
whitley schaffer editors proc 
int workshop combinations genetic algorithms neural networks pages 
ieee computer society press los alamitos ca 

new interpretation schema notation binary encoding constraint 
schaffer editor proc 
third int conf 
genetic algorithms applications pages 
morgan kaufmann san mateo ca 
yao :10.1.1.51.6553
simulated annealing extended neighbourhood 
int 
computer math 
whitley dominic das 
genetic reinforcement learning multilayer neural networks 
belew booker editors proc 
fourth int conf 
genetic algorithms pages 
morgan kaufmann san mateo ca 
ackley littman 
learning natural selection artificial environment 
proc 
int joint conf 
neural networks vol 
pages washington dc 
lawrence erlbaum associates hillsdale nj 

yao evolutionary artificial neural networks temporal processing recurrent networks evolutionary approach 
belew booker editors proc 
fourth int conf 
genetic algorithms pages 
morgan kaufmann san mateo ca 
fahlman 
faster learning variations back propagation empirical study 
touretzky hinton sejnowski editors proc 
connectionist models summer school pages 
morgan kaufmann san mateo ca 
johansson goodman 
backpropagation learning multi layer feed forward neural networks conjugate gradient method 
int neural systems 
kitano 
empirical studies speed convergence neural network training genetic algorithms 
proc 
eighth nat conf 
ai aaai pages 
mit press cambridge ma 
yao 
optimization genetic annealing 
jabri editor proc 
second australian conf 
neural networks pages sydney australia 
merelo pat ca nas prieto mor 
optimization competitive learning neural network genetic algorithms 
proc 
int workshop artificial neural networks pages 
springer verlag 
lecture notes computer science vol 

miller todd hegde 
designing neural networks genetic algorithms 
yao evolutionary artificial neural networks schaffer editor proc 
third int conf 
genetic algorithms applications pages 
morgan kaufmann san mateo ca 
kitano 
designing neural networks genetic algorithms graph generation system 
complex systems 
harp samad guha 
genetic synthesis neural networks 
schaffer editor proc 
third int conf 
genetic algorithms applications pages 
morgan kaufmann san mateo ca 
schaffer caruana eshelman :10.1.1.13.957
genetic search exploit emergent behavior neural networks 
physica 
wilson 
perceptron redux emergence structure 
physica 
dodd 
optimisation artificial neural network structure genetic techniques implemented multiple transputers 
welch stiles kunii editors proceedings pages 
ios amsterdam 
harp samad guha 
designing application specific neural networks genetic algorithm 
touretzky editor advances neural information processing systems pages 
morgan kaufmann san mateo ca 
dress 
darwinian optimization synthetic neural systems 
yao evolutionary artificial neural networks butler editors proc 
st ieee int conf 
neural networks vol 
pages 
ieee new york ny 
bergman 
breeding intelligent automata 
butler editors proc 
st ieee int conf 
neural networks vol 
pages 
ieee new york ny 
hancock 
neural net face recognition genetic algorithm 
technical report center cognitive computational dept computing sci 
psychology stirling university stirling fk la uk august 
dolan dyer 
evolution symbols 
proc 
nd int conf 
genetic algorithms applications pages 
lawrence erlbaum associates hillsdale nj 
fullmer miikkulainen 
marker genetic encoding neural networks evolve finite state behaviour 
varela bourgine editors practice autonomous systems proc 
european conference artificial life pages 
mit press cambridge ma usa 

fusion technology design evolutionary machines neural networks 
kohonen simula kangas editors artificial neural networks proc 
int conf 
artificial neural networks icann vol 
pages 
north holland amsterdam 
dodd 
optimisation neural network structure genetic techniques 
yao evolutionary artificial neural networks editors proc 
conf 
applications artificial intelligence engineering vi pages 
elsevier applied science london uk 
marshall harrison 
optimization training feedforward neural networks genetic algorithms 
proc 
second iee international conference artificial neural networks pages 
iee press london uk 
furst 
distributed genetic algorithm neural network design training 
complex systems 
mart genetically generated neural networks representational effects 
proc 
int joint conf 
neural networks ijcnn baltimore vol 
iv pages 
ieee press piscataway nj 
joost werner 
synthesis performance analysis multilayer neural network architectures 
technical report university koblenz institute fur physics koblenz 

voigt born nez 
evolutionary structuring artificial neural networks 
technical report technical university berlin evolution techniques lab ack berlin 
mar 
genetic synthesis discrete time recurrent neural network 
proc 
int workshop artificial neural networks pages 
springer verlag 
lecture notes computer science vol 

alba 
fully automatic ann design genetic approach 
yao evolutionary artificial neural networks proc 
int workshop artificial neural networks pages 
springer verlag 
lecture notes computer science vol 

white 
genetic algorithm optimizing topology weights neural network design 
proc 
int workshop artificial neural networks pages 
springer verlag 
lecture notes computer science vol 

andersen 
constructive algorithm multilayer perceptron operative population concepts genetic algorithms 
master thesis university queensland department electrical computer engineering brisbane qld australia september 
bichsel seitz 
minimum class entropy maximum information approach layered networks 
neural networks 
fogel 
information criterion optimal neural network selection 
ieee trans 
neural networks 
moody 
selecting neural network architectures prediction risk application corporate bond rating prediction 
proc 
int conf 
ai applications wall street pages 
ieee computer society press los alamitos ca 
spears anand 
study crossover operators genetic programming 
yao evolutionary artificial neural networks ras editors proc 
th international symposium methodologies intelligent systems ismis pages 
springer verlag berlin germany 
mjolsness sharp alpert 
scaling machine learning genetic neural nets 
advances applied mathematics 
merrill port 
configured neural networks 
neural networks 
gruau 
genetic synthesis boolean neural networks cell rewriting developmental process 
whitley schaffer editors proc 
int workshop combinations genetic algorithms neural networks pages 
ieee computer society press los alamitos ca 
doi 
morphogenesis life forms 
sha 
wilson 
teaching network connectivity simulated annealing massively parallel processor 
proceedings ieee 
hartley 
nonconvex optimization fast simulated annealing 
proceedings ieee 
mani 
learning gradient descent function space 
proc 
ieee int conf 
system man cybernetics pages los angeles ca 
yao evolutionary artificial neural networks lovell tsoi 
performance neocognitron various cell cell transfer functions 
intelligent machines lab dept elec 
eng univ queensland april 
dasgupta schnitger 
efficient approximation neural networks comparison gate functions 
technical report dept computer sci pennsylvania state univ university park pa 
stork walker burns jackson 
neural circuits 
proc 
int joint conf 
neural networks vol 
pages washington dc 
lawrence erlbaum associates hillsdale nj 
narayanan lucas 
genetic algorithm improve neural network predict patient response 
methods information medicine 
guo uhrig 
genetic algorithms select inputs neural networks 
whitley schaffer editors proc 
int workshop combinations genetic algorithms neural networks pages 
ieee computer society press los alamitos ca 
brill brown martin 
fast genetic selection features neural network classifiers 
ieee transactions neural networks 
hsu wu 
input pattern encoding generalised adaptive search 
whitley schaffer editors proc 
int workshop combinations genetic algorithms neural networks pages 
ieee computer society press los alamitos ca 
yao evolutionary artificial neural networks chang lippmann 
genetic algorithms improve pattern classification performance 
lippmann moody touretzky editors advances neural information processing systems pages 
morgan kaufmann san mateo ca 

zhang 
neural networks teach genetic discovery novel examples 
proc 
ieee international joint conference neural networks ijcnn singapore volume pages 
ieee press new york ny 
singer 
different voltage dependent thresholds inducing long term depression long term potentiation slices rat visual cortex 
nature 
hancock smith phillips 
biologically supported error correcting learning rule 
kohonen simula kangas editors proc 
int conf 
artificial neural networks icann vol 
pages 
north holland amsterdam 
maynard smith 
learning guides evolution 
nature 
hinton nowlan 
learning guide evolution 
complex systems 
belew 
evolution learning culture computational metaphors adaptive algorithms 
yao evolutionary artificial neural networks technical report cs computer science engr 
dept 
univ california san diego la jolla ca usa september 
nolfi elman parisi 
learning evolution neural networks 
technical report crt center research language university california san diego la jolla ca july 
muhlenbein kindermann 
dynamics evolution learning genetic neural networks 
pfeifer editor connectionism perspective pages 
elsevier science publishers amsterdam 
muhlenbein 
adaptation open systems learning evolution 
kindermann editors workshop pages 
gmd postfach st augustin germany 
paredis 
evolution behavior experiments 

meyer wilson editors proc 
int conf 
simulation adaptive behavior animals animats 
mit press cambridge ma 
chalmers 
evolution learning experiment genetic connectionism 
touretzky elman hinton editors proceedings connectionist models summer school pages 
morgan kaufmann san mateo ca 
bengio bengio 
learning synaptic learning rule 
technical report informatique de recherche op universit de montr eal canada november 
bengio bengio 
yao evolutionary artificial neural networks optimization synaptic learning rule 
preprints conference optimality artificial biological neural networks univ texas dallas feb 
meir 
evolving learning algorithm binary perceptron 
network 
ackley littman 
interactions learning evolution 
langton taylor farmer rasmussen editors artificial life ii sfi studies sciences complexity vol 
pages reading ma 
addison wesley 
baxter 
evolution learning algorithms artificial neural networks 
green editors complex systems pages 
ios press amsterdam 

artificial evolution generalized class adaptive processes 
yao editor preprints ai workshop evolutionary computation pages november 
jacobs 
increased rates convergence learning rate adaptation 
neural networks 
widrow hoff 
adaptive switching circuits 
ire convention record pages 
ire new york ny 
cecconi nolfi 
neural networks learn environment 
network 
yao evolutionary artificial neural networks yao 
evolution connectionist networks 
editor artificial intelligence creativity pages 
kluwer academic publishers dordrecht 
