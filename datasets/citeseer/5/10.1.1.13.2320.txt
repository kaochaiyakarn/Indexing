scaling bayesian network learning thousands variables local learning techniques ioannis ph aliferis ph alexander laura brown department biomedical informatics vanderbilt university nashville tn state art bayesian network learning algorithms scale variables fall far short addressing challenges posed large datasets biomedical informatics gene expression proteomics text categorization data 
bn learning algorithm called max min bayesian network learning mmbn algorithm induce networks tens thousands variables alternatively selectively reconstruct regions interest time permit full reconstruction 
mmbn local algorithm returns targeted areas network putting pieces 
small dataset mmbn outperforms state art methods 
subsequently scalability demonstrated fully reconstructing data bayesian network variables ordinary pc hardware 
novel algorithm pushes envelope bayesian network learning np complete problem orders magnitude 

bayesian networks bn formalization proved useful important tool medicine building decision support systems bioinformatics discovering gene expression pathways 
automatically learning bns observational data area intense research decade yielding practical algorithms tools 
bn representation learning algorithms naturally lend causal modeling causal discovery 
despite great advances bn learning techniques proved challenge posed number current domains gene expression proteomics text categorization 
current techniques scale variables best case 
example public available versions pc called algorithms accept datasets variables respectively indicating expectations inventors regarding scalability 
bn learning algorithm called max min bayesian network mmbn able scale tens thousands variables pushing envelope bn learning orders magnitude 
compared variety state art methods field small dataset exhibits improved quality output 
addition performance algorithm excellent reconstructing data bn variables training instances 
mmbn additional important advantage previous methods anytime algorithm sense time recover area interest target variable full bn 
longer algorithm allowed run larger reconstructed area 
property allows algorithm learn parts extremely large networks empirically demonstrated proof experiment 

max min bayesian network mmbn algorithm bayesian networks bn mathematical objects compactly represent joint probability distribution set random variables called nodes directed acyclic graph step mmbn discovers edges bn second step orients 
rest focus step edge discovery 
orientation phase pc algorithm constrained hill climbing score fashion sparse candidate edge orientation 
edge identification important edge variables certain conditions corresponds direct causal relation variables directly causing vice versa 
edges may generate accurate hypotheses causal discovery 
may bns capture data joint distribution question mmbn discover 
proved bn faithful joint data distribution set edges 
unique set edges discovered mmbn 
bn faithful joint distribution independencies distribution entailed markov condition 
mmbn target data returns edges bn faithfully captures joint data distribution 
variables 
run 

edges edges 
return edges target data returns parents children phase forward 
cpc 
repeat 
variable find 
subset cpc minimizes assoc 

variable cpc maximizes assoc ind 
cpc cpc add cpc 

cpc changed phase ii backward 
cpc 
cpc ind 
cpc cpc remove cpc 


return cpc algorithms mmbn 
mmbn local discovery algorithm called max min parents children identifies parents children set variables edge target variable interest mmbn calls variables targets returns edges discovered 
real global learning algorithm done 
tests conditional independence measures association 
definition independent iff denoted ind 
implementation conditional tests independence test statistic explained detail 
standard threshold values reject independency accept dependency 
bn faithful joint distribution variable edge conditionally dependent subset 
provides efficient way testing condition holds 
discovers parents children phase scheme phase forward phase variables enter sequentially candidate parents children set denoted cpc heuristic function 
heuristic admissible sense variables edge possibly enter cpc 
phase ii backward phase removes false positives entered phase 
includes cpc variable highest univariate association chooses include cpc variable exhibits maximum association conditioned subset cpc achieves minimum association possible variable 
intuitively heuristic justified follows select variable despite best efforts independent considering minimum association conditioned possible subsets cpc highest minimum association candidate variables 
second phase false positives removed subset cpc discovered conditioned independent measure association function assoc pseudo code negative returned test independence smaller value higher association 
reliable statistical test independence measure association employed 
general tests conditional independence measures association contain exponential number free parameters estimated size conditioning set 
size restricted estimation parameters tests unreliable 
implementation perform tests measures association training instances parameter cell table estimated similar fashion pc algorithm 
restriction limits number subsets tried lines significantly speeds algorithm 
may allow false positives enter output 
unrestricted mmbn returns true edges assumptions exist bn faithful joint probability distribution data ii statistical tests independence measure associations reliable 
correctness corollary correctness 

simulating large bayesian networks typically bn learning algorithms tested randomly generated networks networks real decision support systems structure network known serve rigorous gold standard 
networks real systems expected better representative sample distributions encountered practice 
algorithm sensitivity specificity distance pc sc mi sc sc mmbn table results alarm 
sc mi sc sc stand sparse candidate mutual information score heuristic respectively 
sparse candidate slightly worse results obtained 
algorithm sens spec 
dist time mmbn hours table results mmbn variables tiled alarm training instances 
statistics reported task edge rediscovery 
time measured ghz intel pentium xeon gb memory 
unfortunately currently publicly available bayesian networks real systems sizes required experimental purposes 
relying randomly generated bns invented new method generating large real bn 
method tiles multiple copies smaller real bns way guarantees structural probabilistic properties tiles remain originating networks 
structural properties maintained adding relatively randomly chosen edges interconnect tiles 
preserving probabilistic properties tiles guaranteed follows 
tile variables joint distribution originating network denoted marginal joint distribution large tiled network denoted impose constraint 
implementation free parameters new conditional probability tables generated addition new edges selected uniform probability respecting constraints 
details method omitted due lack space 

experimental results experiment mmbn outperforms state theart bn learning algorithms 
set experiments compares mmbn state art bn algorithms pc sparse candidate algorithm 
mmbn implemented matlab rest algorithms publicly available versions default values 
training instances generated randomly sampling distribution alarm bn medical diagnosis decision support system data fed algorithms 
measure comparison sensitivity specificity edge discovery 
sensitivity algorithm ratio correctly identified edges total number edges original network 
specificity ratio edges correctly identified belonging graph true number edges original network 
algorithm achieve perfect sensitivity specificity including excluding respectively edges output 
combined measure statistics needed 
possible measure euclidean distance sensitivity specificity perfect score sensitivity specificity area roc curve sparse candidate suitable parameter vary create corresponding curve rest algorithms provide points curve large number different thresholds 
results shown table 
algorithms took couple minutes complete pentium xeon ghz 
mmbn outperforms state art global bn learning algorithms task 
experiment mmbn reconstructs variables bn excellent quality 
second experiment demonstrates scalability mmbn 
network approximately variables created tiling copies alarm described section 
training instances randomly generated network mmbn run identify edges networks 
results shown table 
observation mmbn scales large network relatively small decrease quality keeping constant size training sample 
mmbn ordinary hardware experimentation networks size encountered number challenging biomedical domains 
especially true considering mmbn implemented matlab optimized performance 
additionally mmbn easily parallelizable algorithm 
observation specificity increases number variables increase 
preliminary results reported confirm observation 
increasing number variables relatively sparse networks increases number true negatives 
results suggest rate increase false positives reduce specificity lower rate increase true negatives 
depth sens spec 
dist table results mmbn variables tiled alarm training instances 
statistics reported task reconstructing area radius target statistics shown averages variables 
reconstructed area radius mmbn target variable tiled alarm 
textured nodes true positives 
experiment mmbn selective local reconstruction number variables extremely large half certain proteomics datasets mmbn able fully reconstruct bn reasonable amount time 
cases selective reconstruction area target node may option practitioner wants develop causal theory targeted third set experiments modified mmbn return area bn radius edges away node denoted variables depth 
done breadth search staring discovering parents children parents children parents children recursively 
sensitivity algorithm ratio correctly identified edges belonging true number edges 
specificity ratio edges correctly identified belonging true number edges 
notice true number edges edges region possible edge variables 
results shown table 
observation specificity digits accuracy 
quadratic number potential edges case true negatives small area algorithm returns small fraction false positives 
secondly average sensitivity edges false negatives edges table slightly different average sensitivity table targets depth edges false negatives edges 
sensitivity depth encouraging average direct causes direct effects identified purely computational methods experiments 
sensitivity quickly deteriorates depth 
explanation example 
shows reconstructed area variable index network 
textured nodes edges ones correctly identified mmbn breadth search strategy 
depth mmbn misses single node identifying parents children node sensitivity 
missing single node error propagates 
mmbn discover node belongs desired area reconstructed expanded causing turn edges nodes 
sensitivity depth correctly identified edges edges lower sensitivity depth 
mmbn ran node full reconstruction edges variables discovered 
errors propagate general expect error selective local reconstruction increase exponentially increasing depth 
summary experimental results experiments constitute proof concept mmbn outperforms current bn learning algorithms addition scales large networks minimal quality degradation task undirected edge identification 
focusing edge identification local approach selective reconstruction targeted areas possible 
evaluation suggests mmbn discovery complete causal theories possible excellent quality large networks ones encountered biomedicine ii discovery direct causal relations target variable possible excellent quality large networks iii selective reconstruction possible albeit difficult error propagation accumulation 

discussion introduce new algorithm mmbn bn learning preliminary experiments outperforms state art algorithms additionally scales large bn minimal degradation quality 
addition show mmbn selectively reconstruct targeted areas variable provably correct local bn learning algorithm grow shrink algorithm discovering markov parents children sets 
part global bn learning algorithm 
incremental association markov blanket algorithm improves grow shrink employing dynamic variable ordering heuristic 
solves different problem markov blanket discovery parents children identification 
lcd local algorithm different sense identifies subset edges network focus targeted area 
mmbn scales pc constraint algorithms reasons uses powerful heuristic identify potential members parents children different strategy order performing tests independence 
addition compared bayesian search score methods sparse candidate greedy hillclimbing solves relaxed version bn learning problem structure identification 
contrast current search score methods identify direct edges simultaneously 
scalable algorithms essential reasons 
number significant datasets biomedicine uses sizes variable sets outside current reach bn learning algorithms 
second reason indicated experiments full bn learning may necessary order achieve acceptable quality local reconstruction particular regarding sensitivity 
turn local algorithms important worst case full reconstruction impossible rendering local edge identification currently available way forming local causal theory 
discovery direct causal relations depth preliminary results excellent deteriorate indirect causality 
local algorithms important variable selection 
markov blanket target variable shown minimum variable set achieves optimal classification accuracy certain conditions 
markov blanket contained area radius edges target 
sizes datasets currently encountered gene expression reach different genes true number genes human genome estimated neighborhood 
proteomics estimated number different proteins human body order half text categorization tens thou sands different words corpora 
time stamped data number variables number observed quantities multiplied number time steps 
algorithm step addressing challenges datasets pose bn modeling causal discovery 
experiments way determine quality directing edges locally globally mmbn superiority bn algorithms behavior different sample sizes theoretical comparison constraint algorithms 
family provably correct local algorithms 
exploration different heuristics tests independence measure associations way 
aliferis algorithms large scale local causal discovery feature selection presence limited sample large causal neighborhoods 
department biomedical informatics vanderbilt university technical report dsl discover mc vanderbilt edu discover public beinlich suermondt alarm monitoring system case study probabilistic inference techniques belief networks 
second european conference artificial intelligence medicine 

friedman nachman pe er 
learning bayesian network structure massive datasets sparse candidate algorithm 
fifteenth conference uncertainty artificial intelligence uai 

friedman linial bayesian networks analyze expression data 
computational biology 

glymour cooper eds 
computation causation discovery 
aaai press mit press menlo park california cambridge massachusetts london england 
jie greiner learning bayesian networks data information theory approach 
artificial intelligence 

mani cooper 
study causal discovery population infant birth death records 
american medical informatics association amia 

thrun 
bayesian network induction local neighborhoods 
advances neural information processing systems nips 

pearl causality models reasoning inference 
cambridge university press 
spirtes glymour scheines causation prediction search 
second ed 
cambridge massachusetts london england mit press 
aliferis 
principled feature selection relevancy filters wrappers 
ninth international workshop artificial intelligence statistics 

key west florida usa 
aliferis 
algorithms large scale markov blanket discovery 
th international flairs conference 

st florida usa 
