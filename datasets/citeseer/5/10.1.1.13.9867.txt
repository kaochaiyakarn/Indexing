dynamic weighted majority new ensemble method tracking concept drift jeremy marcus department computer science georgetown university washington dc usa cs georgetown edu cstr june algorithms tracking concept drift important applications 
general method weighted majority algorithm line learner concept drift 
dynamic weighted majority dwm maintains ensemble base learners predicts weighted majority vote experts dynamically creates deletes experts response changes performance 
empirically evaluated experimental systems method incremental naive bayes incremental tree inducer iti experts 
sake comparison included blum implementation weighted majority 
stagger concepts sea concepts results suggest ensemble method learns drifting concepts base algorithms learn concept individually 
report best results problems date 
learning algorithms track concept drift important domains 
algorithms applicable variety problems large small 
robust noise 
converge quickly target concepts high accuracy 
ensemble method uses line learning algorithms track drifting concepts 
weighted majority algorithm added mechanisms create delete experts dynamically response changes performance :10.1.1.37.1595
call new method dynamic weighted majority dwm 
incremental version naive bayes incremental tree inducer base algorithms evaluated method synthetic problems involving concept drift stagger concepts sea concepts problem proposed data mining community :10.1.1.30.8004
sake comparison evaluated blum implementation weighted majority stagger concepts 
obvious evaluation knowledge published 
results suggest dwm learns drifting concepts base algorithms learn concept individually perfect forgetting 
contributions 
general method line learning algorithm problems involving concept drift 
second conducted thorough empirical study method included incremental learners base algorithms synthetic problems appeared literature methods sake comparison 
third comprehensive evaluation firmly place results context reported previously 
best knowledge best results problems date 
organized follows 
section discuss background material related section describe new method dynamic weighted majority 
section results empirical evaluation synthetic problems involve drifting concepts 
analyzing discussing results section conclude directions 
background related line learning task acquire set concept descriptions labeled training data distributed time 
type learning important applications computer security intelligent user interfaces market basket analysis 
important class problems line learning involves target concepts change time 
instance customer preferences change new products services available 
algorithms coping concept drift converge quickly accurately new target concepts efficient time space 
years researchers proposed evaluated algorithms coping concept drift 
stagger algorithm designed expressly concept drift algorithms followed flora flora flora aq pm aq pm aq pm wah 
stagger uses probabilistic concept description responds drift adjusting counts weights 
methods learn rules maintain examples input stream 
flora system stores encountered examples dynamically sized period time 
window adjustment heuristic wah adjusts window response changes performance measured terms accuracy coverage examples window 
flora mechanisms coping noise similar ib :10.1.1.37.1595
flora extensions dealing recurring contexts 
aq systems partial memory pm maintain examples store boundaries concept descriptions 
aq pm uses aq algorithm learn new rules stored memory new ones input stream 
forgets examples fixed period time 
aq pm stores boundary examples forgets fixed period time uses aq algorithm form concepts incrementally 
aq pm wah extends aq pm widmer kubat heuristic determine dynamically period keep forget examples 
systems evaluated stagger concepts 
researchers established degree algorithms scale large problems designed expressly learning time varying concepts large data sets 
example learns decision tree copes concept drift maintaining examples window time list alternate subtrees 
concepts drift algorithm replaces active subtree alternates 
algorithms problems formal properties theory 
algorithm category weighted majority :10.1.1.37.1595
method weighting combining decisions experts learning method 
instance blum pairs triples features experts returned majority vote predictions 
algorithm begins creating set experts assigning weight 
new instance arrives algorithm passes receives prediction expert 
algorithm predicts weighted majority vote expert predictions 
expert incorrectly classifies example algorithm decreases weight multiplicative constant 
winnow similar weighted majority experts may abstain called specialists :10.1.1.130.9013
example pair features specialist abstain features instance 
weighted majority winnow begins creating set specialists assigning weight 
new instance arrives algorithm consults specialists predicts weighted majority vote 
weighted majority specialist predicts incorrectly winnow reduces specialist weight multiplicative factor 
specialist predicts correctly winnow increases specialist weight multiplicative factor 
blum evaluated variants weighted majority winnow calendar scheduling task results suggested algorithms responded concept drift executed fast useful real time applications 
pairs features requires experts number relevant features attribute value pairs direct application implementations impractical data mining problems 
case learning scheduling preferences user attributes winnow required specialists 
advantage weighted majority winnow provide general scheme weighting fixed collection experts 
mechanisms dynamically adding removing new experts specialists restricted problems determine priori number required 
provide remedy section section show sophisticated base algorithms reduce number experts 
weighted majority winnow ensemble methods line setting methods create individual classifiers combine predictions classifiers single prediction 
example bagging involves sampling replacement data set building classifier sample predicting majority prediction individual classifiers 
boosting likewise creates series classifiers albeit different method weighting classifier performance :10.1.1.133.1040
empirical evaluations suggest ensembles perform better single classifiers 
ensemble methods line learning tasks concept drift 
unfortunately line setting clear apply ensemble methods directly 
instance bagging new example arrives misclassified inefficient resample available data learn new classifiers 
solution rely user specify number examples input stream base learner approach assumes know great deal structure data stream impractical drifting concepts 
line boosting algorithms reweight classifiers assume fixed number classifiers 
strong assumption concepts change unaware line boosting approaches applied problem concept drift 
streaming ensemble algorithm sea copes concept drift ensemble classifiers 
sea reads fixed amount data uses create new classifier 
new classifier improves performance ensemble added 
ensemble contains maximum number classifiers algorithm replaces poorly performing classifier new classifier 
performance measured predictions performance ensemble new classifier 
unfortunately problems approach 
members ensemble learning formed 
members replaced changing concepts members learned concepts initially may eliminated 
approach assumes fixed period time sufficient learning target concepts 
concepts drift fixed period time learner may able acquire new target concepts 
section describe new ensemble method copes concept drift addresses problems 
dwm new ensemble method concept drift dynamic weighted majority dwm shown maintains concept description ensemble learning algorithms referred expert associated weight 
instance performance element polls experts returning prediction instance 
predictions expert weights dwm returns global prediction class label highest accumulated weight 
learning element new training example polls expert manner described previously 
expert predicts incorrectly weight reduced multiplicative constant 
increase expert weight predicts correctly investigated scheme 
dwm determines global prediction 
incorrect algorithm creates new expert weight 
algorithm normalizes expert weights uniformly scaling highest weight equal 
prevents newly added experts dominating decision making existing ones 
algorithm removes experts weights user defined threshold 
dwm passes training example expert learning element 
note normalizing weights incrementally training experts gives base learners opportunity recover concept drift 
large noisy problems required parameter governs frequency dwm creates experts removes updates weights reduction normalization 
dynamic weighted majority training data feature vector class label factor decreasing weights number classes set experts weights 
global local predictions sum weighted predictions class threshold deleting experts period expert removal creation weight update 

classify ej xi yi mod wj wj wj argmax mod normalize weights delete experts yi em create new expert wm 
ej train ej xi output 
algorithm dynamic weighted majority dwm 
implemented dwm different base learners incremental version naive bayes incremental decision tree learner 
symbolic attributes incremental version naive bayes nb stores concept description counts number examples class attribute value class 
learning entails incrementing appropriate counts new instance 
performance algorithm uses stored counts compute prior probability class ci conditional probability attribute value class vj ci 
assumption attributes conditionally independent uses bayes rule predict probable class argmax ci ci vj ci 
continuous attributes implementation stores class sum attribute values sum squares 
learning simply entails adding attribute value square value appropriate sum 
performance implementation uses example count sums compute mean variance 
assuming jth attribute values normally distributed computes vj ci vj vj vj size interval random variable attribute lies 
see john langley details :10.1.1.8.3257
refer system naive bayes dwm nb 
incremental tree inducer iti complex algorithm unable describe fully 
briefly iti uses concept description decision tree binary tests 
internal nodes iti stores frequency counts symbolic attributes list observed values continuous attributes 
leaf nodes stores examples 
iti updates tree propagating new example leaf node 
descent algorithm updates information node reaching leaf node determines tree extended converting leaf node decision node 
secondary process examines tests node appropriate restructures tree accordingly 
refer system iti dwm iti 
empirical study results section experimental results dwm nb dwm iti 
evaluated systems stagger concepts standard benchmark evaluating learners cope drifting concepts 
included blum implementation weighted majority sake comparison 
know published results algorithm stagger concepts 
effort determine method scales larger problems involving concept drift evaluated dwm nb sea concepts problem proposed data mining community 
include uci data sets evaluation naive bayes iti ensemble methods general studied tasks 
chose evaluate methods problems involving concept drift performance understood 
size green blue red color shape target concept 

size green blue red color shape target concept 

size green blue red color shape target concept 

visualization stagger concepts 
kluwer academic publishers 
stagger concepts stagger concepts comprise standard benchmark evaluating learner performance presence concept drift 
example consists attribute values color green blue red shape triangle circle rectangle size small medium large 
presentation training examples lasts time steps time step learner receives example 
time steps target concept color red size small 
time steps target concept color green shape circle 
time steps target concept size medium size large 
visualization concepts appears 
evaluate learner time step randomly generates examples current target concept presents performance element computes percent correctly predicted 
experiments repeated procedure times averaged accuracies runs 
computed confidence intervals 
evaluated dwm nb dwm iti blum weighted majority pairs features experts stagger concepts 
weighted majority algorithms halved expert weight mistake 
blum weighted majority expert maintained history prediction assumption setting provide reactivity concept drift 
dwm set update weights create remove experts time step 
algorithm removed experts weights fell 
pilot studies indicated optimal settings varying affected performance little selected value affect accuracy reduce number experts considerably 
sake comparison addition algorithms evaluated naive bayes iti naive bayes perfect forgetting iti perfect forgetting 
standard traditional implementations naive bayes iti provided worst case evaluation systems designed cope concept drift learn examples stream regardless target concept 
implementations perfect forgetting training methods target concept individually provided best case evaluation systems burdened examples concept descriptions previous target concepts 
shows results dwm nb stagger concepts 
expected naive bayes perfect forgetting performed best concepts naive bayes forgetting performed worst 
dwm nb performed naive bayes perfect forgetting converged quickly target concept 
time step predictive accuracy dwm nb naive bayes perfect forgetting naive bayes time step predictive accuracy confidence intervals dwm nb stagger concepts 
predictive accuracy dwm iti iti perfect forgetting iti time step predictive accuracy confidence intervals dwm iti stagger concepts 
expert count time step dwm nb dwm iti number experts maintained confidence intervals dwm nb dwm iti stagger concepts 
predictive accuracy blum weighted majority dwm iti dwm nb time step predictive accuracy confidence intervals dwm iti dwm nb blum weighted majority stagger concepts 
target concepts dwm nb performed naive bayes perfect forgetting 
place results context related section 
dwm iti performed similarly shown achieving accuracies nearly high iti perfect forgetting 
dwm iti converged quickly dwm nb second third target concepts compare plots naive bayes iti perfect forgetting see iti converged quickly target concepts naive bayes 
faster convergence due differences base learners inherent dwm 
average number experts system maintained runs 
average dwm iti maintained fewer experts dwm nb attribute fact iti performed better individual concepts naive bayes 
naive bayes mistakes iti dwm nb created experts dwm iti 
see rates removing experts roughly learners 
shows results experiment involving blum implementation weighted majority 
learner outperformed dwm nb dwm iti target concept performed comparably second performed worse third 
evaluated blum implementation weighted majority pairs features experts 
stagger concepts consist attributes possible values 
implementation weighted majority maintained experts presentation examples compared maximum dwm nb maintained 
granted pairs features simpler decision trees iti produced implementation naive bayes quite efficient maintaining integers expert 
occasions weighted majority memory dwm nb anticipate sophisticated classifiers naive bayes pairs features lead scalable algorithms topic section 
performance large data set concept drift determine dwm nb performs larger problems involving concept drift evaluated synthetic problem proposed data mining community 
problem call sea concepts consists attributes xi xi 
predictive accuracy dwm nb naive bayes perfect forgetting naive bayes time step predictive accuracy confidence intervals dwm nb sea concepts class noise 
expert count dwm nb class noise dwm nb noise time step number experts maintained confidence intervals dwm nb sea concepts 
target concept 
irrelevant attribute 
presentation training examples lasts time steps 
fourth time steps target concept 
second third fourth 
periods randomly generated training set consisting examples 
experimental condition added class noise 
randomly generated examples testing 
time step method example tested resulting concept descriptions examples test set computed percent correct 
repeated procedure times averaging accuracy runs 
computed confidence intervals 
problem evaluated dwm nb naive bayes naive bayes perfect forgetting 
set dwm nb halve expert weights update weights create remove experts time steps 
set algorithm remove experts weights 
see predictive accuracies dwm nb naive bayes naive bayes perfect forgetting sea concepts class noise 
stagger concepts naive bayes performed worst method removing outdated concept descriptions 
naive bayes perfect forgetting performed best represents best possible performance implementation problem 
dwm nb achieved accuracies nearly equal achieved naive bayes perfect forgetting 
shows number experts dwm nb maintained runs class noise 
recall dwm creates expert misclassifies example 
noisy condition examples relabeled dwm nb mistakes created experts condition noise 
section analyze results place context related 
analysis discussion section results dwm nb dwm iti stagger concepts 
section focus discussion dwm iti performed better dwm nb problem 
researchers built systems coping concept drift evaluated stagger concepts 
instance target concept dwm iti perform flora 
second third target concepts performed notably better flora terms asymptote terms slope 
dwm iti aq pm performed identically target concept dwm iti significantly outperformed aq pm second third concepts terms asymptote slope 
aq designed cope concept drift outperformed dwm iti terms asymptote concept terms slope third second concept performed significantly worse dwm iti 
comparing aq pm aq pm wah dwm iti perform target concept performed comparably second converged quickly third 
concluded dwm iti outperformed learners terms accuracy slope asymptote 
reaching gave little weight performance concept learners acquire easily doing requires mechanisms coping drift 
second third concepts exception aq dwm iti performed better learners 
aq outperformed dwm iti terms slope third concept mitigate aq poor performance second 
attribute performance dwm iti training multiple experts different sequences examples 
weighting experts contributed discuss topic detail shortly 
assume learner incrementally modifies concept descriptions new examples arrive 
target concept changes new disjoint best policy learn new descriptions scratch modifying existing ones 
intuitive sense learner old concept results empirical studies support assertion 
unfortunately target concepts disjoint difficult determine precisely concepts change challenging identify rules parts rules apply new target concepts 
dwm addresses problems incrementally updating existing descriptions learning new concept descriptions scratch 
regarding results sea concepts reported section dwm nb outperformed sea target concepts 
concept performance similar terms slope terms asymptote subsequent concepts dwm nb converged quickly target concepts higher accuracy 
example concepts just prior point concepts changed sea achieved accuracies range dwm nb range 
suspect due sea unweighted voting procedure method creating removing new classifiers 
recall method trains new classifier fixed number examples 
new classifier improves global performance ensemble added provided ensemble contain maximum number classifiers sea replaces poorly performing classifier ensemble new classifier 
classifier ensemble trained target concept concept changes disjoint sea replace half classifiers ensemble accuracy new target concept surpass old 
instance ensemble consists classifiers learns fixed set examples take additional training examples ensemble contained majority number classifiers trained new concept 
contrast dwm similar circumstances requires examples 
assume ensemble consists fully trained classifiers weight new concept disjoint previous 
example new concept arrives classifiers predict incorrectly dwm reduce weights global prediction incorrect create new classifier weight 
process examples 
assume example arrives original experts misclassify example new expert predicts correctly 
weighted prediction greater global prediction incorrect algorithm reduce weights create new expert weight 
dwm process examples 
assume sequence events occurs example arrives original misclassify new ones predict correctly 
weighted majority vote original greater new experts dwm decrease weight original create new expert process examples 
point new classifiers trained target concept able overrule predictions original 
crucially dwm reached state processing examples 
granted analysis sea dwm take account convergence base learners best case analysis 
actual number examples required converge new target concept may greater relative proportion examples similar 
analysis holds assume dwm replaces experts creating new ones 
generally ensemble methods weighting mechanisms dwm converge quickly target concepts require fewer examples methods replace unweighted learners ensemble 
dwm certainly potential creating large number experts simple heuristic added new expert global prediction incorrect intuitively problematic noisy domains 
sea concepts dwm nb maintained experts say time step maintained experts average runs similar sea reportedly stored 
number experts reach impractical levels dwm simply creating experts obtaining acceptable accuracy training continue 
plus easily distribute training experts processors network course grained parallel machine 
argue better performance dwm nb due differences base learners 
sea ensemble classifiers dwm nb course naive bayes base algorithm 
hypothesis running base learners target concepts 
achieved comparable accuracies concept 
example target concept achieved accuracy naive bayes achieved 
learners performed similarly concluded positive results problem due superiority base learner mechanisms create weight remove experts 
evaluate dwm iti sea concepts iti maintains training examples observed values continuous attributes led impractical memory requirements 
exclude possibility dwm large data sets decision tree learner base algorithm 
instance iti implement schemes index stored training examples reduce memory requirements 
decision tree learner store examples id vfdt :10.1.1.119.3124
suggests opportunities discuss section 
concluding remarks tracking concept drift important applications 
new ensemble method weighted majority algorithm :10.1.1.37.1595
method dynamic weighted majority creates removes base algorithms response changes performance suited problems involving concept drift 
described implementations dwm naive bayes base algorithm iti 
stagger concepts evaluated methods blum implementation weighted majority 
determine performance larger problem evaluated dwm nb sea concepts 
results problems compared methods suggest dwm maintained comparable number experts achieved higher predictive accuracies converged accuracies quickly 
best knowledge best results reported problems 
plan investigate sophisticated heuristics creating new experts creating global prediction wrong dwm take account expert age history predictions 
investigate decision tree learner base algorithm maintain encountered examples periodically restructure tree vfdt candidate :10.1.1.119.3124
removing experts low weight yielded positive results problems considered study beneficial investigate mechanisms explicitly handling noise ib determining examples different target concept hoeffding bounds vfdt :10.1.1.119.3124:10.1.1.37.1595
anticipate investigations lead general robust scalable ensemble methods tracking concept drift 
acknowledgments authors william helpful comments earlier drafts manuscript 
avrim blum paul utgoff releasing respective systems community 
research conducted department computer science georgetown university 
supported part national institute standards technology georgetown undergraduate research opportunities program 
schlimmer granger incremental processing tracking concept drift proceedings fifth national conference artificial intelligence 
menlo park ca aaai press pp 

littlestone warmuth weighted majority algorithm information computation vol :10.1.1.37.1595
pp 

utgoff berkman clouse decision tree induction efficient tree restructuring machine learning vol 
pp 

street kim streaming ensemble algorithm sea large scale classification proceedings seventh acm sigkdd international conference knowledge discovery data mining 
new york ny acm press pp 

blum empirical support winnow weighted majority algorithms results calendar scheduling domain machine learning vol 
pp 

michalski selecting examples partial memory learning machine learning vol 
pp 

maybury wahlster eds readings intelligent user interfaces 
san francisco ca morgan kaufmann 
brin motwani ullman tsur dynamic itemset counting implication rules market basket data proceedings acm sigmod international conference management data 
new york ny acm press pp 
may tucson arizona usa 
littlestone learning quickly irrelevant attributes abound new linear threshold algorithm machine learning vol 
pp 

widmer kubat learning presence concept drift hidden contexts machine learning vol 
pp 

hulten spencer domingos mining time changing data streams proceedings seventh acm sigkdd international conference knowledge discovery data mining 
new york ny acm press pp 

michalski incremental learning partial instance memory foundations intelligent systems ser 
lecture notes artificial intelligence 
berlin springer verlag vol 
pp 
proceedings thirteenth international symposium methodologies intelligent systems lyon france june 
incremental learning partial instance memory artificial intelligence appear 
incremental rule learning partial instance memory changing concepts proceedings international joint conference neural networks ijcnn 
los alamitos ca ieee press appear 
aha kibler albert instance learning algorithms machine learning vol :10.1.1.37.1595
pp 

michalski quasi minimal solution general covering problem proceedings fifth international symposium information processing vol 
pp 

michalski larson incremental generation vl hypotheses underlying methodology description program aq department computer science university illinois urbana technical report uiucdcs 
breiman bagging predictors machine learning vol 
pp 

freund schapire experiments new boosting algorithm proceedings thirteenth international conference machine learning :10.1.1.133.1040
san francisco ca morgan kaufmann pp 

bauer kohavi empirical comparison voting classification algorithms bagging boosting variants machine learning vol 
pp 

dietterich experimental comparison methods constructing ensembles decision trees bagging boosting randomization machine learning vol 
pp 

opitz maclin popular ensemble methods empirical study journal artificial intelligence research vol 
pp 

online 
available www jair org fern givan online ensemble learning empirical study machine learning appear www com issn 
fan stolfo zhang application adaboost distributed scalable online learning proceedings fifth acm sigkdd international conference knowledge discovery data mining 
new york ny acm press pp 

quinlan programs machine learning 
san francisco ca morgan kaufmann 
langley elements machine learning 
san francisco ca morgan kaufmann 
john langley estimating continuous distributions bayesian classifiers proceedings eleventh conference uncertainty artificial intelligence 
san francisco ca morgan kaufmann pp 

blake merz uci repository machine learning databases department information computer sciences university california irvine web site 
online 
available www ics uci edu mlearn mlrepository html kohavi scaling accuracy naive bayes classifiers decision tree hybrid proceedings second international conference knowledge discovery data mining 
menlo park ca aaai press pp 

maclin opitz empirical evaluation bagging boosting proceedings fourteenth national conference artificial intelligence 
menlo park ca aaai press pp 

opitz feature selection ensembles proceedings sixteenth national conference artificial intelligence 
menlo park ca aaai press pp 

schlimmer fisher case study incremental concept induction proceedings fifth national conference artificial intelligence 
menlo park ca aaai press pp 

domingos hulten mining high speed data streams proceedings sixth acm sigkdd international conference knowledge discovery data mining :10.1.1.119.3124
new york ny acm press pp 

hoeffding probability inequalities sums bounded random variables journal american statistical association vol 
pp 


