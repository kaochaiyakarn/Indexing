journal machine learning research submitted published grafting fast incremental feature selection gradient descent function space simon perkins perkins lanl gov space remote sensing sciences los alamos national laboratory los alamos nm usa kevin eecs berkeley edu department computer science university california berkeley ca usa james theiler jt lanl gov space remote sensing sciences los alamos national laboratory los alamos nm usa editors isabelle guyon andr elisseeff novel flexible approach problem feature selection called grafting 
considering feature selection separate learning grafting treats selection suitable features integral part learning predictor regularized learning framework 
regularized learning process sufficiently fast large scale problems grafting operates incremental iterative fashion gradually building feature set training predictor model gradient descent 
iteration fast gradient heuristic quickly assess feature improve existing model feature added model model incrementally optimized gradient descent 
algorithm scales linearly number data points quadratically number features 
grafting variety predictor model classes linear non linear classification regression 
experiments reported variant grafting classification linear non linear models logistic regression inspired loss function 
results variety synthetic real world data sets 
relationship grafting stagewise additive modelling boosting explored 
keywords feature selection functional gradient descent loss functions margin space boosting 

systems performing automated feature selection long occupied strange position acting bridge harsh reality real world idealistic environments inhabited machine learning algorithms 
wonder feature selection seen separate learning altogether ad hoc mysterious 
result feature selection methods independent learning systems 
filter methods example relief algorithm kira rendell quickly computed heuristic estimate value feature individually combination simon perkins kevin james theiler 
perkins theiler select set features underlying learning engine sees data 
wrapper methods kohavi john interact underlying learning engine typically communicate brief summaries performance cross validation scores :10.1.1.30.525
information learning system gleaned data usually ignored choosing features 
great efforts expand applicability robustness general learning engines result distinction feature selection learning look little artificial 
just sides task learning model set training data described large number features special domain knowledge 
observation motivates approach view feature selection performance pragmatic purposes part integrated criterion optimization scheme 
section presents regularized risk minimization framework forms core scheme section introduces fast incremental method finding optimal approximately optimal solutions call grafting 
section provides empirical comparison grafting number learning feature selection techniques section draws contrasts grafting related stagewise additive modelling boosting 

learning select features view feature selection just aspect single problem learning mapping training data described large number features 
core view common modern approach machine learning described regularized risk minimization 
rest section review approach consider adapted include feature selection explain idea 
learning regularized risk minimization definitions 
assume trying find predictor function maps feature vectors fixed length scalar output value 
binary classification problem derive output label sign 
regression problem produce output predicted value 
machine learning 
derived learning procedure training set consisting randomly sampled pairs drawn distribution re attempting model 
assume member family predictor functions parameterized set parameters 
specify particular member family explicitly omit subscript brevity 
want come minimizes expected risk dxdy 
name grafting derived gradient feature testing reasons clear 

term feature variable cover general case arguments functions raw variables describing problem 
grafting fast incremental feature selection loss function specifies penalized returning true target value andp joint probability density function classification problems common loss function misclassification rate sign 
general know usually possible directly optimize criterion respect 
usually empirical risk remp calculated training data integral replaced sum data points 
known directly optimizing empirical risk lead overfitting common modern machine learning attempt minimize combination empirical risk plus regularization term penalize complex solutions 
attempt minimize criterion form xi yi regularization function high value complex predictors loss functions classification theory simply error rate loss function main problems 
loss function inevitably leads optimization problem hard solve exactly 
second experience theory shown obtain robust better generalizing classifiers prefer classifiers separate data wide margin possible 
discussion phenomenon examples smola 

usually improve generalization performance criterion easier optimize choosing loss function encourages large margin solutions 
define margin classifier single data point true label yf 
margin positive point correctly classified sign agrees sign negative 
commonly loss functions conveniently defined terms margin 
illustrates 
classification binomial negative log likelihood loss function hastie ln loss function derived model treats log ratio probability probability 
main value assumption allows calculate relation loss function readily generalized multi class classification problem ideas multi class logistic regression hastie pp 

contains derivation loss function 
defines loss single data point 
useful define total loss training set known empirical risk term loss perkins theiler example loss functions error rate svm perceptron binomial negative log likelihood commonly loss functions plotted function margin yf 
shown svm loss function max perceptron criterion max binomial negative log likelihood ln 
regularization possibly straightforward approach machine learning involves defining family models selecting model minimizes empirical risk 
certainly technique serious problems 
problem certain combinations model family empirical risk function training data optimization problem unbounded respect model parameters 
example consider linear model defined wn vector weights xi th feature feature vector andb constant offset 
training data linearly separable increase magnitude indefinitely reduce 
second problem known overfitting problem 
sufficiently flexible family classifiers possible find low empirical risk generalizes badly previously unseen data 
problems tackled adding regularization term empirical risk 
regularization term regularizer function model parameters returns high value complex models liable generalize badly 
optimizing sum regularizer empirical risk achieve trade model simplicity empirical risk 
balance chosen appropriately improve generalization performance significantly compared simple empirical risk minimization 
form regularizer depends extent form model restrict class models model parameters take form vector real valued numbers length refer weight vector class models includes linear models types multi layer perceptron mlp 
grafting fast incremental feature selection parameter vector define commonly employed family regularizers parameterized non negative integer vector positive real numbers wi members regularizer family correspond different kinds weighted minkowski norm parameter vector referred lq regularizer 
usually choose simply include exclude certain elements regularization 
essence regularizers penalize large values wi 
easy show solutions unconstrained minimization regularizer equivalent constrained optimization problem minimize xi yi wi necessarily simple correspondence parameters 
alternative formulation useful consider regularizer 
interesting members family involve 
summary properties peculiarities regularizers 
regularizer seen ridge regression support vector machine boser sch lkopf smola regularization networks girosi 
give various justifications form regularization 
reason preferring regularizers produces solution arbitrary rotation feature space axes 
norm solution bounded 
increased magnitudes elements tend decrease general go zero 
norm convex function weights loss function optimized convex function weights regularized loss single local global optimum 
regularizer known lasso 
tibshirani describes great detail notes main advantages leads solutions elements exactly zero 
increased number zero weights steadily increases 
regularizer norm means arbitrary rotation feature axes general produces different solution feature axes special status choice regularizer 
regularizer regularizer leads bounded solutions 
similarly norm convex function weights loss function optimized convex function weights regularized loss single local global optimum 
define contributes fixed penalty weight wi 
identical equivalent setting limit maximum number non zero weights 
norm non convex function weights tends exact optimization computationally expensive 

instance linear model usually want exclude constant offset term 
feature selection regularization perkins theiler mathematical expression model feature vector elements appear associated multiplicative weight process feature selection amounts producing model subset weights associated features non zero 
regularizers described lead solutions weights set exactly zero 
justified terms standard reasons feature selection 
motivations feature selection consider broad classes generally encompass reasons commonly 
pragmatic motivations feature selection motivations feature selection pragmatic 
wish reduce training time reduce time takes apply learned model new data reduce storage requirements model improve intelligibility model 
interpret constituting fixed penalty including feature model constraint maximum number features model 
simplest linear model case feature appears model associated just single weight easy see interpretations correspond regularizer wi multiplicative weight feature 
complex models feature associated weights handle slightly modified version regularizer max si set weight indices associated th feature 
different features carry different costs instance features expensive compute adjust associated features accordingly 
performance motivations feature selection common motivation feature selection improve generalization performance learned models 
general feature dimensions model includes greater capacity greater tendency overfit training data called curse dimensionality 
regularization techniques intended prevent overfitting question arises regularization need additional feature selection 
alternatively achieve improved generalization performance regularizer encourages zero weighted features model regularizers 
order explore issue performed simple experiment compare generalization performance simple linear classifier regularizers presence varying numbers irrelevant features 
created sequence simple feature class problem follows 
class features training example drawn independently normal distribution mean equal standard deviation 
features second class generated way normal distribution mean 
randomly permute feature values training examples elements feature vector features 
produces training set feature exactly distribution correlated class label mean misclassification rate grafting fast incremental feature selection bayes error irrelevant features mean misclassification rate irrelevant features comparison different regularization schemes problem varying numbers irrelevant features different optimal bayes error 
features irrelevant 
training sets irrelevant features generated containing samples drawn class 
compared problems 
bayes misclassification rate bayes misclassification rate 
test sets generated way samples class 
linear model trained optimizing regularized risk criterion logistic regression loss function regularizers regularization 
gradient descent algorithm nelder mead backward elimination kohavi john choosing eliminate feature increased loss function step :10.1.1.30.525
greedy procedure may produce optimal answer exhaustive subset comparison impractically slow 
regularization parameters regularizers regularizer generating multiple instances problem searching values minimized average misclassification rate 
compares performance various regularizers number irrelevant features altered 
problem type training test sets randomly generated 
plots show mean test score type regularizer different values 
reasons clarity error bars indicating standard error mean misclassification rate shown small compared separation curves 
values produce qualitatively similar results 
subset selection experiments perform worst subset selection relatively better informative features separated low case 
experiments features relevant regularization slightly outperforms regularization 
number irrelevant features increases regularization takes lead 
interestingly features irrelevant case test performance regularization level performance regularization continues degrade 
performance key concern preferred regularizers 
expect large fraction irrelevant features prefer regularizer 
somewhat similar reached tibshirani 
perkins theiler criterion models pragmatic motivations feature selection 
models performance motivations feature selection 
leads sparse solutions 
performance features relevant 
ok excellent performance features irrelevant 
poor ok convex regularizer 
doesn add extra local optima numerical friendliness poor excellent table comparison different regularizers 
unified optimization criterion argued significant class models described real valued parameter vectors motivations feature selection incorporated regularized risk minimization framework suitable combination regularizers 
table summarizes different qualities regularizers considered 
general want leads optimization criterion 
grafting yi wi wi wish find minimum respect model parameters 
consider done introduce grafting algorithm fast way getting optimal approximately optimal solution non zero 
direct gradient descent optimization probably direct method solution perform gradient descent respect model parameters minimum 
compute gradient loss function regularization term respect parameters conjugate gradient quasi newton methods 
minimization method doesn require gradient information powell direction set method 
see press 
chap 
overviews methods 
unfortunately number problems approach 
firstly gradient descent quite slow 
algorithms conjugate gradient descent typically scale quadratically number dimensions particularly interested domain features large 
quadratic dependence number model weights particularly wasteful regularizer know subset weights non zero 
second problem regularizers continuous derivatives respect model weights 
cause numerical problems general purpose gradient descent algorithms expect derivatives continuous 

conjugate gradient descent requires line minimizations gradient calculation required determine direction minimization typically giving total complexity closer large grafting fast incremental feature selection problem local minima 
regularizers convex functions weights loss function convex function weights single optimum 
regularizer hand convex introduces local minima optimization problem 
stagewise gradient descent optimization regularizer suspect number non zero weights final model going total number weights efficient stagewise optimization procedure suggests 
call algorithm grafting 
basic plan model weights zero 
iteration grafting procedure fast gradient heuristic decide zero weight adjusted away zero order decrease optimization criterion maximum amount 
perform gradient descent weight non zero weights model continue progress 
basic grafting algorithm ease presentation consider case non zero 
return case 
stage discussion applies broad class models loss functions 
described assume model parameterized weight vector stage grafting process model weights divided disjoint sets 
weights wi free altered desired 
remaining weights wi fixed zero 
assume output model training example differentiable respect model weights calculate xi arbitrary feature vector xi arbitrary weight explained grafting step step minimize respect free weights th grafting step wi th grafting step wish move weight sensible select weight going greatest effect reducing optimization criterion gradient criterion respect arbitrary model weight wi wi xi iwi wi xi wi xi xi wi contribution term disappears wi slightly subtle replacement sign wi invites question sign fact sign defined value 
recall interested perkins theiler determining weight adjusted appropriate direction decrease fastest rate 
consider wi derivative total loss respect wi just term expression 
suppose wi wi regardless sign wi 
case order decrease want decrease wi 
wi starts zero infinitesimal adjustment wi take negative 
purposes sign wi 
similarly wi effectively sign wi 
essentially effect derivative simply reduce magnitude wi amount argument shows wi possible produce local decrease adjusting wi away zero 
essence regularizer leads solutions zero valued weights provides basis stopping conditions discussed 
grafting step calculate magnitude wi wi determine maximum magnitude 
add weight set free weights call general purpose gradient descent routine optimize respect free weights 
know calculate gradient wi efficient quasi newton method 
fletcher goldfarb shanno bfgs variant see press chap 
description 
start optimization weight values th step general added weight changes significantly 
note choosing weight magnitude wi guarantee best weight add stage 
faster alternative trying optimization member turn picking best 
shall see procedure take solution locally optimal 
incorporating regularizer regularizer means transferring weight wi incurs penalty fixed penalty substantially harder determine weight promising transfer heuristic case empirical observation sequence grafting steps magnitude added weight typically decreases monotonically 
allows estimate upper limit magnitude weight add turn means roughly estimate bound change result adding weight wi grafting step weight added wi wi picking best weight add amounts choosing wi minimizes 
note wi wj heuristic equivalent simple heuristic previously discussed 
stopping conditions regularizer general model contain zero valued weights grafting procedure terminate empty 
case advantage grafting full gradient descent optimization 

exception weights incur penalty case simple linear model simply penalize number included features 
case reasonable standard heuristic 
grafting fast incremental feature selection regularizer reach point wi wi point possible decrease moving weight adjusting weights local global minimum terminate grafting procedure 
regularizer may reach point increases adding wi set wi zero remove undo optimization step convenient keep copy previous model order avoid extra optimization step 
choice 
possible different choice wi lead decrease try optimization step wi associated lowest value wi 
cycle repeated remaining weights eliminated algorithm terminates 
alternatively just terminate algorithm time happens recognizing regularizer solution greedy approximation optimal solution best 
approach recommend cases 
optimality convex loss function function weights just regularizers convex functions weights minimum 
examination stopping conditions reveal grafting algorithm guaranteed local optimum grafting guaranteed find global optimum cases 
seen regularizer harder find optimal solution 
grafting procedure non zero amounts greedy heuristic forward subset selection method sacrifices optimality return fast learning 
problem hand depends situation 
note smaller smaller relative chances sub optimal situation decrease 
inclined fairly small cases 
computational complexity claimed grafting substantially faster full gradient descent 
examine claim carefully 
weights weight vector full gradient descent requires multiple line minimizations optimize criterion say cp minimizations 
determining direction requires derivatives wi computed 
computation derivative dominated computation wi simply weighted sum simple derivatives wi 
line minimizations require function evaluations large minor contribution 
denote time taken calculate simple derivative total time taken full gradient descent cmp 
grafting select number weights algorithm terminates 
select weight grafting step take steps 
th step consists phases 
evaluate wi weights noted derivative calculation takes time devoted gradient testing steps msp 

criterion perfect quadratic form 
perkins theiler second phase involves optimizing respect free weights 
take ck line minimizations take free weights close optimal values 
compensate constant probably high ignore time taken line minimizations small multiple 
time taken optimization cmk overs steps cms 
putting total grafting run time msp cms 
assume clear grafting algorithm substantially quicker cmp required full gradient descent 
note full gradient descent algorithm deal discontinuities gradient slow significantly 
keeping zero valued weights optimization steps grafting avoids difficulty 
normalization order gradient magnitude heuristic fair comparison usually important normalize features approximately scale 
linearly scale feature vectors feature mean value zero standard deviation 
course important scale testing data scaling parameters derived training data 
grafting examples helpful illustrate grafting algorithm detail particular models loss functions 
concern binary classification problems suitable loss function binomial negative log likelihood 
simplicity assume regularization 
linear model consider linear models weights form define margin training pair xi yi yi xi regularized optimization criterion wi number selected features 
note constant offset term appear regularizer want penalize mere translation linear discriminant surface 
derivatives need ignoring term xi th component xi 
grafting fast incremental feature selection instructive interpret derivative geometric way 
imagine dimensional margin space dimension training point 
think coordinate axes representing margins current model point xi total loss function thought function space calculate full gradient function margin space imagine feature margin vector write grafting heuristic selecting weight add model interested magnitude derivative regularizer component acts reduce magnitude see picking weight amounts choosing feature margin vector aligned direction steepest descent loss function margin space 
linear model initialize contain just perform initial optimization 
proceed usual grafting fashion picking new weight add grafting step stopping condition reached 
case weight corresponds feature 
simplicity linear model means fit training data 
problem somewhat reduced features general extra features tend problem linearly separable 
multi layer perceptron model powerful model mlp hidden units having sigmoid transfer functions linear output unit unit output weights 
write mlp function xi model weight th feature th hidden unit 
sigmoid transfer function defined usual neural net sigmoid function scaled vertically net slightly better behaved grafting 
optimization criterion identical different substituted 
weights mlp model included equally regularization term bias weights node excluded constant weights output node 
fixed number hidden units grafting procedure describe allows hidden units added grafting process continues 
mlp model weight added corresponds perkins theiler new connection network 
containing just output bias weight network hidden units perform initial optimization respect just bias weight 
grafting step decide grow network possible ways adding new hidden unit hidden units involves adding hidden node new connection output node new connection hidden node feature input weight question feature connected new hidden unit 
derivative need jx adding input connection existing hidden unit hidden units may connected input features connections candidates adding model 
considering adding connection th input feature th hidden unit relevant derivative jx xi grafting steps possibilities consider adding new hidden unit hn possibilities consider adding connection new hidden unit 
usual grafting procedure calculate derivatives candidates pick largest magnitude 
corresponding weight added 
cycle repeated stopping conditions met 
new hidden unit added need include associated bias weight initialized zero variants number variants basic grafting procedure possible 
interesting alternative attempt full optimization full set grafting step 
added weight bias weights adjusted 
grafting step faster expense loss accuracy strong possibility solution local optimum 
practice small fraction possible weights non zero grafting finishes time spent checking gradients determine weight add dominates run time saving little effort optimization little difference 
grafting readily extended regression problems suitable loss function squared error loss function 
implemented 

grafting experiments section compare performance grafting number different approaches feature selection set synthetic real world test problems 
simplicity concentrate entirely binary classification problems 
grafting fast incremental feature selection datasets datasets experiments labeled dataset consists training set test set 
datasets synthetic problems instances basic task described 
datasets real world problems taken online uci machine learning repository blake merz 
synthetic problems variations task call threshold max tm problem 
basic version problem feature space contains nr informative features uniformly distributed 
output label point defined max xi nr nr points occupy large hypercube corner larger hypercube containing points 
points fill remaining space 
constant expression chosen half feature space belongs class 
variations basic problem derived adding ni irrelevant features uniformly distributed nc redundant features just copies informative features 
tm problem designed informative features provides little information problem completely separable 
addition optimal discriminating surface problem non linear problem asymmetric linear discriminant able better random 
instantiations training testing sets synthetic problems generated obtain statistics relative algorithm performance 
detail datasets dataset tm problem nr nc 
training set test set contain points 
dataset explores effect irrelevant features tm problem 
dataset tm problem nr nc ni 
training set test set contain points 
dataset explores effect redundant features tm problem 
dataset tm problem nr nc ni 
training set contains training points despite problem dimensionality 
test set contains points 
dataset explores extreme situation features training points 
dataset multiple features database uci repository 
handwritten digit recognition task digitized images digits represented features various types 
task tackled distinguish digit digits 
training test sets consist points 
features scaled zero mean unit variance 
dataset arrhythmia database uci repository 
task distinguish normal abnormal heartbeat behavior ecg data described numeric binary attributes 
data slightly modified original easier 
feature number missing records removed records 
perkins theiler instances database missing attribute values instances removed leaving instances described attributes 
divided training set points test set points 
datasets experiments online nis www lanl gov data jmlr algorithms different algorithms denote letters compared datasets described 
described implementations relied matlab including matlab optimization toolbox 
linear grafting algorithm described section regularization 
mlp grafting algorithm described section regularization 
simple gradient descent fit regularized linear combination input features 
gradient descent complete weights magnitude maximum magnitude weight vector pruned order obtain subset selection driven regularization 
simple gradient descent fit regularized fully connected mlp similar form learned mlp grafting algorithm exception number hidden nodes fixed nodes course connectivity higher 
gradient descent complete weights magnitude maximum magnitude weight vector pruned order obtain subset selection driven regularization 
linear svm effectively uses regularization slightly different loss function grafting implementations 
svm implementation libsvm chang lin 
gaussian rbf kernel svm default libsvm kernel parameters 
gaussian rbf kernel svm conjunction wrapper feature subset selection 
feature selection step possible features considered addition current feature set fold cross validation select best 
process repeated selected features final rbf svm trained just features 
essentially greedy forward subset selection 
gaussian rbf kernel svm conjunction filter feature subset selection 
dataset simply take features highly correlated label train svm features 
note efficient implementation grafting directly exploit sparsity models trained 
matlab implementations take care 
grafting fast incremental feature selection linear graft gd mlp graft gd linear svm rbf svm table regularization parameters experiments dataset chosen fold cross validation 
gd stands gradient descent 
experimental details algorithm applied datasets 
exception fully connected mlp algorithm failed converge larger datasets 
suspect caused large number weights close zero model discontinuous derivative regularizer point 
synthetic problems training runs repeated different random instantiations training test sets assess sensitivity small changes dataset 
regularization parameters experiments chosen fold cross validation training sets 
algorithms share parameters algorithms algorithms 
note grafting algorithms regularization requiring parameters corresponding simple gradient descent algorithms regularization 
due difficulty incorporating regularization standard gradient descent algorithm 
parameters listed table 
matlab implementations algorithms training time dataset recorded see grafting gave speedup simple gradient descent 
direct speed comparison svm algorithms written matlab implementations attempted time due inherent efficiency differences matlab code 
recorded number features selected number weights general algorithms select subset features algorithm 
addition synthetic problems directly calculate measure useful selected feature subsets 
recall problems nr informative features including redundant duplicates 
define feature set saliency measure ng nr ng ng number features selected informative features including duplicate redundant features total number features selected 
saliency evaluates nr features selected selected 
evaluates features selected evaluates approximately zero features selected irrelevant redundant features 
note number selected features linear mlp models trained simple gradient descent algorithms relies somewhat subjective pruning low valued weights described 
measurements numbers selected features perkins theiler linear graft mlp graft linear gd mlp gd table timing comparisons matlab implemented algorithms datasets 
entries table mean number cpu seconds training algorithm dataset 
note mlp gradient descent algorithm applied larger datasets due convergence problems 
feature subset saliency treated suspicion algorithms 
grafting avoids issue adjusting weights non zero 
results analysis summarizes test performance subset selection results experiments 
table details mean run time various matlab algorithms dataset 
consider results dataset turn 
dataset features dataset irrelevant weakly relevant 
problem linearly separable error rate linear algorithms probably couldn better observed level optimal choice hyperplane 
curiously non linear svm algorithm performed badly presumably due large number irrelevant features 
adding subset selection non linear svm perform somewhat better filter selection method slightly outperforming wrapper method 
winner long way mlp graft algorithm 
reasons performance differences largely explained saliency chart 
gradient test heuristic able pick just salient features accuracy mlp flexible fit non linear discriminant boundary high degree accuracy 
linear grafting algorithm picked sets features unable exploit due inflexible model 
correlation filter method picked third salient set features came third terms performance 
performance difference mlp grafting algorithm non linear svm due fact mlp network matches structure threshold max problem better rbf network 
dataset time irrelevant features merely copies features 
task pick ones necessary 
linear models poorly linear grafting able pick salient minimal subset features 
absence noise features non linear svm better 
interestingly wrapper subset selection affect performance svm significantly 
mlp grafting algorithm runaway winner picking extremely salient feature subset achieving lower error rates algorithms 
worst algorithm significant margin error rate saliency grafting fast incremental feature selection performance test set percentage features final model saliency selected features experimental comparison linear non linear grafting learning methods datasets 
see main text description data sets algorithms 
short lines seen top groups bars test performance chart represent standard error mean test scores obtained different random instantiations training test sets 
datasets bar corresponding algorithm missing 
algorithm gradient descent trained fully connected regularized mlp failed converge solution larger datasets 
perkins theiler filter subset selection technique 
expected simple filter reason pick multiple redundant features 
dataset dimensions training points hard problem 
looking saliency chart clear grafting algorithms wrapper filter selection methods pick poor subsets features 
algorithms perform better random classifier svm algorithms subset selection 
non linear svm performs best significant margin 
problem features data points grafting subset selection methods find irrelevant features explain labels better true informative features merely chance 
seize features useful 
contrast basic svm regularization subset selection tendency focus just features better 
dataset digit recognition problem turns easy algorithms misclassifying member test set 
challenge solve problem quickly features possible features relatively expensive compute real world challenge 
linear non linear grafting algorithms excellent job respect achieving excellent performance features 
parsimony pays terms training time linear grafting algorithm times faster training full linear model gradient descent 
dataset arrhythmia problem quite hard 
best algorithm linear svm average error rate 
algorithms achieve performance figures vicinity grafting algorithms achieve error rate small fraction available features 

discussion finish compare grafting better known algorithms 
connections stagewise additive modelling linear mlp models considered examples additive models hastie 
chap 
provide depth discussion class models long history statistics 
describe method called additive regression allows fit model backfitting friedman stuetzle 
backfitting simple iterative procedure individual functions fi adjusted time optimize chosen loss function cycling process times optimize model 
additive logistic regression specializes problem classification binomial negative log likelihood loss function optimization criterion 
classes additive model considered simply sophisticated general purpose gradient descent algorithms optimize model efficient simple backfitting procedure 
fi grafting fast incremental feature selection alternative model fixed number components incrementally develop model greedy stagewise algorithm 
approach time step add new component additive model designed reduce current loss possible 
previous components fixed new function adjusted 
example matching pursuit described mallat zhang 
approach discussed length friedman 
develop greedy stagewise form additive logistic regression 
looks similar grafting procedure described new grafting 
important differences grafting build models components may combined complicated ways simply added 
mlp graft instance grafting step consider adding new hidden node adding input existing node 
options considered framework involve different alterations model 
stagewise additive modelling usually concerned adding fi model step 
step grafting considers possibilities altering current model typically weight decides gradient test heuristic 
stagewise additive modelling usually consider possibility add literally new function fi fit weighted version training set 
updating model stagewise additive modelling typically adjusts parameters relating added fi 
grafting usually adjusts free parameters model gradient descent engine 
feasible model output differentiable respect model parameters 
additive model practitioners backfitting fit models efficient quasi newton gradient descent algorithm 
connections boosting boosting freund schapire active fields research modern machine learning 
boosting particular adaboost algorithm provides way training sequence classifiers data combining form ensemble classifier substantially better member classifiers 
boosting viewed form stagewise additive modelling friedman optimizes logistic regression loss function combined regularizer 
works reweighting training data stage training new classifier weak learner forming composite classifier linear combination classifiers learned stage 
reweighting data shown encourage new classifier learned maximally decreases logistic regression loss friedman 
consider variant grafting mentioned section update added weights linear model regularization extreme version boosting weak learner simply fits model form xk choosing single feature xk add model 
technique boosting stumps similar usually thresholded zero produce output required original adaboost algorithm 
perkins theiler versions boosting relax requirement schapire singer fact variant linear grafting simple form boosting 
original boosting algorithm mandated regularizer logistic regression loss function 
mason 
devised general purpose algorithm called anyboost grafting stagewise additive modelling variety loss functions regularization methods 
implications regularization achieved best performance grafting algorithms regularizer 
argued see tibshirani regularization represents compromise needs coefficient shrinkage generalization performance needs subset selection pragmatic reasons 
discussed section really separate goals accounted separately 
experience attempted regularization impossible find single value produced generalization performance incorporating largely irrelevant features 
noted regularizer smallest value regularizer convex 
related das describes technique boosting feature selection 
combine feature selection model building integrated way described 
jebara jaakkola examine feature selection regularized risk minimization context linear models 
define prior describing values weights choose tends drive number weights zero similar fashion regularization 
fact regularization viewed equivalent sharply peaked prior weight values tibshirani 
weston 
describe gradient descent method select subset features non linear support vector machines 
input features weighted vector weights 
vector optimized gradient descent minimize bound expected leave error subject constraint magnitude 
leads elements driven zero 
svms normally associated regularization possible formulate norm 
fung mangasarian describe gradient descent technique rapidly optimizing svms property usual weights driven zero 
blake merz 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
university california irvine dept information computer science 
boser guyon vapnik 
training algorithm optimal margin classifiers 
proc 
fifth annual workshop computational learning theory pages pittsburgh acm 
grafting fast incremental feature selection chang lin 
libsvm library support vector machines 
software available www csie ntu edu tw cjlin libsvm 
das 
filters wrappers boosting hybrid feature selection 
proc 
icml 
morgan kauffmann 
freund schapire 
experiments new boosting algorithm 
machine learning proc 
th int 
conf pages 
morgan kaufmann 
friedman hastie tibshirani 
additive logistic regression statistical view boosting 
annals statistics 
friedman stuetzle 
projection pursuit regression 
journal american statistical association 
fung mangasarian 
feature selection newton method support vector machine classification 
technical report data mining institute dept computer sciences university wisconsin madison sept 
girosi jones poggio 
regularization theory neural networks architectures 
neural computation 
hastie tibshirani friedman 
elements statistical learning 
springer 

ridge regression biased estimation nonorthogonal problems 
technometrics 
jebara jaakkola 
feature selection dualities maximum entropy discrimination 
proc 
int 
conf 
artificial intelligence 
kira rendell 
practical approach feature selection 
sleeman edwards editors proc 
int 
conf 
machine learning pages 
morgan kaufmann 
kohavi john 
wrappers feature subset selection 
artificial intelligence 
mallat zhang 
matching pursuit time frequency dictionaries 
ieee transactions signal processing 
mason baxter bartlett frean 
functional gradient techniques combining hypotheses 
smola bartlett sc lkopf schuurmans editors advances large margin classifiers pages 
mit press 
nelder mead 
simplex method function minimization 
computer journal 
press teukolsky vetterling flannery 
numerical recipes cambridge university press nd edition 
schapire singer 
improved boosting algorithms confidence rated predictions 
machine learning 
perkins theiler sch lkopf smola 
learning kernels 
mit press cambridge ma 
smola bartlett sch lkopf schuurmans editors 
advances large margin classifiers 
mit press 
tibshirani 
regression shrinkage selection lasso 
technical report dept statistics university toronto 
weston mukherjee chapelle pontil poggio vapnik 
feature selection svms 
advances neural information processing systems 

