round robin ensembles johannes rnkranz austrian research institute artificial intelligence wien austria tr investigate performance pairwise round robin classification originally technique turning multi class problems class problems general ensemble technique 
particular show round robin ensembles increase classification performance decision tree learners directly handle multiclass problems 
performance gain large bagging boosting hand round robin ensembles clearly defined semantics 
furthermore investigate confidence estimates improve accuracy predictions ensemble 
show advantage pairwise classification direct multi class classification binarization increases number classes round robin ensembles form interesting alternative problems ordered class values 
rnkranz analyzed performance pairwise classification call round robin learning handling multi class problems rule learning 
rule learning algorithms concept learning algorithms restricted learning binary classes handle multi class problems converting series class problems class 
problems uses examples corresponding class positive examples negative examples 
procedure known class binarization 
round robin binarization hand converts class problem series class problems learning classifier pair classes training examples classes ignoring 
new example classified submitting binary classifiers combining predictions simple voting 
important result previous study procedure increases predictive accuracy expensive commonly approach 
obviously round robin classifier may interpreted ensemble classifier similar error correcting output codes dietterich bakiri constructs ensemble transforming learning problem multiple class problems learning classifier 
fact allwein 
show pairwise classification class binarization techniques draft previously appeared rnkranz 
main changes sections completely rewritten complemented new results section original contribution 
special case generalized version error correcting output codes allows specify certain classes ignored problems addition assigning positive negative class conventional output codes 
investigate properties round robin learning general ensemble technique 
start brief recapitulation previous results round robin learning section 
section investigate performance round robin binarization general ensemble technique show may lead performance improvements multi class base learning algorithms case 
comparison round robin ensembles bagging boosting section show performance gain may match gain bagging boosting ripper base learner lack boosting 
evaluate various combinations bagging boosting round robin 
section discuss confidence estimates improving prediction quality round robin ensemble 
classes result larger ensemble classifiers reasonable expect performance round robin ensembles depends crucially number classes problem 
section investigate relation classification problems identical attributes varying numbers classes obtain discretizing target variables regression problems 
results show round robin learning improve performance decision tree learners higher number classes increases performance particular comparison binarization 
results show round robin learning viable alternative ordered classification section 
round robin classification section briefly review round robin learning aka pairwise classification context previous rule learning rnkranz 
separate conquer rule learning algorithms rnkranz typically formulated concept learning framework 
goal find explicit definition unknown concept implicitly defined set positive negative examples 
framework multi class problems problems examples may belong exactly categories usually addressed defining separate concept learning problem class 
original learning problem transformed set binary concept learning problems class positive training examples belonging corresponding class negative training examples belonging classes 
clark boswell proposed technique dealing multi class rule learning problems known areas neural networks anand support vector machines cortes vapnik statistics cf 
multi response linear regression 
variant technique classes ordered relative frequencies training set ripper rule learning algorithm cohen 
hand basic idea round robin classification transform class problem binary problems pair classes 
note case binary decision problems contain fewer training examples examples belong pair classes ignored decision boundaries binary problem may considerably simpler case binarization 
fact example shown pair classes separated linear decision boundary complex functions required separate class classes 
evidence decision boundaries binary problems fact simpler practical applications 
observed classes digit recognition task pairwise linearly separable corresponding task amenable single layer networks 
similarly hsu class binarization left transforms class problem binary problems class problems uses examples class positive examples examples negatives 
round robin class binarization right transforms class problem binary problems pair classes ignoring examples classes 
lin obtained larger advantage round robin binarization unordered binarization support vector machines linear kernel support vector machines non linear kernel 
basic idea pairwise classification fairly known literature 
areas statistics bradley terry friedman neural networks price lu ito support vector machines schmidt gish hastie tibshirani kre el hsu lin 
refer rnkranz section brief survey literature topic 
main contributions previous rnkranz hand empirically evaluate technique rule learning algorithms show preferable technique rule learning algorithms 
importantly analyzed computational complexity approach demonstrated despite fact complexity quadratic number classes algorithm slower conventional technique 
easy see consider case training example times binary problems round robin approach examples times binary problems class paired classes 
furthermore advantage pairwise classification increases computationally expensive super linear learning algorithms 
reason expensive learning algorithms learn small problems faster large problems 
details refer rnkranz 
round robin ensembles ensemble techniques received considerable attention machine learning literature dietterich opitz maclin bauer kohavi 
idea obtain diverse set classifiers single learning problem vote average predictions simple powerful obtained accuracy gains sound theoretical foundation freund schapire breiman 
averaging predictions multiple classifiers reduces variance increases reliability classifier 
techniques obtaining diverse set classifiers 
common technique subsampling diversify training sets bagging breiman boosting freund schapire 
techniques include exploiting randomness base algorithms kolen pollack possibly artificially randomizing behavior dietterich different feature subsets bay multiple representations domain objects example information originating different hyperlinks pointing web page rnkranz press 
classifier diversity ensured modifying output labels transforming learning tasks collection related learning tasks input examples different assignments class labels 
error correcting output codes dietterich bakiri prominent example type ensemble method 
section suggest round robin classification may interpreted ensemble technique performance gain may viewed context 
conventional ensemble techniques final prediction exploiting redundancy provided multiple models constructed subset original data 
contrary subsampling approaches bagging boosting datasets constructed deterministically 
respect pairwise classification quite similar error correcting output codes differs fixed procedure setting new binary problems fact new problems smaller original problem 
foremost question interested round robin learning may boost performance learning algorithms principle capable dealing multi class problems 
performed direct comparison performance rr round robin procedure base learning algorithm 
rr transforms class problem binary problems uses learn decision tree 
predicting class test example submitted classifiers predictions combined unweighted voting 
ties broken favor larger classes 
left half table shows results datasets classes uci repository blake merz 
datasets estimated error rates fold stratified cross validation letter standard examples hold set 
difference include letter dataset computation averages 
shown error rates round robin version ratio tells relative improvement performance ratio means round robin version reduces error rate base learner 
comparison right half table shows results ripper 
boosting deterministic base learner able directly weighted examples 
example weights interpreted probabilities drawing sample boosting iteration 
domains dedicated test sets abalone sat vowel considerably smaller letter decided pool training test examples evaluated fold cross validation 
letter frequently benchmark class binarization techniques 
dedicated hold set results domain comparable previous results 
table comparison round robin ensemble base learner left half table ripper round robin version right half 
shown estimated error rates ratio performance round robin version base learner 
line shows mean error rates geometric average performance ratios 
results letter performed different setup included averages 
dataset round robin ripper round robin letter abalone car glass image lr spectrometer optical page blocks sat solar solar soybean thyroid hyper thyroid hypo thyroid repl 
vehicle vowel yeast average line shows arithmetic mean error rates geometric mean performance ratios average 
note summary measures interpreted care average error rate dominated results large variations algorithms particularly run time results discussed performance ratios somewhat influenced fact default accuracy problems decreases increasing number classes 
consequently error differences problems classes receive lower weight assuming correlation performance algorithms default accuracy problem 
thing note performance improve average round robin binarization pre processing step 
despite fact directly handle multi class problems depend class binarization routine 
presumably simpler decision boundaries binary problems allow learn class binarized problems fairly combination binary predictions produces reliable multi class predictions 
relative improvement large relative improvement ripper improvement ratios general somewhat lower cases large gains optical soybean average reduction error rate opposed ripper 
absolute terms hand performance round robin versions ripper similar error rates approximately range averages dominated larger error rates discussed approximately 
apparently ensembles reach peak performance performance respective base learners different ripper average performance multi class problems study general considerably blame inadequacy ripper class binarization scheme 

case results indicate round robin ensembles may lead considerable reductions error rate 
section try quantify size improvement comparison bagging boosting 
comparison bagging boosting section compare performance round robin ensembles bagging boosting section investigate potential combining techniques section compare properties round robin ensembles bagging boosting section 
experimental comparison doing comparison round robin bagging boosting implemented simple bagging algorithm draws samples replacement available data trains base learning algorithm combines predictions way round robin binarization simple voting priori class probability tie breaker 
boosting relied built strategies algorithms option boosting ripper separate performs boosting 
default algorithms limit number iterations change setting 
choice iterations bagging arbitrarily default values boosting certainly optimal 
table shows results comparison round robin learning bagging boosting top half ripper bottom half base learners 
case round robin ensembles reach performance boosted 
cases round robin outperforms boosting performance gains obtained boosting general larger round robin learning 
average error reduction boosting datasets 
situation somewhat different ripper base learner 
ripper built boosting procedure quite reach performance level round robin learning able outperform significantly cases vowel 
general ripper boosting method robust witnessed crash abalone dataset 
know artifact implementation due fundamental differences rule decision tree learning algorithms 
case round robin ripper outperforms bagged ripper bagged outperforms round robin version interpreted evidence quality base learner performs better ripper set multi class problems strong impact performance bagging boosting 
interpret evidence large part gain round robin learning produces ripper due class binarization technique 
worth look training times algorithm shown right half table 
comparison things kept mind noted boosting bit advantage boosting strategies directly implemented algorithms code round robin bagging implemented perl form results ripper default mode ordered binarization 
results unordered binarization approximately rnkranz 
table comparison round robin learning bagging boosting top half ripper bottom half base learner 
shown error rates performance ratios base learner left half table average training time cpu secs 
user time algorithm 
bagging boosting show ratio run time round robin run time 
ripper implementation boosting crashed abalone dataset 
error rates error reduction rates training times dataset round robin bagging boosting rr bagging boosting letter abalone car glass image lr spectrometer optical page blocks sat solar solar soybean thyroid hyper thyroid hypo thyroid repl 
vehicle vowel yeast average letter abalone car glass image lr spectrometer optical page blocks sat solar solar soybean thyroid hyper thyroid hypo thyroid repl 
vehicle vowel yeast average ripper wrapper writes multiple training sets base algorithm disk 
second bagging boosting perform fixed number iterations ripper boosting algorithm stopped reached iterations round robin learning performs iterations number classes 
problems classes car vehicle table pairwise correlation coefficients relative improvements boosting bagging round robin 
round robin boosting bagging round robin boosting bagging thyroid repl cases round robin performs iterations 
letter domain example classes letter alphabet 
domain round robin algorithm wrote training sets disk 
surprisingly rr slowest algorithm domain times slower iterations boosting 
illustrates fact number members round robin ensembles grows quadratically number classes run time complexity grows linearly rnkranz 
domains class domain page blocks class domain sat round robin algorithms 
ripper round robin learning considerably faster bagging boosting 
surprise consider bagging boosting perform internal class iteration theoretical results show round robin binarization faster unordered class binarization ripper rnkranz 
nicely seen letter dataset bagging times slow round robin single iteration bagging expensive entire round robin procedure 
integration bagging boosting motivations study earlier results rnkranz observed improvements accuracy obtained round robin ripper ripper quite similar obtained boosted problems 
round robin binarization ripper boosting worked 
correlation coefficient performance ratios ripper rr ripper boost range correlation coefficient reported error reduction rates bagging boosting decision trees table opitz maclin 
table shows pairwise correlation coefficients results table 
correlation bagging boosting consistent mentioned results opitz maclin round robin correlate bagging boosting base learner 
hand refutes earlier hypothesis hand throws important question weakness correlation round robin ensemble methods case exploit different domain characteristics getting respective performance improvements 
case integration round robin learning bagging boosting result additional performance gains 
try answer question 
integration bagging round robin learning realized straight forward extension basic algorithm multiple classifiers trained pair classes analogous round robin tournaments sports games team plays team times 
order guarantee variety individual results algorithms random kolen pollack randomized dietterich components random subsets available data training algorithm 
procedure equivalent bagging breiman 
realized integration bagging round robin learning drawing replacement independent samples size pairwise classification table comparison various ways combining bagging boosting round robin learning 
number show accuracies relative improvement base learner 
dataset bag rr boost rr bag boost letter abalone car glass image lr spectrometer optical page blocks sat solar solar soybean thyroid hyper thyroid hypo thyroid repl 
vehicle vowel yeast average problem 
obtained total predictions class problem 
regular round robin procedure final prediction computed simple voting classifiers 
integration boosting round robin learning simply realized boost base learner inside round robin procedure 
similarly boosting bagging combined combining predictions boost classifiers trained bootstrap samples data 
combination ensembles realized boost base learner combination bagging round robin described previous paragraph 
table shows results 
combination bagging boosting round robin learning produce substantially different results bagging boosting combination bagging boosting emerged clear winner 
light results table showed predictions round robin learning bagging boosting correlated predictions bagging boosting came surprise 
expected classifiers differ better chance complementing better classifier exploits different strengths constituents 
hand confirms previous results combinations bagging boosting pfahringer krieger 
bagging boosting average improvement ratios combination round robin learning different individual results compare shown average improvement ratios table 
pattern previous results bagging round robin learning ripper base learner rnkranz trend clearer 
integrating bagging round robin learning fact produced average results comparable combination bagging boosting base learner appears considerably better base learner original multi class problems 
hand cases combination performs better constituents round robin classification boost base learner average lead performance improvements boosting may hurt performance 
sense results analogous schapire compared adaboost oc error correcting output codes binarization scheme conventional class adaboost adaboost freund schapire straight forward adaptation adaboost multiclass base learners version presumably implemented quinlan significant differences quinlan base learner 
analogous comparison boost round robin ensembles schapire boosting outperformed binarization error correcting output codes 
subsequent allwein 
showed performance gain pairwise classification adaboost base learner average performance gain alternative binarization schemes including employing error correcting output codes adaboost oc 
discussion results mixed 
hand show round robin ensembles improve performance considered pre processing method reduces complexity decision boundary mapping problem set smaller subproblems 
hand results show respect gain accuracy round robin learning en par boosting quite bagging consider tried setting crucial parameter number iterations 
results ripper caused ripper class binarization strategy 
ripper previous results comparison probably re investigated respect dimension number classes problem 
example problems studied cohen multi class problems expect ripper results datasets 
investigations necessary definite answer question 
reach performance level alternative ensemble methods believe round robin ensembles deserve attention 
boosting provide larger gains accuracy price pay learned ensemble classifiers longer easy comprehend 
round robin ensembles hand advantage element ensemble defined semantics separating classes questions class class answered showing corresponding theory part 
cases may justifying prediction 
fact proposes similar technique called pairwise ranking order facilitate human decision making ranking problems 
claims easier human determine order items pairwise comparisons individual items adds wins item trying order items right away 
aspect able rank available classifications example intermediate version predicting class value providing full probability distribution interesting aspect round robin binarization briefly return section 
course assumes corresponding binary classifier voted class case voted ruled classifiers vote potentially classifiers ensemble involve think possible exploit clear semantics binary classifier better approach explaining classifiers decisions 
noted contrary boosting individual runs depend performed succession pairwise classification entirely parallelized 
binary task smaller original task total training time round robin ensemble size significantly parallelized version bagging binary classifier trained separate processor 
reason round robin ensembles may provide memory efficient alternative multi class tasks problems large number classes training sets pair classes may fit memory entire dataset 
weighted voting crucial component ensemble technique voting procedure combining predictions member ensemble final prediction 
proposals range simple voting meta learning techniques stacking wolpert arbiters combiners chan stolfo grading rnkranz 
meta learning techniques straightforwardly applicable approach typically depend having fixed number classifiers ensembles constructing meta training set 
studies compared voting techniques combining predictions individual classifiers ensemble various contexts moreira allwein 
specifically tailored pairwise classification proposals combining class probability estimates pairwise classifiers class probability distributions multi class problems hastie tibshirani price 
elaborate suggestions aimed learning separate classifiers deciding example classes train classifier pairwise ensemble moreira organizing classifiers efficient graph structure derive prediction steps platt 
section evaluate ways confidence estimates improving performance round robin ensembles 
previous results ensemble techniques notably boosting schapire singer indicate techniques include confidence estimates ensemble predictors computation final predictions preferable 
basic idea generalize basic voting procedure gives equal weight say vote binary classifier way uses value confidence estimate weight prediction vote classifier quite confident classification vote higher weight low confidence votes lower weight 
estimating confidence predictions add utility writes predictions confidence estimates test file 
commercial product known confidence estimates computed assumed estimates purity leaf classify example largest proportion training examples leaf class 
ripper directly provide confidence estimates implementation comes utility predict quite similar mentioned tool 
delivering confidence estimates predict writes number positive negative training examples classified rule classified test example 
numbers compute straightforward confidence estimates particular purity laplace estimate 
internally ripper breaking ties case example covered rule 
utility freely available fromhttp www com download html 
table impact weighted voting 
shown results round robin unweighted weighted voting ripper form error rates improvement ratios weighted voting unweighted voting 
ripper dataset unweighted weights unweighted purity weights laplace weights letter abalone car glass image lr spectrometer optical page blocks sat solar solar soybean thyroid hyper thyroid hypo thyroid repl 
vehicle vowel yeast average table shows results weighted voting mentioned versions weighted voting ripper purity laplace estimates 
seen weighted voting improve performance particular ripper lead improvements datasets reducing error rate round robin classification factor purity weights laplace weights 
gain large consistent 
tested versions weighted voting ripper shown 
tried call generalized laplace estimates form user settable parameter 
find values lead systematic improvement 
particular option set number classes lead considerably worse results 
second tried different voting procedure predictions originate default rule eligible vote 
reasoning setting ripper learns pair theories pair classes theory learns rules class classifies uncovered examples belonging second class theory roles classes reversed 
felt default rules entirely different characteristics regularly learned rules tried ignore predictions originating 
led bad performance 
suspect reason theories agree default prediction ignored 
presumably hopefully case happens disagreement predictions quite frequently ignored 
may eventually lead situations erroneous predictions get weight 
problem discussed voting procedure guarantee size confidence estimates corresponds relative weight corresponding vote 
investigate question looked different ways rescaling weights 
particular transformed original weights predictions weights predictions ripper different exponents weights form left graph shows average errors ripper ripper laplace correction right graph shows average improvement ratio weighted version respective algorithm unweighted version version 
raising th power prediction weighted may viewed generalization trade weighted unweighted voting equivalent unweighted voting corresponds regular voting discussed 
values trade extremes values successively increase penalty weights increase importance weight close 
shows results values 
show average error rates left graph average performance ratios unweighted voting right graph 
graph shows performance optimal 
achieves better results ripper results better 
experiments needed determine optimal value results relatively stable 
choosing values leads degrade performance increase error continues include graphs 
results show higher exponents penalize laplace correction estimating ripper votes 
summary results section showed weighted voting may lead fairly consistent moderate additional performance gain round robin ensembles 
ensemble size size round robin ensemble depends number classes problem 
section analyze behavior round robin learning varying number classes 
particular interested answering open questions previous research rnkranz advantage round robin binarization binarization increases number classes 
goal mind decided follow experimental set described frank hall 
set regression problems transformed series classification problems different number classes 
transformation performed equal frequency discretization target variable 
resulting problems class table comparison error rates rr regression datasets classification problems class values 
classes classes classes dataset rr rr rr abalone ailerons delta ailerons elevators delta elevators planes pole telecom friedman artificial mv artificial kinematics cpu small cpu act house house auto mpg auto price boston housing diabetes machine cpu servo wisconsin breast pumadyn nh pumadyn bank fm bank nh california housing stocks balanced 
exactly datasets evaluation compare clone implemented weka data mining library witten frank rr version uses pairwise classification base learner 
implementation rr provided richard weka team gave opportunity check previous findings independent implementation algorithm 
table shows fold cross validation error rates algorithm problems sign indicates round robin version higher estimated accuracy 
significance test compute individual differences settings rr outperformed datasets 
conservative sign test comparably high type ii error reject null hypothesis performance rr identical datasets confidence 
datasets pole telecom mv artificial auto mpg completely pairwise classification performs better classification settings 
hard estimate relative size improvement main concern want show performance round robin ensembles improves number classes 
table error training time testing time round robin version version regular binarization technique ordered classification frank hall 
rr ord error training time testing time inspection cases table reveals datasets advantage rr increase number classes step classes cf abalone 
attempt observation objective summarized results algorithms table included results version uses binarization 
show average performance algorithms geometric averages performance ratios rr 
results show performance improvement round robin approach increases steadily measures 
performance improvement increases absolute terms stays relation error rate improvement approximately error rate 
indicate class binarization dangerous larger numbers classes 
possible reason class distributions binary problems case skewed increasing number classes number examples class decreases 
experiments get confirmation independent implementation round robin favorable run time results 
lower parts table shows summaries training test times 
expected round robin binarization considerably faster approach despite fact round robin binarization generates binary problems class problem technique generates problems 
advantage decrease increasing number classes 
consistent theoretical predictions rnkranz theorem case run time complexity quadratic 
slower run time grows faster square training set size expected performance advantage binarization increase number classes 
completeness table shows results testing time clearly worst round robin case 
example tested theories round robin case theories case theory regular round robin testing time grow square number classes 
problem consider technique proposed platt 
suggested organize classifiers efficient graph structure derive prediction steps 
note ordered classification noted basically chose experimental setup frank hall 
difference evaluation procedure theirs single fold cross validation frank hall averaged fold cross validation runs 
differences negligible experiments performed average accuracy estimates estimates differed 
allows evaluate performance round robin learning domains ordered classification 
particular directly compare results results ord algorithm frank hall suggested adaptation class binarization forms groups classes order 
performance ord computed results shown frank hall shown column table 
interesting result difference approach round robin learning 
apparently general round robin learning ordered classification tailored modification learning 
opens question similar adaptation round robin learning improve results leave open 
sense results parallel psychological studies easier rank items making pairwise comparisons try directly construct ranking 
results study mixed hand show round robin ensembles improve performance learners depend class binarization techniques 
round robin binarization considered interesting pre processing technique increasing performance multi class problems 
hand results demonstrate performance gain large gain bagging boosting 
optimizing accuracy concern round robin binarization competitor 
contrary bagging boosting systematic way generating ensemble members pair classes maintains certain amount interpretability certainly bagging boosting 
may form interesting trade performance gain interpretability results 
second investigated confidence estimates combining predictions individual classifiers ensemble prediction 
showed simply purity rule prediction weight classifier vote leads small consistent performance improvement unweighted voting 
procedure optimized choosing appropriate exponent weight fairly experiment 
looked generalized laplace estimates possibility ignoring predictions originate default rules ripper learns default class techniques helpful 
third main result shows performance improvements round robin ensembles increase number classes problem ordered classes 
improvement grows approximately linearly error rate growth performance increase class binarization dramatic 
believe illustrates handling classes major problem binarization technique possibly resulting binary learning problems increasingly skewed class distributions 
showed round robin binarization valid alternative learning ordered classification 
acknowledgments eibe frank mark hall providing regression datasets originally collected luis richard providing implementation pairwise classification weka johann experimentation scripts help maintainers contributors uci collection machine learning databases 
author supported apart austrian academy sciences 
austrian research institute artificial intelligence supported austrian federal ministry education science culture 
allwein schapire singer 
reducing multiclass binary unifying approach margin classifiers 
journal machine learning research 
anand mehrotra mohan ranka 
efficient classification multiclass problems modular neural networks 
ieee transactions neural networks 
bauer kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
bay 
nearest neighbor classification multiple feature subsets 
intelligent data analysis 
blake merz 
uci repository machine learning databases 
www ics 
uci edu mlearn mlrepository html 
department information computer science university california irvine irvine ca 
bradley terry 
rank analysis incomplete block designs method paired comparisons 
biometrika 
breiman 
bagging predictors 
machine learning 
chan stolfo 
comparative evaluation voting meta learning partitioned data 
proceedings th international conference machine learning icml pages 
morgan kaufmann 
clark boswell 
rule induction cn improvements 
proceedings th european working session learning ewsl pages porto portugal 
springer verlag 
cohen 
fast effective rule induction 
prieditis russell editors proceedings th international conference machine learning ml pages lake tahoe ca 
morgan kaufmann 
cortes vapnik 
support vector networks 
machine learning 
dietterich 
machine learning research current directions 
ai magazine winter 
dietterich 
ensemble methods machine learning 
kittler roli editors international workshop multiple classifier systems pages 
springer verlag 
dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
machine learning 
dietterich bakiri 
solving multiclass learning problems error correcting output codes 
journal artificial intelligence research 
frank hall 
simple approach ordinal classification 
raedt flach editors proceedings th european conference machine learning ecml pages freiburg germany 
springer verlag 
freund schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences 
friedman 
approach classification 
technical report department statistics stanford university stanford ca 
rnkranz 
separate conquer rule learning 
artificial intelligence review february 
rnkranz 
round robin rule learning 
brodley editors proceedings th international conference machine learning icml pages ma 
morgan kaufmann publishers 
rnkranz 
pairwise classification ensemble technique 
mannila toivonen editors proceedings th european conference machine learning ecml volume lecture notes artificial intelligence pages helsinki finland 
springer verlag 
rnkranz 
round robin classification 
journal machine learning research 
rnkranz 
hyperlink ensembles case study hypertext classification 
information fusion press 
special issue fusion multiple classifiers 
hastie tibshirani 
classification pairwise coupling 
jordan kearns solla editors advances neural information processing systems nips pages 
mit press 

hsu 
lin 
comparison methods multi class support vector machines 
ieee transactions neural networks march 
personnaz dreyfus 
single layer learning revisited stepwise procedure building training neural network 
fogelman editors neurocomputing algorithms architectures applications volume nato asi series pages 
springer verlag 
personnaz dreyfus 
handwritten digit recognition neural networks training 
ieee transactions neural networks 
kolen pollack 
back propagation sensitive initial conditions 
advances neural information processing systems nips pages 
morgan kaufmann 

kre el 
pairwise classification support vector machines 
sch lkopf burges smola editors advances kernel methods support vector learning chapter pages 
mit press cambridge ma 
krieger wyner long 
boosting noisy data 
brodley editors proceedings th international conference machine learning icml pages ma 
morgan kaufmann publishers 

lu ito 
task decomposition module combination class relations modular neural network pattern classification 
ieee transactions neural networks september 
moreira 
decomposition dichotomies 
proceedings th international conference machine learning icml pages nashville tn 
morgan kaufmann 
moreira 
improved pairwise coupling classification correcting classifiers 
dellec rouveirol editors proceedings th european conference machine learning ecml chemnitz germany 
springer verlag 
opitz maclin 
popular ensemble methods empirical study 
journal artificial intelligence research 
pfahringer 
winning kdd classification cup bagged boosting 
sigkdd explorations 
platt cristianini shawe taylor 
large margin dags multiclass classification 
solla leen 
ller editors advances neural information processing systems nips pages 
mit press 
price personnaz dreyfus 
pairwise neural network classifiers probabilistic outputs 
tesauro touretzky leen editors advances neural information processing systems nips pages 
mit press 

data preparation data mining 
morgan kaufmann san francisco ca 
quinlan 
programs machine learning 
morgan kaufmann san mateo ca 
quinlan 
bagging boosting 
proceedings th national conference artificial intelligence aaai pages 
aaai mit press 
schapire 
output codes boost multiclass learning problems 
fisher editor proceedings fo th international conference machine learning icml pages tn 
morgan kaufmann 
schapire singer 
improved boosting algorithms confidence rated predictions 
machine learning 
schmidt gish 
speaker identification support vector classifiers 
proceedings st ieee international conference conference acoustics speech signal processing icassp pages atlanta ga 
rnkranz 
evaluation grading classifiers 
hoffmann adams fisher editors advances intelligent data analysis proceedings th international conference ida pages portugal 
springer verlag 
witten frank 
data mining practical machine learning tools techniques java implementations 
morgan kaufmann publishers 
wolpert 
stacked generalization 
neural networks 

