generative lexicalised models statistical parsing propose new sta tistical parsing model genera tive model lexicalised context free gram mar extend model clude probabilistic treatment sub categorisation wh movement 
results wall street journal text show parser performs constituent precision recall average improvement collins 
generative models syntax central linguistics introduced sky 
sentence tree pair lan guage associated top derivation con sequence rule applications gram mar models extended cal defining probability distributions points non determinism derivations assign ing probability pair 
proba context free grammar booth thompson early example statistical grammar 
pcfg lexicalised associating head word non terminal parse tree far magerman jelinek collins heavy lexical informa tion reported best statistical parsing formance wall street journal text 
models generative esti mate directly 
proposes new parsing models 
model essentially generative version model described collins 
model extend parser complement adjunct distinction adding probabilities frames head words 
model give probabilistic treatment wh movement research supported arpa 
michael collins dept computer information science university pennsylvania philadelphia pa gradient cis 
upenn edu derived analysis generalized phrase structure grammar gazdar 
advances previous models model performs significantly better collins models give im final results con precision recall average improvement collins 
second parsers collins magerman jelinek produce trees information wh movement subcategorisation 
nlp applica tions need information extract predicate argument structure parse trees 
remainder describe models section discuss practical issues sec tion give results section give section 
parsing models model general statistical parsing model defines conditional probability candidate parse tree sentence parser algorithm searches tree tb st maximises 
generative model uses observation maximising equivalent maximising tbe xt tls arg estimated attaching probabilities top derivation tree 
pcfg tree derived applications context free re write rules re write rules internal tree lhs non terminal rhs string constant maximising equiv maximising 
top bought jj nn nnp vb np week marks bought nnp brooks top bought bought np week np week jj np marks nnp marks vp bought vb bought np brooks nnp brooks np marks vp bought nn week np brooks lexicalised parse tree list rules contains 
brevity omit pos tag associated word 
non terminals lexical lhs part speech tag rhs word 
pcfg lexicalised associating word part speech pos tag non terminal tree 
write non terminal constituent label 
rule form ln ni ll rl rl rm rm head child phrase inherits head word parent rm left right modifiers may zero unary rules 
shows tree example 
addition lexical heads leads enormous number potential rules making direct estimation rhs lhs infeasible sparse data problems 
decompose generation rhs rule lhs steps generating head making inde assumptions left right mod generated separate th order markov processes 
generate head constituent label phrase probability 

generate modifiers right head probability ii ri ri 
defined symbol added vocabulary non terminals model stops generating right modifiers generated 
find lexical heads penn treebank data rules similar magerman jelinek 
exception top rule tree form top 
exception rule tree probability prop 
generate modifiers left head probability rl li ln ln 
example probability rule bought np week np marks yp bought es yp bought np marks yp bought np week vp bought vp bought vp bought th order markov assumptions li li ll li li li pr ri ri rl ri ri ri general probabilities conditioned preceding modifiers 
fact derivation order fixed depth modifier recursively generates sub tree modifier generated model condition structure preceding modifiers 
moment exploit making approximations li li ll ll li ni li ai ri rl ri ri ri ri functions surface string head word edge constituent see 
distance measure collins vector fol lowing elements string zero length 
allowing model learn preference right branching structures string contain verb 
allowing model learn preference modification verb 
string contain commas 
comma tagged 
distance child ra generated probability 
distance function surface string word word inclusive 
principle model condition struc ture dominated 
model complement adjunct distinction subcategorisation tree example importance complement adjunct distinction 
useful identify marks subject week adjunct temporal modifier distinction tree nps position sisters vp node 
identify complements attaching suffix non terminals fig ure gives example tree 
top bought np ought week marks vbd np brooks bought brooks tree suffix identify complements 
marks brooks subject object position respectively 
week adjunct 
post processing stage add detail parser output give reasons mak ing distinction parsing identifying complements complex warrant prob treatment 
lexical information needed marks closer vp note marks subject marks week bought brooks 
example knowledge week temporal modifier 
knowledge subcat preferences example verb takes exactly subject required 
problems restricted nps compare said sbar asbestos dangerous vs bonds beat short term invest ments sbar market sbar headed complement headed adjunct 
second reason making ment adjunct distinction parsing may help parsing accuracy 
assumption complements generated independently leads incorrect parses see explanation 
identifying complements adjuncts penn treebank add suffix non terminals training data satisfy conditions 
non terminal np sbar parent np sbar vp parent vp parent sbar 

non terminal fol lowing semantic tags adv voc bnf dir ext loc mnr tmp clr prp 
see marcus explanation tags signify 
example np week tmp tempo ral tag sbar sbar market adv tag 
addition child head prepositional phrase marked complement 
probabilities subcategorisation frames model retrained training data enhanced set non terminals learn lexical properties distinguish complements adjuncts marks vs week vs 
suffer bad independence assumptions illustrated 
solve kinds problems gen process extended include probabilistic choice left right subcategorisation frames 
choose head probability 

choose left right subcat frames lc rc probabilities lc 
incorrect correct np vp np np vp dreyfus best fund adjp np 
np adjp low dreyfus best fund low 
incorrect correct np vp np vp issue issue np np vp bill bill funding np funding np congress congress examples assumption modifiers generated independently leads errors 
probability generating dreyfus fund sub jects np dreyfus vp np fund vp unreasonably high 
similar np bill vp funding vp vb np bill vp vb vp funding vp vb bad independence assumption 
prc 
subcat frame multiset specifying complements head requires left right modifiers 

generate left right modifiers prob abilities li li lc ri rc re spectively 
subcat requirements added conditioning context 
ments generated removed appropriate subcat multiset 
importantly probability generating symbol subcat frame non empty probability generating complement subcat frame required complements generated 
probability phrase bought np week np marks vp bought bought np vp bought vp bought np marks vp bought np np week vp bought bought pr vp bought head initially decides take sin gle np subject left complements bag set may contain du non terminal labels 
right 
np marks immediately gener ated required subject np removed lc leaving empty modi np week generated 
incorrect struc tures low probabil ity ic np np vp bought prc np vp vp vb small 
model traces wh movement obstacle extracting predicate argument structure parse trees wh movement 
section describes probabilistic treatment extrac tion relative clauses 
noun phrases extracted subject position object position pps example store sbar trace bought brooks brothers example store sbar marks bought trace example store sbar marks bought brooks brothers tom trace possible write rule patterns identify traces parse tree 
argue task best integrated parser task complex warrant probabilistic treatment integration may help parsing accuracy 
couple complexities modification sbar involve extraction fact sbar np store np store sbar gap store whnp wdt bought gap gap marks bought eek week np np sbar gap whnp gap np vp gap vb sbar gap gap vp gap trace np gap feature added non terminals describe np extraction 
top level np initially generates sbar modifier specifies contain np trace adding gap feature 
gap passed tree discharged trace complement right bought 
played ball bat un common extraction occur con changes sbar said government prepared trace 
second reason integrated treatment traces improve parameterisation model 
particular subcategorisation proba bilities smeared extraction 
examples bought transitive verb knowledge traces example training data contribute probability bought intransitive verb 
formalisms similar gpsg gazdar handle np extraction adding gap feature non terminal tree propagating gaps tree discharged trace complement see 
extraction cases penn treebank annotation indexes trace whnp head sbar straight forward add information trees training data 
lhs rule gap ways gap passed rhs head gap passed head phrase rule 
left right gap passed recursively left right modifiers head discharged trace argument left right head 
rule passed right modifier complement 
rule trace generated right head vb 
specify parameter gip head left right 
generative pro cess extended choose cases generating head phrase 
rest phrase generated different ways depend ing gap propagated head case left right modifiers generated normal 
left right cases gap require ment added left right subcat variable 
requirement fulfilled removed subcat list trace modifier non terminal gap feature gener ated 
example rule sbar gap whnp bought gap probability whnp sbar right sbar whnp lc sbar whnp rc sbar whnp bought gap sbar whnp gap sbar whnp pc sbar whnp rule vp bought gap vb bought trace np week probability vb vp bought pg right vp bought vb plc vp bought vb prc np vp bought vb trace vp bought vb np gap pr np week vp bought vb vp bought vb vp bought vb rule right chosen gap requirement added rc 
generation bought gap prob pr hip 
ri prob prob prob prob pl 

life constituent chart 
means constituent complete includes probabilities means constituent incomplete 
new constituent started projecting complete rule upwards constituent takes left right modifiers unary 
probabilities added complete constituent 
ri prob ri ri 
back ph pa 
pl li level plc lc 
pm ri rti 
rc rc 
lc lc lc pl 
pr rwi 
li iti lc lti lc li lti table conditioning variables level back 
example estimation interpolates el ph 
distance measure 
gap requirements rc 
rule right chosen 
note gen eration trace satisfies np gap subcat requirements 
practical issues smoothing unknown words table shows various levels back type parameter model 
note de compose pl li lti lc iti word pos tag generated non terminal li distance measure product li lti zx lc ili lti lc smooth probabilities separately jason eisner 
case final estimate ale ea ex maximum likelihood esti mates context levels table kl smoothing parameters ki 
words occurring times training data words test data cases levels 
seen training replaced unknown token 
allows model robustly handle statistics rare new words 
part speech tagging parsing part speech tags generated words model 
parsing pos tags word limited seen training data word 
un known words output tagger described ratnaparkhi single possible tag word 
cky style dynamic programming chart parser find maximum probability tree sentence see 
results parser trained sections wall street journal portion penn treebank mar cus approximately sentences tested section sentences 
par measures black compare performance labeled precision number correct constituents proposed parse number constituents proposed parse model magerman collins model model model cbs table results section wsj treebank 
lr lp labeled recall precision 
cbs average number crossing brackets sentence 
cbs cbs percentage sentences crossing brackets respectively 
labeled recall number correct constituents proposed parse number constituents treebank parse crossing brackets number con violate constituent boundaries constituent treebank parse 
constituent correct span set words ignoring punctuation tagged commas colons quotes label constituent treebank parse 
table shows results models 
precision recall traces model cases section treebank criteria met trace correct argu ment correct head word correct position relation head word pre dominated correct non terminal label 
example trace argument bought fol lows dominated vp 
cases string vacuous extraction subject po sition recovered precision recall longer distance cases recovered precision recall comparison previous model similar structure collins major differences score bigram dependency magerman collapses advp prt label comparison removed distinction calculating scores 
exclude infinitival relative clauses fig ures example called trace fix sink indexed trace sub ject infinitival 
algorithm scored precision recall cases section relatives extremely difficult human annotators distinguish purpose clauses case infinitival purpose clause modifying called ann taylor pz li li additional probabilities ing head symbols con 
model advantages may account improved performance 
model collins deficient sentences prob ability mass lost dependency structures violate hard constraint links may cross 
reasons space describe model advantages treatment unary rules distance measure 
generative model condition structure previously generated exploit models collins restricted condi features surface string 
charniak uses lexicalised genera tive model 
notation decomposes hl lm ri lil li 
penn treebank annotation style leads large number context free rules directly estimating 
hl lm may lead sparse data problems problems coverage rule seen training may required test data sentence 
com adjunct distinction traces increase number rules compounding problem 
eisner proposes dependency models gives results show generative model sim ilar model performs best 
pure dependency model omits non terminal infor mation important 
example hope generate vp modifier hope vp sleep require gen erate modifier require jim sleep omitting non terminals conflates cases giving high probability incorrect struc tures hope jim sleep require sleep 
alshawi extends generative depen dency model include additional state variable equivalent having non terminals suggestions may close models fully specify details model doesn give results parsing accuracy 
miller describe model rhs rule generated markov process pro cess head centered 
increase set non terminals adding semantic labels adding lexical head words 
magerman jelinek describe history approach uses decision trees estimate 
models gram estimation methods benefit methods decision tree estima tion condition richer history just surface distance 
interest dependency parsing models speech recog nition example stolcke 
interesting note models lan guage models 
probability sentence estimated tp making viterbi approximation efficiency reasons tb st 
intend perform experi ments compare perplexity various mod els structurally similar pure pcfg 
proposed generative lexicalised probabilistic parsing model 
shown lin fundamental ideas wh movement statistical interpretation 
improves parsing performance importantly adds useful information parser output 
mitch marcus jason eisner dan melamed adwait ratnaparkhi useful discussions comments earlier versions 
benefited greatly suggestions advice scott miller 
alshawi 

head automata bilingual tiling translation minimal representa tions 
proceedings th annual meeting association computational linguistics pages 
black 
procedure tively comparing syntactic coverage en glish grammars 
proceedings february darpa speech natural language workshop 
anonymous reviewers sug experiments 
booth thompson 

applying probability measures languages 
ieee transactions computers pages 
charniak 

parsing context free gram mars word statistics 
technical report cs dept computer science brown univer sity 
chomsky 

syntactic structures mouton hague 
collins 

new statistical parser bigram lexical dependencies 
proceedings th annual meeting association computational linguistics pages 
eisner 

new probabilistic models dependency parsing exploration 
proceed ings coling pages 
gazdar klein sag 

generalized phrase structure grammar 
harvard university press 
jelinek lafferty magerman mercer ratnaparkhi roukos 

decision tree pars ing hidden derivation model 
proceedings human language technology shop pages 


statistical decision tree mod els parsing 
proceedings rd annual meeting association computational linguistics pages 
marcus santorini marcinkiewicz 

building large annotated corpus en glish penn treebank 
computational linguis tics 
marcus kim marcinkiewicz macintyre ferguson katz 

penn treebank ing predicate argument structure 
proceedings human language technology workshop pages 
miller schwartz 

fully statistical approach natural language interfaces 
proceedings th annual meeting association computational linguistics pages 
ratnaparkhi 

maximum entropy model part speech tagging 
conference em methods natural language processing 
stolcke 

linguistic dependency modeling 
proceedings icslp fourth international conference spoken language processing 
