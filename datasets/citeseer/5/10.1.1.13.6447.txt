optimization em expectation conjugate gradient cs toronto edu sam roweis roweis cs toronto edu department computer science university toronto king college rd canada zoubin ghahramani zoubin gatsby ucl ac uk gatsby computational neuroscience unit university college london queen square london wc ar uk show close relationship expectation maximization em algorithm direct optimization algorithms gradient methods parameter learning 
identify analytic conditions em exhibits quasi newton behavior conditions possesses poor rst order convergence 
analysis propose novel algorithms maximum likelihood estimation latent variable models report empirical results showing predicted theory proposed new algorithms substantially outperform standard em terms speed convergence certain cases 

problem maximum likelihood ml parameter estimation latent variable models important problem area machine learning pattern recognition 
ml learning unobserved quantities arises probabilistic models density estimation dimensionality reduction classi cation generally reduces relatively hard optimization problem terms model parameters hidden quantities integrated 
common technique ml estimation model parameters presence latent variables expectation maximization em algorithm 
em algorithm alternates estimating unobserved variables current model re tting model estimated complete data 
takes discrete steps parameter space similar rst order method operating gradient locally reshaped likelihood function 
spite tremendous success em algorithm practice due simplicity fast initial progress authors argued speed em convergence extremely slow complicated second order methods generally favored em 
methods proposed enhance convergence speed em algorithm conventional optimization theory 
authors proposed hybrid approaches ml learning advocating switching newton quasi newton method performing em iterations 
approaches successful terms convergence complex em dicult analyze popular practice 
goal contrast em algorithm direct gradient optimization approach 
concrete alternative gradient ecg algorithm maximum likelihood estimation latent variable models show outperform em terms convergence certain cases 
cases performance em superior 
understand behaviours study convergence properties em algorithm identify analytic conditions em algorithm exhibits quasi newton convergence behavior conditions possesses extremely poor rst order convergence 
analysis introduce simple hybrid algorithm switches em ecg estimated quantities suggested analysis 
report empirical results synthetic real world data sets showing predicted theory simple algorithm performs worse standard em substantially outperform em convergence 

linear newton convergence expectation maximization rst focus analysis convergence properties expectation maximization em algorithm 
consider probabilistic model observed data uses latent variables log likelihood objective function written di erence expected complete log likelihood negative entropy terms ln xj yjx ln xj dy yjx ln yj dy yjx ln yjx dy em algorithm implicitly de nes mapping parameter space 
iterates converge continuous neighbourhood taylor series expansion em essentially linear iteration algorithm convergence rate matrix typically nonzero 
objective functions em step parameter space true gradient related transformation matrix changes iteration rl de ne rl certain conditions transformation matrix guaranteed positive de nite respect gradient 
particular expected complete log likelihood ned di erentiable 
xed single critical point direction located maximum rl second condition may strong 
em algorithm satis ed note directional derivative function direction imply quantity positive mean value theorem critical point direction located point 
identity rl rl 
step single unique solution 
important consequence analysis expected complete log likelihood function unique optimum em appealing quality step having positive projection true gradient objective function 
em similar rst order method operating gradient locally reshaped likelihood function 
maximum likelihood learning mixture gaussians model em algorithm positive definite transformation matrix rst described xu jordan :10.1.1.18.5213:10.1.1.18.5213
extended results deriving explicit form transformation matrix latent variables models factor analysis fa probabilistic principal component analysis ppca mixture mixture fas hidden markov models 
study structure transformation matrix relate convergence rate matrix negative derivatives sides respect rl hessian objective function ij inputoutput derivative matrix em mapping tensor derivative respect regions rl approaches zero nite rst term rhs equation smaller second term transformation matrix rescaled version negative inverse hessian particular em algorithm iterates converge local optima near point su ciently large em may exhibit quasi newton convergence behavior 
true plateau regions gradient small near local optimum 
nature quasi newton behavior controlled eigenvalues matrix 
eigenvalues tend zero em true newton particular holds exponential family model due known convexity property models 
derived general form transformation matrix exponential family models term natural parameters 
gradient em newton gradient em newton 
contour plots likelihood function mog examples separated upper panels separated lower panels dimensional data sets 
axes correspond means 
line shows direction true gradient rl solid line shows direction rl dashed line shows direction rl 
right panels dashed regions left 
numbers indicate log norm gradient 
note separated case vicinity maximum vectors rl rl identical 
method rescaling gradient exactly negative inverse hessian 
eigenvalues tend unity em takes smaller smaller stepsizes giving poor rst order convergence 
dempster laird rubin showed em iterates converge interpreted ratio missing information complete information near local optimum 
neighbourhood solution suciently large formulation em algorithm interesting interpretation applicable latent variable model missing information small compared complete information em exhibits quasi newton behavior enjoys fast typically superlinear convergence neighborhood fraction missing information approaches unity eigenvalues rst term approach zero em exhibit extremely slow convergence 
illustrates results simple case tting mixture gaussians model data em exhibits quasi newton convergence clustered data em slow 
see empirical results sections models show ect 
example hidden markov models aggregate markov models trained structured sequences em exhibits quasi newton behavior particular state transition matrix sparse output distributions deterministic state :10.1.1.11.5851:10.1.1.11.5851
analysis experiments motivates alternative optimization techniques regime missing information high em perform poorly 
section analyze exactly alternative gradient ecg algorithm simple direct optimization method learning parameters latent variables models 

expectation conjugate gradient ecg algorithm key idea ecg algorithm note easily compute derivative ln zj complete log likelihood knowing posterior zjx compute exact gradient rl 
particular rl zjx log zj dz 
exact gradient utilized standard manner example gradient descent control line search technique 
note derive exact em latent variable model derive ecg computing integral hidden variables 
example describe conjugate gradient algorithm expectation conjugate gradient algorithm apply conjugate gradient optimizer performing step value gradient requested line search 
gradient computation step compute posterior zjx loglikelihood normal 
step rl zjx log zj dz certain parameters obey positivity linear constraints modify optimizer respect constraints allow unconstrained optimization 
experiments simple model parameters allow optimizers arbitrary values 
example mog model softmax parameterization mixing coef cients exp exp covariance matrices symmetric positive de nite decomposition log variances diagonal covariance matrices 
hmms probabilities softmax functions 
em line search em ecg steps log likelihood const gene sequence 
em ecg 
left panel illustrates ecg may converge better local optimum 
right panel displays learning curves em ecg algorithms training fully connected state hmm model human dna sequences 
algorithms started initial parameter values converged di erent local optimum 
course choice initial conditions important em algorithm ecg 
em optimizing convex lower bound likelihood em trapped poor basin attraction nd better local optimum 
algorithms split merge em developed escape con gurations 
turns direct optimization methods ecg may avoid problem nonlocal nature line search 
experiments ecg converges better local optimum em gure illustrates exactly case 

hybrid em ecg algorithm seen relative performance em versus direct optimization depends missing information ratio model data set 
key practical speedups ability design hybrid algorithm estimate local missing information ratio detect em direct approach ecg 
authors attacked problem nding top eigenvector approaches conventional numerical methods nite di erence approximations power methods 
approaches computationally intensive dicult implement popular practice 
propose entropy posterior hidden variables computed performing step crude estimate local missing information ratio 
entropy natural interpretation uncertainty missing information serve guide switching regimes em ecg 
models discrete hidden variables quantity quite easy compute 
particular de ne normalized entropy term ln ijx ln ijx discrete hidden variable values observed data vectors xn simply switch em ecg thresholding quantity hybrid em ecg algorithm perform em iterations evaluating step switch ecg perform ecg evaluating line search switch back em exit phase 
abs tol 
tmax near optimum plateau region high entropy see experimental results simple hybrid em ecg algorithm performs worse far better em ecg 
em ecg ecg em em ecg ecg em unstructured sequence 
em ecg ecg em ecg em ecg em structured sequence 
ecg em ecg em 
log likelihood const em ecg em ecg steps 
learning curves ecg em ecg em algorithms showing superior upper panels inferior lower panels performance ecg di erent conditions models mog left hmm middle aggregate markov models right 
number steps taken algorithm shown horizontal axis log likelihood shown vertical axis 
ecg em ecg diamonds indicate maximum line search zero level likelihood corresponds converging point em algorithm 
bottom panels structured data em possesses quasi newton convergence behavior 
models case converge iterations stopping criterion abs upper panels overlapping aliased unstructured data proposed algorithms performs better 

experimental results empirical results comparing performance em ecg hybrid em ecg learning parameters latent variable models mixtures gaussians mog hidden markov models hmm aggregate markov models 
latent variable models performing inference step signi cantly expensive compared parameter updates step line search overhead cg step ecg 
compare performance algorithms simply compare number steps algorithm executes convergence 
rst show results synthetic data sets properties control verify certain aspects theoretical analysis 
report empirical results real world data sets showing algorithms practice 
show examples single runs con rmed convergence results experiments vary signi cantly di erent initial parameter conditions 
reported experiments tol 

synthetic data sets consider mixture gaussians mog model 
considered types data sets data separated distinct clusters separated case data overlaps contiguous region 
shows ecg hybrid em ecg outperform standard em poorly separated cases 
separated case hybrid em ecg algorithm switches ecg due small normalized entropy term em converges quickly 
predicted analysis vicinity local optima directions vectors rl rl identical suggesting em quasi newton convergence behavior 
applied algorithms training hidden markov models hmms 
missing information model high observed data determine underlying state sequence parameters generated data sets state hmm alphabet size characters 
rst data set aliased sequences generated hmm output parameters set uniform values plus small noise 
second data set structured sequences generated hmm sparse transition output matrices 
ambiguous aliased data ecg hybrid em ecg outperform em substantially 
structured data em performs exhibits second em ecg em ecg component mog em ecg ecg em component mog em ecg ecg em component mog dna sequence 
ecg em ecg em state hmm ecg em ecg em dna sequence 
state hmm dna sequence 
em ecg ecg em state hmm em ecg ecg em class amm em ecg em ecg class amm em ecg ecg em steps class amm 
learning curves ecg em ecg em algorithms displaying convergence performance di erent conditions models mog upper hmm middle aggregate markov models bottom 
number steps taken algorithm shown horizontal axis log likelihood shown vertical axis 
ecg em ecg diamonds indicate maximum line search zero level likelihood corresponds converging point em algorithm 
number learned clusters mog model left middle right 
hmm model number states left middle right 
number learned themes amm model left middle right 
order convergence vicinity local optimum 
experimented aggregate markov models :10.1.1.11.5851:10.1.1.11.5851
model de ne discrete conditional probability table ij jjx low rank approximation 
context gram models word sequences classbased bigram models mapping words classes probabilistic 
particular class bigram model predicts word followed word probability jw jc cjw total number classes 
concept missing information corresponds poor set words determine class labels observation words follow 
right panels gure show training class state amm model ambiguous aliased data words determine class labels structured data proportion missing information small 
ecg hybrid em ecg superior em factor ambiguous data structured data em shows expected quasi newton convergence behavior 

real world data sets rst experiment cluster set greyscale pixel image patches mixture gaussians model 
patches extracted natural images described see example natural image sample patches 
speed experiments patch data projected pca dimensional linear sub data set data set publicly available ftp phys rug nl pub samples space mixing proportions covariances model held xed 
means initialized performing means 
experimented mixtures having clusters 

example natural image samples grey pixel image patches clustering experiment 
displays convergence em ecg hybrid em ec algorithms 
experimental results show fewer mixture components em outperforms ecg components generally model data fairly distinct non contiguous clusters 
number mixtures components increases clusters overlap contiguous regions normalized entropy term grows suggesting relatively high proportion missing information 
case ecg outperform em orders magnitude 
hybrid em ecg algorithm inferior em ecg untuned setting switching threshold 
second experiment consisted training fully connected hmm model dna sequences 
training publicly available genie gene nding data set provided ucsc lbnl contains unrelated human genomic dna sequences 
applied di erent algorithms dna sequences length varying genes sequence 
number states ranged parameter values randomly initialized 
shows convergence em ecg hybrid em ecg algorithms 
data set contains complex structure easily modeled hmms resulting high proportion missing information 
result hybrid em ecg ecg substantially outperform em terms convergence 
experiment applied aggregate markov models data set consisting nips authors corresponding counts top frequently words nips conference proceedings volumes 
goal model probability author word nips corpus experiments publicly available www cs toronto edu roweis data html ing small number soft classes ja jt 
observe simple model data set large fraction missing information 
displays convergence em ecg em ecg algorithms 
hybrid em ecg ecg having superior convergence em 

discussion focused discrete latent variables ecg hybrid algorithms derived latent variable models continuous hidden variables 
example gure illustrates convergence behaviour probabilistic principal component analysis ppca latent variable model continuous discrete hidden variables 
concept missing information related ratios leading eigenvalues sample covariance corresponds ellipticity distribution 
low rank data large ratio em performs nearly circular data ecg converges faster 
degenerate cases proportion missing information high approaches identity em convergence exponentially slow 
illustrates example case hmm training random sequences 
takes iterations ecg em ecg converge ml estimate iterations em approaching local optimum 
comparative analysis em direct optimization algorithms latent variable models developed theoretical connection approaches 
analyzed determined conditions em algorithm demonstrate local gradient convergence behaviors 
results extend xu jordan analyzed convergence properties em algorithm special case gaussian mixtures apply exponential family model :10.1.1.18.5213:10.1.1.18.5213
motivated analyses proposed alternative hybrid optimization method significantly outperform em cases inferior 
tested proposed algorithms slow convergence em ppca true factor analysis especially linear dynamic systems 
models large amount missing information due fact latent variables continuous rotated ecting likelihood long parameters rotated accordingly 
probabilistic pca ecg em probabilistic pca ecg em em ecg ecg steps em state hmm 
learning curves ecg dots em solid lines algorithms showing superior left inferior middle performance ecg 
left panel uses ill conditioned data ecg converges quickly middle panel uses low rank data em performs better 
right panel displays non converging case em 
unstructured data sequences length generated full state hmm alphabet size 
parameter values set uniform plus small uniform noise 
ecg em ecg converge iterations iterations em approaching ml estimate 
training basic latent variable models synthetic real world data sets reporting convergence behavior explaining results analysis 
convergence analysis extended broader class bound optimization techniques iterative scaling algorithms parameter estimation maximum entropy models cccp algorithm minimizing bethe free energy approximate inference problems 
analyses allow gain deeper understanding nature algorithms conditions certain optimization techniques expected outperform 
extended analyses designing accelerated tting algorithms models 
acknowledgments bengio drew useful comments carl rasmussen providing initial version conjugate gradient code 
atkinson 
performance standard hybrid em algorithms ml estimates normal mixture model censoring 
stat 
computation simulation 
stephen della pietra vincent della pietra john la erty 
inducing features random elds 
ieee transactions pattern analysis machine intelligence 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
rs society series 
chris fraley 
computing largest fraction missing information em algorithm worst linear function data augmentation 
technical report university washington 
genie gene data set 
lbnl uc santa cruz www org sequence 
robert 
acceleration em algorithm quasi newton methods 
rs society series 
meng van fast em type implementations mixed ects models 
royal statistical society series 
richard redner homer walker 
mixture densities maximum likelihood em algorithm 
siam review april 
roweis 
em pca spca 
advances neural information processing systems volume pages cambridge ma 

relationship gradient em steps latent variable models 
www cs toronto edu ecg 
lawrence saul fernando pereira :10.1.1.11.5851:10.1.1.11.5851
aggregate mixed order markov models statistical language processing 
proceedings second conference empirical methods natural language processing pages 

tipping bishop 
mixtures probabilistic principal component analysers 
neural computation 
ueda nakano zoubin geo rey hinton 
algorithm mixture models 
neural computation 
van hateren van der schaaf 
independent component lters natural images compared simple cells primary visual cortex 
proceedings royal society london pages 
xu jordan :10.1.1.18.5213:10.1.1.18.5213
convergence properties em algorithm gaussian mixtures 
neural computation 
alan yuille anand rangarajan 
computational procedure cccp 
advances nips volume 
