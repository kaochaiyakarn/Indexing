appear usenix annual technical conference june 
cohort scheduling enhance server performance james larus michael parkes larus microsoft 
com microsoft research microsoft way redmond wa server application commonly organized collection concurrent threads executes code necessary process request 
software architecture causes frequent control transfers unrelated pieces code decreases instruction data locality consequently reduces effec hardware mechanisms caches tlbs branch predictors 
numerous measurements demonstrate effect server applications utilize fraction modern processor computational throughput 
addresses problem cohort scheduling new policy increases code data locality batching execution similar operations arising different server requests 
effective implementation policy relies new programming abstraction staged computation replaces threads 
stagedserver library provides efficient implementation cohort scheduling staged computation 
measurements server applications written library show cohort scheduling improve server throughput reducing processor cycles instruction cache misses 
server application program manages ac cess shared resource database mail store file system web site 
server receives stream requests processes produces stream results 
server performance important determines latency access resource constrains server ability handle multiple requests 
commercial servers databases focus considerable research improve underlying hardware algorithms parallelism considerable development improve code 
hardware effort concentrated memory hierarchy rapidly increasing processor speed parallelism slowly declining memory access time created growing gap major performance bottleneck systems 
processors loading word memory cost hundreds cycles times instructions execute 
high performance processors attempt alleviate performance mismatch numerous mechanisms caches tlbs branch predictors 
mechanisms exploit known program property spatial temporal reuse code data keep hand data reused quickly predict program behavior 
server software exhibits program locality consequently achieves poorer performance software 
example studies commercial database systems running line transaction processing oltp benchmarks incur high rates cache misses instruction stalls re duce processor performance low tenth peak potential 
part problem may attributable database systems code size execution model responsible 
systems structured process thread runs short period invoking blocking operation relinquishing control processors execute succession diverse non looping code segments exhibit little locality 
example compared tpc oltp benchmark threads execute average instructions blocking tpc compute intensive system dss benchmark threads execute average instructions blocking 
alphaserver tpc rate rate performance cycles instruction cpi 
con trast tpc rate rate cpi 
focusing hardware takes alternative complementary approach modifying program behavior improve performance 
presents new user level soft ware architecture enhances instruction data locality increases server software performance 
architecture consists scheduling policy programming model 
policy cohort scheduling con executes cohort similar computations arise distinct requests server 
computations cohort roughly stage processing tend similar code data consecutively executing improves program locality increases hardware performance 
staged computation programming model provides programming abstraction programmer identify group related computations explicit dependences constrain scheduling 
staged computation additional benefits reducing concurrency overhead need expensive error prone synchronization 
implemented scheduling policy programming model reusable library stagedserver 
experiments intensive server compute bound server code stagedserver performed significantly better threaded versions 
stagedserver lowered response time reduced cycles instruction reduced cache misses 
organized follows 
section introduces cohort scheduling explains improve program performance 
section describes staged computation 
section briefly describes stagedserver library 
section contains performance meas 
section discusses related 
computation time threaded execution processor execution time 
cohort scheduling operation 
shaded boxes indicate different computations performed processing requests server 
cohort scheduling reorders computations similar ones execute consecutively processor increases program locality processor performance 
cohort scheduling cohort scheduling technique organizing computation server applications improve program locality 
key insight distinct requests server execute similar computations 
server defer processing request cohort computations arrive similar point processing execute cohort consecutively processor 
scheduling policy increases opportunities code data reuse reducing interleaving unrelated computations causes cache conflicts evicts live cache lines 
approach similar loop tiling blocking restructures matrix computation submatrix computations repeatedly data turning submatrix 
cohort scheduling dynamic process reorganizes series computations items input stream similar computations different items execute consecutively 
technique applies uniprocessors multiprocessors depend program locality achieve performance 
illustrates results simple experiment demonstrates benefit cohort scheduling uniprocessor 
reports cost call executing different sized cohorts asynchronous writes random blocks file 
cohort ran consecutively system cache branch table buffer flushed 
cohort increased size cost call decreased rapidly 
single call consumed cycles average cost dropped cohort calls cohort calls 
direct measure locality cache misses improved dramatically 
cohort calls misses call dropped initial value declined cohort calls 
im required changes operating system code reordering operations application 
improvement requires reductions os misses roughly system call amortizing roughly cold start misses 
assembling cohorts cohort scheduling tied staged computation benefits may lost programmer explicitly form cohorts 
example consider transparently integrating cohort scheduling threads 
basic idea simple 
modified thread scheduler identifies groups threads identical program counter npc values 
threads starting point execute similar operations behavior eventually diverges 
scheduler runs cohort threads identical turning cohort 
vs cohort size cycles misses consecutive calls cohort boo boo 
performance cohorts system calls window advanced server dell precision intel pentium iii processor 
chart reports cost call processor cycles cache misses asynchronous write random block file 
easy believe scheme improve performance requires minor changes scheduler changes applications 
clear shortcomings 
particular npc values coarse indirect indicator program behavior 
threads identical cohort misses pieces code similar behavior 
example routines access data structure belong cohort 
simple extensions scheme distance pcs measure similarity little connection logical behavior perturbed compiler linking code scheduling 
disadvantage cohorts start blocking system calls application appropriate points 
particular applications programs asynchronous scheme block 
correct shortcomings properly assemble cohort programmer delimit computations identify ones belong cohort 
staged computation provides programming abstraction neatly captures dimensions cohorts 
staged computation staged computation programming abstraction intended replace threads construct underlying concurrent parallel programs 
stages offer compelling performance correctness advantages particularly amenable cohort scheduling 
model program constructed collection stages consists group exported operations private data 
operation asynchronous procedure call invocation execution reply decoupled 
stage scheduling autonomy enables control order concurrency operations execute 
stage conceptually similar class object language extent program structuring abstraction providing local state operations 
stages differ objects major respects 
operations stage invoked asynchronously caller wait computation complete continues necessary retrieve result 
second stage autonomy control execution operations 
autonomy extends deciding execute computations associated invoked operations 
stages control abstraction organize process objects data representation acted entities functions threads stages 
stage facilitates cohort scheduling provides natural abstraction grouping operations similar behavior locality control autonomy implement cohort scheduling 
operations stage typically access local data effective cohort scheduling requires simple scheduler accumulates pending operations form cohort 
stages provide additional programming advantages 
control internal concurrency promote programming style reduces need expensive error prone explicit synchronization 
stages provide basis specifying verifying properties asynchronous programs 
section briefly describes staged programming model 
section elaborates implementation class library 
stage design programmers group operations stage variety reasons 
regulate access program state static data wrapping data type 
operations grouped way form obvious cohort typically considerable instruction data locality 
programmer control concurrency stage reduce eliminate synchronization data section 
second reason group logically related operations provide rounded complete programming abstraction 
reason may compelling logically related operations frequently share code data collecting stage identifies operations benefit cohort scheduling 
third encapsulate program control logic form finite state automaton 
discussed low stage asynchronous operations easily imple ment reactive transitions event driven state machine 
stage stage invoke op 
example stages operations 
stage runs op invokes operations stage waking complete running op continuation 
practice designing program stages focuses partitioning tasks sub tasks self contained considerable code data local ity logical unity 
ways process control analogue object oriented design 
operations operations asynchronous computations exported stage 
invocation operation requires eventual execution invoker operation run independently 
operation executes invoke number child operations stage including 
parent wait children finish retrieve results computation continue processing 
shows operation op running stage invokes operations op op stage performs computation waits children 
complete return results op continues execution processes children results 
code operation executes sequentially invoke conventional synchronous calls asynchronous operations 
started operation non preemptible runs relinquishes processor 
programmers unfortunately careful invoke blocking operations suspend thread running operations processor 
operation relinquishes processor wait event asynchronous synchronization operation completion provides continua tion invoked event occurs 
continuation consisting function saved state permit computation resume point suspended 
explicit continuations simplest approach operation saves live state structure called closure 
alternative implicit continuations requires system save executing operation stack resumed 
scheme similar fibers simplifies programming performance cost 
asynchronous operations provide low cost parallelism enables programmer express exploit concurrency application 
overhead time space invoking operation close procedure call entails allocating initializing closure passing stage 
operation runs completion require stack area preserve processor state eliminates cost threads 
similarly returning value re enabling continuation simple inexpensive operations 
programming styles staged computation supports variety programming styles including software pipelining eventdriven state machines bi directional pipelines fork join parallelism 
conceptually stages server arranged pipeline requests arrive responses flow 
form computation easily supported representing request object passed stages 
linear pipelining sort simple efficient stage retains information completed computa tions 
stages constrained linear style 
common programming idiom bidirectional pipelining asynchronous analogue call return 
approach stage passes subtasks stages 
parent stage eventually suspends request turns attention requests resumes original computation subtasks produce results 
style requires operation broken series subcomputations run results appear 
explicit continuations programmer partitions computation hand compiler easily produce code close wellknown continuation passing style 
implicit continuations programmer needs indicate original computation suspends waits subtasks complete 
generalization style event driven programming uses finite state automaton fsa control reactive system :10.1.1.130.8002:10.1.1.23.5560
fsa logic encapsulated stage driven external events network messages completions internal events asynchronous stages 
operation closure contains fsa state particular server request 
fsa changes state child operation completes external events arrive 
transitions invoke computations associated edges fsa 
computation runs blocks specifies state fsa 
disk profile staged web server section 
performance metrics stage broken processor system running processors 
column average queue length 
second column contains metrics operations stage quantity average wait time millisecond maximum wait time 
third column contains corresponding metrics operations suspended restarted 
fourth column contains corresponding metrics completed operations 
numbers arcs number operations started restarted stage processor pairs 
example web server section driven control logic stage consisting fsa fifteen states 
fsa describes process get request arrives parsed refer file cache disk file blocks read transmitted file con closed 
describing control logic server fsa opens possibility verifying properties entire system deadlock freedom applying techniques model checking developed model verify systems communicating fsas 
scheduling policy refinements stage mutex amortized cohort op erations 
partitioned stage divides invocations key passed parameter avoid sharing data operations running different processors 
example consider file cache stage partitions requests hash function file number 
processor maintains hash table memory disk blocks 
hash table accessed processor enhances locality eliminates synchronization 
policy reminiscent shared databases permits parallel data structures fine grain synchronization 
third attribute stage scheduling autonomy 
stage activated processor stage determines operations execute order 
scheduling freedom allows refinements cohort scheduling reduce need synchronization 
particular policies useful exclusive stage executes operations time 
operations run sequentially completely access stage local data need synchronization 
type stage similar monitor interface asynchronous clients delegate computation stage block obtain access resource 
strict serialization cause performance bottleneck policy offers fast access data simple programming model 
approach works fine grained operations cost acquiring releasing shared stage runs operations concurrently processors 
operations stage execute concurrently shared data accesses synchronized 
policies possible easily implemented stage 
important keep mind policies implemented general framework cohort scheduling 
stage activated processor executes outstanding operations 
staged model requires cohort scheduling 
programming model scheduling policy naturally fit 
stage groups logically related operations share data provides freedom reorder computations 
cohort scheduling exploits scheduling freedom consecutively running similar operations 
understanding performance compelling advantage staged model performance system relatively easy visualize understand 
stage similar node queuing system 
parameters average maximum queue length average maximum wait time average maximum processing time easily measured displayed 
measurements provide overview system performance help identify bottlenecks 
stage computation example example staged computation consider file cache web server section 
file cache important component servers 
stores accessed disk blocks memory maps file identifier offset disk block 
staged file cache consists partitioned stages 
cache logically partitioned processors manages unique subset files determined hashed file identifier 
alternatively large files file identifier offset hashed file disk blocks stripped table 
stage processor maintains hash table maps file identifiers memory resident disk blocks 
processor table accesses require synchronization data migrate processor caches 
disk block cached memory cache invokes operation aggregator stage role merge requests adjacent disk blocks improve system efficiency 
stage utilizes cohort scheduling different way accumulating requests cohort combining larger request operating system 
disk stage reads writes disk blocks 
issues asynchronous system calls perform operations invokes operation event server stage describing pending operation suspends completes 
stage interfaces operating system asynchronous notification mechanism staged programming model 
utilizes separate thread waits completion port system uses signal completion asynchronous notification stage matches event waiting closure passes information completion port 
disk stage turn returns disk blocks aggregator passes stage data recorded passed back client 

architecture staged file cache 
requests disk blocks partitioned processors avoid sharing hash table 
block requested aggregator combines requests adjacent blocks passes disk stage asynchronously reads files 
completes event server thread notified passes completion back disk stage 
stagedserver library stagedserver library collection classes implement staged computation cohort scheduling uniprocessor multiprocessor 
library enables programmer define stages operations policies writing applicationspecific code 
stagedserver implements aggressive efficient version cohort scheduling 
section briefly describes library primary interfaces 
library functionality partitioned principal classes 
stage class provides stage local storage mechanisms collecting scheduling operations 
second closure class encapsulates operation continuations provides invocation state supports invoking operation returning result 
fundamental action stagedserver system invoke operation creating initializing closure handing stage 
stage class stage class base class application uses derive classes various stages 
base class provides basic functionality managing closures scheduling executing operations processors 
scheduling policy stagedserver implements cohort scheduling policy enhancements increase processor affinity data 
assignment operations processors occurs operation submitted stage 
default operation invoked code running processor executes processor subsequent stages 
affinity policy enhances temporal locality reduces cache traffic operation data tend remain processor cache 
program override default execute operation different processor processor execute operation explicitly specified stage partitions operations processors stage uses lead balancing redistribute operations 
stage maintains stack queue processor system 
general operations originating local processor pushed stack operations processors enqueued queue 
stage starts processing cohort empties stack life order turning queue 
scheme rationales 
processing invoked operations increases likelihood operation data reside cache 
addition stack require synchronization accessed processor reduces common case cost invoking opera tion 
processor scheduling stagedserver currently uses simple wavefront algorithm supply processors stages 
programmer specifies ordering stages application 
wavefront scheduling processors independently alter nate forward backward traversals list stages 
stage processor executes operations pending stack queue 
operations finished processor proceeds stage 
processor repeatedly finds sleeps exponentially increasing periods time interval 
processor gain access exclusive stage processor working stage processor skips stage 
alternating traversal order wavefront scheduling corresponds common communications pattern stage passes requests successors perform computation produce result 
easy imagine scheduling policies evaluated approach works applications studied 
topic worth investigation 
thresholds orthogonal attribute stage pair thresholds force stagedserver activate stage number operations waiting fixed interval 
situation arises stagedserver stops currently running stage completes operation runs threshold exceeding stage returns suspended stage 
simplicity interrupting stage interrupted stages exceed thresholds deferred processing returns original stage 
thresholds particularly useful latency sensitive stages interacting operating system regularly supplied requests ensure devices go idle 
useful refinement feedback mecha nism stage informs stages sufficient tasks 
stages suspend processing effectively turning processor stage 
far voluntary cooperation hard queue limits sufficed 
partitioned data partitioned stage typically divides data operations running processor access non shared portion 
avoiding sharing eliminates need synchronize access data reduces cache traffic results data accessed processor 
current system partitions variable known technique privatization storing values vector entry processor 
code uses processor id index vector obtain private value 
closure class closure base class defining clo sures combination code data 
stagedserver uses closures implement operations continuations 
operation invoked stage invoker creates closure initializes parameter values 
stage executes operation invoking closure methods specified operation invocation 
method ordinary method 
returns method state operation complete optionally returns value waiting child finish waiting operation resume execution 
operation invoke operations stages children 
original operation waits children providing continuation routine system runs children finish 
continuation routine simply method original closure 
closure passes arguments parent continuation results child parent 
process may repeat multiple times continuation role parent 
words closures multiple entry closures entry original operation invocation entries subsequent continuations 
practice stage treats methods identically distinguish operation continuation 
experimental evaluation evaluate benefits cohort scheduling stagedserver library built prototypical server applications 
web server task consists responding get requests retrieving files disk file cache sending network 
second server compute bound amount data transferred relatively small computation match event database subscriptions expensive memory intensive 
intensive server compare threads stages implemented web servers 
structured thread pool second uses stagedserver 
took care servers efficient comparable share common code 
particular servers microsoft window asynchronous operations 
threaded server organized conventional manner thread accepting connections passing pool worker threads performs server full functionality parsing request reading file transmitting con tents 
server kernel file cache 
server process simultaneously requests 
organized control logic stage network stage disk caching stages described section 
parameters chosen experimentation yielded robust performance benchmark hardware configuration 
baseline comparison ran experiments microsoft iis web server highly tuned commercial product 
iis performed better servers difference small partially validates implementations 
test configuration consisted server clients 
server compaq dl containing mhz pentium iii xeon processors mb cache gb ram 
rpm scsi disks connected compaq smart array controller 
clients ran dell containing mhz pentium ii xeon processors gb ram 
clients server connected dedicated gigabit ethernet network ran windows server sp 
surge benchmark retrieves web pages size distributions pattern modeled actual systems 
surge measures ability web server process get requests retrieve pages disk send back client 
benchmark attempt capture full behavior web server handle types requests execute dynamic content perform server management log data 
increase load run large configuration web site pages gb stream containing requests 
surge workload characterized user equivalents ues models user accessing web site 
run ues client 
tests run ue workload balanced client machines 
reported numbers minutes client execution starting freshly ini server 
shows bandwidth latency thread stagedserver servers compares commercial web server iis 
chart contains number pages retrieved clients second requests follow fixed sequence number pages measure bandwidth second chart contains average latency perceived client retrieve page 
trends notable 
light load performance approximately lower load increases responds requests unit time 
second chart part explains difference 
latency higher latency light load factor load increases latency grows times latency increases times level equal 
commercial server microsoft iis outperformed 
latency heavy load better servers latency 
get bandwidth ils ooo es get latency ils 
es 
performance web servers 
charts show performance threaded server stagedserver server microsoft ils server ils 
records number web pages received clients second 
second records average latency perceived client retrieve page 
error bars standard deviation latency 
performance stable predictable heavy load threaded server appropriate servers performance challenges arise offered load increases 
server performance relatively better processor performance degraded load server 
improved processor performance reflected measurably improved throughput load 
compute bound server evaluate performance stagedserver compute bound application built simple pub subscribe server 
server efficient cache friendly algorithm match events database subscriptions 
subscription conjunction terms comparing variables integer 
event set assignments values variables 
event matches subscription terms satisfied value assignments event 
threaded stagedserver version application shared common cpi es kernel cache misses kernel user user ues 
processor performance servers 
charts show processor performance threaded stagedserver web server 
chart shows cycles instruction cpi second shows rate cache asses 
publish subscribe implementation difference threads stages structure computation 
benchmark fabret workload subscriptions events 
platform 
response time stagedserver version events better load 
clients publishing events responded average ms request 
clients responded average time ms response improved ms clients improvement threaded version 
large measure improvement due improved processor usage 
clients averaged cycles instruction cpi entire benchmark averaged cpi reduction 
compute intensive event matching portion averaged cpi averaged cpi reduction 
large measure improvement attributable greater reduction caches misses user space cache requests 
average response time threaded stagedserver processor performance clients threaded interval performance publish subscribe server 
top chart records average response time match publish events subscriptions 
bottom chart compares average cycles instruction cpi thread stagedserver versions second intervals 
initial part curve construction internal data structures flat part curves event matching 
application large data structure approximately mb benchmark 
matching event subscriptions fabret algorithm cache efficient may access large amount data particular locations data dependent 
stagedserver performance advantage results factors 
code organized processor subset subscriptions reduces number distinct locations processor increases possibility data reuse 
locality optimization runs speed 
second stagedserver batches cohorts event matches data structure 
measured benefit cohort scheduling limiting cohort size 
cohorts items reduced performance items reduced performance items reduced performance 
optimizations beneficial threaded code structure resulting server isomorphic stagedserver version thread bound processor performing event lookups subset data structure queue front process accumulate cohort 
related advantages disadvantages threads processes widely known 
papers investigated alternative server architec tures 
chankhunthod described harvest web cache uses event driven reactive architecture invoke computation transitions state machine control logic 
system stagedserver uses non blocking careful avoidance page faults non blocking non preemptive scheduling policy 
pai proposed fold characterization server architectures multi process multi threaded event driven asymmetric multi process event driven 
alternatives orthogonal task scheduling policy discussion section illustrates cohort scheduling increase locality 
pai favored event driven programming style offers opportunities cohort scheduling event handlers partition computation distinct easily identifiable subcomputations clear operation boundaries 
hand ad hoc event systems offer obvious way group handlers belong cohort associate data operations 
section describes staged computation programming model provides programmer control computation cohort 
welsh described seda system similar staged computation model 
seda stagedserver explicit cohort scheduling uses stages architecture structuring event driven servers 
performance results similar intensive server applications 
blackwell blocked layer processing improve instruction locality tcp ip stack 
noted tcp ip code larger mips instruction cache protocol stack processed packet completely code lower protocol layers remained cache packet 
solution process packets layer 
modified stack lower cache rate reduced processing latency 
blackwell related approach blocked matrix computations focus instruction locality 
cohort scheduling genesis predates blackwell general scheduling policy system architecture applicable computation cleanly network stack 
cohort scheduling improves data just instruction locality reduces synchronization 
stage similar respects object object language provides local state operations manipulate 
differ objects part passive methods synchronous asynchronous object models exist 
object oriented languages java integrate threads synchronization active entity remains thread synchronously run method passive object 
contrast staged computation stage asked perform operation autonomy decide execute 
decoupling request response valuable enables stage control concurrency adopt efficient scheduling policy cohort scheduling 
stages similar respects agha actors 
start model asynchronous com munication autonomous entities 
actors internal concurrency give entities control scheduling presume reactive model actor responds message invoking computation 
stages internal concurrency scheduling autonomy better suited cohort scheduling 
actors turn instance dataflow general computing model 
stages viewed instance dataflow computation 
cilk language provably efficient scheduling policy 
language thread object shares characteristics stages 
started computation preempted 
running computation spawn tasks return results invoking continuation 
cilk stealing scheduling policy implement cohort scheduling program control 
improved data locality stealing scheduling algorithms 
jaws object oriented framework writing web servers 
consists collection design patterns construct servers adapted particular operating system selecting appropriate concurrency mechanism processes threads creating thread pool reducing synchronization caching files scatter gather employing various tcp specific optimizations 
stagedserver simpler library provides programming model directly enhances program locality performance 
earlier version published short extended 
servers commonly structured collection parallel tasks executes code necessary process request 
threads processes event handlers underlie software architecture servers 
unfortunately software architecture interact poorly modern processors performance depends mechanisms caches tlbs branch predictors exploit program locality bridge increasing processor memory performance gap 
servers little inherent locality 
thread typically runs short unpredictable amount time followed unrelated thread working set 
servers interact frequently operating system large disruptive working set 
poor processor performance servers natural consequence threaded architecture 
remedy propose cohort scheduling increases server locality consecutively executing related operations different server requests 
running similar code processor increases instruction data locality aids hardware mechanisms cache branch predictors 
architecture naturally issues operating system requests batches reduces system disruption 
describes staged computation programming model supports cohort scheduling providing abstraction grouping related operations mechanisms program implement cohort scheduling 
approach implemented stagedserver library 
series tests web server publish subscribe server stagedserver code performed better threaded code lower level cache misses instruction stalls better performance heavy load 
going long time countless people provided invaluable insights feedback 
list incomplete apologize omissions 
rick important contributions idea cohort scheduling early implementations 
jim gray supporter advocate 
kevin helped write web server run early experiments 
chilimbi jim gray vinod grover mark hill murali krishnan paul larson milo martin ron murray luke mcdowell scott mcfarling simon peyton jones mike smith ben zorn provided helpful questions comments 
referees shepherd carla ellis provided helpful comments 
guy blelloch robert blumofe data locality stealing proceedings twelfth acm symposium parallel algorithms architectures spaa 
bar harbor july 
atul adya jon howell marvin theimer william bolosky john douceur cooperative tasking manual stack management proceedings usenix annual technical conference 
monterey ca june 
gul agha actors model concurrent computation distributed systems 
cambridge ma mit press 
ailamaki david dewitt mark hill david wood dbmss modern processor time go proceedings th international conference large data bases 
edinburgh scotland morgan kaufmann september pp 

thomas anderson performance implications thread management alternatives shared memory multiprocessors ieee transactions parallel distributed systems vol 
num 
pp 

andrew appel compiling continuations 
cambridge university press 
gaurav banga peter druschel jeffrey mogul better operating system features faster network servers proceedings workshop internet server performance 
madison wi june 
paul barford mark crovella generating representative web workloads network server performance evaluation proceedings acm sigmetrics joint international conference measurement modeling computer systems 
madison wi june pp 

luiz andr kourosh gharachorloo edouard bugnion memory system characterization commercial workloads proceedings th annual international symposium computer architecture 
barcelona spain june pp 

trevor blackwell speeding protocols small messages proceedings acm sigcomm conference applications technologies architectures protocols computer communication 
palo alto ca august pp 

robert blumofe christopher joerg bradley charles leiserson keith randall zhou cilk efficient multithreaded runtime system journal parallel distributed computing vol 
num 
pp 

satish chandra bradley richards james larus teapot domain specific language writing cache coherence protocols ieee transactions software engineering vol 
num 
pp 

chankhunthod peter danzig chuck neerdaels michael schwartz kurt worrell hierarchical internet object cache proceedings usenix annual technical conference 
san diego ca january 
richard draves brian bershad richard rashid randall dean continuations implement thread communication operating systems proceedings thirteenth acm symposium operating system principles 
pacific grove ca october pp 

edmund clarke jr orna grumberg doron peled model checking 
cambridge ma mit press 
fran fabret arno jacobsen fran ois llirbat joao pereira kenneth ross dennis shasha filtering algorithms implementation fast publish subscribe systems proceedings acm sigmod international conference management data symposium principles database systems 
santa barbara ca may pp 

james gosling bill joy guy steele java language specification addison wesley 
james hu douglas schmidt jaws framework high performance web servers domain specific application frameworks frameworks experience industry johnson eds john wiley sons october 
irigoin supernode partitioning proceedings fifteenth annual acm symposium principles programming languages 
san diego ca january pp 

kimberly keeton david patterson yong qiang roger raphael walter alter performance characterization quad pentium pro smp oltp workloads proceedings th annual international symposium computer architecture 
barcelona spain june pp 

james larus michael parkes cohort scheduling enhance server performance extended proceedings workshop optimization middleware distributed systems 
snowbird ut june pp 

james larus sriram rajamani jakob rehof behavioral types structured asynchronous programming microsoft research redmond wa may 
edward lee thomas parks dataflow process networks proceedings ieee vol 
num 
pp 

walid edward lee guang gao advances dataflow computation model parallel computing vol 

akinori yonezawa executing parallel programs synchronization bottlenecks efficiently proceedings international workshop parallel distributed computing symbolic irregular applications 
sendai japan world scientific july pp 

vivek pai peter druschel willy zwaenepoel flash efficient portable web server proceedings usenix annual technical conference 
monterey ca june pp 

david patterson john hennessy computer architecture quantitative approach ed 
morgan kaufmann 
sharon perl richard sites studies windows nt performance dynamic execution traces proceedings second usenix symposium operating systems design implementation osdi 
seattle wa october pp 

matt welsh david culler eric brewer seda architecture conditioned scalable internet services proceedings th acm symposium operating systems principles sosp 
alberta canada october pp 

michael wolfe high performance compilers parallel computing 
addison wesley 

