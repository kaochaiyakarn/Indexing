fast distributed algorithm mining association rules david jiawei hans vincent ada department computer science university hang kang hang kang 
email cs hku hk 
school computing science simon fraser university canada 
email han cs sfu ca 
tt department computing hong kong polytechnic university hong kong 
email comp edu hk 
department computer science engineering chinese university hong kong hong kong 
email cs cuhk hk 
existence large transaction databases huge amounts data high scalability distributed systems easy partition distribution centralized database important investigate efficient methods distributed mining association rules 
study discloses interesting relationships locally large globally large itemsets proposes interesting dis tributed association rule mining algorithm fdm fast distributed mining association rules gener ates small number candidate sets substantially reduces number messages passed mining association rules 
performance study shows fdm superior performance direct application typical sequential algorithm 
performance enhancement leads variations algorithm 
association rule rule implies certain association relationships set objects occur implies database 
finding interesting association rules databases may disclose useful patterns decision support selective marketing financial forecast medical diagnosis applications attracted lot attention data min ing research 
mining association rules may require iterative scanning large transaction relational databases quite costly processing 
efficient mining association rules transaction relational databases studied substantially :10.1.1.103.5437:10.1.1.50.1686:10.1.1.144.4956:10.1.1.40.6757:10.1.1.40.8600
research author supported part hong kong research council 
research second author supported part research nserc natural sciences engineering research council canada re search nce iris hmi networks centres excellence canada research hughes research laboratories 
previous studies examined efficient mining association rules different angles 
influential association rule mining algorithm apriori developed rule mining large transaction databases 
dhp algorithm extension apriori hashing technique :10.1.1.50.1686
scope study extended efficient mining sequential patterns generalized association rules multiple level association rules quantitative association rules maintenance discovered association rules incremental updating studied :10.1.1.144.4956:10.1.1.40.8600
studies sequential data mining techniques algorithms parallel mining association rules proposed 
feel development distributed algorithms efficient mining association rules unique importance reasoning 
databases data warehouses may store huge amount data 
mining association rules databases may require substantial processing power distributed system possible solution 
large databases distributed nature 
example huge number transaction records hundreds sears department stores stored different sites 
observation motivates study efficient distributed algorithms mining association rules databases 
study may shed new light parallel data mining 
furthermore distributed mining algorithm mine association rules single large database partitioning database set sites processing task distributed manner 
high flexibility scalability low cost performance ratio easy connectivity distributed system ideal platform mining association rules 
study assume database studied transaction database method easily extended relational databases 
database consists huge number transac tion records transaction identifier tid set data items 
assume database horizontally partitioned grouped transactions allocated sites dis tributed system communicate message passing 
assumptions examine distributed mining association rules 
known major cost mining association rules computation set large itemsets frequently occurring sets items see section database 
distributed computing large itemsets encounters new problems 
may compute locally large itemsets easily locally large itemset may globally large 
expensive broadcast data set sites option broadcast counts itemsets matter locally large small sites 
database may contain enormous combinations itemsets involve passing huge number messages 
observation exist interesting properties locally large globally large itemsets 
maximally take advantages properties reduce number messages passed confine substantial amount processing local sites 
mentioned algorithms parallel mining association rules proposed 
proposed algorithms pdm count distribution cd designed parallel systems 
adapted distributed environment 
proposed efficient distributed data mining algo rithm fdm vast distributed mining association rules distinct feature com parison proposed parallel mining algo rithms 

generation candidate sets spirit apriori 
interesting relationships locally large sets globally large ones explored generate smaller set candidate sets iteration reduce number messages passed 

candidate sets generated pruning techniques local pruning global pruning developed prune away candidate sets individual sites 

order determine candidate set large algorithm requires messages support count exchange num ber sites network 
straight adaptation apriori requires messages 
notice different combinations local global prunings adopted fdm 
studied versions fdm fdm lp fdm fdm lpp see section similar framework different combinations pruning techniques 
fdm lp explores local pruning fdm lup local pruning upperbound pruning fdm lpp local pruning polling site pruning 
extensive experiments conducted study performance fdm compare count distribution algorithm 
study demonstrates efficiency distributed mining algo rithm 
remaining organized follows 
tasks mining association rules sequential distributed environments defined sec tion 
section techniques distributed mining association rules important results discussed 
algorithms different versions fdm section 
performance study reported section 
discussions respectively sections 
problem definition sequential algorithm mining rules ira set items 
db database transactions transaction consists set items itemset transaction contains association rule implication 
association rule holds db confidence probability transaction db contains contains association rule support db probability transaction db contains task mining association rules find association rules support larger minimum support threshold confidence larger minimum confidence threshold 
itemset support percentage transactions db contains support count denoted sup number transactions db containing itemset large precisely frequently occurring support minimum support threshold 
itemset size called itemset 
shown problem mining association rules reduced subproblems find large itemsets minimum support threshold generate association rules large itemsets 
dominates cost mining association rules research focused develop efficient methods solve subproblem 
interesting algorithm apriori proposed computing large itemsets mining asso ciation rules transaction database 
studies mining association rules sequential algorithms centralized databases viewed vari ations extensions apriori :10.1.1.50.1686
example extension apriori dhp algorithm uses direct hashing technique eliminate size candidate sets apriori algorithm :10.1.1.50.1686
distributed algorithm mining rules examine mining association rules distributed environment 
db database transactions 
assume sites distributed system database db partitioned sites 
bn respectively 
size partitions dbi di sup support counts itemset db dbi respectively 
sup called global support count local support count site minimum support threshold globally large sup correspondingly locally large site di 
de notes globally large itemsets db globally large itemsets essential task distributed association rule mining algorithm find globally large itemsets comparison outline count distribution cd algorithm follows 
algorithm adaptation apriori algorithm distributed case 
iteration cd generates candidate sets site applying apriori gen function set large itemsets previous iteration 
site computes local support counts candidate sets broadcasts sites 
subsequently sites find globally large itemsets iteration proceed iteration 
techniques distributed data mining generation candidate sets important observe interesting properties related large itemsets distributed environments properties may substantially reduce number messages passed network mining association rules 
important relationship large itemsets sites distributed database globally large itemsets locally large site 
itemset globally large locally large site si called gl large site si 
set gl large itemsets site form basis site generate candidate sets 
monotonic properties easily observed locally large gl large itemsets 
itemset locally large site subsets locally large site secondly itemset gl large site subsets gl large site notice similar relationship exists large itemsets centralized case 
important result effective technique candidate sets generation distributed case developed 
lemma itemset globally large ex site subsets gl large site proof 
locally large site di sup globally large 
contradiction locally large site gl large consequently subsets gl large gli denote set gl large itemsets site si gli denote set gl large itemsets site si 
follows lemma exists site si size subsets gl large site si belong gli straightforward adaptation apriori set candidate sets th iteration denoted ca stands size candidate sets apriori generated applying ori gen function ca apriori gen 
site si cgi set candidates sets generated applying apriori gen gli cgi apriori gen gli cg stands candidate sets generated gl large itemsets 
cgi generated gli gli cgi subset ca 
cg denote set theorem set subset cgi apriori gen gli 
proof 

follows lemma exists site si size subsets gl large site 
cgi 
cg cgi apriori gen gli 
theorem indicates cg subset ca smaller ca taken set candidate sets size large itemsets 
difference sets ca cg depends distribution item sets 
theorem forms basis generation set candidate sets algorithm fdm 
set candidate sets cgi gener ated locally site th iteration 
exchange support counts gl large itemsets gl cg iteration 
gl candidate sets st iteration generated 
performance study section approach number candidate sets generated substantially reduced generated cd 
example illustrates effectiveness reduction candidate sets theorem 
example assuming sites system partitions db db db db 
suppose set large itemsets computed iteration locally large site locally large site locally large site 
fore glo glo gl 
theo rem set size candidate sets site cg cg apriori gen gl ab bc ac 
similarly cg bc cd bd cg ef eh fg fh gh 
set candidate sets large itemsets cg cg cg cg total candidates 
apriori gen applied lo set candidate sets ca apriori gen lu candidates 
shows effective apply theorem reduce candidate sets 
local pruning candidate sets previous subsection shows theorem usually generate distributed en vironment smaller set candidate sets direct application apriori algorithm 
set candidate set cg generated find globally large itemsets support counts candidate sets exchanged sites 
notice candidate sets cg pruned local pruning technique count exchange starts 
general idea site candidate set cgi locally large site si need si find global support count determine globally large 
case small globally large locally large site site locally large need responsible find global support count order compute large itemsets site candidate sets confined sets cgi locally large site si 
convenience denote candidate sets cgi lo cally large site si 
discussion iteration th iteration gl large itemsets computed site si procedure 
candidate sets generation generate candidate sets cgi gl large itemsets site si st iteration formula cgi apriori gert gli 
pruning cgi scan partition dbi compute local support count 
locally large site el excluded candidate sets lli 
note pruning removes candidate set site el 
candidate set site 
support count exchange broadcast candidate sets lli sites collect support counts 
compute global support counts find gl large itemsets site si 

broadcast mining results broadcast computed gl large itemsets sites 
clarity notations far listed table 
ca sup di gli lli number transactions db support threshold globally large itemsets candidate sets generated global support count number transactions dbi gl large itemsets candidate sets generated locally large itemsets local support count table notation table 
illustrate procedure continue working example follows 
example assume database example contains transactions partitions transactions 
assume support threshold 
example second iteration candidate sets generated site cg ab bc ac site cg bc bd cd site cg ef eh fg fh gh 
order compute large itemsets local support counts sup sup ab bc ef bc cd ac bd eh fg fh gh table locally large itemsets 
site computed 
result recorded table 
table seen ac sup 
ac locally large 
candidate set ac pruned away site hand ab bc local support counts survive local pruning 
ll ab bc 
similarly ll bc cd lla ef gh 
local pruning number size candidate sets reduced half original size 
local pruning completed site broadcasts messages containing remaining candidate sets sites collect support counts 
result count support exchange recorded table 
locally broadcast sup sup large request candidates ab bc cd ef gh table globally large itemsets 
request support count ab broadcasted site counts sent back recorded site second row table 
rows record similar count ex change activities sites 
iteration site finds bc bc sup ab sup itemset site gl bc 
similarly gl bc cd gl ef 
broadcast gl large itemsets sites return large itemsets bc cd ef 
notice candidate set bc example locally large site 
case messages broadcasted sites bc locally large 
unnecessary candidate itemset broadcast needed 
section optimization technique eliminate redundancy discussed 
subtlety implementation steps outlined finding globally large itemsets 
order support step local pruning step support count exchange site sets support counts 
local pruning find local support counts candidate sets cgi 
support count exchange find local support counts possibly different candidate sets sites order answer count requests sites 
simple approach scan dbi twice collection counts local cgi responding count requests sites 
substantially degrade perfor mance 
fact need scans 
cgi available th iteration sets gli broadcasted site st iteration sets didate sets cgi computed corresponding gli iteration gl large itemsets previous iteration broadcasted sites site compute candidate sets site 
local support counts candidate sets scan stored data structure hash tree apriori 
technique data structure built scan different sets support counts required local pruning support count exchange retrieved data structure 
global pruning candidate sets local pruning site uses local support counts dbi prune candidate set 
fact local support counts sites pruning 
global pruning technique developed facilitate pruning outlined follows 
iteration local support global support counts candidate set available 
local support counts broadcasted global support counts candidate set globally large 
information global pruning performed candidate sets subsequent iteration 
assume local support count didate itemset broadcasted sites globally large itera tion 
suppose size candidate itemset th iteration 
local support counts size subsets available site 
respect partition dbi denote minimum value local support counts size subsets min iyi 
follows subset relationship upper bound local support count 
sum upper bounds partitions denoted maxsup upper bound sup 
words sup maxsup ein 
note maxsup computed site th iteration 
maxsup upper bound global support count pruning maxsup candidate itemset 
technique called global pruning 
global pruning combined local pruning form different pruning strategies 
particular variations strategy adopted introduce versions fdm section 
method called upper bound pruning second called polling site pruning 
discuss upper bound pruning method detail 
polling site pruning method explained subsection 
upper bound pruning uses techniques subsections generate perform local pruning candidate sets 
count exchange starts site applies global pruning remaining candidate sets 
possible upper bound global support count candidate set sum 
ji local pruning 
upper bound computed prune candidate set site si 
example examine global pruning local pruning done example 
table survived candidate sets local pruning ab bc 
local support counts table 
furthermore local support counts subsets sites available listed table 
tables upper bound support count ab denoted ab sp ab sp ab sup min sup sup min sup sup upper bound support threshold ab removed set candidate itemsets 
large local support count itemset sup sup table local support counts 
hand upper bound support count bc denoted bc sp bc sp bc sup min sup sup min sup sup larger threshold bc pruned away remains candidate itemset global pruning useful technique reducing number candidate sets 
effectiveness depends distribution local support counts 
count polling cd algorithm local support count candidate itemset broadcasted site site 
number messages required count exchange candidate itemset number partitions 
method candidate itemset locally large site si si needs messages collect support counts general candidate itemsets locally large sites 
fdm algorithm usually require messages computing candidate itemset 
ensure fdm requires messages candidate itemset cases count polling technique introduced 
candidate itemset technique uses assignment function hash func tion assign polling site assuming assignment function known site 
polling site assigned independent sites founded locally large 
locally large site sent polling site 
candidate itemset polling site responsible find globally large 
achieve purpose polling site broadcast polling request collect local support counts compute global support count 
polling site candidate itemset number messages required count exchange reduced 
th iteration pruning phase local global pruning completed fdm uses procedure site si count polling 

send candidate sets polling sites site polling site find candidate itemsets lli polling site store ll candidates put groups polling sites 
local support counts candidate itemsets stored corresponding set ll 
send ll corresponding polling site 
poll collect support counts polling site receives sent sites 
candidate itemset received finds list originating sites sent 
broadcasts polling requests sites list collect support counts 

compute gl large itemsets receives support counts sites computes global support counts candidates finds itemsets 
eventually broadcasts itemsets global support counts sites 
example example assuming assigned polling site ab bc assigned polling site cd assigned polling site ef gh 
assignment site responsible polling ab bc 
simple case ab sends polling requests collect support counts 
bc locally large pair bc bc sup bc sent 
receives message sends polling request remaining site 
support count bc sup received finds bc sup 
bc gl large itemset 
example polling site double polling messages bc eliminated 
algorithm distributed mining association rules section basic version fdm fdm lp fdm local pruning algorithm adopts techniques candidate set reduction local pruning discussed section 
performance study section fdm lp efficient cd 
fdm lp algorithm algorithm fdm lp fdm local pruning input dbi database partition site si 
output set globally large itemsets 
method iteratively execute program fragment th iteration distributively site si 
algorithm terminates set candidate sets cg 
ilk get local count db cg cg apriori gen gli ti get local count db cg ti di polling site sj insert lli send lli site receive lp insert update large sites send polling request reply polling request ti receive sup sites large sites sup ei sup insert gi broadcast receive gs sites gi 
divide gl return 
explanation algorithm algorithm site initially home site set candidate sets generates 
polling site serve requests sites 
subsequently changes status remote site supply local support counts polling sites 
corresponding steps algorithm different roles activities grouped explained follows 

home site generate candidate sets submit polling sites lines 
iteration site calls get local count scan partition dbi store local support counts itemsets array tio 
th iteration si computes set didate set cg scans dbi build hash tree ti containing locally support counts sets cg 
traversing ti si finds locally large itemsets group polling sites 
sends candidate sets local support counts polling sites 

polling site receive candidate sets send polling requests lines 
polling site site receives candidate sets sites insert 
candidate set si stores home sites large sites contains sites sent si polling 
order perform count exchange si calls send polling request send sites list large sites collect remaining support counts 

remote site return support counts polling site line 
si receives polling requests sites acts remote site 
candidate set receives polling site retrieves hash tree ti returns polling site 

polling site receive support counts find large itemsets lines 
polling site receives local support counts candidate sets 
computes global support counts candidate sets find globally large itemsets 
globally large itemsets stored set gi 
broadcasts set gi sites 

home site receive large itemsets lines 
home site receives sets globally large itemsets gi polling sites 
union gi si finds set size large itemsets 
finds set gli gl large itemsets site site list large sites 
sets gli candidate set generation iteration 
fdm lp described utilized tech niques described subsections 
illustration fdm lp example examples 
refinements fdm lp adoption different global pruning techniques 
fdm lup algorithm algorithm fdm lup fdm local upper bound pruning method program fragment fdm lup obtained fdm lp inserting condition line line algorithm 
upper bound explanation algorithm new step fdm lup upper bound pruning line 
function upper bound computes upper bound candidate set formula suggested subsection 
words upper bound returns upper bound sum 
ji explained subsection computed local pruning step values computed local support counts st iter ation 
upper bound smaller global support threshold prune away usually smaller number candidate sets count exchange comparison fdm lp 
fdm lpp algorithm algorithm fdm lpp fdm local pruning polling site pruning method program fragment fdm lpp obtained algorithm replacing line lines 
upper bound send polling request explanation algorithm new step fdm lpp polling site pruning line 
stage polling site received requests sites perform polling 
request contains locally large itemset local support count site sent note large sites set originating sites requests polling sent polling site line 
site large sites local support count sent 
site large sites locally large local support count smaller local threshold dq 
discussion subsection bounded value min dq 
upper bound sup computed sum lar ge site min dq 
gx lar ge sites fdm lpp si calls upper bound compute upper bound sup formula 
upper bound prune away smaller global support threshold 
discussed fdm lup may candidate sets fdm lp 
require storage communication messages local support counts 
efficiency comparing fdm lp depend largely data distribution 
performance study fdm depth performance study performed compare fdm cd 
chosen implement representative version fdm compare cd 
algorithms implemented distributed system pvm parallel virtual machine 
series rs workstations running aix system connected mb lan perform experiment 
databases experiment composed synthetic data 
experiment result number candidate sets fdm site cd 
total message size fdm cd 
execution time fdm cd 
reduction number candidate sets message size fdm significant 
reduction execution time substantial 
directly proportional reduction candidate sets message size 
mainly due overhead running fdm cd pvm 
observed overhead pvm fdm close cd amount message communication significantly smaller fdm 
results experiments clear performance gain fdm cd higher distributed systems communication bandwidth important performance factor 
example mining done distributed database wide area long haul network 
performance fdm lp apriori large database compared 
case response time fdm lp longer parameter interpretation value transaction mean size mean size maximal potentially large itemsets number potentially large itemsets number items clustering size ps pool size cr correlation level 
multiplying factor table parameter table 
response time apriori number sites 
ideal speed 
terms total execution time fdm lp close apriori 
test bed workstations 
local disk partition loaded local disk experiment starts 
databases experiment synthetic data generated techniques introduced 
parameters similar :10.1.1.50.1686
table list parameters values synthetic databases 
readers familiar parameters refer 
notation tx 
iy 
dm denote database thousands iti iii 
number nodes fdm cd candidate sets reduction candidate sets message size re duction sizes databases study range transactions minimum support threshold ranges 
note number candidate sets site cd different fdm 
experiment witnessed reduction candidate sets 
minimum support fdm cd candidate sets reduction average site fdm lp compared cd 
average number candidate sets generated fdm lp cd transaction database plotted number partitions 
fdm lp reduction candidate sets 
percentage reduction increases number partitions increases 
shows fdm effective system scaled 
comparison fdm lp cd database partitions different thresholds 
case experienced similar amount reduction 
number nodes fdm cb message size reduction reduction candidate sets proportional impact reduction messages comparison 
discussed polling site technique guarantees fdm requires messages candidate set smaller messages required cd 
experiment fdm reduction total message size cases compared cd 
total message size fdm cd database plotted number partitions 
comparison database partitions dif 
oo minimum support fdm cd message size reduction ferent support thresholds 
results confirm analysis fdm lp effective cutting number messages required 
number nodes fdm cd execution time 
minimum support fdm cd execution time execution time reduction compared execution time fdm lp cd 
execution time fdm lp cd database plotted number partitions 
fdm lp faster cd cases 
comparison plotted different thresholds database partitions 
shown similar amount speed 
response time sec total execution time sec apriori fdm lp table efficiency fdm lp 
compared fdm lp sites apriori respect transactions database order find efficiency large database 
result shown table 
re sponse time fdm lp slightly larger apriori 
terms total ex ecution time fdm lp close apriori 
large database fdm lp may bigger portion database residing distributed memory apriori 
faster running apriori database single ma chine 
shows fdm lp scalable dis tributed system efficient effective technique mining association rules large databases 
performance study demonstrated fdm generates smaller set candidate sets requires significantly smaller amount messages comparing cd 
improvement execution time substantial overhead incurred pvm prevents fdm achieving speed proportional reduction candidate sets message size 
compared cd fdm lp evidence show fdm efficient cd distributed environment 
follow ing sections discuss plan implementing versions fdm 
discussions discussion discuss issue possible extension fdm fast parallel mining association rules 
discuss related issues relationship effectiveness fdm distribution data support threshold relaxation possible reduction message overhead 
cd pdm algorithms designed share parallel environment 
particular cd implemented tested ibm sp machine 
designing algorithm parallel mining association rules number size messages required minimized number synchronizations number rounds message communication 
cd simple synchronization scheme 
requires round message communication iteration 
second iteration pdm synchronization scheme cd 
fdm parallel environment shortcoming requires message cd needs synchronizations 
fdm modified overcome problem 
fact iteration candidate set reduction global pruning techniques eliminate candidates broadcast exchange local support counts remaining candidates 
approach generate candidate sets cd number synchronization 
perform better cd cases 
performance studies carried nodes ibm sp study variations approach result promising 
interesting issue relationship performance fdm distribution itemsets partitions 
theorem example clear number candidate sets decreases dramatically distribution itemsets quite skewed partitions 
globally large itemsets locally large sites reduction candidate sets fdm significant 
worst case globally large itemset locally large sites candidate sets fdm cd 
data skewness may improve performance fdm general 
special partitioning technique increase data skewness optimize performance fdm 
study required explore issue 
issue want discuss possible usage relaxation factor proposed 
fdm site sends candidate sets locally large locally large polling sites polling sites may local support counts sites perform global pruning candidate sets 
example support threshold site send candidate sets local support counts exceed polling sites 
case candidate sets polling sites may receive local support counts sites relaxation case 
global pruning may effective 
trade sending candidate sets polling sites pruning candidate sets polling sites 
study necessary detailed relationship relaxation factor performance pruning 
proposed studied efficient effective distributed algorithm fdm mining association rules 
interesting properties locally globally large itemsets observed leads effective technique reduction candidate sets discovery large itemsets 
powerful pruning techniques local global prunings proposed 
furthermore optimization communications participating sites performed fdm polling sites 
variations fdm different combination pruning techniques described 
representative version fdm lp implemented performance compared cd algorithm distributed system 
result shows high performance fdm mining association rules 
issues related extensions method discussed 
techniques candidate set reduction global pruning integrated cd perform mining parallel envi ronment better cd consider ing message communication synchronization 
improvement performance fdm algorithm data distribution relaxation support thresholds discussed 
interesting studies mining generalized association rules association rules quantitative association rules extension method mining kinds rules distributed parallel system interesting issues research :10.1.1.144.4956:10.1.1.40.8600
parallel distributed data mining kinds rules characteristic rules classification rules clustering important direction studies 
performance studies implementation different versions fdm ibm sp system nodes carried result promising 
agrawal sharer 
parallel mining association rules design implementation experience 
ibm research report 
agrawal srikant 
fast algorithms mining association rules 
proc 
int 
conf 
large data bases pages santiago chile september 
agrawal srikant 
mining sequential patterns 
proc 
int 
conf 
data engineering pages taipei taiwan march 
cheung hah ng wong 
maintenance discovered association rules large databases incremental updating technique 
proc 
int conf 
data engineering new orleans louisiana feb 
fayyad piatetsky shapiro smyth uthurusamy 
advances knowledge discovery data mining 
aaai mit press 
geist beguelin dongarra jiang manchek sunderam :10.1.1.50.1686:10.1.1.144.4956:10.1.1.40.8600
pvm parallel virtual machine users guide tutorial networked parallel computing 
mit press 
hah cai cercone 
datadriven discovery quantitative rules relational databases 
ieee trans 
knowledge data engineering 
hah fu 
discovery multiple level association rules large databases 
proc 
int 
conf 
large data bases pages zurich switzerland sept 
ng ham efficient effective clustering method spatial data mining 
proc 
int 
conf 
large data bases pages santiago chile september 
park chen yu 
effective hash algorithm mining association rules 
proc 
acm int 
conf 
management data pages san jose ca may 
park chen yu 
efficient parallel mining association rules 
proc 
th int 
conf 
information knowledge management pages baltimore maryland nov 
savasere omiecinski navathe 
efficient algorithm mining association rules large databases 
proc 
int 
conf 
large data bases pages zurich switzerland sept 
silberschatz stonebraker ullman 
database research achievements opportunities st century 
report nsf workshop database systems research may 
srikant agrawal 
mining generalized association rules 
proc 
int 
conf 
large data bases pages zurich switzerland sept 
srikant agrawal 
mining quantitative association rules large relational tables 
proc 
acm int 
conf 
management data montreal canada june 
