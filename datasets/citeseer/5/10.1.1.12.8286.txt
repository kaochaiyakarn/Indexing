locally weighted naive bayes eibe frank mark hall bernhard pfahringer department computer science university waikato hamilton new zealand cs waikato ac nz april despite simplicity naive bayes classi er surprised machine learning researchers exhibiting performance variety learning problems 
encouraged results researchers looked overcome naive bayes primary weakness attribute independence improve performance algorithm 
presents locally weighted version naive bayes relaxes independence assumption learning local models prediction time 
experimental results show locally weighted naive bayes rarely degrades accuracy compared standard naive bayes cases improves accuracy dramatically 
main advantage method compared techniques enhancing naive bayes conceptual computational simplicity 
principle bayes theorem enables optimal prediction class label new instance vector attribute values 
unfortunately straightforward application bayes theorem machine learning impractical inevitably insucient training data obtain accurate estimate full joint probability distribution 
independence assumptions inference feasible 
naive bayes approach takes extreme assuming attributes statistically independent value class attribute 
assumption holds practice naive bayes performs surprisingly classi cation problems 
furthermore computationally ecient training linear number instances attributes simple implement 
interest naive bayes learning algorithm machine learning circles attributed clark niblett cn rule learner clark niblett 
included simple bayesian classi er naive bayes straw man experimental evaluation noted performance compared sophisticated learners 
explained naive bayes cases attribute independence assumption violated domingos pazzani fact remains probability estimation accurate performance degrades attribute independence hold 
various techniques developed improve performance naive bayes aimed reducing naivete algorithm retaining desirable aspects simplicity computational eciency 
zheng webb zheng webb provide excellent overview area 
existing techniques involve restricted sub classes bayesian networks combine attribute selection naive bayes incorporate naive bayes models type classi er decision tree 
presents lazy approach learning naive bayes models 
lazy learning methods approach simply stores training data defers ort involved learning classi cation time 
called classify new instance construct new naive bayes model weighted set training instances locale test instance 
local learning helps mitigate ects attribute dependencies may exist data expect method strong dependencies neighbourhood test instance 
naive bayes requires relatively little data training neighbourhood kept small reducing chance encountering strong dependencies 
implementation size neighbourhood chosen data dependent fashion distance th nearest neighbour test instance 
experimental results show locally weighted naive bayes relatively insensitive choice attractive alternative nearest neighbour algorithm requires ne tuning achieve results 
results show locally weighted naive bayes uniformly improves standard naive bayes 
structured follows 
section approach enhancing naive bayes locally weighted learning 
section contains experimental results arti cial domains collection benchmark datasets demonstrating predictive accuracy naive bayes improved learning locally weighted models prediction time 
section discusses related enhancing performance naive bayes 
section summarizes contributions 
locally weighted learning naive bayes method enhancing naive bayes borrows technique originally proposed estimating non linear regression models cleveland linear regression model data weighting function centered instance prediction generated 
resulting estimator nonlinear weighting function changes instance processed 
explore locally weighted learning classi cation appears received little attention machine learning literature atkeson 
loader hastie 
discuss called local likelihood methods statistical perspective including locally weighted linear logistic regression locally weighted density estimation 
naive bayes example density estimation classi cation 
compared logistic regression advantage linear number attributes making computationally ecient learning problems attributes 
naive bayes exactly way linear regression locally weighted linear regression local naive bayes model subset data neighbourhood instance class value predicted call instance test instance 
training instances neighbourhood weighted weight assigned instances test instance 
classi cation obtained naive bayes model attribute values test instance input 
subsets data train locally weighted naive bayes model determined nearest neighbours algorithm 
user speci ed parameter controls instances 
implemented weighting function compact support setting width bandwidth distance kth nearest neighbour 
euclidean distance ith nearest neighbour assume attributes normalized lie zero distance computed nominal attributes binarized 
weighting function 
set weight instance means instance receives weight zero instances away test instance receive weight zero instance identical test instance receives weight 
monotonically decreasing function property candidate weighting function 
experiments linear weighting function linear de ned linear words weight decrease linearly distance 
higher values result models vary response uctuations data lower values enable models conform closer data 
small value may result models noise data 
experiments show method particularly sensitive choice long small 
caveat 
order avoid zero frequency problem implementation naive bayes uses laplace estimator estimate conditional probabilities nominal attributes interacts weighting scheme 
empirically opportune scale weights total weight instances generate naive bayes model approximately assume training instances rescaled weights computed follows total number training instances 
naive bayes computes posterior probability class test instance attribute values am follows ja am jc jc total number classes 
individual probabilities right hand side equation estimated weighted data 
prior probability class class value training instance index indicator function zero 
assuming attribute nominal conditional probability value attribute test instance jc ij ij spheres dataset 
number values attribute ij value attribute instance data contains numeric attribute discretize fayyad irani mdl discretization scheme fayyad irani treat result nominal attribute normality assumption estimating mean variance weighted data 
empirical results approaches 
experimental results rst illustrative results arti cial problems discussing performance method standard benchmark datasets 
evaluation arti cial data section compare behaviour locally weighted naive bayes nearest neighbour algorithm arti cially generated datasets 
particular interested sensitive techniques size neighbourhood choice discuss results standard naive bayes normality assumption numeric attributes 
shows rst arti cial dataset 
problem involves predicting spheres instance contained 
spheres arranged rst sphere radius completely contained larger hollow second sphere radius 
instances described terms coordinates dimensional space 
dataset contains randomly drawn instances spheres classes 
plots performance locally weighted naive bayes nearest neighbours knn nearest neighbours distance weighting spheres data increasing values point graph represents accuracy scheme averaged folds single run fold cross validation 
seen performance nearest neighbour su ers increasing instances expanding band boundary spheres get misclassi ed 
locally weighted naive bayes hand initially improves performance slightly decreases increases 
data suited naive bayes knn performance nearest neighbours knn nearest neighbours distance weighting locally weighted naive bayes spheres data 
checkers board dataset 
normal distributions placed dimensions sphere suciently di erent 
standard naive bayes achieves accuracy spheres data 
set include training instances locally weighted naive bayes gets correct 
shows second arti cial dataset 
problem involves predicting instance belongs black white square checkers board coordinates 
instances generated randomly sampling values square checkers board width height 
plots performance locally weighted naive bayes nearest neighbours nearest neighbours distance weighting checkers board data increasing values strong interaction attributes data impossible standard naive bayes learn target concept 
seen locally weighted naive bayes begins performance performance correct 
comparison nearest neighbours performance far predictable respect value exhibits knn performance nearest neighbours knn nearest neighbours distance weighting locally weighted naive bayes checkers board data 
performance improves correct starts decrease 
evaluation uci datasets section evaluates performance locally weighted naive bayes collection benchmark datasets uci repository blake merz 
properties datasets shown table 
ran experiments 
rst compares locally weighted naive bayes standard naive bayes nb nearest neighbours distance weighting knn 
experiment normal distributions nb numeric attributes 
second experiment compares locally weighted naive bayes standard naive bayes lazy bayesian rule learner lbr zheng webb averaged dependence estimators webb 
case lbr handle nominal attributes discretized numeric attributes method fayyad irani fayyad irani 
accuracy estimates obtained averaging results separate runs strati ed fold cross validation 
words scheme applied times generate estimate particular dataset 
case discretization applied pre processing step intervals rst estimated training folds applied test folds 
speak results dataset signi cantly di erent di erence statistically signi cant level corrected resampled test nadeau bengio pair data points consisting estimates obtained folds learning schemes compared 
show standard deviations results 
table shows results rst experiment 
compared standard naive bayes locally weighted naive bayes signi cantly accurate datasets signi cantly accurate datasets 
cases method improves performance naive bayes considerably 
example vowel data accuracy increases 
similar levels improvement seen glass autos pendigits sonar vehicle segment 
compared nearest neighbours locally weighted naive bayes signi cantly accurate table datasets experiments dataset inst 
num 
nom 
class anneal arrhythmia audiology australian autos bal scale breast breast diabetes ecoli german glass heart heart heart stat hepatitis horse colic hypothyroid ionosphere iris kr vs kp labor lymph mushroom optdigits pendigits prim tumor segment sick sonar soybean splice vehicle vote vowel waveform zoo datasets respectively 
distance weighting nearest neighbours method signi cantly accurate datasets respectively 
locally weighted naive bayes signi cantly accurate nearest neighbours diabetes australian 
table shows results second experiment 
experiment compares discretized locally weighted naive bayes discretized naive bayes lazy bayesian rules averaged dependence estimators 
compared naive bayes method signi cantly accurate datasets signi cantly accurate 
similar situation rst experiment improvements naive bayes quite considerable 
compared lazy bayesian rules method signi cantly better datasets signi cantly worse 
note results lazy bayesian rules missing method computational complexity 
averaged dependence estimators result signi cant wins favour method versus signi cant table experimental results locally weighted naive bayes versus naive bayes nb nearest neighbours distance weighting knn data set nb knn knn anneal arrhythmia audiology australian autos bal scale breast breast diabetes ecoli german glass heart heart heart stat hepatitis horse colic hypothyroid ionosphere iris kr vs kp labor lymph mushroom optdigits pendigits prim tumor segment sick sonar soybean splice vehicle vote vowel waveform zoo statistically signi cant improvement degradation losses 
related course lot prior tried improve performance naive bayes 
usually approaches address main weakness naive bayes independence assumption explicitly directly estimating dependencies implicitly increasing number parameters estimated 
approaches allow tighter training data 
typically independence assumption relaxed way keeps table experimental results discretized locally weighted naive bayes versus discretized naive bayes lazy bayesian rules lbr averaged estimators data set lbr anneal arrhythmia audiology australian autos bal scale breast breast diabetes ecoli german glass heart heart heart stat hepatitis horse colic hypothyroid ionosphere iris kr vs kp labor lymph mushroom optdigits pendigits prim tumor segment sick sonar soybean splice vehicle vote vowel waveform zoo statistically signi cant improvement degradation computational advantages pure naive bayes 
methods naive bayes friedman webb 
allow capture attribute dependencies computationally ef cient 
alternative approaches try transform original problem form allows correct treatment dependencies 
semi naive bayes kononenko cartesian product method pazzani transformation attempts capturing pairwise dependencies 
methods implicitly increase number parameters estimated include kohavi lazy bayesian rules zheng webb 
approaches fuse standard rule learner local naive bayes models 
similar approach sense lazy technique albeit higher computational requirements 
technique recursive naive bayes langley builds hierarchy naive bayes models trying accommodate concepts need complicated decision surfaces 
focused investigation locally weighted version standard naive bayes model similar spirit locally weighted regression 
empirically locally weighted naive bayes outperforms standard naive bayes nearest neighbor methods datasets investigation 
additionally new method exhibit robust behaviour respect important parameter neighbourhood size 
considering computational complexity locally weighted naive bayes runtime obviously dominated distance computation 
assuming naive implementation nearest neighbour operation linear number training examples test instance 
improvements sophisticated data structures kd trees 
long size selected neighbourhood constant sublinear function training set size naive bayes replaced complex learning method 
provided complex method scales linearly number attributes increase computational complexity full learning process 
exploring general locally weighted classi cation direction 
directions include exploring di erent weighting kernels preferably adaptive setting respective parameters 
application wise plan employ locally weighted naive bayes text classi cation area standard naive bayes nearest neighbor methods quite competitive support vector machines 
atkeson moore schaal 

locally weighted learning 
arti cial intelligence review 
blake merz 

uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
clark niblett 

cn induction algorithm 
machine learning 
cleveland 

robust locally weighted regression smoothing scatterplots 
journal american statistical association 
domingos pazzani 

optimality simple bayesian classi er zero loss 
machine learning 
fayyad irani 

multi interval discretization attributes classi cation learning 
proceedings thirteenth international joint conference articial intelligence pp 

morgan kaufmann 
friedman geiger goldszmidt 

bayesian network classi ers 
machine learning 
hastie tibshirani friedman 

elements statistical learning data mining inference prediction 
springer verlag 
kohavi 

scaling accuracy naive bayes classi ers decision tree hybrid 
proceedings second international conference knowledge discovery data mining pp 

acm press 
kononenko 

semi naive bayesian classi er 
proceedings sixth european working session learning pp 

springer verlag 
langley 

induction recursive bayesian 
proceedings european conference machine learning ecml volume lnai 
springer verlag 
loader 

local regression likelihood 
springer verlag 
nadeau bengio 

inference generalization error 
advances neural information processing systems pp 

mit press 
pazzani 

constructive induction cartesian product attributes 
proceedings information statistics induction science conference isis pp 

world scienti webb 

naive bayesian classi cation 
submitted 
zheng webb 

lazy learning bayesian rules 
machine learning 

