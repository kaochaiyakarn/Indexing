feature extraction lococode technical report revised sepp hochreiter fakultat fur informatik technische universitat munchen munchen germany informatik tu muenchen de www informatik tu muenchen de jurgen schmidhuber idsia corso lugano switzerland juergen idsia ch www idsia ch juergen revised january low complexity coding decoding lococode novel approach sensory coding unsupervised learning 
previous methods explicitly takes account information theoretic complexity code generator computes convey information input data computed decoded low complexity mappings 
implement lococode training flat minimum search general method discovering low complexity neural nets 
turns approach unknown number independent data sources extracting minimal number low complexity features necessary representing data 
experiments show codes obtained standard feature detectors unstructured usually sparse factorial local depending statistical properties data 
lococode explicitly designed enforce sparse factorial codes extracts optimal codes difficult versions bars benchmark problem ica pca 
produces familiar biologically plausible feature detectors applied real world images 
preprocessor vowel recognition benchmark problem sets stage excellent classification performance 
results interesting previously ignored connection important fields regularizer research ica related research 
goal sensory coding field 
generally agreed answer 
information theoretic objective functions ofs proposed evaluate quality sensory codes 
ofs focus statistical properties code components mutual information refer code component oriented ofs 
explicitly favor near factorial minimally redundant codes input data see watanabe barlow linsker schmidhuber schmidhuber schraudolph sejnowski redlich deco parra 
codes advantageous data compression speeding subsequent gradient descent learning becker simplifying subsequent bayes classifiers schmidhuber 
approaches favor local codes rumelhart zipser barrow kohonen 
help achieve minimal crosstalk subsequent gradient descent speedups facilitation post training analysis simultaneous representation different data items 
encouraging biologically plausible sparse distributed codes field barlow mozer ak ak young palm zemel hinton field saund dayan zemel li olshausen field zemel hinton ghahramani 
sparse codes share certain advantages local dense codes 
coding costs 
express desirable properties code neglecting costs constructing code data 
instance coding input data redundancy may expensive terms information bits required describe code generating network may need finely tuned free parameters 
previous argument ignoring coding costs zemel zemel hinton hinton zemel principle minimum description length mdl solomonoff wallace boulton rissanen focuses hypothetical costs transmitting data sender receiver bits necessary enable receiver reconstruct data 
goes total transmission cost number bits required describe data code reconstruction error decoding procedure 
input exemplars encoded decoded mapping coding decoding costs negligible occur 
doubt sensory coding sole objective transform data compact code cheaply ideal channel 
fact compact code possible environmental inputs true probabilistic causal model corresponding universe basic physical laws 
generating code dealing everyday events far costly 
believe sensory coding objectives reduce cost code generation data transformations existing channels synapses denying usefulness certain postulate important scarce resource bits required describe mappings generate process codes mappings need implemented limited hardware 

reasons shift point view focus information theoretic costs code generation 
novel approach unsupervised learning called low complexity coding decoding lococode see hochreiter schmidhuber 
assuming particular goals data compression simplifying subsequent classification mdl spirit lococode generates called convey information input data computed data low complexity mapping lcm decoded lcm 
minimizing coding decoding costs lococode yield efficient robust noise tolerant mappings processing inputs codes 
fms 
implement lococode apply flat minimum search fms hochreiter schmidhuber aa hidden layer activations represent code 
fms general gradient method finding networks described bits information 
coding input simple basis functions 
basis function function determining activation code component response input 
analysis section show lococode tries reproduce current input code components possible computed separate low complexity basis function implementable subnetwork low precision weights 
reflects basic assumption true input causes hinton dayan zemel ghahramani simple 
training sets elements describable features result sparse codes sparseness necessarily mean active code components code components contribute reproducing input difference nonlinear case 
see lococode encourages noise tolerant feature detectors reminiscent observed mammalian visual cortex 
inputs mixtures regular features edges images described sparse fashion code components corresponding features contribute coding input 
contrast previous approaches sparseness viewed priori thing enforced explicitely note mammalian visual cortex rarely just transmits data transforming 
input data described naturally sparse fashion 
sparse factorial depending input decomposable factorial features 
likewise may deviate sparseness locality input exhibits single characteristic feature 
code factorial knowledge component representing characteristic feature implies knowledge natural represents true cause fashion reconstruction types processing simple 
outline 
fms review follow section 
section analyze beneficial effects fms error terms context 
remainder devoted empirical justifications lococode 
experiments section show kinds code discussed previous local sparse factorial natural 
section lococode extract optimal sparse codes reflecting independent features random horizontal vertical noisy bars ica pca won 
section lococode generate plausible sparse codes known center surround appropriate feature detectors real world images 
section lococode preprocessor standard overfitting back propagation bp speech data classifier 
surprisingly combination achieves excellent generalization performance 
conclude speech data lococode conveys essential noise free information form useful processing classification 
section discuss findings 
flat minimum search review fms overview 
fms general method finding low complexity networks high generalization capability 
fms finds large region weight space weight vector region similar small error 
regions called flat minima 
mdl terminology bits information required pick weight vector flat minimum corresponding low complexity network weights may low precision 
contrast weights sharp minimum require high precision specification 
natural product net complexity reduction fms automatically prunes weights setting zero units giving strong bias reduces output sensitivity respect remaining weights units 
previous fms applications focused supervised learning hochreiter schmidhuber fms led better stock market prediction results weight decay optimal brain surgeon hassibi stork 
unsupervised coding 
architecture 
layer feedforward net 
layer fully connected layer 
denote index sets output hidden input units respectively 
denote number elements set 
activation unit lm net input unit lm denotes weight connection unit unit denotes activation function denotes th component input vector 
theta theta number weights 
algorithm 
fms objective function features unconventional error term thetai log ij log thetai fi fi fi ij fi fi fi ij minimized gradient descent training set mean squared error mse positive regularizer scaling influence 
defining corresponds choosing tolerable error level priori optimal way doing 
measures weight precision number bits needed describe weights net 
reducing increasing means removing weight precision increasing mse 
constant number output units done efficiently standard bp order computational complexity 
details see hochreiter schmidhuber article home pages 
general attempts reducing net complexity see schmidhuber 
effects additional term come 
discover flat minima fms searches large axis aligned boxes weight space weight vectors box yield similar network behavior 
boxes satisfy flatness conditions fc fc 
fc enforces tolerable output variation response weight vector perturbations near flatness error surface current weight vector weight space directions 
boxes satisfying fc fc selects unique minimal net output variance 
negative logarithm box volume ignoring constant terms effect gradient descent algorithm 
number bits save constant required describe current net function change significantly changing weights box 
box edge length determines required weight precision 
see hochreiter schmidhuber details derivation 
term sparseness simple basis functions preferred 
simple basis functions 
term thetai log ij reduces output sensitivity respect weights units 
responsible pruning weights units 
unit activations decrease zero proportion fan outs order derivatives activation functions decrease zero proportion fan ins influence units output decrease zero proportion unit fan 
detailed analysis see hochreiter schmidhuber 
reason low complexity simple basis functions preferred 
sparseness 
point favors sparse hidden unit activations point favors noninformative hidden unit activations hardly affected small input changes 
point favors sparse hidden unit activations sense hidden units contribute producing output 
particular sigmoid hidden units active near zero activations preferred 
second term separated common basis functions preferred 
term log thetai fi fi fi ij fi fi fi ij punishes units similar influence output 
reformulate log thetai thetai fi fi fi ij fi fi fi fi fi fi fi fi fi ij ij ij rewritten log thetai thetai fi fi fi fi fi fi fi fi fi fi fi fi theta fi fi fi fi fi fi holds 
obtain log joj jo theta hj jij fi fi fi fi fi fi fi fi fi fi fi fi observe output unit sensitive respect hidden units heavily contribute compare numerator term brackets 
large contribution reduced making hidden units large impact output units see denominator term brackets 
choice basis functions 
fms tries way basis functions possible output unit leads separation basis functions simultaneously basis functions output units possible common basis functions 
special case linear output activation 
targets usually linear range sigmoid output activation function consider linear case detail 
suppose output units linear activation function cx real valued constant 
cw ki hidden unit obtain log joj jo theta hj jij jw ki jw ku kw denotes outgoing weight vector unit ki euclidean vector norm kxk pp kth component vector 
basis functions preferred 
observe hidden units outgoing weight vectors near zero weights yield small contributions number basis functions get minimized 
common basis functions preferred 
outgoing weight vectors hidden units encouraged large effect output see denominator term brackets 
implies preference basis functions generating output components 
basis function separation relevant basis functions output unit 
hand hidden units outgoing weight vectors solely consist near zero weights encouraged influence output different ways representing input feature see numerator term brackets 
fact fms punishes outgoing weight vectors opposite directions vectors obtained flipping signs weights multiple reflections hyperplanes trough origin orthogonal axis 
units performing redundant tasks activating ouput unit activating de activating cause large contributions 
encourages separation basis functions basis functions output unit 
low complexity data set fms find low complexity aa hidden layer activations code individual training exemplars 
aa split modules coding decoding 
previous aas 
backprop trained aas narrow hidden bottleneck bottleneck refers hidden layer containing fewer units layers typically produce redundant continuous valued codes unstructured weight patterns 
baldi hornik studied linear aas hidden layer bottleneck produce codes orthogonal projections subspace spanned principal eigenvectors covariance matrix associated training patterns 
showed mean squared error mse surface unique minimum 
nonlinear codes obtained nonlinear bottleneck aas layers kramer oja demers cottrell 
methods produces sparse factorial local codes produce principal components nonlinear equivalents principal manifolds 
see fms aas yield quite different results 
fms aas 
subsections low complexity coding aspect codes tend binary sigmoid units active values near easily generated low precision weights require code components hidden units hus units weights needed presenting input simple basic functions low precision weights 
low complexity decoding part codes tend hus near zero outgoing weights may units weights needed decoding sparsely locally distributed code components conveying information useful generating output activations possible hus equally important fewer weights needed 
encourage minimally redundant binary codes 
encourage sparse distributed local codes 
lead codes simply computable code components convey lot information active code components possible 
collectively code components represent simple input features 
experiments outline 
section provides overview experimental conditions 
section uses simple artificial tasks show various lococode types factorial local sparse feature depend input output properties 
visual coding experiments divided sections section deals artificial bars section real world images 
section true causes input data known show lococode learns represent optimally pca ica 
section generates plausible feature detectors 
section lococode preprocessor speech data fed standard backpropagation classifier 
provokes significant performance improvement 
experimental conditions experiments associate input data fms trained layer aa semilinear activation function hidden layer 
stated training exemplars sigmoid hidden units hus active sigmoid output units active units additional bias input normal weights initialized gamma bias hidden weights 
parameters details 
ffl learning rate conventional learning rate error term just backprop 
ffl positive regularizer hyperparameter scaling influence 
computed heuristically described hochreiter schmidhuber 
ffl delta value updating learning 
represents absolute change epoch 
ffl tol tolerable mean squared error mse training set 
dynamically computing deciding switch phases phase learning 
ffl phase learning speeds algorithm phase conventional backprop phase fms 
start phase switch phase tol average epoch error 
switch back phase fl tol finish phase 
experimental sections indicate phase learning mentioning values fl 
ffl pruning weights units judge weight ij pruned required precision ij hochreiter schmidhuber article input times lower corresponding decimal digits highest precision weights input 
unit considered pruned incoming weights pruned bias weight outgoing weights pruned 
details see hochreiter schmidhuber home pages 
comparison 
sections compare lococode independent component analysis ica cardoso schuster bell sejnowski amari component analysis pca oja 
ica realized cardoso jade joint approximate diagonalization eigen matrices algorithm matlab jade version obtained ftp sig enst fr 
jade whitening subsequent joint diagonalization th order cumulant matrices 
pca ica training exemplars case theta theta input fields 
information content 
measure information conveyed various codes obtained sections train standard backprop net training set code generation 
inputs code components task reconstruct original input tasks noisy bars original input scaled input components gamma 
net biased sigmoid hidden units active biased sigmoid output units active gamma 
train epochs caring overfitting 
training set consists fixed exemplars case theta input fields case theta input fields 
test set consists training set exemplars case real world images separate test image 
average mse test set determine reconstruction error 
experiment local sparse factorial codes feature detectors task 
data consists input vectors components 
th component th vector 
components 
sigmoid hidden units hus active 
code units output units additional bias input 
code units initialized negative bias 
experiment uniformly distributed inputs training examples 
input vector probability chosen 
parameters learning rate tolerable error tol delta fl 
architecture input units hus output units 
results factorial codes 
trials fms effectively pruned hus produced factorial binary code statistically independent code components 
trials fms pruned hus produced binary code unit values 
trial fms produced binary code hu pruned away 
obviously certain constraints input data fms strong tendency compact nonredundant codes advocated numerous researchers 
experiment experiment architecture training examples input values opposed input values 
parameters experiment 
hus experiment clear case fewer units pruned 
results local codes 
trials conducted 
fms produced binary code 
trials hu pruned remaining trials hus 
standard bp inputs coded entirely local manner hu switched switched 
recall local codes advocated researchers precisely opposite factorial codes previous experiment 
lococode justify different codes 
explain apparent discrepancy 
explanation 
reason different input representation additional hus necessarily result additional complexity mappings coding decoding 
zero valued inputs allow low weight precision low coding complexity connections leading hus similarly connections leading output units 
contrast experiment possible describe th possible input feature th input component equal zero 
implemented low complexity basis function 
contrasts features experiment hidden units zero input components better code code components possible yields factorial code 
experiment experiment architecture dimensional inputs 
parameters learning rate tol delta fl 
results feature detectors 
trials conducted 
fms produced code binary hu making distinction input values input values greater hus continuous values zero binary unit zero 
remaining hus adopt constant values essentially pruned away 
binary unit serves binary feature detector grouping inputs classes 
lococode recognizes causes 
data generated follows choose uniform probability value choose add values 
cause data recognized perfectly second divided code components due non linearity output unit adding different adding consider order derivatives 
experiment nonuniformly distributed inputs 
input vectors experiment occurring probabilities parameters learning rate tol delta fl 
architecture 
results sparse codes 
trials fms binary code hus pruned 
trials binary code hu pruned 
trial code hu removed unit adopting values 
trials code pruned hu hus 
obviously set fms prefers codes known sparse distributed representations 
inputs higher probability coded fewer active code components inputs lower probability 
typically inputs probability lead active code component inputs probability 
explanation 
result different experiment 
achieve equal error contributions inputs weights coding decoding highly probable inputs higher precision weights coding decoding inputs low probability input distribution experiment result complex network 
experiment effect pronounced 
experiment experiment architecture 
results sparse codes 
trials fms produced binary codes 
trials hu pruned 
trial units pruned 
trials units pruned 
standard bp inputs coded sparse distributed manner typically hus switched switched hus responded exactly different input patterns 
mean probability unit switched probabilities different hus switched tended equal 
table provides overview experiments 

fms finds codes quite different standard bp unstructured ones 
tends discover represent underlying causes 
usually resulting lococode exp input input values input architecture code result coding distribution components local uniform factorial code local uniform local code dense uniform feature detectors local sparse code local sparse code table overview experiments type input coding possible values input components distribution input vectors architecture form input hidden output units nature resulting lococode mainly depends nature input data 
sparse informative feature detectors 
depending properties data may factorial local 
suggests lococode may represent general principle unsupervised learning subsuming previous approaches 
feature automatically take account input output properties binary local input probabilities noise number zero input components 
experiment independent bars task adapted dayan zemel see ak zemel saund difficult compare baumgartner diploma thesis 
input theta pixel grid horizontal vertical bars random independent positions 
see example 
task extract independent features bars 
dayan task example partly overlapping bars 
nd th vertical bar nd horizontal bar switched simultaneously 
left corresponding input values 
zemel simpler variant vertical horizontal bars may mixed input trivial toy problem theta bar task hidden units turns quite hard algorithms discuss 
coding cost making error bar goes linearly size grid aspect problem gets easier large grids 
see difficult variants task hard lococode 
training testing 
possible bars appears probability contrast dayan zemel set allow bar type mixing 
task harder dayan zemel 
test lococode ability reduce redundancy hus required minimum 
dayan zemel report aa trained fms hus consistently failed 
result confirmed baumgartner 
pixels input unit 
input units see pixel bar take activation gamma 
see example 
dayan zemel net trained randomly generated patterns may pattern repetitions 
learning stopped epochs 
say pattern processed correctly absolute error output units 
details 
sigmoid hus active sigmoid output units active 
units additional bias input 
target avoid tiny derivatives caused targets 
normal weights initialized gamma bias weights 
parameters learning rate tol delta 
architecture 
pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned input hidden hidden output task independent bars 
left lococode input hidden weights 
right hidden output weights 
see text visualization details 
results factorial sparse codes 
training mse reconstruction error averages trials 
net generalizes test patterns processed correctly 
hus automatically pruned 
remaining hus binary lococode finds optimal factorial code exactly mirrors pattern generation process 
expected number bars input code sparse 
hus left shows theta square depicting typical post training weights connections inputs right outputs 
white black circles gray white background positive negative weights 
circle radius proportional weight absolute value 
left shows bias weights top squares upper left corners circle representing hu maximal absolute weight maximal possible radius circles representing weights scaled accordingly 
bias weights hus gamma gamma weights hus gamma bias weights output units gamma gamma weights hidden output units gamma 
backprop fails 
comparison run task conventional bp hus 
hus reconstruction error 
backprop prune units resulting weight patterns highly unstructured underlying input statistics discovered 
pca ica 
tried components 
shows results 
pca produces unstructured dense code ica sparse code sources recognizable separated 
ica finds dense code sources 
reconstruction errors pca pca ica ica 
ica pca codes components convey information component 
higher reconstruction errors pca ica due overfitting backprop net specializes training set 
pca ica ica task independent bars 
pca ica weights code components ica components 
ica sources recognizable achieve lococode quality 
task noisy bars 
task additional noise bar intensities vary input units see pixel bar activated correspondingly recall constant intensity task adopt activation gamma 
add gaussian noise variance mean pixel 
shows training exemplars generated way 
task adapted hinton 
hinton ghahramani difficult vertical horizontal bars may mixed input 
details 
training testing coding learning task tol delta 
tol set times expected minimal squared error tol number inputs oe 
achieve consistency task target pixel value times input pixel value compare task 
learning parameters task 
results 
training mse reconstruction error averages trials net generalizes 
hus pruned away 
lococode extracts optimal factorial code exactly mirrors pattern generation process 
due bar intensity variations remaining hus binary task 
task noisy bars examples theta training inputs depicted similarly weights previous figures 
pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned input hidden hidden output task independent noisy bars 
left lococode input hidden weights 
right hidden output weights 
depicts typical weights hus 
bias weights hus gamma gamma weights hus gamma bias weights output units gamma gamma weights hus output units gamma 
pca ica 
shows results comparable task 
pca codes ica codes unstructured dense 
ica codes sparse sources recognizable 
separated 
reconstruction errors pca pca ica ica 
observe pca ica codes components convey information component 
lower reconstruction error pca ica due information current noise conveyed additional code components reconstruct noisy images 
pca ica ica task independent noisy bars 
pca ica weights code components ica components 
ica codes extract sources achieve quality 

lococode solves hard variant standard bars problem 
discovers underlying statistics extracts essential statistically independent features presence noise 
standard bp aas accomplish dayan zemel confirmed additional experiments conducted 
ica pca fail extract true input causes optimal features 
lococode achieves success solely reducing information theoretic de coding costs 
previous approaches depend explicit terms enforcing independence schmidhuber zero mutual information code components linsker deco parra sparseness field zemel hinton olshausen field zemel hinton ghahramani 
lococode vs ica 
simple methods independent component analysis ica cardoso bell sejnowski amari lococode mixtures independent data sources 
methods need know advance number sources predictability minimization nonlinear ica approach schmidhuber simply prunes away superfluous code components 
visual coding applications sources determine value output input component sources easily computable input 
lococode outperforms simple ica minimizes number low complexity sources responsible output component 
experiment realistic visual data task 
experiment goal extract features visual data 
input data realistic aerial shot village 
train test task village image 
image sections training left testing right 
details 
shows images theta pixels gray levels 
theta pixels subsections left hand side right hand side image randomly chosen training inputs test inputs gray levels scaled input activations gamma 
sigmoid hus active sigmoid output units 
targets scaled gamma 
normal weights initialized gamma bias weights units 
training training examples 
parameters learning rate tol delta 
architecture 
image structure 
image dark certain white regions 
preprocessing stage map pixel values white pixel values different gray values 
largest reconstruction errors due absent information white pixels 
receptive fields small capture structures lines streets 
result sparse codes center surrounds 
trials led similar results trials sufficient due tiny variance 
hus survive 
reflect structure image compare preprocessing stage informative white spots captured center surround hus 
image dark causes surround effect output units negatively biased 
bright spots connected white pixels surrounded white pixels output input units near active output input unit tend active positive weight strength decreases moves away center 
entire input covered centers surviving units white regions input detected 
code sparse surviving white spot detectors active simultaneously inputs dark 
reconstruction error 
depict typical weights connections hus 
bias weights hus output units negative 
activate hu input match structure incoming weights 
hus pruned survive 
backprop results 
compare nets taught standard bp see typical example 
reconstruction errors hu 
hardly recognizable structures hus close pruned away 
pca ica 
shows results pca ica 
reconstruction errors pca pca ica ica 
codes obtained pca ica pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned input hidden hidden output task village 
left lococode input hidden weights 
right hidden weights 
units essentially pruned away 
input hidden hidden output task village 
left standard backprop input hidden weights 
right output weights 
pruned units hardly recognizable structures 
convey information components 
eigenvalues pca components 
significant difference subsequent eigenvalues 
occurs th th 
indicates code components 
lococode discovers automatically 
lococode codes white spots suit image better pca ica code component structures 
task 
task theta pixels subsections randomly chosen training pca ica ica task village 
pca ica weights code components ica components 
test inputs tol architecture 
training training examples 
parameters task 
pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned input hidden hidden output task village 
left lococode input hidden weights 
right hidden weights 
units essentially pruned away 
results sparse codes center surrounds 
trials led results similar task trials sufficient due tiny variance 
hus survive 
entire input covered white centers surviving units exhibit center surround weight structures 
allows detecting white regions input field 
code sparse compare task 
reconstruction error 
depicts typical weights connections hus output units negatively biased 
units survive 
task leads larger centers task typically provokes centers corresponding weights 
pca ica 
shows results pca ica 
reconstruction errors pca pca ica ica 
pca codes ica codes informative component ica bit pca 
pca codes convey information lococode ica suit image structure better 
pca eigenvalues 
significant difference subsequent eigenvalues th 
pca ica ica task village 
pca ica components weights code components 
task 
task inputs stem theta pixels section image wood cells left training image right test image 
tol delta 
training training examples 
parameters task 
image structure 
image consists elliptic cells various sizes 
cell interiors bright cell borders dark 
results 
trials led similar results trials sufficient due tiny variance 
bias weights hus negative 
activate hu input match structure incoming weights cancel inhibitory bias 
units survive 
obvious feature detectors characterized positions centers center surround structures relative input field 
specialized detecting cases north south west east northeast northwest southeast southwest cell centered cell cells 
entire input covered position specialized centers 
reconstruction error 
depicts typical weights connections hus training examples 
shows learning additional examples system concentrate centers left hand side negative weights pronounced positive ones right hand side 
longer training code sparser train test task wood cells 
image sections training left testing right 
pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned input hidden hidden output task cells 
left lococode input hidden weights exemplars 
right hidden output weights 
units survive 
causes pruned units 
typical feature detectors unit detects southeastern cell unit western eastern cells unit cells northwest southeast corners 
pca ica 
shows results pca ica 
reconstruction errors pca pca ica ica 
pca codes ica informative component lococode ica little pca 
component structures ica codes similar compare component component ica component ica compare component component ica component ica compare component component ica component ica 
lococode ica detect relevant sources positions cell interiors cell borders relative input field 
eigenvalues pca components pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned input hidden hidden output task cells 
left lococode input hidden weights exemplars 
right hidden output weights 
units survive 

gaps occur th th th th 
lococode essentially gap 
task 
task images striped piece wood 
see 
tol 
training training examples 
parameters task 
image structure 
image consists dark vertical stripes brighter background 
results 
trials led similar results hus survive obvious feature detectors different kind detect receptive field covers dark stripe left right middle 
reconstruction error 
examples examples depict typical weights connections hus 
example feature detectors unit detects dark stripe left unit dark stripe middle unit dark stripes left right unit dark stripe right 
unit informative gets pruned away 
pca ica 
see 
reconstruction errors pca pca ica ica 
pca codes ica codes informative component 
component structures pca ica codes similar detect positions dark stripes relative input field 
eigenvalues pca components 
gaps occur rd th th th th th 
lococode automatically extracts relevant components 
ica ica pca task cells 
pca ica components weights code components 
train test task striped wood 
image sections training left testing right 
overview experiments table shows ica codes sparse pca codes dense 
assuming visual input consists components collectively describable input features lococode 
pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned input hidden hidden output task stripes 
left lococode input hidden weights exemplars 
right hidden output weights 
units survive 
pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned pruned input hidden hidden output task stripes 
left lococode input hidden weights exemplars 
right hidden output weights 
units survive 

standard bp trained aas fms trained aas generate highly structured sensory codes 
fms automatically prunes superfluous units 
pca experiments indicate remaining code units suit various coding tasks 
account statistical properties visual input data lococode generates appropriate feature detectors familiar center surround bar detectors 
produces biologically plausible sparse codes standard aas 
fms objective function contain explicit terms enforcing codes contrasts previous methods olshausen field 
experiments show equally sized pca codes ica codes convey approximately information 
contrast pca ica lococode determines pca ica ica task stripes 
pca ica components 
code size automatically 
feature detectors obtained lococode similar ica 
cases know true input causes lococode find ica 
experiment vowel recognition justified previous ideas desirable code 
show help achieve superior generalization performance standard supervised learning benchmark problem 
section focus speech data illustrates versatility applicability limited visual data 
task 
recognize vowels vowel data scott fahlman cmu benchmark collection see robinson 
vowels speakers 
speaker spoke vowel times 
data speakers training 
data testing 
means frames training frames testing 
frame consists input components obtained low pass filtering khz digitized bits khz sampling rate 
twelfth order linear predictive analysis carried sample segments steady part vowel 
reflection coefficients calculate log area parameters providing dimensional input space 
coding 
training data coded fms aa 
architecture 
sigmoid hus active sigmoid output units active 
units additional bias input 
input components linearly scaled 
aa trained pattern presentations 
weights frozen 
classification 
vowel codes nonconstant hus inputs conventional supervised bp classifier trained recognize vowels code 
classifier architecture gamma number pruned hus aa 
hidden output units sigmoid active receive additional bias input 
classifier trained pattern presentations 
parameters 
aa net learning rate tol delta fl 
backprop classifier learning rate 
overfitting 
confirm robinson results classifier tends overfit trained simple bp learning test error rate decreases increases 
exp input method code 
code type field comp 
error bars theta loc sparse factorial bars theta ica sparse bars theta pca dense bars theta ica dense bars theta pca dense noisy bars theta loc sparse factorial noisy bars theta ica sparse noisy bars theta pca dense noisy bars theta ica dense noisy bars theta pca dense village image theta loc sparse village image theta ica sparse village image theta pca dense village image theta ica sparse village image theta pca dense village image theta loc sparse village image theta ica dense village image theta pca dense village image theta ica dense village image theta pca dense wood cell image theta loc sparse wood cell image theta ica sparse wood cell image theta pca sparse wood cell image theta ica sparse wood cell image theta pca dense wood piece image theta loc sparse wood piece image theta ica sparse wood piece image theta pca sparse wood piece image theta ica sparse wood piece image theta pca sparse table overview experiments name experiment input field size coding method number relevant code components code size reconstruction error nature code observed test set 
pca ica code sizes 
lococode automatically 
comparison 
compare various neural nets see table 
nearest neighbor classifies item belonging class closest example training set euclidean distance 
lda linear discriminant analysis 
softmax observation assigned class best fit value 
quadratic discriminant analysis observations classified belonging class closest centroid mahalanobis distance class specific covariance matrix 
cart classification regression tree coordinate splits default input parameter values 
fda flexible discriminant analysis additive models adaptive selection terms splines smoothing parameters 
provides set basis functions better class separation 
softmax best fit value classification 
fda mars fda multivariate adaptive regression splines 
mars builds basis expansion better class separation 
softmax mars best fit value classification mars 
lococode backprop unsupervised codes generated lococode fms fed conventional overfitting bp classifier 
technique nr 
hidden error rates units training test single layer perceptron multi layer perceptron multi layer perceptron multi layer perceptron modified kanerva model modified kanerva model radial basis function radial basis function gaussian node network gaussian node network gaussian node network gaussian node network square node network square node network square node network nearest neighbor lda softmax qda cart cart linear comb 
splits fda softmax fda mars degree fda mars degree softmax mars degree softmax mars degree lococode backprop table vowel recognition task generalization performance different methods 
surprisingly fms generated fed conventional overfitting backprop classifier led excellent results 
see text details 
results 
see table 
fms generates different 
fed bp classifiers different weight initializations table entry lococode backprop represents mean trials 
results neural nets nearest neighbor taken robinson 
results lococode taken hastie 

method led excellent generalization results 
error rates bp learning vary 
backprop fed lococode code goes error rate due overfitting error rate increases course test set performance may influence training procedure 
bp naive approach quite surprising excellent generalization performance obtained just feeding bp specific 
typical feature detectors 
number pruned hus constant activation varies 
hus binary 
codes observed apparently certain hus feature detectors speaker identification 
hu activation near words heed hid sounds 
hu activation high values words hod hoard hood words low nonzero values hard heard 
lococode supports feature detection 
sparse code 
real valued input components described precisely activations feature detectors generated lococode 
additional real valued hus necessary representing missing information 
better results additional information 
hastie obtained additional slightly better results fda mars variant average error rate 
mentioned data subject goal directed preprocessing splines clearly defined classes 
furthermore determine input dimension hastie special kind generalized cross validation error constant obtained unspecified simulation studies 
hastie tibshirani obtained average error rate discriminant adaptive nearest neighbor classification 
error rate obtained flake rbf networks hybrid architectures 
experiments conducted time review showed better results obtained additional context information improve classification performance turney herrmann tenenbaum freeman 
overview see schraudolph 
interesting combine methods lococode 

attempt preventing classifier overfitting achieved excellent results 
conclude fed classifier conveyed essential noise free information necessary excellent classification 
led believe lococode promising method data preprocessing 
lococode novel approach unsupervised learning sensory coding define code optimality solely properties code takes account information theoretic complexity mappings coding decoding 
resulting typically compromise conflicting goals 
tend sparse exhibit low minimal redundancy costs generating high 
tend binary informative feature detectors occasionally continuous valued code components complexity considerations suggest alternatives 
general principle 
analysis lococode essentially attempts describing single inputs simple features possible 
depending statistical properties input result local factorial sparse codes biologically plausible sparseness common case 
objective functions previous methods olshausen field lococode contain explicit term enforcing say sparse codes sparseness viewed things priori 
suggest lococode objective may embody general principle unsupervised learning going previous specialized ones 
regularizers unsupervised learning 
way looking results representative fms broad class algorithms regularizer algorithms reduce net complexity optimal feature extraction product 
interesting previously ignored connection important fields regularizer research ica related research 
advantages 
lococode appropriate single inputs input components described features computable simple functions 
assuming visual data reduced simple causes lococode appropriate visual coding 
simple ica lococode inherently limited linear case need priori information number independent data sources 
coding problem linear number sources known lococode outperform coding methods 
demonstrated lococode implementation fms trained aas easily solves linear coding tasks described hard authors input causes perfectly separable standard aas pca ica 
furthermore applied realistic visual data lococode produces familiar surround receptive fields biologically plausible sparse codes standard aas 
experiments demonstrate utility lococode data preprocessing subsequent classification 
limitations 
fms order computational complexity depends number output units 
typical classification tasks requiring output units equals standard backprop 
aa case output dimensionality grows input 
large scale fms trained aas require parallel implementation 
furthermore lococode works visual inputs may useful discovering input causes represented high input transformations discovering features causes collectively determining single input components acoustic signal separation ica suffer fact source influences input component computable low complexity function 

encouraged familiar obtained experiments visual data intend move higher dimensional inputs larger receptive fields 
may lead pronounced feature detectors observed schmidhuber 

interesting test successive lococode stages feeding code lead complex feature detectors discovered deeper regions mammalian visual cortex 
encouraged successful application vowel classification intend look complex pattern recognition tasks 
intend look alternative lococode implementations fms aas 
improve understanding relationship low complexity codes low complexity art see schmidhuber informal notions beauty art 
acknowledgments peter dayan manfred opper nic schraudolph helpful discussions comments draft 
results realistic visual data obtained modification code written baumgartner diploma thesis 
supported dfg deutsche forschungsgemeinschaft 
schmidhuber acknowledge support snf predictability minimization 
amari cichocki yang 

new learning algorithm blind signal separation 
touretzky mozer hasselmo editors advances neural information processing systems pages 
mit press cambridge ma 
baldi hornik 

neural networks principal component analysis learning examples local minima 
neural networks 
barlow 

understanding natural vision 
springer verlag berlin 
barlow mitchison 

finding minimum entropy codes 
neural computation 
barrow 

learning receptive fields 
proceedings ieee st annual conference neural networks volume iv pages 
ieee 
baumgartner 

mit 
diploma thesis institut fur informatik lehrstuhl prof brauer technische universitat munchen 
becker 

unsupervised learning procedures neural networks 
international journal neural systems 
bell sejnowski 

information maximization approach blind separation blind deconvolution 
neural computation 
cardoso 


blind beamforming non gaussian signals 
iee proceedings 
dayan zemel 

competition multiple cause models 
neural computation 
deco parra 

nonlinear features extraction unsupervised redundancy reduction stochastic neural network 
technical report siemens ag st sn 
demers cottrell 

non linear dimensionality reduction 
hanson giles editors advances neural information processing systems pages 
morgan kaufmann san mateo ca 
field 

relations statistics natural images response properties cortical cells 
journal optical society america 
field 

goal sensory coding 
neural computation 
flake 

square unit augmented radially extended multilayer perceptrons 
orr muller editors tricks trade 
springer verlag berlin 
appear lecture notes computer science 
ak 

forming sparse representations local anti hebbian learning 
biological cybernetics 
ak young 

sparse coding primate cortex 
arbib editor handbook brain theory neural networks pages 
mit press cambridge massachusetts 
ghahramani 

factorial learning em algorithm 
tesauro touretzky leen editors advances neural information processing systems pages 
mit press cambridge ma 
hassibi stork 

second order derivatives network pruning optimal brain surgeon 
hanson giles editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
hastie tibshirani 

discriminant adaptive nearest neighbor classification 
ieee transactions pattern analysis machine intelligence 
hastie tibshirani buja 

flexible discriminant analysis optimal scoring 
technical report bell laboratories 
herrmann 

merits topography neural maps 
kohonen editor proceedings workshop self organizing maps pages 
helsinki university technology 
hinton dayan frey neal 

wake sleep algorithm unsupervised neural networks 
science 
hinton ghahramani 

generative models discovering sparse distributed representations 
technical report university toronto department computer science toronto ontario canada 
modified version appear philosophical transactions royal society hinton zemel 

minimum description length helmholtz free energy 
cowan tesauro alspector editors advances neural information processing systems pages 
morgan kaufmann san mateo ca 
hochreiter schmidhuber 

simplifying nets discovering flat minima 
tesauro touretzky leen editors advances neural information processing systems pages 
mit press cambridge ma 
hochreiter schmidhuber 

flat minima 
neural computation 
hochreiter schmidhuber 

low complexity coding decoding 
wong king yeung editors theoretical aspects neural computation hong kong pages 
springer 
hochreiter schmidhuber 

unsupervised coding lococode 
gerstner nicoud editors proceedings international conference artificial neural networks lausanne switzerland pages 
springer 
kohonen 

self organization associative memory 
springer second ed 
kramer 

nonlinear principal component analysis autoassociative neural networks 
aiche journal 
li 

theory visual motion coding primary visual cortex 
neural computation 
linsker 

self organization perceptual network 
ieee computer 
schuster 

separation independent signals time delayed correlations 
phys 
reviews letters 
mozer 

discovering discrete distributed representations iterative competitive learning 
lippmann moody touretzky editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
oja 

neural networks principal components subspaces 
international journal neural systems 
oja 

data compression feature extraction autoassociation feedforward neural networks 
kohonen simula kangas editors artificial neural networks volume pages 
elsevier science publishers north holland 
olshausen field 

emergence simple cell receptive field properties learning sparse code natural images 
nature 
palm 

information storage capacity local learning rules 
neural computation 
redlich 

redundancy reduction strategy unsupervised learning 
neural computation 
rissanen 

modeling shortest data description 
automatica 
robinson 

dynamic error propagation networks 
phd thesis trinity hall cambridge university engineering department 
rumelhart zipser 

feature discovery competitive learning 
parallel distributed processing pages 
mit press 
saund 

unsupervised learning mixtures multiple causes binary data 
cowan tesauro alspector editors advances neural information processing systems pages 
morgan kaufmann san mateo ca 
saund 

multiple cause mixture model unsupervised learning 
neural computation 
schmidhuber 

learning factorial codes predictability minimization 
neural computation 
schmidhuber 

discovering neural nets low kolmogorov complexity high generalization capability 
neural networks 
press 
schmidhuber 

low complexity art 
leonardo journal international society arts sciences technology 
schmidhuber 

semilinear predictability minimization produces known feature detectors 
neural computation 
schmidhuber 

discovering predictable classifications 
neural computation 
schraudolph 

centering neural network weight updates 
orr muller editors tricks trade 
springer verlag berlin 
appear lecture notes computer science 
schraudolph sejnowski 

unsupervised discrimination clustered data optimization binary information gain 
hanson giles editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 
solomonoff 

formal theory inductive inference 
part information control 
tenenbaum freeman 

separating style content 
mozer jordan petsche editors advances neural information processing systems pages 
mit press cambridge ma 
turney 

exploiting context learning classify 
proceedings european conference machine learning pages 
ftp ai iit nrc ca pub ksl papers nrc 
ps wallace boulton 

information theoretic measure classification 
computer journal 
watanabe 

pattern recognition human mechanical 
new york 
zemel 

minimum description length framework unsupervised learning 
phd thesis university toronto 
zemel hinton 

developing population codes minimizing description length 
cowan tesauro alspector editors advances neural information processing systems pages 
san mateo ca morgan kaufmann 

