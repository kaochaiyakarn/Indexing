combining conjugate direction methods stochastic approximation gradients nicol schraudolph graepel institute computational sciences technische hochschule eth ch urich switzerland www ethz ch method conjugate directions provides ective way optimize large deterministic systems gradient descent 
standard form amenable stochastic approximation gradient 
explore ideas conjugate gradient stochastic online setting fast hessian gradient products set low dimensional krylov subspaces individual mini batches 
benchmark experiments resulting online learning algorithms converge orders magnitude faster ordinary stochastic gradient descent 
conjugate gradient optimization large di erentiable systems algorithms require inversion curvature matrix levenberg marquardt storage iterative approximation inverse quasi newton methods bfgs see press prohibitively expensive 
conjugate gradient methods stiefel exactly minimize dimensional unconstrained quadratic problem iterations requiring explicit knowledge curvature matrix method choice problems 
stochastic gradient empirical loss functions minimized noisy measurements gradient applicable curvature taken small random subsamples data individual data points 
proceedings ninth international workshop arti cial intelligence statistics appear jan 
done reasons computational eciency large redundant data sets necessity adapting online continual stream noisy potentially non stationary data 
unfortunately fast convergence conjugate gradient breaks function optimized noisy impossible maintain conjugacy search directions multiple iterations 
state art stochastic problems simple gradient descent coupled adaptation local step size momentum parameters 
efficient curvature matrix vector products advanced parameter adaptation methods stochastic gradient descent orr schraudolph graepel schraudolph rely fast curvature matrix vector products obtained eciently automatically pearlmutter schraudolph 
calculation require explicit storage hessian goes measures curvature gauss newton approximation hessian fisher information matrix schraudolph 
algorithmic di erentiation software provides generic implementations building blocks algorithms constructed 
employ techniques eciently compute hessian gradient products implement stochastic conjugate direction method 
stochastic quadratic optimization deterministic bowl dimensional quadratic bowl provides simpli ed test setting aspect op see www unix mcs anl gov timization controlled 
de ned unconstrained problem minimizing respect parameters function jj jacobian matrix location minimum choosing 
de nition hessian jj positive semidefinite constant respect parameters crucial simpli cations compared realistic non linear problems 
gradient rf 
stochastic bowl stochastic optimization problem analogous deterministic minimization respect function matrix collecting batch random input vectors system drawn normal distribution 
means xx expectation identical deterministic formulation ex je xx optimization problem harder objective probed supplying stochastic inputs system giving rise noisy estimates rw true hessian gradient respectively 
degree stochasticity determined batch size system deterministic limit 
line search common optimization technique rst determine search direction look optimum direction 
quadratic bowl step minimum direction 
hv hv calculated eciently pearlmutter stochastic settings 
line search gradient direction called steepest descent 
fully stochastic steepest descent degenerates normalized lms method known signal processing 
distribution stochastic gradient steps equivalent starting points crosses circles right vs ellipses left scaled arbitrarily line search ill conditioned quadratic bowl 
black high white low probability density 
compare deterministic steepest descent arrows 
choice jacobian experiments choose hessian eigenvalues widely di ering magnitude eigenvectors intermediate sparsity 
conditions model mixture axis aligned oblique narrow valleys characteristic multi layer perceptrons primary cause diculty optimizing systems 
achieve imposing sparsity notoriously ill conditioned hilbert matrix de ning ij def mod mod call optimization problem resulting setting matrix modi ed hilbert bowl 
experiments reported modi ed hilbert bowl dimension condition number 
stochastic ill conditioning ill conditioned systems particularly challenging stochastic gradient descent 
directions associated large eigenvalues rapidly optimized progress oor valley spanned small eigenvalues extremely slow 
line search ameliorate problem amplifying small gradients happen search direction lie valley oor rst place 
stochastic setting gradients direction just small extremely contrast deterministic gradients stochastic gradients contain large components directions associated large eigenvalues points right bottom valley 
illustrates consequence line search stretch narrow ellipses possible stochastic gradient steps circles minimum shift probability mass direction 
hg construction conjugate direction subtracting gradient projection hg 
stochastic conjugate directions looking ways improve convergence stochastic gradient methods narrow valleys note relevant directions associated large eigenvalues identi ed multiplying stochastic estimates hessian gradient system 
subtracting projection hg yields conjugate descent direction emphasizes directions associated small eigenvalues virtue orthogonal hg hg hg shows stochastic descent direction dotted sports better late convergence steepest descent dashed 
directions large eigenvalues subtracted takes far longer reach valley oor rst place 
dimensional method combine respective strengths gradient conjugate direction performing stochastic iteration dimensional minimization plane spanned hg 
seek produce optimal step 
hg def gives 
hg 
express optimality condition system linear equations quadratic forms def 

solving yields shows technique dot dashed combines advantages gradient conjugate directions 
hg hg normal cg log log plot average loss runs vs number stochastic iterations modi ed hilbert bowl minimizing direction dashed direction dotted plane spanned hg dot dashed subspace spanned hg dot dot dashed 
compare normal conjugate gradient mini batches solid line 
stochastic krylov subspace approach extended performing minimizations dimensional stochastic krylov subspace km def hg min 
expansion 
km 
optimality require def 
def 
def 
bring form standard toeplitz system solved little log operations press 
numerically iteration simply precomputed analytic solution 
analytic solution rst order system steepest descent evident third order system quadratic forms required solution strategy calculated eciently inner products fast hessian vector products dot dot dashed illustrates rapid convergence approach 
relation conjugate gradient methods escaped notice quadratic optimization problems solving linear system equations explicitly equivalently perform steps ordinary conjugate gradient mini batch nd optimum krylov subspace km single dimensional optimization successive line searches 
initial search direction set gradient subsequent ones calculated formula hv hv known variants polak see press 
crucial di erence standard conjugate gradient techniques propose perform just steps conjugate gradient small stochastic mini batch 
reset gradient direction moving mini batch recommended mandatory approach stochasticity collapses krylov subspace 
illustrate show solid line inadequate performance standard conjugate gradient fashion 
non realizable problems stochastic quadratic bowl models realizable problems loss optimum reaches zero inputs greater practical relevance non realizable problems optimum carries non zero loss re ecting best compromise con icting demands placed model data 
model incorporating multivariate gaussian random adaptive log log plot average excess loss runs non realizable modi ed hilbert bowl number cycles stochastic krylov subspace method adaptive solid vs various xed batch sizes dashed 
shown steepest descent adaptive batch size dotted 
vector zero mean variance rr objective def expected loss optimum presence impossible determine precisely nite data sample smaller sample size greater uncertainty conventional stochastic gradient methods address issue multiplying gradient step rate factor gradually annealed zero ectively averaging gradient progressively larger stretches data 
invest signi cantly greater computational ort proportional order krylov subspace learning batch data partial limit nitesimal steps inferred goal attractive 
propose adaptively increase batch size time 
speci cally fail reduce loss compared previous batch data know due uncertainty consequently double batch size obtain better estimate 
shows performance resulting algorithm solid non realizable modi ed hilbert regions benchmark left neural network trained right 
bowl initial batch size 
fair comparison various xed batch sizes dashed steepest descent dotted plot excess loss number cycles obtained multiplying number data points seen krylov subspace order stationary optimization problem simple batch size adaptation heuristic successful obtaining rapid initial convergence characteristic small batch sizes eliminating noise oor limits asymptotic performance xed batch size 
see stochastic krylov subspace approach continues perform non realizable case greatly outperforms steepest descent convergence increasing subspace order min 
note passing expected stochastic setting naive implementation conjugate gradient single iteration batch reset batches performs poorly 
non linear problems extension techniques non linear optimization problems online learning multi layer perceptrons raises additional questions 
case conjugate gradient methods equivalent explicit solution raising question preferable stochastic gradient setting 
approaches protected near zero negative eigenvalues ller schraudolph 
report rst experiments designed exploring issues 
regions benchmark regions benchmark singhal wu extensive experience accelerated stochastic gradient methods schraudolph graepel schraudolph 
fully connected feedforward multi layer perceptron hg hg normal cg log log plot average loss runs regions problem number cycles stochastic krylov subspace method 
compare normal conjugate gradient solid stochastic meta descent dotted 
hidden layers units right classify continuous inputs range disjoint non convex regions left 
standard softmax output nonlinearity cross entropy loss function bishop chapter hyperbolic tangent hidden units 
run weights including bias weights units initialized uniformly random values range 
training patterns generated online drawing independent uniformly random input samples 
obtain unbiased estimate generalization ability record network loss batch samples optimization 
non linear optimization setup permit batch size adaptation batch size 
avoid near zero negative eigenvalues extended gauss newton approximation hessian schraudolph 
clear methods vary trust region parameter ller adapted stochastic setting simply smallest xed value provide reasonably stable convergence 
results shows batch size remaining stochasticity problem causes conventional conjugate gradient solid line peter loss 
contrast stochastic krylov subspace method implemented explicit solution toeplitz system continues reduce loss converging rate increases order due large batch size compare stochastic meta descent schraudolph local learning rate adaptation method far smaller mini batches dotted line parameters 
increase may help narrow gap 
performing steps conjugate gradient batch simple line search resulted performance worse conventional conjugate gradient solid line 
accurate iterative line search bound improve albeit signi cant computational expense directly solving toeplitz system involve costly line searches may preferable 
summary outlook considered problem stochastic ill conditioning optimization problems leads ciency standard stochastic gradient methods 
geometric arguments arrived conjugate search directions eciently fast products 
resulting algorithm interpreted stochastic conjugate gradient technique introduces krylov subspace methods stochastic optimization 
numerical results show approach outperforms standard gradient descent unconstrained quadratic optimization problems realizable non realizable orders magnitude stochastic setting standard conjugate gradient fails 
experiments regions benchmark indicate approach improves standard conjugate gradient methods non linear problems 
working advance implementation non linear optimization incorporating adaptation batch size trust region parameters toeplitz solver orders 
interested developing better methods controlling batch size subspace order simple heuristic employed section 
acknowledgments want gert lanckriet discussion learning workshop anonymous referees helpful comments eth urich nancial support 
earlier account linear realizable case schraudolph graepel 
bishop 
neural networks pattern recognition 
clarendon press oxford 
editor 
proc 
intl 
conf 
arti cial neural networks volume lecture notes computer science madrid spain 
springer verlag berlin 
graepel schraudolph 
stable adaptive momentum rapid online learning nonlinear systems 
pages 
www inf ethz ch pubs sam ps gz 
stiefel 
methods conjugate gradients solving linear systems 
journal research national bureau standards 
levenberg 
method solution certain non linear problems squares 
quarterly journal applied mathematics ii 
marquardt 
algorithm squares estimation non linear parameters 
journal society industrial applied mathematics 
ller 
scaled conjugate gradient algorithm fast supervised learning 
neural networks 
orr 
dynamics algorithms stochastic learning 
phd thesis department computer science engineering oregon graduate institute beaverton 
ftp neural cse ogi 
edu pub neural papers ps ps pearlmutter 
fast exact multiplication hessian 
neural computation 
press teukolsky vetterling flannery 
numerical recipes art scienti computing 
cambridge university press second edition 
schraudolph 
local gain adaptation stochastic gradient descent 
proc 
intl 
conf 
arti cial neural networks pages edinburgh scotland 
iee london 
www inf ethz ch pubs ps gz 
schraudolph 
fast curvature matrix vector products second order gradient descent 
neural computation 
www inf 
ethz ch pubs mvp ps gz 
schraudolph graepel 
conjugate directions stochastic gradient descent 
pages 
www inf ethz ch pubs hg ps gz 
singhal wu 
training multilayer perceptrons extended kalman lter 
touretzky editor advances neural information processing systems 
proceedings conference pages san mateo ca 
morgan kaufmann 
