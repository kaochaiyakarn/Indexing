loosely tree alignment machine translation augment model translation re ordering nodes syntactic trees order allow alignments conforming original tree structure keeping computational complexity polynomial sentence length 
done adding new subtree cloning operation tree string tree tree alignment algorithms 
systems automatic translation languages divided transfer approaches rely interpreting source string semantic representation text generated target language statistical approaches pioneered brown 
estimate parameters model word word correspondences word re orderings directly large corpora parallel bilingual text 
hybrid approaches begun emerge apply probabilistic models structured representation source text 
wu showed restricting word level alignments sentence pairs observe syntactic bracketing constraints significantly reduces complexity alignment problem allows polynomial time solution 
alshawi 
induce parallel tree structures parallel text modeling generation node children finite state transducer 
yamada knight algorithm estimating probabilistic parameters similar model represents translation sequence re ordering operations children nodes syntactic tree automatic parser output initial tree structures 
explicit daniel gildea university pennsylvania cis upenn edu syntactic information target language model led excellent translation results yamada knight raises prospect training statistical system syntactic information sides parallel corpus 
tree tree alignment techniques probabilistic tree substitution grammars haji trained parse trees parallel treebanks 
real bitexts generally exhibit parse tree isomorphism systematic differences languages express concept syntactically dorr simply relatively free translations training material 
introduce loosely tree alignment techniques address problem 
analogous extensions tree string tree tree models allow alignments obeying constraints original syntactic tree tree pair alignments incur cost probability 
achieved introducing clone operation copies entire subtree source language syntactic structure moving target language sentence 
careful parameterization probability model allows estimated additional cost computational complexity 
expect relatively unconstrained clone operation allow various types structural divergence providing sort hybrid tree unstructured ibm style models 
tree string model followed tree tree model moving alignment results parallel syntactically annotated korean english corpus measured terms alignment perplexities held test data agreement human annotated word level alignments 
tree string model summarizing model yamada knight thought representing translation alexander calder mobile 
follow process english sentence transformation french english sentence syntactic tree representation statistical parser collins 
step translation process children node tree re ordered 
node children 
re orderings possible assigned probability conditioned syntactic categories parent node children 
second step french words inserted node parse tree 
insertions modeled steps predicting insertion left insertion right insertion takes place conditioned syntactic category node parent 
second step choice inserted word pins predicted conditioning information 
final step french translation original english word leaves tree chosen distribution pt 
french word predicted conditioned english word english word generate french word generate null symbol representing deletion 
original tree re ordering insertion translation probabilities node independent choices node 
independence relations analogous stochastic context free grammar allow efficient parameter estimation inside outside expectation maximization em algorithm 
computation inside probabilities outlined considers possible reordering nodes original tree bottom manner nodes input tree orderings children partitions span fl algorithm computational complexity maximum number children node input tree length input string 
storing partially completed arcs chart interleaving inner loops complexity achieved 
algorithm fan grammar polynomial size input string 
model efficiency comes cost 
independence assumptions alignments source target sentences simply represented 
minimal example take tree possible re orderings terminals involve crossing bracketing original tree allowed 
constraint gives way syntactic information translation may cases rigid 
part deal problem yamada knight flatten trees pre processing step collapsing nodes lexical head word 
allows example english subject verb object svo structure analyzed having vp node spanning verb object re ordered language arabic 
larger syntactic divergences trees may require relaxation constraint practice expect divergences frequent 
example nominal modifier language may show adverbial due choices information represented main verb syntactic correspondence sentences may break completely 
tree string clone operation order provide flexibility modify model order allow copy translated subtree english sentences occur cost point resulting french sentence 
example case input tree clone operation making copy node new child produce tree nnc ul np nnc ci null np nnc pad vp vv pat null lv vv pau vp ci null nnc vp su kap np nnc np pca vp re pairs vp np re nnc su kap gloves np nnc original korean parse tree transformed tree reordering children subtree cloning indicated arrow word translation 
insertion operation shown tree english yield pairs gloves issued winter 
pca vp null vp np nnc ci vv pat lv vv vp ci vv pat vp lv vv np ci nnc ci issued np pad nnc pau null nnc ul winter operation combined deletion original node produces alignment disallowed original tree reordering model 
shows example korean english corpus clone operation allows model handle case wh movement english sentence realized reordering subtrees korean parse 
probability adding clone original node child node calculated steps choice insert clone probability pins clone choice original node copy probability clone probability original node producing copy 
implementation simplicity pins constant estimated em algorithm conditioned parent node constant meaning node copied chosen nodes original tree uniform probability 
important note dependent clone node question node may reused number times 
independence assumption crucial computational tractability algorithm model estimated dynamic programming method keeping counts expected number times node cloned increase computational complexity 
assumption parameter estimation problem parsing crossing dependencies exponential length input string barton 
special case subtree cloning model consider model allows individual words leaves tree cloned 
achieved simply probability word appearing insertion pins include possible translations source words ins pins pt ei pins unconditioned word insertion probability original model pt original model word word translation probability french word english word insertion operations broken prediction insertion takes place node ins prediction inserted word ins 
em procedure parameter estimation extended order maintain counts expected number times insertion operations result priori insertion distribution pins expected counts insertions result translation operation allowing estimate mixture parameter statistically sound manner model parameters increase computational complexity 
tree tree model tree tree alignment model tree transformation operations similar tree tostring model described 
transformed tree match surface string target language tree structure assigned string treebank annotators 
order provide flexibility possible additional tree transformation operations allow single node source tree produce nodes target tree nodes source tree grouped produce single node target tree 
model thought synchronous tree substitution grammar probabilities parameterized generate target tree conditioned structure source tree 
probability tb ta transforming source tree ta target tree tb modeled sequence steps proceeding root target tree 
level tree 
current node children grouped current node single elementary tree probability 

alignment children current elementary tree chosen probability 
alignment operation similar reorder operation tree string model extension nodes source target may correspond node side representing insertions deletions case nodes target tree grouped children re ordered step 
final step process tree tostring model lexical items leaves tree translated target language distribution pt 
order generate complete target tree step necessary choose structure target side specifically elementary tree nodes labels nodes nodes child attaches second 
ultimately interested predicting correct target string regardless structure assign probabilities steps 
nonterminals target side ignored entirely alignment algorithm considers possible pairs nodes elementary trees target side training generative probability model thought generating single nodes target side 
alignment algorithm constrained bracketing target side generate entire target tree structure 
allowing non correspondences nodes trees necessary handle fact depth corresponding words trees differs 
consequence allowing elementary trees size reorderings allowed reordering children individual node separately possible 
example simple tree nodes considered elementary tree collective children reordered step producing target tree desired word alignment 
computational complexity data sparsity prevent considering arbitrarily large elementary trees number nodes considered limits possible alignments 
example maximum nodes transformation tree capable generating alignment 
probability model tree transformation operates top tree probability estimation aligning trees takes place iterating pairs nodes tree bottom order sketched nodes source tree ta bottom order elementary trees ta rooted nodes target tree tb bottom order elementary trees tb rooted alignments children ta tb ta outer loops iterating nodes tree require 
restrict elementary trees include child root node side choosing elementary trees node pair refers maximum number children node 
computing alignment children elementary tree side requires choosing subset source nodes delete subset target nodes insert clone reorder remaining nodes source target tree 
complexity algorithm quadratic size input sentences exponential fan grammar 
tree tree clone operation tree tree alignment model imposes stricter constraints possible word level alignments tree string model 
tree tostring model alignment possible results crossing dependencies projecting source tree target string 
furthermore crossing dependencies allowed projecting target tree back source tree 
leads think may helpful allow departures strict tree model done principled manner dramatically increasing computational complexity 
reason introduce clone operation allows copy node source tree target tree 
clone operation takes place transformation source target tree takes place tree decomposition subtree alignment operations 
basic algorithm previous section remains unchanged exception alignments children elementary trees include cloned inserted nodes target side 
specifies new cloned node child choice node clone tree string model clone 
node source tree cloned equal probability regardless probability clone operation computed dynamic programming assumptions basic tree tree model 
tree string cloning operation independence assumption essential keep complexity polynomial size input sentences 
data experiments parallel korean english corpus military domain han 
syntactic trees annotated hand korean english sentences korean trees modeling transformation english text 
corpus contains sentences training data holding sentences evaluation 
average korean sentence length words 
korean agglutinative language words contain sequences meaning bearing suffixes 
purposes model represented syntax trees fairly aggressive tokenization breaking words separate leaves tree 
gave average tokens korean sentences 
average english sentence length 
maximum number children node korean trees corresponds comma separated list items 
korean trees children node children children 
vocabulary size number unique types words english korean splitting multi words korean vocabulary size 
reasons computation speed trees children excluded experiments described 
experiments evaluate translation models terms alignment perplexity held data agreement human annotated word level alignments sentence pairs 
perplexity geometric mean reciprocal probability assigned model test data normalized sentence length pp log ei ki ei ki ith korean sentence ei english translation 
perplexity thought number possible words predicted model position lower perplexity indicates better fit data 
scoring viterbi alignments system gold standard annotated alignments alignment error rate aer och ney measures agreement level pairs words aer set word pairs aligned automatic system set aligned gold standard 
provide comparison treebased models sequence successively complex models brown 

results shown table 
held data control training process halting training perplexity held data begins increase order prevent overfitting 
test perplexities shown table represent minimum model 
models ibm models brown 

tree string model yamada knight tree string clone allows node cloning operation section tree string insert allows cloning lexical leaf nodes tree 
shows viterbi alignment produced tree string clone system sentence test set 
tree tree indicates model section tree tree clone adds node cloning operation section 
model initialized parameters model model och ney differentiate sure possible hand annotated alignments gold standard alignments come variety 
alignment perplexity alignment full viterbi error rate train test train test aer model model model tree string tree string clone tree string insert tree tree tree tree clone table alignment perplexities alignment error rate korean english corpus initialized model 
lexical translation probabilities pt tree models initialized model node reordering probabilities initialized uniformly 
columns headed full represent perplexities summing possible alignments pair sentences model subset considered reasons computational complexity 
viterbi perplexities single highest probability alignment case model guarantee optimal alignment 
ibm models exhibit significant overfitting relatively small corpus 
model despite limited representational capacity lower perplexity held data model model particular wildly overfitting training data 
data set tree string model outperforms unmodified tree string model ibm models terms perplexity alignment error 
conclude tree tostring models introduce additional parameters model assigns possible alignments uniform probability principled manner models help prevent overfitting limited data 
loose tree model thought hybrid model strict tree model better 
clone operation results better performance tree string insert model indicating alignments conform strict tree reordering model helpful allow subtrees original syntactic structure move unit 
measure degree model departs strict tree re ordering mixture parameter equation set em algorithm probability insertion place 
probability mass goes tree re ordering ar alignments produced insertions insertions take place predominantly considered drawn translations words source sentence 
tree tree models predict target string bracketing perplexities significantly higher ibm tree string models 
adding clone operation tree tree model significantly improves perplexity alignment error equivalent ibm model alignment error 
loosely tree alignment techniques allow statistical models machine translation syntactic information retaining flexibility handle cases non isomorphic source target trees 
achieved clone operation parameterized way alignment probabilities computed increase asymptotic computational complexity 
versions technique string models making parse trees languages tree tree models parallel parse trees 
results terms perplexity alignment error rate indicate clone operation result better alignments cases 
tree string models outperform unstructured ibm models corpus tree tree models substantially higher perplexities equivalent alignment error 
remain hopeful trees provide information feel extensions loosely tree approach may provide way integrating information 
important question plan pursue degree results borne larger corpora models may refined training data available 
ex ample tree representation expect conditioning model lexical information improve results done percolating lexical heads existing trees switching strict dependency representation 
alshawi srinivas bangalore douglas 

learning dependency translation models collections finite state head transducers 
computational linguistics 
edward barton jr 
complexity id lp parsing 
computational linguistics 
peter brown john cocke stephen della pietra vincent della pietra frederick jelinek john lafferty robert mercer paul 

statistical approach machine translation 
computational linguistics june 
peter brown stephen della pietra vincent della pietra robert mercer 

mathematics statistical machine translation parameter estimation 
computational linguistics 
michael john collins 

head driven statistical models natural language parsing 
ph thesis university pennsylvania philadelphia 
bonnie dorr 

machine translation divergences formal description proposed solution 
computational linguistics 
jan haji martin bonnie dorr yuan ding jason eisner daniel gildea terry koo gerald penn dragomir radev owen rambow 

natural language generation context machine translation 
technical report center language speech processing johns hopkins university baltimore 
summer workshop final report 
chung han na rae han eon ko 

bracketing guidelines penn korean treebank 
technical report ircs ircs university pennsylvania 
franz josef och hermann ney 

improved statistical alignment models 
proceedings th annual conference association computational linguistics acl pages hong kong october 
wu 

stochastic inversion transduction grammars bilingual parsing parallel corpora 
computational linguistics 
kenji yamada kevin knight 

statistical translation model 
proceedings th annual conference association computational linguistics acl toulouse france 
kenji yamada kevin knight 

decoder syntax statistical mt proceedings th annual conference association computational linguistics acl philadelphia pa 
