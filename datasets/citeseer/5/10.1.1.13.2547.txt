growing hierarchical self organizing map exploratory analysis high dimensional data andreas rauber dieter merkl michael department software technology interactive systems vienna university technology 
vienna austria mail dieter ifs tuwien ac self organizing map popular unsupervised neural network model analysis high dimensional input data data mining applications 
limitations noted related hand static architecture model hand limited capabilities representation hierarchical relations data 
novel growing hierarchical selforganizing map address limitations 
growing hierarchical self organizing map arti cial neural network model hierarchical architecture composed independent growing self organizing maps 
motivation provide model adapts architecture unsupervised training process particular requirements input data 
furthermore providing global orientation independently growing maps individual layers hierarchy navigation branches facilitated 
bene ts novel neural network rst problem dependent architecture second intuitive representation hierarchical relations data 
especially appealing explorative data mining applications allowing inherent structure data unfold highly intuitive fashion 
keywords self organizing map som data mining hierarchical clustering exploratory data analysis pattern recognition 
data mining generally pattern recognition knowledge acquisition heavily depend suitable unsupervised learning methods 
purpose methods develop optimal partitioning clustering data set analyzed 
cluster analysis organization collection patterns usually represented vectors measurements points multidimensional space clusters similarity 
intuitively patterns valid cluster similar pattern belonging di erent cluster :10.1.1.18.2720
words objective unsupervised learning methods data mining applications identify groupings unlabeled set data vectors share semantic similarities 
helps user build cognitive model data fostering detection inherent structure interrelationship data 
applications little prior information underlying models data available 
situation clustering provides particularly appropriate approach analysis data 
self organizing map som widely tool mapping high dimensional data dimensional representation space 
mapping retains relationship input data faithfully possible describing topology preserving representation input similarities terms distances output space 
possible visually identify clusters map 
main advantage mapping ease user gains idea regarding structure data analyzing map 
diculties som utilization remained largely untouched large number research papers applications som years 
som uses xed network architecture terms number arrangement neural processing elements de ned prior training 
obviously case largely unknown input data characteristics remains far trivial determine network architecture provides satisfying results 
certainly worth considering neural network models determine number arrangement units unsupervised training process 
refer proposed models som allow adaptation network architecture training 
second hierarchical relations input data mirrored straight forward fashion 
relations shown representation space hard identify 
hierarchical relations may observed wide spectrum application domains 
proper identi cation remains highly important data mining task addressed conveniently framework som resolve limitations som uniform fashion propose novel arti cial neural network architecture growing hierarchical self organizing map ghsom 
model hierarchical architecture som neural networks adaptive architecture build various layers hierarchy 
size som neural networks depth hierarchy ghsom determined unsupervised training process structure data 
hierarchical structuring imposed data results separation clusters mapped different branches 
principle desirable characteristic helping understand cluster structure data may lead large clusters mapped expanded neighboring di erent units 
similar input data arbitrarily separated di erent branches hierarchy 
choosing initial orientation deeper layers respective higher layer maps maintain learned similarities input data creation hierarchical structure ghsom consequence negative ects generating strictly disjoint clusters eliminated neighboring maps deeper layers hierarchy show similar characteristics respective borders 
show usefulness growing hierarchical self organizing map application document archive organization 
document archives represent convenient application scenario nature represented highdimensional data 
particular show results experiments 
rst time magazine collection 
collection comprises articles time magazine covering variety topics ranging international politics social gossip 
second experiment larger document collection articles daily austrian newspaper der standard 
remainder structured follows section provides architecture training process selforganizing map followed review related architectures section 
ghsom architecture introduced detail section 
di erent data sets demonstrate characteristics capabilities ghsom model section starting smaller time magazine collection section followed extensive collection articles newspaper der standard section 
remarks conclude section 
self organizing map self organizing map som proposed described thoroughly distinguished arti cial neural network models adhering unsupervised learning paradigm 
som general unsupervised tool ordering high dimensional data way similar items grouped spatially close 
range applications som utilized successfully impressive see fairly bibliography 
model strong tradition text mining area number research groups described self organizing map :10.1.1.32.2117
som consists number neural processing elements units arranged topology common choice marked dimensional rectangular hexagonal grid 
units assigned model vector important note model vectors dimensionality input patterns 
training process soms may described terms input pattern presentation model vector adaptation 
training iteration starts random selection input pattern pattern som unit determines activation 
usually euclidean distance input pattern model vector calculate unit activation 
case unit having model vector smallest euclidean distance input pattern referred winner 
index denoting winner cf 
expression 
arg min jjg model vector winner model vectors units vicinity winner adapted 
adaptation implemented gradual reduction di erence corresponding components input pattern model vector shown expression 
note discrete time notation denoting current training iteration 

ci 
geometrically speaking model vectors adapted units moved bit input pattern 
amount model vector movement guided learning rate decreasing time 
number units ected adaptation strength adaptation depending unit distance winner determined neighborhood function ci number units decreases time training process winner adapted 
typically neighborhood function unimodal function symmetric location winner monotonically decreasing increasing distance winner 
gaussian neighborhood function expression 
ci exp jjr jj 
expression jjr jj denotes distance units output space representing dimensional location vector unit grid 
time varying parameter guides reduction neighborhood kernel training 
common practice neighborhood kernel selected large cover wide area output space learning 
spatial width kernel reduced gradually training process just winner adapted 
related architectures years number modi cations suggested enhance usefulness self organizing map data mining applications 
particular identi cation inter intra cluster similarity addressed 
approaches matrix adaptive coordinates cluster connections represent techniques put emphasis detection visualization cluster structures soms techniques analyze distances neighboring units mirror ect model vector adaptation dimensional output space 
similar cluster information obtained labelsom method described :10.1.1.40.4095
labelsom method characteristics various units described terms shared features input patterns mapped particular unit 
grouping units characteristics allows identify clusters output space som modi cations architecture address problems arising mapping lattice structure providing smooth manifold output space self organizing field generative topographic mapping gtm 
methods identi ed facilitates detection hierarchical structure inherent data adapt size network 
hierarchical feature map tries uncover hierarchical structure data modifying som architecture 
training som balanced hierarchical structure soms trained 
data mapped single unit represented level detail lower level map assigned unit 
model merely represents data hierarchical way really re ecting hierarchical structure data 
architecture network de ned advance number layers sizes maps layer xed prior network training 
leads de nition balanced tree represent data 
desirable network architecture de ned peculiarities input data 
tree structured som hierarchical modi cation som introduced 
focus model computational speedup winner selection treebased organization units 
model provide hierarchical decomposition input space input patterns organized single som shortcoming having de ne size som advance addressed number di erent models 
consider example incremental grid growing growing grid growing som som 
rst incremental grid growing allows addition new units boundary map 
furthermore connections units map may established removed threshold settings similarity respective model vectors 
may result separated irregularly shaped map structures 
quite similar spirit growing som uses spread factor control growth process map 
manual intervention supervisor decide train separate soms speci units order obtain detailed representation 
obviously results manually created hierarchies approach basically possible variant som growing grid hand adds rows columns units training process starting som initially units 
decision insert new units governed computation measure unit winner counter 
extension som allows growth process som dimensions providing improved data representation visual interpretability 
stated main focus adaptive variants selforganizing map lies equal distribution input patterns map adding new units neighborhood units represent high number input data 
primarily re ect concept representation certain level detail expressed terms quantization error number input data mapped speci areas 
adaptive models takes inherently hierarchical structure data account 
growing hierarchical self organizing map principles som proven suitable tool detecting structure high dimensional data organizing accordingly twodimensional output space shortcomings mentioned 
include inability capture inherent hierarchical structure data 
furthermore size map determined advance proper insight characteristics data distribution available 
drawbacks addressed separately modi ed architectures som outlined section 
approaches provides architecture fully adapts characteristics input data 
overcome limitations sized non hierarchically adaptive architectures developed growing hierarchical self organizing map ghsom dynamically ts multilayered architecture structure data 
ghsom hierarchical structure multiple layers layer consists independent growing self organizing maps starting top level map map similar growing grid model grows size represent collection data speci level detail 
certain improvement regarding granularity data representation reached units analyzed see represent data speci minimum level granularity 
units layer layer layer layer trained ghsom ghsom evolves structure soms re ecting hierarchical structure input data 
represent diverse input data expanded form new small growing som subsequent layer respective data shall represented detail 
new maps grow size speci ed improvement quality data representation reached 
units representing homogeneous set data hand require expansion subsequent layers 
resulting ghsom fully adaptive re ect architecture hierarchical structure inherent data allocating space representation inhomogeneous areas input space 
graphical representation ghsom 
map layer consists units provides rough organization main clusters input data 
independent maps second layer er detailed view data 
input data map subset mapped corresponding unit upper layer 
units second layer maps expanded third layer maps provide suciently granular input data representation 
noted maps di erent sizes structure data relieves burden prede ning structure architecture 
layer serves representation complete data set necessary control growth process 
training algorithm initial setup global network control principle ghsom architecture adaptation training data 
quality adaptation measured terms deviation unit model vector input vectors represented particular unit 
basically di erent strategies control growth process mean quantization error unit commonly quality measure data representation soms absolute value quantization error unit 
formally mean quantization error unit calculated expression mean euclidean distance model vector input vectors elements set input vectors mapped unit km jc starting point ghsom training process calculation mean quantization error unit forming layer map provided expression 
refer number input vectors input data set denotes mean input data 
km jij measures dissimilarity input data mapped particular unit control growth process neural network 
speci cally minimum quality data representation unit speci ed fraction indicated parameter words units represent respective subsets data mean quantization error smaller fraction satisfy global termination criterion speci ed expression 
units satisfying condition detailed data representation required leading addition units provide map space data representation 
alternatively quantization error unit expression denoted qe may mean quantization error resulting global termination criterion expression 
qe km qe 
qe data distribution global quality measure ghsom training process may intuitive qe follows closely principle characteristic selforganizing maps providing map space densely populated regions input space referred magni cation factor 
expression expression global stopping criterion produces maps intuitively re ect characteristics data distributions capturing ner di erences densely populated clusters 
ect speci cally important resulting maps shall explorative interfaces data sets frequently case som architectures 
remainder quantization error qe basis ghsom training 
decision qe global control training process ghsom architecture initialized creating new growing som beneath layer map 
initial size rst layer map set units model vectors initialized random values 
training growth process growing som newly created map trained standard som training procedure described section 
xed number training iterations units provided expression analyzed 
high qe indicates inhomogeneous part input space containing dissimilar data large set input data homogenous part input space represented unit 
new units needed provide space appropriate data representation 
unit highest qe selected denoted error unit 
refer error unit dissimilar neighboring unit terms input space distance selected 
done comparing model vectors neighboring units model vector error unit new row column units inserted error unit dissimilar neighbor model vectors new units initialized average corresponding neighbors 
shows graphical representation insertion process realization growing som newly inserted units depicted shaded circles 
arrows point respective neighboring units model vector initialization 
formally growth process growing som described follows 
subset vectors input data mapped unit model vector unit error unit determined unit maximum qe provided expression arg max km jc please note mean quantization error may quantization error resulting ghsom architecture focusing homogeneity data representation capturing higher degrees detail populated areas data space 
selection error unit dissimilar neighbor determined listed expression set neighboring units insertion row insertion column insertion units row column units shaded gray inserted error unit neighboring unit largest distance model vector model vector euclidean space 
error unit arg max km row column units inserted obtain smooth positioning newly added units input space model vectors initialized means respective neighbors 
insertion learning rate neighborhood range reset original values training continues som fashion iterations 
training process single growing som highly similar growing grid model 
di erence far decreasing learning rate decreasing neighborhood range xed values 
termination growth process units added growing som decrease units represents smaller concise subset input space 
basically training process continue units satisfy global stopping criterion represent respective subset data granularity lower certain fraction initial standard deviation data resulting large som representing data required granularity layer 
order reveal hierarchical structure data map shall explain portion data similarity 
growth process continues map mean quantization error referred capital letters reaches certain fraction qe corresponding unit upper layer unit constituting layer map rst layer map 
map computed mean units quantization errors qe cf 
expression subset maps units data mapped nu qe nu ju general terms stopping criterion growth single map de ned 
qe qe quantization error corresponding unit upper layer 
obviously smaller parameter chosen larger resulting map explaining data higher granularity 
larger detailed data representation delegated additional maps hierarchy 
parameter serves control parameter depth resulting hierarchical ghsom architecture 
case rst layer map stopping criterion training process 
qe hierarchical growth global map orientation training map nished criterion speci ed expression unit checked ful llment global stopping criterion expression 
units representing set diverse input vectors expanded form new map subsequent layer layer layer bde de cef ef ab egh abd bc gh fhi bce hi de deg ef efh initialization units corner units newly created maps initialized preserve orientation parents map 
hierarchy 
units satisfying global stopping criterion require expansion 
similar procedure chosen creation layer map originating single unit map layer new map initially units created 
random initialization chosen model vector orientation usually distort global topology neighboring maps 
orientation data map subclusters located area map determined selforganizing process random orientation map data space 
navigation maps layer hierarchy prevented leading serious disadvantages organization disjoint clusters hierarchical manner especially larger clusters split 
provide global orientation individual maps various layers hierarchy orientation conform orientation data distribution parent map 
achieved creating coherent initialization units newly created map 
unit expanded form new map subsequent layer hierarchy 
map model vectors initialized mirror orientation neighboring units parent provides illustration initialization new maps uence par ent unit neighbors 
geometrically speaking model vectors corner units moved data space directions respective parent neighbors certain fraction 
initial orientation map preserved training process 
new units may inserted corner units similar respective corner units maps neighboring branches 
exact amount corner units moved respective directions uence characteristic topology preserving initialization 
choose set mean parent neighbors respective directions setting 
simplest case neighbors model vectors may directly initial corner positions new maps data space 
input vectors train newly added map ones mapped unit just expanded subset data space mapped parent 
map continue grow procedures detailed section 
process repeated subsequent layers global stopping criterion expression met leaf units 
analysis ghsom characteristics som ers analysis large highdimensional data collections allowing shortcuts accelerate training process see concise treatment :10.1.1.32.2117
additionally due hierarchical structuring partitioning input data ghsom provides improved scalability 
transition layer number input vectors training particular map decreases subset vectors mapped respective upper layer unit 
furthermore map hierarchy explains particular set characteristics input data 
subsequently input vector components features data set expected identical input vectors mapped speci unit 
features ignored transition subsequent layer hierarchy allowing shorten input vectors 
especially high dimensional sparse data sets ect allows signi cant reduction computational ort 
training growth process ghsom entirely data driven requiring prior knowledge estimates parameter speci cation 
hierarchical structure data represented di erent forms favoring lower hierarchies detailed re nements subsequent layer deeper hierarchies provide stricter separation various sub clusters assigning separate maps 
parameter control trade shallow deep hierarchies 
rst case prefer larger maps layer explain larger portions data representation providing hierarchical structuring 
extreme example consider growing grid grows size explaining complete structure data single map 
ignores hierarchical information tries best preserve mapping various clusters structure 
hand consider setting large requires limited growth individual maps resulting deeper hierarchical structure small maps focusing hierarchical structure 
basically total number units lowest level maps may expected similar cases number neural processing units necessary representing data required level granularity 
principle choice may crucial result arbitrary separation larger cluster homogeneous data distribution sub clusters di erent branches 
due global orientation provided initialization model vectors new maps navigation maps layer hierarchy facilitated setting damage cluster separation 
furthermore analyzing distances input space model vectors neighboring map boundaries similarity neighboring maps detected indicated 
exploratory data analysis homogeneous distribution data samples map space desired allowing capture ner di erences clusters densely populated areas data space 
quantization error unit mean quantization error shown produce favorable results 
global stopping criterion absolute value fraction speci ed parameter directly uences size resulting ghsom number units available data space representation 
noted training process usually lead balanced hierarchy terms branches having depth 
main advantages ghsom hierarchical feature map structure hierarchy adapts requirements input space 
areas input space require units appropriate data representation create deeper branches 
experiments experiments information retrieval application testbed growing hierarchical self organizing map 
nutshell topical clusters shall detected collection free form documents documents covering similar topics grouped 
application domain represents ideal challenging scenario clustering algorithms typically highdimensional feature spaces involved 
furthermore data considered highly noisy result indexing process approximately capture content particular document 
di erent experimental settings focusing di erent characteristics ghsom section classic time magazine collection compare characteristics features hierarchical structuring data collection respect som section analyze characteristics shallow deep hierarchies depending various settings parameter bene ts preservation global orientation ghsom hierarchy 
experiments larger collection news articles austrian newspaper der standard 
experiments implementation ghsom architecture available project homepage interactive evaluation 
experiment time magazine data representation rst step documents mapped representation language order enable analysis 
process termed indexing information retrieval literature 
number di erent strategies suggested years information retrieval research 
common representation techniques single term full text indexing text documents accessed various words forming document extracted 
words reduced word stem yielding terms represent documents 
resulting set terms cleared words words appear rarely document collection little uence discrimination different documents just unnecessarily increase computational load classi cation 
vector space model information retrieval documents contained collection represented means feature vectors form representation correspond index terms extracted documents described 
speci value corresponds importance index term describing content particular document hand 
nd lot strategies prescribe importance index term particular document 
loss www ifs tuwien ac andi generality may assume importance represented scalar range zero means particular index term absolutely unimportant describe document 
deviation zero proportional increased importance index term hand 
vector space model similarity documents corresponds similarity vector representations 
time magazine article collection consists articles time magazine covering broad range topics political issues social gossip 
indexing process identi ed content terms terms document representation omitting words appear documents 
terms roughly stemmed weighted tf idf term frequency times inverse document frequency weighting scheme 
weighting scheme assigns high values terms appear frequently document rarely document collection terms considered important describing contents document 
feature extraction process vectors describing documents dimensional feature space 
vectors neural network training 
som time magazine collection shows self organizing map trained time magazine data set 
consists units represented table cells number articles mapped individual unit 
articles mapped neighboring units considered similar terms topic deal 
due space considerations articles collection 
selected number units detailed discussion 
nd som succeeded creating topology preserving representation topical clusters articles 
example lower left flat som map time magazine collection corner nd group units representing articles con ict vietnam 
name nd articles unit neighboring unit dealing government crack buddhist monks number articles units neighboring ones covering ghting su ering vietnam war 
cluster documents covering airs middle east located lower right corner map unit cluster keeler air political scandal great britain units 
area units neighboring ones nd articles elections italy possible coalitions units covering elections india 
similarly units map identi ed represent topical cluster news articles 
detailed discussion articles topic clusters map refer 
nd som provide topologically ordered representation various topics notion refer unit located column row map starting upper left corner ghsom layer time magazine article collection ghsom article collection information topical hierarchies identi ed resulting map 
apart nd size map quite large respect number topics identi ed 
mainly size map determined advance information number topical clusters available 
hierarchical archive time magazine collection unit representing mean data points layer ghsom training algorithm starts som rst layer 
training process map continues additional units added quantization error drops certain percentage quantization error unit layer 
resulting rst layer map depicted 
map grown twice adding row column respectively resulting units representing major topics document collection 
convenience list topics various units individual articles gure 
example nd unit represent articles related situation vietnam middle east topics covered unit articles related elections political topics map map map map map map map ghsom layer som unit layer map unit lower left corner name 
rst separation dominant topical clusters article collection maps automatically trained represent various topics detail 
results individual maps second layer representing data respective higher layer unit detail 
units maps expanded distinct soms third layer 
resulting second layer maps depicted 
please note maps second layer grown di erent sizes structure data 
particular nd small maps representing articles unit rst layer map maps units 
detailed look rst map second layer representing unit rst layer nd give clearer representation articles covering situation vietnam 
units map represent articles ghting vietnam war remaining units represent articles internal con ict catholic government buddhist monks 
layer units expanded form separate maps units third layer 
give example hierarchical structures identi ed growing hierarchical self organizing map training process take look map representing articles unit rst layer map 
articles deal political matters rst layer 
common topic displayed detail resulting second layer map 
example nd unit represent articles elections india 
unit expanded form map third layer 
unit nd articles covering elections discussions political coalitions christian democrats italy 
remaining units map deal di erent issues related keeler scandal great britain covering political parliament background information scandal persons involved related issues pertaining elections great britain 
example consider map representing articles unit rst layer 
rst layer nd unit cover articles related east west relationships mainly dealing post war germany relationships germany soviet union nato 
second layer map nd topics separated clearer units covering mainly germany related articles topics represented remaining units 
case expansions maps necessary articles represented sucient detail units second layer map 
comparison som ghsom representation comparing ghsom som identify locations articles second layer maps corresponding som allows view hierarchical structure data map 
nd example cluster vietnam simply forms larger coherent cluster map lower left corner map covering rectangle spanned units 
applies cluster middle east airs represented map unit growing hierarchical self organizing map 
cluster mainly located lower right corner som cluster political airs represented unit rst layer ghsom explained detail subsequent layers spread right hand side som covering units columns rows 
note common topic political issues easily discernible map representation som exactly hierarchical information lost 
subdivision cluster political matters evident consider second layer classi cation topic area various sub topics clearly separated covering indian elections italian coalitions british keeler scandal 
interesting hierarchical structure evident som represented relationships cluster unit ghsom identifying areas som represented branch ghsom nd covers areas 
hand group units upper left corner som representing germany related articles second area upper right area som covers nato related articles cluster 
relationship sub clusters lost large som may size som organization map needs determined rst training steps neighborhood range learning function covers large area som similar situation identi ed smaller clusters scattered di erent areas som nicely combined rst layer growing hierarchical self organizing map analyzed separated independent sub clusters subsequent layers 
interesting feature ghsom want emphasize reduction map size 
analysis second layer ghsom represent data level topical detail corresponding som number units individual second layer maps combined opposed units som ghsom model number units determined automatically necessary number units created level detail representation required respective layer 
furthermore branches grown depth hierarchy 
seen units expanded third layer map 
resulting maps layers hierarchy small activation calculation winner evaluation ghsom orders magnitude faster som model 
apart speed gained reduced network size orientation user highly improved compared huge maps easily comprehended 
experiment hierarchies newspaper articles der standard data representation second experiment take closer look uence parameter providing tradeo shallow deep hierarchies topology preserving orientation various maps di erent branches hierarchy 
larger collection articles austrian newspaper der standard covering second quarter 
map training vector space representation single documents created full text indexing 
de ning language content speci word lists discard terms appear articles 
vector dimensionality unique terms 
articles represented automatically extracted dimensional feature vectors word histograms weighted tf idf weighting scheme normalized unit length 
feature vectors train ghsom layer map evaluates serving basis global stopping criterion 
deep hierarchy training ghsom parameters results deep hierarchical structure layers 
impossible complete topic hierarchy months news articles concentrate sample topical sections 
rst layer map depicted grown size units expanded subsequent layers 
separated main topical branches nd sports culture radio tv programs political situation national politics business weather reports name 
topics clearly identi able automatically extracted keywords labelsom technique weather sun reach degrees section weather reports branch articles covering political situation located upper left corner top layer map labeled uno :10.1.1.40.4095
nd branch national politics lower right corner map listing largest political parties austria key provide english translations original german labels 
politicians labels 
unit expanded form map second layer shown 
upper left area map dominated articles related freedom party example articles focusing social democrats located lower left corner 
dominant clusters map neutrality elections european parliament unit carrying speci cally political parties term election labels 
units second layer map expanded third layer example unit lower right corner representing articles related coalition people party social democrats 
articles represented detail map third layer 
shallow hierarchy show ects di erent parameter settings trained second ghsom set half previous value absolute granularity data representation remained unchanged 
leads shallow hierarchical structure layers rst layer map evolving size units 
nd dominant branches example sports located upper right corner map national politics lower right corner internet related articles left hand side map name 
large size resulting rst layer map ne grained representation data provided layer 
results larger clusters represented neighboring units rst layer split lower layer hierarchy 
example nd cluster national politics represented neighboring units 
position covers solely articles related freedom party political leader org representing dominant political topics austria time resulting accordingly large number news articles covering topic 
neighboring unit right located lower right corner position covers aspects national politics main topics elections european parliament 
shows second layer maps 
nd articles related freedom party second branch covering general national politics reporting role campaigns elections european parliament 
expected closely related articles freedom party located neighboring branch left 
obviously left hand side map allow transition map continuous orientation topics 
initialization added maps training process continuous orientation preserved easily seen automatically extracted labels provided 
continuing second layer map unit right reach second layer map unit rst nd articles focusing freedom party moving social democrats people party green party liberal party 
units layer branches quantization error unit expanded third layer 
nd global orientation preserved map 
cluster national politics split dominant subclusters shallow hierarchy articles organized correctly separate maps second layer hierarchy 
allows user continue exploration map boundaries 
purpose labels upper layer neighboring unit serves general guideline topic covered neighboring map 
deeper hierarchy sub clusters represented single branch second layer covering upper lower area map respectively 
sports internal affairs radio tv culture weather business neutrality freedom party social democrats eu elections top layer map units showing general topics second layer map units representing national politics 
described growing hierarchical self organizing map ghsom 
characteristic feature novel neural network model adaptive architecture grows unsupervised training process uncover hierarchical structure analyzed data collection 
nutshell ghsom layered architecture composed independent som neural networks 
networks determine structure training process 
major bene ts ghsom model compared standard som 
training time largely reduced necessary number units developed organize data collection certain degree detail 
second ghsom uncovers hierarchical structure data architecture allowing user understand analyze large amounts data explorative way 
third various emerging maps layer hierarchy small size easier user keep overview various clusters 
ensuring consistent global orientation individual maps respective layers topological similarities neighboring maps preserved 
navigation map boundaries facilitated allowing exploration similar clusters represented neighboring branches ghsom structure 
shown potential ghsom information retrieval application topical clustering free form documents 
nature challenging application clustering algorithms high dimensional noisy feature spaces 
results experiments clearly indicated ghsom successfully identi ed topical clusters document collections 
jain murty flynn data clustering review acm computing surveys vol :10.1.1.18.2720
pp 
september kohonen self organized formation topologically correct feature maps neutral 
wahl neutral 
lif kopf lif neutral 
joerg schmidt joerg joerg joerg klagenfurt klagenfurt joerg joerg joerg martin martin wahl entwurf wolfgang elisabeth inland liste joerg joerg joerg stein joerg stein klagenfurt joerg neighboring second layer maps national politics cal cybernetics vol 
pp 

kaski kangas kohonen bibliography self organizing map som papers neural computing surveys vol 
pp 
blackmore miikkulainen incremental grid growing encoding high dimensional structure dimensional feature map proceedings ieee international conference neural networks icnn san francisco ca usa vol 
pp 
fritzke growing grid self organizing network constant neighborhood range adaption strength neural processing letters vol 
pp 
srinivasan dynamic self organizing maps controlled growth knowledge discovery ieee transactions neural networks vol 
pp 
may 
kohonen self organization associative memory springer verlag berlin germany edition 
kangas kohonen laaksonen variants self organizing maps ieee transactions neural networks vol 
pp 
march kohonen self organizing maps springerverlag berlin 
kohonen kaski lagus honkela saarela selforganization massive document collection ieee transactions neural networks vol :10.1.1.32.2117
pp 
may lin self organizing semantic map information retrieval proceedings 
annual international acm sigir conference research development information retrieval sigir chicago il october pp 
acm merkl text classi cation selforganizing maps lessons learned neurocomputing vol 
pp 

merkl rauber document classi cation unsupervised neural networks soft computing information retrieval crestani pasi eds pp 

physica verlag chen information navigation web clustering summarizing query results information process ing management vol 
pp 

ultsch self organizing neural networks visualization classi cation information classi cation 
concepts methods application opitz lausen eds studies classi cation data analysis knowledge organization pp 

springer dortmund germany april 
merkl rauber alternative ways cluster visualization self organizing maps proceedings workshop self organizing maps kohonen ed espoo finland june 

helsinki university technology pp 
hut rauber labelsom labeling self organizing maps proceedings international joint conference neural networks ijcnn washington dc july 

merkl rauber automatic labeling self organizing maps information retrieval proceedings 
international conference neural information processing iconip perth australia november 

santini self organizing eld ieee transactions neural networks vol 
pp 
november 
bishop williams gtm generative topographic mapping neural computation vol 
pp 
miikkulainen script recognition hierarchical feature maps connection science vol 
pp 

oja self organizing hierarchical feature maps proceedings international joint conference neural networks san diego ca vol 
pp 

fast deterministic selforganizing maps proceedings international conference arti cial neural networks paris france vol 
pp 


bauer villmann growing output space self organizing feature map ieee transactions neural networks vol 
pp 

merkl rauber growing hierarchical self organizing map proceedings international joint conference neural networks ijcnn amari giles gori puri eds como italy july vol 
vi pp 
ieee computer society rauber merkl advances growing hierarchical self organizing map proceedings rd workshop self organizing maps yin slack eds lincoln england june advances self organizing maps pp 
springer salton buckley term weighting approaches automatic text retrieval information processing management vol 
pp 

turtle croft comparison text retrieval models computer journal vol 
pp 

salton automatic text processing transformation analysis retrieval information computer addison wesley reading ma 
rauber merkl selforganizing maps organize document collections characterize subject matters map tell news world proceedings 
international conference database expert systems applications dexa bench capon soda tjoa eds florence italy september 

number lncs lecture notes computer science pp 
springer biographies andreas rauber received sc 
ph degrees computer science vienna university technology vienna austria respectively 
member academic faculty department software technology vienna university technology 
held ercim research fellowship italian national research council cnr pisa italy 
currently ercim research fellow inria paris france 
published papers refereed journals international conferences 
current research interests include neural computation digital libraries information visualization 
dr rauber received award austrian society arti cial intelligence 
dieter merkl received diploma doctoral degrees social economic sciences university vienna vienna austria respectively 
associate professor department software technology vienna university technology vienna austria 
held research position university vienna 
member academic faculty vienna university technology 
visiting research fellow department computer science royal melbourne institute technology melbourne australia 
published articles refereed journals international conferences 
current research interests include neural computation information retrieval software engineering 
dr merkl member ieee computer society 
michael received diploma degree computer science vienna university technology vienna austria 
currently research assistant commerce competence center ec adaptive multilingual interfaces junior researcher department software technology vienna university technology 
published papers international conferences 
currently main research interests include natural language processing cross language information retrieval text mining digital libraries neurocomputing 
erratum article appeared ieee transactions neural networks vol 
pp 
november 
inadvertently corrections included nal release resulting erroneous version article published 
print line version currently contain errors provide correct version article pre print pointers errors article 
section iv page column sentence immediately preceeding expression erroneous version reads error unit determined unit follows read error unit determined unit maximum qe follows comment stands mean quantization error mean qe short 
error unit exhibiting maximum quantization error mean 
obviously eager quest shorter abbreviations longer versions decided abbreviation de ned mean qe 

section iv page column half printed page proofs 
correcting omitted nal version resulting page followed page 

